@inproceedings{10.1145/3341620.3341629,
author = {El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi},
title = {Big Data Quality Metrics for Sentiment Analysis Approaches},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341629},
doi = {10.1145/3341620.3341629},
abstract = {In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {36–43},
numpages = {8},
keywords = {Sentiment analysis, Big data, Big data quality metrics},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3419604.3419803,
author = {Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir},
title = {Towards a Data Quality Assessment in Big Data},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419803},
doi = {10.1145/3419604.3419803},
abstract = {In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {16},
numpages = {6},
keywords = {Quality Models, Data Quality, Data Quality evaluation, Big Data},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.1145/3281022.3281026,
author = {Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario},
title = {From Big Data to Smart Data: A Data Quality Perspective},
year = {2018},
isbn = {9781450360548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281022.3281026},
doi = {10.1145/3281022.3281026},
abstract = {Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company’s core business data, using typically large datasets. However, data that doesn’t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI…). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to “smartizing” data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {Big Data, Data Quality, Smart Data},
location = {Lake Buena Vista, FL, USA},
series = {EnSEmble 2018}
}

@inproceedings{10.1145/3010089.3010090,
author = {Emmanuel, Isitor and Stanier, Clare},
title = {Defining Big Data},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010090},
doi = {10.1145/3010089.3010090},
abstract = {As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {5},
numpages = {6},
keywords = {Big Data characteristics, Data Quality Dimensions, Data Quality, Big Data},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/2513591.2527071,
author = {Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.},
title = {Big Data: A Research Agenda},
year = {2013},
isbn = {9781450320252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513591.2527071},
doi = {10.1145/2513591.2527071},
abstract = {Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.},
booktitle = {Proceedings of the 17th International Database Engineering &amp; Applications Symposium},
pages = {198–203},
numpages = {6},
keywords = {OLAP over big data, privacy of big data, big data, big data posting},
location = {Barcelona, Spain},
series = {IDEAS '13}
}

@inproceedings{10.1145/2658840.2658845,
author = {Neamtu, Rodica and Ahsan, Ramoza and Stokes, Jeff and Hoxha, Armend and Bao, Jialiang and Gvozdenovic, Stefan and Meyer, Ted and Patel, Nilesh and Rangan, Raghu and Wang, Yumou and Zhang, Dongyun and Rundensteiner, Elke A.},
title = {Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness Analytics},
year = {2014},
isbn = {9781450331869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658840.2658845},
doi = {10.1145/2658840.2658845},
abstract = {In an era where Big Data can greatly impact a broad population, many novel opportunities arise, chief among them the ability to integrate data from diverse sources and "wrangle" it to extract novel insights. Conceived as a tool that can help both expert and non-expert users better understand public data, MATTERS was collaboratively developed by the Massachusetts High Tech Council, WPI and other institutions as an analytic platform offering dynamic modeling capabilities. MATTERS is an integrative data source on high fidelity cost and talent competitiveness metrics. Its goal is to extract, integrate and model rich economic, financial, educational and technological information from renowned heterogeneous web data sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the Institute of Education Sciences, all known to be critical factors influencing economic competitiveness of states. This demonstration of MATTERS illustrates how we tackle challenges of data acquisition, cleaning, integration and wrangling into appropriate representations, visualization and story-telling with data in the context of state competitiveness in the high-tech sector.},
booktitle = {Proceedings of the First International Workshop on Bringing the Value of "Big Data" to Users (Data4U 2014)},
pages = {25–28},
numpages = {4},
keywords = {data integration, Big data, diverse data sources},
location = {Hangzhou, China},
series = {Data4U '14}
}

@inproceedings{10.1145/3141128.3141139,
author = {Pti\v{c}ek, Marina and Vrdoljak, Boris},
title = {Big Data and New Data Warehousing Approaches},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141139},
doi = {10.1145/3141128.3141139},
abstract = {Big data are a data trend present around us mainly through Internet -- social networks and smart devices and meters -- mostly without us being aware of them. Also they are a fact that both industry and scientific research needs to deal with. They are interesting from analytical point of view, for they contain knowledge that cannot be ignored and left unused. Traditional system that supports the advanced analytics and knowledge extraction -- data warehouse -- is not able to cope with large amounts of fast incoming various and unstructured data, and may be facing a paradigm shift in terms of utilized concepts, technologies and methodologies, which have become a very active research area in the last few years. This paper provides an overview of research trends important for the big data warehousing, concepts and technologies used for data storage and (ETL) processing, and research approaches done in attempts to empower traditional data warehouses for handling big data.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {6–10},
numpages = {5},
keywords = {NoSQL, NewSQL, data warehouse, MapReduce, big data, databases},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@article{10.1145/3408314,
author = {Davoudian, Ali and Liu, Mengchi},
title = {Big Data Systems: A Software Engineering Perspective},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3408314},
doi = {10.1145/3408314},
abstract = {Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {110},
numpages = {39},
keywords = {software engineering, quality assurance, software reference architecture, Big Data systems, requirements engineering, Big Data}
}

@inproceedings{10.1145/2811222.2811235,
author = {Abell\'{o}, Alberto},
title = {Big Data Design},
year = {2015},
isbn = {9781450337854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811222.2811235},
doi = {10.1145/2811222.2811235},
abstract = {It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.},
booktitle = {Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP},
pages = {35–38},
numpages = {4},
keywords = {nosql, database design, big data},
location = {Melbourne, Australia},
series = {DOLAP '15}
}

@inproceedings{10.1145/3366030.3366121,
author = {Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim},
title = {Data Source Selection in Big Data Context},
year = {2019},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366121},
doi = {10.1145/3366030.3366121},
abstract = {Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {611–616},
numpages = {6},
keywords = {Big Data Source Selection, Data quality, Big Data integration, Source reliability},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3538950.3538951,
author = {Zhang, Lin and Jiang, Rong and Wang, Meng and Yang, Yue and Wang, Chenguang},
title = {A Drug Safety Traceability Model Based on Big Data},
year = {2022},
isbn = {9781450395632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538950.3538951},
doi = {10.1145/3538950.3538951},
abstract = {In order to solve the storage and management problems of heterogeneous drug data, this paper uses big data technology to complete the cleaning and distributed storage of drug data, and improve the function of data sharing and traceability. At the same time, in order to improve the drug traceability function, ensure the reliable storage of traceability information, and make the traceability process more reliable. This paper will put forward a drug traceability system model based on big data on the basis of existing research. Secondly, an evidence chain framework is proposed to verify evidence files. At last, the simulation experiment is carried out to test and illustrate the credibility of the traceability verification model.},
booktitle = {Proceedings of the 4th International Conference on Big Data Engineering},
pages = {1–7},
numpages = {7},
keywords = {Big Data, Traceability Verification, Drug Traceability},
location = {Beijing, China},
series = {BDE '22}
}

@inproceedings{10.1145/2656346.2656358,
author = {Mohammad, Atif and Mcheick, Hamid and Grant, Emanuel},
title = {Big Data Architecture Evolution: 2014 and Beyond},
year = {2014},
isbn = {9781450330282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656346.2656358},
doi = {10.1145/2656346.2656358},
abstract = {This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.},
booktitle = {Proceedings of the Fourth ACM International Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {139–144},
numpages = {6},
keywords = {big data, cloud computing},
location = {Montreal, QC, Canada},
series = {DIVANet '14}
}

@inproceedings{10.1145/2640087.2644168,
author = {Cho, Wonhee and Lim, Yoojin and Lee, Hwangro and Varma, Mohan Krishna and Lee, Moonsoo and Choi, Eunmi},
title = {Big Data Analysis with Interactive Visualization Using R Packages},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644168},
doi = {10.1145/2640087.2644168},
abstract = {Compared to the traditional data storing, processing, analyzing and visualization which have been performed, Big data requires evolutionary technologies of massive data processing on distributed and parallel systems, such as Hadoop system. Big data analytic systems, thus, have been popular to derive important decision making in various areas. However, visualization on analytic system faces various limitation due to the huge amount of data. This brings the necessity of interactive visualization techniques beyond the traditional static visualization. R has been used and improved for a big data analysis and mining tool. Also, R is supported with various and abundant packages for different targets with visualization. However interactive visualization packages are not easily found in the market. This paper compares and analyzes interactive web packages with visualization packages for R. This paper also proposes interactive web visualized analysis environment for big data with a combination of interactive web packages and visualization packages. In particular, Big data analysis techniques with sensed data are presented as the result by reflecting the decision view on sensing field.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {18},
numpages = {6},
keywords = {Visualization, Mining, R, Big data, Hadoop},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/3206157.3206166,
author = {Khan, Nawsher and Alsaqer, Mohammed and Shah, Habib and Badsha, Gran and Abbasi, Aftab Ahmad and Salehian, Soulmaz},
title = {The 10 Vs, Issues and Challenges of Big Data},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206166},
doi = {10.1145/3206157.3206166},
abstract = {In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {52–56},
numpages = {5},
keywords = {Big Data, Data Management},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/2699026.2699136,
author = {Thuraisingham, Bhavani},
title = {Big Data Security and Privacy},
year = {2015},
isbn = {9781450331913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2699026.2699136},
doi = {10.1145/2699026.2699136},
abstract = {This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.},
booktitle = {Proceedings of the 5th ACM Conference on Data and Application Security and Privacy},
pages = {279–280},
numpages = {2},
keywords = {security, big data, privacy},
location = {San Antonio, Texas, USA},
series = {CODASPY '15}
}

@inproceedings{10.1145/3404687.3404694,
author = {Raza, Muhammad Umair and XuJian, Zhao},
title = {A Comprehensive Overview of BIG DATA Technologies: A Survey},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404694},
doi = {10.1145/3404687.3404694},
abstract = {In as much as the approaches of the new revolution, machines including transmission media like social media sites, nowadays quantity of data swell hastily. So, size is the core and only facet that leaps the mention of BIG DATA. In this article, an effort to touch a comprehensive view of big data technologies, because of the swift evolution of data by an industry trying the academic press to catch up. This paper also offers a unified explanation of big data as well as the analytics methods. A practical discriminate characteristic of this paper is core analytics associated with unstructured data which is more than 90% of big data. To deal with complicated Big Data problems, great work has been done. This paper analyzes contemporary Big Data technologies. Therein article further strengthens the necessity to formulate new tools for analytics. It bestows not sole an intercontinental overview of big data techniques even though the valuation according to big data Hadoop Ecosystem. It classifies and debates the main technologies feature, challenges, and usage as well.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {23–31},
numpages = {9},
keywords = {HDFS, MapReduce, Apache Hadoop, Big Data Technology},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/2896825.2896837,
author = {Chen, Hong-Mei and Kazman, Rick and Garbajosa, Juan and Gonzalez, Eloy},
title = {Toward Big Data Value Engineering for Innovation},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896837},
doi = {10.1145/2896825.2896837},
abstract = {This article articulates the requirements for an effective big data value engineering method. It then presents a value discovery method, called Eco-ARCH (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for addressing these requirements, filling a methodological void. Eco-ARCH promotes a fundamental shift in design thinking for big data system design -- from "bounded rationality" for problem solving to "expandable rationality" for design for innovation. The Eco-ARCH approach is most suitable for big data value engineering when system boundaries are fluid, requirements are ill-defined, many stakeholders are unknown and design goals are not provided, where no architecture pre-exists, where system behavior is non-deterministic and continuously evolving, and where co-creation with consumers and prosumers is essential to achieving innovation goals. The method was augmented and empirically validated in collaboration with an IT service company in the energy industry to generate a new business model that we call "eBay in the Grid".},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {energy industry, innovation, ecosystem, value discovery, architecture landscape, value engineering, big data},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3216122.3216124,
author = {Cappiello, Cinzia and Sam\'{a}, Walter and Vitali, Monica},
title = {Quality Awareness for a Successful Big Data Exploitation},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216124},
doi = {10.1145/3216122.3216124},
abstract = {The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed. We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario. We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {37–44},
numpages = {8},
keywords = {Data Quality Assessment, Veracity, Big Data},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@inproceedings{10.1145/2593882.2593889,
author = {Mockus, Audris},
title = {Engineering Big Data Solutions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593889},
doi = {10.1145/2593882.2593889},
abstract = {Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.},
booktitle = {Future of Software Engineering Proceedings},
pages = {85–99},
numpages = {15},
keywords = {Game Theory, Data Quality, Operational Data, Data Science, Data Engineering, Statistics, Analytics},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.5555/2819289.2819302,
author = {Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha},
title = {Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm},
year = {2015},
publisher = {IEEE Press},
abstract = {Big data system development is dramatically different from small (traditional, structured) data system development. At the end of 2014, big data deployment is still scarce and failures abound. Outsourcing has become a main strategy for many enterprises. We therefore selected an outsourcing company who has successfully deployed big data projects for our study. Our research results from analyzing 10 outsourced big data projects provide a glimpse into early adopters of big data, illuminates the challenges for system development that stem from the 5Vs of big data and crystallizes the importance of architecture design choices and technology selection. We followed a collaborative practice research (CPR) method to develop and validate a new method, called BDD. BDD is the first attempt to systematically combine architecture design with data modeling approaches to address big data system development challenges. The use of reference architectures and a technology catalog are advancements to architecture design methods and are proving to be well-suited for big data system architecture design and system development.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {system engineering, big data, collaborative practice research, data system design methods, embedded case study methodology, software architecture},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/3456389.3456390,
author = {Cao, Shuangshuang},
title = {Opportunities and Challenges of Marketing in the Context of Big Data},
year = {2021},
isbn = {9781450389945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456389.3456390},
doi = {10.1145/3456389.3456390},
abstract = {In the era of big data, under the conditions of rapid economic development in our country, various enterprises have also vigorously carried out marketing. In the context of big data, marketing research should be strengthened to effectively improve market. Market issues ensure that marketing has improved its status in the era of big data. This article has conducted a research and analysis on marketing in the context of big data. And then the opportunities and challenges of marketing in the context of big data has been explained, which gradually optimize the marketing implementation effect. The challenges faced by marketing has been understood which ensures that the big data model plays its best role in it.},
booktitle = {2021 Workshop on Algorithm and Big Data},
pages = {79–82},
numpages = {4},
keywords = {Big data, Marketing, Personalized service},
location = {Fuzhou, China},
series = {WABD 2021}
}

@article{10.1145/2627534.2627561,
author = {Savas, Onur and Sagduyu, Yalin and Deng, Julia and Li, Jason},
title = {Tactical Big Data Analytics: Challenges, Use Cases, and Solutions},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627561},
doi = {10.1145/2627534.2627561},
abstract = {We discuss tactical challenges of the Big Data analytics regarding the underlying data, application space, and com- puting environment, and present a comprehensive solution framework motivated by the relevant tactical use cases. First, we summarize the unique characteristics of the Big Data problem in the Department of Defense (DoD) context and underline the main differences from the commercial Big Data problems. Then, we introduce two use cases, (i) Big Data analytics with multi-intelligence (multi-INT) sensor data and (ii) man-machine crowdsourcing using MapReduce framework. For these two use cases, we introduce Big Data analytics and cloud computing solutions in a coherent frame- work that supports tactical data, application, and computing needs.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {86–89},
numpages = {4},
keywords = {big data, tactical environment, algorithms, analytics, cloud computing}
}

@inproceedings{10.1109/CCGRID.2018.00100,
author = {Cuzzocrea, Alfredo and Damiani, Ernesto},
title = {Pedigree-Ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00100},
doi = {10.1109/CCGRID.2018.00100},
abstract = {This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the "pedigree" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacy-preserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {675–681},
numpages = {7},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/2479724.2479730,
author = {Bertot, John Carlo and Choi, Heeyoon},
title = {Big Data and E-Government: Issues, Policies, and Recommendations},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479730},
doi = {10.1145/2479724.2479730},
abstract = {The promises and potential of Big Data in transforming digital government services, governments, and the interaction between governments, citizens, and the business sector, are substantial. From "smart" government to transformational government, Big Data can foster collaboration; create real-time solutions to challenges in agriculture, health, transportation, and more; and usher in a new era of policy- and decision-making. There are, however, a range of policy challenges to address regarding Big Data, including access and dissemination; digital asset management, archiving and preservation; privacy; and security. This paper selectively reviews and analyzes the U.S. policy context regarding Big Data and offers recommendations aimed at facilitating Big Data initiatives.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {1–10},
numpages = {10},
keywords = {big data, open government},
location = {Quebec, Canada},
series = {dg.o '13}
}

@article{10.1145/3150226,
author = {Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.},
title = {Multimedia Big Data Analytics: A Survey},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3150226},
doi = {10.1145/3150226},
abstract = {With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {10},
numpages = {34},
keywords = {survey, machine learning, Big data analytics, 5V challenges, retrieval, multimedia analysis, multimedia databases, mobile multimedia, indexing, data mining}
}

@inproceedings{10.1145/3335484.3335545,
author = {Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang},
title = {Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335545},
doi = {10.1145/3335484.3335545},
abstract = {With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {72–76},
numpages = {5},
keywords = {large-scale complex systems, big data, evaluation, effectiveness},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@inproceedings{10.1145/2896387.2900335,
author = {Cuzzocrea, Alfredo},
title = {Warehousing and Protecting Big Data: State-Of-The-Art-Analysis, Methodologies, Future Challenges},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900335},
doi = {10.1145/2896387.2900335},
abstract = {This paper proposes a comprehensive critical survey on the issues of warehousing and protecting big data, which are recognized as critical challenges of emerging big data research. Indeed, both are critical aspects to be considered in order to build truly, high-performance and highly-flexible big data management systems. We report on state-of-the-art approaches, methodologies and trends, and finally conclude by providing open problems and challenging research directions to be considered by future efforts.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {14},
numpages = {7},
keywords = {Protecting Big Data, Big Data Analytics, Big Data, Warehousing Big Data},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/3209281.3209372,
author = {Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher G.},
title = {Census Big Data Analytics Use: International Cross Case Analysis},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209372},
doi = {10.1145/3209281.3209372},
abstract = {Despite the growing practices in big data and big data analytics use, there is still the paucity of research on links between government big data analytics use and public value creation. This multi-case study of Australia, Ireland, Mexico, and U.S.A. examines the state of big data and big data analytics use in the national census context. The census agencies are at varying stages in digitally transforming their national census process, products and services through assimilating and using big data and big data analytics. The cross-case analysis of government websites and documents identified emerging agency challenges in creating public value in the national census context: (1) big data analytics capability development, (2) cross agency data access and data integration, and (3) data security, privacy &amp; trust. Based on the insights gained, a research model aims to postulate the possible links among challenges, big data/big data analytics use, and public value creation.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {10},
numpages = {10},
keywords = {big data challenges, census big data, public value creation, use, big data analytics, electronic census, cross case analysis},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3289100.3289108,
author = {Bahadi, Jihane and El Asri, Bouchra and Courtine, M\'{e}lanie and Rhanoui, Maryem and Kergosien, Yannick},
title = {Towards Efficient Big Data: Hadoop Data Placing and Processing},
year = {2018},
isbn = {9781450365079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289100.3289108},
doi = {10.1145/3289100.3289108},
abstract = {Currently, the generated data flow is growing at a high rate resulting to the problem of data obesity and abundance, but yet a lack of pertinent information. To handle this Big Data, Hadoop is a distributed framework that facilitates data storage and processing. Although Hadoop is designed to deal with demands of storage and analysis of ever-growing Data, its performance characteristics are still to improve. In this regard, many approaches have been proposed to enhance Hadoop capabilities. Nevertheless, an overview of these approaches shows that several aspects need to be improved in terms of performance and data relevancy. The main challenge is how to extract efficiently value from the big data sources. For this purpose, we propose in this paper to discuss Hadoop architecture and intelligent data discovery, and propose an effective on-demand Big Data contribution enabling to process relevant data in efficient and effective way according to the stakeholder's needs, and aiming to boost Data appointment by integrating multidimensional approach.},
booktitle = {Proceedings of the 2nd International Conference on Smart Digital Environment},
pages = {42–47},
numpages = {6},
keywords = {Intelligent processing, Multidimensional approach, Big Data, Hadoop, Data placing, MapReduce jobs},
location = {Rabat, Morocco},
series = {ICSDE'18}
}

@article{10.1145/3492546,
author = {Johnson, Justin M and Khoshgoftaar, Taghi M},
title = {A Survey on Classifying Big Data with Label Noise},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3492546},
doi = {10.1145/3492546},
abstract = {Class label noise is a critical component of data quality that directly inhibits the predictive performance of machine learning algorithms. While many data-level and algorithm-level methods exist for treating label noise, the challenges associated with big data call for new and improved methods. This survey addresses these concerns by providing an extensive literature review on treating label noise within big data. We begin with an introduction to the class label noise problem and traditional methods for treating label noise. Next, we present 30 methods for treating class label noise in a range of big data contexts, i.e. high volume, high variety, and high velocity problems. The surveyed works include distributed solutions capable of operating on data sets of arbitrary sizes, deep learning techniques for large-scale data sets with limited clean labels, and streaming techniques for detecting class noise in the presence of concept drift. Common trends and best practices are identified in each of these areas, implementation details are reviewed, empirical results are compared across studies when applicable, and references to 17 open-source projects and programming packages are provided. An emphasis on label noise challenges, solutions, and empirical results as they relate to big data distinguishes this work as a unique contribution that will inspire future research and guide machine learning practitioners.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = {oct},
keywords = {machine learning, label noise, big data, classification, deep learning, data streams, data quality}
}

@article{10.1145/3331651.3331659,
author = {Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral, Janet and Kwan, Bethany and Ressalam, Julie},
title = {Co-Designing Learning Materials to Empower Laypersons to Better Understand Big Data and Big Data Methods},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/3331651.3331659},
doi = {10.1145/3331651.3331659},
abstract = {University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.},
journal = {SIGKDD Explor. Newsl.},
month = {may},
pages = {41–44},
numpages = {4},
keywords = {big data, co-design, education, community engagement}
}

@inproceedings{10.1145/2351316.2351318,
author = {Gopalkrishnan, Vivekanand and Steier, David and Lewis, Harvey and Guszcza, James},
title = {Big Data, Big Business: Bridging the Gap},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351316.2351318},
doi = {10.1145/2351316.2351318},
abstract = {Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of "Big Data" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of "Big Data" to start from a well-defined business goal, and remain moored to fundamental principles of both cost/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving "Big Data", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.},
booktitle = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {7–11},
numpages = {5},
location = {Beijing, China},
series = {BigMine '12}
}

@inproceedings{10.1109/MET.2019.00019,
author = {Auer, Florian and Felderer, Michael},
title = {Addressing Data Quality Problems with Metamorphic Data Relations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MET.2019.00019},
doi = {10.1109/MET.2019.00019},
abstract = {In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.},
booktitle = {Proceedings of the 4th International Workshop on Metamorphic Testing},
pages = {76–83},
numpages = {8},
keywords = {data quality, quality assessment, metamorphic data relations, big data, metamorphic testing},
location = {Montreal, Quebec, Canada},
series = {MET '19}
}

@inproceedings{10.1145/2723372.2742794,
author = {Huang, Yiqing and Zhu, Fangzhou and Yuan, Mingxuan and Deng, Ke and Li, Yanhua and Ni, Bing and Dai, Wenyuan and Yang, Qiang and Zeng, Jia},
title = {Telco Churn Prediction with Big Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742794},
doi = {10.1145/2723372.2742794},
abstract = {We show that telco big data can make churn prediction much more easier from the $3$V's perspectives: Volume, Variety, Velocity. Experimental results confirm that the prediction performance has been significantly improved by using a large volume of training data, a large variety of features from both business support systems (BSS) and operations support systems (OSS), and a high velocity of processing new coming data. We have deployed this churn prediction system in one of the biggest mobile operators in China. From millions of active customers, this system can provide a list of prepaid customers who are most likely to churn in the next month, having $0.96$ precision for the top $50000$ predicted churners in the list. Automatic matching retention campaigns with the targeted potential churners significantly boost their recharge rates, leading to a big business value.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {607–618},
numpages = {12},
keywords = {big data, customer retention, telco churn prediction},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3047273.3047377,
author = {Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil and Meijer, Ronald},
title = {Exploiting Big Data for Evaluation Studies},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047377},
doi = {10.1145/3047273.3047377},
abstract = {The collection and analysis of relevant data for evaluating public policies is not a straightforward task. An important type of such studies is the so-called ex-post evaluation. The main objective of ex-post evaluations is to determine to what extent a realized intervention is successful in tackling a societal challenge, e.g., youth unemployment. At a first glance an obvious method is to collect some baseline measurements for a set of relevant variables, apply the intervention for a while and collect the new measurement values for the same set of variables. Then, comparing the measurement values of the variables before and after the intervention provides an insight into the extent of successfulness of the intervention. This, however, is only true if the "ceteris paribus" condition holds. In practice it is infeasible to enforce this condition for societal challenges. Often, after having the baseline measurements, several phenomena emerge that may impact the new measurements without being taken into account. This makes it difficult to determine how much of the measured differences between the values of the variables before and after the intervention should be attributed to the emerging phenomena (or the so-called counterfactuals) and how much of the differences can be attributed to the applied intervention.This paper discusses how exploiting big data may contribute to the task of elucidating the influences of counterfactuals (and interventions) in ex-post evaluation studies. The paper proposes a framework to utilize big data for accounting for the impact of emerging phenomena in ex-post evaluation studies.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {228–231},
numpages = {4},
keywords = {data linkage, Big data, counterfactuals, ex-post policy evaluation},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3328833.3328841,
author = {Adenuga, Kayode I. and Muniru, Idris O. and Sadiq, Fatai I. and Adenuga, Rahmat O. and Solihudeen, Muhammad J.},
title = {Big Data in Healthcare: Are We Getting Useful Insights from This Avalanche of Data?},
year = {2019},
isbn = {9781450361057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328833.3328841},
doi = {10.1145/3328833.3328841},
abstract = {The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Information Engineering},
pages = {196–199},
numpages = {4},
keywords = {Challenges, Benefits, Big Data, Analytics},
location = {Cairo, Egypt},
series = {ICSIE '19}
}

@inproceedings{10.1145/3345252.3345282,
author = {Petrova-Antonova, Dessislava and Krasteva, Iva and Ilieva, Sylvia and Pavlova, Irena},
title = {Conceptual Architecture of GATE Big Data Platform},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345282},
doi = {10.1145/3345252.3345282},
abstract = {Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {261–268},
numpages = {8},
keywords = {Emerging Architectures, Big Data Value Chain, GATE Platform, Smart City, Big Data},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@article{10.1145/3148238,
author = {Heinrich, Bernd and Hristova, Diana and Klier, Mathias and Schiller, Alexander and Szubartowicz, Michael},
title = {Requirements for Data Quality Metrics},
year = {2018},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148238},
doi = {10.1145/3148238},
abstract = {Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {12},
numpages = {32},
keywords = {data quality metrics, data quality assessment, Data quality, requirements for metrics}
}

@inproceedings{10.1145/3006299.3006311,
author = {Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Mar\'{\i}n-Torder, Eva},
title = {Towards a Comprehensive Data Lifecycle Model for Big Data Environments},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006311},
doi = {10.1145/3006299.3006311},
abstract = {A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {100–106},
numpages = {7},
keywords = {big data, data lifecycle, data organization, data complexity, data management, vs challenges},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1109/BDC.2014.10,
author = {Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay},
title = {A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning},
year = {2014},
isbn = {9781479918973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BDC.2014.10},
doi = {10.1109/BDC.2014.10},
abstract = {In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},
pages = {16–25},
numpages = {10},
keywords = {Scientific workflow, Ensemble learning, Kepler, Bayesian network, Distributed computing, Hadoop, Big Data},
series = {BDC '14}
}

@inproceedings{10.1145/3175684.3175687,
author = {Kuschicke, Felix and Thiele, Thomas and Meisen, Tobias and Jeschke, Sabina},
title = {A Data-Based Method for Industrial Big Data Project Prioritization},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175687},
doi = {10.1145/3175684.3175687},
abstract = {The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {6–10},
numpages = {5},
keywords = {Project Selection, Framework, Industrial Big Data, Manufacturing, Project Prioritization},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@article{10.1145/2854006.2854008,
author = {Fan, Wenfei},
title = {Data Quality: From Theory to Practice},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2854006.2854008},
doi = {10.1145/2854006.2854008},
abstract = {Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {7–18},
numpages = {12}
}

@inproceedings{10.1145/3063955.3063968,
author = {Wang, Hongzhi and Gao, Hong and Yin, Shenjun and Zhu, Jie},
title = {The Design of Course Architecture for Big Data},
year = {2017},
isbn = {9781450348737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3063955.3063968},
doi = {10.1145/3063955.3063968},
abstract = {Big data is one of the hottest topic in not only academic but also enterprise, which provide grate requirements for the people with knowledge and experiences of big data. However, current education architecture of computer science could not provide sufficient training for big data. For the education for people suitable for big data era, we attempt to design a novel course architecture. Such course architecture will not change the skeleton of traditional course architecture of computer science but just add content and subjects that is adaptive for big data. In this paper, we discuss the goal, architecture and content of the course architecture.},
booktitle = {Proceedings of the ACM Turing 50th Celebration Conference - China},
articleno = {13},
numpages = {6},
keywords = {data science, course architecture, big data},
location = {Shanghai, China},
series = {ACM TUR-C '17}
}

@inproceedings{10.1145/2743065.2743099,
author = {Amudhavel, J. and Padmapriya, V. and Gowri, V. and Lakshmipriya, K. and Kumar, K. Prem and Thiyagarajan, B.},
title = {Perspectives, Motivations and Implications Of Big Data Analytics},
year = {2015},
isbn = {9781450334419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2743065.2743099},
doi = {10.1145/2743065.2743099},
abstract = {As today there is an enormous volume of data, examining these large sets contains structure and unstructured data of different types and sizes; big data analytics is used. Data Analytics allows the user to analyze the unusable data to make a faster and better decision. The Latest supply chain professionals are suffused with data, which provokes various new ways of thoughts regarding how the data are produced, ordered, controlled and analyzed. Data Quality in Supply-Chain Management is used to monitor and control the data. This paper presents the knowledge infrastructure about trends in big data analytics. Big Data can also be given as an all-encompassing term for any collection of data sets so large or complex that it becomes difficult to process using traditional data processing applications. The challenges include examination, confine, extent, exploration, sharing, storage, relocate, and visualization and privacy infringement. The Big Data have their application in various fields like in Tourism, Climate Research and many other fields.},
booktitle = {Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)},
articleno = {34},
numpages = {5},
keywords = {monitor, infringement, application, unstructured data, Big data, exploration, Data analytics},
location = {Unnao, India},
series = {ICARCSET '15}
}

@inproceedings{10.1145/3127942.3127961,
author = {Al-Qirim, Nabeel and Tarhini, Ali and Rouibah, Kamel},
title = {Determinants of Big Data Adoption and Success},
year = {2017},
isbn = {9781450352840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127942.3127961},
doi = {10.1145/3127942.3127961},
abstract = {This research investigates the large hype surrounding big data (BD) and Analytics (BDA) in both academia and the business world. Initial insights pointed to large and complex amalgamations of different fields, techniques and tools. Above all, BD as a research field and as a business tool found to be under developing and is fraught with many challenges. The intention here in this research is to develop an adoption model of BD that could detect key success predictors. The research finds a great interest and optimism about BD value that fueled this current buzz behind this novel phenomenon. Like any disruptive innovation, its assimilation in organizations oppressed with many challenges at various contextual levels. BD would provide different advantages to organizations that would seriously consider all its perspectives alongside its lifecycle in the pre-adoption or adoption or implementation phases. The research attempts to delineate the different facets of BD as a technology and as a management tool highlighting different contributions, implications and recommendations. This is of great interest to researchers, professional and policy makers.},
booktitle = {Proceedings of the International Conference on Algorithms, Computing and Systems},
pages = {88–92},
numpages = {5},
keywords = {big data challenges, big data success factors, big data strategy, Big data analytics},
location = {Jeju Island, Republic of Korea},
series = {ICACS '17}
}

@article{10.1145/3461015,
author = {Fugini, Mariagrazia and Finocchi, Jacopo},
title = {Data and Process Quality Evaluation in a Textual Big Data Archiving System},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3461015},
doi = {10.1145/3461015},
abstract = {The article presents a textual Big Data analytics solution developed in a real setting as a part of a high-capacity document digitization and storage system. A software based on machine learning techniques performs automated extraction and processing of textual contents. The work focuses on performance and data confidence evaluation and describes the approach to computing a set of indicators for textual data quality. It then presents experimental results.},
journal = {J. Comput. Cult. Herit.},
month = {mar},
articleno = {2},
numpages = {19},
keywords = {machine learning, Big Data analytics, unstructured Big Data, data quality, content management, text analytics}
}

@inproceedings{10.1145/3207677.3278000,
author = {Ke, Changwen and Wang, Kuisheng},
title = {Research and Application of Enterprise Big Data Governance},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3278000},
doi = {10.1145/3207677.3278000},
abstract = {With1 the further development of information technology, data has become one of the core resources of enterprises. In the current era of big data, data governance has gradually become an important means for enterprises to make intelligent decisions, helping enterprises to occupy a favorable position in a highly competitive market environment. This paper briefly describes the current situation of enterprise big data governance, and analyzes the problems of current data governance from the perspectives of enterprise management and data itself. To deal with these problems, this paper proposes several suggestions and strategies from the perspectives of organization and management system construction, construction of a data standard system, improve the level of data quality management, data technology surpport, etc. Fully combining the advanced theories and methods of domestic data governance, this paper designs a data governance framework from the perspective of data application and innovation. The framework includes supervision and control modules, data governance staff organization modules, application and service modules, and data processing and integration modules. And the framework is applied to the data governance of electric power enterprises, which has important reference significance and value for enterprises to carry out data governance.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {29},
numpages = {5},
keywords = {governance framework, Data governance, data quality},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/2513190.2513198,
author = {Ordonez, Carlos},
title = {Can We Analyze Big Data inside a DBMS?},
year = {2013},
isbn = {9781450324120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513190.2513198},
doi = {10.1145/2513190.2513198},
abstract = {Relational DBMSs remain the main data management technology, despite the big data analytics and no-SQL waves. On the other hand, for data analytics in a broad sense, there are plenty of non-DBMS tools including statistical languages, matrix packages, generic data mining programs and large-scale parallel systems, being the main technology for big data analytics. Such large-scale systems are mostly based on the Hadoop distributed file system and MapReduce. Thus it would seem a DBMS is not a good technology to analyze big data, going beyond SQL queries, acting just as a reliable and fast data repository. In this survey, we argue that is not the case, explaining important research that has enabled analytics on large databases inside a DBMS. However, we also argue DBMSs cannot compete with parallel systems like MapReduce to analyze web-scale text data. Therefore, each technology will keep influencing each other. We conclude with a proposal of long-term research issues, considering the "big data analytics" trend.},
booktitle = {Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP},
pages = {85–92},
numpages = {8},
keywords = {mapreduce, parallel algorithms, big data, dbms, sql},
location = {San Francisco, California, USA},
series = {DOLAP '13}
}

@inproceedings{10.1145/3368691.3368717,
author = {Mohammed, Tareq Abed and Ghareeb, Ahmed and Al-bayaty, Hussein and Aljawarneh, Shadi},
title = {Big Data Challenges and Achievements: Applications on Smart Cities and Energy Sector},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368717},
doi = {10.1145/3368691.3368717},
abstract = {In this paper, the Big Data challenges and the processing is analyzed, recently great attention has been paid to the challenges for great data, largely due to the wide spread of applications and systems used in real life, such as presentation, modeling, processing and large (often unlimited) data storage. Mass Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection. Consequently, we focus on further research trends and, as a default, we will explore a future research challenge research project in this area of research.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {26},
numpages = {5},
keywords = {OLAP, data processing, data mining, machin learning, big data},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/2691195.2691196,
author = {Ramasamy, Ramachandran},
title = {Towards Big Data Analytics Framework: ICT Professionals Salary Profile Compilation Perspective},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691196},
doi = {10.1145/2691195.2691196},
abstract = {This paper elucidates the opportunity on expanding the on-going preparation of salary profile of information communications technology (ICT) professionals of Malaysia into a big data analytics (BDA) activity. The current activity is based on structured database provided by the online job service providers. In essence, BDA framework entailing 5 Vs namely, volume, variety, velocity, veracity and value used in gauging the gaps and potential areas to consider in next stages.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {450–451},
numpages = {2},
keywords = {business intelligence, big data analytics, ICT salary profile},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@inproceedings{10.1145/2983323.2983345,
author = {Zhu, Fangzhou and Luo, Chen and Yuan, Mingxuan and Zhu, Yijian and Zhang, Zhengqing and Gu, Tao and Deng, Ke and Rao, Weixiong and Zeng, Jia},
title = {City-Scale Localization with Telco Big Data},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983345},
doi = {10.1145/2983323.2983345},
abstract = {It is still challenging in telecommunication (telco) industry to accurately locate mobile devices (MDs) at city-scale using the measurement report (MR) data, which measure parameters of radio signal strengths when MDs connect with base stations (BSs) in telco networks for making/receiving calls or mobile broadband (MBB) services. In this paper, we find that the widely-used location based services (LBSs) have accumulated lots of over-the-top (OTT) global positioning system (GPS) data in telco networks, which can be automatically used as training labels for learning accurate MR-based positioning systems. Benefiting from these telco big data, we deploy a context-aware coarse-to-fine regression (CCR) model in Spark/Hadoop-based telco big data platform for city-scale localization of MDs with two novel contributions. First, we design map-matching and interpolation algorithms to encode contextual information of road networks. Second, we build a two-layer regression model to capture coarse-to-fine contextual features in a short time window for improved localization performance. In our experiments, we collect 108 GPS-associated MR records in the centroid of Shanghai city with 12 x 11 square kilometers for 30 days, and measure four important properties of real-world MR data related to localization errors: stability, sensitivity, uncertainty and missing values. The proposed CCR works well under different properties of MR data and achieves a mean error of 110m and a median error of $80m$, outperforming the state-of-art range-based and fingerprinting localization methods.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {439–448},
numpages = {10},
keywords = {localization, regression models, telco big data},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/3312614.3312623,
author = {Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri Noorulhasan and Ahmad, Naim and Qamar, Shamimul},
title = {The 51 V's Of Big Data: Survey, Technologies, Characteristics, Opportunities, Issues and Challenges},
year = {2019},
isbn = {9781450366403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312614.3312623},
doi = {10.1145/3312614.3312623},
abstract = {Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.},
booktitle = {Proceedings of the International Conference on Omni-Layer Intelligent Systems},
pages = {19–24},
numpages = {6},
keywords = {data storage, Big Data, data characteristics, data generation},
location = {Crete, Greece},
series = {COINS '19}
}

@inproceedings{10.1145/3472163.3472195,
author = {Sahri, Soror and Moussa, Rim},
title = {Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472195},
doi = {10.1145/3472163.3472195},
abstract = {Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi’ trips.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {157–165},
numpages = {9},
keywords = {Big data, Veracity},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3524383.3524442,
author = {Cai, Mingjun and Sam, Francis and Asante Boadi, Evans},
title = {Research on the Application of Big Data in University's Public Opinion Monitoring and Processing},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3524442},
doi = {10.1145/3524383.3524442},
abstract = {Higher educational institutions rely on reputation to attract and earn the legitimacy of the public. However, the emerging trend of public misinformation due to the chunk of information available on the internet affects public opinion formation (POF) about universities. Therefore, narrowing down on the fallouts in POF can provide evidence for managerial and policy interventions. This paper explores five-layer POF and its application with big data in university public opinion monitoring and processing. It reveals that big data technology can be actively employed to safeguard the image of university public opinion formation, but this can be inhibited by the lack of commitment to integrating multiple stakeholders on a common platform, privacy, and security concerns. The paper recommends collaboration between universities and the government to increase momentum on big data with requisite actions for mutual benefits.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {121–126},
numpages = {6},
keywords = {public opinion system, data analysis, university public opinion, Big data},
location = {Shanghai, China},
series = {ICBDE '22}
}

@inproceedings{10.1145/3003733.3003767,
author = {Petrou, Charilaos and Paraskevas, Michael},
title = {Signal Processing Techniques Restructure The Big Data Era},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003767},
doi = {10.1145/3003733.3003767},
abstract = {Big data science has been developed into a topic that attracts attention from industry, academia and governments. The main objective in Big Data science is to recognize and extract meaningful information from huge amounts of heterogeneous data and unstructured data (which constitute 95% of big data). Signal Processing (SP) techniques and related statistical learning (SL) tools such as Principal Component Analysis (PCA), R-PCA (Robust PCA), Compressive Sampling (CS), convex optimization (CO), stochastic approximation (SA), kernel based learning (KBL) tasks are used for robustness, compression and dimensionality reduction in Big Data arising challenges. This review paper introduces Big Data related SP techniques and presents applications of this emerging field.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {52},
numpages = {6},
keywords = {signal processing techniques, stochastic approximation, convex optimization, statistical learning tools, big data},
location = {Patras, Greece},
series = {PCI '16}
}

@article{10.1145/2935753,
author = {Berti-Equille, Laure and Ba, Mouhamadou Lamine},
title = {Veracity of Big Data: Challenges of Cross-Modal Truth Discovery},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935753},
doi = {10.1145/2935753},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {12},
numpages = {3},
keywords = {information extraction, data quality, Truth discovery, fact checking, data fusion}
}

@inproceedings{10.1145/3168390.3168425,
author = {Samosir, Ridha Sefina and Hendric, Harco Leslie and Gaol, Ford Lumban and Abdurachman, Edi and Soewito, Benfano},
title = {Measurement Metric Proposed For Big Data Analytics System},
year = {2017},
isbn = {9781450353922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168390.3168425},
doi = {10.1145/3168390.3168425},
abstract = {Big data is defined as a very large data set (volume), velocity and variety. Big data analytics systems must be supports for parallel processing and large storage. The problem of this research is how to identify measurement metric based on big data analytics system characteristic. One device that support big data platform is Hadoop. Measurement is a process for assigning values or symbols to the attributes of an entity. The purpose of measurement is to distinguish between entities one to another. Indicator for software measurement represented with a metric. The aim of this research is to proposes some measurement metric for big data analytics system. This research using UML exactly a class diagram in system modelling to identify the measurement metric. Both of dynamic and static metric is proposed as solution to measure big data analytics system. Result for this researh are some measurement ndicator both of dynamic and static metric based on class diagram for big data analytics.},
booktitle = {Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence},
pages = {265–269},
numpages = {5},
keywords = {Measurement, Software, Big Data Analytics, Metric},
location = {Jakarta, Indonesia},
series = {CSAI 2017}
}

@article{10.1145/2331042.2331058,
author = {Heer, Jeffrey and Kandel, Sean},
title = {Interactive Analysis of Big Data},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331058},
doi = {10.1145/2331042.2331058},
abstract = {New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.},
journal = {XRDS},
month = {sep},
pages = {50–54},
numpages = {5}
}

@inproceedings{10.1145/3503928.3503929,
author = {Barzan Abdalla, Hemn and Mustafa, Nasser and Ihnaini, Baha},
title = {Big Data: Finding Frequencies of Faulty Multimedia Data},
year = {2021},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503929},
doi = {10.1145/3503928.3503929},
abstract = {In many health care domains, big data has arrived. How to manage and use big data better has become the focus of all walks of life. Many data sources provide the repeated fault data—the repeated fault data forming the delay of processing time and storage capacity. Big data includes properties like volume, velocity, variety, variability, value, complexity, and performance put forward more challenges. Most healthcare domains face the problem of testing for structured and unstructured data validation in big data. It provides low-quality data and delays in response. In testing process is delay and not provide the correct response. In Proposed, pre-testing and post-testing are used for big data testing. In pre-testing, classify fault data from different data sources. After Classification to group big data using SVM algorithms such as Text, Image, Audio, and Video file. In post-testing, to implement the pre-processing, remove the zero file size, unrelated file extension, and de-duplication after pre-processing to implement the Map-reduce algorithm to find out the big data efficiently. This process reduces the pre-processing time, reduces the server energy, and increases the processing time. To remove the fault data before pre-processing means to increase the processing time and data storage.},
booktitle = {2021 the 6th International Conference on Information Systems Engineering},
pages = {1–6},
numpages = {6},
keywords = {Pre-Processing, Classification using SVM, Map-reduce, Big Data, Fault data detection},
location = {Shanghai, China},
series = {ICISE 2021}
}

@article{10.1145/3419634,
author = {Bansal, Maggi and Chana, Inderveer and Clarke, Siobh\'{a}n},
title = {A Survey on IoT Big Data: Current Status, 13 V’s Challenges, and Future Directions},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3419634},
doi = {10.1145/3419634},
abstract = {Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {131},
numpages = {59},
keywords = {V’s challenges for IoT big data, IoT big data, cloud computing in IoT, cloud IoT services, big data 2.0, IoT big data survey}
}

@inproceedings{10.1145/3404687.3404693,
author = {Xin, Li and Tianyun, Shi and Xiaoning, Ma},
title = {Research on the Big Data Platform and Its Key Technologies for the Railway Locomotive System},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404693},
doi = {10.1145/3404687.3404693},
abstract = {In order to improve the efficiency of locomotive organization and the quality of locomotive operation, this paper analyzes and discusses the big data platform and some key technologies suitable for the big data application of the railway locomotive system. Firstly, the definition of big data of the railway locomotive system is proposed, and the current data characteristics of the railway locomotive system are summarized, then the status quo and demands of big data application of the railway locomotive system are analyzed. Secondly, the overall architecture of the big data platform for the railway locomotive system is proposed. Furthermore, seven application scenarios available for the big data platform are analyzed, including locomotive running organization, high-speed railway, repair, maintenance and other fields. Finally, some key technologies, which consist of data collection system of front-line operations, locomotive equipment portrait analysis, staff portrait analysis, transmission and analysis of locomotive video, intelligent auxiliary driving system, are provided to increase efficiency of the locomotive organization and capability of safety management. The obtained results can play a positive role in the construction and application of big data of the railway locomotive system.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {6–12},
numpages = {7},
keywords = {Key technology, Locomotive system, Railway, Application platform, Big data},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/3418688.3418697,
author = {Liou, Teau-Jiuan and Weng, Ming-Wei and Lee, Liza},
title = {The Effect of Big Data Platforms on Multi-Stage Production System in Industrie 4.0},
year = {2020},
isbn = {9781450387866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418688.3418697},
doi = {10.1145/3418688.3418697},
abstract = {The aim of this paper is to analyze how Industrie 4.0 triggers changes in the business models of manufacturing SMEs (small and medium-sized enterprises) by big data platforms in selected casting manufacturer in Taiwan. A generalized model is presented to determine the optimal production run time, production rate, the advertising effort and demand with observation features that minimize the total cost per unit time. Advances in science and technology such as IoT technology, big data platform to investigate information asymmetry between manufacturer and customers. Numerical examples and sensitivity analysis are then provided by the collecting real data from Taiwan. Finally, concluding remarks are offered.},
booktitle = {2020 the 3rd International Conference on Computing and Big Data},
pages = {48–54},
numpages = {7},
keywords = {Multi-stage assembly, Industrie 4.0, Big data, Digital transformation},
location = {Taichung, Taiwan},
series = {ICCBD '20}
}

@inproceedings{10.1145/2463676.2463707,
author = {Sumbaly, Roshan and Kreps, Jay and Shah, Sam},
title = {The Big Data Ecosystem at LinkedIn},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463707},
doi = {10.1145/2463676.2463707},
abstract = {The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1125–1134},
numpages = {10},
keywords = {machine learning, data pipeline, offline processing, hadoop, big data, data mining},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3472163.3472171,
author = {Bhardwaj, Dave and Ormandjieva, Olga},
title = {Rigorous Measurement Model for Validity of Big Data: MEGA Approach},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472171},
doi = {10.1145/3472163.3472171},
abstract = {Big Data is becoming a substantial part of the decision-making processes in both industry and academia, especially in areas where Big Data may have a profound impact on businesses and society. However, as more data is being processed, data quality is becoming a genuine issue that negatively affects credibility of the systems we build because of the lack of visibility and transparency of the underlying data. Therefore, Big Data quality measurement is becoming increasingly necessary in assessing whether data can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses Big Data quality measurement modelling and automation by proposing a novel quality measurement framework for Big Data (MEGA) that objectively assesses the underlying quality characteristics of Big Data (also known as the V's of Big Data) at each step of the Big Data Pipelines. Five of the Big Data V's (Volume, Variety, Velocity, Veracity and Validity) are currently automated by the MEGA framework. In this paper, a new theoretically valid quality measurement model is proposed for an essential quality characteristic of Big Data, called Validity. The proposed measurement information model for Validity of Big Data is a hierarchy of 4 derived measures / indicators and 5 based measures. Validity measurement is illustrated on a running example.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {285–291},
numpages = {7},
keywords = {Big Data, Representational Theory of Measurement,, Quality Characteristics (V's), Validity, Measurement Hierarchical Model},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@article{10.1145/3186549.3186559,
author = {Sadiq, Shazia and Dasu, Tamraparni and Dong, Xin Luna and Freire, Juliana and Ilyas, Ihab F. and Link, Sebastian and Miller, Miller J. and Naumann, Felix and Zhou, Xiaofang and Srivastava, Divesh},
title = {Data Quality: The Role of Empiricism},
year = {2018},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3186549.3186559},
doi = {10.1145/3186549.3186559},
abstract = {We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {35–43},
numpages = {9}
}

@inproceedings{10.1145/3297730.3297743,
author = {Liu, Yi and Peng, Jiawen and Yu, Zhihao},
title = {Big Data Platform Architecture under The Background of Financial Technology: In The Insurance Industry As An Example},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297743},
doi = {10.1145/3297730.3297743},
abstract = {With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {31–35},
numpages = {5},
keywords = {insurance industry, big data platform, platform architecture, Financial technology, time and space data},
location = {Chengdu, China},
series = {BDET 2018}
}

@inproceedings{10.1145/2623330.2623615,
author = {Anagnostopoulos, Christos and Triantafillou, Peter},
title = {Scaling out Big Data Missing Value Imputations: Pythia vs. Godzilla},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623615},
doi = {10.1145/2623330.2623615},
abstract = {Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {651–660},
numpages = {10},
keywords = {clustering, missing value, big data},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1145/3178315.3178323,
author = {Arruda, Darlan},
title = {Requirements Engineering in the Context of Big Data Applications},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3178315.3178323},
doi = {10.1145/3178315.3178323},
abstract = {Requirements Engineering (RE) plays an essential role in the software engineering process, being considered as one of the most critical phases of the software development life-cycle. As we might expect, then, the Requirements Engineering would play a similar role in the context of Big Data applications. However, practicing Requirements Engineering is a challenging and complex task. It involves (i) stakeholders with diverse backgrounds and levels of knowledge, (ii) different application domains, (iii) it is expensive and error-prone, (iii) it is important to be aligned with business goals, to name a few. Because it involves such complex activities, a lot has to be understood in order to properly address Requirements Engineering. Especially, when the technology domain (e.g., Big Data) is not yet well explored. In this context, this paper describes a research plan on Requirements Engineering involving the development of Big Data applications. The high-level goal is to investigate: (i) On the technical front, the Requirements Engineering activities with respect to the analysis and specification of Big Data requirements and, (ii) on the management side, the relationship between RE and Business Goals in the development of Big Data Software applications.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–6},
numpages = {6},
keywords = {big data applications, empirical studies, empirical software engineering., business goals, big data requirements engineering}
}

@inproceedings{10.1145/3545801.3545802,
author = {Liu, Xiaobao and Li, Qi and Zhu, Shaosong and Wang, Cong and Meng, Lingzhen},
title = {A Multi-Head Attention Mechanism Base Multi-Dimensional Data Quality Evaluation Model},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545801.3545802},
doi = {10.1145/3545801.3545802},
abstract = {High-quality power data is the basis for reliable operation of power systems, efficient data processing, and effective mining of the potential value of power data. How to use big data, artificial intelligence and other technologies to evaluate the quality of power data is a hot research topic in the field of electric power. At present, most of the power data quality evaluation methods are simple and lack the research of general data quality evaluation model. Therefore, this paper proposes a multi-dimensional data quality evaluation model based on a multi-head attention mechanism. The model measures multiple indicators such as completeness, accuracy, smoothness, and correlation. The corresponding methods are used to quantify these indicators to form a data quality evaluation index system oriented to multi-dimensional indicators; then, an application feedback mechanism based on a multi-head attention network is used to correct the calculation weights and score outputs, so as to achieve the evaluation of power data quality. Finally, the validation analysis is carried out based on the electricity data of a region in China. The experimental results show that the proposed method can effectively evaluate the quality of electric power data.},
booktitle = {Proceedings of the 7th International Conference on Big Data and Computing},
pages = {1–5},
numpages = {5},
keywords = {Electric power data, multi-head attention, data quality evaluation, multi-dimensional indicators},
location = {Shenzhen, China},
series = {ICBDC '22}
}

@inproceedings{10.1145/3297730.3297731,
author = {Chen, Rui-Yang},
title = {Target Data Optimization Based on Big Data-Streaming for Two-Stage Fuzzy Extraction System},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297731},
doi = {10.1145/3297730.3297731},
abstract = {How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {26–30},
numpages = {5},
keywords = {big data-streaming, fuzzy case-based reasoning, Target data optimization, fuzzy decision tree},
location = {Chengdu, China},
series = {BDET 2018}
}

@inproceedings{10.1145/2513190.2517828,
author = {Cuzzocrea, Alfredo and Bellatreche, Ladjel and Song, Il-Yeol},
title = {Data Warehousing and OLAP over Big Data: Current Challenges and Future Research Directions},
year = {2013},
isbn = {9781450324120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513190.2517828},
doi = {10.1145/2513190.2517828},
abstract = {In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.},
booktitle = {Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP},
pages = {67–70},
numpages = {4},
keywords = {olap, big multidimensional data, data warehousing, big data},
location = {San Francisco, California, USA},
series = {DOLAP '13}
}

@inproceedings{10.1145/3372454.3372478,
author = {Puangpontip, Supadchaya and Hewett, Rattikorn},
title = {Assessing Reliability of Big Data Stream for Smart City},
year = {2019},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372478},
doi = {10.1145/3372454.3372478},
abstract = {Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {18–23},
numpages = {6},
keywords = {Theory of evidence, Data Reliability, Smart City, IoT},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@inproceedings{10.1145/3396956.3398253,
author = {Potiguara Carvalho, Artur and Potiguara Carvalho, Fernanda and Dias Canedo, Edna and Potiguara Carvalho, Pedro Henrique},
title = {Big Data, Anonymisation and Governance to Personal Data Protection},
year = {2020},
isbn = {9781450387910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396956.3398253},
doi = {10.1145/3396956.3398253},
abstract = {In a massive processing data era, an emerging impasse has taking scenario: privacy. In this context, personal data receive particular attention, witch its laws and guidelines that ensure better and legal use of data. The General Data Protection Regulation (GDPR) - in the European Union - and the Brazilian General Data Protection Law (LGPD) - in Brazil - lead to anonymisation (and its processes and techniques) as a way to reach secure use of personal data. However, expectations placed on this tool must be reconsidered according to risks and limits of its use, mainly when this technique is applied to Big Data. We discussed whether anonymisation used in conjunction with good data governance practices could provide greater protection for privacy. We conclude that good governance practices can strengthen privacy in anonymous data belonging to a Big Data, and we present a suggestive governance framework aimed at privacy.},
booktitle = {The 21st Annual International Conference on Digital Government Research},
pages = {185–195},
numpages = {11},
keywords = {Privacy, Anonymisation, Governance, Personal Data Protection, Big Data},
location = {Seoul, Republic of Korea},
series = {dg.o '20}
}

@inproceedings{10.1145/3415958.3433082,
author = {Dessalk, Yared Dejene and Nikolov, Nikolay and Matskin, Mihhail and Soylu, Ahmet and Roman, Dumitru},
title = {Scalable Execution of Big Data Workflows Using Software Containers},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433082},
doi = {10.1145/3415958.3433082},
abstract = {Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {76–83},
numpages = {8},
keywords = {Big Data workflows, Domain-specific languages, Software containers},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/3085228.3085275,
author = {Gong, Yiwei and Janssen, Marijn},
title = {Enterprise Architectures for Supporting the Adoption of Big Data},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085275},
doi = {10.1145/3085228.3085275},
abstract = {Governments from all over the world are struggling to take advantage of big data developments. Enterprise Architecture (EA) can be used as an instrument to integrate big data (BD) in the existing business processes and ICT-landscape. In this policy paper, we explore the role of EA in the adoption of BD. For this, we adopted a qualitative case study approach and investigated a large administrative organization that was in the process of adopting BD. We found in our case study that the first attempts were focused on integrating big data in the current landscape, but this encountered too many challenges that halt progress. To overcome the challenges, a separate BD department and accompanying infrastructure was created. The strategy was first to reap the benefits of BD and to understand what should be done, and thereafter integrating the working systems in the existing landscape. The findings suggest that current infrastructures might not be suitable for integrating BD and substantial changes are needed first. In the case the role of BD needed to be first clarified before EA could play a role in adopting BD. EA should deal with the uncertainties and complexities by ensuring a configurable landscape, by providing an incremental approach for adapting the infrastructure step-by-step, before the benefits of big data can be gained. Developing an incremental migration plan was found to be a key aspect for the adoption of BD.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {505–510},
numpages = {6},
keywords = {open data, enterprise architecture, ICT-architecture, big data, BOLD, infrastructure, e-government},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@inproceedings{10.1145/3453187.3453340,
author = {Hu, Zhifeng and Zhao, Feng and Zhao, Xiaona},
title = {Research on Smart Education Service Platform Based on Big Data},
year = {2020},
isbn = {9781450389099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453187.3453340},
doi = {10.1145/3453187.3453340},
abstract = {The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.},
booktitle = {Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science},
pages = {228–233},
numpages = {6},
keywords = {Big data, Smart education, Information-oriented education},
location = {Wuhan, China},
series = {EBIMCS 2020}
}

@inproceedings{10.5555/2840819.2840927,
author = {Zhu, Yada and Xiong, Jinjun},
title = {Modern Big Data Analytics for "Old-Fashioned" Semiconductor Industry Applications},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an "old-fashioned" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {776–780},
numpages = {5},
keywords = {Big data, semiconductor, analytics, manufacturing},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1145/2676536.2676538,
author = {Chen, Xin and Vo, Hoang and Aji, Ablimit and Wang, Fusheng},
title = {High Performance Integrated Spatial Big Data Analytics},
year = {2014},
isbn = {9781450331326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676536.2676538},
doi = {10.1145/2676536.2676538},
abstract = {The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {11–14},
numpages = {4},
keywords = {MapReduce, spatial analytics, GIS, database, data warehouse},
location = {Dallas, Texas},
series = {BigSpatial '14}
}

@inproceedings{10.1145/3433996.3434027,
author = {Ding, Shifu and Liu, Yan and Zhang, Jianjun and Tan, Yaqi and Li, Xiaoxia and Tang, RuiChun},
title = {The Planning and Construction of Healthcare Big Data Platform},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434027},
doi = {10.1145/3433996.3434027},
abstract = {Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates "difficulty and expensive" problem effectively.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {170–176},
numpages = {7},
keywords = {EMR, Healthcare, Electronic Health Record, Big Data, SOA},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/3424978.3425010,
author = {Man, Rui and Zhou, Guomin and Fan, Jingchao},
title = {Research on Scientific Data Management in Big Data Era},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425010},
doi = {10.1145/3424978.3425010},
abstract = {Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {32},
numpages = {6},
keywords = {Scientific data, Big data, Scientific data management, Opening and sharing of data resource},
location = {Sanya, China},
series = {CSAE 2020}
}

@article{10.1145/2998575,
author = {Labouseur, Alan G. and Matheus, Carolyn C.},
title = {An Introduction to Dynamic Data Quality Challenges},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2998575},
doi = {10.1145/2998575},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {6},
numpages = {3},
keywords = {big data, graph systems, Dynamic data quality, internet of things, relational systems}
}

@inproceedings{10.1145/3508259.3508284,
author = {Sun, Yu and Niu, Yanfang and Lu, Le},
title = {Research on Influencing Factors of Government Audit Big Data Capability},
year = {2021},
isbn = {9781450384162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508259.3508284},
doi = {10.1145/3508259.3508284},
abstract = {The widespread application of big data has had a profound impact on social and economic development. Government auditing is the guarantee for the modernization of national governance, and the development of big data audit capability has become the key to improving national governance capability. This paper summarizes the concept of government audit big data capability, and constructs the influencing factor model of government audit big data capability. This study finds that the construction degree of audit big data platform, big data management ability, big data audit technology and auditors' big data technology ability have a significant positive impact on the government audit big data ability, and the audit organization coordination ability plays a positive moderating effect in the whole impact process. This study provides guidance for the improvement and development of government audit big data capability.},
booktitle = {2021 4th Artificial Intelligence and Cloud Computing Conference},
pages = {172–178},
numpages = {7},
keywords = {Big data analysis capability, Government audit, Influencing factors},
location = {Kyoto, Japan},
series = {AICCC '21}
}

@article{10.1145/2481244.2481247,
author = {Lin, Jimmy and Ryaboy, Dmitriy},
title = {Scaling Big Data Mining Infrastructure: The Twitter Experience},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/2481244.2481247},
doi = {10.1145/2481244.2481247},
abstract = {The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on "big data". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life "in the trenches" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall "big picture" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as "plumbing". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.},
journal = {SIGKDD Explor. Newsl.},
month = {apr},
pages = {6–19},
numpages = {14}
}

@article{10.1145/3383464,
author = {Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv},
title = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3383464},
doi = {10.1145/3383464},
abstract = {Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {46},
numpages = {40},
keywords = {big data analytics application, service level agreement, SLA, SLA metrics, Big data, service layer}
}

@article{10.1145/2932707,
author = {Fang, Ruogu and Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Iyengar, S. S.},
title = {Computational Health Informatics in the Big Data Age: A Survey},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2932707},
doi = {10.1145/2932707},
abstract = {The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {12},
numpages = {36},
keywords = {survey, clinical decision support, computational health informatics, data mining, 4V challenges, machine learning, Big data analytics}
}

@inproceedings{10.1145/3402569.3409041,
author = {Han, Ping},
title = {Research on Foreign Exchange Management Model Based on Big Data},
year = {2020},
isbn = {9781450377546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402569.3409041},
doi = {10.1145/3402569.3409041},
abstract = {The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.},
booktitle = {Proceedings of the 5th International Conference on Distance Education and Learning},
pages = {162–165},
numpages = {4},
keywords = {foreign exchange management, Big data background, mode},
location = {Beijing, China},
series = {ICDEL 2020}
}

@inproceedings{10.1145/2791347.2791380,
author = {Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken},
title = {Privacy-Preserving Big Data Publishing},
year = {2015},
isbn = {9781450337090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791347.2791380},
doi = {10.1145/2791347.2791380},
abstract = {The problem of privacy-preserving data mining has been studied extensively in recent years because of its importance as a key enabler in the sharing of massive data sets. Most of the work in privacy has focussed on issues involving the quality of privacy preservation and utility, though there has been little focus on the issue of scalability in privacy preservation. The reason for this is that anonymization has generally been seen as a batch and one-time process in the context of data sharing. However, in recent years, the sizes of data sets have grown tremendously to a point where the effective application of the current algorithms is becoming increasingly difficult. Furthermore, the transient nature of recent data sets has resulted in an increased need for the repeated application of such methods on the newer data sets which have been collected. Repeated application demands even greater computational efficiency in order to be practical. For example, an algorithm with quadratic complexity is unlikely to be implementable in reasonable time over terabyte scale data sets. A bigger issue is that larger data sets are likely to be addressed by distributed frameworks such as MapReduce. In such frameworks, one has to address the additional issue of minimizing data transfer across different nodes, which is the bottleneck. In this paper, we discuss the first approach towards privacy-preserving data mining of very massive data sets using MapReduce. We study two most widely-used privacy models k-anonymity and l-diversity for anonymization, and present experimental results illustrating the effectiveness of the approach.},
booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
articleno = {26},
numpages = {11},
location = {La Jolla, California},
series = {SSDBM '15}
}

@inproceedings{10.1145/2815782.2815793,
author = {Malaka, Iman and Brown, Irwin},
title = {Challenges to the Organisational Adoption of Big Data Analytics: A Case Study in the South African Telecommunications Industry},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815793},
doi = {10.1145/2815782.2815793},
abstract = {The purpose of this interpretive study was to explore the challenges to the adoption of Big Data Analytics (BDA) in organisations. The Technology-Organisation-Environment (TOE) model was used to guide the study. Data was collected from a large telecommunication organization in South Africa. Seven participants, from both Information Technology (IT) and business were interviewed to gain a holistic overview of challenges towards the adoption of BDA. An inductive approach was used for analysis. Findings revealed technological challenges to the adoption of BDA as being Data Integration; Data Privacy; Return on Investment; Data Quality; Cost; Data Integrity; and Performance and Scalability. From the organizational perspective, the major challenges were Ownership and Control; Skills Shortages; Business Focus and Prioritisation; Training and Exposure; Silos; and Unclear Processes. From the environmental context there were no major challenges highlighted. Organisational challenges were deemed to be the major inhibitors to adoption of BDA},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {27},
numpages = {9},
keywords = {South Africa, Big Data, Technology Adoption, Big Data Analytics},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3374587.3374650,
author = {Shen, Shaoyi and Li, Bin and Li, Situo},
title = {Construction and Application of Big Data Analysis Platform for Enterprise},
year = {2019},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374650},
doi = {10.1145/3374587.3374650},
abstract = {A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {54–58},
numpages = {5},
keywords = {Distributed, Analysis of big data, Shared, Data asset, Construction},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.1145/3501409.3501593,
author = {Diao, Yanhua},
title = {Tourism Prediction Based on Multi-Source Big Data Fusion Technology},
year = {2021},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501593},
doi = {10.1145/3501409.3501593},
abstract = {In the practical application of existing big data tourism prediction, there are some practical problems, such as complicated data sources and difficult fusion, low prediction accuracy and poor guiding practice effect. In view of this situation, this paper intends to build a tourism big data index prediction model suitable for the characteristics of tourism development through core data extraction, multi-source data fusion, complex data modeling and other key technologies. With the help of the improved tourism prediction model based on multi-source big data fusion technology, the tourist flow and consumption characteristics of Shandong province are more accurately identified and predicted. It can provide help for optimizing public service of tourism, strengthening early warning of tourist flow and improving marketing strategy of tourist destination. This study innovatively supplements the effective integration theory of multi-source tourism big data and the organic integration theory of big data and traditional sampling survey data. At the same time, the relevant methods of tourism big data forecasting model are extended.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1030–1036},
numpages = {7},
keywords = {Tourism prediction, Multi-source big data, Data fusion technology},
location = {Xiamen, China},
series = {EITCE 2021}
}

@inproceedings{10.1145/3193063.3193069,
author = {Cheng, Susu and Zhao, Haijun},
title = {An Overview of Techniques for Confirming Big Data Property Rights},
year = {2018},
isbn = {9781450363785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193063.3193069},
doi = {10.1145/3193063.3193069},
abstract = {The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.},
booktitle = {Proceedings of the 2018 International Conference on Intelligent Information Technology},
pages = {59–64},
numpages = {6},
keywords = {Information property index, Big Data, Confirmation of Information Property, Method for confirming information property rights},
location = {Ha Noi, Viet Nam},
series = {ICIIT 2018}
}

@inproceedings{10.1145/3357292.3357302,
author = {Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang},
title = {Big Data Informatization Applied to Optimization of Human Resource Performance Management},
year = {2019},
isbn = {9781450371445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357292.3357302},
doi = {10.1145/3357292.3357302},
abstract = {With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.},
booktitle = {Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences},
pages = {12–17},
numpages = {6},
keywords = {Big data, human resources, performance management},
location = {Chengdu, China},
series = {IMMS 2019}
}

@inproceedings{10.1145/3167918.3167924,
author = {Al-Mansoori, Ahmed and Yu, Shui and Xiang, Yong and Sood, Keshav},
title = {A Survey on Big Data Stream Processing in SDN Supported Cloud Environment},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167924},
doi = {10.1145/3167918.3167924},
abstract = {Big data is the term which denotes data with features such as voluminous data, a variety of data and streaming data as well. Processing big data became essential for enterprises to garner general intelligence and avoid biased conclusions. Due to these features, big data processing is considered to be a challenging task. Big data Processing should rely on a robust network. Cloud computing offers a suitable environment for these processes. However, it is more challenging when we move big data to the cloud, as managing the cloud resources is the main issue. Software Defined Network (SDN) has a potential solution to this issue. In this paper, first, we survey the present state of the art of SDN, cloud computing, and Big data Stream processing (BDSP). Then, we discuss SDN in the context of Big Data Stream Processing in Cloud environment. Finally, critical issues and research opportunity are discussed.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {12},
numpages = {11},
keywords = {cost optimization, big data, cloud computing, SDN, big data stream processing, resource optimization},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@inproceedings{10.1145/3297662.3365797,
author = {Musto, Jiri and Dahanayake, Ajantha},
title = {Integrating Data Quality Requirements to Citizen Science Application Design},
year = {2019},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365797},
doi = {10.1145/3297662.3365797},
abstract = {Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {166–173},
numpages = {8},
keywords = {Conceptual model, Data Quality Characteristics, Citizen science, Data quality, Data Quality requirements},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/3220228.3220236,
author = {Saraee, Mo and Silva, Charith},
title = {A New Data Science Framework for Analysing and Mining Geospatial Big Data},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220236},
doi = {10.1145/3220228.3220236},
abstract = {Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {98–102},
numpages = {5},
keywords = {data mining, data science, machine learning, geospatial big data, big data},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3524383.3524431,
author = {Wu, Min and Hao, Xinxin and Wan, Xuehong and Ma, Chenwei and Wu, Yu},
title = {Opportunities and Challenges of Joint Training of Postgraduate Students by the University-Industry Collaboration Institutions in Big Data Era},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3524431},
doi = {10.1145/3524383.3524431},
abstract = {University-industry cooperative education is an important way to cultivate graduate students' innovative ability and practical ability. However, there are some problems in the traditional joint training model of graduate students, such as low efficiency, conflict of objectives of cooperative subjects, a mismatch between supply and demand of cooperative entities, and so on. The big data technology has brought new opportunities and challenges to the joint training of graduate students by university-industry cooperation institutions. Based on analyzing the connotation and characteristics of the big data era, the paper points out that the arrival of the big data era can improve the information integration efficiency of university-industry cooperation institutions, optimize the traditional joint training model of graduate students, and provide an effective evaluation mechanism of educational quality for university-industry cooperation institutions. At the same time, the paper discusses the difficulties of data collection and disclosure of data privacy faced by university-industry cooperative education in the big data era. The paper also discusses how to deal with the challenges from the perspective of the government, colleges and universities, scientific research institutions and enterprises.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {194–198},
numpages = {5},
keywords = {big data, cooperative education of university-industry institutions, postgraduate education},
location = {Shanghai, China},
series = {ICBDE '22}
}

@inproceedings{10.1145/3194206.3194229,
author = {Zhichao, Xu and Jiandong, Zhao and Huan, Huang},
title = {Based on Hadoop's Tech Big Data Combination and Mining Technology Framework},
year = {2018},
isbn = {9781450363457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194206.3194229},
doi = {10.1145/3194206.3194229},
abstract = {With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.},
booktitle = {Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence},
pages = {59–63},
numpages = {5},
keywords = {Hadoop, tech big data, mining, combination},
location = {Shanghai, China},
series = {ICIAI '18}
}

@article{10.1145/2094114.2094129,
author = {Bizer, Christian and Boncz, Peter and Brodie, Michael L. and Erling, Orri},
title = {The Meaningful Use of Big Data: Four Perspectives -- Four Challenges},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2094114.2094129},
doi = {10.1145/2094114.2094129},
abstract = {Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.},
journal = {SIGMOD Rec.},
month = {jan},
pages = {56–60},
numpages = {5}
}

@inproceedings{10.1145/3321454.3321474,
author = {Yu, Bangbo and Zhao, Haijun},
title = {Research on the Construction of Big Data Trading Platform in China},
year = {2019},
isbn = {9781450366335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321454.3321474},
doi = {10.1145/3321454.3321474},
abstract = {As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.},
booktitle = {Proceedings of the 2019 4th International Conference on Intelligent Information Technology},
pages = {107–112},
numpages = {6},
keywords = {big data trading platform, Data assets, regulatory construction},
location = {Da, Nang, Viet Nam},
series = {ICIIT '19}
}

@inproceedings{10.1145/3234698.3234723,
author = {El Bousty, Hicham and krit, Salah-ddine and Elasikri, Mohamed and Dani, Hassan and Karimi, Khaoula and Bendaoud, Kaoutar and Kabrane, Mustapha},
title = {Investigating Business Intelligence in the Era of Big Data: Concepts, Benefits and Challenges},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234723},
doi = {10.1145/3234698.3234723},
abstract = {Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {25},
numpages = {9},
keywords = {Big Data, Cloud Computing, Business Intelligence, Data Warehouse},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

