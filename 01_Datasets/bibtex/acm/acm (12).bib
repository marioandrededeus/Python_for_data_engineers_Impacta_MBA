@inproceedings{10.1145/3323679.3326513,
author = {Xue, Hongfei and Jiang, Wenjun and Miao, Chenglin and Yuan, Ye and Ma, Fenglong and Ma, Xin and Wang, Yijiang and Yao, Shuochao and Xu, Wenyao and Zhang, Aidong and Su, Lu},
title = {DeepFusion: A Deep Learning Framework for the Fusion of Heterogeneous Sensory Data},
year = {2019},
isbn = {9781450367646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323679.3326513},
doi = {10.1145/3323679.3326513},
abstract = {In recent years, significant research efforts have been spent towards building intelligent and user-friendly IoT systems to enable a new generation of applications capable of performing complex sensing and recognition tasks. In many of such applications, there are usually multiple different sensors monitoring the same object. Each of these sensors can be regarded as an information source and provides us a unique "view" of the observed object. Intuitively, if we can combine the complementary information carried by multiple sensors, we will be able to improve the sensing performance. Towards this end, we propose DeepFusion, a unified multi-sensor deep learning framework, to learn informative representations of heterogeneous sensory data. DeepFusion can combine different sensors' information weighted by the quality of their data and incorporate cross-sensor correlations, and thus can benefit a wide spectrum of IoT applications. To evaluate the proposed DeepFusion model, we set up two real-world human activity recognition testbeds using commercialized wearable and wireless sensing devices. Experiment results show that DeepFusion can outperform the state-of-the-art human activity recognition methods.},
booktitle = {Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {151–160},
numpages = {10},
keywords = {Sensor Fusion, Deep Learning, Internet of Things},
location = {Catania, Italy},
series = {Mobihoc '19}
}

@inproceedings{10.5555/3242181.3242194,
author = {Cheng, Russell},
title = {History of Input Modeling},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {In stochastic simulation, input modeling refers to the process of identifying and selecting the probability distributions, called input models, from which are generated the random variates that are the source of the stochastic variation in the simulation model when it is run. This article reviews the history of the development and use of such models with the main focus on discrete-event simulation (DES).},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {12},
numpages = {21},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3025453.3025777,
author = {Du, Fan and Plaisant, Catherine and Spring, Neil and Shneiderman, Ben},
title = {Finding Similar People to Guide Life Choices: Challenge, Design, and Evaluation},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025777},
doi = {10.1145/3025453.3025777},
abstract = {People often seek examples of similar individuals to guide their own life choices. For example, students making academic plans refer to friends; patients refer to acquaintances with similar conditions, physicians mention past cases seen in their practice. How would they want to search for similar people in databases? We discuss the challenge of finding similar people to guide life choices and report on a need analysis based on 13 interviews. Our PeerFinder prototype enables users to find records that are similar to a seed record, using both record attributes and temporal events found in the records. A user study with 18 participants and four experts shows that users are more engaged and more confident about the value of the results to provide useful evidence to guide life choices when provided with more control over the search process and more context for the results, even at the cost of added complexity.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {5498–5544},
numpages = {47},
keywords = {similarity, temporal visualization, temporal event analytics, decision making, visual analytics},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.5555/3242181.3242205,
author = {Brailsford, Sally C and Carter, Michael W and Jacobson, Sheldon H},
title = {Five Decades of Healthcare Simulation},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {In this paper we have not attempted to produce any kind of systematic review of simulation in healthcare to compete with the dozen (at least) excellent and comprehensive survey papers on this topic that already exist. We begin with a glance back at the early days of Wintersim, but then proceed, in line with the theme of this special track, to reflect on general developments in healthcare simulation over the years from our own personal perspectives. We include some memories and reflections by several pioneers in this area, both academics and healthcare practitioners, on both sides of the Atlantic. We also asked four current simulation modelers, who all specialize in healthcare applications but from very diverse perspectives, to reflect on their experiences. We endeavor to identify some common or recurring themes across the years, and end with a glimpse into the future.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {23},
numpages = {20},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@article{10.1145/3449252,
author = {Van Kleunen, Lucy and Muller, Brian and Voida, Stephen},
title = {"Wiring a City": A Sociotechnical Perspective on Deploying Urban Sensor Networks},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449252},
doi = {10.1145/3449252},
abstract = {We use a sociotechnical perspective to expand upon prior characterizations of deploying end-to-end urban sensor networks that focus primarily on the technical aspects of such systems. Via exploratory, semi-structured interviews with those deploying a number of urban sensor networks in a single American city, we identify ways that human decision-making and collaborative processes influence how these infrastructures are built. We synthesize these findings into a framework in which sociotechnical factors show up across the phases of data collection, management, analysis, and impacts within smart city projects. Each phase can display variability in immediacy, automation, geographic scope, and ownership. Finally, we use our situated work to discuss a generalizable tension within smart city projects between cross-domain data integration and fragmentation and provide implications for CSCW research, the design of smart city data platforms, and municipal policy.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {178},
numpages = {22},
keywords = {sociotechnical system, civic data, smart cities, urban sensor networks}
}

@article{10.1109/TCBB.2015.2453944,
author = {Masseroli, Marco and Canakoglu, Arif and Ceri, Stefano},
title = {Integration and Querying of Genomic and Proteomic Semantic Annotations for Biomedical Knowledge Extraction},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2453944},
doi = {10.1109/TCBB.2015.2453944},
abstract = {Understanding complex biological phenomena involves answering complex biomedical questions on multiple biomolecular information simultaneously, which are expressed through multiple genomic and proteomic semantic annotations scattered in many distributed and heterogeneous data sources; such heterogeneity and dispersion hamper the biologists’ ability of asking global queries and performing global evaluations. To overcome this problem, we developed a software architecture to create and maintain a Genomic and Proteomic Knowledge Base (GPKB), which integrates several of the most relevant sources of such dispersed information (including Entrez Gene, UniProt, IntAct, Expasy Enzyme, GO, GOA, BioCyc, KEGG, Reactome, and OMIM). Our solution is general, as it uses a flexible, modular, and multilevel global data schema based on abstraction and generalization of integrated data features, and a set of automatic procedures for easing data integration and maintenance, also when the integrated data sources evolve in data content, structure, and number. These procedures also assure consistency, quality, and provenance tracking of all integrated data, and perform the semantic closure of the hierarchical relationships of the integrated biomedical ontologies. At http://www.bioinformatics.deib.polimi.it/GPKB/, a Web interface allows graphical easy composition of queries, although complex, on the knowledge base, supporting also semantic query expansion and comprehensive explorative search of the integrated data to better sustain biomedical knowledge extraction.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {209–219},
numpages = {11}
}

@inproceedings{10.1145/3535782.3535831,
author = {Thuensuwan, Komkrish and Chutima, Parames},
title = {Expert System Development to Predict Canned Motor Pump Status},
year = {2022},
isbn = {9781450395816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535782.3535831},
doi = {10.1145/3535782.3535831},
abstract = {This research presents the development of an Expert System to predict Canned Motor Pump (CMP) Status by applying a machine learning (ML) algorithm with domain expert knowledge in the case study plant. A Case study plant is a petrochemical plant that uses CMP to transfer process medium within inside plant battery limit (ISBL). At present, The CMP maintenance strategy is improving from condition-based maintenance to predictive maintenance. To archive desired level of predictive maintenance need CMP domain expert knowledge to find potential failure signs. This expert system is contributing to reducing expertise human load by substitution with the system. The research contains identifying system framework, experiment steps, including dataset preparation and model testing. The experiment result shows Random Forest (RF) algorithm is suitable for this system due to model performance evaluation comparing four algorithms with confusion matrix and similar data resampling and hyperparameter tuning method. Further on, this contribution is a role model, and enrolling in other equipment in the case study plant is a benefit of this work. Recommendation and key success factors found during this research are also mentioned in the conclusion for further work as a continuous improvement process cycle.},
booktitle = {Proceedings of the 4th International Conference on Management Science and Industrial Engineering},
pages = {373–382},
numpages = {10},
location = {Chiang Mai, Thailand},
series = {MSIE '22}
}

@inproceedings{10.1145/3388440.3412475,
author = {Chen, Junjie and Mowlaei, Mohammad Erfan and Shi, Xinghua},
title = {Population-Scale Genomic Data Augmentation Based on Conditional Generative Adversarial Networks},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412475},
doi = {10.1145/3388440.3412475},
abstract = {Although next generation sequencing technologies have made it possible to quickly generate a large collection of sequences, current genomic data still suffer from small data sizes, imbalances, and biases due to various factors including disease rareness, test affordability, and concerns about privacy and security. In order to address these limitations of genomic data, we develop a Population-scale Genomic Data Augmentation based on Conditional Generative Adversarial Networks (PG-cGAN) to enhance the amount and diversity of genomic data by transforming samples already in the data rather than collecting new samples. Both the generator and discriminator in the PG-CGAN are stacked with convolutional layers to capture the underlying population structure. Our results for augmenting genotypes in human leukocyte antigen (HLA) regions showed that PC-cGAN can generate new genotypes with similar population structure, variant frequency distributions and LD patterns. Since the input for PC-cGAN is the original genomic data without assumptions about prior knowledge, it can be extended to enrich many other types of biomedical data and beyond.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {26},
numpages = {6},
keywords = {data augmentation, deep learning, generative adversarial networks, genomics, machine learning},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3366623.3368140,
author = {Skluzacek, Tyler J. and Chard, Ryan and Wong, Ryan and Li, Zhuozhao and Babuji, Yadu N. and Ward, Logan and Blaiszik, Ben and Chard, Kyle and Foster, Ian},
title = {Serverless Workflows for Indexing Large Scientific Data},
year = {2019},
isbn = {9781450370387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366623.3368140},
doi = {10.1145/3366623.3368140},
abstract = {The use and reuse of scientific data is ultimately dependent on the ability to understand what those data represent, how they were captured, and how they can be used. In many ways, data are only as useful as the metadata available to describe them. Unfortunately, due to growing data volumes, large and distributed collaborations, and a desire to store data for long periods of time, scientific "data lakes" quickly become disorganized and lack the metadata necessary to be useful to researchers. New automated approaches are needed to derive metadata from scientific files and to use these metadata for organization and discovery. Here we describe one such system, Xtract, a service capable of processing vast collections of scientific files and automatically extracting metadata from diverse file types. Xtract relies on function as a service models to enable scalable metadata extraction by orchestrating the execution of many, short-running extractor functions. To reduce data transfer costs, Xtract can be configured to deploy extractors centrally or near to the data (i.e., at the edge). We present a prototype implementation of Xtract and demonstrate that it can derive metadata from a 7 TB scientific data repository.},
booktitle = {Proceedings of the 5th International Workshop on Serverless Computing},
pages = {43–48},
numpages = {6},
keywords = {file systems, materials science, data lakes, metadata extraction, serverless},
location = {Davis, CA, USA},
series = {WOSC '19}
}

@inproceedings{10.1145/3147234.3148104,
author = {Jansen, Christoph and Beier, Maximilian and Witt, Michael and Frey, Sonja and Krefting, Dagmar},
title = {Towards Reproducible Research in a Biomedical Collaboration Platform Following the FAIR Guiding Principles},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3148104},
doi = {10.1145/3147234.3148104},
abstract = {Replication of computational experiments is essential for verifiable research. However, it requires a comprehensive and unambiguous description of all employed digital artifacts, in particular data, code and the computational environment. Recently, the FAIR Guiding Principles have been published to support reproducible research. In this paper, a cloud-based biomedical collaboration platform has been evaluated regarding FAIR principles and has been extended to support reproducibility. The FAICE suite is presented, encompassing tools to thoroughly describe and reproduce a computational experiment within the original execution environment as well as within a dynamically configured VM.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {3–8},
numpages = {6},
keywords = {reproducibility, docker, cloud computing, repeatability, medical data, xnat},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@inproceedings{10.1145/3209281.3209309,
author = {Alarabiat, Ayman and Soares, Delfina and Ferreira, Luis and de S\'{a}-Soares, Filipe},
title = {Analyzing E-Governance Assessment Initiatives: An Exploratory Study},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209309},
doi = {10.1145/3209281.3209309},
abstract = {This paper presents an exploratory study aimed at identifying, exploring, and analyzing current EGOV assessment initiatives. We do so based on data obtained from a desktop research and from a worldwide questionnaire directed to the 193 countries that are part of the list used by the Statistics Division of the United Nations Department of Economic and Social Affairs (UNDESA). The study analyses 12 EGOV assessment initiatives: a) seven of them are international/regional EGOV assessment initiatives performed by the United Nations (UN), European Union (EU), Waseda-IAC, Organisation for Economic Co-operation and Development (OECD), World Bank (WB), WWW Foundation, and Open Knowledge Network (OKN); b) five of them are country-level EGOV assessment initiatives performed by Norway, Germany, India, Saudi Arabia, and the United Arab Emirates. Further, the study provides general results obtained from a questionnaire with participation from 18 countries: Afghanistan, Angola, Brazil, Cabo Verde, Denmark, Estonia, Finland, Germany, Ghana, Ireland, Latvia, the Netherlands, Norway, Oman, Pakistan, the Philippines, Portugal, and Slovenia. The findings show that there is no shortage of interest in assessing EGOV initiatives. However, the supply side of EGOV initiatives is the dominant perspective being assessed, particularly by regional and international organizations. While there is an increasing interest in assessing the users' perspective (demand side) by individual countries, such attempts still seem to be at an early stage. Additionally, the actual use and impact of various EGOV services and activities are rarely well identified and measured. This study represents a stepping stone for developing instruments for assessing EGOV initiatives in future works. For the current stage, the study presents several general suggestions to be considered during the assessment process.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {30},
numpages = {10},
keywords = {e-governance, e-government, assessment, evaluation},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3448016.3457552,
author = {Fu, Yupeng and Soman, Chinmay},
title = {Real-Time Data Infrastructure at Uber},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457552},
doi = {10.1145/3448016.3457552},
abstract = {Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2503–2516},
numpages = {14},
keywords = {real-time infrastructure, streaming processing},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3333165.3333185,
author = {Soufan, Ayah},
title = {Deep Learning for Sentiment Analysis of Arabic Text},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333185},
doi = {10.1145/3333165.3333185},
abstract = {Deep learning has been very successful in the past decades, especially in Computer Vision and Speech Recognition fields. It has been also used successfully in the Natural Language Processing field because of the availability of an enormous amount of online text data, such as social networks and reviews websites, which have gained a lot of popularity and success in the past years. Sentiment Analysis is one of the hottest applications of Natural Language Processing (NLP). Many researchers have done excellent work on Sentiment Analysis for English language. However, the amount of work on Sentiment Analysis for Arabic language is, in comparison, very limited due to the complexity of the Arabic language's morphology and orthography. Unlike the English language, Arabic has many different dialects which makes Sentiment Analysis for Arabic more difficult and challenging, especially when working on data collected from social networks, which is known to be unstructured and noisy. Most of the work that has been done on Sentiment Analysis of Arabic language, focused on using lexicons and basic machine learning models. In addition, most of the work has been done on small datasets because of the limited number of the available annotated datasets for Arabic language. This paper proposes state-of-the-art research for Sentiment Analysis of Arabic microblogging using new techniques, and a sophisticated Arabic text data preprocessing.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {20},
numpages = {8},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@article{10.1145/3232863,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms, and Fundamental Limits},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2167-8375},
url = {https://doi.org/10.1145/3232863},
doi = {10.1145/3232863},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. One primary goal of the data collector is to learn some desired information from the elicited data. Specifically, this information is modeled by an underlying state, and the private data of each individual represents his of her knowledge about the state. Departing from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Further, an individual takes full control of his or her own data privacy and reports only a privacy-preserving version of his or her data.In this article, the value of ϵ units of privacy is measured by the minimum payment among all nonnegative payment mechanisms, under which an individual’s best response at a Nash equilibrium is to report his or her data in an ϵ-locally differentially private manner. The higher ϵ is, the less private the reported data is. We derive lower and upper bounds on the value of privacy that are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use a lower payment to buy ϵ units of privacy, and the upper bound is given by an achievable payment mechanism that we design. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given accuracy target for learning the underlying state and show that the total payment of the designed mechanism is at most one individual’s payment away from the minimum.},
journal = {ACM Trans. Econ. Comput.},
month = {aug},
articleno = {8},
numpages = {26},
keywords = {differential privacy, randomized response, Data collection}
}

@article{10.1145/2993231.2993234,
author = {Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno},
title = {A Retrospective Analysis of SAC Requirements: Engineering Track},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2993231.2993234},
doi = {10.1145/2993231.2993234},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {aug},
pages = {26–41},
numpages = {16},
keywords = {relevance, systematic mapping study, retrospective, requirements engineering, scoping study, symposium on applied computing, SAC, trends}
}

@inproceedings{10.1145/3410566.3410606,
author = {Seong, Younho and Nuamah, Joseph and Yi, Sun},
title = {Guidelines for Cybersecurity Visualization Design},
year = {2020},
isbn = {9781450375030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410566.3410606},
doi = {10.1145/3410566.3410606},
abstract = {Cyber security visualization designers can benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design. We survey human factors concepts and principles that have been applied in the past decade of human factors research. We highlight these concepts and relate them to cybersecurity visualization design. We provide guidelines to help cybersecurity visualization designers address some human factors challenges in the context of interface design. We use ecological interface design approach to present human factors-based principles of interface design for visualization. Cyber security visualization designers will benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design.},
booktitle = {Proceedings of the 24th Symposium on International Database Engineering &amp; Applications},
articleno = {25},
numpages = {6},
keywords = {affordance, ecological interface design, visualization, cognition, cybersecurity},
location = {Seoul, Republic of Korea},
series = {IDEAS '20}
}

@article{10.1145/3323334,
author = {Usman, Muhammad and Jan, Mian Ahmad and He, Xiangjian and Chen, Jinjun},
title = {A Survey on Big Multimedia Data Processing and Management in Smart Cities},
year = {2019},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3323334},
doi = {10.1145/3323334},
abstract = {Integration of embedded multimedia devices with powerful computing platforms, e.g., machine learning platforms, helps to build smart cities and transforms the concept of Internet of Things into Internet of Multimedia Things (IoMT). To provide different services to the residents of smart cities, the IoMT technology generates big multimedia data. The management of big multimedia data is a challenging task for IoMT technology. Without proper management, it is hard to maintain consistency, reusability, and reconcilability of generated big multimedia data in smart cities. Various machine learning techniques can be used for automatic classification of raw multimedia data and to allow machines to learn features and perform specific tasks. In this survey, we focus on various machine learning platforms that can be used to process and manage big multimedia data generated by different applications in smart cities. We also highlight various limitations and research challenges that need to be considered when processing big multimedia data in real-time.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {54},
numpages = {29},
keywords = {multimedia, machine learning, smart cities, management, IoMT}
}

@inproceedings{10.1145/2608029.2608030,
author = {Gannon, Dennis and Fay, Dan and Green, Daron and Takeda, Kenji and Yi, Wenming},
title = {Science in the Cloud: Lessons from Three Years of Research Projects on Microsoft Azure},
year = {2014},
isbn = {9781450329118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2608029.2608030},
doi = {10.1145/2608029.2608030},
abstract = {Microsoft Research is now in its fourth year of awarding Windows Azure cloud resources to the academic community. As of April 2014, over 200 research projects have started. In this paper we review the results of this effort to date. We also characterize the computational paradigms that work well in public cloud environments and those that are usually disappointing. We also discuss many of the barriers to successfully using commercial cloud platforms in research and ways these problems can be overcome.},
booktitle = {Proceedings of the 5th ACM Workshop on Scientific Cloud Computing},
pages = {1–8},
numpages = {8},
keywords = {scalable systems, infrastructure as a service, cloud computing, map reduce, cloud programming models, platform as a service},
location = {Vancouver, BC, Canada},
series = {ScienceCloud '14}
}

@article{10.1145/3185048,
author = {Zhang, Han and Hill, Shawndra and Rothschild, David},
title = {Addressing Selection Bias in Event Studies with General-Purpose Social Media Panels},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3185048},
doi = {10.1145/3185048},
abstract = {Data from Twitter have been employed in prior research to study the impacts of events. Conventionally, researchers use keyword-based samples of tweets to create a panel of Twitter users who mention event-related keywords during and after an event. However, the keyword-based sampling is limited in its objectivity dimension of data and information quality. First, the technique suffers from selection bias since users who discuss an event are already more likely to discuss event-related topics beforehand. Second, there are no viable control groups for comparison to a keyword-based sample of Twitter users. We propose an alternative sampling approach to construct panels of users defined by their geolocation. Geolocated panels are exogenous to the keywords in users’ tweets, resulting in less selection bias than the keyword panel method. Geolocated panels allow us to follow within-person changes over time and enable the creation of comparison groups. We compare different panels in two real-world settings: response to mass shootings and TV advertising. We first show the strength of the selection biases of keyword panels. Then, we empirically illustrate how geolocated panels reduce selection biases and allow meaningful comparison groups regarding the impact of the studied events. We are the first to provide a clear, empirical example of how a better panel selection design, based on an exogenous variable such as geography, both reduces selection bias compared to the current state of the art and increases the value of Twitter research for studying events. While we advocate for the use of a geolocated panel, we also discuss its weaknesses and application scenario seriously. This article also calls attention to the importance of selection bias in impacting the objectivity of social media data.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {4},
numpages = {24},
keywords = {Twitter, panels, survey, geolocation, coverage bias, selection bias, non-response bias, social media}
}

@inproceedings{10.1145/3152178.3152186,
author = {Shivaprabhu, Vivek R. and Balasubramani, Booma Sowkarthiga and Cruz, Isabel F.},
title = {Ontology-Based Instance Matching for Geospatial Urban Data Integration},
year = {2017},
isbn = {9781450354950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152178.3152186},
doi = {10.1145/3152178.3152186},
abstract = {To run a smart city, data is collected from disparate sources such as IoT devices, social media, private and public organizations, and government agencies. In the US, the City of Chicago has been a pioneer in the collection of data and in the development of a framework, called OpenGrid, to curate and analyze the collected data. OpenGrid is a geospatial situational awareness platform that allows policy makers, service providers, and the general public to explore city data and to perform advanced data analytics to enable planning of services, prediction of events and patterns, and identification of incidents across the city. This paper presents the instance matching module of GIVA, a Geospatial data Integration, Visualization, and Analytics platform, as applied to the integration of information related to businesses, which is spread across several datasets. In particular, we describe the integration of two datasets, Business Licenses and Food Inspections, so as to enable predictive analytics to determine which food establishments the city should inspect first. The paper describes semantic web-based instance matching mechanisms to compare the Business Names and Address fields.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
articleno = {8},
numpages = {8},
keywords = {Ontology, Data Integration, Record Linkage, Instance Matching, Geospatial Data},
location = {Redondo Beach, CA, USA},
series = {UrbanGIS'17}
}

@inproceedings{10.1145/3047273.3047299,
author = {Attard, Judie and Orlandi, Fabrizio and Auer, S\"{o}ren},
title = {Exploiting the Value of Data through Data Value Networks},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047299},
doi = {10.1145/3047273.3047299},
abstract = {Open data is increasingly permeating into all dimensions of our society and has become an indispensable commodity that serves as a basis for many products and services. Governments are generating a huge amount of data spanning different dimensions. This dataification shows the paramount need to identify the means and methods in which the value of data and knowledge can be exploited. While not restricted to the government domain, this dataification is certainly relevant in a government context, particularly due to the large volume of data generated by public institutions. In this paper we identify the various activities and roles within a data value chain, and hence proceed to provide our own definition of a Data Value Network. We specifically cater for non-tangible data products and characterise three dimensions that play a vital role within the Data Value Network. We also propose a Demand and Supply Distribution Model with the aim of providing insight on how an entity can participate in the global data market by producing a data product, as well as a concrete implementation through the Demand and Supply as a Service. Through our contributions we therefore project our vision of enhancing the process of open (government) data exploitation and innovation, with the aim of achieving the highest possible impact.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {475–484},
numpages = {10},
keywords = {exploitation, impacts, data demand, innovation, data supply, open data, data value chain, value creation, data value network},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@book{10.1145/3226595,
editor = {Brodie, Michael L.},
title = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
year = {2018},
isbn = {9781947487192},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {22},
abstract = {At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the "Nobel Prize" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker," the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource ("The Economist," May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.}
}

@inproceedings{10.1145/3209415.3209481,
author = {Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil},
title = {Exploiting Data Analytics for Social Services: On Searching for Profiles of Unlawful Use of Social Benefits},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209481},
doi = {10.1145/3209415.3209481},
abstract = {In this paper we present a data-driven profiling approach that we have adopted and implemented for a municipality. Our aim was to make profiles transparent and meaningful for citizens, policymakers and authorities so that they can validate, scrutinize and challenge the profiles. Our approach relies on a Genetic Algorithm (GA) that searches for useful and human understandable group profiles. Furthermore, we discuss some of the challenges encountered, show a selection of the profiles that were found by the GA, and discuss the necessity and a number of ways of validating these profiles in accordance with, e.g., privacy and non-discrimination laws and guidelines before using them in practice.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {550–559},
numpages = {10},
keywords = {profiling, social benefits, Data analytics, Genetic Algorithm},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inproceedings{10.1145/2889160.2889231,
author = {Barik, Titus and DeLine, Robert and Drucker, Steven and Fisher, Danyel},
title = {The Bones of the System: A Case Study of Logging and Telemetry at Microsoft},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889231},
doi = {10.1145/2889160.2889231},
abstract = {Large software organizations are transitioning to event data platforms as they culturally shift to better support data-driven decision making. This paper offers a case study at Microsoft during such a transition. Through qualitative interviews of 28 participants, and a quantitative survey of 1,823 respondents, we catalog a diverse set of activities that leverage event data sources, identify challenges in conducting these activities, and describe tensions that emerge in data-driven cultures as event data flow through these activities within the organization. We find that the use of event data span every job role in our interviews and survey, that different perspectives on event data create tensions between roles or teams, and that professionals report social and technical challenges across activities.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {92–101},
numpages = {10},
keywords = {developer tools, practices, telemetry, boundary object, collaboration, logging},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1145/3448119,
author = {Chen, Wenqiang and Chen, Lin and Ma, Meiyi and Parizi, Farshid Salemi and Patel, Shwetak and Stankovic, John},
title = {ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3448119},
doi = {10.1145/3448119},
abstract = {Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {45},
numpages = {25},
keywords = {text input, wearable devices, micro finger writing, vibration intelligence}
}

@article{10.1145/3447541,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh},
title = {TabReformer: Unsupervised Representation Learning for Erroneous Data Detection},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3447541},
doi = {10.1145/3447541},
abstract = {Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86%.},
journal = {ACM/IMS Trans. Data Sci.},
month = {may},
articleno = {18},
numpages = {29},
keywords = {bidirectional encoder, data augmentation, transformers, Error detection}
}

@article{10.1145/2794400,
author = {Guo, Bin and Wang, Zhu and Yu, Zhiwen and Wang, Yu and Yen, Neil Y. and Huang, Runhe and Zhou, Xingshe},
title = {Mobile Crowd Sensing and Computing: The Review of an Emerging Human-Powered Sensing Paradigm},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2794400},
doi = {10.1145/2794400},
abstract = {With the surging of smartphone sensing, wireless networking, and mobile social networking techniques, Mobile Crowd Sensing and Computing (MCSC) has become a promising paradigm for cross-space and large-scale sensing. MCSC extends the vision of participatory sensing by leveraging both participatory sensory data from mobile devices (offline) and user-contributed data from mobile social networking services (online). Further, it explores the complementary roles and presents the fusion/collaboration of machine and human intelligence in the crowd sensing and computing processes. This article characterizes the unique features and novel application areas of MCSC and proposes a reference framework for building human-in-the-loop MCSC systems. We further clarify the complementary nature of human and machine intelligence and envision the potential of deep-fused human--machine systems. We conclude by discussing the limitations, open issues, and research opportunities of MCSC.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {7},
numpages = {31},
keywords = {cross-space sensing and mining, crowd intelligence, human-machine systems, urban/community dynamics, Mobile phone sensing}
}

@inproceedings{10.1145/3491102.3501998,
author = {Cambo, Scott Allen and Gergle, Darren},
title = {Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501998},
doi = {10.1145/3491102.3501998},
abstract = {Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model’s development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {572},
numpages = {19},
keywords = {Computational reflexivity, human-centered machine learning, annotator fingerprinting, data science, model positionality, human-centered data science, critical data studies, position mining},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3342515,
author = {Chen, Yueyue and Guo, Deke and Bhuiyan, MD Zakirul Alam and Xu, Ming and Wang, Guojun and Lv, Pin},
title = {Towards Profit Optimization During Online Participant Selection in Compressive Mobile Crowdsensing},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3342515},
doi = {10.1145/3342515},
abstract = {A mobile crowdsensing (MCS) platform motivates employing participants from the crowd to complete sensing tasks. A crucial problem is to maximize the profit of the platform, i.e., the charge of a sensing task minus the payments to participants that execute the task. In this article, we improve the profit via the data reconstruction method, which brings new challenges, because it is hard to predict the reconstruction quality due to the dynamic features and mobility of participants. In particular, two Profit-driven Online Participant Selection (POPS) problems under different situations are studied in our work: (1) for S-POPS, the sensing cost of the different parts within the target area is the Same. Two mechanisms are designed to tackle this problem, including the ProSC and ProSC+. An exponential-based quality estimation method and a repetitive cross-validation algorithm are combined in the former mechanism, and the spatial distribution of selected participants are further discussed in the latter mechanism; (2) for V-POPS, the sensing cost of different parts within the target area is Various, which makes it the NP-hard problem. A heuristic mechanism called ProSCx is proposed to solve this problem, where the searching space is narrowed and both the participant quantity and distribution are optimized in each slot. Finally, we conduct comprehensive evaluations based on the real-world datasets. The experimental results demonstrate that our proposed mechanisms are more effective and efficient than baselines, selecting the participants with a larger profit for the platform.},
journal = {ACM Trans. Sen. Netw.},
month = {aug},
articleno = {38},
numpages = {29},
keywords = {online participant selection, Compressive mobile crowdsensing, data reconstruction}
}

@inproceedings{10.1145/3170358.3170367,
author = {Tsai, Yi-Shan and Moreno-Marcos, Pedro Manuel and Tammets, Kairit and Kollom, Kaire and Ga\v{s}evi\'{c}, Dragan},
title = {SHEILA Policy Framework: Informing Institutional Strategies and Policy Processes of Learning Analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170367},
doi = {10.1145/3170358.3170367},
abstract = {This paper introduces a learning analytics policy development framework developed by a cross-European research project team - SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed using the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents three case studies to illustrate the development process of the SHEILA policy framework, which can be used to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {320–329},
numpages = {10},
keywords = {strategy, policy, learning analytics, higher education, ROMA model},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3485190.3485193,
author = {Li, Yonghan and Lv, Hongjiang},
title = {The Dilemma of Digital Transformation of China's Hotel Industry and the Construction of Technology Platform: A Survey of Hotels Industry in China},
year = {2021},
isbn = {9781450384278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485190.3485193},
doi = {10.1145/3485190.3485193},
abstract = {The digital economy has been a hot spot in social development in recent years, and all walks of life are facing the opportunities and challenges of digital transformation. Successful digital transformation can enable traditional industries to gain dynamic capabilities in a changing environment, thereby gaining a leading competitive advantage. The previous literature paid more attention to the digital transformation of traditional industries, but lacked enough attention to the hotel industry. Through the case analysis of several major hotel groups in China, this article has gained profound insights in the digital transformation, enriched the influence of digital technology on the organization and management changes of the hotel industry, and has enlightening significance for guiding the hotel industry's practice.},
booktitle = {2021 4th International Conference on Information Management and Management Science},
pages = {13–18},
numpages = {6},
keywords = {management dilemma, Chinese hotel groups, digital transformation, technology platform},
location = {Chengdu, China},
series = {IMMS 2021}
}

@inproceedings{10.1145/3387940.3391466,
author = {Suni-Lopez, Franci and Condori-Fernandez, Nelly and Catala, Alejandro},
title = {Understanding Implicit User Feedback from Multisensorial and Physiological Data: A Case Study},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391466},
doi = {10.1145/3387940.3391466},
abstract = {Ensuring the quality of user experience is very important for increasing the acceptance likelihood of software applications, which can be affected by several contextual factors that continuously change over time (e.g., emotional state of end-user). Due to these changes in the context, software continually needs to adapt for delivering software services that can satisfy user needs. However, to achieve this adaptation, it is important to gather and understand the user feedback. In this paper, we mainly investigate whether physiological data can be considered and used as a form of implicit user feedback. To this end, we conducted a case study involving a tourist traveling abroad, who used a wearable device for monitoring his physiological data, and a smartphone with a mobile app for reminding him to take his medication on time during four days. Through the case study, we were able to identify some factors and activities as emotional triggers, which were used for understanding the user context. Our results highlight the importance of having a context analyzer, which can help the system to determine whether the detected stress could be considered as actionable and consequently as implicit user feedback.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {563–569},
numpages = {7},
keywords = {Physiological data, Actionable emotion, Case study, Context information, Implicit user feedback},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/2961111.2962628,
author = {Baltes, Sebastian and Diehl, Stephan},
title = {Worse Than Spam: Issues In Sampling Software Developers},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962628},
doi = {10.1145/2961111.2962628},
abstract = {Background: Reaching out to professional software developers is a crucial part of empirical software engineering research. One important method to investigate the state of practice is survey research. As drawing a random sample of professional software developers for a survey is rarely possible, researchers rely on various sampling strategies. Objective: In this paper, we report on our experience with different sampling strategies we employed, highlight ethical issues, and motivate the need to maintain a collection of key demographics about software developers to ease the assessment of the external validity of studies. Method: Our report is based on data from two studies we conducted in the past. Results: Contacting developers over public media proved to be the most effective and efficient sampling strategy. However, we not only describe the perspective of researchers who are interested in reaching goals like a large number of participants or a high response rate, but we also shed light onto ethical implications of different sampling strategies. We present one specific ethical guideline and point to debates in other research communities to start a discussion in the software engineering research community about which sampling strategies should be considered ethical.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {52},
numpages = {6},
keywords = {Software Developers, Empirical Research, Ethics, Sampling},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/2536780.2536783,
author = {Hafen, Ryan and Gibson, Tara D. and van Dam, Kerstin Kleese and Critchlow, Terence},
title = {Large-Scale Exploratory Analysis, Cleaning, and Modeling for Event Detection in Real-World Power Systems Data},
year = {2013},
isbn = {9781450325103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536780.2536783},
doi = {10.1145/2536780.2536783},
abstract = {In this paper, we present an approach to large-scale data analysis, Divide and Recombine (D&amp;R), and describe a hardware and software implementation that supports this approach. We then illustrate the use of D&amp;R on large-scale power systems sensor data to perform initial exploration, discover multiple data integrity issues, build and validate algorithms to filter bad data, and construct statistical event detection algorithms. This paper also reports on experiences using a non-traditional Hadoop distributed computing setup on top of a HPC computing cluster.},
booktitle = {Proceedings of the 3rd International Workshop on High Performance Computing, Networking and Analytics for the Power Grid},
articleno = {4},
numpages = {9},
keywords = {power systems, phasor measurement unit, divide and recombine, exploratory data analysis, R, Hadoop},
location = {Denver, Colorado},
series = {HiPCNA-PG '13}
}

@article{10.14778/2850583.2850587,
author = {Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.},
title = {QuERy: A Framework for Integrating Entity Resolution with Query Processing},
year = {2015},
issue_date = {November 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/2850583.2850587},
doi = {10.14778/2850583.2850587},
abstract = {This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {120–131},
numpages = {12}
}

@inproceedings{10.1145/3424771.3424822,
author = {Zimmermann, Olaf and L\"{u}bke, Daniel and Zdun, Uwe and Pautasso, Cesare and Stocker, Mirko},
title = {Interface Responsibility Patterns: Processing Resources and Operation Responsibilities},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424822},
doi = {10.1145/3424771.3424822},
abstract = {Remote Application Programming Interfaces (APIs), as for instance offered in microservices architectures, are used in almost any distributed system today and are thus enablers for many digitalization efforts. It is hard to design such APIs so that they are easy and effective to use; maintaining their runtime qualities while preserving backward compatibility is equally challenging. Finding well suited granularities in terms of the architectural capabilities of endpoints and the read-write semantics of their operations are particularly important design concerns. Existing pattern languages have dealt with local APIs in object-oriented programming, with remote objects, with queue-based messaging and with service-oriented computing platforms. However, patterns or equivalent guidances for the architectural design of API endpoints, operations and their request and response message structures are still missing. In this paper, we extend our microservice API pattern language (MAP) and introduce endpoint role and operation responsibility patterns, namely Processing Resource, Computation Function, State Creation Operation, Retrieval Operation, and State Transition Operation. Known uses and examples of the patterns are drawn from public Web APIs, as well as application development and system integration projects the authors have been involved in.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {9},
numpages = {24},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@article{10.1145/3177852,
author = {Antunes, Rodolfo S. and Seewald, Lucas A. and Rodrigues, Vinicius F. and Costa, Cristiano A. Da and Jr., Luiz Gonzaga and Righi, Rodrigo R. and Maier, Andreas and Eskofier, Bj\"{o}rn and Ollenschl\"{a}ger, Malte and Naderi, Farzad and Fahrig, Rebecca and Bauer, Sebastian and Klein, Sigrun and Campanatti, Gelson},
title = {A Survey of Sensors in Healthcare Workflow Monitoring},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3177852},
doi = {10.1145/3177852},
abstract = {Activities of a clinical staff in healthcare environments must regularly be adapted to new treatment methods, medications, and technologies. This constant evolution requires the monitoring of the workflow, or the sequence of actions from actors involved in a procedure, to ensure quality of medical services. In this context, recent advances in sensing technologies, including Real-time Location Systems and Computer Vision, enable high-precision tracking of actors and equipment. The current state-of-the-art about healthcare workflow monitoring typically focuses on a single technology and does not discuss its integration with others. Such an integration can lead to better solutions to evaluate medical workflows. This study aims to fill the gap regarding the analysis of monitoring technologies with a systematic literature review about sensors for capturing the workflow of healthcare environments. Its main scientific contribution is to identify both current technologies used to track activities in a clinical environment and gaps on their combination to achieve better results. It also proposes a taxonomy to classify work regarding sensing technologies and methods. The literature review does not present proposals that combine data obtained from Real-time Location Systems and Computer Vision sensors. Further analysis shows that a multimodal analysis is more flexible and could yield better results.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {42},
numpages = {37},
keywords = {workflow monitoring, healthcare, Real-time Location Systems, Computer Vision}
}

@inproceedings{10.1145/3234698.3234743,
author = {Nagaraja, Arun and Kumar, T. Satish},
title = {An Extensive Survey on Intrusion Detection- Past, Present, Future},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234743},
doi = {10.1145/3234698.3234743},
abstract = {Intrusion Detection is the most eminent fields in the network which can also be called as anomaly detection. Various methods used by early research tells that, the kind of measures used to detect the intrusion is not specified. Research has grown extensively in Anomaly intrusion detection by using different data mining techniques. Most researchers have not briefed on the kinds of distances measures used, the classification and feature selection techniques used in identifying intrusion detection. Intrusion detection is classified with problems as Outlier problems, Sparseness problem and Data Distribution. One of the important observations made is, High Dimensional Data Reduction is not performed, and conventional dataset is not used or maintained by any researchers. A survey is performed to identify the type of distance measures used and the type of datasets used in the early research. In this extended survey, the measures like Distance measure, pattern behaviors are used in identifying the Network Intrusion Detection. In this paper, we present the various methods used by authors to obtain feature selection methods. Also, the discussion is towards, Computation of High Dimensional Data, how to decide the Choice of Learning algorithm, Efficient Distance and similarity measures to identify the intrusion detection from different datasets.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {45},
numpages = {9},
keywords = {Datasets, Measures, Intrusion Detection, Anomaly Detection, Feature Selection, Clustering, Classification},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@inproceedings{10.1145/3144789.3144797,
author = {Zhang, Zhiqiang and Huang, Xiangbing and Iqbal, Muhammad Faisal Buland and Ye, Songtao},
title = {Better Weather Forecasting through Truth Discovery Analysis},
year = {2017},
isbn = {9781450352871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144789.3144797},
doi = {10.1145/3144789.3144797},
abstract = {In many real world applications, the same object or event may be described by multiple sources. As a result, conflicts among these sources are inevitable and these conflicts cause confusion as we have more than one value or outcome for each object. One significant problem is to resolve the confusion and to identify a piece of information which is trustworthy. This process of finding the truth from conflicting values of an object provided by multiple sources is called truth discovery or fact-finding. The main purpose of the truth discovery is to find more and more trustworthy information and reliable sources. Because the major assumption of truth discovery is on this intuitive principle, the source that provides trustworthy information is considered more reliable, and moreover, if the piece of information is from a reliable source, then it is more trustworthy. However, previously proposed truth discovery methods either do not conduct source reliability estimation at all (Voting Method), or even if they do, they do not model multiple properties of the object separately. This is the motivation for researchers to develop new techniques to tackle the problem of truth discovery in data with multiple properties. We present a method using an optimization framework which minimizes the overall weighted deviation between the truths and the multi-source observations. In this framework, different types of distance functions can be plugged in to capture the characteristics of different data types. We use weather datasets collected by four different platforms for extensive experiments and the results verify both the efficiency and precision of our methods for truth discovery.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent Information Processing},
articleno = {4},
numpages = {7},
keywords = {heterogeneous data, truth discovery, weather},
location = {Bangkok, Thailand},
series = {IIP'17}
}

@inproceedings{10.1145/3501247.3539017,
author = {Stan, George Vlad and Baart, Andr\'{e} and Dittoh, Francis and Akkermans, Hans and Bon, Anna},
title = {A Lightweight Downscaled Approach to Automatic Speech Recognition for Small Indigenous Languages},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3539017},
doi = {10.1145/3501247.3539017},
abstract = {Development of fully featured Automatic Speech Recognition (ASR) systems for a complete language vocabulary generally requires large data repositories, massive computing power, and a stable digital network infrastructure. These conditions are not met in the case of many indigenous languages. Based on our research for over a decade in West Africa, we present a lightweight and downscaled approach to AI-based ASR and describe a set of associated experiments. The aim is to produce a variety of limited-vocabulary ASRs as a basis for the development of practically useful (mobile and radio) voice-based information services that fit needs, preferences and knowledge of local rural communities.},
booktitle = {14th ACM Web Science Conference 2022},
pages = {451–458},
numpages = {8},
keywords = {neural networks, under-resourced/indigenous languages, voice-based technologies, automatic speech recognition, low resource environments, machine learning},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@inproceedings{10.1145/2660114.2660119,
author = {Sulser, Fabio and Giangreco, Ivan and Schuldt, Heiko},
title = {Crowd-Based Semantic Event Detection and Video Annotation for Sports Videos},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660119},
doi = {10.1145/2660114.2660119},
abstract = {Recent developments in sport analytics have heightened the interest in collecting data on the behavior of individuals and of the entire team in sports events. Rather than using dedicated sensors for recording the data, the detection of semantic events reflecting a team's behavior and the subsequent annotation of video data is nowadays mostly performed by paid experts. In this paper, we present an approach to generating such annotations by leveraging the wisdom of the crowd. We present the CrowdSport application that allows to collect data for soccer games. It presents crowd workers short video snippets of soccer matches and allows them to annotate these snippets with event information. Finally, the various annotations collected from the crowd are automatically disambiguated and integrated into a coherent data set. To improve the quality of the data entered, we have implemented a rating system that assigns each worker a trustworthiness score denoting the confidence towards newly entered data. Using the DBSCAN clustering algorithm and the confidence score, the integration ensures that the generated event labels are of high quality, despite of the heterogeneity of the participating workers. These annotations finally serve as a basis for a video retrieval system that allows users to search for video sequences on the basis of a graphical specification of team behavior or motion of the individual player. Our evaluations of the crowd-based semantic event detection and video annotation using the Microworkers platform have shown the effectiveness of the approach and have led to results that are in most cases close to the ground truth and can successfully be used for various retrieval tasks.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {63–68},
numpages = {6},
keywords = {video annotation, multimedia retrieval, sports, crowdsourcing},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@article{10.1109/TCBB.2019.2937862,
author = {Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat},
title = {Use of Electronic Health Data for Disease Prediction: A Comprehensive Literature Review},
year = {2021},
issue_date = {March-April 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2937862},
doi = {10.1109/TCBB.2019.2937862},
abstract = {Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {745–758},
numpages = {14}
}

@inproceedings{10.1145/3443467.3444711,
author = {Tian, Luogeng and Yang, Bailong and Yin, Xinli and Su, Yang},
title = {A Survey of Personalized Recommendation Based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3444711},
doi = {10.1145/3443467.3444711},
abstract = {Personalized recommendation is a key technology to effectively solve the overload of online information and eliminate information islands. It is widely known as an important way to improve the quality of information services. However, cold start, data sparseness, algorithm performance, recommendation accuracy and surprise are still the key issues that restrict users' personalized recommendations. Firstly, we review the development trend of personalized information recommendation algorithms in the past 15 years. And then we propose a new classification method for users' personalized recommendation based on machine learning algorithms with cold start, data sparseness, and the performance of the algorithm as the main goals. On this basis, we summarize and compare the ideas, practices and conclusions of related machine learning algorithms. Finally, we further summarize the main advantages and disadvantages of the 10 kinds of personalized recommendation algorithms from the perspective of classification proposed, and look forward to the development directions, difficulties, focus and methods of personalized recommendation algorithms.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {602–610},
numpages = {9},
keywords = {Graph Neural Networks, Machine learning, Personalized recommendation, Sparse matrix},
location = {Xiamen, China},
series = {EITCE 2020}
}

@inproceedings{10.1145/3302425.3302447,
author = {Qiao, Lin and Ran, Ran and Wu, He and Zhou, Qiaoni and Liu, Sai and Liu, Yunfei},
title = {Imputation Method of Missing Values for Dissolved Gas Analysis Data Based on Iterative KNN and XGBoost},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302447},
doi = {10.1145/3302425.3302447},
abstract = {Power transformers are an important part of the power system. Accurate monitoring of its operating status is particularly important for the normal and stable operation of the entire power system and the timely diagnosis of potential faults. Dissolved Gas Analysis (DGA) can detect and judge the oil-immersed power transformer failure by comparing the dissolved gas content of the power transformer in the normal operating state and the oil in the fault state. However, in the operation process of the grid transformer, the detection data is often missing. This paper proposes an effective method based on iterative KNN and XGBoost method for missing values. Firstly, according to the XGBoost integration tree, there are missing values. Information such as the number of attribute divisions obtained by data set training calculates the importance scores of different attributes to determine the priority of the attributes, and then performs interpolation on the missing values ?in an iterative manner. The experimental results in the case of DGA dataset and different missing rate show that the proposed method is superior to the existing similar methods in accuracy, and the dataset after interpolation has a significant improvement on the classification effect of the classifier.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {11},
numpages = {7},
keywords = {Iterative KNN, Missing Values, Dissolved Gas Analysis, Interpolation Priority},
location = {Sanya, China},
series = {ACAI 2018}
}

@inproceedings{10.1145/2503859.2503863,
author = {Bento, Fernando and Costa, Carlos J.},
title = {ERP Measure Success Model; a New Perspective},
year = {2013},
isbn = {9781450322997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503859.2503863},
doi = {10.1145/2503859.2503863},
abstract = {This paper addresses the problem of defining and evaluating the success of ERP throughout the life cycle of the information system. In order to solve this problem, many of the theoretical and empirical contributions on the success of the information system are analysed and discussed.This approach allows the development of a new model; especially in Delone &amp; Mclean supported research.This work will try to establish a different perspective on the success of the ERP and can be an encouragement to some organizations or the many researchers that will be engaging in these areas, in order to help achieve more clearly the expected performance in the acquisition phase of ERPs. Many times that performance does not always happen [1].},
booktitle = {Proceedings of the 2013 International Conference on Information Systems and Design of Communication},
pages = {16–26},
numpages = {11},
keywords = {success measuring models, ERP's, ERP's life cycle, information systems, performance},
location = {Lisboa, Portugal},
series = {ISDOC '13}
}

@inproceedings{10.1145/3468791.3469119,
author = {Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt},
title = {Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3469119},
doi = {10.1145/3468791.3469119},
abstract = {The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets.},
booktitle = {33rd International Conference on Scientific and Statistical Database Management},
pages = {61–72},
numpages = {12},
keywords = {Ranking, Knowledge Graphs, Question Answering},
location = {Tampa, FL, USA},
series = {SSDBM 2021}
}

@article{10.14778/2536222.2536238,
author = {Elmeleegy, Hazem and Li, Yinan and Qi, Yan and Wilmot, Peter and Wu, Mingxi and Kolay, Santanu and Dasdan, Ali and Chen, Songting},
title = {Overview of Turn Data Management Platform for Digital Advertising},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536238},
doi = {10.14778/2536222.2536238},
abstract = {This paper gives an overview of Turn Data Management Platform (DMP). We explain the purpose of this type of platforms, and show how it is positioned in the current digital advertising ecosystem. We also provide a detailed description of the key components in Turn DMP. These components cover the functions of (1) data ingestion and integration, (2) data warehousing and analytics, and (3) real-time data activation. For all components, we discuss the main technical and research challenges, as well as the alternative design choices. One of the main goals of this paper is to highlight the central role that data management is playing in shaping this fast growing multi-billion dollars industry.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1138–1149},
numpages = {12}
}

@inproceedings{10.1145/3473856.3473879,
author = {Herbert, Franziska and Schmidbauer-Wolf, Gina Maria and Reuter, Christian},
title = {Who Should Get My Private Data in Which Case? Evidence in the Wild},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3473879},
doi = {10.1145/3473856.3473879},
abstract = {As a result of the ongoing digitalization of our everyday lives, the amount of data produced by everyone is steadily increasing. This happens through personal decisions and items, such as the use of social media or smartphones, but also through more and more data acquisition in public spaces, such as e.g., Closed Circuit Television. Are people aware of the data they are sharing? What kind of data do people want to share with whom? Are people aware if they have Wi-Fi, GPS, or Bluetooth activated as potential data sharing functionalities on their phone? To answer these questions, we conducted a representative online survey as well as face-to-face interviews with users in Germany. We found that most users wanted to share private data on premise with most entities, indicating that willingness to share data depends on who has access to the data. Almost half of the participants would be more willing to share data with specific entities (state bodies &amp; rescue forces) in the event that an acquaintance is endangered. For Wi-Fi and GPS the frequencies of self-reported and actual activation on the smartphone are almost equal, but 17% of participants were unaware of the Bluetooth status on their smartphone. Our research is therefore in line with other studies suggesting relatively low privacy awareness of users.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {281–293},
numpages = {13},
keywords = {survey, awareness, data sharing, privacy},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inproceedings{10.1145/3491102.3502140,
author = {Rixen, Jan Ole and Colley, Mark and Askari, Ali and Gugenheimer, Jan and Rukzio, Enrico},
title = {Consent in the Age of AR: Investigating The Comfort With Displaying Personal Information in Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502140},
doi = {10.1145/3491102.3502140},
abstract = {Social Media (SM) has shown that we adapt our communication and disclosure behaviors to available technological opportunities. Head-mounted Augmented Reality (AR) will soon allow to effortlessly display the information we disclosed not isolated from our physical presence (e.g., on a smartphone) but visually attached to the human body. In this work, we explore how the medium (AR vs. Smartphone), our role (being augmented vs. augmenting), and characteristics of information types (e.g., level of intimacy, self-disclosed vs. non-self-disclosed) impact the users’ comfort when displaying personal information. Conducting an online survey (N=148), we found that AR technology and being augmented negatively impacted this comfort. Additionally, we report that AR mitigated the effects of information characteristics compared to those they had on smartphones. In light of our results, we discuss that information augmentation should be built on consent and openness, focusing more on the comfort of the augmented rather than the technological possibilities.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {295},
numpages = {14},
keywords = {Augmented Reality, User Acceptance, Personal Information, Mixed Reality, Social Acceptability, Public Experiences, Disclosure, Data Glasses, Consent, Comfort},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inbook{10.1145/3310205.3310212,
title = {Rule-Based Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310212},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1145/3462764,
author = {Bellio, Maura and Furniss, Dominic and Oxtoby, Neil P. and Garbarino, Sara and Firth, Nicholas C. and Ribbens, Annemie and Alexander, Daniel C. and Blandford, Ann},
title = {Opportunities and Barriers for Adoption of a Decision-Support Tool for Alzheimer’s Disease},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1957},
url = {https://doi.org/10.1145/3462764},
doi = {10.1145/3462764},
abstract = {Clinical decision-support tools (DSTs) represent a valuable resource in healthcare. However, lack of Human Factors considerations and early design research has often limited their successful adoption. To complement previous technically focused work, we studied adoption opportunities of a future DST built on a predictive model of Alzheimer’s Disease (AD) progression. Our aim is two-fold: exploring adoption opportunities for DSTs in AD clinical care, and testing a novel combination of methods to support this process. We focused on understanding current clinical needs and practices, and the potential for such a tool to be integrated into the setting, prior to its development. Our user-centred approach was based on field observations and semi-structured interviews, analysed through workflow analysis, user profiles, and a design-reality gap model. The first two are common practice, whilst the latter provided added value in highlighting specific adoption needs. We identified the likely early adopters of the tool as being both psychiatrists and neurologists based in research-oriented clinical settings. We defined ten key requirements for the translation and adoption of DSTs for AD around IT, user, and contextual factors. Future works can use and build on these requirements to stand a greater chance to get adopted in the clinical setting.},
journal = {ACM Trans. Comput. Healthcare},
month = {sep},
articleno = {32},
numpages = {19},
keywords = {healthcare, design-reality gap, user-centred design, Diffusion of innovation, technology adoption}
}

@inproceedings{10.1145/3502181.3531473,
author = {Yu, Xiaodong and Di, Sheng and Zhao, Kai and Tian, Jiannan and Tao, Dingwen and Liang, Xin and Cappello, Franck},
title = {Ultrafast Error-Bounded Lossy Compression for Scientific Datasets},
year = {2022},
isbn = {9781450391993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502181.3531473},
doi = {10.1145/3502181.3531473},
abstract = {Today's scientific high-performance computing applications and advanced instruments are producing vast volumes of data across a wide range of domains, which impose a serious burden on data transfer and storage. Error-bounded lossy compression has been developed and widely used in the scientific community because it not only can significantly reduce the data volumes but also can strictly control the data distortion based on the user-specified error bound. Existing lossy compressors, however, cannot offer ultrafast compression speed, which is highly demanded by numerous applications or use cases (such as in-memory compression and online instrument data compression). In this paper, we propose a novel ultrafast error-bounded lossy compressor that can obtain fairly high compression performance on both CPUs and GPUs and with reasonably high compression ratios. The key contributions are threefold. (1) We propose a generic error-bounded lossy compression framework---called SZx---that achieves ultrafast performance through its novel design comprising only lightweight operations such as bitwise and addition/subtraction operations, while still keeping a high compression ratio. (2) We implement SZx on both CPUs and GPUs and optimize the performance according to their architectures. (3) We perform a comprehensive evaluation with six real-world production-level scientific datasets on both CPUs and GPUs. Experiments show that SZx is 2~16x faster than the second-fastest existing error-bounded lossy compressor (either SZ or ZFP) on CPUs and GPUs, with respect to both compression and decompression.},
booktitle = {Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing},
pages = {159–171},
numpages = {13},
keywords = {gpu, scientific data, error-bounded lossy compression, high-speed compressor},
location = {Minneapolis, MN, USA},
series = {HPDC '22}
}

@article{10.1145/3411824,
author = {Zhang, Yun C. and Zhang, Shibo and Liu, Miao and Daly, Elyse and Battalio, Samuel and Kumar, Santosh and Spring, Bonnie and Rehg, James M. and Alshurafa, Nabil},
title = {SyncWISE: Window Induced Shift Estimation for Synchronization of Video and Accelerometry from Wearable Sensors},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3411824},
doi = {10.1145/3411824},
abstract = {The development and validation of computational models to detect daily human behaviors (e.g., eating, smoking, brushing) using wearable devices requires labeled data collected from the natural field environment, with tight time synchronization of the micro-behaviors (e.g., start/end times of hand-to-mouth gestures during a smoking puff or an eating gesture) and the associated labels. Video data is increasingly being used for such label collection. Unfortunately, wearable devices and video cameras with independent (and drifting) clocks make tight time synchronization challenging. To address this issue, we present the Window Induced Shift Estimation method for Synchronization (SyncWISE) approach. We demonstrate the feasibility and effectiveness of our method by synchronizing the timestamps of a wearable camera and wearable accelerometer from 163 videos representing 45.2 hours of data from 21 participants enrolled in a real-world smoking cessation study. Our approach shows significant improvement over the state-of-the-art, even in the presence of high data loss, achieving 90% synchronization accuracy given a synchronization tolerance of 700 milliseconds. Our method also achieves state-of-the-art synchronization performance on the CMU-MMAC dataset.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {107},
numpages = {26},
keywords = {Wearable Camera, Video, Time Synchronization, Temporal Drift, Automatic Synchronization, Wearable Sensor, Accelerometry}
}

@inbook{10.1145/3310205.3310213,
title = {Machine Learning and Probabilistic Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310213},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3300061.3345456,
author = {Rodrigues, Jo\~{a}o G. P. and Aguiar, Ana},
title = {Extracting 3D Maps from Crowdsourced GNSS Skyview Data},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3345456},
doi = {10.1145/3300061.3345456},
abstract = {3D maps of urban environments are useful in various fields ranging from cellular network planning to urban planning and climatology. These models are typically constructed using expensive techniques such as manual annotation with 3D modeling tools, extrapolated from satellite or aerial photography, or using specialized hardware with depth sensing devices. In this work, we show that 3D urban maps can be extracted from standard GNSS data, by analyzing the received satellite signals that are attenuated by obstacles, such as buildings. Furthermore, we show that these models can be extracted from low-accuracy GNSS data, crowdsourced opportunistically from standard smartphones during their user's uncontrolled daily commute trips, unleashing the potential of applying the principle to wide areas. Our proposal incorporates position inaccuracies in the calculations, and accommodates different sources of variability of the satellite signals' SNR. The diversity of collection conditions of crowdsourced GNSS positions is used to mitigate bias and noise from the data. A binary classification model is trained and evaluated on multiple urban scenarios using data crowdsourced from over 900 users. Our results show that the generalization accuracy for a Random Forest classifier in typical urban environments lies between 79% and 91% on 4 m wide voxels, demonstrating the potential of the proposed method for building 3D maps for wide urban areas.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {55},
numpages = {15},
keywords = {crowdsensing, 3d mapping, gnss snr measurements},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inproceedings{10.1145/2487575.2488217,
author = {Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils},
title = {Online Controlled Experiments at Large Scale},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488217},
doi = {10.1145/2487575.2488217},
abstract = {Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1168–1176},
numpages = {9},
keywords = {randomized experiments, controlled experiments, a/b testing},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2858036.2858445,
author = {West, Peter and Giordano, Richard and Van Kleek, Max and Shadbolt, Nigel},
title = {The Quantified Patient in the Doctor's Office: Challenges &amp; Opportunities},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858445},
doi = {10.1145/2858036.2858445},
abstract = {While the Quantified Self and personal informatics fields have focused on the individual's use of self-logged data about themselves, the same kinds of data could, in theory, be used to improve diagnosis and care planning. In this paper, we seek to understand both the opportunities and bottlenecks in the use of self-logged data for differential diagnosis and care planning during patient visits to both primary and secondary care. We first conducted a literature review to identify potential factors influencing the use of self-logged data in clinical settings. This informed the design of our experiment, in which we applied a vignette-based role-play approach with general practitioners and hospital specialists in the US and UK, to elicit reflections on and insights about using patient self-logged data. Our analysis reveals multiple opportunities for the use of self-logged data in the differential diagnosis workflow, identifying capture, representational, and interpretational challenges that are potentially preventing self-logged data from being effectively interpreted and applied by clinicians to derive a patient's prognosis and plan of care.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {3066–3078},
numpages = {13},
keywords = {quantified self, self-tracking, clinical decision making},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3448016.3457330,
author = {Peng, Jinglin and Wu, Weiyuan and Lockhart, Brandon and Bian, Song and Yan, Jing Nathan and Xu, Linghao and Chi, Zhixuan and Rzeszotarski, Jeffrey M. and Wang, Jiannan},
title = {DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457330},
doi = {10.1145/3448016.3457330},
abstract = {Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2271–2280},
numpages = {10},
keywords = {statistical modeling, python, data exploration, data profiling, exploratory data analysis, data preparation},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1145/3210548,
author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur},
title = {Machine Learning for the Developing World},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3210548},
doi = {10.1145/3210548},
abstract = {Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {aug},
articleno = {9},
numpages = {14},
keywords = {Global development, developing countries}
}

@inproceedings{10.1145/3477314.3507007,
author = {Haudenschild, Christian and Vaickus, Louis and Levy, Joshua},
title = {Configuring a Federated Network of Real-World Patient Health Data for Multimodal Deep Learning Prediction of Health Outcomes},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507007},
doi = {10.1145/3477314.3507007},
abstract = {Vast quantities of electronic patient medical data are currently being collated and processed in large federated data repositories. For instance, TriNetX, Inc., a global health research network, has access to more than 300 million patients, sourced from healthcare organizations, biopharmaceutical companies, and contract research organizations. As such, pipelines that are able to algorithmically extract huge quantities of patient data from multiple modalities present opportunities to leverage machine learning and deep learning approaches with the possibility of generating actionable insight. In this work, we present a modular, semi-automated end-to-end machine and deep learning pipeline designed to interface with a federated network of structured patient data. This proof-of-concept pipeline is disease-agnostic, scalable, and requires little domain expertise and manual feature engineering in order to quickly produce results for the case of a user-defined binary outcome event. We demonstrate the pipeline's efficacy with three different disease workflows, with high discriminatory power achieved in all cases.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {627–635},
numpages = {9},
keywords = {comorbidity, lab measurements, deep learning, electronic health records, federated data network},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3450613.3459657,
author = {Amyrotos, Christos},
title = {Adaptive Visualizations for Enhanced Data Understanding and Interpretation},
year = {2021},
isbn = {9781450383660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450613.3459657},
doi = {10.1145/3450613.3459657},
abstract = {In a data driven economy where data volume and dimensions are explosively increasing, businesses rely on business intelligence and analytics (BI&amp;A) platforms for analysing their data and coming to beneficial decisions. With the ever-growing generation of data, the process of data analysis is becoming more complicated for the business users, as the exploration of more demanding use cases increases. While the existing BI&amp;A platforms provide myriads of data visualizations that support data exploration, none of those account for the user’s individual differences, needs or requirements, and thus may hinder the user’s understanding of visual data and consequently their decision-making processes. This work embarks on an interdisciplinary endeavour to introduce a human-centred adaptive data visualizations framework in the context of business, as the core of an adaptive data analytics platform, that aims to enhance the business user’s decision making by increasing her understanding of data. The framework is built using a multi-dimensional human-centred user model that goes beyond traditional user characteristics and accounts for cognitive factors, domain expertise and experience and factors related to the business context i.e., data, visualizations and tasks; a data visualization engine that will recommend to the unique-user the best-fit data visualizations based on the abovementioned user model; and an intelligent data analytics component that enhances the efficiency and effectiveness of the data exploration process by leveraging user interactions during the explorations to further inform the user model on the user’s expertise and experience.},
booktitle = {Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {291–297},
numpages = {7},
keywords = {user modeling, business analytics, personalization, adaptation, human factors, data visualizations},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}

@inbook{10.1145/3447404.3447409,
author = {McMenemy, David},
title = {The Internet of Everything—Introducing Privacy},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447409},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {57–58},
numpages = {2}
}

@inbook{10.1145/3447404.3447430,
title = {Authors’ Biographies/Index},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447430},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {433–455},
numpages = {23}
}

@inproceedings{10.1145/3377049.3377083,
author = {Chowdhury, S. M. Habibul Mursaleen and Jahan, Ferdous and Sara, Sarawat Murtaza and Nandi, Dip},
title = {Secured Blockchain Based Decentralised Internet: A Proposed New Internet},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377083},
doi = {10.1145/3377049.3377083},
abstract = {Throughout this paper, we try to describe with blockchain technology the decentralization of the internet. A decentralized network that encourages the internet to operate from the smartphone or tablet of anybody instead of centralized servers. A decentralized implementation would be based on a peer-to-peer network that is dependent on a user community. Their machines connected to the internet will host the network, not a community of more powerful servers. Each site would be distributed across thousands of nodes on various devices. The data is therefore not contained, owned by private storage facilities. There is therefore no central point to hack, and no way for an oligarchy of entities to take control of it. A proposed alternative was formed based on a systematic literature review that demonstrates that Internet decentralization is what this modern technology needs in order to address not only the weaknesses of current servers including server down issue, hacking and data manipulation or single point of failure, but also to prevent companies from monetizing the data of citizens through their server and to market them to the advertisers.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {8},
numpages = {7},
keywords = {Web 3.0, Cryptography, Data Privacy, DApp, Decentralised Web, Encryption, Ethereum, Peer-To-Peer Network, Whisper, Server vulnerabilities, Smart Contracts, Blockchain, Bitcoin},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@inbook{10.1145/3447404.3447427,
author = {McMenemy, David},
title = {Ethics and Adaptive Touch Interfaces},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447427},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {407–408},
numpages = {2}
}

@inbook{10.1145/3447404.3447413,
author = {McMenemy, David},
title = {Ethical Issues of Digital Signal Processing},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447413},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {141–142},
numpages = {2}
}

@inbook{10.1145/3447404.3447429,
author = {McMenemy, David},
title = {Ethics in Automotive User Interface},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447429},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {431–432},
numpages = {2}
}

@inbook{10.1145/3447404.3447425,
author = {McMenemy, David},
title = {Ethics and Personal Context},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447425},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {377–378},
numpages = {2}
}

@article{10.1145/3533378,
author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533378},
doi = {10.1145/3533378},
abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow we show that practitioners face issues at each stage of the deployment process. The goal of this paper is to lay out a research agenda to explore approaches addressing these challenges.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {apr},
keywords = {sofware deployment, Machine learning applications}
}

@inbook{10.1145/3447404.3447421,
author = {McMenemy, David},
title = {Ethical Issues in Probabilistic Text Entry},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447421},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {321–322},
numpages = {2}
}

@inproceedings{10.1145/3277139.3277179,
author = {Lu, Yi and Yin, Jun and Ge, Shilun},
title = {Analysis of Dynamic Complexity Feature of Information System Data Based on Visualization},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277179},
doi = {10.1145/3277139.3277179},
abstract = {The volume and complexity of the data in the information system continues to increase during the operation of the information system. The phenomenon leads to the fact that decision-makers are faced with data-rich and information-deficient situations. How to intuitively grasp the dynamic complexity of information system data has become a concern of scholars. Therefore, this paper collected and organized the system log data of two shipping companies, adopted multiple visualization forms, and combined the dynamic complexity measurement methods of information systems, focusing on the overall distribution status and development trend of the data dynamic complexity and internal information system and the influence of complexity of subsystems within the information system on overall data dynamic operation complexity, and then analyzed the overall feature of the dynamic complexity of information system data, and summarized the feature regulars of the complexity of the visualization method. The results show that the visual feature analysis of the dynamic complexity of information systems can improve the ability of enterprises to analyze and make decisions, and provide a basis for enterprise managers to develop corresponding management measures.},
booktitle = {Proceedings of the 2018 International Conference on Information Management &amp; Management Science},
pages = {206–215},
numpages = {10},
keywords = {visualization, feature analysis, dynamic complexity, management information system},
location = {Chengdu, China},
series = {IMMS '18}
}

@inproceedings{10.1145/3209281.3209317,
author = {Batubara, F. Rizal and Ubacht, Jolien and Janssen, Marijn},
title = {Challenges of Blockchain Technology Adoption for E-Government: A Systematic Literature Review},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209317},
doi = {10.1145/3209281.3209317},
abstract = {The ability of blockchain technology to record transactions on distributed ledgers offers new opportunities for governments to improve transparency, prevent fraud, and establish trust in the public sector. However, blockchain adoption and use in the context of e-Government is rather unexplored in academic literature. In this paper, we systematically review relevant research to understand the current research topics, challenges and future directions regarding blockchain adoption for e-Government. The results show that the adoption of blockchain-based applications in e-Government is still very limited and there is a lack of empirical evidence. The main challenges faced in blockchain adoption are predominantly presented as technological aspects such as security, scalability and flexibility. From an organizational point of view, the issues of acceptability and the need of new governance models are presented as the main barriers to adoption. Moreover, the lack of legal and regulatory support is identified as the main environmental barrier of adoption. Based on the challenges presented in the literature, we propose future research questions that need to be addressed to inform how the public sector should approach the blockchain technology adoption.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {76},
numpages = {9},
keywords = {literature review, blockchain, public service, government, adoption},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.5555/3192424.3192631,
author = {Musciotto, Federico and Delpriori, Saverio and Castagno, Paolo and Pournaras, Evangelos},
title = {Mining Social Interactions in Privacy-Preserving Temporal Networks},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {The opportunities to empirically study temporal networks nowadays are immense thanks to Internet of Things technologies along with ubiquitous and pervasive computing that allow a real-time fine-grained collection of social network data. This empowers data analytics and data scientists to reason about complex temporal phenomena, such as disease spread, residential energy consumption, political conflicts etc., using systematic methologies from complex networks and graph spectra analysis. However, a misuse of these methods may result in privacy-intrusive and discriminatory actions that may threaten citizens' autonomy and put their life under surveillance. This paper studies highly sparse temporal networks that model social interactions such as the physical proximity of participants in conferences. When citizens can self-determine the anonymized proximity data they wish to share via privacy-preserving platforms, temporal networks may turn out to be highly sparse and have low quality. This paper shows that even in this challenging scenario of privacy-by-design, significant information can be mined from temporal networks such as the correlation of events happening during a conference or stable groups interacting over time. The findings of this paper contribute to the introduction of privacy-preserving data analytics in temporal networks and their applications.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1103–1110},
numpages = {8},
location = {Davis, California},
series = {ASONAM '16}
}

@article{10.1613/jair.1.12228,
author = {Burkart, Nadia and Huber, Marco F.},
title = {A Survey on the Explainability of Supervised Machine Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12228},
doi = {10.1613/jair.1.12228},
abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {245–317},
numpages = {73}
}

@inproceedings{10.1145/3489525.3511699,
author = {Leznik, Mark and Grohmann, Johannes and Kliche, Nina and Bauer, Andr\'{e} and Seybold, Daniel and Eismann, Simon and Kounev, Samuel and Domaschka, J\"{o}rg},
title = {Same, Same, but Dissimilar: Exploring Measurements for Workload Time-Series Similarity},
year = {2022},
isbn = {9781450391436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489525.3511699},
doi = {10.1145/3489525.3511699},
abstract = {Benchmarking is a core element in the toolbox of most systems researchers and is used for analyzing, comparing, and validating complex systems. In the quest for reliable benchmark results, a consensus has formed that a significant experiment must be based on multiple runs. To interpret these runs, mean and standard deviation are often used. In case of experiments where each run produces a time series, applying and comparing the mean is not easily applicable and not necessarily statistically sound. Such an approach ignores the possibility of significant differences between runs with a similar average. In order to verify this hypothesis, we conducted a survey of 1,112 publications of selected performance engineering and systems conferences canvassing open data sets from performance experiments. The identified 3 data sets purely rely on average and standard deviation. Therefore, we propose a novel analysis approach based on similarity analysis to enhance the reliability of performance evaluations. Our approach evaluates 12 (dis-)similarity measures with respect to their applicability in analysing performance measurements and identifies four suitable similarity measures. We validate our approach by demonstrating the increase in reliability for the data sets found in the survey.},
booktitle = {Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering},
pages = {89–96},
numpages = {8},
keywords = {distance metrics, workload analysis, time series similarity, data sets, time series},
location = {Beijing, China},
series = {ICPE '22}
}

@inbook{10.1145/3447404.3447423,
author = {McMenemy, David},
title = {Ethics and Secure Gestures},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447423},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {339–340},
numpages = {2}
}

@inbook{10.1145/3447404.3447405,
title = {Preface},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447405},
abstract = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice provides a comprehensive resource on what has become the dominant paradigm for novel interaction design methods involving gesture, speech, text, and touch embedded in novel and emerging interfaces. These interfaces support smartphones, wearables, in-vehicle devices, virtual reality, robotic, the Internet of Things (IoT), brain–computer interaction, and many other applications that are now highly competitive commercially.This edited collection is written by international experts and pioneers in the field of digital signal processing (DSP) and machine learning (ML) for interactive systems. It provides a textbook for students, and a reference and technology roadmap for developers and professionals working in interaction design on emerging platforms. This introductory textbook presents theory chapters on statistical grounding, signal processing, and ML foundations for guiding the development of novel interactive systems. Additional chapters discuss case studies on smart cities, brain–computer interfaces (BCI), probabilistic text entry, secure gestures, personal context from mobile phones, building adaptive touch interfaces, and automotive user interfaces (UIs). The chapters on case studies also highlight an in-depth look at domain-specific language (DSL) and ML methods used, for example, in touch, gesture, electroencephalography (EEG), electrocardiography (ECG), and galvanic skin response (GSR) signals, or embedded sensor inputs. A common theme throughout is the ubiquitous support for humans as they go about their daily professional or personal activities.This introductory book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal–multisensor systems. After each chapter an expert on the legal and ethical issues explores the wider ethical issues on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during interaction with novel platforms.Parisa Eslambolchilar, Andreas Komninos, and Mark D. Dunlop, March 2020AcknowledgmentsWe would like to thank our external reviewers for their valuable feedback throughout the writing process.},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {xv–xvi}
}

@article{10.1109/TNET.2018.2829173,
author = {Tu, Zhen and Xu, Fengli and Li, Yong and Zhang, Pengyu and Jin, Depeng},
title = {A New Privacy Breach: User Trajectory Recovery From Aggregated Mobility Data},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2829173},
doi = {10.1109/TNET.2018.2829173},
abstract = {Human mobility data have been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual’s mobility records usually gives rise to privacy issues, data sets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users’ privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals’ trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual’s trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world data sets collected from both the mobile application and cellular network, we reveal that the attack system is able to recover users’ trajectories with an accuracy of about 73%~91% at the scale of thousands to ten thousands of mobile users, which indicates severe privacy leakage in such data sets. Our extensive analysis also reveals that by generalization and perturbation, this kind of privacy leakage can only be mitigated. Through the investigation on aggregated mobility data, this paper recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both the academy and industry.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1446–1459},
numpages = {14}
}

@inproceedings{10.1145/3287560.3287577,
author = {Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa, Boyang and Whittington, Jan and Howe, Bill},
title = {Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287577},
doi = {10.1145/3287560.3287577},
abstract = {Data too sensitive to be "open" for analysis and re-purposing typically remains "closed" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {191–200},
numpages = {10},
keywords = {data ethics, algorithmic bias, data governance, data sharing, privacy},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@article{10.1145/3531533,
author = {Gram, Dennis and Karapanagiotis, Pantelis and Liebald, Marius and Walz, Uwe},
title = {Design and Implementation of a Historical German Firm-Level Financial Database},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3531533},
doi = {10.1145/3531533},
abstract = {Broad, long-term financial, and economic datasets are scarce resources, particularly in the European context. In this article, we present an approach for an extensible data model that is adaptable to future changes in technologies and sources. This model may constitute a basis for digitized and structured long-term historical datasets for different jurisdictions and periods. The data model covers the specific peculiarities of historical financial and economic data and is flexible enough to reach out for data of different types (quantitative as well as qualitative) from different historical sources, hence, achieving extensibility. Furthermore, we outline a relational implementation of this approach based on historical German firm and stock market data from 1920 to 1932.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {19},
numpages = {22},
keywords = {cliometrics, financial data, Databases, Germany, economic history}
}

@inproceedings{10.5555/3433701.3433811,
author = {Grosset, Pascal and Biwer, Christopher M. and Pulido, Jesus and Mohan, Arvind T. and Biswas, Ayan and Patchett, John and Turton, Terece L. and Rogers, David H. and Livescu, Daniel and Ahrens, James},
title = {Foresight: Analysis That Matters for Data Reduction},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {As the computation power of supercomputers increases, so does simulation size, which in turn produces orders-of-magnitude more data. Because generated data often exceed the simulation's disk quota, many simulations would stand to benefit from data-reduction techniques to reduce storage requirements. Such techniques include autoencoders, data compression algorithms, and sampling. Lossy compression techniques can significantly reduce data size, but such techniques come at the expense of losing information that could result in incorrect post hoc analysis results. To help scientists determine the best compression they can get while keeping their analyses accurate, we have developed Foresight, an analysis framework that enables users to evaluate how different data-reduction techniques will impact their analyses. We use particle data from a cosmology simulation, turbulence data from Direct Numerical Simulation, and asteroid impact data from xRage to demonstrate how Foresight can help scientists determine the best data-reduction technique for their simulations.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {83},
numpages = {15},
keywords = {performance evaluation, data compression, multi-layer neural network},
location = {Atlanta, Georgia},
series = {SC '20}
}

@inproceedings{10.1145/2988336.2988349,
author = {Singh, Jatinder and Pasquier, Thomas and Bacon, Jean and Powles, Julia and Diaconu, Raluca and Eyers, David},
title = {Big Ideas Paper: Policy-Driven Middleware for a Legally-Compliant Internet of Things},
year = {2016},
isbn = {9781450343008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988336.2988349},
doi = {10.1145/2988336.2988349},
abstract = {Internet of Things (IoT) applications, systems and services are subject to law. We argue that for the IoT to develop lawfully, there must be technical mechanisms that allow the enforcement of specified policy, such that systems align with legal realities. The audit of policy enforcement must assist the apportionment of liability, demonstrate compliance with regulation, and indicate whether policy correctly captures legal responsibilities. As both systems and obligations evolve dynamically, this cycle must be continuously maintained.This poses a huge challenge given the global scale of the IoT vision. The IoT entails dynamically creating new services through managed and flexible data exchange. Data management is complex in this dynamic environment, given the need to both control and share information, often across federated domains of administration.We see middleware playing a key role in managing the IoT. Our vision is for a middleware-enforced, unified policy model that applies end-to-end, throughout the IoT. This is because policy cannot be bound to things, applications, or administrative domains, since functionality is the result of composition, with dynamically formed chains of data flows.We have investigated the use of Information Flow Control (IFC) to manage and audit data flows in cloud computing; a domain where trust can be well-founded, regulations are more mature and associated responsibilities clearer. We feel that IFC has great potential in the broader IoT context. However, the sheer scale and the dynamic, federated nature of the IoT pose a number of significant research challenges.},
booktitle = {Proceedings of the 17th International Middleware Conference},
articleno = {13},
numpages = {15},
keywords = {audit, regulation, policy specification and enforcement, Law},
location = {Trento, Italy},
series = {Middleware '16}
}

@article{10.1145/3512944,
author = {Gupta, Srishti and Jablonski, Julia and Tsai, Chun-Hua and Carroll, John M.},
title = {Instagram of Rivers: Facilitating Distributed Collaboration in Hyperlocal Citizen Science},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512944},
doi = {10.1145/3512944},
abstract = {Citizen science project leaders collecting field data in a hyperlocal community often face common socio-technical challenges, which can potentially be addressed by sharing innovations across different groups through peer-to-peer collaboration. However, most citizen science groups practice in isolation, and end up re-inventing the wheel when it comes to addressing these common challenges. This study seeks to investigate distributed collaboration between different water monitoring citizen science groups. We discovered a unique social network application called Water Reporter that mediated distributed collaboration by creating more visibility and transparency between groups using the app. We interviewed 8 citizen science project leaders who were users of this app, and 6 other citizen science project leaders to understand how distributed collaboration mediated by this app differed from collaborative practices of Non Water Reporter users. We found that distributed collaboration was an important goal for both user groups, however, the tasks that support these collaboration activities differed for the two user groups.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {97},
numpages = {22},
keywords = {collaboratory, distributed collaboration, citizen science, sustainability}
}

@inbook{10.1145/3447404.3447417,
author = {McMenemy, David},
title = {Ethics and Smart Cities},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447417},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {235–236},
numpages = {2}
}

@inproceedings{10.5555/3103620.3103631,
author = {Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua},
title = {Validating a Deep Learning Framework by Metamorphic Testing},
year = {2017},
isbn = {9781538604243},
publisher = {IEEE Press},
abstract = {Deep learning has become an important tool for image classification and natural language processing. However, the effectiveness of deep learning is highly dependent on the quality of the training data as well as the net model for the learning. The training data set for deep learning normally is fairly large, and the net model is pretty complex. It is necessary to validate the deep learning framework including the net model, executing environment, and training data set before it is used for any applications. In this paper, we propose an approach for validating the classification accuracy of a deep learning framework that includes a convolutional neural network, a deep learning executing environment, and a massive image data set. The framework is first validated with a classifier built on support vector machine, and then it is tested using a metamorphic validation approach. The effectiveness of the approach is demonstrated by validating a deep learning classifier for automated classification of biology cell images. The proposed approach can be used for validating other deep learning framework for different applications.},
booktitle = {Proceedings of the 2nd International Workshop on Metamorphic Testing},
pages = {28–34},
numpages = {7},
keywords = {software validation, support vector machine, deep learning, metamorphic testing, neural network},
location = {Buenos Aires, Argentina},
series = {MET '17}
}

@inproceedings{10.1145/3442188.3445918,
author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445918},
doi = {10.1145/3442188.3445918},
abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {560–575},
numpages = {16},
keywords = {requirements engineering, datasets, machine learning},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inbook{10.1145/3447404.3447419,
author = {McMenemy, David},
title = {Ethical Issues in Brain–Computer Interfaces},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447419},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {273–275},
numpages = {3}
}

@inproceedings{10.5555/3242181.3242206,
author = {McGinnis, Leon F. and Rose, Oliver},
title = {History and Perspective of Simulation in Manufacturing},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Manufacturing systems incorporate many semi-independent, yet strongly interacting processes, usually exhibiting some stochastic behavior. As a consequence, overall system behavior, in the long run but also in the short run, is very difficult to predict. Not surprisingly, both practitioners and academics recognized in the 1950's the potential value of discrete event simulation technology in supporting manufacturing system decision-making. This short history is one perspective on the development and evolution of discrete event simulation technology and applications, specifically focusing on manufacturing applications. This assessment is based on an examination of the literature, our own experiences, and interviews with leading practitioners. History is interesting, but it's useful only if it helps us see a way forward, so we offer some opinions on the state of the research and practice of simulation in manufacturing, and the opportunities to further advance the field.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {24},
numpages = {13},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3371158.3371167,
author = {Bansal, Chahat and Jain, Arpit and Barwaria, Phaneesh and Choudhary, Anuj and Singh, Anupam and Gupta, Ayush and Seth, Aaditeshwar},
title = {Temporal Prediction of Socio-Economic Indicators Using Satellite Imagery},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371167},
doi = {10.1145/3371158.3371167},
abstract = {Machine learning models based on satellite data have been actively researched to serve as a proxy for the prediction of socio-economic development indicators. Such models have however rarely been tested for transferability over time, i.e. whether models learned on data for a certain year are able to make accurate predictions on data for another year. Using a dataset from the Indian census at two time points, for the years 2001 and 2011, we evaluate the temporal transferability of a simple machine learning model at sub-national scales of districts and propose a generic method to improve its performance. This method can be especially relevant when training datasets are small to train a robust prediction model. Then, we go further to build an aggregate development index at the district-level, on the lines of the Human Development Index (HDI) and demonstrate high accuracy in predicting the index based on satellite data for different years. This can be used to build applications to guide data-driven policy making at fine spatial and temporal scales, without the need to conduct frequent expensive censuses and surveys on the ground.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {73–81},
numpages = {9},
keywords = {socio-economic development, Landsat, temporal prediction, poverty mapping, census, satellite imagery},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@article{10.1145/2724730,
author = {Basole, Rahul C. and Russell, Martha G. and Huhtam\"{a}ki, Jukka and Rubens, Neil and Still, Kaisa and Park, Hyunwoo},
title = {Understanding Business Ecosystem Dynamics: A Data-Driven Approach},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2724730},
doi = {10.1145/2724730},
abstract = {Business ecosystems consist of a heterogeneous and continuously evolving set of entities that are interconnected through a complex, global network of relationships. However, there is no well-established methodology to study the dynamics of this network. Traditional approaches have primarily utilized a single source of data of relatively established firms; however, these approaches ignore the vast number of relevant activities that often occur at the individual and entrepreneurial levels. We argue that a data-driven visualization approach, using both institutionally and socially curated datasets, can provide important complementary, triangulated explanatory insights into the dynamics of interorganizational networks in general and business ecosystems in particular. We develop novel visualization layouts to help decision makers systemically identify and compare ecosystems. Using traditionally disconnected data sources on deals and alliance relationships (DARs), executive and funding relationships (EFRs), and public opinion and discourse (POD), we empirically illustrate our data-driven method of data triangulation and visualization techniques through three cases in the mobile industry Google’s acquisition of Motorola Mobility, the coopetitive relation between Apple and Samsung, and the strategic partnership between Nokia and Microsoft. The article concludes with implications and future research opportunities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jun},
articleno = {6},
numpages = {32},
keywords = {business ecosystem, interorganizational networks, information visualization, Data triangulation}
}

@inproceedings{10.1145/3529190.3534768,
author = {Appenzeller, Arno and Terzer, Nick and Krempel, Erik and Beyerer, J\"{u}rgen},
title = {Towards Private Medical Data Donations by Using Privacy Preserving Technologies},
year = {2022},
isbn = {9781450396318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529190.3534768},
doi = {10.1145/3529190.3534768},
abstract = {Through the growing amount of personal health data collected by the individual itself digital data donations become more and more attractive. Wearables like Apple Watch or Fitbit trackers make tracking of heart rate, daily step counts and other lifestyle data easier than ever. While this data is collected on the dedicated device, it can help research in many promising ways. Even if the potential benefit of this data is very clear, there are open questions regarding privacy. Traditional privatization measures like anonymization and pseudonymization can only provide limited privacy guarantees especially with the growing amount of personalized data. To mitigate those risks privacy enhancing technologies like differential privacy can be used. While the theoretical foundation of such technologies is strong, only limited data is available about their practical use in large scale applications and the trade-off between privacy and utility. In this paper we will present a data donation scenario that is inspired by a real-world use case using lifestyle data for its analyses. We will apply the local differential privacy technology ”RAPPOR” to improve the privacy protection for the data donors and evaluate the impact of this technique to the data utility.},
booktitle = {Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {446–454},
numpages = {9},
keywords = {Data donation, Privacy Enhancing Technology, Differential Privacy, Medical data protection},
location = {Corfu, Greece},
series = {PETRA '22}
}

@inproceedings{10.1145/2464464.2464475,
author = {Rieder, Bernhard},
title = {Studying Facebook via Data Extraction: The Netvizz Application},
year = {2013},
isbn = {9781450318891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464464.2464475},
doi = {10.1145/2464464.2464475},
abstract = {This paper describes Netvizz, a data collection and extraction application that allows researchers to export data in standard file formats from different sections of the Facebook social networking service. Friendship networks, groups, and pages can thus be analyzed quantitatively and qualitatively with regards to demographical, post-demographical, and relational characteristics. The paper provides an overview over analytical directions opened up by the data made available, discusses platform specific aspects of data extraction via the official Application Programming Interface, and briefly engages the difficult ethical considerations attached to this type of research.},
booktitle = {Proceedings of the 5th Annual ACM Web Science Conference},
pages = {346–355},
numpages = {10},
keywords = {research tool, media studies, data extraction, Facebook, social networking services, social network analysis},
location = {Paris, France},
series = {WebSci '13}
}

@inproceedings{10.1145/2213836.2213961,
author = {Morton, Kristi and Bunker, Ross and Mackinlay, Jock and Morton, Robert and Stolte, Chris},
title = {Dynamic Workload Driven Data Integration in Tableau},
year = {2012},
isbn = {9781450312479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2213836.2213961},
doi = {10.1145/2213836.2213961},
abstract = {Tableau is a commercial business intelligence (BI) software tool that supports interactive, visual analysis of data. Armed with a visual interface to data and a focus on usability, Tableau enables a wide audience of end-users to gain insight into their datasets. The user experience is a fluid process of interaction in which exploring and visualizing data takes just a few simple drag-and-drop operations (no programming or DB experience necessary). In this context of exploratory, ad-hoc visual analysis, we describe a novel approach to integrating large, heterogeneous data sources. We present a new feature in Tableau called data blending, which gives users the ability to create data visualization mashups from structured, heterogeneous data sources dynamically without any upfront integration effort. Users can author visualizations that automatically integrate data from a variety of sources, including data warehouses, data marts, text files, spreadsheets, and data cubes. Because our data blending system is workload driven, we are able to bypass many of the pain-points and uncertainty in creating mediated schemas and schema-mappings in current pay-as-you-go integration systems.},
booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
pages = {807–816},
numpages = {10},
keywords = {visualization, data integration},
location = {Scottsdale, Arizona, USA},
series = {SIGMOD '12}
}

@inproceedings{10.1145/2882903.2915232,
author = {Fan, Wenfei and Wu, Yinghui and Xu, Jingbo},
title = {Functional Dependencies for Graphs},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2915232},
doi = {10.1145/2882903.2915232},
abstract = {We propose a class of functional dependencies for graphs, referred to as GFDs. GFDs capture both attribute-value dependencies and topological structures of entities, and subsume conditional functional dependencies (CFDs) as a special case. We show that the satisfiability and implication problems for GFDs are coNP-complete and NP-complete, respectively, no worse than their CFD counterparts. We also show that the validation problem for GFDs is coNP-complete. Despite the intractability, we develop parallel scalable algorithms for catching violations of GFDs in large-scale graphs. Using real-life and synthetic data, we experimentally verify that GFDs provide an effective approach to detecting inconsistencies in knowledge and social graphs.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1843–1857},
numpages = {15},
keywords = {implication, validation, graphs, functional dependencies, satisfiability},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/2723372.2750549,
author = {Wang, Xiaolan and Dong, Xin Luna and Meliou, Alexandra},
title = {Data X-Ray: A Diagnostic Tool for Data Errors},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2750549},
doi = {10.1145/2723372.2750549},
abstract = {A lot of systems and applications are data-driven, and the correctness of their operation relies heavily on the correctness of their data. While existing data cleaning techniques can be quite effective at purging datasets of errors, they disregard the fact that a lot of errors are systematic, inherent to the process that produces the data, and thus will keep occurring unless the problem is corrected at its source. In contrast to traditional data cleaning, in this paper we focus on data diagnosis: explaining where and how the errors happen in a data generative process.We develop a large-scale diagnostic framework called DATA X-RAY. Our contributions are three-fold. First, we transform the diagnosis problem to the problem of finding common properties among erroneous elements, with minimal domain-specific assumptions. Second, we use Bayesian analysis to derive a cost model that implements three intuitive principles of good diagnoses. Third, we design an efficient, highly-parallelizable algorithm for performing data diagnosis on large-scale data. We evaluate our cost model and algorithm using both real-world and synthetic data, and show that our diagnostic framework produces better diagnoses and is orders of magnitude more efficient than existing techniques.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1231–1245},
numpages = {15},
keywords = {error diagnosis, data profiling, data cleaning},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@article{10.14778/3554821.3554825,
author = {Das, Prakash and Srivastava, Shivangi and Moskovich, Valentin and Chaturvedi, Anmol and Mittal, Anant and Xiao, Yongqin and Chowdhury, Mosharaf},
title = {CDI-E: An Elastic Cloud Service for Data Engineering},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554825},
doi = {10.14778/3554821.3554825},
abstract = {We live in the gilded age of data-driven computing. With public clouds offering virtually unlimited amounts of compute and storage, enterprises collecting data about every aspect of their businesses, and advances in analytics and machine learning technologies, data driven decision making is now timely, cost-effective, and therefore, pervasive. Alas, only a handful of power users can wield today's powerful data engineering tools. For one thing, most solutions require knowledge of specific programming interfaces or libraries. Furthermore, running them requires complex configurations and knowledge of the underlying cloud for cost-effectiveness.We decided that a fundamental redesign is in order to democratize data engineering for the masses at cloud scale. The result is Informatica Cloud Data Integration - Elastic (CDI-E). Since the early 1990s, Informatica has been a pioneer and industry leader in building no-code data engineering tools. Non-experts can express complex data engineering tasks using a graphical user interface (GUI). Informatica CDI-E is built to incorporate the simplicity of GUI in the design layer with an elastic and highly scalable run time to handle data in any format without little to no user input using automated optimizations. Users upload their data to the cloud in any format and can immediately use them in conjunction with their data management and analytic tools of choice using CDI-E GUI. Implementation began in the Spring of 2017, and Informatica CDI-E has been generally available since the Summer of 2019. Today, CDI-E is used in production by a growing number of small and large enterprises to make sense of data in arbitrary formats.In this paper, we describe the architecture of Informatica CDI-E and its novel no-code data engineering interface. The paper highlights some of the key features of CDI-E: simplicity without loss in productivity and extreme elasticity. It concludes with lessons we learned and an outlook of the future.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3319–3331},
numpages = {13}
}

@article{10.1145/3379504,
author = {Wu, Jian and Sheng, Victor S. and Zhang, Jing and Li, Hua and Dadakova, Tetiana and Swisher, Christine Leon and Cui, Zhiming and Zhao, Pengpeng},
title = {Multi-Label Active Learning Algorithms for Image Classification: Overview and Future Promise},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379504},
doi = {10.1145/3379504},
abstract = {Image classification is a key task in image understanding, and multi-label image classification has become a popular topic in recent years. However, the success of multi-label image classification is closely related to the way of constructing a training set. As active learning aims to construct an effective training set through iteratively selecting the most informative examples to query labels from annotators, it was introduced into multi-label image classification. Accordingly, multi-label active learning is becoming an important research direction. In this work, we first review existing multi-label active learning algorithms for image classification. These algorithms can be categorized into two top groups from two aspects respectively: sampling and annotation. The most important component of multi-label active learning is to design an effective sampling strategy that actively selects the examples with the highest informativeness from an unlabeled data pool, according to various information measures. Thus, different informativeness measures are emphasized in this survey. Furthermore, this work also makes a deep investigation on existing challenging issues and future promises in multi-label active learning with a focus on four core aspects: example dimension, label dimension, annotation, and application extension.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {28},
numpages = {35},
keywords = {multi-label image, Image classification, active learning, annotation, sampling strategy}
}

@article{10.1109/TCBB.2015.2474376,
author = {Lee, En-Shiun Annie and Sze-To, Ho-Yin Antonio and Wong, Man-Hon and Leung, Kwong-Sak and Lau, Terrence Chi-Kong and Wong, Andrew K. C.},
title = {Discovering Protein-DNA Binding Cores by Aligned Pattern Clustering},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2474376},
doi = {10.1109/TCBB.2015.2474376},
abstract = {Understanding binding cores is of fundamental importance in deciphering Protein-DNA TF-TFBS binding and gene regulation. Limited by expensive experiments, it is promising to discover them with variations directly from sequence data. Although existing computational methods have produced satisfactory results, they are one-to-one mappings with no site-specific information on residue/nucleotide variations, where these variations in binding cores may impact binding specificity. This study presents a new representation for modeling binding cores by incorporating variations and an algorithm to discover them from only sequence data. Our algorithm takes protein and DNA sequences from TRANSFAC a Protein-DNA Binding Database as input; discovers from both sets of sequences conserved regions in Aligned Pattern Clusters APCs; associates them as Protein-DNA Co-Occurring APCs; ranks the Protein-DNA Co-Occurring APCs according to their co-occurrence, and among the top ones, finds three-dimensional structures to support each binding core candidate. If successful, candidates are verified as binding cores. Otherwise, homology modeling is applied to their close matches in PDB to attain new chemically feasible binding cores. Our algorithm obtains binding cores with higher precision and much faster runtime (≥1,600x) than that of its contemporaries, discovering candidates that do not co-occur as one-to-one associated patterns in the raw data. Availability: http://www.pami.uwaterloo.ca/~ealee/files/tcbbPnDna2015/Release.zip.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {254–263},
numpages = {10}
}

@inproceedings{10.1145/3506860.3506939,
author = {Tsai, Yi-Shan and Singh, Shaveen and Rakovic, Mladen and Lim, Lisa-Angelique and Roychoudhury, Anushka and Gasevic, Dragan},
title = {Charting Design Needs and Strategic Approaches for Academic Analytics Systems through Co-Design},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506939},
doi = {10.1145/3506860.3506939},
abstract = {Academic analytics focuses on collecting, analysing and visualising educational data to generate institutional insights and improve decision-making for academic purposes. However, challenges that arise from navigating a complex organisational structure when introducing analytics systems have called for the need to engage key stakeholders widely to cultivate a shared vision and ensure that implemented systems create desired value. This paper presents a study that takes co-design steps to identify design needs and strategic approaches for the adoption of academic analytics, which serves the purpose of enhancing the measurement of educational quality utilising institutional data. Through semi-structured interviews with 54 educational stakeholders at a large research university, we identified particular interest in measuring student engagement and the performance of courses and programmes. Based on the observed perceptions and concerns regarding data use to measure or evaluate these areas, implications for adoption strategy of academic analytics, such as leadership involvement, communication, and training, are discussed.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {381–391},
numpages = {11},
keywords = {higher education, co-design, implementation strategy, educational quality, academic analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3512290.3528801,
author = {Angrick, Sebastian and Bals, Ben and Hastrich, Niko and Kleissl, Maximilian and Schmidt, Jonas and Dosko\v{c}, Vanja and Molitor, Louise and Friedrich, Tobias and Katzmann, Maximilian},
title = {Towards Explainable Real Estate Valuation via Evolutionary Algorithms},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528801},
doi = {10.1145/3512290.3528801},
abstract = {Human lives are increasingly influenced by algorithms, which therefore need to meet higher standards not only in accuracy but also with respect to explainability. This is especially true for high-stakes areas such as real estate valuation. Unfortunately, the methods applied there often exhibit a trade-off between accuracy and explainability.One explainable approach is case-based reasoning (CBR), where each decision is supported by specific previous cases. However, such methods can be wanting in accuracy. The unexplainable machine learning approaches are often observed to provide higher accuracy but are not scrutable in their decision-making.In this paper, we apply evolutionary algorithms (EAs) to CBR predictors in order to improve their performance. In particular, we deploy EAs to the similarity functions (used in CBR to find comparable cases), which are fitted to the data set at hand. As a consequence, we achieve higher accuracy than state-of-the-art deep neural networks (DNNs), while keeping interpretability and explainability.These results stem from our empirical evaluation on a large data set of real estate offers where we compare known similarity functions, their EA-improved counterparts, and DNNs. Surprisingly, DNNs are only on par with standard CBR techniques. However, using EA-learned similarity functions does yield an improved performance.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1130–1138},
numpages = {9},
keywords = {real estate valuation, evolutionary algorithms, case-based reasoning, neural networks},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

