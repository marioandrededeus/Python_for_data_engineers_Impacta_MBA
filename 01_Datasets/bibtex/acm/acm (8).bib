@inproceedings{10.1145/2884781.2884783,
author = {Kim, Miryung and Zimmermann, Thomas and DeLine, Robert and Begel, Andrew},
title = {The Emerging Role of Data Scientists on Software Development Teams},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884783},
doi = {10.1145/2884781.2884783},
abstract = {Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {96–107},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1145/2883611,
author = {Cao, Tien-Dung and Pham, Tran-Vu and Vu, Quang-Hieu and Truong, Hong-Linh and Le, Duc-Hung and Dustdar, Schahram},
title = {MARSA: A Marketplace for Realtime Human Sensing Data},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/2883611},
doi = {10.1145/2883611},
abstract = {This article introduces a dynamic cloud-based marketplace of near-realtime human sensing data (MARSA) for different stakeholders to sell and buy near-realtime data. MARSA is designed for environments where information technology (IT) infrastructures are not well developed but the need to gather and sell near-realtime data is great. To this end, we present techniques for selecting data types and managing data contracts based on different cost models, quality of data, and data rights. We design our MARSA platform by leveraging different data transferring solutions to enable an open and scalable communication mechanism between sellers (data providers) and buyers (data consumers). To evaluate MARSA, we carry out several experiments with the near-realtime transportation data provided by people in Ho Chi Minh City, Vietnam, and simulated scenarios in multicloud environments.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {16},
numpages = {21},
keywords = {Internet of Things, cost model, platform, data contract}
}

@inproceedings{10.1145/3041021.3055510,
author = {Lehmann, Jens and Auer, S\"{o}ren and Capadisli, Sarven and Janowicz, Krzysztof and Bizer, Christian and Heath, Tom and Hogan, Aidan and Berners-Lee, Tim},
title = {LDOW2017: 10th Workshop on Linked Data on the Web},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3055510},
doi = {10.1145/3041021.3055510},
abstract = {The 10th Linked Data on the Web workshop (LDOW2017) was held in Perth, Western Australia on April 3, 2017, co-located with the 26th International World Wide Web Conference (WWW2017). In its 10th anniversary edition, the LDOW workshop aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data on the Web as well as mining knowledge from said data.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1679–1680},
numpages = {2},
keywords = {linked data, semantic web},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.1145/2380776.2380789,
author = {Maz\'{o}n, Jose-Norberto and Garrig\'{o}s, Irene and Daniel, Florian and Castellanos, Malu},
title = {Report of the International Workshop on Business Intelligence and the Web: BEWEB 2011},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2380776.2380789},
doi = {10.1145/2380776.2380789},
abstract = {The 2nd International Workshop on Business intelligencE and the WEB (BEWEB) was co-located with the EDBT/ICDT 2011 Joint Conference in Uppsala (Sweden) on March 25, 2011. BEWEB intends to be an international forum for researchers and practitioners to exchange ideas on how to leverage the huge amount of data that is available on the Web in BI applications and on how to apply Web engineering methods and techniques to the design of BI applications. This report summarizes the 2011 edition of BEWEB.},
journal = {SIGMOD Rec.},
month = {oct},
pages = {51–53},
numpages = {3}
}

@inproceedings{10.1145/2912160.2912180,
author = {Netten, Niels and Bargh, Mortaza S. and van den Braak, Susan and Choenni, Sunil and Leeuw, Frans},
title = {On Enabling Smart Government: A Legal Logistics Framework for Future Criminal Justice Systems},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912180},
doi = {10.1145/2912160.2912180},
abstract = {While in business and private settings the disruptive impact of advanced information communication technology (ICT) have already been felt, the legal sector is now starting to face great disruptions due to such ICTs. Bits and pieces of innovations in the legal sector have been emerging for some time, affecting the performance of core functions and the legitimacy of public institutions.In this paper, we present our framework for enabling the smart government vision, particularly for the case of criminal justice systems, by unifying different isolated ICT-based solutions. Our framework, coined as Legal Logistics, supports the well-functioning of a legal system in order to streamline the innovations in these legal systems. The framework targets the exploitation of all relevant data generated by the ICT-based solutions. As will be illustrated for the Dutch criminal justice system, the framework may be used to integrate different ICT-based innovations and to gain insights about the well-functioning of the system. Furthermore, Legal Logistics can be regarded as a roadmap towards a smart and open justice.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {293–302},
numpages = {10},
keywords = {legal design, smart governance, efficiency, effectivity, open justice, Law enforcement, and penal law},
location = {Shanghai, China},
series = {dg.o '16}
}

@inproceedings{10.1145/3450508.3464558,
author = {Bednarz, Tomasz and Hughes, Rowan T. and Mathews, Alex and Chen, Dawei and Zhu, Liming and Filonik, Daniel},
title = {Visual Analytics for Large Networks: Theory, Art and Practice},
year = {2021},
isbn = {9781450383615},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450508.3464558},
doi = {10.1145/3450508.3464558},
booktitle = {ACM SIGGRAPH 2021 Courses},
articleno = {15},
numpages = {213},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3447548.3470825,
author = {Zalmout, Nasser and Zhang, Chenwei and Li, Xian and Liang, Yan and Dong, Xin Luna},
title = {All You Need to Know to Build a Product Knowledge Graph},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470825},
doi = {10.1145/3447548.3470825},
abstract = {Knowledge graphs have been pivotal in supporting downstream applications like search, recommendation, and question answering, among others. Therefore, knowledge graphs have naturally become key enabling technologies in e-Commerce platforms. Developing a high coverage product knowledge graph is more challenging than generic knowledge graphs. The highly specific and complex domain, the sparsity of training data, along with the dynamic taxonomies and product types, can constrain the resulting knowledge graphs. In this tutorial we present best practices and ML innovations in industry towards building a scalable product knowledge graph. Contributions in this domain benefit from the general literature in areas including information extraction and data mining, tailored to address the specific characteristics of e-Commerce platforms.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4090–4091},
numpages = {2},
keywords = {data cleaning, taxonomy, information extraction, knowledge graphs},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1145/3403976,
author = {Hoffmann, Leah},
title = {Seeing Light at the End of the Cybersecurity Tunnel},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3403976},
doi = {10.1145/3403976},
abstract = {After decades of cybersecurity research, Elisa Bertino remains optimistic.},
journal = {Commun. ACM},
month = {jul},
pages = {104–ff},
numpages = {2}
}

@inproceedings{10.1145/3085228.3085255,
author = {Parycek, P. and Pereira, G. Viale},
title = {Drivers of Smart Governance: Towards to Evidence-Based Policy-Making},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085255},
doi = {10.1145/3085228.3085255},
abstract = {This paper presents the preliminary framework proposed by the authors for drivers of Smart Governance. The research question of this study is: What are the drivers for Smart Governance to achieve evidence-based policy-making? The framework suggests that in order to create a smart governance model, data governance and collaborative governance are the main drivers. These pillars are supported by legal framework, normative factors, principles and values, methods, data assets or human resources, and IT infrastructure. These aspects will guide a real time evaluation process in all levels of the policy cycle, towards to the implementation of evidence-based policies.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {564–565},
numpages = {2},
keywords = {Data Governance, Collaborative Governance, Decision-making, Evaluation},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@inproceedings{10.1145/3366625.3369437,
author = {Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad},
title = {An In-Depth Look of BFT Consensus in Blockchain: Challenges and Opportunities},
year = {2019},
isbn = {9781450370400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366625.3369437},
doi = {10.1145/3366625.3369437},
abstract = {Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.},
booktitle = {Proceedings of the 20th International Middleware Conference Tutorials},
pages = {6–10},
numpages = {5},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/3029387.3029421,
author = {Anjum, Shahid W.},
title = {Risk Magnification Framework for Clouds Computing Architects in Business Intelligence},
year = {2017},
isbn = {9781450348034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3029387.3029421},
doi = {10.1145/3029387.3029421},
abstract = {IT infrastructure and applications in enterprise systems started with traditional client-server architecture and have gone through key paradigm shifts in infrastructure, software, enterprise, and service architectures to current age of cloud computing and internet of everything. Using strengths-weaknesses-opportunities-threats and analytical hierarchy process of multi-criteria decision making frameworks together, various aspects of cloud computing characteristics related to opportunities, benefits, costs, value and risks can be understood in a more detailed way and can be ranked. This article has combined these two frameworks for the ranking of various business intelligence architects for cloud computing by using 'business automation with sustainable hedging for information risks' framework for cloud computing from conservative perspective where risk relevancy attracts the prime focus. The results have shown that moving operational business intelligence is the best business intelligence architecture for cloud computing as its strengths are more than inherent risks as has become evident by using this approach.},
booktitle = {Proceedings of the 5th International Conference on Information and Education Technology},
pages = {140–144},
numpages = {5},
keywords = {Multi Criteria Decision Making, IT Risk Management, Cloud Computing, Business Intelligence Architecture, A'WOT Analysis},
location = {Tokyo, Japan},
series = {ICIET '17}
}

@article{10.5555/3344081.3344085,
author = {Li, Xinming and Talburt, John R. and Li, Ting and Liu, Xiangwen},
title = {Scoring Matrix Combined with Machine Learning for Heterogeneously Structured Entity Resolution},
year = {2019},
issue_date = {April 2019},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {7},
issn = {1937-4771},
abstract = {This paper describes how machine learning works with "coring matrix", which is designed for measuring the similarity between heterogeneously structured references, to get a better performance in Entity Resolution (ER). In the scoring matrix, each entity reference is tokenized and all pairs of tokens between the references are scored by a similarity scoring function such as the Levenshtein edit distance. In so doing, a similarity score vector can measure the similarity between references. With the similarity score vector, machine learning is used to make the linking decision. Our experiments show that machine learning based on score vector outperforms TF-IDF and FuzzyWuzzy benchmarks. One possible explanation is that a similarity score vector conveys much more information than a single similarity score. Random forest and neural network even get better performance with raw score vector input than with the statistic characteristic input.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {38–45},
numpages = {8}
}

@inproceedings{10.1145/3503928.3503944,
author = {Wu, Ji and Zhou, Ming and Xu, Min and Zhang, Jin and Wu, Yue and Zha, Weiwei and Zhang, Chengping},
title = {Design and Research of IoT Management Architecture for Power Grid Enterprises Based on Digital Transformation: Application of IoT in Power Grid Enterprises According to Enterprise Architecture Method},
year = {2021},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503944},
doi = {10.1145/3503928.3503944},
abstract = {Internet of things technology, as the core technology in digital transformation, helps enterprises in digital transformation to carry out comprehensive perception, intelligent management and secure transmission. Following the information architecture of State Grid Corporation of China and combined with the business objectives and Strategies of electric power company, carry out the differentiated design of power Internet of things architecture, put forward the improvement direction of power Internet of things architecture, clarify the application scenario and future evolution route of power Internet of things business, and ensure the realization of technology.},
booktitle = {2021 the 6th International Conference on Information Systems Engineering},
pages = {84–88},
numpages = {5},
keywords = {Digital transformation, IoT, Intelligent IoT management system},
location = {Shanghai, China},
series = {ICISE 2021}
}

@article{10.1145/2536669.2536682,
author = {Zhou, Xiaofang and Sadiq, Shazia},
title = {Data Centric Research at the University of Queensland},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2536669.2536682},
doi = {10.1145/2536669.2536682},
journal = {SIGMOD Rec.},
month = {oct},
pages = {63–68},
numpages = {6}
}

@inproceedings{10.1145/3428690.3429172,
author = {Khadivizand, Sam and Beheshti, Amin and Sobhanmanesh, Fariborz and Sheng, Quan Z. and Istanbouli, Elias and Wood, Steven and Pezaro, Damon},
title = {Towards Intelligent Feature Engineering for Risk-Based Customer Segmentation in Banking},
year = {2020},
isbn = {9781450389242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428690.3429172},
doi = {10.1145/3428690.3429172},
abstract = {Business Processes, i.e., a set of coordinated tasks and activities to achieve a business goal, and their continuous improvements are key to the operation of any organization. In banking, business processes are increasingly dynamic as various technologies have made dynamic processes more prevalent. For example, customer segmentation, i.e., the process of grouping related customers based on common activities and behaviors, could be a data-driven and knowledge-intensive process. In this paper, we present an intelligent data-driven pipeline composed of a set of processing elements to move customers' data from one system to another, transforming the data into the contextualized data and knowledge along the way. The goal is to present a novel intelligent customer segmentation process which automates the feature engineering, i.e., the process of using (banking) domain knowledge to extract features from raw data via data mining techniques, in the banking domain. We adopt a typical scenario for analyzing customer transaction records, to highlight how the presented approach can significantly improve the quality of risk-based customer segmentation in the absence of feature engineering.},
booktitle = {Proceedings of the 18th International Conference on Advances in Mobile Computing &amp; Multimedia},
pages = {74–83},
numpages = {10},
keywords = {feature engineering, business process, banking processes, risk-based customer segmentation},
location = {Chiang Mai, Thailand},
series = {MoMM '20}
}

@inproceedings{10.1145/3503823.3503900,
author = {Stakoulas, Konstantinos and Georgiou, Konstantinos and Mittas, Nikolaos and Angelis, Lefteris},
title = {An Analysis of User Profiles from Covid-19 Questions in Stack Overflow},
year = {2021},
isbn = {9781450395557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503823.3503900},
doi = {10.1145/3503823.3503900},
abstract = {The COVID-19 pandemic brought many changes in society, with one of the most important being an explosion of software development concerning technological solutions for combatting its crippling effects. In this global crisis, many software enthusiasts, combined with seasoned developers and specialists turned their attention to Questions and Answers platforms such as Stack Overflow to expand their knowledge and ask questions regarding their COVID-19 related solutions. This paper examines the different characteristics of these users, dividing them into Newcomers and Oldcomers and pinpoints popularity differences, scientific and technological backgrounds by analyzing key technologies, as well as the role of gender in their participation.},
booktitle = {25th Pan-Hellenic Conference on Informatics},
pages = {419–424},
numpages = {6},
location = {Volos, Greece},
series = {PCI 2021}
}

@inproceedings{10.1145/3469213.3470424,
author = {Yan, Feng},
title = {A Building Integrated Control Platform Oriented Towards Intelligent Building},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470424},
doi = {10.1145/3469213.3470424},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {217},
numpages = {10},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.5555/2873003.2873011,
author = {Barhak, Jacob},
title = {Modeling Clinical Data from Publications},
year = {2015},
isbn = {9781510801028},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Medical data is becoming increasingly available. Access to such data is generally restricted and researchers cannot access it easily. On the other hand, clinical trial data is freely available and published without restriction for access to the public at the summary level. With proper analysis, it is possible to extract valuable conclusions from such data. This paper will review new methods to look at such public data and will discuss possible future trends.},
booktitle = {Proceedings of the Symposium on Modeling and Simulation in Medicine},
pages = {47–52},
numpages = {6},
keywords = {reference modeling, clinical trial, high performance computing, publications, Monte-Carlo, disease modeling},
location = {Alexandria, Virginia},
series = {MSM '15}
}

@inproceedings{10.1145/3401025.3404099,
author = {Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad},
title = {Blockchain Consensus Unraveled: Virtues and Limitations},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3404099},
doi = {10.1145/3401025.3404099},
abstract = {Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {218–221},
numpages = {4},
keywords = {byzantine learning, cluster-sending, resilient transaction processing, permissioned blockchains, consensus, geo-scale, sharding},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3267305.3267694,
author = {Han, Yang and Li, Victor O.K. and Lam, Jacqueline C.K. and Lu, Zhiyi},
title = {UMeAir: Predicting Momentary Happiness Towards Air Quality via Machine Learning},
year = {2018},
isbn = {9781450359665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267305.3267694},
doi = {10.1145/3267305.3267694},
abstract = {Subjective well-being (SWB) refers to people's subjective evaluation of their own quality of life. Previous studies show that environmental pollution, such as air pollution, has generated significant negative impacts on one's SWB. However, such works are often constrained by the lack of appropriate representation of SWB specifically related to air quality. In this study, we develop UMeAir, which collects one's real-time SWB, specifically, one's momentary happiness at a given air quality, pre-processes input data and detects outliers via Isolation Forests, trains and selects the best model via Support Vector Machine and Random Forests, and predicts the momentary happiness towards any air quality one experienced. Unlike traditional representation of air quality by pollution concentration/Air Pollution Index, UMeAir intends to represent air quality in a more user-comprehensible way, by connecting the air quality experienced at a particular time and location with the corresponding momentary happiness perceived towards the air. The higher the momentary happiness, the better the air quality one experienced. Our work is the first attempt to predict momentary happiness towards air quality in real-time, with the development of the-first-of-its-kind UMeAir Happiness Index (HAPI) towards air quality via machine learning.},
booktitle = {Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
pages = {702–705},
numpages = {4},
keywords = {Subjective well-being prediction, Short-term happiness, Data interpretability, Machine learning, Air quality},
location = {Singapore, Singapore},
series = {UbiComp '18}
}

@article{10.1145/3368091,
author = {Douglas, David M.},
title = {Should Researchers Use Data from Security Breaches?},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3368091},
doi = {10.1145/3368091},
abstract = {Evaluating the arguments for and against using digital data derived from security breaches.},
journal = {Commun. ACM},
month = {nov},
pages = {22–24},
numpages = {3}
}

@inbook{10.1145/3310205.3310207,
title = {Introduction},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310207},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1109/ICSE-Companion.2019.00023,
author = {Dang, Yingnong and Lin, Qingwei and Huang, Peng},
title = {AIOps: Real-World Challenges and Research Innovations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00023},
doi = {10.1109/ICSE-Companion.2019.00023},
abstract = {AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and applications at scale with artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help improve service quality and customer satisfaction, boost engineering productivity, and reduce operational cost. In this technical briefing, we first summarize the real-world challenges in building AIOps solutions based on our practice and experience in Microsoft. We then propose a roadmap of AIOps related research directions, and share a few successful AIOps solutions we have built for Microsoft service products.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {4–5},
numpages = {2},
keywords = {DevOps, software analytics, AIOps},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3543434.3543445,
author = {Valle-Cruz, David and Garc\'{\i}a-Contreras, Rigoberto and Mu\~{n}oz-Ch\'{a}vez, J. Patricia},
title = {Mind the Gap: Towards an Understanding of Government Decision-Making Based on Artificial Intelligence},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543445},
doi = {10.1145/3543434.3543445},
abstract = {Decision-making has become more critical for organizations in the 21st century. The citizens’ countless needs and the emerging problems (internal and external) faced by governments increase the complexity of government decisions worldwide. The research question guiding this attempt is: How is government decision-making grounded on artificial intelligence (AI)? Based on the PRISMA approach and empirical analysis of some international cases are adopted. The authors analyze different organizational and environmental factors, the objectives, benefits, and risks of AI-supported decision-making. The findings show an increasing interest in the research on government decision-making based on AI. Finally, there is the potential of AI to support decision-making for the benefit of citizens and public value generation, collaboratively between governments, industry, and society. Future work will further analyze AI-based decision-making in government in depth.},
booktitle = {DG.O 2022: The 23rd Annual International Conference on Digital Government Research},
pages = {226–234},
numpages = {9},
keywords = {Organizational and environmental, Public Sector, Artificial Intelligence, Benefits, Goals, Risks, Government Decision-Making},
location = {Virtual Event, Republic of Korea},
series = {dg.o 2022}
}

@article{10.1145/2435221.2435222,
author = {Talburt, John R.},
title = {SPECIAL ISSUE ON ENTITY RESOLUTION Overview: The Criticality of Entity Resolution in Data and Information Quality},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2435221.2435222},
doi = {10.1145/2435221.2435222},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {6},
numpages = {2}
}

@article{10.1145/3335150,
author = {Hastings, Justine S. and Howison, Mark and Lawless, Ted and Ucles, John and White, Preston},
title = {Unlocking Data to Improve Public Policy},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3335150},
doi = {10.1145/3335150},
abstract = {When properly secured, anonymized, and optimized for research, administrative data can be put to work to help government programs better serve those in need.},
journal = {Commun. ACM},
month = {sep},
pages = {48–53},
numpages = {6}
}

@inproceedings{10.1145/3080546.3080550,
author = {Frey, Remo Manuel and Hardjono, Thomas and Smith, Christian and Erhardt, Keeley and Pentland, Alex 'Sandy'},
title = {Secure Sharing of Geospatial Wildlife Data},
year = {2017},
isbn = {9781450350471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3080546.3080550},
doi = {10.1145/3080546.3080550},
abstract = {Modern tracking technologies enables new ways for data mining in the wild. It allows wildlife monitoring centers to permanently collect geospatial data in a non-intrusive manner in real-time and at low cost. Unfortunately, wildlife data is exposed to crime and there is already a first reported case of 'cyber-poaching'. Based on stolen geospatial data, poachers can easily track and kill animals. As a result, cautious monitoring centers limited data access for research and public use. This means that the data cannot fully exploit its potential. We propose a novel solution to overcome the security problem. It allows monitoring centers to securely answer questions from the research community and to provide aggregated data to the public while the raw data is protected against unauthorized third parties. This data service can also be monetized. Several new applications are conceivable, such as a mobile app for preventing conflicts between human and wildlife or for engaging people in wildlife donation. Besides presenting the solution and potential use cases, the intention of present article is to start a discussion about the need for data protection and privacy in the animal world.},
booktitle = {Proceedings of the Fourth International ACM Workshop on Managing and Mining Enriched Geo-Spatial Data},
articleno = {5},
numpages = {6},
keywords = {geospatial, data sharing, species protection, animal, crime, privacy, blockchain, cyber-poaching, security, wildlife, GPS, hunting},
location = {Chicago, Illinois},
series = {GeoRich '17}
}

@article{10.1145/3377391.3377398,
author = {Winslett, Marianne and Braganholo, Vanessa},
title = {Michael Franklin Speaks Out on Data Science},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3377391.3377398},
doi = {10.1145/3377391.3377398},
abstract = {Welcome to ACM SIGMOD Record series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we're at the 2017 SIGMOD and PODS conference in Chicago. I have here with me Mike Franklin, who is the chair of the Computer Science department at the University of Chicago. Before that, for many years, Mike was a professor at Berkeley where he also served as a chair of the Computer Science division. Mike was a co-founder and director of the Algorithms, Machines, and People Lab, better known as the AMPLab. He is an ACM fellow, a two-time winner of the SIGMOD Ten Year Test of Time Award, and a founder of the successful startup, Truviso. Mike's Ph.D. is from the University of Wisconsin Madison. So, Mike, welcome!},
journal = {SIGMOD Rec.},
month = {dec},
pages = {29–35},
numpages = {7}
}

@article{10.1145/2805789.2805795,
author = {Calyam, Prasad and Swany, Martin},
title = {Research Challenges in Future Multi-Domain Network Performance Measurement and Monitoring},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2805789.2805795},
doi = {10.1145/2805789.2805795},
abstract = {The perfSONAR-based Multi-domain Network Performance Measurement and Monitoring Workshop was held on February 20-21, 2014 in Arlington, VA. The goal of the workshop was to review the state of the perfSONAR effort and catalyze future directions by cross-fertilizing ideas, and distilling common themes among the diverse perfSONAR stakeholders that include: network operators and managers, end-users and network researchers. The timing and organization for the second workshop is significant because there are an increasing number of groups within NSF supported data-intensive computing and networking programs that are dealing with measurement, monitoring and troubleshooting of multi-domain issues. These groups are forming explicit measurement federations using perfSONAR to address a wide range of issues. In addition, the emergence and wide-adoption of new paradigms such as software-defined networking are taking shape to aid in traffic management needs of scientific communities and network operators. Consequently, there are new challenges that need to be addressed for extensible and programmable instrumentation, measurement data analysis, visualization and middleware security features in perfSONAR. This report summarizes the workshop efforts to bring together diverse groups for delivering targeted short/long talks, sharing latest advances, and identifying gaps that exist in the community for solving end-to-end performance problems in an effective, scalable fashion.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
pages = {29–34},
numpages = {6},
keywords = {research challenges, future multi-domain network monitoring, next-generation measurement infrastructures}
}

@inproceedings{10.1145/3494193.3494250,
author = {Schuch de Azambuja, Luiza},
title = {Drivers and Barriers for the Development of Smart Sustainable Cities: A Systematic Literature Review},
year = {2021},
isbn = {9781450390118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494193.3494250},
doi = {10.1145/3494193.3494250},
abstract = {The term Smart Sustainable City (SSC) has been gaining popularity due to the growth of initiatives to address urban problems towards sustainable development. SSC can be considered as a combination of sustainable city and smart city, and some variance between the concepts may be expected. As this is a modern term, the literature falls short of studies presenting factors that hinder and/or facilitate the complex phenomenon of SSC development. Therefore, this paper aims to analyse scientific studies to identify aspects that influence the progress of smart sustainable cities. The methodological approach undertaken was a systematic literature review that included 169 papers. The results offer a comprehensive list of 57 drivers and 63 barriers, classified according to five main dimensions of a smart sustainable city, which are the three sustainability pillars (society, environment, and economy), combined to governance, and urban infrastructure. The findings revealed ‘governance’ as the most significant domain for SSC development, and multistakeholder engagement as one of the main challenges. This study shows that SSC is not a research field itself, but an interdisciplinary concept, contributing to academics, government, and policymakers for eradicating potential interferences in the development of smart and sustainable cities.},
booktitle = {14th International Conference on Theory and Practice of Electronic Governance},
pages = {422–428},
numpages = {7},
keywords = {enablers, sustainable city, challenges, smart city},
location = {Athens, Greece},
series = {ICEGOV 2021}
}

@inproceedings{10.1145/3132300.3132305,
author = {Yatim, Ir. Fazilah Mat and Majid, Zulkepli and Amerudin, Shahabuddin},
title = {Locating Success Within A Geographic Information System},
year = {2017},
isbn = {9781450352895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132300.3132305},
doi = {10.1145/3132300.3132305},
abstract = {Tenaga Nasional Berhad (TNB) being one of the largest utilities in the Southeast Asia has embarked on enriching their GIS solutions suite for its business operations. A distribution station was chosen as a pilot project to run the business processes using GIS. The successful implementation is to be measured through its impact on the station day-today operations. Key success factors (KSF) were defined and will be measured with reference to component of GIS. The outcome of the measurement will guide the implementation of GIS nation-wide in TNB Distribution, Malaysia. This paper is aimed at providing insights for utilities who are keen in identifying those success factors and methodology of measuring the success of the GIS implementation.},
booktitle = {Proceedings of the International Conference on Imaging, Signal Processing and Communication},
pages = {138–142},
numpages = {5},
keywords = {TNB-Tenaga Nasional Berhad, GIS-Geographic Information System, KSF-Key Success Factors},
location = {Penang, Malaysia},
series = {ICISPC 2017}
}

@article{10.1145/3529098,
author = {Chen, Rongli and Chen, Xiaozhong and Wang, Lei and Li, Jianxin},
title = {The Core Industry Manufacturing Process of Electronics Assembly Based on Smart Manufacturing},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2158-656X},
url = {https://doi.org/10.1145/3529098},
doi = {10.1145/3529098},
abstract = {This research takes a case study approach to show the development of a diverse adoption and product strategy distinct from the core manufacturing industry process. It explains the development status in all aspects of smart manufacturing, via the example of ceramic circuit board manufacturing and electronic assembly, and outlines future smart manufacturing plans and processes. This research proposed two experiments using Artificial Intelligence and deep learning are used to demonstrate the problems and solutions regarding methods in manufacturing and factory facilities, respectively. In the first experiment, a Bayesian network inference is used to find the cause of the problem of metal residues between electronic circuits through key process and quality correlations. In the second experiment, a Convolutional Neural Network (CNN) is used to identify false defects that were over-inspected during Automatic Optical Inspection. This improves the manufacturing process by enhancing yield rate and reducing cost. The contributions of the study in circuit board production. Smart manufacturing, with the application of a Bayesian network to an IoT setup, has addressed the problem of residue and redundant conductors on the edge of the ceramic circuit board pattern, and has improved and prevented leakage and high-frequency interference. CNN and deep learning were used to improve the accuracy of the AOI system, reduce the current manual review ratio, save labour costs and provide defect classification as a reference for pre-process improvement.},
note = {Just Accepted},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {mar},
keywords = {Industry manufacturing process, Artificial intelligence, Smart manufacturing, Neural network, Bayesian network}
}

@article{10.14778/3415478.3415565,
author = {Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad},
title = {Building High Throughput Permissioned Blockchain Fabrics: Challenges and Opportunities},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415565},
doi = {10.14778/3415478.3415565},
abstract = {Since the introduction of Bitcoin---the first widespread application driven by blockchains---the interest in the design of blockchain-based applications has increased tremendously. At the core of these applications are consensus protocols that securely replicate client requests among all replicas, even if some replicas are Byzantine faulty. Unfortunately, these consensus protocols typically have low throughput, and this lack of performance is often cited as the reason for the slow wider adoption of blockchain technology. Consequently, many works focus on designing more efficient consensus protocols to increase throughput of consensus.We believe that this focus on consensus protocols only explains part of the story. To investigate this belief, we raise a simple question: Can a well-crafted system using a classical consensus protocol outperform systems using modern protocols? In this tutorial, we answer this question by diving deep into the design of blockchain systems. Further, we take an in-depth look at the theory behind consensus, which can help users select the protocol that best-fits their requirements. Finally, we share our vision of high-throughput blockchain systems that operate at large scales.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3441–3444},
numpages = {4}
}

@inproceedings{10.1145/3027385.3027414,
author = {Hoel, Tore and Griffiths, Dai and Chen, Weiqin},
title = {The Influence of Data Protection and Privacy Frameworks on the Design of Learning Analytics Systems},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027414},
doi = {10.1145/3027385.3027414},
abstract = {Learning analytics open up a complex landscape of privacy and policy issues, which, in turn, influence how learning analytics systems and practices are designed. Research and development is governed by regulations for data storage and management, and by research ethics. Consequently, when moving solutions out the research labs implementers meet constraints defined in national laws and justified in privacy frameworks. This paper explores how the OECD, APEC and EU privacy frameworks seek to regulate data privacy, with significant implications for the discourse of learning, and ultimately, an impact on the design of tools, architectures and practices that now are on the drawing board. A detailed list of requirements for learning analytics systems is developed, based on the new legal requirements defined in the European General Data Protection Regulation, which from 2018 will be enforced as European law. The paper also gives an initial account of how the privacy discourse in Europe, Japan, South-Korea and China is developing and reflects upon the possible impact of the different privacy frameworks on the design of LA privacy solutions in these countries. This research contributes to knowledge of how concerns about privacy and data protection related to educational data can drive a discourse on new approaches to privacy engineering based on the principles of Privacy by Design. For the LAK community, this study represents the first attempt to conceptualise the issues of privacy and learning analytics in a cross-cultural context. The paper concludes with a plan to follow up this research on privacy policies and learning analytics systems development with a new international study.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {243–252},
numpages = {10},
keywords = {data protection by default, learning analytics, data protection by design, learning analytics process requirements, learning analytics systems design, personal information, privacy by design, privacy frameworks, data protection},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3530019.3531346,
author = {Jiang, Shanshan and R\ae{}der, Truls Bakkejord},
title = {Experience on Using ArchiMate Models for Modelling Blockchain-Enhanced Value Chains},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3531346},
doi = {10.1145/3530019.3531346},
abstract = {Blockchain is an emerging disruptive technology with a great potential to impact business value creation. Yet it is challenging to understand how blockchain can be utilised to improve enterprises’ performance and value creation. A formalised conceptual and enterprise modelling may help bridge the communication gap between domain experts and blockchain system designers. The goal of this study is to explore how can modelling facilitate blockchain-enhanced value chain design from a motivation viewpoint. We report our experience from modelling a blockchain-enhanced seafood supply chain using ArchiMate motivation and strategy models, where stakeholders have diverse concerns and goals. Preliminary results have indicated that such a modelling approach facilitates the common understanding, and the decision and prioritisation of the business strategies, as well as the identification of the blockchain benefits for the enterprises. It may help stakeholders along the value chain to align their business strategies and priorities, and can be an effective tool for value co-creation and enhancing the collaboration and communication among stakeholders and with blockchain application developers.},
booktitle = {Proceedings of the International Conference on Evaluation and Assessment in Software Engineering 2022},
pages = {375–382},
numpages = {8},
keywords = {Blockchain, Enterprise modelling, ArchiMate, Strategy, Goal, Motivation},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3371238.3371269,
author = {Pan, Zhiwen and Zhao, Shuangye and Pacheco, Jesus and Zhang, Yuxin and Song, Xiaofan and Chen, Yiqiang and Dai, Lianjun and Zhang, Jun},
title = {Comprehensive Data Management and Analytics for General Society Survey Dataset},
year = {2019},
isbn = {9781450376402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371238.3371269},
doi = {10.1145/3371238.3371269},
abstract = {The General Society Survey(GSS) is a kind of government-funded survey which aims at examining the Socio-economic status, quality of life, and structure of contemporary society. GSS dataset is regarded as one of the authoritative source for the government and organization practitioners to make data-driven policies. The previous analytic approaches for GSS dataset are designed by combining expert knowledges and simple statistics. In this paper, we proposed a comprehensive data management and data mining approach for GSS datasets. The approach is designed to be operated in a two-phase manner: a data management phase which can improve the quality of GSS data by performing attribute preprocessing and filter-based attribute selection; a data mining phase which can extract hidden knowledges from the dataset by performing data mining analysis including prediction analysis, classification analysis, association analysis and clustering analysis. By leveraging the power of data mining techniques, our proposed approach can explore knowledges in a fine-grained manner with minimum human interference. Experiments on Chinese General Social Survey dataset are conducted at the end to evaluate the performance of our approach.},
booktitle = {Proceedings of the 4th International Conference on Crowd Science and Engineering},
pages = {195–203},
numpages = {9},
keywords = {Data mining, Knowledge discovery, Decision support systems, Data management, Society survey},
location = {Jinan, China},
series = {ICCSE'19}
}

@inproceedings{10.1145/3396956.3396975,
author = {van Donge, W. and Bharosa, N. and Janssen, M. F. W. H. A.},
title = {Future Government Data Strategies: Data-Driven Enterprise or Data Steward? Exploring Definitions and Challenges for the Government as Data Enterprise},
year = {2020},
isbn = {9781450387910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396956.3396975},
doi = {10.1145/3396956.3396975},
abstract = {Comparable to the concept of a data(-driven) enterprise, the concept of a ‘government as data (-driven) enterprise’ is gaining popularity as a data strategy. However, what it implies is unclear. The objective of this paper is to clarify the concept of the government as data (-driven) enterprise, and identify the challenges and drivers that shape future data strategies. Drawing on literature review and expert interviews, this paper provides a rich understanding of the challenges for developing sound future government data strategies. Our analysis shows that two contrary data strategies dominate the debate. On the one hand is the data-driven enterprise strategy that focusses on collecting and using data to improve or enrich government processes and services (internal orientation). On the other hand, respondents point to the urgent need for governments to take on data stewardship, so other parties can use data to develop value for society (external orientation). Since these data strategies are not mutually exclusive, some government agencies will attempt to combine them, which is very difficult to pull off. Nonetheless, both strategies demand a more data minded culture. Moreover, the successful implementation of either strategy requires mature data governance – something most organisations still need to master. This research contributes by providing more depth to these strategies. The main challenge for policy makers is to decide on which strategy best fits their agency's roles and responsibilities and develop a shared roadmap with the external actors while at the same time mature on data governance.},
booktitle = {The 21st Annual International Conference on Digital Government Research},
pages = {196–204},
numpages = {9},
keywords = {data governance, Data-driven government, e-government, data stewardship, data enterprise},
location = {Seoul, Republic of Korea},
series = {dg.o '20}
}

@inproceedings{10.1145/3292500.3332296,
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
title = {Data Integration and Machine Learning: A Natural Synergy},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3332296},
doi = {10.1145/3292500.3332296},
abstract = {As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3193–3194},
numpages = {2},
keywords = {entity linkage, data cleaning, data fusion, schema mapping, data integration},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3445969.3450427,
author = {Empl, Philip and Pernul, G\"{u}nther},
title = {A Flexible Security Analytics Service for the Industrial IoT},
year = {2021},
isbn = {9781450383196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445969.3450427},
doi = {10.1145/3445969.3450427},
abstract = {In Cloud Computing, the cloud serves as a central data hub for the Industrial Internet of Things' (IIoT) data and is deployed in diverse application fields, e.g., Smart Grid or Smart Manufacturing. Therefore, the aggregated and contextualized data is bundled in a central data hub, bringing tremendous cybersecurity advantages. Given the threat landscape in IIoT systems, especially SMEs (small and medium-sized enterprises) need to be prepared regarding their cybersecurity, react quickly, and strengthen their overall cybersecurity. For instance, with the application of machine learning algorithms, security-related data can be analyzed predictively in order to be able to ward off a potential attack at an early stage. Since modern reference architectures for IIoT systems, such as RAMI 4.0 or IIRA, consider cybersecurity approaches on a high level and SMEs lack financial funds and knowledge, this paper conceptualizes a security analytics service used as a security add-on to these reference architectures. Thus, this paper conceptualizes a flexible security analytics service that implements security capabilities with flexible analytical techniques that fit specific SMEs' needs. The security analytics service is also evaluated with a real-world use case.},
booktitle = {Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {23–32},
numpages = {10},
keywords = {security as a service, security analytics, industrial IoT},
location = {Virtual Event, USA},
series = {SAT-CPS '21}
}

@inproceedings{10.1145/2883851.2883893,
author = {Drachsler, Hendrik and Greller, Wolfgang},
title = {Privacy and Analytics: It's a DELICATE Issue a Checklist for Trusted Learning Analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883893},
doi = {10.1145/2883851.2883893},
abstract = {The widespread adoption of Learning Analytics (LA) and Educational Data Mining (EDM) has somewhat stagnated recently, and in some prominent cases even been reversed following concerns by governments, stakeholders and civil rights groups about privacy and ethics applied to the handling of personal data. In this ongoing discussion, fears and realities are often indistinguishably mixed up, leading to an atmosphere of uncertainty among potential beneficiaries of Learning Analytics, as well as hesitations among institutional managers who aim to innovate their institution's learning support by implementing data and analytics with a view on improving student success. In this paper, we try to get to the heart of the matter, by analysing the most common views and the propositions made by the LA community to solve them. We conclude the paper with an eight-point checklist named DELICATE that can be applied by researchers, policy makers and institutional managers to facilitate a trusted implementation of Learning Analytics.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {89–98},
numpages = {10},
keywords = {learning analytics, ethics, trust, data management, educational data mining, implementation, legal aspects, privacy},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@article{10.1145/3340286,
author = {Wong, Ka-Chun and Zhang, Jiao and Yan, Shankai and Li, Xiangtao and Lin, Qiuzhen and Kwong, Sam and Liang, Cheng},
title = {DNA Sequencing Technologies: Sequencing Data Protocols and Bioinformatics Tools},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3340286},
doi = {10.1145/3340286},
abstract = {The recent advances in DNA sequencing technology, from first-generation sequencing (FGS) to third-generation sequencing (TGS), have constantly transformed the genome research landscape. Its data throughput is unprecedented and severalfold as compared with past technologies. DNA sequencing technologies generate sequencing data that are big, sparse, and heterogeneous. This results in the rapid development of various data protocols and bioinformatics tools for handling sequencing data.In this review, a historical snapshot of DNA sequencing is taken with an emphasis on data manipulation and tools. The technological history of DNA sequencing is described and reviewed in thorough detail. To manipulate the sequencing data generated, different data protocols are introduced and reviewed. In particular, data compression methods are highlighted and discussed to provide readers a practical perspective in the real-world setting. A large variety of bioinformatics tools are also reviewed to help readers extract the most from their sequencing data in different aspects, such as sequencing quality control, genomic visualization, single-nucleotide variant calling, INDEL calling, structural variation calling, and integrative analysis. Toward the end of the article, we critically discuss the existing DNA sequencing technologies for their pitfalls and potential solutions.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {98},
numpages = {30},
keywords = {technology, tools, DNA sequencing, bioinformatics, data protocols, computational biology, history, software, third-generation sequencing (TGS)}
}

@article{10.14778/3415478.3415504,
author = {Wang, Chen and Huang, Xiangdong and Qiao, Jialin and Jiang, Tian and Rui, Lei and Zhang, Jinrui and Kang, Rong and Feinauer, Julian and McGrail, Kevin A. and Wang, Peng and Luo, Diaohan and Yuan, Jun and Wang, Jianmin and Sun, Jiaguang},
title = {Apache IoTDB: Time-Series Database for Internet of Things},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415504},
doi = {10.14778/3415478.3415504},
abstract = {The amount of time-series data that is generated has exploded due to the growing popularity of Internet of Things (IoT) devices and applications. These applications require efficient management of the time-series data on both the edge and cloud side that support high throughput ingestion, low latency query and advanced time series analysis. In this demonstration, we present Apache IoTDB managing time-series data to enable new classes of IoT applications. IoTDB has both edge and cloud versions, provides an optimized columnar file format for efficient time-series data storage, and time-series database with high ingestion rate, low latency queries and data analysis support. It is specially optimized for time-series oriented operations like aggregations query, down-sampling and sub-sequence similarity search. An edge-to-cloud time-series data management application is chosen to demonstrate how IoTDB handles time-series data in real-time and supports advanced analytics by integrating with Hadoop and Spark. An end-to-end IoT data management solution is shown by integrating IoTDB with PLC4x, Calcite, and Grafana.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2901–2904},
numpages = {4}
}

@inproceedings{10.1145/2684200.2684336,
author = {Hu, Bo and Rodrigues, Eduarda Mendes and Viel, Emeric},
title = {Capri: Programmable Analytics for Linked Data},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684336},
doi = {10.1145/2684200.2684336},
abstract = {Link Data (LD) initiative has fundamentally changed the way how data are published, distributed, and consumed. It advocates data transparency and accessibility to fulfill the Web of Data vision. Thus far, tens of billions of data items have been made publicly available in machine-understandable forms (e.g. RDF). The sheer size of LD data, however, has not resulted in a significant increase of data consumption and thus a self-sustainable consumption-driven publication. We contend that this is primarily due to the lack of tooling for exploiting LD. A new programming paradigm is necessary to simplify and encourage value-add LD data utilisation.This paper reports an on-going project towards programmable Linked Open Data. We propose to tap into a distributed computing environment underpinning the popular statistical toolkit R. Where possible, native R operators and functions are used in our approach so as to lower the learning curve for experienced data scientists.We believe a report to the relevant community at this stage can help us to collect critical requirements before moving into the next stage of development. The crux of our future work lies in comprehensive and extensive evaluations, in terms of, but not limited to, system performance, system stability, system scalability, programming productivity and user experience.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {217–223},
numpages = {7},
keywords = {RDF, Linked Data, R},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/3410886.3410913,
author = {Kritzinger, A.K. and Calitz, A.P. and Westraadt, L.},
title = {Data Wrangling for South African Smart City Crime Data},
year = {2020},
isbn = {9781450388474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410886.3410913},
doi = {10.1145/3410886.3410913},
abstract = {South Africa (S.A.) is currently facing economic and social challenges that could benefit from the implementation of international smart city guidelines. Crucial to transforming a city into a smart city is the collection and access to reliable data. One of the main problems experienced by S.A. cities is the limited access to data, resulting from a traditionally fragmented approach to data collection, sharing and use. Crime-related data is one of the most commonly collected datasets in smart cities. In S.A., crime data is predominantly collected by the S.A. Police Services (SAPS) and security companies. While the latter are not readily available for public use, SAPS crime data is consolidated and disseminated at the national level. Initial data exploration, however, shows that temporal, spatial and structural inconsistencies in the data limits the usefulness of available crime data. In this study, the inconsistencies in SAPS crime data are summarised, and standard data wrangling techniques are implemented and evaluated to clean the data. The study proposes a data wrangling model for S.A. crime data. Furthermore, this study will further developments that could benefit S.A. cities in general as they transform into smart cities.},
booktitle = {Conference of the South African Institute of Computer Scientists and Information Technologists 2020},
pages = {198–209},
numpages = {12},
keywords = {Smart City Data, Open Data, Data Wrangling, Data Cleaning},
location = {Cape Town, South Africa},
series = {SAICSIT '20}
}

@inproceedings{10.1145/1963192.1963325,
author = {Baeza-Yates, Ricardo and Masan\`{e}s, Julien and Spaniol, Marc},
title = {The 1st Temporal Web Analytics Workshop (TWAW)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963325},
doi = {10.1145/1963192.1963325},
abstract = {The objective of the 1st Temporal Web Analytics Workshop (TWAW) is to provide a venue for researchers of all domains (IE/IR, Web mining etc.) where the temporal dimension opens up an entirely new range of challenges and possibilities. The workshop's ambition is to help shaping a community of interest on the research challenges and possibilities resulting from the introduction of the time dimension in Web analysis. The maturity of the Web, the emergence of large scale repositories of Web material, makes this very timely and a growing sets of research and services (recorded future1, truthy2 launched just in the last months) are emerging that have this focus in common. Having a dedicated workshop will help, we believe, to take a rich and cross-domain approach to this new research challenge with a strong focus on the temporal dimension.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {307–308},
numpages = {2},
keywords = {distributed data analytics, temporal web analytics, web scale data analytics},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/3289402.3289526,
author = {Dahbi, Kawtar Younsi and Lamharhar, Hind and Chiadmi, Dalila},
title = {Exploring Dimensions Influencing the Usage of Open Government Data Portals},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289526},
doi = {10.1145/3289402.3289526},
abstract = {Governments are considered as one of the major producers of data. Opening up and publishing this Big Government Data in national portals have significant impact on fostering innovation, improving transparency, public accountability and collaboration. Thus, the expected benefits are hindered by several factors that influence the usage of Open Government Data portals, exploring and investigating these factors is the first step to propose an evaluation approach for OGD portals and promote their usage. In this work, we identified a set of evaluation dimensions that affect OGD portal's usage and fulfillment of users' needs and requirements. According to the identified dimensions, we propose an evaluation of two national OGD portals},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {26},
numpages = {6},
keywords = {Open Government Data, usage, Evaluation, Open Government Data portals},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/3486640.3491391,
author = {Devarakonda, Ranjeet and Guntupally, Kavya and Thornton, Michele and Wei, Yaxing and Singh, Debjani and Lunga, Dalton},
title = {FAIR Interfaces for Geospatial Scientific Data Searches},
year = {2021},
isbn = {9781450391238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486640.3491391},
doi = {10.1145/3486640.3491391},
abstract = {Several factors must be considered in designing a highly accurate, reliable, scalable, and user-friendly geospatial data search interfaces. This paper examines four critical questions that ought to be considered during design phase: (1) Is the search interface or API that provides the search capability useable by both humans and machines? (2) Are the results consistent and reliable? (3) Is the output response format free to use, community-defined, and non-propriety? (4) Does the API clearly state the usage clauses? This paper discusses how certain data repositories at the US Department of Energy's Oak Ridge National Laboratory apply FAIR data principles to enable geospatial searches and address the above-mentioned questions.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
pages = {1–4},
numpages = {4},
keywords = {FAIR data principle for scientific data, ARM Data Center, ORNL DAAC, Geospatial search interfaces},
location = {Beijing, China},
series = {GeoSearch'21}
}

@inproceedings{10.1145/3428502.3428576,
author = {Hanbal, Rajesh Dinesh and Prakash, Amit and Srinivasan, Janaki},
title = {Who Drives Data in Data-Driven Governance? The Politics of Data Production in India's Livelihood Program},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428576},
doi = {10.1145/3428502.3428576},
abstract = {The increased digitisation of government information systems, as well as emerging data analytics and visualization techniques, have led lately to a surge in interest in the role of data in governance and development. The latest buzzwords in governance now include data-driven governance, data-for-development, evidence-based policy-making, and open government data. However, not much attention has been paid to understand the process of the production of data in government information systems. Our findings are based on six months of an ethnographic study of India's livelihood program- Mahatma Gandhi National Rural Employment Guarantee Act (MGNREGA) in a rural district of Karnataka. We argue that the practice of data production is carefully managed and controlled by local power elites providing an illusion of transparency in a digital information system. Understanding and recognizing the political nature of data production can help in better evaluation of development interventions, policy-making as well as in the design of more just information systems.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {485–493},
numpages = {9},
keywords = {data justice, Data production, Open government data},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/3457784.3457820,
author = {Al-Khowarizmi, Al-Khowarizmi and Lubis, Muharman and Ridho Lubis, Arif and Fauzi, Fauzi and Ramadhan Nasution, Ilham},
title = {Model of Business Intelligence Applied the Principle of Cooperative Society in the Business Forums},
year = {2021},
isbn = {9781450388825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457784.3457820},
doi = {10.1145/3457784.3457820},
abstract = {Business forums are activities between individuals and organizations that carry out the transactions on online media or within applications, which spread across countries. Along with the development of information technology towards business intelligence (BI), the business processes carried out in the business forum are modeled specifically in order to create an effort and attempt to follow the indicator and criteria from the industrial revolution 4.0. In this paper, a model is designed to combine three type of principles, namely the business forum, BI and the cooperative principle. Actually, cooperatives have been long abandoned since the existence of conventional and Islamic banking concept but it has kinship principle to divide the profits based on the size of the contribution given. Meanwhile, BI model is designed to obtain a formula from the cooperative principle, namely the residual income from operations where the transaction process is successfully implemented through the application to allocate a portion of the profits to the members based on the specified percent.},
booktitle = {2021 10th International Conference on Software and Computer Applications},
pages = {224–228},
numpages = {5},
keywords = {Model and Simulation, Cooperative Society, Business Intelligence, Business Forum},
location = {Kuala Lumpur, Malaysia},
series = {ICSCA 2021}
}

@inproceedings{10.1109/JCDL.2019.00088,
author = {Yu, Qianqian and Zhang, Jianyong and Qian, Li and Dong, Zhipeng and Huang, Yongwen and Jianhua, Liu},
title = {Practice of Constructing Name Authority Database Based on Multi-Source Data Integration},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00088},
doi = {10.1109/JCDL.2019.00088},
abstract = {Name authority is a common issue in digital library. This paper mainly summarizes the practice of constructing name authority database based on multi-source data in NSTL. Firstly, we load, integrate different source data and convert them into unified structure. Then, we extract scientific entities and relationships from these data, according to metadata model. For different entities, we use different disambiguation rules and algorithms. As a result, we have constructed author name authority database, institution authority database, journal authority database, and fund authority database. Compared with Incites, taking six institutions name authority data as a sample, the result shows that the average accuracy can reach 86.8%.1},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {398–399},
numpages = {2},
keywords = {multi-source, name disambiguation, NSTL, name authority},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1145/3378393.3402248,
author = {Ramanujapuram, Arun and Malemarpuram, Charan Kumar},
title = {Enabling Sustainable Behaviors of Data Recording and Use in Low-Resource Supply Chains},
year = {2020},
isbn = {9781450371292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378393.3402248},
doi = {10.1145/3378393.3402248},
abstract = {Public services, such as public health supply chains, in low- and middle-income countries can be characterized as low-resource environments, where both infrastructure and human capacity are limited. There is no strong culture of data recording or use, with ad hoc reporting practices, poor planning and lack of coordination. All these lead to poor supply chain performance, thereby restricting access to medicines, and eventually resulting in poorer health and mortality.We describe the ground-up design of Logistimo SCM, a supply chain management software, offered as a service, that has enabled a transformative change in public health supply chains, leading to improved performance. Our approach is rooted in bottom-up empowerment of the human value chain, based on the principle that higher self-efficacy amongst health workers and managers can lead to sustained changes in data recording and use behaviors. This is achieved through a service that optimizes data collection effort, maximizes supervisory bandwidth, promotes proactive and collaborative operations, and enables frictionless performance recognition. We describe the guiding principles of inclusive software service design and four mechanisms that enable the appropriate conditions for stimulating a behavior of data recording and use. We demonstrate their effectiveness in achieving good supply chain performance through case studies in India and Africa. The principles and methods discussed here are generic and can be applied to any low-resource environment.},
booktitle = {Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {65–75},
numpages = {11},
keywords = {culture of data use, Data collection, supply chain, public health, low-resource environment, data use behavior},
location = {Ecuador},
series = {COMPASS '20}
}

@article{10.1145/3466160,
author = {Sambasivan, Nithya},
title = {Seeing like a Dataset from the Global South},
year = {2021},
issue_date = {July - August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1072-5520},
url = {https://doi.org/10.1145/3466160},
doi = {10.1145/3466160},
abstract = {This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor},
journal = {Interactions},
month = {jun},
pages = {76–78},
numpages = {3}
}

@inproceedings{10.1145/3265757.3265766,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {Developing a Theoretically Founded Data Literacy Competency Model},
year = {2018},
isbn = {9781450365888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265757.3265766},
doi = {10.1145/3265757.3265766},
abstract = {Today, data is everywhere: Our digitalized world depends on enormous amounts of data that are captured by and about everyone and considered a valuable resource. Not only in everyday life, but also in science, the relevance of data has clearly increased in recent years: Nowadays, data-driven research is often considered a new research paradigm. Thus, there is general agreement that basic competencies regarding gathering, storing, processing and visualizing data, often summarized under the term data literacy, are necessary for every scientist today. Moreover, data literacy is generally important for everyone, as it is essential for understanding how the modern world works. Yet, at the moment data literacy is hardly considered in CS teaching at schools. To allow deeper insight into this field and to structure related competencies, in this work we develop a competency model of data literacy by theoretically deriving central content and process areas of data literacy from existing empirical work, keeping a school education perspective in mind. The resulting competency model is contrasted to other approaches describing data literacy competencies from different perspectives. The practical value of this work is emphasized by giving insight into an exemplary lesson sequence fostering data literacy competencies.},
booktitle = {Proceedings of the 13th Workshop in Primary and Secondary Computing Education},
articleno = {9},
numpages = {10},
keywords = {competency model, data, data management, data literacy, CS education, data science},
location = {Potsdam, Germany},
series = {WiPSCE '18}
}

@inproceedings{10.1145/3176349.3176901,
author = {Bogers, Toine and G\"{a}de, Maria and Freund, Luanne and Hall, Mark and Koolen, Marijn and Petras, Vivien and Skov, Mette},
title = {Workshop on Barriers to Interactive IR Resources Re-Use},
year = {2018},
isbn = {9781450349253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176349.3176901},
doi = {10.1145/3176349.3176901},
abstract = {The goal of this workshop is to serve as a starting point for a community-driven effort to design and implement a platform for the collection, organization, maintenance, and sharing of resources for IIR experimentation. As in all scientific endeavors, progress in IIR research is contingent on the ability to build on previous ideas, approaches, and resources. However, we believe there to be a number of barriers to reproducibility and re-use of resources in IIR research: the fragmentary nature of how the community»s resources are organized, the lack of awareness of their existence, documentation and organization of the resources, the nature of the typical research publication cycle, and the effort required to make such resources available. We believe that an online platform dedicated to the collection and organization of IIR resources could be a promising way of overcoming these barriers. The workshop therefore aims to serve both as a brainstorming opportunity about the shape this iRepository should take, as well as a way of building support in the community for its implementation.},
booktitle = {Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval},
pages = {382–385},
numpages = {4},
keywords = {repository, intertactive information retrieval, evaluation},
location = {New Brunswick, NJ, USA},
series = {CHIIR '18}
}

@article{10.1145/2070736.2070750,
author = {Badia, Antonio and Lemire, Daniel},
title = {A Call to Arms: Revisiting Database Design},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2070736.2070750},
doi = {10.1145/2070736.2070750},
journal = {SIGMOD Rec.},
month = {nov},
pages = {61–69},
numpages = {9}
}

@article{10.1145/3516515,
author = {Sambasivan, Nithya},
title = {All Equation, No Human: The Myopia of AI Models},
year = {2022},
issue_date = {March - April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1072-5520},
url = {https://doi.org/10.1145/3516515},
doi = {10.1145/3516515},
abstract = {This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor},
journal = {Interactions},
month = {feb},
pages = {78–80},
numpages = {3}
}

@inproceedings{10.1145/2908131.2908172,
author = {Weller, Katrin and Kinder-Kurlanda, Katharina E.},
title = {A Manifesto for Data Sharing in Social Media Research},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908172},
doi = {10.1145/2908131.2908172},
abstract = {More and more researchers want to share research data collected from social media to allow for reproducibility and comparability of results. With this paper we want to encourage them to pursue this aim -- despite initial obstacles that they may face. Sharing can occur in various, more or less formal ways. We provide background information that allows researchers to make a decision about whether, how and where to share depending on their specific situation (data, platform, targeted user group, research topic etc.). Ethical, legal and methodological considerations are important for making this decision. Based on these three dimensions we develop a framework for social media sharing that can act as a first set of guidelines to help social media researchers make practical decisions for their own projects. In the long run, different stakeholders should join forces to enable better practices for data sharing for social media researchers. This paper is intended as our call to action for the broader research community to advance current practices of data sharing in the future.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {166–172},
numpages = {7},
keywords = {reproducibility, methodology, data protection, social media, archiving, data sharing, privacy, data archives, legal issues},
location = {Hannover, Germany},
series = {WebSci '16}
}

@article{10.1145/3517189,
author = {Suhail, Sabah and Hussain, Rasheed and Jurdak, Raja and Oracevic, Alma and Salah, Khaled and Hong, Choong Seon and Matulevi\v{c}ius, Raimundas},
title = {Blockchain-Based Digital Twins: Research Trends, Issues, and Future Challenges},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {11s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3517189},
doi = {10.1145/3517189},
abstract = {Industrial processes rely on sensory data for decision-making processes, risk assessment, and performance evaluation. Extracting actionable insights from the collected data calls for an infrastructure that can ensure the dissemination of trustworthy data. For the physical data to be trustworthy, it needs to be cross validated through multiple sensor sources with overlapping fields of view. Cross-validated data can then be stored on the blockchain, to maintain its integrity and trustworthiness. Once trustworthy data is recorded on the blockchain, product lifecycle events can be fed into data-driven systems for process monitoring, diagnostics, and optimized control. In this regard, digital twins (DTs) can be leveraged to draw intelligent conclusions from data by identifying the faults and recommending precautionary measures ahead of critical events. Empowering DTs with blockchain in industrial use cases targets key challenges of disparate data repositories, untrustworthy data dissemination, and the need for predictive maintenance. In this survey, while highlighting the key benefits of using blockchain-based DTs, we present a comprehensive review of the state-of-the-art research results for blockchain-based DTs. Based on the current research trends, we discuss a trustworthy blockchain-based DTs framework. We also highlight the role of artificial intelligence in blockchain-based DTs. Furthermore, we discuss the current and future research and deployment challenges of blockchain-supported DTs that require further investigation.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {240},
numpages = {34},
keywords = {industrial control systems (ICSs), digital twins (DTs), Artificial intelligence (AI), blockchain, Internet of Things (IoT), cyber-physical systems (CPSs), Industry 4.0}
}

@inproceedings{10.1145/3299819.3299850,
author = {Li, Ying and Zhang, AiMin and Zhang, Xinman and Wu, Zhihui},
title = {A Data Lake Architecture for Monitoring and Diagnosis System of Power Grid},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299850},
doi = {10.1145/3299819.3299850},
abstract = {In this paper, a data lake architecture is proposed for a class of monitoring and diagnostic systems applied to power grid. The differences between data lake and data warehouse is studied to make an informed decision on how to manage a huge amount of data. To adapt to the characteristics and performances of historical data and real-time data of power grid equipment, a monitoring and diagnosis system based on data lake storage architecture is designed. The application of the framework indicates the applicability and effectiveness of data lake architecture.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {192–198},
numpages = {7},
keywords = {Data Lake, Power Grid, Monitoring And Diagnostic, Data Pond},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@article{10.1145/3154525,
author = {Fathy, Yasmin and Barnaghi, Payam and Tafazolli, Rahim},
title = {Large-Scale Indexing, Discovery, and Ranking for the Internet of Things (IoT)},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3154525},
doi = {10.1145/3154525},
abstract = {Network-enabled sensing and actuation devices are key enablers to connect real-world objects to the cyber world. The Internet of Things (IoT) consists of the network-enabled devices and communication technologies that allow connectivity and integration of physical objects (Things) into the digital world (Internet). Enormous amounts of dynamic IoT data are collected from Internet-connected devices. IoT data are usually multi-variant streams that are heterogeneous, sporadic, multi-modal, and spatio-temporal. IoT data can be disseminated with different granularities and have diverse structures, types, and qualities. Dealing with the data deluge from heterogeneous IoT resources and services imposes new challenges on indexing, discovery, and ranking mechanisms that will allow building applications that require on-line access and retrieval of ad-hoc IoT data. However, the existing IoT data indexing and discovery approaches are complex or centralised, which hinders their scalability. The primary objective of this article is to provide a holistic overview of the state-of-the-art on indexing, discovery, and ranking of IoT data. The article aims to pave the way for researchers to design, develop, implement, and evaluate techniques and approaches for on-line large-scale distributed IoT applications and services.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {29},
numpages = {53},
keywords = {large-scale data, discovery, ranking, Internet of things (IoT), indexing, wireless sensor network (WSN)}
}

@inproceedings{10.1145/3267305.3274762,
author = {Budde, Matthias and Riedel, Till},
title = {Challenges in Capturing and Analyzing High Resolution Urban Air Quality Data},
year = {2018},
isbn = {9781450359665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267305.3274762},
doi = {10.1145/3267305.3274762},
abstract = {Classic measurement grids with their static and expensive infrastructure are unfit to realize modern air quality monitoring needs, such as source appointment, pollution tracking or the assessment of personal exposure. Fine grained air quality assessment (both in time and space) is the future. Different approaches, ranging from measurement with low-cost sensors over advanced modeling and remote sensing to combinations of these have been proposed. This position paper summarizes our previous contributions in this field and lists what we see as open challenges for future research.},
booktitle = {Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
pages = {1162–1165},
numpages = {4},
keywords = {PM2.5, particulate matter, Air quality, challenges, sensing, PM10, urban air},
location = {Singapore, Singapore},
series = {UbiComp '18}
}

@inproceedings{10.1145/2957276.2957280,
author = {Muller, Michael and Guha, Shion and Baumer, Eric P.S. and Mimno, David and Shami, N. Sadat},
title = {Machine Learning and Grounded Theory Method: Convergence, Divergence, and Combination},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2957280},
doi = {10.1145/2957276.2957280},
abstract = {Grounded Theory Method (GTM) and Machine Learning (ML) are often considered to be quite different. In this note, we explore unexpected convergences between these methods. We propose new research directions that can further clarify the relationships between these methods, and that can use those relationships to strengthen our ability to describe our phenomena and develop stronger hybrid theories.},
booktitle = {Proceedings of the 19th International Conference on Supporting Group Work},
pages = {3–8},
numpages = {6},
keywords = {axial coding, machine learning, coding families, supervised learning, grounded theory, unsupervised learning},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@inproceedings{10.1145/3333165.3333168,
author = {Lamghari, Zineb and Radgui, Maryam and Saidi, Rajaa and Rahmani, Moulay Driss},
title = {Passage Challenges from Data-Intensive System to Knowledge-Intensive System Related to Process Mining Field},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333168},
doi = {10.1145/3333165.3333168},
abstract = {Process mining has emerged as a research field that focuses on the analysis of processes using event data. Process mining is a current topic that reveals several challenges, the most important of which have defined in the Process Mining Manifesto [1]. However, none of the published works have mentioned the progress of process challenges from data-intensive system to knowledge-intensive system related to process mining field. Therefore, the objective of this paper is to provide researchers with the recent challenges emerged during the passage from data-intensive system to knowledge-intensive system.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {3},
numpages = {6},
keywords = {Process mining challenges, Adaptive Case Management, Business Process Management, Knowledge-intensive, Data-intensive, Process Mining},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@inproceedings{10.1145/3495018.3495486,
author = {Liu, Ying and Yu, Wei and Xiao, Suhong},
title = {Digital Protection of Nanfeng Nuo Mask Based on AR Technology},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495486},
doi = {10.1145/3495018.3495486},
abstract = {The sculpture of Nanfeng Nuo mask originated in the Han Dynasty, developed in the Tang and Song Dynasties, and prospered in the Ming and Qing Dynasties. The carving art has been passed down to this day. The sculptures of Nanfeng Nuo masks are famous for their simple and profound, vivid shapes and delicate techniques. This article first stated that under the background of AR technology, in terms of digital protection mode, limited by ideological understanding and technology, the current digital protection of Chinese traditional culture is still at the stage of digital information collection and preservation. How to enrich and perfect the existing digital protection mode with new digital technology is an urgent problem to be solved in the new era. Taking the Nanfeng Nuo mask as an example, this research analyzes the inheritance dilemma of the Nanfeng Nuo mask wood carving and the modern and innovative protection model through the reading of historical documents, field inspections of existing wood carvings, survey visits to the protection status, and understanding of digital technology. Through the fuzzy KNN algorithm and AR compared to the database, the various databases are related to form a complete protection system; in the "live inheritance protection mode", AR technology is proposed as the basic technology, and AR image acquisition technology and AR display technology are proposed. , AR human-computer interaction technology and digital protection mode are combined, and then a digital protection platform based on AR technology is designed to achieve the realization of the live inheritance mode by the way audiences participate in the use of the digital protection platform. Experimental research results show that people in their 30s to 40s may take a long time to receive and learn when facing a new interactive operating system, but now due to the popularity of social software such as WeChat, the 30 to 40 age group 24.4% of the population can also perform basic operations, which provides a prerequisite for the use of AR technology to digitize the Nanfeng Nuo mask.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1792–1796},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3297280.3297354,
author = {Satti, Fahad Ahmed and Khan, Wajahat Ali and Lee, Ganghun and Khattak, Asad Masood and Lee, Sungyoung},
title = {Resolving Data Interoperability in Ubiquitous Health Profile Using Semi-Structured Storage and Processing},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297354},
doi = {10.1145/3297280.3297354},
abstract = {Advancements in the field of healthcare information management have led to the development of a plethora of software, medical devices and standards. As a consequence, the rapid growth in quantity and quality of medical data has compounded the problem of heterogeneity; thereby decreasing the effectiveness and increasing the cost of diagnostics, treatment and follow-up. However, this problem can be resolved by using a semi-structured data storage and processing engine, which can extract semantic value from a large volume of patient data, produced by a variety of data sources, at variable rates and conforming to different abstraction levels. Going beyond the traditional relational model and by re-purposing state-of-the-art tools and technologies, we present, the Ubiquitous Health Profile (UHPr), which enables a semantic solution to the data interoperability problem, in the domain of healthcare1.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {762–770},
numpages = {9},
keywords = {text tagging, ACM proceedings},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3185504,
author = {Liu, Jinwei and Shen, Haiying and Narman, Husnu S. and Chung, Wingyan and Lin, Zongfang},
title = {A Survey of Mobile Crowdsensing Techniques: A Critical Component for The Internet of Things},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3185504},
doi = {10.1145/3185504},
abstract = {Mobile crowdsensing serves as a critical building block for emerging Internet of Things (IoT) applications. However, the sensing devices continuously generate a large amount of data, which consumes much resources (e.g., bandwidth, energy, and storage) and may sacrifice the Quality-of-Service (QoS) of applications. Prior work has demonstrated that there is significant redundancy in the content of the sensed data. By judiciously reducing redundant data, data size and load can be significantly reduced, thereby reducing resource cost and facilitating the timely delivery of unique, probably critical information and enhancing QoS. This article presents a survey of existing works on mobile crowdsensing strategies with an emphasis on reducing resource cost and achieving high QoS. We start by introducing the motivation for this survey and present the necessary background of crowdsensing and IoT. We then present various mobile crowdsensing strategies and discuss their strengths and limitations. Finally, we discuss future research directions for mobile crowdsensing for IoT. The survey addresses a broad range of techniques, methods, models, systems, and applications related to mobile crowdsensing and IoT. Our goal is not only to analyze and compare the strategies proposed in prior works, but also to discuss their applicability toward the IoT and provide guidance on future research directions for mobile crowdsensing.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {jun},
articleno = {18},
numpages = {26},
keywords = {quality of service, redundancy elimination, cost-effectiveness, Mobile crowdsensing, Internet of Things}
}

@inproceedings{10.1145/3544109.3544134,
author = {Zhang, Shaochen and Qu, Youyang and Wang, Peng},
title = {Design of Cloud Computing Data Center Security System Based on Virtualization Environment},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544134},
doi = {10.1145/3544109.3544134},
abstract = {In order to improve the security of cloud computing data center in the virtualized environment, a security system design of cloud computing data center is proposed based on the virtualization environment. Firstly, the security architecture of cloud computing data center is constructed, and the security of data center is evaluated. By optimizing the system equipment structure and operation steps, the security performance of cloud computing data center can be improved. The experimental results show that the design method of cloud computing data center security architecture based on Virtualization environment has high precision, good practical effect and fully meets the research requirements.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {137–145},
numpages = {9},
location = {Dalian, China},
series = {IPEC '22}
}

@article{10.14778/2536274.2536300,
author = {Okcan, Alper and Riedewald, Mirek and Panda, Biswanath and Fink, Daniel},
title = {Scolopax: Exploratory Analysis of Scientific Data},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536274.2536300},
doi = {10.14778/2536274.2536300},
abstract = {The formulation of hypotheses based on patterns found in data is an essential component of scientific discovery. As larger and richer data sets become available, new scalable and user-friendly tools for scientific discovery through data analysis are needed. We demonstrate Scolopax, which explores the idea of a search engine for hypotheses. It has an intuitive user interface that supports sophisticated queries. Scolopax can explore a huge space of possible hypotheses, returning a ranked list of those that best match the user preferences. To scale to large and complex data sets, Scolopax relies on parallel data management and mining techniques. These include model training, efficient model summary generation, and novel parallel join techniques that together with traditional approaches such as clustering manipulate massive model-summary collections to find the most interesting hypotheses. This demonstration of Scolopax uses a real observational data set, provided by the Cornell Lab of Ornithology. It contains more than 3.3 million bird sightings reported by citizen scientists and has almost 2500 attributes. Conference attendees have the opportunity to make novel discoveries in this data set, ranging from identifying variables that strongly affect bird populations in specific regions to detecting more sophisticated patterns such as habitat competition and migration.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1298–1301},
numpages = {4}
}

@article{10.1145/3532090,
author = {Wu, Jimmy Ming-Tai and Teng, Qian and Huda, Shamsul and Chen, Yeh-Cheng and Chen, Chien-Ming},
title = {A Privacy Frequent Itemsets Mining Framework for Collaboration in IoT Using Federated Learning},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3532090},
doi = {10.1145/3532090},
abstract = {Rapid advancement of industrial internet of things (IoT) technology has changed the supply chain network to an open system to meet the high demand for individualized products and provide better customer experiences. However the open-system supply chain has forced many small and midsize enterprises (SMEs) to adopt vertical integration by being divided into smaller companies with a distinctive business for each SME but a central alliance to produce a range of products and gain competencies. Therefore, existing models do not guarantee the protection of data privacy of individual SMEs. Moreover, especially for the IoT environment, collecting data in a secure way and revealing valuable knowledge in an IoT network is difficult. How to share data in a secure framework is of paramount importance in the internet of behavior field. In this article, a privacy-preserving data-mining framework is proposed for joint-venture industrial collaborative activities by combining federated learning and a ”pre-large concept” of data-mining techniques. The novelty of the proposed approach is that, while mining high-utility itemsets from multiple datasets, it does not require direct data sharing. In the proposed method, the federated-learning framework can learn from aggregated learning parameters without scanning all data from different sets. The pre-large concept in this approach reduces the amount of scanning into different datasets. Thus, the approach makes it possible to train federated learning more quickly while protecting the privacy of individual data owners. The approach has been tested on real industrial datasets in a collaborative environment. Extensive experimental results show that the approach achieves high accuracy compared with conventional data-mining techniques while preserving the privacy of datasets.},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = {apr},
keywords = {industrial collaborative data mining, pre-large concept, privacy protection, federated learning, Frequent itemset mining, Internet of Behaviour}
}

@inproceedings{10.1145/2666158.2666183,
author = {Nakuc\c{c}i, Emona and Theodorou, Vasileios and Jovanovic, Petar and Abell\'{o}, Alberto},
title = {Bijoux: Data Generator for Evaluating ETL Process Quality},
year = {2014},
isbn = {9781450309998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666158.2666183},
doi = {10.1145/2666158.2666183},
abstract = {Obtaining the right set of data for evaluating the fulfillment of different quality standards in the extract-transform-load (ETL) process design is rather challenging. First, the real data might be out of reach due to different privacy constraints, while providing a synthetic set of data is known as a labor-intensive task that needs to take various combinations of process parameters into account. Additionally, having a single dataset usually does not represent the evolution of data throughout the complete process lifespan, hence missing the plethora of possible test cases. To facilitate such demanding task, in this paper we propose an automatic data generator (i.e., Bijoux). Starting from a given ETL process model, Bijoux extracts the semantics of data transformations, analyzes the constraints they imply over data, and automatically generates testing datasets. At the same time, it considers different dataset and transformation characteristics (e.g., size, distribution, selectivity, etc.) in order to cover a variety of test scenarios. We report our experimental findings showing the effectiveness and scalability of our approach.},
booktitle = {Proceedings of the 17th International Workshop on Data Warehousing and OLAP},
pages = {23–32},
numpages = {10},
keywords = {process quality, ETL, data generator},
location = {Shanghai, China},
series = {DOLAP '14}
}

@inproceedings{10.1145/3306446.3340821,
author = {Saddiqa, Mubashrah and Rasmussen, Lise and Magnussen, Rikke and Larsen, Birger and Pedersen, Jens Myrup},
title = {Bringing Open Data into Danish Schools and Its Potential Impact on School Pupils},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340821},
doi = {10.1145/3306446.3340821},
abstract = {Private and public institutions are using open and public data to provide better services, which increases the impact of open data on daily life. With the advancement of technology, it becomes also important to equip our younger generation with the essential skills for future challenges. In order to bring up a generation equipped with 21st century skills, open data could facilitate educational processes at school level as an educational resource. Open data could acts as a key resource to enhance the understanding of data through critical thinking and ethical vision among the youth and school pupils. To bring open data into schools, it is important to know the teacher's perspective on open data literacy and its possible impact on pupils. As a research contribution, we answered these questions through a Danish public school teacher's survey where we interviewed 10 Danish public school teachers of grade 5-7th and analyzed their views about the impact of open data on pupils' learning development. After analyzing Copenhagen city's open data, we identified four open data educational themes that could facilitate different subjects, e.g. geography, mathematics, basic science and social science. The survey includes interviews, open discussions, questionnaires and an experiment with the grade 7th pupils, where we test the pupils' understanding with open data. The survey concluded that open data cannot only empower pupils to understand real facts about their local areas, improve civics awareness and develop digital and data skills, but also enable them to come up with the ideas to improve their communities.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {9},
numpages = {10},
keywords = {open data, impact, educational themes, educational resource, school pupils},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@article{10.1145/3360000,
author = {Andersen, Kim Normann and Lee, Jungwoo and Henriksen, Helle Zinner},
title = {Digital Sclerosis? Wind of Change for Government and the Employees},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2691-199X},
url = {https://doi.org/10.1145/3360000},
doi = {10.1145/3360000},
abstract = {Contrasting the political ambitions on the next generation of government, the uptake of technology can lead to digital sclerosis characterized by stiffening of the governmental processes, failure to respond to changes in demand, and lowering innovation feedback from workers. In this conceptual article, we outline three early warnings of digital sclerosis: decreased bargaining and discretion power of governmental workers, enhanced agility and ability at shifting and extended proximities, and panopticonization. To respond proactively and take preventive care initiatives, policy makers and systems developers need to be sensitized about the digital sclerosis, prepare the technology, and design intelligent augmentations in a flexible and agile approach.},
journal = {Digit. Gov.: Res. Pract.},
month = {feb},
articleno = {9},
numpages = {14},
keywords = {changing nature of work, public sector, future work, e-Government, digitalization, workplace, work, digital sclerosis}
}

@inproceedings{10.5555/3370272.3370329,
author = {Alexander, Rohan and Lyons, Kelly and Alexopoulos, Michelle and Austin, Lisa},
title = {Workshop on Barriers to Data Science Adoption: Why Existing Frameworks Aren't Working},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {Data science is an interdisciplinary scientific approach that provides methods to understand and solve problems in an evidence-based manner, using data and experience. Despite the clear benefits from adoption, many firms face challenges, be that legal, organisational, or business practices, when seeking to implement and embed data science within an existing framework. In this workshop, panel and audience members drew on their experiences to elaborate on the challenges encountered when attempting to deploying data science within existing frameworks. Panel and audience members were drawn from business, academia, and think-tanks. For discussion purposes the challenges were grouped within three themes: regulatory; investment; and workforce.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {384–385},
numpages = {2},
keywords = {legal, organisational, challenges, data science adoption, business practices},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@article{10.1145/3423923,
author = {Tentori, Monica and Ziviani, Artur and Muchaluat-Saade, D\'{e}bora C. and Favela, Jesus},
title = {Digital Healthcare in Latin America: The Case of Brazil and Mexico},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3423923},
doi = {10.1145/3423923},
journal = {Commun. ACM},
month = {oct},
pages = {72–77},
numpages = {6}
}

@inproceedings{10.1145/3421766.3421800,
author = {Wang, Deli},
title = {Research on Bank Marketing Behavior Based on Machine Learning},
year = {2020},
isbn = {9781450375535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421766.3421800},
doi = {10.1145/3421766.3421800},
abstract = {At present, under the background that data mining technology is becoming more mature and widely used in various fields, and due to the advent of the customer-oriented era and increased competition from banks, data mining technology is being widely used in the field of banking and finance to determine the target customer group And promote bank sales. Therefore, based on the Bank Marketing data in the UCI Machine Learning Repository database, this article uses the C5.0 algorithm to classify customers on the clementine experimental platform, and proposes corresponding suggestions for bank marketing based on the classification results.This article first explores and understands the Bank Marketing data set, and describes the distribution of the customer background in the data set. The quality of the data set was further explored, and the outliers and outliers were corrected by replacing them with normal data that were closest to the outliers or extreme values.This paper further selects the optimal feature variable. First, use the Filter node to filter the unimportant variables of the classification, and further select one of the more relevant variables to reduce the redundancy of the variables. The final variables are: previous, age, duration, outcome, contact, housing, job, loan, marital, education.Secondly, this paper uses sampling nodes to perform undersampling to balance the data set. On this basis, the C5.0 algorithm is used to establish a classification model and optimize parameters, and finally obtain eight classification rules. Based on this, suggestions are provided for target group determination.Finally, this article introduces the remaining four classification algorithms: C&amp;T, QUEST, CHAID, Neural Networks, and compares the C5.0 algorithm with the four classification algorithms based on the balanced data set. It is concluded that several algorithms have certain differences and the overall prediction accuracy is good.This article combines data mining theory with practical problems of banking business, and establishes a bank target customer classification model based on C5.0 algorithm. The obtained classification rules can effectively help banks to divide customer groups and take targeted measures to improve marketing efficiency.},
booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {150–154},
numpages = {5},
keywords = {C5.0 algorithm, Classification algorithm, Customer segmentation, Bank Direct Sales Project},
location = {Manchester, United Kingdom},
series = {AIAM2020}
}

@inproceedings{10.1145/3405962.3405983,
author = {Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis and Symeonidis, Moysis and Pallis, George and Dikaiakos, Marios and Pouis, Loukas and Orphanou, Kalia and Lampathaki, Fenareti and Alexandrou, Dimitrios},
title = {The ICARUS Ontology: A General Aviation Ontology Developed Using a Multi-Layer Approach},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405983},
doi = {10.1145/3405962.3405983},
abstract = {The management of aviation data is a great challenge in the aviation industry, as they are complex and can be derived from heterogeneous data sources. To handle this challenge, ontologies can be applied to facilitate the modelling of the data across multiple data sources. This paper presents an aviation domain ontology, the ICARUS ontology, which aims at facilitating the semantic description and integration of information resources that represent the various assets of the ICARUS platform and their use. To present the functionality and usability of the proposed ontology, we present the results of querying the ontology using SPARQL queries through three use case scenarios. As shown from the evaluation, the ICARUS ontology enables the integration and reasoning over multiple sources of heterogeneous aviation-related data, the semantic description of metadata produced by ICARUS, and their storage in a knowledge-base which is dynamically updated and provides access to its contents via SPARQL queries.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {21–32},
numpages = {12},
keywords = {ontology, queries, aviation, datasets, services},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3482632.3484010,
author = {Yang, Rui},
title = {Statistics and Mining Analysis of Lightning Monitoring Data in Power Grid Based on Classical Metrology Model},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484010},
doi = {10.1145/3482632.3484010},
abstract = {Lightning, also known as lightning, is a strong catastrophic discharge phenomenon between clouds and between clouds and the ground in the process of atmospheric convection. There are two types: cloud flash (between clouds) and ground flash (between clouds and the earth). Lightning location system is a system that uses telemetry technology to monitor lightning activities in full-automatic, large-area, high-precision, continuous and real-time. By analyzing lightning location data collected for a long time, lightning accident points can be quickly located, the distribution of regional lightning activities can be counted, the development trend can be analyzed, and early warning can be carried out, which can provide reference for lightning protection research of ground buildings, thus reducing the harm to human activities. In this paper, the grid method is used to store and query lightning data based on the classical measurement model. Taking the lightning protection technology of transmission network as an example, the method and application of statistics and mining of lightning monitoring data in power grid are studied.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1649–1653},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3197026.3200209,
author = {Klein, Martin and Xie, Zhiwu and Fox, Edward A.},
title = {Web Archiving and Digital Libraries (WADL)},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3200209},
doi = {10.1145/3197026.3200209},
abstract = {The 2018 edition of the Workshop on Web Archiving and Digital Libraries (WADL) will explore the integration of Web archiving and digital libraries. The workshop aims at addressing aspects covering the entire life cycle of digital resources and will also explore areas such as community building and ethical questions around web archiving.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {425–426},
numpages = {2},
keywords = {community building, digital preservation, web archiving},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3531072.3535324,
author = {Hellerstein, Joseph M. and Parameswaran, Aditya G.},
title = {Piloting Data Engineering at Berkeley},
year = {2022},
isbn = {9781450393508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531072.3535324},
doi = {10.1145/3531072.3535324},
abstract = {In the Spring of 2021, we launched a pilot edition of a new Data Engineering course at Berkeley, targeted at our burgeoning Data Science major. We discuss aspects of the design of our first offering of the course, focusing on fluency of data models, languages and transformation tasks.},
booktitle = {1st International Workshop on Data Systems Education},
pages = {38–43},
numpages = {6},
location = {Philadelphia, PA, USA},
series = {DataEd '22}
}

@inproceedings{10.1145/3482632.3487461,
author = {Tang, Xinzhong and Zhuang, Bing and Yao, Ying and Dong, Xuesong},
title = {Research on High-Reliability Intelligent-Sensing Health Service Support Platform and Key Technologies Based on Biometrics and Blockchain Security Technology},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487461},
doi = {10.1145/3482632.3487461},
abstract = {A new type of high-reliability intelligent-sensing health service support platform and its key technologies are introduced in this article. By the technologies of automatic data collection, perceptual data removal, abnormal data detection, perceptual heterogeneous data identification, it is realizable to collect, analyze and process the health data of users. In order to solve the security problem in the process of data transmission, biometric identification and blockchain are used to realize the high-reliability transmission. At the end of the paper, a high-reliability intelligent-sensing health service support platform is built. And the implementation and service support process are expounded, indicating that the platform has high practical value.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2514–2518},
numpages = {5},
keywords = {deep learning, High-reliability, health services, blockchain, biological characteristics},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3078564.3078572,
author = {Song, Zekun and Zhang, Lvyang and Liu, Tao and Chen, Ying},
title = {Ranking Learning Algorithm of Information Retrieval Based on WeChat Public Numbers},
year = {2017},
isbn = {9781450352109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078564.3078572},
doi = {10.1145/3078564.3078572},
abstract = {On the basis of obtaining the data of mass WeChat public1, in order to improve the operational efficiency and quality of WeChat public number. On the basis of the retrieval technology, the quality evaluation model of WeChat public number was established. A sort learning algorithm based on model retrieval is proposed. Use the vector space technology based on the weight of the entry position to retrieve the contents of WeChat public number, and then use the WeChat public number quality evaluation model to sort. The retrieved articles sorted data to recommend to the operator, so that the operator can be faster and more efficient to find their hope to find high quality WeChat number of public articles.},
booktitle = {Proceedings of the 6th International Conference on Information Engineering},
articleno = {4},
numpages = {5},
keywords = {Rank learning algorithm, Recommendation system, WeChatpublic number, Meta data model},
location = {Dalian Liaoning, China},
series = {ICIE '17}
}

@inproceedings{10.1145/3401025.3406443,
author = {Baban, Philsy},
title = {Pre-Processing and Data Validation in IoT Data Streams},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3406443},
doi = {10.1145/3401025.3406443},
abstract = {In the last few years, distributed stream processing engines have been on the rise due to their crucial impacts on real-time data processing with guaranteed low latency in several application domains such as financial markets, surveillance systems, manufacturing, smart cities, etc. Stream processing engines are run-time libraries to process data streams without knowing the lower level streaming mechanics. Apache Storm, Apache Flink, Apache Spark, Kafka Streams and Hazelcast Jet are some of the popular stream processing engines. Nowadays, critical systems like energy systems, are interconnected and automated. As a result, these systems are vulnerable to cyber-attacks. In real-world applications, the sensing values come from sensor devices contains missing values, redundant data, data outliers, manipulated data, data failures, etc. Therefore, our system must be resilient to these conditions. In this paper, we present an approach to check if there is any above mentioned conditions by pre-processing data streams using a stream processing engine like Apache Flink which will be updated as a library in future. Then, the pre-processed streams are forwarded to other stream processing engines like Apache Kafka for real stream processing. As a result, data validation, data consistency and integrity for a resilient system can be accomplished before initiating the actual stream processing.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {226–229},
numpages = {4},
keywords = {resiliency, stream processing, data pre-processing, data validation},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3047273.3047327,
author = {Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega},
title = {An Ontology for Open Government Data Business Model},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047327},
doi = {10.1145/3047273.3047327},
abstract = {Despite the existence of number of well-known conceptualization in e-Business and e-Commerce, there have been no efforts so far to develop a detailed, comprehensive conceptualization for business model. Current business literature is replete with fragmented conceptualizations, which only partially describe aspects of a business model. In addition, the existing conceptualizations do not explicitly support the emerging phenomenon of open government data -- an increasingly valuable economic and strategic resource. Consequently, no comprehensive, formal, executable open government data business model ontology exists, that could be directly leveraged to facilitate the design, development of an operational open data business model. This paper bridges this gap by providing a parsimonious yet sufficiently detailed, conceptualization and formal ontology of open government data business model for open data-driven organizations. Following the design science approach, we developed the ontology as a 'design artefact' and validate the ontology by using it to describe an open data business model of an open data-driven organization.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {195–203},
numpages = {9},
keywords = {formal conceptualization, open data-driven organization, e-Commerce ontology, Open government data, e-Business ontology, open data business model, and business model ontology},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3132218.3132241,
author = {Beek, Wouter and Fern\'{a}ndez, Javier D. and Verborgh, Ruben},
title = {LOD-a-Lot: A Single-File Enabler for Data Science},
year = {2017},
isbn = {9781450352963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132218.3132241},
doi = {10.1145/3132218.3132241},
abstract = {Many data scientists make use of Linked Open Data (LOD) as a huge interconnected knowledge base represented in RDF. However, the distributed nature of the information and the lack of a scalable approach to manage and consume such Big Semantic Data makes it difficult and expensive to conduct large-scale studies. As a consequence, most scientists restrict their analyses to one or two datasets (often DBpedia) that contain at most hundreds of millions of triples. LOD-a-lot is a dataset that integrates a large portion (over 28 billion triples) of the LOD Cloud into a single ready-to-consume file that can be easily downloaded, shared and queried with a small memory footprint. This paper shows there exists a wide collection of Data Science use cases that can be performed over such a LOD-a-lot file. For these use cases LOD-a-lot significantly reduces the cost and complexity of conducting Data Science.},
booktitle = {Proceedings of the 13th International Conference on Semantic Systems},
pages = {181–184},
numpages = {4},
location = {Amsterdam, Netherlands},
series = {Semantics2017}
}

@article{10.1145/2826686.2826692,
author = {Resch, Bernd and Blaschke, Thomas},
title = {Fusing Human and Technical Sensor Data: Concepts and Challenges},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/2826686.2826692},
doi = {10.1145/2826686.2826692},
abstract = {As geo-sensor webs have not grown as quickly as expected, new, alternative data sources have to be found for near real-time analysis in areas like emergency management, environmental monitoring, public health, or urban planning. This paper assesses the ability of human sensors, i.e., user-generated observations in a wide range of social networks, the mobile phone network, or micro-blogs, to complement geo-sensor networks. We clearly delineate the concepts of People as Sensors, Collective Sensing and Citizen Science. Furthermore, we point out current challenges in fusing data from technical and human sensors, and sketch future research areas in this field.},
journal = {SIGSPATIAL Special},
month = {sep},
pages = {29–35},
numpages = {7}
}

@inproceedings{10.1145/3341162.3347758,
author = {Manea, Vlad and Berrocal, Allan and De Masi, Alexandre and M\o{}ller, Naja Holten and Wac, Katarzyna and Bayer, Hannah and Lehmann, Sune and Ashley, Euan},
title = {LDC '19: International Workshop on Longitudinal Data Collection in Human Subject Studies},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3347758},
doi = {10.1145/3341162.3347758},
abstract = {Individuals increasingly use mobile, wearable, and ubiquitous devices capable of unobtrusive collection of vast amounts of scientifically rich personal data over long periods (months to years), and in the context of their daily life. However, numerous human and technological factors challenge longitudinal data collection, often limiting research studies to very short data collection periods (days to weeks), spawning recruitment biases, and affecting participant retention over time. This workshop is designed to bring together researchers involved in longitudinal data collection studies to foster an insightful exchange of ideas, experiences, and discoveries to improve the studies' reliability, validity, and perceived meaning of longitudinal mobile, wearable, and ubiquitous data collection for the participants.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {878–881},
numpages = {4},
keywords = {in situ, panel technique, mobile devices, human sensing, human subject studies, longitudinal studies},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@article{10.1145/3376915,
author = {De Aguiar, Erikson J\'{u}lio and Fai\c{c}al, Bruno S. and Krishnamachari, Bhaskar and Ueyama, J\'{o}},
title = {A Survey of Blockchain-Based Strategies for Healthcare},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3376915},
doi = {10.1145/3376915},
abstract = {Blockchain technology has been gaining visibility owing to its ability to enhance the security, reliability, and robustness of distributed systems. Several areas have benefited from research based on this technology, such as finance, remote sensing, data analysis, and healthcare. Data immutability, privacy, transparency, decentralization, and distributed ledgers are the main features that make blockchain an attractive technology. However, healthcare records that contain confidential patient data make this system very complicated because there is a risk of a privacy breach. This study aims to address research into the applications of the blockchain healthcare area. It sets out by discussing the management of medical information, as well as the sharing of medical records, image sharing, and log management. We also discuss papers that intersect with other areas, such as the Internet of Things, the management of information, tracking of drugs along their supply chain, and aspects of security and privacy. As we are aware that there are other surveys of blockchain in healthcare, we analyze and compare both the positive and negative aspects of their papers. Finally, we seek to examine the concepts of blockchain in the medical area, by assessing their benefits and drawbacks and thus giving guidance to other researchers in the area. Additionally, we summarize the methods used in healthcare per application area and show their pros and cons.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {27},
numpages = {27},
keywords = {Distributed systems, survey, blockchain, healthcare, distributed ledger technology, medical}
}

@inproceedings{10.1145/2910896.2926734,
author = {Cabanac, Guillaume and Chandrasekaran, Muthu Kumar and Frommholz, Ingo and Jaidka, Kokil and Kan, Min-Yen and Mayr, Philipp and Wolfram, Dietmar},
title = {Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016)},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2926734},
doi = {10.1145/2910896.2926734},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval~(IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {299–300},
numpages = {2},
keywords = {bibliometrics, text mining, natural language processing, information retrieval, digital libraries},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.1145/3527049.3527050,
author = {Kochinev, Yury and Antysheva, Elena and Alpysbayev, Kaisar},
title = {A System of Financial Indicators for Assessing the Risk of Material Misstatement of Accounting Information in the Context of a Continuous Online Audit},
year = {2021},
isbn = {9781450386944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527049.3527050},
doi = {10.1145/3527049.3527050},
abstract = {In the time of digitalization of the economy, the audit of financial reporting is bound to shift from periodic inspections (annual, semi-annual, quarterly) to continuous monitoring of financial information. Continuous monitoring ensures that the control results of activities are produced simultaneously or within the shortest period possible after discovering the relevant events. The risk of possible misstatement of financial statements in the process of this monitoring is assessed using analytical procedures. The source data for them are the values of financial indicators of the audited entity selected by the auditor. Analytical procedures are aimed at obtaining audit evidence and carried out through a number of actions taken to find out, study, analyze and assess the correlations between the financial-economic and other performance indicators of the organization in order to discover non-standard phenomena as well as facts and the causes of these discrepancies. The main purpose of the article was to develop a system of financial indicators, the continuous monitoring of which would allow for assessing the risk of material misstatement in real time. The methodological basis of the research is determined by the following fundamental techniques and principles: analysis, synthesis, attributive and proportional analogies, abstractions, descriptive generalizations, formulation and confirmation of working analytical hypotheses, economic/mathematical and mathematical/statistical methods. The paper analyzes the financial indicators, suggested for these purposes by a number of authors, and highlights the lack of substantiation of the former. The relationships between possible financial indicators and the items of financial statements are studied and a set of indicators has been formed. Their growth rate, if monitored in the online auditing process, makes it possible to assess the risk of material misstatement of accounting information and react accordingly.},
booktitle = {3rd International Scientific Conference on Innovations in Digital Economy},
pages = {402–407},
numpages = {6},
keywords = {Risk of material misstatement, Continuous online auditing, Analytical procedures, Financial indicators, Digitalization of the economy},
location = {Saint - Petersburg, Russian Federation},
series = {SPBPU IDE-2021}
}

@inproceedings{10.1145/3373477.3373499,
author = {Tian, Bing and Lv, Shuqing and Yin, Qilin and Li, Ning and Zhang, Yue and Liu, Ziyan},
title = {Real-Time Dynamic Data Desensitization Method Based on Data Stream},
year = {2019},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373499},
doi = {10.1145/3373477.3373499},
abstract = {With the rapid development of the data mining industry, the value hidden in the massive data has been discovered, but at the same time it has also raised concerns about privacy leakage, leakage of sensitive data and other issues. These problems have also become numerous studies. Among the methods for solving these problems, data desensitization technology has been widely adopted for its outstanding performance. However, with the increasing scale of data and the increasing dimension of data, the traditional desensitization method for static data can no longer meet the requirements of various industries in today's environment to protect sensitive data. In the face of ever-changing data sets of scale and dimension, static desensitization technology relies on artificially designated desensitization rules to grasp the massive data, and it is difficult to control the loss of data connotation. In response to these problems, this paper proposes a real-time dynamic desensitization method based on data flow, and combines the data anonymization mechanism to optimize the data desensitization strategy. Experiments show that this method can efficiently and stably perform real-time desensitization of stream data, and can save more information to support data mining in the next steps.},
booktitle = {Proceedings of the International Conference on Advanced Information Science and System},
articleno = {22},
numpages = {6},
keywords = {dynamic desensitization, stream data, data desensitization},
location = {Singapore, Singapore},
series = {AISS '19}
}

@article{10.1007/s00778-017-0486-1,
author = {Herschel, Melanie and Diestelk\"{a}mper, Ralf and Ben Lahmar, Houssem},
title = {A Survey on Provenance: What for? What Form? What From?},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0486-1},
doi = {10.1007/s00778-017-0486-1},
abstract = {Provenance refers to any information describing the production process of an end product, which can be anything from a piece of digital data to a physical object. While this survey focuses on the former type of end product, this definition still leaves room for many different interpretations of and approaches to provenance. These are typically motivated by different application domains for provenance (e.g., accountability, reproducibility, process debugging) and varying technical requirements such as runtime, scalability, or privacy. As a result, we observe a wide variety of provenance types and provenance-generating methods. This survey provides an overview of the research field of provenance, focusing on what provenance is used for (what for?), what types of provenance have been defined and captured for the different applications (what form?), and which resources and system requirements impact the choice of deploying a particular provenance solution (what from?). For each of these three key questions, we provide a classification and review the state of the art for each class. We conclude with a summary and possible future research challenges.},
journal = {The VLDB Journal},
month = {dec},
pages = {881–906},
numpages = {26},
keywords = {Provenance capture, Provenance applications, Provenance types, Data provenance, Provenance requirements, Workflow provenance, Survey}
}

@inbook{10.1145/3310205.3310206,
title = {Preface},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310206},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3236024.3236056,
author = {Wang, Ying and Wen, Ming and Liu, Zhenwei and Wu, Rongxin and Wang, Rui and Yang, Bo and Yu, Hai and Zhu, Zhiliang and Cheung, Shing-Chi},
title = {Do the Dependency Conflicts in My Project Matter?},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236056},
doi = {10.1145/3236024.3236056},
abstract = {Intensive dependencies of a Java project on third-party libraries can easily lead to the presence of multiple library or class versions on its classpath. When this happens, JVM will load one version and shadows the others. Dependency conflict (DC) issues occur when the loaded version fails to cover a required feature (e.g., method) referenced by the project, thus causing runtime exceptions. However, the warnings of duplicate classes or libraries detected by existing build tools such as Maven can be benign since not all instances of duplication will induce runtime exceptions, and hence are often ignored by developers. In this paper, we conducted an empirical study on real-world DC issues collected from large open source projects. We studied the manifestation and fixing patterns of DC issues. Based on our findings, we designed Decca, an automated detection tool that assesses DC issues' severity and filters out the benign ones. Our evaluation results on 30 projects show that Decca achieves a precision of 0.923 and recall of 0.766 in detecting high-severity DC issues. Decca also detected new DC issues in these projects. Subsequently, 20 DC bug reports were filed, and 11 of them were confirmed by developers. Issues in 6 reports were fixed with our suggested patches.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {319–330},
numpages = {12},
keywords = {Empirical study, third party library, static analysis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3532213.3532252,
author = {Ming Tang, Chun and Tao, Peng and Li, Yan},
title = {Design and Generalization of Enterprise Knowledge Graph},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532252},
doi = {10.1145/3532213.3532252},
abstract = {Knowledge Graph is widely used in artificial intelligence fields such as intelligent search, intelligent recommendation and intelligent question answering, and EKG (Enterprise Knowledge Graph) is an important foundation for enterprises to build intelligent platforms. This paper proposes the design idea of constructing EKG based on the five dimensions of human, financial, material, time and information. First, the EKG framework is built by applying component microservices, pre-construction and business orchestration; Mining and analyzing prediction methods, and finally designing an intelligent visual EKG with auxiliary decision-making functions.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {259–264},
numpages = {6},
keywords = {component microservice, intelligent visualization, Enterprise knowledge graph, knowledge generalization},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1145/3339252.3342112,
author = {Schaberreiter, Thomas and Kupfersberger, Veronika and Rantos, Konstantinos and Spyros, Arnolnt and Papanikolaou, Alexandros and Ilioudis, Christos and Quirchmayr, Gerald},
title = {A Quantitative Evaluation of Trust in the Quality of Cyber Threat Intelligence Sources},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3342112},
doi = {10.1145/3339252.3342112},
abstract = {Threat intelligence sharing has become a cornerstone of cooperative and collaborative cybersecurity. Sources providing such data have become more widespread in recent years, ranging from public entities (driven by legislatorial changes) to commercial companies and open communities that provide threat intelligence in order to help organisations and individuals to better understand and assess the cyber threat landscape putting their systems at risk. Tool support to automatically process this information is emerging concurrently. It has been observed that the quality of information received by the sources varies significantly and that in order to assess the quality of a threat intelligence source it is not sufficient to only consider qualitative indications of the source itself, but it is necessary to monitor the data provided by the source continuously to be able to draw conclusions about the quality of information provided by a source. In this paper, we propose a methodology for evaluating cyber threat information sources based on quantitative parameters. The methodology aims to facilitate trust establishment to threat intelligence sources, based on a weighted evaluation method that allows each entity to adapt it to its own needs and priorities. The approach facilitates automated tools utilising threat intelligence, since information to be considered can be prioritised based on which source is trusted the most at the time the intelligence arrives.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {83},
numpages = {10},
keywords = {cyber threat intelligence source evaluation, Cooperative and collaborative cybersecurity, quality parameters, cyber threat information sharing, trust indicators},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/2987491.2987521,
author = {de Jager, Tiaan and Brown, Irwin},
title = {A Descriptive Categorized Typology of Requisite Skills for Business Intelligence Professionals},
year = {2016},
isbn = {9781450348058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987491.2987521},
doi = {10.1145/2987491.2987521},
abstract = {Business Intelligence (BI) is regarded by executives as a critical practice to adopt and invest in. The purpose of this research is to develop a categorized typology of skills required by BI professionals. A review of extant literature resulted in the identification of twenty three skills. The research aimed to validate these skills, and add additional skills to this typology based on the experiences of BI professionals within industry. These experiences were captured through interviews. Skills were then categorized by identifying commonalities across them. No additional skills were identified by the interviewed participants. A categorized typology of skills was developed which grouped the initial twenty three skills into seven higher order categories. The seven categories of skills were identified as: (1) Prepare data for subject matter expert (SME), analyst or other external party for further analysis; (2) Apply simulation modelling, statistical techniques and provide business insight; (3) Manage stakeholders and project and operational tasks; (4) Develop strategic long term BI roadmap that links to corporate strategy; (5) Understand business processes in order to effectively extract user requirements; (6) Design and code sustainable solutions; (7) Absorb and distribute knowledge.},
booktitle = {Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists},
articleno = {14},
numpages = {10},
keywords = {Typology, Analytics, Business Intelligence, IT Skills, IS Profession},
location = {Johannesburg, South Africa},
series = {SAICSIT '16}
}

@article{10.1007/s00778-019-00588-3,
author = {Qin, Xuedi and Luo, Yuyu and Tang, Nan and Li, Guoliang},
title = {Making Data Visualization More Efficient and Effective: A Survey},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00588-3},
doi = {10.1007/s00778-019-00588-3},
abstract = {Data visualization is crucial in today’s data-driven business world, which has been widely used for helping decision making that is closely related to major revenues of many industrial companies. However, due to the high demand of data processing w.r.t. the volume, velocity, and veracity of data, there is an emerging need for database experts to help for efficient and effective data visualization. In response to this demand, this article surveys techniques that make data visualization more efficient and effective. (1) Visualization specifications define how the users can specify their requirements for generating visualizations. (2) Efficient approaches for data visualization process the data and a given visualization specification, which then produce visualizations with the primary target to be efficient and scalable at an interactive speed. (3) Data visualization recommendation is to auto-complete an incomplete specification, or to discover more interesting visualizations based on a reference visualization.},
journal = {The VLDB Journal},
month = {jan},
pages = {93–117},
numpages = {25},
keywords = {Efficient data visualization, Data visualization, Visualization languages, Data visualization recommendation}
}

@inproceedings{10.5555/2814058.3252433,
author = {Siqueira, Sean W. M. and Carvalho, Sergio T.},
title = {Session Details: Main Track - Management, Governance, and Government},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
location = {Goiania, Goias, Brazil},
series = {SBSI 2015}
}

@article{10.1145/3292384.3292389,
author = {Al-Jaroodi, Jameela and Mohamed, Nader and Jawhar, Imad},
title = {A Service-Oriented Middleware Framework for Manufacturing Industry 4.0},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
url = {https://doi.org/10.1145/3292384.3292389},
doi = {10.1145/3292384.3292389},
abstract = {The advantages of the Internet of things (IoT) initiated the vision of Industry 4.0 in Europe and smart manufacturing in USA. Both visions aim to implement the smart factory to achieve similar objectives by utilizing new technologies. These technologies include cloud computing, fog computing, cyber-physical systems (CPS), and data analytics. Together they help automate and autonomize the manufacturing processes and controls to optimize the productivity, reliability, quality, cost-effeteness, and safety of these processes. While both visions are promising, developing and operating Industry 4.0 applications are extremely challenging. This is due to the complexity of the manufacturing processes as well as their management, controls, and integration dynamics. This paper introduces Man4Ware, a service-oriented middleware for Industry 4.0. Man4Ware can help facilitate the development and operations of cloud and fog-integrated smart manufacturing applications. Man4Ware offers many advantages through service level interfaces to enable easy utilization of new technologies and integration of different services to relax many of the challenges facing the development and operations of such applications1.},
journal = {SIGBED Rev.},
month = {nov},
pages = {29–36},
numpages = {8},
keywords = {cloud computing, IoT, middleware, fog computing, cyber-physical systems, industry 4.0, smart manufacturing}
}

@inproceedings{10.1145/3384544.3384596,
author = {Valachamy, Mageshwari and Sahibuddin, Shamsul and Ahmad, Noor Azurati and Bakar, Nur Azaliah Abu},
title = {Geospatial Data Sharing: Preliminary Studies on Issues and Challenges in Natural Disaster Management},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384596},
doi = {10.1145/3384544.3384596},
abstract = {The rapid development of information technology has led to the demand for the latest, precise and easy to understand data. Data especially geospatial data is becoming increasingly crucial in all types of planning and decision making. Geospatial data sharing can be categorized into different disciplines such as public safety, disaster management, transportation, traffic control, tracking, health, environment, natural resources, mining, agriculture, utilities and many more. Whether as a way of distribution or retrieval of data, geospatial data has become an essential component of government GIS operations. Despite the prominence of this activity and its centrality to the day-to-day function of many government systems, the geospatial data sharing is still given less attention in the field of natural disaster management. Preliminary information is gathered from Literature Reviews (LR) and unstructured interviews with experts to seek information in depth. Thirteen (13) issues and challenges of geospatial data sharing in Malaysia Public Sector (MPS) for natural disaster management have been identified.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {51–56},
numpages = {6},
keywords = {Sharing, Issues and Challenges, Spatial data, Geospatial data, Spatial Data Infrastructure},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

