@inproceedings{10.1145/3307339.3342169,
author = {Perkins, Patrick and Heber, Steffen},
title = {Using a Novel Negative Selection Inspired Anomaly Detection Algorithm to Identify Corrupted Ribo-Seq and RNA-Seq Samples},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3342169},
doi = {10.1145/3307339.3342169},
abstract = {RNA-seq and Ribo-seq are popular techniques for quantifying cellular transcription and translation. These experiments use next-generation sequencing to produce genome-wide high-resolution snapshots of the total populations of mRNAs and translating ribosomes within the investigated samples. When performed in concert, these experiments yield valuable information about protein synthesis rates and translational efficiency. Due to their intricate experimental protocols and demanding data processing requirements, quality control and analysis of such experiments are often challenging. Therefore, methods for accurately assessing data quality, and for identifying contaminated samples, are greatly needed. In the following we use a novel negative selection inspired algorithm called Boundary Detection Using Nearest Neighbors (BDUNN), for the identification of corrupted samples. Our algorithm constructs a detector set and reduced training set that defines the boundaries between normal data points and potential anomalies. Subsequently, a nearest neighbor algorithm is used to classify unseen observations. We compare the performance of BDUNN with other popular negative selection and one-class classification algorithms, and show that BDUNN is capable of accurately and efficiently detecting anomalies in standard anomaly detection datasets and simulated RNA-seq and Ribo-seq data sets. Furthermore, we have implemented our method within an existing R Shiny platform for analyzing RNA-seq an Ribo-seq datasets, which permits downstream analysis of anomalous samples.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {457–465},
numpages = {9},
keywords = {ribosome profiling, machine learning, sample quality, negative selection algorithm, anomaly detection, rna-seq},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/2846012.2846026,
author = {Lipuntsov, Yuri P.},
title = {On the Relationship Between the Information and Analytical Components in the Shared E-Government},
year = {2015},
isbn = {9781450340700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2846012.2846026},
doi = {10.1145/2846012.2846026},
abstract = {Economic and mathematical models and information models are the two main components of the information environment. These two components perform different functions - the information models is responsible for the data quality, the data delivery, and the economic and mathematical models defines data mining and intelligence. This category of models is constantly being developed often independently of each other. The information models as methods of data presentation and data integration are considered as separate from economic and mathematical modeling area. This paper discuss the relationship between the two types of models as sequence of steps for models development with the horizontal and vertical traceability. The connection between two types of models presented as the reflection of the real word logic to the data layer and after that to the software layer, and the feedback from the application to the information and to the operation logic.},
booktitle = {Proceedings of the 2015 2nd International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {109–115},
numpages = {7},
keywords = {Economic and mathematical modeling, Simulation, Data exchange, Shared environment, Information modeling},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '15}
}

@inproceedings{10.1145/3491101.3503724,
author = {Pine, Kathleen and Bossen, Claus and Holten M\o{}ller, Naja and Miceli, Milagros and Lu, Alex Jiahong and Chen, Yunan and Horgan, Leah and Su, Zhaoyuan and Neff, Gina and Mazmanian, Melissa},
title = {Investigating Data Work Across Domains: New Perspectives on the Work of Creating Data},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503724},
doi = {10.1145/3491101.3503724},
abstract = {In the wake of the hype around big data, artificial intelligence, and “data-drivenness,” much attention has been paid to developing novel tools to capitalize upon the deluge of data being recorded and gathered automatically through IT systems. While much of this literature tends to overlook the data itself—sometimes even characterizing it as “data exhaust” that is readily available to be fed into algorithms, which will unlock the insights held within it—a growing body of literature has recently been directed at the (often intensive and skillful) work that goes into creating, collecting, managing, curating, analyzing, interpreting, and communicating data. These investigations detail the practices and processes involved in making data useful and meaningful so that aims of becoming ‘data-driven’ or ‘data-informed’ can become real. Further, In some cases, increased demands for data work have led to the formation of new occupations, whereas at other times data work has been added to the task portfolios of existing occupations and professions, occasionally affecting their core identity. Thus, the evolving forms of data work are requiring individual and organizational resources, new and re-tooled practices and tools, development of new competences and skills, and creation of new functions and roles. While differences exist across the global North and the global South experience of data work, such factors of data production remain paramount even as they exist largely for the benefit of the data-driven system [21, 32]. This one-day workshop will investigate existing and emerging tasks of data work. Further, participants will seek to understand data work as it impacts: individual data workers; occupations tasked with data work (existing and emerging); organizations (e.g. changing their skill-mix and infrastructuring to support data work); and teaching institutions (grappling with incorporation of data work into educational programs). Participants are required to submit a position paper or a case study drawn from their research to be reviewed and accepted by the organizing committee (submissions should be up to four pages in length). Upon acceptance, participants will read each other's paper, prepare to shortly present and respond to comments by two discussants and other participants. Subsequently, the workshop will focus on developing a set of core processes and tasks as well as an outline of a research agenda for a CHI-perspective on data work in the coming years.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {87},
numpages = {6},
keywords = {Data Work, Datafication, Data-Driven, Labor, Occupations},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/2944165.2944172,
author = {El-Atawy, Sameh S. and Khalefa, Mohamed E.},
title = {Building an Ontology-Based Electronic Health Record System},
year = {2016},
isbn = {9781450342933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2944165.2944172},
doi = {10.1145/2944165.2944172},
abstract = {Electronic health record (EHR) solutions are complex, spanning multiple specialties and domains of expertise. These systems need to handle clinical concepts, temporal data, documents, and financial transactions, which leads to a large code base that is tightly coupled with data models and inherently hard to maintain. These difficulties can greatly increase the cost of developing EHR systems, result in a high failure rate of implementation, and threaten investments in this sector. Moreover, due to the wide variance in the level of detail across different settings, data exchange is becoming a serious problem, further increasing the cost of development and maintenance.To overcome these issues, we adopt ontologies to model our proposed EHR solution, not only allowing code reuse; but also enabling later extension and customization. Adopting software factory techniques, we build tools to transform ontological models into deployment-ready code. This automatically provides handling of data persistence, access, and exchange. Business logic is expressed as ontology-based process flows and rules, ensuring data quality and supporting special needs. This logic is enforced transparently and can be modified on the fly. We optimized the user experience by facilitating fast data entry and retrieval.In this paper, we present the requirements of an effective EHR solution, explain the techniques we employed, describe the main modules of our proposed system, and discuss the technical decisions we made.},
booktitle = {Proceedings of the 2nd Africa and Middle East Conference on Software Engineering},
pages = {40–45},
numpages = {6},
keywords = {Query Language, Electronic Health Record Management, Ontology},
location = {Cairo, Egypt},
series = {AMECSE '16}
}

@inproceedings{10.1145/3219819.3219914,
author = {Samel, Karan and Miao, Xu},
title = {Active Deep Learning to Tune Down the Noise in Labels},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219914},
doi = {10.1145/3219819.3219914},
abstract = {The great success of supervised learning has initiated a paradigm shift from building a deterministic software system to a probabilistic artificial intelligent system throughout the industry. The historical records in enterprise domains can potentially bootstrap the traditional business into the modern data-driven approach almost everywhere. The introduction of the Deep Neural Networks (DNNs) significantly reduces the efforts of feature engineering so that supervised learning becomes even more automated. The last bottleneck is to ensure the data quality, particularly the label quality, because the performance of supervised learning is bounded by the errors present in labels. In this paper, we present a new Active Deep Denoising (ADD) approach that first builds a DNN noise model, and then adopts an active learning algorithm to identify the optimal denoising function. We prove that under the low noise condition, we only need to query the oracle with log n examples where n is the total number in the data. We apply ADD on one enterprise application and show that it can effectively reduce 1/3 of the prediction error with only 0.1% of examples verified by the oracle.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {685–694},
numpages = {10},
keywords = {denoising, deep neural networks, classification, active learning},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/3460000,
author = {Thirumuruganathan, Saravanan and Kunjir, Mayuresh and Ouzzani, Mourad and Chawla, Sanjay},
title = {Automated Annotations for AI Data and Model Transparency},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3460000},
doi = {10.1145/3460000},
abstract = {The data and Artificial Intelligence revolution has had a massive impact on enterprises, governments, and society alike. It is fueled by two key factors. First, data have become increasingly abundant and are often available openly. Enterprises have more data than they can process. Governments are spearheading open data initiatives by setting up data portals such as data.gov and releasing large amounts of data to the public. Second, AI engineering development is becoming increasingly democratized. Open source frameworks have enabled even an individual developer to engineer sophisticated AI systems. But with such ease of use comes the potential for irresponsible use of data.Ensuring that AI systems adhere to a set of ethical principles is one of the major problems of our age. We believe that data and model transparency has a key role to play in mitigating the deleterious effects of AI systems. In this article, we describe a framework to synthesize ideas from various domains such as data transparency, data quality, data governance among others to tackle this problem. Specifically, we advocate an approach based on automated annotations (of both data and the AI model), which has a number of appealing properties. The annotations could be used by enterprises to get visibility of potential issues, prepare data transparency reports, create and ensure policy compliance, and evaluate the readiness of data for diverse downstream AI applications. We propose a model architecture and enumerate its key components that could achieve these requirements. Finally, we describe a number of interesting challenges and opportunities.},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {2},
numpages = {9},
keywords = {data cleaning, machine learning, Data transparency}
}

@article{10.14778/2809974.2809975,
author = {K\"{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang},
title = {Possible and Certain SQL Keys},
year = {2015},
issue_date = {July 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2809974.2809975},
doi = {10.14778/2809974.2809975},
abstract = {Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1118–1129},
numpages = {12}
}

@inproceedings{10.1145/3487553.3524718,
author = {Yu, Shuo and Huang, Huafei and Dao, Minh N. and Xia, Feng},
title = {Graph Augmentation Learning},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524718},
doi = {10.1145/3487553.3524718},
abstract = {Graph Augmentation Learning (GAL) provides outstanding solutions for graph learning in handling incomplete data, noise data, etc. Numerous GAL methods have been proposed for graph-based applications such as social network analysis and traffic flow forecasting. However, the underlying reasons for the effectiveness of these GAL methods are still unclear. As a consequence, how to choose optimal graph augmentation strategy for a certain application scenario is still in black box. There is a lack of systematic, comprehensive, and experimentally validated guideline of GAL for scholars. Therefore, in this survey, we in-depth review GAL techniques from macro (graph), meso (subgraph), and micro (node/edge) levels. We further detailedly illustrate how GAL enhance the data quality and the model performance. The aggregation mechanism of augmentation strategies and graph learning models are also discussed by different application scenarios, i.e., data-specific, model-specific, and hybrid scenarios. To better show the outperformance of GAL, we experimentally validate the effectiveness and adaptability of different GAL strategies in different downstream tasks. Finally, we share our insights on several open issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability, and generalization.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1063–1072},
numpages = {10},
keywords = {Graph representation learning, Graph neural networks, Graph augmentation learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3546930.3547494,
author = {Lou, Yuze and Cafarella, Michael},
title = {Enabling Useful Provenance in Scripting Languages with a Human-in-the-Loop},
year = {2022},
isbn = {9781450394420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546930.3547494},
doi = {10.1145/3546930.3547494},
abstract = {Most data scientists must build substantial data pipelines using scripting languages like Python and R. These pipelines are hard to get correct due to the large volume of data they process (thus the long execution time), and the fact that they are tested mainly by inspection of output data quality. It is therefore crucial for developers to reason about data through each step in the pipeline, starting from the raw input; this information is akin to data provenance in a relational setting. Past efforts for capturing data provenance for scripting languages have required substantial manual modifications to the scripts, or else yield information that is too inflexible for many debugging tasks.We instead propose a "human-in-the-loop" provenance generation model with three key improvements: (1) allowing humans to express the desired provenance through a provenance schema, (2) enabling one-time execution capture of scripts to produce traces that are later combined with different provenance schemata to yield useful provenance for different tasks, (3) providing a modular rule-based recommendation component to help design provenance schemata through a user interaction interface. We describe the concepts, the user experience with our system, explain the system components, and present preliminary experiment results.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {5},
numpages = {7},
location = {Philadelphia, Pennsylvania},
series = {HILDA '22}
}

@article{10.5555/2809974.2809975,
author = {K\"{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang},
title = {Possible and Certain SQL Keys},
year = {2015},
issue_date = {July 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {11},
issn = {2150-8097},
abstract = {Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1118–1129},
numpages = {12}
}

@article{10.14778/3297753.3297757,
author = {Li, Yanying and Sun, Haipei and Dong, Boxiang and Wang, Hui (Wendy)},
title = {Cost-Efficient Data Acquisition on Online Data Marketplaces for Correlation Analysis},
year = {2018},
issue_date = {December 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3297753.3297757},
doi = {10.14778/3297753.3297757},
abstract = {Incentivized by the enormous economic profits, the data marketplace platform has been proliferated recently. In this paper, we consider the data marketplace setting where a data shopper would like to buy data instances from the data marketplace for correlation analysis of certain attributes. We assume that the data in the marketplace is dirty and not free. The goal is to find the data instances from a large number of datasets in the marketplace whose join result not only is of high-quality and rich join informativeness, but also delivers the best correlation between the requested attributes. To achieve this goal, we design DANCE, a middleware that provides the desired data acquisition service. DANCE consists of two phases: (1) In the off-line phase, it constructs a two-layer join graph from samples. The join graph includes the information of the datasets in the marketplace at both schema and instance levels; (2) In the online phase, it searches for the data instances that satisfy the constraints of data quality, budget, and join informativeness, while maximizing the correlation of source and target attribute sets. We prove that the complexity of the search problem is NP-hard, and design a heuristic algorithm based on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark and one real datasets demonstrate the efficiency and effectiveness of our heuristic data acquisition algorithm.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {362–375},
numpages = {14}
}

@inproceedings{10.1145/3414274.3414505,
author = {Fu, Qingwen and Zhu, Jiahui and Chen, Yuepeng and Wan, Jintao and He, Bin},
title = {An Automatic Learning Model for Trajectory Outlier Detection},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414505},
doi = {10.1145/3414274.3414505},
abstract = {The rapid development of global positioning system has given birth to a large number of spatial-temporal data, and there are many outliers of points obviously in these trajectory data. It is very important to detect the outliers in the trajectory to improve the data quality and accuracy of trajectory mining. In this paper, we propose a trajectory outlier detection algorithm based on bi-directional long short-term memory model and attention mechanism. Firstly, an eight-dim eigenvector is extracted from each point of trajectory, and then a two-layer bi-directional long short-term memory model is constructed. Finally, representing the trajectory points in an interactive way which is called attention mechanism. The input of the model is the trajectory point with a certain length, and the output is the type of the trajectory point. The model can automatically learn the difference between the normal point and the adjacent abnormal point with motion features. Experimental dataset based on real trajectory data of taxi from Beijing, and results showed that the performance of this algorithm is significantly better than constant speed threshold method or classical machine learning classification. Especially the precision and recall reaches 0.93 and 0.90 separately, which proves the effectiveness of this algorithm.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {220–226},
numpages = {7},
keywords = {spatial-temporal data, attention mechanism, outlier detection, Bi-LSTM},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/3184558.3192324,
author = {Spaniol, Marc and Baeza-Yates, Ricardo and Masan\`{e}s, Julien},
title = {TempWeb 2018 Chairs' Welcome and Organization},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3192324},
doi = {10.1145/3184558.3192324},
abstract = {Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1729–1730},
numpages = {2},
keywords = {temporal web analytics, data quality metrics, content evolution on the web, web science, web trends, web scale data analytics, time aware web archiving, web spam evolution, large scale data storage, terminology evolution, large scale data processing, web dynamics, data aggregation, topic mining, community detection and evolution, systematic exploitation of web archives, distributed data analytics},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1109/TNET.2021.3105427,
author = {Shi, Zhiguo and Yang, Guang and Gong, Xiaowen and He, Shibo and Chen, Jiming},
title = {Quality-Aware Incentive Mechanisms Under Social Influences in Data Crowdsourcing},
year = {2022},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3105427},
doi = {10.1109/TNET.2021.3105427},
abstract = {Incentive mechanism design and quality control are two key challenges in data crowdsourcing, because of the need for recruitment of crowd users and their limited capabilities. Without considering users’ social influences, existing mechanisms often result in low efficiency in terms of the platform’s cost. In this paper, we exploit social influences among users as incentives to motivate users’ participation, in order to reduce the cost of recruiting users. Based on social influences, we design incentive mechanisms with the goal of achieving high quality of crowdsourced data and low cost of incentivizing users’ participation. Specifically, we consider three scenarios. In the full information scenario, we design task assignment and user recruitment mechanisms to optimize the data quality while reducing the incentive cost. In the partial information scenario, users’ qualities and costs are unknown. We exploit the correlation between tasks to overcome the information asymmetry, for both cases of opportunistic crowdsourcing and participatory crowdsourcing. Further, in the dynamic social influence scenario, we investigate the dynamics of users’ social influences and design extra rewards for users to make full use of the social influence and achieve maximum cost saving. We evaluate the incentive mechanisms using numerical results, which demonstrate their effectiveness.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {176–189},
numpages = {14}
}

@inproceedings{10.1145/3383783.3383793,
author = {Dvo\v{r}\'{a}kov\'{a}, Eli\v{s}ka and Kumar, Sajal and Kl\'{e}ma, Ji\v{r}\'{\i} and \v{Z}elezn\'{y}, Filip and Drbal, Karel and Song, Mingzhou},
title = {Evaluating Model-Free Directional Dependency Methods on Single-Cell RNA Sequencing Data with Severe Dropout},
year = {2019},
isbn = {9781450372183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383783.3383793},
doi = {10.1145/3383783.3383793},
abstract = {As severe dropout in single-cell RNA sequencing (scRNA-seq) degrades data quality, current methods for network inference face increased uncertainty from such data. To examine how dropout influences directional dependency inference from scRNA-seq data, we thus studied four methods based on discrete data that are model-free without parametric model assumptions. They include two established methods: conditional entropy and Kruskal-Wallis test, and two recent methods: causal inference by stochastic complexity and function index. We also included three non-directional methods for a contrast. On simulated data, function index performed most favorably at varying dropout rates, sample sizes, and discrete levels. On an scRNA-seq dataset from developing mouse cerebella, function index and Kruskal-Wallis test performed favorably over other methods in detecting expression of developmental genes as a function of time. Overall among the four methods, function index is most resistant to dropout for both directional and dependency inference. The next best choice, Kruskal-Wallis test, carries a directional bias towards a uniformly distributed variable. We conclude that a method robust to marginal distributions with a sufficiently large sample size can reap benefits of single-cell over bulk RNA sequencing in understanding molecular mechanisms at the cellular resolution.},
booktitle = {Proceedings of the 2019 6th International Conference on Bioinformatics Research and Applications},
pages = {55–62},
numpages = {8},
keywords = {single-cell sequencing, directional dependency, model-free},
location = {Seoul, Republic of Korea},
series = {ICBRA '19}
}

@inproceedings{10.1145/3137133.3137140,
author = {Jia, Ruoxi and Sangogboye, Fisayo Caleb and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun},
title = {PAD: Protecting Anonymity in Publishing Building Related Datasets},
year = {2017},
isbn = {9781450355445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3137133.3137140},
doi = {10.1145/3137133.3137140},
abstract = {The diffusion of low-cost sensor network technologies in smart buildings has enabled the collection of massive amounts of data regarding indoor environments, energy use and occupants, which, in turn, creates opportunities for knowledge- and information-based building management. Driven by benefits mutual to occupants, building managers, and research communities, there is a demand for data publication to foster more sophisticated and robust models and algorithms. Data in the original form, however, contains sensitive information about occupants' behavioral patterns, and publishing such data will violate individuals' privacy. The current practice on publishing building-related datasets relies primarily on policies for dictating which types of data can be published and agreements on the use of published data. This approach alone provides insufficient protection as it does not prevent privacy breaches from occurring in the first place.In this paper, we present PAD, which to our knowledge is the first system that provides a technological solution for publishing building related datasets in a privacy-preserving manner while maintaining high data quality. PAD is able to offer a strong anonymity guarantee by perturbing data records. The unique feature of PAD is that it offers an interface to incorporate dataset users into the loop of data publication and customizes the perturbation such that useful information in the dataset can be better retained. We study the efficacy of PAD using occupancy and plug load data collected in real buildings. The experiments demonstrate that PAD can achieve high resilience to privacy threats without introducing any significant data fidelity penalties.},
booktitle = {Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments},
articleno = {4},
numpages = {10},
keywords = {convex optimization, occupancy privacy, k-anonymity, clustering},
location = {Delft, Netherlands},
series = {BuildSys '17}
}

@article{10.1145/3501295,
author = {Zafar, Farkhanda and Khattak, Hasan Ali and Aloqaily, Moayad and Hussain, Rasheed},
title = {Carpooling in Connected and Autonomous Vehicles: Current Solutions and Future Directions},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3501295},
doi = {10.1145/3501295},
abstract = {Owing to the advancements in communication and computation technologies, the dream of commercialized connected and autonomous cars is becoming a reality. However, among other challenges such as environmental pollution, cost, maintenance, security, and privacy, the ownership of vehicles (especially for Autonomous Vehicles) is the major obstacle in the realization of this technology at the commercial level. Furthermore, the business model of pay-as-you-go type services further attracts the consumer, because there is no need for upfront investment. In this vein, the idea of car-sharing (aka carpooling) is getting ground due to, at least in part, its simplicity, cost-effectiveness, and affordable choice of transportation. Carpooling systems are still in their infancy and face challenges such as scheduling, matching passengers interests, business model, security, privacy, and communication. To date, a plethora of research work has already been done covering different aspects of carpooling services (ranging from applications to communication and technologies); however, there is still a lack of a holistic, comprehensive survey that can be a one-stop-shop for the researchers in this area to (i) find all the relevant information and (ii) identify the future research directions. To fill these research challenges, this article provides a comprehensive survey on carpooling in autonomous and connected vehicles and covers architecture, components, and solutions, including scheduling, matching, mobility, pricing models of carpooling. We also discuss the current challenges in carpooling and identify future research directions. This survey is aimed to spur further discussion among the research community for the effective realization of carpooling.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {218},
numpages = {36},
keywords = {vehicular networks, ride-sharing, carpooling, Connected autonomous vehicles, intelligent transportation systems}
}

@article{10.1145/3131611,
author = {Chen, Qingyu and Wan, Yu and Zhang, Xiuzhen and Lei, Yang and Zobel, Justin and Verspoor, Karin},
title = {Comparative Analysis of Sequence Clustering Methods for Deduplication of Biological Databases},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3131611},
doi = {10.1145/3131611},
abstract = {The massive volumes of data in biological sequence databases provide a remarkable resource for large-scale biological studies. However, the underlying data quality of these resources is a critical concern. A particular challenge is duplication, in which multiple records have similar sequences, creating a high level of redundancy that impacts database storage, curation, and search. Biological database deduplication has two direct applications: for database curation, where detected duplicates are removed to improve curation efficiency, and for database search, where detected duplicate sequences may be flagged but remain available to support analysis.Clustering methods have been widely applied to biological sequences for database deduplication. Since an exhaustive all-by-all pairwise comparison of sequences cannot scale for a high volume of data, heuristic approaches have been recruited, such as the use of simple similarity thresholds. In this article, we present a comparison between CD-HIT and UCLUST, the two best-known clustering tools for sequence database deduplication. Our contributions include a detailed assessment of the redundancy remaining after deduplication, application of standard clustering evaluation metrics to quantify the cohesion and separation of the clusters generated by each method, and a biological case study that assesses intracluster function annotation consistency to demonstrate the impact of these factors on a practical application of the sequence clustering methods. Our results show that the trade-off between efficiency and accuracy becomes acute when low threshold values are used and when cluster sizes are large. This evaluation leads to practical recommendations for users for more effective uses of the sequence clustering tools for deduplication.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {17},
numpages = {27},
keywords = {databases, clustering, Deduplication, validation}
}

@inproceedings{10.1145/2623330.2623685,
author = {Li, Furong and Lee, Mong Li and Hsu, Wynne},
title = {Entity Profiling with Varying Source Reliabilities},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623685},
doi = {10.1145/2623330.2623685},
abstract = {The rapid growth of information sources on the Web has intensified the problem of data quality. In particular, the same real world entity may be described by different sources in various ways with overlapping information, and possibly conflicting or even erroneous values. In order to obtain a more complete and accurate picture for a real world entity, we need to collate the data records that refer to the entity, as well as correct any erroneous values. We observe that these two tasks are often tightly coupled: rectifying erroneous values will facilitate data collation, while linking similar records provides us with a clearer view of the data and additional evidence for error correction. In this paper, we present a framework called Comet that interleaves record linkage with error correction, taking into consideration the source reliabilities on various attributes. The proposed framework first utilizes confidence based matching to discriminate records in terms of ambiguity and source reliability. Then it performs adaptive matching to reduce the impact of erroneous values. Experiment results demonstrate that Comet outperforms the state-of-the-art techniques and is able to build complete and accurate profiles for real world entities.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1146–1155},
numpages = {10},
keywords = {source reliability, entity profiling, record linkage, truth discovery},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1145/3409501.3409542,
author = {Hongmeng, Zhang and Zhiqiang, Zhu and Lei, Sun and Xiuqing, Mao and Yuehan, Wang},
title = {A Detection Method for DeepFake Hard Compressed Videos Based on Super-Resolution Reconstruction Using CNN},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409501.3409542},
doi = {10.1145/3409501.3409542},
abstract = {The DeepFake video detection method based on convolutional neural networks has a poor performance in the dataset of hard compressed DeepFake video. And a large number of false tests will occur to the real data. To solve this problem, a networks model detection method for super-resolution reconstruction of DeepFake video is proposed. First of all, the face area of real data is processed by Gaussian blur, which is converted into negative data, and the real data and processing data are input into neural network for training. Then the residual network is used for super-resolution reconstruction of test data. Finally, the trained model is used to test the video after super-resolution reconstruction. Experiments show that the proposed method can reduce the false detection rate and improve the accuracy in detection of single frames.},
booktitle = {Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
pages = {98–103},
numpages = {6},
keywords = {Deep Learning, Hard compressed video, Super-resolution reconstruction, DeepFake detection},
location = {Qingdao, China},
series = {HPCCT &amp; BDAI 2020}
}

@article{10.1145/2808198,
author = {Hao, Fei and Jiao, Mingjie and Min, Geyong and Yang, Laurence T.},
title = {Launching an Efficient Participatory Sensing Campaign: A Smart Mobile Device-Based Approach},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2808198},
doi = {10.1145/2808198},
abstract = {Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {oct},
articleno = {18},
numpages = {22},
keywords = {recruitment, DTA, deployment, trajectory, Participatory sensing, tensor}
}

@inproceedings{10.1145/3012071.3012077,
author = {Madera, Cedrine and Laurent, Anne},
title = {The next Information Architecture Evolution: The Data Lake Wave},
year = {2016},
isbn = {9781450342674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012071.3012077},
doi = {10.1145/3012071.3012077},
abstract = {Data warehouses and data marts have long been considered as the unique solution for providing end-users with decisional information. More recently, data lakes have been proposed in order to govern data swamps. However, no formal definition has been proposed in the literature. Existing works are not complete and miss important parts of the topic. In particular, they do not focus on the influence of the data gravity, the infrastructure role of those solutions and of course are proposing divergent definitions and positioning regarding the usage and the interaction with existing decision support system.In this paper, we propose a novel definition of data lakes, together with a comparison with other over several criteria as the way to populate them, how to use, what is the Data Lake end user profile. We claim that data lakes are complementary components in decisional information systems and we discuss their position and interactions regarding the other components by proposing an interaction model.},
booktitle = {Proceedings of the 8th International Conference on Management of Digital EcoSystems},
pages = {174–180},
numpages = {7},
keywords = {internet of things, data lab, digital transformation, data governance, data lakes, data laboratory, data warehouses, data reservoirs},
location = {Biarritz, France},
series = {MEDES}
}

@inproceedings{10.1145/2463676.2465337,
author = {Golab, Lukasz and Johnson, Theodore},
title = {Data Stream Warehousing},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465337},
doi = {10.1145/2463676.2465337},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {949–952},
numpages = {4},
keywords = {data streams, real-time analytics, data warehousing},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@article{10.1145/2430456.2430472,
author = {Dong, Xin Luna and Dragut, Eduard Constantin},
title = {10th International Workshop on Quality in Databases: QDB 2012},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2430456.2430472},
doi = {10.1145/2430456.2430472},
journal = {SIGMOD Rec.},
month = {jan},
pages = {55–59},
numpages = {5}
}

@inproceedings{10.1145/3333165.3333180,
author = {Elmekki, Hanae and Chiadmi, Dalila and Lamharhar, Hind},
title = {Open Government Data: Towards a Comparison of Data Lifecycle Models},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333180},
doi = {10.1145/3333165.3333180},
abstract = {Government, through Open Government Data "OGD, becomes one of the important producers of open data. OGD is an opportunity to create valuable services and innovative products useful for citizens as a primarily targeted consumer. However, the expected benefits of OGD are not yet met. That is to say, several research communities' studies insist on the necessity of creating valuable data in order to generate valuable services. These studies are still insufficient for a shared understanding of how OGD contribute to the creation of value. For this purpose, this paper presents a review of a set of data lifecycle models compared against their contribution to the creation of value in the context of OGD.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {15},
numpages = {6},
keywords = {data value creation, data lifecycle, Open Government Data},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@inproceedings{10.1145/3041021.3053059,
author = {Tao, Shibo and Wang, Xiaorong and Huang, Weijing and Chen, Wei and Wang, Tengjiao and Lei, Kai},
title = {From Citation Network to Study Map: A Novel Model to Reorganize Academic Literatures},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3053059},
doi = {10.1145/3041021.3053059},
abstract = {As the number of academic papers and new technologies soars, it has been increasingly difficult for researchers, especially beginners, to enter a new research field. Researchers often need to study a promising paper in depth to keep up with the forefront of technology. Traditional Query-Oriented study method is time-consuming and even tedious. For a given paper, existent academic search engines like Google Scholar tend to recommend relevant papers, failing to reveal the knowledge structure. The state-of-the-art Map-Oriented study methods such as AMiner and AceMap can structure scholar information, but they're too coarse-grained to dig into the underlying principles of a specific paper. To address this problem, we propose a Study-Map Oriented method and a novel model called RIDP (Reference Injection based Double-Damping PageRank) to help researchers study a given paper more efficiently and thoroughly. RIDP integrates newly designed Reference Injection based Topic Analysis method and Double-Damping PageRank algorithm to mine a Study Map out of massive academic papers in order to guide researchers to dig into the underlying principles of a specific paper. Experiment results on real datasets and pilot user studies indicate that our method can help researchers acquire knowledge more efficiently, and grasp knowledge structure systematically.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1225–1232},
numpages = {8},
keywords = {reference injection, topic analysis, academic papers, double-damping pagerank, study map},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3482632.3484077,
author = {Wang, Chunxia and Xie, Jian},
title = {Constructing a Computer Model for Discipline Data Governance Using the Contingency Theory and Data Mining},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484077},
doi = {10.1145/3482632.3484077},
abstract = {Data governance is an important part of modernizing the governance capacity of universities. Discipline data governance plays an important role in promoting the development of university disciplines, and is a key factor in improving the governance of university disciplines, the science of educational decision-making and the effectiveness of management. It is an important way to promote the "precision" and "science" of discipline governance. In this paper, we construct a model of discipline data governance based on the Contingency Theory with a view to shedding light on discipline governance.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1967–1970},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3444370.3444575,
author = {Zhang, Bo and Kong, Dehua},
title = {Dynamic Estimation Model of Insurance Product Recommendation Based on Naive Bayesian Model},
year = {2020},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444575},
doi = {10.1145/3444370.3444575},
abstract = {Aiming at the dynamic estimation of insurance product recommendation, considering the particularity and complexity of purchasing insurance product and the uncertainty of influencing factors, a dynamic estimation model of insurance product recommendation based on Naive Bayes is proposed. The model combines customer insurance information with machine learning. The results show that the naive Bayesian classification algorithm can be compared with the decision tree and neural network classification algorithm, showing high accuracy and high speed.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {219–224},
numpages = {6},
keywords = {dynamic estimation, Naive Bayes, recommendation, insurance products},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@inproceedings{10.1145/3035918.3054772,
author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
title = {Data Profiling: A Tutorial},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3054772},
doi = {10.1145/3035918.3054772},
abstract = {is to understand the dataset at hand and its metadata. The process of metadata discovery is known as data profiling. Profiling activities range from ad-hoc approaches, such as eye-balling random subsets of the data or formulating aggregation queries, to systematic inference of structural information and statistics of a dataset using dedicated profiling tools. In this tutorial, we highlight the importance of data profiling as part of any data-related use-case, and we discuss the area of data profiling by classifying data profiling tasks and reviewing the state-of-the-art data profiling systems and techniques. In particular, we discuss hard problems in data profiling, such as algorithms for dependency discovery and profiling algorithms for dynamic data and streams. We also pay special attention to visualizing and interpreting the results of data profiling. We conclude with directions for future research in the area of data profiling. This tutorial is based on our survey on profiling relational data [2].},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1747–1751},
numpages = {5},
keywords = {data exploration, dependency discovery, data profiling},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3384544.3384588,
author = {Nugroho, Heru and Gumilang, Soni Fajar Surya},
title = {Recommendations for Improving Data Management Process in Government of Bandung Regency Using COBIT 4.1 Framework},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384588},
doi = {10.1145/3384544.3384588},
abstract = {Data is an valuable asset that potentially provides substantial benefits for the government and society. To make the performance of local government apparatus runs optimally and the public gets the best service, the government of Bandung Regency strives to improve data management. The initial stage of optimizing data management is the assessment of the maturity level in managing data (DS-11) using COBIT 4.1. Base on the assessment maturity level for DS-11, the government of Bandung Regency needs to raise the level from 2.46 (Repeatable but Intuitive) to 3.0 (Defined). Recommendations given to improve data management in Government with focuses on maintaining the completeness, accuracy, availability, and protection of data.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {57–61},
numpages = {5},
keywords = {Recommendations, Data, Maturity, DS-11, COBIT 4.1},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@article{10.1145/2737817.2737831,
author = {Pedersen, Torben Bach and Castellanos, Malu and Dayal, Umesh},
title = {Report on the Seventh International Workshop on Business Intelligence for the Real Time Enterprise (BIRTE 2013)},
year = {2015},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2737817.2737831},
doi = {10.1145/2737817.2737831},
journal = {SIGMOD Rec.},
month = {feb},
pages = {55–58},
numpages = {4}
}

@article{10.1145/3190579,
author = {Bertino, Elisa and Jahanshahi, Mohammad R.},
title = {Adaptive and Cost-Effective Collection of High-Quality Data for Critical Infrastructure and Emergency Management in Smart Cities—Framework and Challenges},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3190579},
doi = {10.1145/3190579},
journal = {J. Data and Information Quality},
month = {may},
articleno = {1},
numpages = {6},
keywords = {edge computing, Civil engineering, device swarms}
}

@inproceedings{10.1145/3368756.3368965,
author = {Rhazal, Oumaima El and Tomader, Mazri},
title = {Study of Smart City Data: Categories and Quality Challenges},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3368965},
doi = {10.1145/3368756.3368965},
abstract = {Lately, the world attention is directed to transforming daily life to a smarter one, we cannot deny the smart city concept that became pervading. This concept will give every device the chance to communicate with other devices, it will simply create the smarter version of everything. However, data heterogeneity and quality changes are one of the best priorities and challenges that should be handled in this promising concept.In this paper we present a review about data categories circulating in a smart city depending on its required services. We also study the quality of information as one of both, major challenges and treasures in a smart city.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {4},
numpages = {7},
keywords = {quality of information (QoI), smart city, internet of things (IoT)},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@book{10.1145/3453538,
author = {ACM Data Science Task Force},
title = {Computing Competencies for Undergraduate Data Science Curricula},
year = {2021},
isbn = {9781450390606},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@inproceedings{10.1145/3463677.3463696,
author = {Geci, Mentor and CsAki, Csaba},
title = {BOLD in National Budget Planning – a Comparison of International Cases: BOLD in National Budget Planning},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463696},
doi = {10.1145/3463677.3463696},
abstract = {Anticipating the continues increase in quantity and consequently their importance, data are becoming a new currency. Overall purpose of this paper is to show the link between Big Open Linked Data (BOLD) and national budget planning. The methodology that follows is a qualitative case-study based approach leading to a comparative analysis of five cases. Research problems investigated are the commonalities and differences that may be identified in the handling of national budget data in developing countries, as well as best practices or potential ‘lessons learned’ from international cases of handling budget data as BOLD. In addition, the study investigates how Kosovo as a young developing country may benefit from the experiences of other countries. To create a framework of analysis, international cases are reviewed. Their comparison reveals that there are not that many commonalities among these developing countries in terms of issues and challenges regarding the use of BOLD in national budgeting. As it is mainly country specific, the approach used toward BOLD largely depends on the general landscape of each country. In comparison, although there is some progress made, Kosovo is still behind those countries in terms of applying BOLD.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {189–197},
numpages = {9},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@article{10.1145/2983463,
author = {Peek, Geerten and Taspinar, Ahmet},
title = {One Thousand Interviews},
year = {2016},
issue_date = {Fall 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2983463},
doi = {10.1145/2983463},
abstract = {How customer insights keep one company agile, and challenge these data scientist to stay ahead in an ever-changing world.},
journal = {XRDS},
month = {sep},
pages = {11–12},
numpages = {2}
}

@article{10.1145/3015456,
author = {Cao, Longbing},
title = {Data Science: Challenges and Directions},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3015456},
doi = {10.1145/3015456},
abstract = {While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.},
journal = {Commun. ACM},
month = {jul},
pages = {59–68},
numpages = {10}
}

@inproceedings{10.1145/3379310.3379322,
author = {Rozi, Muhamad Fahru and Sucahyo, Yudho Giri and Gandhi, Arfive and Ruldeviyani, Yova},
title = {Appraising Personal Data Protection in Startup Companies in Financial Technology: A Case Study of ABC Corp},
year = {2020},
isbn = {9781450376853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379310.3379322},
doi = {10.1145/3379310.3379322},
abstract = {Financial Technology (fintech) has been immerged extensively in the last decade. In the realm of disruptive world, there are many areas in which startup companies are developing their business. There is always contradiction when dealing with innovation as core of digital disruption and how privacy remains as hot issues at the edge of everybody's talks. Internet plays important roles to sustain the trends. As rapidly growing country, 68% of Indonesian has access to the Internet. It drives startup companies on financial technology to innovate more and besides that they must comply to regulation in regard with personal data protection. This research aims to appraise how startup company on financial technology protect users' personal data. Personal data protection principles from international organization and Indonesian regulation regarding personal data protection are used to appraise how ABC Corp as a startup company that deliver financial technology service in Indonesian society. To ensure that its service is qualified and trustable, ABC Corp should be appraised using relevant criteria and qualitative approach. The results showed that most of regulations from sectorial supervising agency have been adhered by ABC Corp. The results bring meaningful insight to improve performance on personal data protection. They can became lessons for similar emerging startup companies in financial technology when acquiring their qualifications to protect users' personal data and keep their sustainability.},
booktitle = {Proceedings of the 2020 2nd Asia Pacific Information Technology Conference},
pages = {9–15},
numpages = {7},
keywords = {data privacy, Financial technology, personal data, digital economy, data protection},
location = {Bali Island, Indonesia},
series = {APIT 2020}
}

@article{10.1145/3511322.3511326,
author = {Dennis, Louise A.},
title = {Conference Reports},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3511322.3511326},
doi = {10.1145/3511322.3511326},
abstract = {This section is compiled from reports of recent events sponsored or run in cooperation with ACM SIGAI. In general these reports were written and submitted by the conference organisers.},
journal = {AI Matters},
month = {jan},
pages = {15–17},
numpages = {3}
}

@article{10.14778/2824032.2824073,
author = {Qiao, Lin and Li, Yinan and Takiar, Sahil and Liu, Ziyang and Veeramreddy, Narasimha and Tu, Min and Dai, Ying and Buenrostro, Issac and Surlaker, Kapil and Das, Shirshanka and Botev, Chavdar},
title = {Gobblin: Unifying Data Ingestion for Hadoop},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824073},
doi = {10.14778/2824032.2824073},
abstract = {Data ingestion is an essential part of companies and organizations that collect and analyze large volumes of data. This paper describes Gobblin, a generic data ingestion framework for Hadoop and one of LinkedIn's latest open source products. At LinkedIn we need to ingest data from various sources such as relational stores, NoSQL stores, streaming systems, REST endpoints, filesystems, etc. into our Hadoop clusters. Maintaining independent pipelines for each source can lead to various operational problems. Gobblin aims to solve this issue by providing a centralized data ingestion framework that makes it easy to support ingesting data from a variety of sources.Gobblin distinguishes itself from similar frameworks by focusing on three core principles: generality, extensibility, and operability. Gobblin supports a mixture of data sources out-of-the-box and can be easily extended for more. This enables an organization to use a single framework to handle different data ingestion needs, making it easy and inexpensive to operate. Moreover, with an end-to-end metrics collection and reporting module, Gobblin makes it simple and efficient to identify issues in production.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1764–1769},
numpages = {6}
}

@inproceedings{10.1145/3195106.3195177,
author = {Baolong, Yang and Hong, Wu and Haodong, Zhang},
title = {Research and Application of Data Management Based on Data Management Maturity Model (DMM)},
year = {2018},
isbn = {9781450363532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195106.3195177},
doi = {10.1145/3195106.3195177},
abstract = {Through the analysis and contrast of the different Data Management Maturity Model, such as DCAM, DMM, DCMM and the model of IBM, we try to make empirical research under the framework of data management maturity model. This article take a project whose main research object is about the academic career of scientists and with massive unstructured data for example, through analysis of the goal, management processes and influence factors of this project in detail, we built up an evaluation system for data management for such projects under the framework of DCMM. It is expected to have a positive significance to the evaluation of similar data management capability.},
booktitle = {Proceedings of the 2018 10th International Conference on Machine Learning and Computing},
pages = {157–160},
numpages = {4},
keywords = {Unstructured data, Measurement and evaluation, Data management, maturity model},
location = {Macau, China},
series = {ICMLC 2018}
}

@inproceedings{10.1145/2656450.2656453,
author = {Kumar, Sathish Alampalayam},
title = {Designing a Graduate Program in Information Security and Analytics: Masters Program in Information Security and Analytics (MISA)},
year = {2014},
isbn = {9781450326865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656450.2656453},
doi = {10.1145/2656450.2656453},
abstract = {This paper introduces the concept of the Master of Information Security and Analytics (MISA) program for the graduate students with a background in CS, IS and IT. The 10-course graduate level program is benchmarked against existing masters programs in the areas of Information Security and Data Analytics, and an assessment was done on the estimated demand for MISA graduates in the nation. The program outcomes were then mapped against the course objectives to insure the correct mix of courses and topics. The program's admission requirement is also being discussed. This paper discusses the design process and possible ways to reduce risk in the start-up of a new degree program. How a program is marketed to prospective students and what program graduates will do after program completion is just as important as the initial design of the program. Planning for the administration of the program and the assessment process is an important phase of the initial design.},
booktitle = {Proceedings of the 15th Annual Conference on Information Technology Education},
pages = {141–146},
numpages = {6},
keywords = {analytics, information technology education, cybersecurity},
location = {Atlanta, Georgia, USA},
series = {SIGITE '14}
}

@inproceedings{10.1145/3463677.3463732,
author = {Ahn, Michael and Chu, Shengli},
title = {What Matters in Maintaining Effective Open Government Data Systems? The Role of Government Managerial Capacity, and Political and Legal Environment},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463732},
doi = {10.1145/3463677.3463732},
abstract = {This paper aims to identify key institutional factors that contribute to effective open data systems. Rapid advancement in new technologies such as machine learning, algorithms, IoT, and Cloud Computing has amplified the importance of national open data systems. The availability of relevant public data has become a crucial factor in creating sophisticated machine learning platforms or algorithms that will have a considerable impact on national competitiveness. Effective national open data strategies will matter in shaping an environment that will facilitate data production, dissemination, and utilization. Using multiple sources of data that measure the qualities of open data systems and various political, governmental, and legal attributes at the national level, we seek to identify key institutional factors that contribute to robust open data policies and outcomes. Our findings point to the importance of the existence of a national open data strategy and support (especially "open by default" strategy), pre-existing e-government capability, and countries operating under full democracy with its guarantees to civil liberties and political freedom. In addition, the nature of the open data matters as different managerial, political, and demographic conditions affected the quality of different open data systems. Policy implications of our findings are discussed.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {444–457},
numpages = {14},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@inproceedings{10.1145/3487075.3487110,
author = {An, Zhenpeng and Zhang, Di and Liang, Yunjie},
title = {Research on Data Governance Framework for Fire Department},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487110},
doi = {10.1145/3487075.3487110},
abstract = {This paper analyzes data governance elements, models and frameworks, provides a clear plan for data governance for fire department. Using the method of literature research, network investigation and conclude data system of fire departments, the china domestic and foreign research status of data governance is reviewed. We build the framework of data governance for fire department, including Data resource directory system, Data technology support system and Data standardization system. This paper preliminarily forms the framework of data governance for fire department. This framework was applied to the fire information planning work. The results indicate that based on the status and characteristics of fire industry, the implementation of this framework is effective and feasible, and it is also the basis of standard fire control data governance in future.},
booktitle = {The 5th International Conference on Computer Science and Application Engineering},
articleno = {35},
numpages = {5},
keywords = {Data governance, Data standard system, Fire Department component;},
location = {Sanya, China},
series = {CSAE 2021}
}

@inproceedings{10.1145/3192975.3193004,
author = {Nabipourshiri, Rouzbeh and Abu-Salih, Bilal and Wongthongtham, Pornpit},
title = {Tree-Based Classification to Users' Trustworthiness in OSNs},
year = {2018},
isbn = {9781450364102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192975.3193004},
doi = {10.1145/3192975.3193004},
abstract = {In the light of the information revolution, and the propagation of big social data, the dissemination of misleading information is certainly difficult to control. This is due to the rapid and intensive flow of information through unconfirmed sources under the propaganda and tendentious rumors. This causes confusion, loss of trust between individuals and groups and even between governments and their citizens. This necessitates a consolidation of efforts to stop penetrating of false information through developing theoretical and practical methodologies aim to measure the credibility of users of these virtual platforms. This paper presents an approach to domain-based prediction to user's trustworthiness of Online Social Networks (OSNs). Through incorporating three machine learning algorithms, the experimental results verify the applicability of the proposed approach to classify and predict domain-based trustworthy users of OSNs.},
booktitle = {Proceedings of the 2018 10th International Conference on Computer and Automation Engineering},
pages = {190–194},
numpages = {5},
keywords = {Trust, data mining, Twitter, social media, users' trustworthiness, machine learning},
location = {Brisbane, Australia},
series = {ICCAE 2018}
}

@inproceedings{10.1145/2968219.2985840,
author = {Hui, Pan and Ou, Zhonghong and Zhang, Yanyong and Striegel, Aaron D},
title = {The 7th International Workshop on Hot Topics in Planet-Scale Measurement (HotPlanet '16)},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2985840},
doi = {10.1145/2968219.2985840},
abstract = {The recent advances of mobile devices, online social networks, and the emergence of the Internet of Things have driven the corresponding data collection and analytics to planetary scale. It is, thus, essential to provide a forum to discuss the technical advances, share the lessons, experiences, and challenges associated with real-world large-scale deployment. The 7th International Workshop on Hot Topics in Planet-Scale Measurement (HotPlanet '16) is to provide such a forum for the researchers and practitioners in the fields mentioned above. By bringing together the experts in these fields, and through thoughtful discussions and valuable sharing, HotPlanet '16 aims to advance the work in these fields forward.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1275–1278},
numpages = {4},
keywords = {planet-scale measurement, crowdsourcing, data analytics, cloud computing, social computing, crowd sensing, deployment experiences},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3543712.3543749,
author = {Raab, Raphaele and Granigg, Wolfgang and Melcher, Michael},
title = {Need for Skilled Workers in the Area of Data Science and Cloud Computing in Styria},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543749},
doi = {10.1145/3543712.3543749},
abstract = {The aim of this paper is to discuss the results of a survey conducted to assess the need for skilled workers in the areas Data Science &amp; Cloud Computing in Styria, Austria. Firstly, the relevant roles and skills in the abovementioned areas had to be selected. Initially, this selection process is described. Consequently, a survey was designed and given to a representative group of companies. The survey includes questions regarding the need for skilled workers with respect to the domains and the selected skills in the areas Data Science &amp; Cloud Computing. Moreover, the respondents were asked about the importance of further education and the necessity of academic education in these areas. Overall, our survey concludes that the requirements for skilled workers in the areas of Data Science and Cloud Computing in Styria will increase significantly in the coming years.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {28–34},
numpages = {7},
keywords = {cloud computing, statistical analysis, cloud platform expertise, data science, machine learning, Styria, business intelligence, personnel requirement, artificial intelligence, data driven innovation, data management},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@inproceedings{10.1145/3357384.3360314,
author = {Duan, Rong and Xiao, Yanghua},
title = {Enterprise Knowledge Graph From Specific Business Task to Enterprise Knowledge Management},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3360314},
doi = {10.1145/3357384.3360314},
abstract = {Data driven Knowledge Graph is rapidly adapted by different societies. Many open domain and specific domain knowledge graphs have been constructed, and many industries have benefited from knowledge graph. Currently, enterprise related knowledge graph is classified as specific domain, but the applications span from solving a narrow specific problem to Enterprise Knowledge Management system. With the digital transform of traditional industry, Enterprise knowledge becomes more and more complicated, it involves knowledge from common domain, multiple specific domains, and corporate-specific in general. This tutorial provides an overview of current Enterprise Knowledge Graph(EKG). It distinguishes the EKG from specific domain according to the knowledge it covers, and provides the examples to illustrate the difference between EKG and specific domain KG. The tutorial further summarizes EKG into three types: Specific Business Task Enterprise KG, Specific Business Unit Enterprise KG and Cross Business Unit Enterprise KG, and illustrates the characteristics, steps, challenges, and future research in constructing and consuming of each of these three types of EKG .},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2965–2966},
numpages = {2},
keywords = {relation extraction, entity recognition, enterprise knowledge management, ontology, knowledge graph},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3085228.3085280,
author = {Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega},
title = {Competitive Capability Framework for Open Government Data Organizations},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085280},
doi = {10.1145/3085228.3085280},
abstract = {Open data-driven organizations compete in a complex and uncertain environment with growing global competition, changing and emerging demand and market, and increasing levels of analytical tools and technology. For these organizations to exploit open data for competitive advantage, they need to develop the requisite competitive capabilities. This article presents an open data competitive capability framework grounded in theory and practice of open data. Based on extant literature and insights from domain experts, we identify and describe four dimensions of competitive capabilities required for open data driven organizations. We argue that by implementing the proposed framework, organizations can increase their chances to favorably compete in their respective markets. We further argue that by understanding open government data as a strategic resource for enterprises, government as producers or suppliers of this resource become key partners to data-driven organizations.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {250–259},
numpages = {10},
keywords = {organizational capabilities, competitive strategies, open data organization, competitive advantage, open data capabilities, Competitiveness in open data businesses},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@inproceedings{10.1145/2670757.2670779,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {A Comparison of the Field Data Management and Its Representation in Secondary CS Curricula},
year = {2014},
isbn = {9781450332507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670757.2670779},
doi = {10.1145/2670757.2670779},
abstract = {In the last few years, the focus of data management has changed from handling relatively small amounts of data, often in relational databases, to managing large amounts of data using various different database types. In many secondary school curricula, data management is mainly considered from a "database" perspective. However, in contrast to the developments in computer science research and practice, the new and changing aspects of data management have hardly been discussed with respect to CS education. We suggest re-evaluating the focus and relevance of the established database syllabi, to discuss the educational value of the newly arising developments and to prevent the teaching of outdated concepts. In this paper, we will contrast current educational standards and curricula with an up-to-date characterization of data management in order to identify gaps between the principles and concepts of data management that are considered as important today from a professional point of view on the one side, and the emphasis in current CS education on the other side.The findings of this analysis will provide a basis for aligning the concepts taught in CS education with the developments in data management research and practice, as well as for re-evaluating the educational value of these concepts.},
booktitle = {Proceedings of the 9th Workshop in Primary and Secondary Computing Education},
pages = {29–36},
numpages = {8},
keywords = {data management, standards, analysis, databases, characterization, secondary school, curricula},
location = {Berlin, Germany},
series = {WiPSCE '14}
}

@inproceedings{10.1145/3348445.3348464,
author = {Lin, Yuting},
title = {Government Management Model of Non-Profit Organizations Based on E-Government},
year = {2019},
isbn = {9781450371957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348445.3348464},
doi = {10.1145/3348445.3348464},
abstract = {With the development and popularization of Internet technology, our country is increasingly aware of the importance of e-government, and continuously expands the channels and means of e-government development in policy, such as the application of e-government to the management of non-profit organizations. However, in practice, "e-government + NPO (non-profit organization) management" still has problems such as digital divide, information sharing and insufficient disclosure, and information security. Therefore, this paper proposes a more complete non-profit organization management model based on e-government. From the perspectives of optimization services, information sharing, network supervision and information security, it is explained how to effectively realize the efficient management of non-profit organizations based on e-government.},
booktitle = {Proceedings of the 2019 7th International Conference on Computer and Communications Management},
pages = {164–168},
numpages = {5},
keywords = {management model, E-government, non-profit organization},
location = {Bangkok, Thailand},
series = {ICCCM 2019}
}

@inproceedings{10.1145/3429889.3429921,
author = {Ren, Kang and Liu, Fan and Zhuang, Haimei and Ling, Yun},
title = {AI-Based Multimodal Data Management and Intelligent Analysis System for Parkinson's Disease: GYENNO PD CIS},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429921},
doi = {10.1145/3429889.3429921},
abstract = {The GYENNO PD CIS is an AI-based multimodal data management and intelligent analysis system for Parkinson's disease (PD). The main purpose is to solve the problems in traditional diagnosis of PD such as lack of objective evaluation data, lack of reproducible diagnosis system, and lack of closed-loop treatment tracking, and then to construct a multimodal data management and intelligent analysis platform for PD, which can achieve the goals - standardization of data, objectification of evaluation, standardization of diagnosis, individualization of treatment, continuousness of management. It also helps Parkinson's experts in patient management, clinical data management, analysis and data mining, and supports multi-center projects, and finally lets patients benefit a lot from innovative technology.},
booktitle = {Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences},
pages = {166–170},
numpages = {5},
keywords = {Quantitative evaluation and diagnostic data set, Multimodal, Parkinson's disease (PD), Intelligent analysis system},
location = {Beijing, China},
series = {ISAIMS 2020}
}

@inproceedings{10.1145/3433996.3434019,
author = {Li, Ting and Zhang, Bo},
title = {Development Dilemma and Countermeasures of Data Journalism},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434019},
doi = {10.1145/3433996.3434019},
abstract = {In the field of news, with the operation of data journalism, the traditional press is facing great innovation and shock in the production, circulation, distribution and consumption of information. As McLuhan said, the birth of new media has opened up new possibilities in this era. Data, as a medium of the new era, is creating a new way for people to understand the world.This paper mainly discusses that it is still facing the problem of low degree of data opening in the current development, and the negative impact of disclosing users' personal privacy and information cocoon room. In view of these problems, relevant departments need to further strengthen the policy of data opening, improve the legal system, and optimize the link mode of information content dissemination, so as to promote the better development of data journalism and make data benefit people truly.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {127–131},
numpages = {5},
keywords = {Data journalism, data opening, visualization, information cocoons},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/2671491.2671497,
author = {Best, Daniel M. and Endert, Alex and Kidwell, Daniel},
title = {7 Key Challenges for Visualization in Cyber Network Defense},
year = {2014},
isbn = {9781450328265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2671491.2671497},
doi = {10.1145/2671491.2671497},
abstract = {What does it take to be a successful visualization in cyber security? This question has been explored for some time, resulting in many potential solutions being developed and offered to the cyber security community. However, when one reflects upon the successful visualizations in this space they are left wondering where all those offerings have gone. Excel and Grep are still the kings of cyber security defense tools; there is a great opportunity to help in this domain, yet many visualizations fall short and are not utilized.In this paper we present seven challenges, informed by two user studies, to be considered when developing a visualization for cyber security purposes. Cyber security visualizations must go beyond isolated solutions and "pretty picture" visualizations in order to impact users. We provide an example prototype that addresses the challenges with a description of how they are met. Our aim is to assist in increasing utility and adoption rates for visualization capabilities in cyber security.},
booktitle = {Proceedings of the Eleventh Workshop on Visualization for Cyber Security},
pages = {33–40},
numpages = {8},
keywords = {defense, cyber security, visualization},
location = {Paris, France},
series = {VizSec '14}
}

@article{10.1145/2430456.2430466,
author = {Beskales, George and Das, Gautam and Elmagarmid, Ahmed K. and Ilyas, Ihab F. and Naumann, Felix and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge and Tang, Nan},
title = {The Data Analytics Group at the Qatar Computing Research Institute},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2430456.2430466},
doi = {10.1145/2430456.2430466},
journal = {SIGMOD Rec.},
month = {jan},
pages = {33–38},
numpages = {6}
}

@inproceedings{10.1145/3326365.3326374,
author = {Liu, Shuhua Monica and Pan, Liting and Lei, Yupei},
title = {What is the Role of New Generation of ICTs in Transforming Government Operation and Redefining State-Citizen Relationship in the Last Decade?},
year = {2019},
isbn = {9781450366441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326365.3326374},
doi = {10.1145/3326365.3326374},
abstract = {This article first introduce a new government initiative emerging after the US presidential election in 2008. Comparing to the more descriptive definitions of e-government, supporters of these new government initiatives emphasize the transformative and normative aspect of the newest generation of Information and Communication Technology (ICTs). They argue that the new initiative redefines how government should operate and transform state-citizen relationships. To understand the core of this initiative and whether it offers new opportunities to solve public problems, we collected and analyzed research papers published in the e-governance area between 2008 and 2017. Our analysis demonstrates that the use of new generation of ICTs has promoted the government information infrastructure. In other words, the application of new ICTs enables the government to accumulate and use a large amount of data, so that the government makes better decisions. The advancement of open data, the wide use of social media, and the potential of data analytics have also generated pressure to address challenging questions and issues in e-democracy. However, the analysis leads us to deliberate on whether the use of new generation of ICTs worldwide have actually achieved their goal. In the conclusion, we present challenges to be addressed before new innovative ICTs realize their potential towards better public governance.},
booktitle = {Proceedings of the 12th International Conference on Theory and Practice of Electronic Governance},
pages = {65–75},
numpages = {11},
keywords = {E-governance, Transformative governance, Information and communication technology (ICT)},
location = {Melbourne, VIC, Australia},
series = {ICEGOV2019}
}

@inproceedings{10.5555/2693848.2693973,
author = {Elmegreen, Bruce G. and Sanchez, Susan M. and Szalay, Alexander S.},
title = {The Future of Computerized Decision Making},
year = {2014},
publisher = {IEEE Press},
abstract = {Computerized decision making is becoming a reality with exponentially growing data and machine capabilities. Some decision making is extremely complex, historically reserved for governing bodies or market places where the collective human experience and intelligence come to play. Other decision making can be trusted to computers that are on a path now into the future through novel software development and technological improvements in data access. In all cases, we should think about this carefully first: what data are really important for our goals and what data should be ignored or not even stored? The answer to these questions involves human intelligence and understanding before the data-to-decision process begins.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {943–949},
numpages = {7},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3491204.3527495,
author = {Bauer, Andr\'{e} and Leznik, Mark and Iqbal, Md Shahriar and Seybold, Daniel and Trubin, Igor and Erb, Benjamin and Domaschka, J\"{o}rg and Jamshidi, Pooyan},
title = {SPEC Research - Introducing the Predictive Data Analytics Working Group: Poster Paper},
year = {2022},
isbn = {9781450391597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491204.3527495},
doi = {10.1145/3491204.3527495},
abstract = {The research field of data analytics has grown significantly with the increase of gathered and available data. Accordingly, a large number of tools, metrics, and best practices have been proposed to make sense of this vast amount of data. To this end, benchmarking and standardization are needed to understand the proposed approaches better and continuously improve them. For this purpose, numerous associations and committees exist. One of them is SPEC (Standard Performance Evaluation Corporation), a non-profit corporation for the standardization and benchmarking of performance and energy evaluations. This paper gives an overview of the recently established SPEC RG Predictive Data Analytics Working Group. The mission of this group is to foster interaction between industry and academia by contributing research to the standardization and benchmarking of various aspects of data analytics.},
booktitle = {Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
pages = {13–14},
numpages = {2},
keywords = {measurements, standardization, benchmarking, data management, spec, metrics, data analytics},
location = {Bejing, China},
series = {ICPE '22}
}

@article{10.1145/2579167,
author = {Raschid, Louiqa},
title = {Editorial},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2579167},
doi = {10.1145/2579167},
journal = {J. Data and Information Quality},
month = {may},
articleno = {14},
numpages = {2}
}

@inproceedings{10.1145/3152465.3152473,
author = {Wang, Deqiang and Guo, Danhuai and Zhang, Hui},
title = {Spatial Temporal Data Visualization In Emergency Management: A View from Data-Driven Decision},
year = {2017},
isbn = {9781450354936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152465.3152473},
doi = {10.1145/3152465.3152473},
abstract = {Recent years, extreme events caused a great loss of human society. Emergency management is playing a more and more important role in handling disaster events. With the raising of data-intensive decision making, how to visualize large, multi-dimension data become an important challenge. Spatial temporal data visualization, a powerful tool, could transform data in to visual structure and make core information easily be captured by human. It could support spatial analysis, decision making and be used in all phase of emergency management. In this paper, we reviewed the general method of spatial temporal data visualization and the methods in data-intensive environment. Summarized the problems of each phase of emergency management and presented how spatial temporal visualization tools applied in each phase of emergency management. Finally, we conduct a short conclusion and outlook the future of spatial temporal visualization applied in data-driven emergency management environment.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Emergency Management Using},
articleno = {8},
numpages = {7},
keywords = {emergency management, review, spatio-temporal visualization},
location = {Redondo Beach, CA, USA},
series = {EM-GIS'17}
}

@article{10.1145/3143313,
author = {Raschid, Louiqa},
title = {Editor-in-Chief (January 2014-May 2017) Farewell Report},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3143313},
doi = {10.1145/3143313},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {7},
numpages = {2}
}

@inproceedings{10.5555/2814058.2814102,
author = {Barata, Andre Montoia and Prado, Edmir Parada Vasques},
title = {Data Governance in Brazilian Organizations},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {Organizations are increasingly looking for data integrity and quality to assist in strategic making decision and value creation. In this context Data Governance (DG) provide processes and practices that assist in the management and maintenance data. There are many frameworks to implementation DG process and benefits they may provide, however there are few implementation reported in the literature. This study aims to identify the DG process and frameworks implemented in Brazilian organizations and compare the benefits in implementation with those proposed by literature. For this will be carried out case studies in Brazilian organizations that implemented or are implementing DG frameworks.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {267–272},
numpages = {6},
keywords = {Data Governance, Management Frameworks, System Information},
location = {Goiania, Goias, Brazil},
series = {SBSI 2015}
}

@article{10.1145/3524284,
author = {Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip A. and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, Anhai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Ooi, Beng Chin and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and Re, Christopher and Stonebraker, Michael and Suciu, Dan},
title = {The Seattle Report on Database Research},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3524284},
doi = {10.1145/3524284},
abstract = {Every five years, a group of the leading database researchers meet to reflect on their community's impact on the computing industry as well as examine current research challenges.},
journal = {Commun. ACM},
month = {jul},
pages = {72–79},
numpages = {8}
}

@inproceedings{10.1145/3300115.3312508,
author = {Cassel, Lillian and Hongzhi, Wang},
title = {Panel: The Computing in Data Science},
year = {2019},
isbn = {9781450362597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300115.3312508},
doi = {10.1145/3300115.3312508},
abstract = {This panel brings the workings and results of the ACM Education Council Task Force on Data Science Education. The task force has gathered information on existing programs and has reviewed documents such as the result of the National Academies deliberations on data science. The task force is charged with exploring the role of computer science in data science education, understanding that data science is an inherently interdisciplinary field and not exclusively a computer science field. The panel will present a summary of the task force findings by two members of the task force and perspectives from leaders in data-intensive applications from China. The goal of the panel is to present the findings, but also to obtain perspectives from the attendees in order to enrich the task force's work.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education},
pages = {192–193},
numpages = {2},
keywords = {data science, computing for data science, computing curriculum},
location = {Chengdu,Sichuan, China},
series = {CompEd '19}
}

@inproceedings{10.1145/3428757.3429152,
author = {Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"{o}lz, Alexander},
title = {Machine Learning as a Service: Challenges in Research and Applications},
year = {2020},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429152},
doi = {10.1145/3428757.3429152},
abstract = {This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {396–406},
numpages = {11},
keywords = {MLaaS, Machine Learning Services, Machine Learning as a Service, Machine Learning, Machine Learning Platform},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3085228.3085269,
author = {Chen, Yumei and Dawes, Sharon S. and Chen, Shanshan},
title = {E-Government Support for Administrative Reform in China},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085269},
doi = {10.1145/3085228.3085269},
abstract = {This1 paper summarizes the history of Chinese administrative modernization and reform and discusses the ways in which China's e-government development agenda supports reform in the areas of transforming functions, streamlining processes, and enhancing transparency and citizen engagement. It offers a conceptual model of how e-government supports reform through policies, technologies, management improvements, and data designed to overcome the barriers of technical capability, staff resistance, and lack of cross-boundary collaboration. The analysis also shows how this interaction has generated new issues regarding official corruption and public engagement. We conclude with a future research agenda.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {329–335},
numpages = {7},
keywords = {Chinese government and reform, Administrative reform, E-government},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@article{10.1145/3447269,
author = {Tufi\c{s}, Mihnea and Boratto, Ludovico},
title = {Toward a Complete Data Valuation Process. Challenges of Personal Data},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3447269},
doi = {10.1145/3447269},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {20},
numpages = {7},
keywords = {data markets, Datasets, data valuation}
}

@article{10.1007/s00778-015-0389-y,
author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
title = {Profiling Relational Data: A Survey},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-015-0389-y},
doi = {10.1007/s00778-015-0389-y},
abstract = {Profiling data to determine metadata about a given dataset is an important and frequent activity of any IT professional and researcher and is necessary for various use-cases. It encompasses a vast array of methods to examine datasets and produce metadata. Among the simpler results are statistics, such as the number of null values and distinct values in a column, its data type, or the most frequent patterns of its data values. Metadata that are more difficult to compute involve multiple columns, namely correlations, unique column combinations, functional dependencies, and inclusion dependencies. Further techniques detect conditional properties of the dataset at hand. This survey provides a classification of data profiling tasks and comprehensively reviews the state of the art for each class. In addition, we review data profiling tools and systems from research and industry. We conclude with an outlook on the future of data profiling beyond traditional profiling tasks and beyond relational databases.},
journal = {The VLDB Journal},
month = {aug},
pages = {557–581},
numpages = {25}
}

@article{10.1145/2063504.2063505,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editorial Notes Classification and Assessment of Large Amounts of Data: Examples in the Healthcare Industry and Collaborative Digital Libraries},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2063504.2063505},
doi = {10.1145/2063504.2063505},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {12},
numpages = {2}
}

@inproceedings{10.1145/3479645.3479669,
author = {Sulistyowati, Ira and Fransisca, Dyna and Ruldeviyani, Yova},
title = {Data Analytics Readiness Model in Indonesian Government},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479669},
doi = {10.1145/3479645.3479669},
abstract = {The development of information technology encourages the government to digitize business processes. It is generated with a large and varied volume from various data sources so that advanced data analytics (DA) is required to overcome this to support organization's data driven decision making. It's necessary to prepare DA based on DA readiness model so that the implementation of DA can run successfully. Whereas currently, there is limited study and no standard model for DA readiness. The focus of this study is to propose model readiness of implementing data analytics that is suitable in Indonesian government. The model refers to DA readiness model based on literature review on 15 papers relevant to DA readiness. Then it's verified by 7 experts. Furthermore, online survey was conducted to test the model that affects the readiness of implementing data analytics in Indonesian government. The survey results were analyzed using factor analysis. As a result, DA readiness model contains 4 dimensions, 11 factors, and 78 indicators where its dimensions consist of information system, organizational and cultural, organization structure and resource readiness. This model can describe 85% of the data analysis readiness requirements in the Indonesian government. In order to implement data analytics successfully, the government needs to improve the readiness of information systems, organizational and cultural, organizational structures, and resources.},
booktitle = {6th International Conference on Sustainable Information Engineering and Technology 2021},
pages = {100–105},
numpages = {6},
location = {Malang, Indonesia},
series = {SIET '21}
}

@inproceedings{10.1145/3379177.3388909,
author = {Munappy, Aiswarya Raj and Mattos, David Issa and Bosch, Jan and Olsson, Helena Holmstr\"{o}m and Dakkak, Anas},
title = {From Ad-Hoc Data Analytics to DataOps},
year = {2020},
isbn = {9781450375122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379177.3388909},
doi = {10.1145/3379177.3388909},
abstract = {The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow.DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps.This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {165–174},
numpages = {10},
keywords = {Data Pipelines, Continuous Monitoring, Data technologies, DataOps, DevOps, Agile Methodology},
location = {Seoul, Republic of Korea},
series = {ICSSP '20}
}

@article{10.1145/2782759.2782762,
author = {Laube, Patrick},
title = {The Low Hanging Fruit is Gone: Achievements and Challenges of Computational Movement Analysis},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/2782759.2782762},
doi = {10.1145/2782759.2782762},
abstract = {This position paper reviews the achievements and open challenges of movement analysis within Geographical Information Science. The paper argues that the simple problems of movement analysis have mostly been addressed to a sufficient level ("the low hanging fruit"), leaving the research community with the much more challenging problems for the years ahead ("the high hanging fruit"). Whereas the community has made good progress in structuring trajectory data (segmentation, similarity, clustering) and conceptualizing and detecting movement patterns, the much harder task of semantic annotation of structures and patterns remains difficult. The position paper summarizes both achievements and challenges with two sets assertions and calls for the establishment of a unifying theory of Computational Movement Analysis.},
journal = {SIGSPATIAL Special},
month = {may},
pages = {3–10},
numpages = {8}
}

@article{10.1145/3092931.3092933,
author = {Abiteboul, Serge and Arenas, Marcelo and Barcel\'{o}, Pablo and Bienvenu, Meghyn and Calvanese, Diego and David, Claire and Hull, Richard and H\"{u}llermeier, Eyke and Kimelfeld, Benny and Libkin, Leonid and Martens, Wim and Milo, Tova and Murlak, Filip and Neven, Frank and Ortiz, Magdalena and Schwentick, Thomas and Stoyanovich, Julia and Su, Jianwen and Suciu, Dan and Vianu, Victor and Yi, Ke},
title = {Research Directions for Principles of Data Management (Abridged)},
year = {2017},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3092931.3092933},
doi = {10.1145/3092931.3092933},
journal = {SIGMOD Rec.},
month = {may},
pages = {5–17},
numpages = {13}
}

@inproceedings{10.1145/3468784.3471607,
author = {Umejiaku, Afamefuna and Dang, Tommy},
title = {Visualising Developing Nations Health Records: Opportunities, Challenges and Research Agenda},
year = {2021},
isbn = {9781450390125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468784.3471607},
doi = {10.1145/3468784.3471607},
abstract = {The benefits of effectively visualizing health records in huge volumes has resulted in health organizations, insurance companies, policy and decision makers, governments and drug manufactures’ transformation in the way research is conducted. This has also played a key role in determining investment of resources. Health records contain highly valuable information; processing these records in large volumes is now possible due to technological advancement which allows for the extraction of highly valuable knowledge that has resulted in breakthroughs in scientific communities. To visualize health records in large volumes, the records need to be stored in electronic forms, properly documented, processed, and analyzed. A good visualization technique is used to present the analyzed information, allowing for effective knowledge extraction which is done in a secured manner protecting the privacy of the patients whose health records were used. As research and technological advancement have improved, the quality of knowledge extracted from health records have also improved; unfortunately, the numerous benefits of visualizing health records have only been felt in developed nations, unlike other sectors where technological advancement in developed nations have had similar impact in developing nations. This paper identifies the characteristics of health records and the challenges involved in processing large volumes of health records. This is to identify possible steps that could be taken for developing nations to benefit from visualizing health records in huge volumes.},
booktitle = {The 12th International Conference on Advances in Information Technology},
articleno = {38},
numpages = {9},
keywords = {Health records, Visualisation, Developing Nations},
location = {Bangkok, Thailand},
series = {IAIT2021}
}

@inproceedings{10.1145/2591888.2591901,
author = {Millard, Jeremy},
title = {ICT-Enabled Public Sector Innovation: Trends and Prospects},
year = {2013},
isbn = {9781450324564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591888.2591901},
doi = {10.1145/2591888.2591901},
abstract = {This experience paper is a personal thinkpiece which outlines many of the main issues and discussions taking place in Europe and elsewhere about the future of the public sector and how it can respond positively to some of the acute challenges it faces in light of the financial crisis and other global challenges. The paper examines how ICT-enabled public sector innovation highlights concepts like open governance, public value, government as a platform, open assets, open services and open engagement. It develops a vision of an 'open governance framework', moving beyond 'new public management', based on ICT-enabled societal-wide collaboration. It recognises that although the public sector can in principle create public value on its own, its potential to do so is greatly enhanced and extended by direct cooperation with other actors, or by facilitating public value creation by other actors on their own. It also examines the role of bottom-up innovation and public policy experimentation, as well as the need to focus on empowering civil servants and changing public sector working practices and mindsets.},
booktitle = {Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance},
pages = {77–86},
numpages = {10},
keywords = {public value, government as a platform, open governance, open services, open assets, open engagement},
location = {Seoul, Republic of Korea},
series = {ICEGOV '13}
}

@inproceedings{10.1145/3351108.3351110,
author = {Hattingh, Mari\'{e} and Marshall, Linda and Holmner, Marlene and Naidoo, Rennie},
title = {Data Science Competency in Organisations: A Systematic Review and Unified Model},
year = {2019},
isbn = {9781450372657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351108.3351110},
doi = {10.1145/3351108.3351110},
abstract = {The paper presents a systematic literature review of the literature on the competencies that are essential to develop a globally competitive workforce in the field of data science. The systematic review covers a wide range of literature but focuses primarily, but not exclusively, on the computing, information systems, management, and organisation science literature. The paper uses a broad research search strategy covering four separate electronic databases. The search strategy led the researchers to scan 139 titles, abstracts and keywords. Sixty potentially relevant articles were identified, of which 42 met the quality criteria and contributed to the analysis. A critical appraisal checklist assessed the validity of each empirical study. The researchers grouped the findings under six broad competency themes: organisational, technical, analytical, ethical and regulatory, cognitive and social. Thematic analysis was used to develop a unified model of data science competency based on the evidence of the findings. This model will be applied to case studies and survey research in future studies. A unified data science competency model, supported by empirical evidence, is crucial in closing the skills gap, thereby improving the quality and competitiveness of the South Africa's data science workforce. Researchers are encouraged to contribute to the further conceptual development of data science competency.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists 2019},
articleno = {1},
numpages = {8},
keywords = {Skills, Systematic Literature Review, Competency, Data Science},
location = {Skukuza, South Africa},
series = {SAICSIT '19}
}

@inproceedings{10.1145/3397056.3397078,
author = {Ge, Juan and Han, Wenli and Zhang, Xunhu and Zhou, Jin},
title = {Research on Construction of Quality Service Platform of Survey and Mapping},
year = {2020},
isbn = {9781450377416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397056.3397078},
doi = {10.1145/3397056.3397078},
abstract = {Quality data of Surveying and mapping is an intuitive reflection of the industry's quality situation and technical development situation. The construction of quality service platform of Surveying and mapping is discussed for the problems existing in the management of surveying and mapping quality data and for the demand for the use of quality data. It discusses the contents, framework and techniques used by the platform. The platform can be used to assist scientific decision-making and improve the service level.},
booktitle = {Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis},
pages = {57–61},
numpages = {5},
keywords = {Quality, Service platform, Data management},
location = {Marseille, France},
series = {ICGDA 2020}
}

@inproceedings{10.1145/3330431.3330457,
author = {Aljawarneh, Shadi and Radhakrishna, Vangipuram and Kumar, Gunupudi Rajesh},
title = {A Recent Survey on Challenges in Security and Privacy in Internet of Things},
year = {2019},
isbn = {9781450372121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330431.3330457},
doi = {10.1145/3330431.3330457},
abstract = {Computing environment in IoT (Internet of Things) is surrounded with huge amounts of heterogeneous data fulfilling many services in everyone's daily life. Since, communication process in IoT takes place using different devices such as smart phones, sensors, mobile devices, household devices, embedded equipment etc. With the use of these variety of devices, the exchange of data in open internet environment is prone to vulnerabilities. The main cause for these vulnerabilities is the weaknesses in the design of software components and hardware components. Bridging communications gaps in the IoT is a complex process as the data is from heterogeneous sources. An effort is made in this paper to discuss various challenges that are being faced in security and privacy of data. This will be very much helpful for researchers who want to pursue research.},
booktitle = {Proceedings of the 5th International Conference on Engineering and MIS},
articleno = {25},
numpages = {9},
keywords = {IoT services, vulnerability, challenges in IoT, security and privacy, IoT classification, research issues, S/W weakness, IoT architecture},
location = {Astana, Kazakhstan},
series = {ICEMIS '19}
}

@inproceedings{10.1145/2872518.2890599,
author = {Auer, S\"{o}ren and Heath, Tom and Bizer, Christian and Berners-Lee, Tim},
title = {LDOW2016: 9th Workshop on Linked Data on the Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890599},
doi = {10.1145/2872518.2890599},
abstract = {The ninth workshop on Linked Data (LDOW2016) on the Web is held in Montreal, Quebec, Canada on April 12, 2016 and co-located with the 25rd International World Wide Web Conference (WWW2016). The Web is developing from a medium for publishing textual documents into a medium for sharing structured data. This trend is fueled on the one hand by the adoption of the Linked Data principles by a growing number of data providers. On the other hand, large numbers of websites have started to semantically mark up the content of their HTML pages and thus also contribute to the wealth of structured data available on the Web. The 9th Workshop on Linked Data on the Web aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data from the Web as well as mining knowledge from the global Web of Data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1039–1040},
numpages = {2},
keywords = {rdf, semantic web, linked data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/3377929.3389894,
author = {Torresen, Jim},
title = {Addressing Ethical Challenges within Evolutionary Computation Applications: GECCO 2020 Tutorial},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389894},
doi = {10.1145/3377929.3389894},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1206–1223},
numpages = {18},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1145/2724721,
author = {Alonso, Omar},
title = {Challenges with Label Quality for Supervised Learning},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2724721},
doi = {10.1145/2724721},
abstract = {Organizations that develop and use technologies around information retrieval, machine learning, recommender systems, and natural language processing depend on labels for engineering and experimentation. These labels, usually gathered via human computation, are used in machine-learned models for prediction and evaluation purposes. In such scenarios, collecting high-quality labels is a very important part of the overall process. We elaborate on these challenges and discuss research directions.},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {2},
numpages = {3},
keywords = {Label quality, machine learning, human computation, crowdsourcing}
}

@article{10.1145/3385658.3385668,
author = {Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, AnHai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Chin Ooi, Beng and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and R\'{e}, Christopher and Stonebraker, Michael and Suciu, Dan},
title = {The Seattle Report on Database Research},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3385658.3385668},
doi = {10.1145/3385658.3385668},
abstract = {Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {44–53},
numpages = {10}
}

@inproceedings{10.1145/3227696.3227715,
author = {Tanaka, Yasuhiro and Kodate, Akihisa and Bolt, Timothy},
title = {Data Sharing System Based on Legal Risk Assessment},
year = {2018},
isbn = {9781450364652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227696.3227715},
doi = {10.1145/3227696.3227715},
abstract = {Regulations on protection of personal information vary from country to country. Therefore, when conducting international surveys for research, it is required to collect, manage and operate personal data properly complying with the laws and regulations of each country.We design a support system to fulfill conditions in terms of compliance for the proper and efficient management of data collection and utilization especially universities by making compliance management related to data cooperation a common foundation.This study aims to discuss requirements for the compliance management base system for data alliance and shared use of data.},
booktitle = {Proceedings of the 5th Multidisciplinary International Social Networks Conference},
articleno = {17},
numpages = {5},
keywords = {Privacy Protection, Data Sharing, Information system, Personal Data, Legal Risk Assessment},
location = {Saint-Etienne, France},
series = {MISNC '18}
}

@inproceedings{10.5555/3017447.3017522,
author = {Lucic, Ana and Blake, Catherine},
title = {Preparing a Workforce to Effectively Reuse Data},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {For centuries, library and information science professionals have been responsible for curating and preserving access to information resources. The last few decades have seen an unprecedented change in how new knowledge is created, disseminated and reused both within academe and industry, which provides new opportunities to intervene within the data lifecycle. This paper documents efforts to create a graduate educational program that produces alum who understand both the social and technical aspects of data analytics and who can effectively employ data to address questions in academe and industry. We share perspectives gained from initial interviews with project partners who have data needs, and report on how those needs directly informed curricula development of the Socio-technical Data Analytics (SODA) program at the School of Information Sciences at the University of Illinois. We also provide a formative student evaluation of the program that was conducted to identify aspects that are successful, and those where further work is needed in order to help other schools who are developing similar programs that prepare a workforce who can effectively reuse data.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology},
articleno = {75},
numpages = {10},
keywords = {data science, survey results, data analytics and evaluation, program development and evaluation},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1145/3512826.3512836,
author = {Wu, Xueqiong and Chen, Lei and Ji, Kun and Wang, Huidong and Qian, Hao and Ma, Lidong},
title = {Design and Application of Virtual Production Command Service in Power Distribution Network Based on Artificial Intelligence},
year = {2022},
isbn = {9781450395489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512826.3512836},
doi = {10.1145/3512826.3512836},
abstract = {Abstract: As the distribution network business hub, the distribution network production command center faces the need to improve the efficiency of the distribution network production command business. This article draws on international mainstream artificial intelligence (such as Google AlphaGo) and other independent learning models to explore the integration of artificial intelligence and power grid professional business. This paper analyzes the development trend of artificial intelligence technology in the fields of power grid distribution and power knowledge map, and proposes a distribution network virtual production commander engine with dispatch operation, remote monitoring, and intelligent screen monitoring capabilities based on the distribution network knowledge map to realize power grid dispatch Intelligent applications in the fields of operation command, emergency repair, and smart services, and some functions have been verified by the State Grid Hangzhou Electric Power Company.},
booktitle = {2022 The 3rd International Conference on Artificial Intelligence in Electronics Engineering},
pages = {33–37},
numpages = {5},
keywords = {Power Distribution Network Regulations, Power Knowledge Graph, Dispatch Professional Decision, Power Distribution Network Virtual Production Command Engine, AI},
location = {Bangkok, Thailand},
series = {AIEE 2022}
}

@article{10.1145/3450751,
author = {Zhou, Ke and Song, Jingkuan},
title = {Introduction to the Special Issue on Learning-Based Support for Data Science Applications},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1922},
url = {https://doi.org/10.1145/3450751},
doi = {10.1145/3450751},
journal = {ACM/IMS Trans. Data Sci.},
month = {apr},
articleno = {9},
numpages = {1}
}

@article{10.14778/3352063.3352116,
author = {Nargesian, Fatemeh and Zhu, Erkang and Miller, Ren\'{e}e J. and Pu, Ken Q. and Arocena, Patricia C.},
title = {Data Lake Management: Challenges and Opportunities},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352116},
doi = {10.14778/3352063.3352116},
abstract = {The ubiquity of data lakes has created fascinating new challenges for data management research. In this tutorial, we review the state-of-the-art in data management for data lakes. We consider how data lakes are introducing new problems including dataset discovery and how they are changing the requirements for classic problems including data extraction, data cleaning, data integration, data versioning, and metadata management.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1986–1989},
numpages = {4}
}

@article{10.1145/2334184.2334188,
author = {Churchill, Elizabeth F.},
title = {From Data Divination to Data-Aware Design},
year = {2012},
issue_date = {September + October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/2334184.2334188},
doi = {10.1145/2334184.2334188},
journal = {Interactions},
month = {sep},
pages = {10–13},
numpages = {4}
}

@inproceedings{10.1145/3463531.3463536,
author = {Wan, Xinxin},
title = {A Study on the Current Development of Artificial Intelligence in Education Industry in China},
year = {2021},
isbn = {9781450389662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463531.3463536},
doi = {10.1145/3463531.3463536},
abstract = {This article first explained the definition of AI in education (AIEd) and reported findings regarding the current development of AIEd industry in the Chinese context. The research design is a context-specific case study using the supply and demand theoretical framework. From a demand-side perspective, the author made an in-depth analysis of the specific AI applications employed in different educational scenarios, including the automated speaking assessment system, the content-based image retrieval system, adaptive learning system, AI-supported classrooms, and AI-assisted campus safety system. For the supply analysis of the AIEd industry, this article summarized key AIEd industry chains and technologies currently widely used in China, obtaining the industry market scale through data collected from different sources. In addition, the iFLYTEK company, as a typical enterprise in the AIEd industry, was taken as a medium to conduct a case analysis. The employment of various AI applications in smart classrooms, smart exams, and smart terminals were comprehensively discussed. In a nutshell, this article discussed the development status and future trends of Chinese AIEd industry, with an aim to offer suggestions and implications for education practitioners.},
booktitle = {2021 7th International Conference on Education and Training Technologies},
pages = {28–35},
numpages = {8},
keywords = {Oral Assessment, Smart Classroom, Education Informatization, AI Education, Adaptive Learning},
location = {Macau, China},
series = {ICETT 2021}
}

@inproceedings{10.1145/3368756.3369005,
author = {Bentalha, Badr and Hmioui, Aziz and Alla, Lhoussaine},
title = {The Digitalization of the Supply Chain Management of Service Companies: A Prospective Approach},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369005},
doi = {10.1145/3368756.3369005},
abstract = {Supply Chain Management (SCM) was born and developed first in an industrial context. In the field of services, little research has addressed the issue of the company's SCM. According to [1] "service logistics is an approach that stabilizes and guarantees the continuity of flows: it is then oriented more towards the service provided than towards reducing traffic costs". The SCM of services is of increasing interest to companies facing strong competition, market globalization and rapid changes in information and communication technologies. This evolution has led to a rapid integration of new digital practices in this field.So, how is the digitalization of the SCM of service companies looking today and what will be the future trends? On the one hand, with the help of the literature review, we seek to identify the concept of the SCM in services and its specificities, then that of digitization of the SCM and its organizational dimension. On the other hand, we are attempting a prospective approach to the current practices and digitalization prospects of the service company's SCM.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {29},
numpages = {8},
keywords = {SCM, supply chain, service company, prospective approach, digital},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3447548.3470799,
author = {Lee, Jae-Gil and Roh, Yuji and Song, Hwanjun and Whang, Steven Euijong},
title = {Machine Learning Robustness, Fairness, and Their Convergence},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470799},
doi = {10.1145/3447548.3470799},
abstract = {Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4046–4047},
numpages = {2},
keywords = {machine learning, convergence, robustness, fairness},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3447548.3470814,
author = {Zhou, Zirui and Chu, Lingyang and Liu, Changxin and Wang, Lanjun and Pei, Jian and Zhang, Yong},
title = {Towards Fair Federated Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470814},
doi = {10.1145/3447548.3470814},
abstract = {Federated learning has become increasingly popular as it facilitates collaborative training of machine learning models among multiple clients while preserving their data privacy. In practice, one major challenge for federated learning is to achieve fairness in collaboration among the participating clients, because different clients' contributions to a model are usually far from equal due to various reasons. Besides, as machine learning models are deployed in more and more important applications, how to achieve model fairness, that is, to ensure that a trained model has no discrimination against sensitive attributes, has become another critical desiderata for federated learning. In this tutorial, we discuss formulations and methods such that collaborative fairness, model fairness, and privacy can be fully respected in federated learning. We review the existing efforts and the latest progress, and discuss a series of potential directions.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4100–4101},
numpages = {2},
keywords = {collaborative fairness, federated learning, data privacy, model fairness, data leakage, distributed learning},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3424311.3424326,
author = {Wang, Lei and Wang, Yang},
title = {Application of Machine Learning for Process Control in Semiconductor Manufacturing},
year = {2020},
isbn = {9781450377348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424311.3424326},
doi = {10.1145/3424311.3424326},
abstract = {In this article, the authors attempt to describe the core quality inspection during semiconductor manufacturing in terms of production efficiency and yield. Special focus is therefore given to photolithography, which is the most critical step for the fabrication of wafer patterns in front-end processes. Further, machine learning approaches are demonstrated and their applicability in semiconductor manufacturing industry is discussed. Also, a technical concept regarding virtual metrology for advanced process control in semiconductor production is introduced as a potential utilization case. Finally, current status and future trends in technology as well as application are summarized based on authors' perspective in the concluding section.},
booktitle = {Proceedings of the 2020 International Conference on Internet Computing for Science and Engineering},
pages = {109–111},
numpages = {3},
keywords = {Advanced process control, Machine learning, Data analytics, Virtual metrology, Semiconductor manufacturing},
location = {Male, Maldives},
series = {ICICSE '20}
}

@inproceedings{10.1145/3479162.3479167,
author = {Suaprae, Phanintorn and Nilsook, Prachyanun and Wannapiroon, Panita},
title = {System Framework of Intelligent Consulting Systems with Intellectual Technology},
year = {2021},
isbn = {9781450390071},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479162.3479167},
doi = {10.1145/3479162.3479167},
abstract = {The purposes of this research were: 1) Analyze factors affecting the student retention of higher education students, 2) Develop intelligent consulting system models with intellectual technology for the student retention of higher education students, 3) Design intelligent consulting system architecture with intellectual technology for the student retention of higher education students, 4) Develop intelligent consulting systems with intellectual technology for the student retention of higher education students, and 5) Study the results of intelligent consultation systems with intellectual technology for the student retention of higher education students. An intelligent counseling system with intellectual technology for the student retention of higher education students is a system that can reduce students' mid-exit rates and increase student retention rates. The research has synthesized analysis of factors that affect Student retention applied to Cognitive technology, machine learning can provide accurate student retention forecasts. Counselors can know before students drop out.},
booktitle = {The 2021 9th International Conference on Computer and Communications Management},
pages = {31–36},
numpages = {6},
location = {Singapore, Singapore},
series = {ICCCM '21}
}

@inproceedings{10.1145/2729104.2729134,
author = {Kokkinakos, Panagiotis and Koutras, Costas and Markaki, Ourania and Koussouris, Sotirios and Trutnev, Dmitrii and Glikman, Yuri},
title = {Assessing Governmental Policies' Impact through Prosperity Indicators and Open Data},
year = {2014},
isbn = {9781450334013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729104.2729134},
doi = {10.1145/2729104.2729134},
abstract = {The aim of this paper is to provide an overview of (the theory and practice of) prosperity indicators for assessing the impact of governmental policies and the data sources associated to their calculation, touching also on the broad theme of Open Data which opens up new horizons for the calculation and exploitation of Social Indicators. Following a quick overview of the basics of prosperity indicators, their basic methodological principles and their typology, a presentation of the Policy Compass project approach and the description of its pilot application in St. Petersburg are provided, which are tackling the above mentioned issue with the provision of a powerful ICT platform.},
booktitle = {Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {70–74},
numpages = {5},
keywords = {Open Data, Fuzzy Cognitive Maps, Policy Making, Policy Impact Evaluation, Prosperity Indicators},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '14}
}

@inproceedings{10.5555/2693848.2694146,
author = {Wu, Xinghao and Qiao, Fei and Poon, Kwok},
title = {Cloud Manufacturing Application in Semiconductor Industry},
year = {2014},
publisher = {IEEE Press},
abstract = {This paper aims to shed some light on how the concept of cloud manufacturing has been applied to the semiconductor manufacturing operations. It starts with describing the challenges to the semiconductor manufacturing due to evolving of outsourcing business model in global context, then discusses the different forms of cloud manufacturing and proposes the semiconductor industry oriented architecture for cloud manufacturing. Serus is used as a case study to share how the cloud manufacturing has created the values for the customer and its outsourced suppliers in the semiconductor industry.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {2376–2383},
numpages = {8},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3159652.3160594,
author = {Lin, Yu-Ru and Castillo, Carlos and Yin, Jie},
title = {The 5th International Workshop on Social Web for Disaster Management(SWDM'18): Collective Sensing, Trust, and Resilience in Global Crises},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3160594},
doi = {10.1145/3159652.3160594},
abstract = {During large-scale emergencies such as natural and man-made disasters, a massive amount of information is posted by the public in social media. Collecting, aggregating, and presenting this information to stakeholders can be extremely challenging, particularly if an understanding of the "big picture»» is sought. This international workshop, the fifth in the series, is a key venue for researchers and practitioners to discuss research challenges and technical issues around the usage of social media in disaster management. Workshop»s website: https://sites.google.com/site/swdm2018/},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {791–792},
numpages = {2},
keywords = {social media, emergency management, disaster response},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@article{10.1145/3356773.3356810,
author = {Xie, Xiaoyuan and Poon, Pak-Lok and Pullum, Laura L.},
title = {Workshop Summary: 2019 IEEE / ACM Fourth International Workshop on Metamorphic Testing (MET 2019)},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3356773.3356810},
doi = {10.1145/3356773.3356810},
abstract = {MET is a relatively new workshop on metamorphic testing for academic researchers and industry practitioners. The first international workshop on MET (MET 2016) was co-located with the 38th International Conference on Software Engineering (ICSE 2016) in Austin TX, USA on May 16, 2016. Since then the workshop has become an annual event. This paper reports on the fourth International Workshop on Metamorphic Testing (MET 2019) held in Montr\'{e}al, Canada on May 26, 2019, as part of the 41st International Conference on Software Engineering (ICSE 2019). We first outline the aims of the workshop, followed by a discussion of its keynote speech and technical program.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {nov},
pages = {56–59},
numpages = {4},
keywords = {software testing, software verification and validation, software engineering, metamorphic testing}
}

@inproceedings{10.1145/3047273.3047386,
author = {Matheus, Ricardo and Janssen, Marijn},
title = {How to Become a Smart City? Balancing Ambidexterity in Smart Cities},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047386},
doi = {10.1145/3047273.3047386},
abstract = {Most cities have limited resources to become a smart city. Yet some cities have been more successful than others in becoming a smart city. This raises the questions why were some cities able to become smart, whereas other were not able to do so? This research is aimed at identifying factors influencing the shift towards becoming a smart city. In this way insight is gained into factors that governments can influence to become a smart city. First, Literature was reviewed to identify dimensions and factors enabling or impeding the process of becoming a smart city. These factors were used to compare two similar type of case studies. The cases took different paths to become a smart city and had different levels of success. This enabled us to identify factors influencing the move towards smart cities. The results reveal that existing infrastructures should be used and extended in such a way that they can facilitate a variety of different applications. Synergy from legacy systems can avoid extra expenditures. Having such an infrastructure in place facilitates the development of new organizational models. These models are developed outside the existing organization structure to avoid hinder from existing practices and organizational structures. This finding suggests that smart cities focussed on structural ambidexterity innovate quicker.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {405–413},
numpages = {9},
keywords = {innovation, e-government, transformation, exploitation, ambidexterity, exploration, smart cities},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3394486.3406473,
author = {Pei, Jian},
title = {Data Pricing -- From Economics to Data Science},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406473},
doi = {10.1145/3394486.3406473},
abstract = {Data are invaluable. How can we assess the value of data objectively and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, data management, data mining, electronic commerce, and marketing. In this tutorial, we present a unified and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing, review the development and evolution of pricing models, and compare the proposals of marketplaces of data. We cover both digital products, such as ebooks and MP3 music, and data products, such as data sets, data queries and machine learning models. We also connect data pricing with the highly related areas, such as cloud service pricing, privacy pricing, and decentralized privacy preserving infrastructure like blockchains.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3553–3554},
numpages = {2},
keywords = {digital products, data products, trustfulness, data pricing, subscription, bundling, revenue maximization, privacy, auctions, arbitrage, information goods, fairness},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

