@inproceedings{10.1145/2536714.2536720,
author = {Meurisch, Christian and Planz, Karsten and Sch\"{a}fer, Daniel and Schweizer, Immanuel},
title = {Noisemap: Discussing Scalability in Participatory Sensing},
year = {2013},
isbn = {9781450324304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536714.2536720},
doi = {10.1145/2536714.2536720},
abstract = {Environmental pollutants are an ever increasing problem in dense urban environments. To assess the effect of these pollutants, an unprecedented density of data is needed for large areas (cities, states, countries). In the past, participatory sensing has been proposed as a mean to acquire large sets of data. Since the smartphone is ubiquitous, scalability seems to be no problem anymore.In reality this far from the truth. Measuring their environment, people need to invest their time. For Android and iOS the application needs to compete with more than 700,000 other applications. Measuring large amounts of data is only possible, if we can attract large amounts of casual users.Since 2011, we have been working with and on Noisemap. Noisemap is one of many applications that uses the microphone to measure sound pressure. It then uploads the captured data to our backend, where the data is processed and visualized. Noisemap is officially available since February 2012, has been downloaded over 2,500 times, and has more than 1,000 registered users, which have collected over 500,000 unique data points in 39 countries and 58 cities. We want to share the current state of Noisemap as a multi-platform tool on Android and iOS, as well as our experience in scaling the application.},
booktitle = {Proceedings of First International Workshop on Sensing and Big Data Mining},
pages = {1–6},
numpages = {6},
keywords = {Sensing Campaign, Participatory Sensing, Environmental Pollution Modeling},
location = {Roma, Italy},
series = {SENSEMINE'13}
}

@inproceedings{10.1145/3335550.3335577,
author = {Li, Ruixue and Peng, Can and Sun, Huiliang},
title = {Product Selection Strategy Analysis of Crowdsourcing Platform from the Full Cost Perspective},
year = {2019},
isbn = {9781450362641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335550.3335577},
doi = {10.1145/3335550.3335577},
abstract = {From the perspective of full cost, this paper uses Coase's transaction cost theory to analyze the causes of crowdsourcing, and on this basis to analyze the applicability of crowdsourcing platform products. At the same time, based on the crowdsourcing platform--zbj.com, we use the big data technology to grasp and analyze the related data of the crowdsourcing platform's successful cases in the past five months, and use the relevant statistical analysis method to categorize and analyze the industry attributes of the top five orders of the success cases of the zbj.com, in order to verify the theory mentioned in the article.},
booktitle = {Proceedings of the 2019 International Conference on Management Science and Industrial Engineering},
pages = {92–97},
numpages = {6},
keywords = {Selection Strategy Analysis, Crowdsourcing platform, Full cost, Appropriate products},
location = {Phuket, Thailand},
series = {MSIE 2019}
}

@inproceedings{10.1145/3301761.3301767,
author = {Tang, Haijing and Zhou, Yangdong and Yang, Xu and Gao, Keyan and Zheng, Wenhao and Zhao, Jinfeng},
title = {Adopting Data Analysis and Visualization Technology to Construct Clinical Research Data Management and Analysis System},
year = {2018},
isbn = {9781450361279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301761.3301767},
doi = {10.1145/3301761.3301767},
abstract = {With the development of information technology, information systems have been widely used in medical institutions, and more and more clinical research data has been digitized, which provides the possibility to carry out clinical research with data as the source. However, the complexity and multi-dimensionality of clinical data make medical scientists' progress slow, and comprehensive use of various big data technologies is needed to help improve the efficiency of clinical research. Visualization technology can display data in an intuitive and easy-to-read way, helping medical researchers understand data, while parallel computing can greatly improve computing efficiency. Therefore, this paper explores the application strategies of data analysis technology and visualization technology in the management and analysis of clinical research data, and builds a set of clinical research data management analysis system, which combines various technologies to help effectively promote medical clinical.},
booktitle = {Proceedings of the 2018 2nd International Conference on Software and E-Business},
pages = {49–53},
numpages = {5},
keywords = {Visualization, Data analysis, Clinical data},
location = {Zhuhai, China},
series = {ICSEB '18}
}

@inproceedings{10.1109/MET.2019.00018,
author = {Yan, Boyang and Yecies, Brian and Zhou, Zhi Quan},
title = {Metamorphic Relations for Data Validation: A Case Study of Translated Text Messages},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MET.2019.00018},
doi = {10.1109/MET.2019.00018},
abstract = {In conventional metamorphic testing, metamorphic relations (MRs) are identified as necessary properties of a computer program's intended functionality, whereby violations of MRs reveal faults in the program---under the assumption that the source and follow-up inputs (test cases used in metamorphic testing) are valid. In the present study, the authors argue that MRs can also be used to validate and assess the quality of the program's input data---under the assumption that the source or follow-up inputs can be inappropriately generated. Using this new perspective, a case study in the natural language processing domain is used to explore the different types of text messages that are difficult to interpret by (Chinese-English) machine translation. A total of 46,180 short user comments on Personal Tailor (a 2013 Chinese film), collected from Douban (a popular Chinese social media platform), has been used as the primary dataset of this study, and the analysis of results demonstrates that the proposed MR-based data validation method is useful for the automatic identification of poorly translated text messages.},
booktitle = {Proceedings of the 4th International Workshop on Metamorphic Testing},
pages = {70–75},
numpages = {6},
keywords = {Douban, social media, metamorphic relation, data quality assessment, sentiment analysis, machine translation, Oracle problem, data validation, metamorphic testing, natural language processing},
location = {Montreal, Quebec, Canada},
series = {MET '19}
}

@article{10.1145/2694428.2694441,
author = {Abadi, Daniel and Agrawal, Rakesh and Ailamaki, Anastasia and Balazinska, Magdalena and Bernstein, Philip A. and Carey, Michael J. and Chaudhuri, Surajit and Dean, Jeffrey and Doan, AnHai and Franklin, Michael J. and Gehrke, Johannes and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Jagadish, H. V. and Kossmann, Donald and Madden, Samuel and Mehrotra, Sharad and Milo, Tova and Naughton, Jeffrey F. and Ramakrishnan, Raghu and Markl, Volker and Olston, Christopher and Ooi, Beng Chin and R\'{e}, Christopher and Suciu, Dan and Stonebraker, Michael and Walter, Todd and Widom, Jennifer},
title = {The Beckman Report on Database Research},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2694428.2694441},
doi = {10.1145/2694428.2694441},
abstract = {Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14- 15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {61–70},
numpages = {10}
}

@inproceedings{10.1145/3018661.3022759,
author = {Ensan, Faezeh and Noorian, Zeinab and Bagheri, Ebrahim},
title = {Mining Actionable Insights from Social Networksat WSDM 2017},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3022759},
doi = {10.1145/3018661.3022759},
abstract = {The first international workshop on Mining Actionable Insights from Social Networks (MAISoN'17) is to be held on February 10, 2017; co-located with the Tenth ACM International Web Search and Data Mining (WSDM) Conference in Cambridge, UK. MAISoN'17 aims at bringing together researchers and participants from different disciplines such as computer science, big data mining, machine learning, social network analysis and other related areas in order to identify challenging problems and share ideas, algorithms, and technologies for mining actionable insight from social network data. We organized a workshop program that includes the presentation of eight peer-reviewed papers and keynote talks, which foster discussions around state-of-the-art in social network mining and will hopefully lead to future collaborations and exchanges.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {821–822},
numpages = {2},
keywords = {web mining, predictive modeling, social network analysis},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/2523616.2525963,
author = {Heintz, Benjamin and Chandra, Abhishek and Sitaraman, Ramesh K.},
title = {Wide-Area Streaming Analytics: Distributing the Data Cube},
year = {2013},
isbn = {9781450324281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523616.2525963},
doi = {10.1145/2523616.2525963},
abstract = {To date, much research in data-intensive computing has focused on batch computation. Increasingly, however, it is necessary to derive knowledge from big data streams. As a motivating example, consider a content delivery network (CDN) such as Akamai [4], comprising thousands of servers in hundreds of globally distributed locations. Each of these servers produces a stream of log data, recording for example every user it serves, along with each video stream they access, when they play and pause streams, and more. Each server also records network- and system-level data such as TCP connection statistics. In aggregate, the servers produce billions of lines of log data from over a thousand locations daily.},
booktitle = {Proceedings of the 4th Annual Symposium on Cloud Computing},
articleno = {55},
numpages = {2},
location = {Santa Clara, California},
series = {SOCC '13}
}

@inproceedings{10.1145/3465480.3466926,
author = {Kourtellis, Nicolas and Herodotou, Herodotos and Grzenda, Maciej and Wawrzyniak, Piotr and Bifet, Albert},
title = {S2CE: A Hybrid Cloud and Edge Orchestrator for Mining Exascale Distributed Streams},
year = {2021},
isbn = {9781450385558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465480.3466926},
doi = {10.1145/3465480.3466926},
abstract = {The explosive increase in volume, velocity, variety, and veracity of data generated by distributed and heterogeneous nodes such as IoT and other devices, continuously challenge the state of art in big data processing platforms and mining techniques. Consequently, it reveals an urgent need to address the ever-growing gap between this expected exascale data generation and the extraction of insights from these data. To address this need, this position paper proposes Stream to Cloud &amp; Edge (S2CE), a first of its kind, optimized, multi-cloud and edge orchestrator, easily configurable, scalable, and extensible. S2CE will enable machine and deep learning over voluminous and heterogeneous data streams running on hybrid cloud and edge settings, while offering the necessary functionalities for practical and scalable processing: data fusion and preprocessing, sampling and synthetic stream generation, cloud and edge smart resource management, and distributed processing.},
booktitle = {Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
pages = {103–113},
numpages = {11},
keywords = {edge analytics, data stream analysis, stream mining, cloud analytics, machine and deep learning},
location = {Virtual Event, Italy},
series = {DEBS '21}
}

@inproceedings{10.1145/2968219.2971593,
author = {De Masi, Alexandre and Ciman, Matteo and Gustarini, Mattia and Wac, Katarzyna},
title = {MQoL Smart Lab: Quality of Life Living Lab for Interdisciplinary Experiments},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2971593},
doi = {10.1145/2968219.2971593},
abstract = {As a base for hypothesis formulation and testing, accurate, timely and reproducible data collection is a challenge for all researchers. Data collection is especially challenging in uncontrolled environments, outside of the lab and when it involves many collaborating disciplines, where the data must serve quality research in all of them. In this paper, we present own "mQoL Smart Lab" for interdisciplinary research efforts on individuals' "Quality of Life" improvement. We present an evolution of our current in-house living lab platform enabling continuous, pervasive data collection from individuals' smartphones. We discuss opportunities for mQoL stemming from developments in machine learning and big data for advanced data analytics in different disciplines, better meeting the requirements put on the platform.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {635–640},
numpages = {6},
keywords = {platforms, data collection, data science, data analysis, smartphones, people centric sensing},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3423603.3424007,
author = {B\"{u}chler, Marco and Riegert, Sarah and Alpi, Federico and Cadeddu, Francesca},
title = {Towards Big Religious Data: RESILIENCE Research Infrastructure for Data on Religion in the Digital Age},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424007},
doi = {10.1145/3423603.3424007},
abstract = {Data in and for religion is arguably as old as humanity. Religious significance has been attached to an immense variety of artifacts and documents, often in written form, in nearly all spoken and written languages over the past millennia. The rise of the digital age gives to the scholar in religious studies the opportunity to build research over a much wider array of data than ever before; institutions which have data repositories (such as libraries, museums, universities, etc.) similarly have the chance to make their collections available to a larger community. On the other hand, however, there is a serious risk that a considerable amount of data gets lost during the "Digital transition". This paper presents the approach of the RESILIENCE Research Infrastructure in dealing with the issue of big data and data loss within the field of religious studies.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {9},
numpages = {5},
keywords = {digital transformation, religious studies, research infrastructures, big religious data},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/3268866.3268877,
author = {Zhong, Junmei and Gao, Chuangui and Yi, Xiu},
title = {Categorization of Patient Disease into ICD-10 with NLP and SVM for Chinese Electronic Health Record Analysis},
year = {2018},
isbn = {9781450365246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268866.3268877},
doi = {10.1145/3268866.3268877},
abstract = {The electronic health record (EHR) analysis has become an increasingly important application for artificial intelligence (AI) algorithms to leverage the insight from the big data for improving the quality of human healthcare. In a lot of Chinese EHR analysis applications, it is very important to categorize the patients' diseases according to the medical coding standard. In this paper, we develop NLP and machine learning algorithms to automatically categorize each patient's individual diseases into the ICD-10 coding standard. Experimental results show that the support vector machine algorithm (SVM) accomplishes very promising classification results.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition},
pages = {101–106},
numpages = {6},
keywords = {Electronic health record, NLP, ICD-10, machine learning, SVM},
location = {Beijing, China},
series = {AIPR 2018}
}

@inproceedings{10.1145/3390557.3394127,
author = {Zhan, Lin and Junhua, Zhao and Fan, Li and Zhifei, Wang},
title = {Research on Intelligent Management Platform of Highspeed Railway Traffic Safety Equipment Based on CPS},
year = {2020},
isbn = {9781450376587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3390557.3394127},
doi = {10.1145/3390557.3394127},
abstract = {From the view of high-speed railway traffic safety, this paper establishes an intelligent management platform for operation safety equipment based on CPS for "person-equipment-environment", and designs a framework of traffic safety system composed of perception control hardware, Internet of Things, cognitive decision-making and information services. The deep fusion of information system and traffic safety equipment is discussed, and the fault diagnosis method of driving equipment based on complex sensing technology is given, such as intelligent identification, online monitoring and ubiquitous sensing of the characteristics of safety protection equipment. Through the application of equipment fault diagnosis, it realizes the rapid retrieval and active collection of safety information, provides early warning and auxiliary decision-making, big data analysis and prediction, and improves the traffic safety.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence},
pages = {147–154},
numpages = {8},
keywords = {equipment fault diagnosis, traffic safety, CPS, High-speed railway},
location = {Xiamen, China},
series = {ICIAI 2020}
}

@inproceedings{10.1145/2638404.2638526,
author = {Wang, Maximilian J. and Mao, Guifen and Chen, Haiquan},
title = {Mining Multivariate Outliers: A Mixture Model-Based Framework},
year = {2014},
isbn = {9781450329231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2638404.2638526},
doi = {10.1145/2638404.2638526},
abstract = {Mining outliers has become more and more important in recent years. It has wide applications in military surveillance for enemy activities, detection of potential terrorist attacks, credit card fraud detection, network intrusion, computer virus attack, clinical trials, severe weather prediction, athlete performance analysis, and many other data mining tasks. In today's big data age, multivariate data sets are very complex. Variables among different dimensions are usually correlated with different variations. Classical data mining methods with Euclidean distance measure are not working well for mining multivariate outliers. In this study, we propose a normal mixture model-based framework of multivariate outlier detection. We fit the model parameters through the robust EM algorithm. The K-means clustering algorithm is used to provide the initial inputs for the EM algorithm. The well-know Mahalanobis distance is used to determine the cutoff points for outlier detection via the chi-square distribution critical values. Implementation details of this framework are also discussed.},
booktitle = {Proceedings of the 2014 ACM Southeast Regional Conference},
articleno = {51},
numpages = {4},
keywords = {outlier detection, mahalanobis distance, EM algorithm, data mining, k-means clustering algorithm, normal mixture models},
location = {Kennesaw, Georgia},
series = {ACM SE '14}
}

@article{10.1145/3404820.3404824,
author = {Gao, Song and Rao, Jinmeng and Kang, Yuhao and Liang, Yunlei and Kruse, Jake},
title = {Mapping County-Level Mobility Pattern Changes in the United States in Response to COVID-19},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
url = {https://doi.org/10.1145/3404820.3404824},
doi = {10.1145/3404820.3404824},
abstract = {To contain the COVID-19 epidemic, one of the non-pharmacological epidemic control measures is reducing the transmission rate of SARS-COV-2 in the population through social distancing. An interactive web-based mapping platform that provides timely quantitative information on how people in different counties and states reacted to the social distancing guidelines was developed by the GeoDS Lab @UW-Madison with the support of the National Science Foundation RAPID program. The web portal integrates geographic information systems (GIS) and daily updated human mobility statistical patterns (median travel distance and stay-at-home dwell time) derived from large-scale anonymized and aggregated smartphone location big data at the county-level in the United States, and aims to increase risk awareness of the public, support data-driven public health and governmental decision-making, and help enhance community responses to the COVID-19 pandemic.},
journal = {SIGSPATIAL Special},
month = {jun},
pages = {16–26},
numpages = {11}
}

@inproceedings{10.1145/3170427.3174367,
author = {Edge, Darren and Larson, Jonathan and White, Christopher},
title = {Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3174367},
doi = {10.1145/3170427.3174367},
abstract = {The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {data, visual analytics, business intelligence, ai, hci},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3325112.3325245,
author = {Harrison, Teresa and F. Luna-Reyes, Luis and Pardo, Theresa and De Paula, Nic and Najafabadi, Mahdi and Palmer, Jillian},
title = {The Data Firehose and AI in Government: Why Data Management is a Key to Value and Ethics},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325245},
doi = {10.1145/3325112.3325245},
abstract = {Technical and organizational innovations such as Open Data, Internet of Things and Big Data have fueled renewed interest in policy analytics in the public sector. This revamped version of policy analysis continues the long-standing tradition of applying statistical modeling to better understand policy effects and decision making, but also incorporates other computational approaches such as artificial intelligence (AI) and computer simulation. Although much attention has been given to the development of capabilities for data analysis, there is much less attention to understanding the role of data management in a context of AI in government. In this paper, we argue that data management capabilities are foundational to data analysis of any kind, but even more important in the present AI context. This is so because without proper data management, simply acquiring data or systems will not produce desired outcomes. We also argue that realizing the potential of AI for social good relies on investments specifically focused on this social outcome, investments in the processes of building trust in government data, and ensuring the data are ready and suitable for use, for both immediate and future uses.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {171–176},
numpages = {6},
keywords = {Policy Analysis, Data Management, Artificial Intelligence, Data Analytics, DMBOK},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1145/2948992.2949007,
author = {Almeida, Ricardo and Maio, Paulo and Oliveira, Paulo and Barroso, Jo\~{a}o},
title = {Ontology Based Rewriting Data Cleaning Operations},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949007},
doi = {10.1145/2948992.2949007},
abstract = {Dealing with increasing amounts of data creates the need to deal with redundant, inconsistent and/or complementary repositories which may be different in their data models and/or in their schema. Current data cleaning techniques developed to tackle data quality problems are just suitable for scenarios were all repositories share the same model and schema. Recently, an ontology-based methodology was proposed to overcome this limitation. In this paper, this methodology is briefly described and applied to a real scenario in the health domain with data quality problems.},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {85–88},
numpages = {4},
keywords = {Data Cleaning, Vocabulary, Ontology, Schema, Rewriting Process},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.5555/2693848.2694087,
author = {Rabe, Markus and Scheidler, Anne Antonia},
title = {An Approach for Increasing the Level of Accuracy in Supply Chain Simulation by Using Patterns on Input Data},
year = {2014},
publisher = {IEEE Press},
abstract = {Setting up simulation scenarios in the field of Supply Chains (SCs) is a big challenge because complex input data must be specified and careful input data management as well as precise model design are necessary. SC simulation needs a large amount of input data -- especially in times of big data, in which the data is often approximated by statistical distributions from real world observations. This paper deals with the question how the model itself and its input can be effectively complemented. This takes into account the commonly known fact, that the accuracy of a model output depends on the model input. Therefore an approach for using techniques of Knowledge Discovery in Databases is introduced to derive logical relations from the data. We discuss how Knowledge Discovery would be applied, as a preprocessing step for simulation scenario setups, in order to provide benefits for the level of accuracy in simulation models.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {1897–1906},
numpages = {10},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3282933.3282935,
author = {Mack, Vincent Z. W. and Kam, Tin Seong},
title = {Is There Space for Violence? A Data-Driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict},
year = {2018},
isbn = {9781450360326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282933.3282935},
doi = {10.1145/3282933.3282935},
abstract = {With recent increases in incidences of political violence globally, the world has now become more uncertain and less predictable. Of particular concern is the case of violence against civilians, who are often caught in the crossfire between armed state or non-state actors. Classical methods of studying political violence and international relations need to be updated. Adopting the use of data analytic tools and techniques of studying big data would enable academics and policy makers to make sense of a rapidly changing world.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities},
articleno = {1},
numpages = {10},
keywords = {Africa, hotspot detection, geospatial autocorrelation, political violence, knowledge discovery},
location = {Seattle, WA, USA},
series = {GeoHumanities'18}
}

@inproceedings{10.1145/3531072.3535322,
author = {Longo, Antonella and Zappatore, Marco and Martella, Angelo and Rucco, Chiara},
title = {Enhancing Data Education with Datathons: An Experience with Open Data on Renewable Energy Systems},
year = {2022},
isbn = {9781450393508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531072.3535322},
doi = {10.1145/3531072.3535322},
abstract = {Data literacy and the fundamentals of big data management are becoming interdisciplinary in Higher Education curricula, also due to the widespread need of data science skills. This casts the need for presenting novel (and more engaging) learning activities to students. Data hackathons (also known as datathons) represent a viable option to allow students practicing with real use cases and datasets, as well as addressing their learning experiences collaboratively. Moreover, datathons promise to improve soft skills and offer hands-on learning opportunities. Therefore, we present in this paper a datathon on a publicly available dataset about renewable energy systems. The datathon involved students from three data-focused courses of three different M.S. degrees at the University of Salento (Italy). A detailed analysis of the design, implementation and evaluation choices is proposed, along with a series of gathered insights and lessons learned that might help systematizing the introduction and use of datathons in data education.},
booktitle = {1st International Workshop on Data Systems Education},
pages = {26–31},
numpages = {6},
keywords = {datathon, data education curricula, renewable energy systems, open data},
location = {Philadelphia, PA, USA},
series = {DataEd '22}
}

@inproceedings{10.1145/3173574.3174043,
author = {Verma, Nitya and Dombrowski, Lynn},
title = {Confronting Social Criticisms: Challenges When Adopting Data-Driven Policing Strategies},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174043},
doi = {10.1145/3173574.3174043},
abstract = {Proponents of data-driven policing strategies claim that it makes policing organizations more effective, efficient, and accountable and has the potential to address some policing social criticisms (e.g. racial bias, lack of accountability and training). What remains less understood are the challenges when adopting data-driven policing as a response to these criticisms. We present results from a qualitative field study about the adoption of data-driven policing strategies in a Midwestern police department in the United States. We identify three key challenges police face with data-driven adoption efforts: data-driven frictions, precarious and inactionable insights, and police metis concerns. We demonstrate the issues that data-driven initiatives create for policing and the open questions police agents face. These findings contribute an empirical account of how policing agents attend to the strengths and limits of big data's knowledge claims. Lastly, we present data and design implications for policing.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {metis, data-driven organizations, data practices, law enforcement, policing, challenges},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3414752.3414800,
author = {Huang, Qibao and Huang, Yiqi},
title = {The Significance of Urban Cockpit for Urban Brain Construction},
year = {2020},
isbn = {9781450388016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414752.3414800},
doi = {10.1145/3414752.3414800},
abstract = {The urban cockpit will comprehensively perceive and process all kinds of data in the city operation, establish the data chassis of the smart city, objectively, comprehensively and multi dimensionally display the operation situation of the city, and carry out early warning, prediction and scientific disposal of outstanding problems and emergencies in the city operation. Moreover, in the near future, with the introduction and use of 5G, artificial intelligence, big data, data Luan Sheng and edge computing in the city brain project, the city cockpit will also give the city managers and visitors a better and more beautiful feeling in the display effect (such as immersion and three-dimensional), thus accelerating the promotion and landing of the city brain project and promoting social governance Intelligent and professional, improve the level of comprehensive city governance, and change the transformation and upgrading of the city from extensive to precise and refined.},
booktitle = {2020 The 11th International Conference on E-Business, Management and Economics},
pages = {70–73},
numpages = {4},
keywords = {Urban brain, Data, Urban cockpit},
location = {Beijing, China},
series = {ICEME 2020}
}

@inproceedings{10.1145/2330601.2330605,
author = {Siemens, George},
title = {Learning Analytics: Envisioning a Research Discipline and a Domain of Practice},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330605},
doi = {10.1145/2330601.2330605},
abstract = {Learning analytics are rapidly being implemented in different educational settings, often without the guidance of a research base. Vendors incorporate analytics practices, models, and algorithms from datamining, business intelligence, and the emerging "big data" fields. Researchers, in contrast, have built up a substantial base of techniques for analyzing discourse, social networks, sentiments, predictive models, and in semantic content (i.e., "intelligent" curriculum). In spite of the currently limited knowledge exchange and dialogue between researchers, vendors, and practitioners, existing learning analytics implementations indicate significant potential for generating novel insight into learning and vital educational practices. This paper presents an integrated and holistic vision for advancing learning analytics as a research discipline and a domain of practices. Potential areas of collaboration and overlap are presented with the intent of increasing the impact of analytics on teaching, learning, and the education system.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {4–8},
numpages = {5},
keywords = {data integration, collaboration, learning analytics, ethics, research, theory, practice},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@article{10.1145/3124391,
author = {Santana, Eduardo Felipe Zambom and Chaves, Ana Paula and Gerosa, Marco Aurelio and Kon, Fabio and Milojicic, Dejan S.},
title = {Software Platforms for Smart Cities: Concepts, Requirements, Challenges, and a Unified Reference Architecture},
year = {2017},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3124391},
doi = {10.1145/3124391},
abstract = {Information and communication technologies (ICT) can be instrumental in progressing towards smarter city environments, which improve city services, sustainability, and citizens’ quality of life. Smart City software platforms can support the development and integration of Smart City applications. However, the ICT community must overcome current technological and scientific challenges before these platforms can be widely adopted. This article surveys the state of the art in software platforms for Smart Cities. We analyzed 23 projects concerning the most used enabling technologies, as well as functional and non-functional requirements, classifying them into four categories: Cyber-Physical Systems, Internet of Things, Big Data, and Cloud Computing. Based on these results, we derived a reference architecture to guide the development of next-generation software platforms for Smart Cities. Finally, we enumerated the most frequently cited open research challenges and discussed future opportunities. This survey provides important references to help application developers, city managers, system operators, end-users, and Smart City researchers make project, investment, and research decisions.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {78},
numpages = {37},
keywords = {software platforms, Wireless sensor networks}
}

@inproceedings{10.1145/2939672.2945388,
author = {Zhu, Qiang and Guo, Songtao and Ogilvie, Paul and Liu, Yan},
title = {Business Applications of Predictive Modeling at Scale},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945388},
doi = {10.1145/2939672.2945388},
abstract = {Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2139–2140},
numpages = {2},
keywords = {machine learning, machine learning platforms, predictive modeling, business analytics},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3106426.3106434,
author = {Martins, Denis Mayr Lima and Vossen, Gottfried and de Lima Neto, Fernando Buarque},
title = {Intelligent Decision Support for Data Purchase},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106434},
doi = {10.1145/3106426.3106434},
abstract = {The Big Data era is affording a paradigm change on decision-making approaches. More and more, companies as well as individuals are relying on data rather than on the so called "gut feeling" to make decisions. However, searching the Web for carrying out purchases is not completely satisfactory yet, given the arduousness of finding suitable quality data. This has contributed to the emergence of data marketplaces as an alternative to traditional data commerce, as they provide appropriate online environments for data offering and purchasing. Nevertheless, as the number of available datasets to purchase increases, the task of buying appropriate offers is, very often, challenging. In this sense, we propose an intelligent decision support system to help buyers in purchasing data offers based on a multiple-criteria decision analysis. Experimental results show that our approach provides an interactive way that addresses buyers' needs, allowing them to state and easily refine their preferences, without any specific order, via a series of dataset recommendations.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {396–402},
numpages = {7},
keywords = {computational intelligence, personalization, decision support, data purchase},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3377458.3377464,
author = {Yuwei, Sun and Jianbao, Zhu and Qingshan, Ma and Xinchun, Yu and Ye, Shi and Yu, Chen},
title = {Picture Management of Power Supply Safety Management System Based on Deep Learning Technology},
year = {2019},
isbn = {9781450372640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377458.3377464},
doi = {10.1145/3377458.3377464},
abstract = {With the advent of the era of big data, power supply security management systems will get a lot of picture data. In the face of massive image data, this paper studies the image management technology based on convolutional neural network. Aiming at the high repetition rate of self built image database samples and the problem that many sample classes contain uncorrelated images, two algorithms are proposed to improve the quality of the database: de duplication and de uncorrelation. By using the depth convolution neural network, the Embedding represented by the corresponding image is taken, and the distance between Embedding is calculated in the Euclidean space to achieve the purpose of de duplication and de uncorrelation. In this paper, "time" and "accuracy" are used to evaluate the performance of de duplication and de uncorrelation algorithms. The comparison examples of some sample classes before and after removing repetition and before and after removing uncorrelation are shown. The Recall-value of the database after removing duplicate and uncorrelated is tested based on the GoogLe Netplus-model respectively, which proves the effectiveness of the two filtering algorithms and overcomes the complexity of the traditional filtering process.},
booktitle = {Proceedings of the 2019 5th International Conference on Systems, Control and Communications},
pages = {71–75},
numpages = {5},
keywords = {deep learning, convolutional neural network, Picture management},
location = {Wuhan, China},
series = {ICSCC 2019}
}

@inproceedings{10.1145/3408877.3432457,
author = {Fekete, Alan and Kay, Judy and R\"{o}hm, Uwe},
title = {A Data-Centric Computing Curriculum for a Data Science Major},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432457},
doi = {10.1145/3408877.3432457},
abstract = {Many universities are introducing a new major in Data Science into their offering, to reflect the explosive growth in this field and the career opportunities it provides. As a field Data Science has elements from Computer Science and from Statistics, and curricula plans differ widely, both in the balance between the CS and Stats aspects, and also in the emphasis within the computing topics. This paper reports on the curriculum that has been taught for three years now at the University of Sydney. In particular, we describe the approach of a sequence of computing subjects which were developed specifically for the major, in order to bring students over several years to a sophisticated understanding of the data-handling aspects of Data Science. Students also take traditional subjects from both CS (such as Data Structures or AI) and from Statistics (such as Learning from Data and Statistical Inference). The data-centric specially-designed subjects we discuss in this paper are (i) Informatics: Data and Computation (in the first year), (ii) Big Data and Data Diversity (in the second year), and then upper-division subjects on (iii) Data Science Platforms, and (iv) Human-in-the-Loop Data Analytics.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {865–871},
numpages = {7},
keywords = {curriculum, data science},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/2522848.2522892,
author = {Neumann, Alexander and Schnier, Christian and Hermann, Thomas and Pitsch, Karola},
title = {Interaction Analysis and Joint Attention Tracking in Augmented Reality},
year = {2013},
isbn = {9781450321297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522848.2522892},
doi = {10.1145/2522848.2522892},
abstract = {Multimodal research in human interaction has to consider a variety of factors, ranging from local short-time phenomena to complex interaction patterns. As of today, no single discipline engaged in communication research offers the methods and tools to investigate the full complexity continuum in a time-efficient way. A synthesis of qualitative and quantitative analysis is required to merge insights about micro-sequential structures with big data patterns. Using the example of a co-present dyadic negotiation analysis to combine methods offered by Conversation Analysis and Data Mining, we show how such a partnership can benefit each discipline and lead to insights as well as new hypotheses evaluation opportunities.},
booktitle = {Proceedings of the 15th ACM on International Conference on Multimodal Interaction},
pages = {165–172},
numpages = {8},
keywords = {data mining, interaction studies, multimodality, conversation analysis},
location = {Sydney, Australia},
series = {ICMI '13}
}

@inproceedings{10.1145/3531146.3533151,
author = {Fischer, Maximilian T. and Hirsbrunner, Simon David and Jentner, Wolfgang and Miller, Matthias and Keim, Daniel A. and Helm, Paula},
title = {Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533151},
doi = {10.1145/3531146.3533151},
abstract = {Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science &amp; Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {877–889},
numpages = {13},
keywords = {Communication Analysis, Visual Analytics, Science &amp; Technology Studies, Intelligence Analysis, Ethic Awareness, Critical Data Studies, Critical Algorithm Studies, Machine Learning, Interdisciplinary Research},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{10.14778/3554821.3554899,
author = {Fan, Wenfei},
title = {Big Graphs: Challenges and Opportunities},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554899},
doi = {10.14778/3554821.3554899},
abstract = {Big data is typically characterized with 4V's: Volume, Velocity, Variety and Veracity. When it comes to big graphs, these challenges become even more staggering. Each and every of the 4V's raises new questions, from theory to systems and practice. Is it possible to parallelize sequential graph algorithms and guarantee the correctness of the parallelized computations? Given a computational problem, does there exist a parallel algorithm for it that guarantees to reduce parallel runtime when more machines are used? Is there a systematic method for developing incremental algorithms with effectiveness guarantees in response to frequent updates? Is it possible to write queries across relational databases and semistructured graphs in SQL? Can we unify logic rules and machine learning, to improve the quality of graph-structured data, and deduce associations between entities? This paper aims to incite interest and curiosity in these topics. It raises as many questions as it answers.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3782–3797},
numpages = {16}
}

@article{10.1145/3532784,
author = {Li, Yuanxia and Currim, Faiz and Ram, Sudha},
title = {Data Completeness and Complex Semantics in Conceptual Modeling: The Need for a Disaggregation Construct},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3532784},
doi = {10.1145/3532784},
abstract = {Conceptual modeling is important for developing databases that maintain the integrity and quality of stored information. However, classical conceptual models have often been assumed to work on well-maintained and high-quality data. With the advancement and expansion of data science, it is no longer the case. The need to model and store data has emerged for settings with lower data quality, which creates the need to update and augment conceptual models to represent lower-quality data. In this paper, we focus on the intersection between data completeness (an important aspect of data quality) and complex class semantics (where a complex class entity represents information that spans more than one simple class entity). We propose a new disaggregation construct to allow the modeling of incomplete information. We demonstrate the use of our disaggregation construct for diverse modeling problems and discuss the anomalies that could occur without this construct. We provide formal definitions and thorough comparisons between various types of complex constructs to guide future application and prove the unique interpretation of our newly proposed disaggregation construct.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = {apr},
keywords = {Database design, Complex semantics, Disaggregation construct}
}

@inproceedings{10.1145/2611040.2611086,
author = {Omitola, Tope and Davies, John and Duke, Alistair and Glaser, Hugh and Shadbolt, Nigel},
title = {Linking Social, Open, and Enterprise Data},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611086},
doi = {10.1145/2611040.2611086},
abstract = {The new world of big data, of the LOD cloud, of the app economy, and of social media means that organisations no longer own, much less control, all the data they need to make the best informed business decisions. In this paper, we describe how we built a system using Linked Data principles to bring in data from Web 2.0 sites (LinkedIn, Salesforce), and other external business sites such as OpenCorporates, linking these together with pertinent internal British Telecommunications enterprise data into that enterprise data space. We describe the challenges faced during the implementation, which include sourcing the datasets, finding the appropriate "join points" from the individual datasets, as well as developing the client application used for data publication. We describe our solutions to these challenges and discuss the design decisions made. We conclude by drawing some general principles from this work.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {41},
numpages = {8},
keywords = {Navigation, Semantic networks, User issues, Architectures, Hypertext/Hypermedia},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@inproceedings{10.1145/2331801.2331803,
author = {Alsubaiee, Sattam and Behm, Alexander and Grover, Raman and Vernica, Rares and Borkar, Vinayak and Carey, Michael J. and Li, Chen},
title = {ASTERIX: Scalable Warehouse-Style Web Data Integration},
year = {2012},
isbn = {9781450312394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2331801.2331803},
doi = {10.1145/2331801.2331803},
abstract = {A growing wealth of digital information is being generated on a daily basis in social networks, blogs, online communities, etc. Organizations and researchers in a wide variety of domains recognize that there is tremendous value and insight to be gained by warehousing this emerging data and making it available for querying, analysis, and other purposes. This new breed of "Big Data" applications poses challenging requirements against data management platforms in terms of scalability, flexibility, manageability, and analysis capabilities. At UC Irvine, we are building a next-generation database system, called ASTERIX, in response to these trends. We present ongoing work that approaches the following questions: How does data get into the system? What primitives should we provide to better cope with dirty/noisy data? How can we support efficient data analysis on spatial data? Using real examples, we show the capabilities of ASTERIX for ingesting data via feeds, supporting set-similarity predicates for fuzzy matching, and answering spatial aggregation queries.},
booktitle = {Proceedings of the Ninth International Workshop on Information Integration on the Web},
articleno = {2},
numpages = {4},
keywords = {hyracks, cloud computing, ASTERIX, semistructured data, data-intensive computing},
location = {Scottsdale, Arizona, USA},
series = {IIWeb '12}
}

@inproceedings{10.1145/3290605.3300561,
author = {Deeb-Swihart, Julia and Endert, Alex and Bruckman, Amy},
title = {Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300561},
doi = {10.1145/3290605.3300561},
abstract = {In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {qualitative, human trafficking, needs analysis, law enforcement},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3535508.3545519,
author = {Goel, Aekansh and Mudge, Zachary and Bi, Sarah and Brenner, Charles and Huffman, Nicholas and Giuste, Felipe and Marteau, Benoit and Shi, Wenqi and Wang, May D.},
title = {Identification of COVID-19 Severity and Associated Genetic Biomarkers Based on ScRNA-Seq Data},
year = {2022},
isbn = {9781450393867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535508.3545519},
doi = {10.1145/3535508.3545519},
abstract = {Bio-marker identification for COVID-19 remains a vital research area to improve current and future pandemic responses. Innovative artificial intelligence and machine learning-based systems may leverage the large quantity and complexity of single cell sequencing data to quickly identify disease with high sensitivity. In this study, we developed a novel approach to classify patient COVID-19 infection severity using single-cell sequencing data derived from patient BronchoAlveolar Lavage Fluid (BALF) samples. We also identified key genetic biomarkers associated with COVID-19 infection severity. Feature importance scores from high performing COVID-19 classifiers were used to identify a set of novel genetic biomarkers that are predictive of COVID-19 infection severity. Treatment development and pandemic reaction may be greatly improved using our novel big-data approach. Our implementation is available on https://github.com/aekanshgoel/COVID-19_scRNAseq.},
booktitle = {Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {45},
numpages = {5},
keywords = {single cell RNA sequencing, gene markers, COVID-19, bronchoalveolar lavage fluid, model interpretation, health informatics},
location = {Northbrook, Illinois},
series = {BCB '22}
}

@inproceedings{10.1145/3487664.3487719,
author = {Caruccio, Loredana and Cirillo, Stefano and Deufemia, Vincenzo and Polese, Giuseppe},
title = {Efficient Discovery of Functional Dependencies from Incremental Databases},
year = {2021},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487719},
doi = {10.1145/3487664.3487719},
abstract = {With the advent of Big Data there is an increasing necessity to incrementally mine information from data originating from sensors and other dynamic sources. Thus, it is necessary to devise algorithms capable of mining useful information upon possible evolutions of databases. Among these, there are certainly data profiling info, such as functional dependencies (fd for short), which are particularly useful for data integration and for assessing the quality of data. The incremental scenario requires the definition of search strategies and validation methods able to analyze only the portion of the dataset affected by the last changes. In this paper, we propose a new validation method, which exploits regular expressions and compressed data structures to efficiently verify whether a candidate fd holds on an updated version of the dataset. Experimental results demonstrate the effectiveness of the proposed method on real-world datasets adapted for incremental scenarios, also compared with a baseline incremental fd discovery algorithm.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {400–409},
numpages = {10},
keywords = {Functional Dependency, Incremental Discovery, Data Mining, Data Profiling},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3035918.3058740,
author = {Castro Fernandez, Raul and Deng, Dong and Mansour, Essam and Qahtan, Abdulhakim A. and Tao, Wenbo and Abedjan, Ziawasch and Elmagarmid, Ahmed and Ilyas, Ihab F. and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
title = {A Demo of the Data Civilizer System},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3058740},
doi = {10.1145/3035918.3058740},
abstract = {Finding relevant data for a specific task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80%) of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate DATA CIVILIZER to ease the pain faced in analyzing data "in the wild". DATA CIVILIZER is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1639–1642},
numpages = {4},
keywords = {data stitching, data cleaning, join path discovery, data discovery, data integration, polystore queries},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3347146.3359090,
author = {Zhao, Kai and Feng, Jie and Xu, Zhao and Xia, Tong and Chen, Lin and Sun, Funing and Guo, Diansheng and Jin, Depeng and Li, Yong},
title = {DeepMM: Deep Learning Based Map Matching with Data Augmentation},
year = {2019},
isbn = {9781450369091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347146.3359090},
doi = {10.1145/3347146.3359090},
abstract = {Map matching is important in many trajectory based applications like route optimization and traffic schedule, etc. As the widely used methods, Hidden Markov Model and its variants are well studied to provide accurate and efficient map matching service. However, HMM based methods fail to utilize the value of enormous trajectory big data, which are useful for the map matching task. Furthermore, with many following-up works, they are still easily influenced by the noisy records, which are very common in the real system. To solve these problems, we revisit the map matching task from the data perspective, and propose to utilize the great power of data to help solve these problems. We build a deep learning based model to utilize all the trajectory data for joint training and knowledge sharing. With the help of embedding techniques and sequence learning model with attention enhancement, our system does the map matching in the latent space, which is tolerant to the noise in the physical space. Extensive experiments demonstrate that our model outperforms the widely used HMM based methods more than 10% (absolute accuracy) and works robustly in the noisy settings in the meantime.},
booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {452–455},
numpages = {4},
keywords = {map matching, deep learning, data driven system},
location = {Chicago, IL, USA},
series = {SIGSPATIAL '19}
}

@inproceedings{10.1145/3429889.3429938,
author = {Chen, Juan and Lu, Yan and Zhang, Ting and Ouyang, Zhaolian},
title = {Artificial Intelligence in Medicine in the United States, China and India},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429938},
doi = {10.1145/3429889.3429938},
abstract = {Objective: To compare the development status of artificial intelligence (AI) in medicine among the United States (US), China and India with bibliometric analysis. Methods: Articles involving AI in medicine published from 2015 to 2019 were retrieved on March 30, 2020 from Web of Science Core Collection. The country-level and the institution-level performance of the US, China and India in the field of AI in medicine were compared with indicators including the amount of papers, 5-year Compound Annual Growth Rate (CAGR) of the amount of papers, the amount of highly-cited papers, the proportion of highly-cited papers and the average citations per paper. In addition, the research hotspots and international cooperation of the three countries in recent 5 years were compared by conducting keywords co-occurrence analysis and co-authorship analysis in VOSviewer. Results: From 2015 to 2019, The US has published 7838 papers and 154 highly-cited papers in the field of AI in medicine, with an average citations per paper to be 9.3, and the proportion of highly-cited papers to be 2.0 %. China has output 6635 papers and 73 highly-cited papers in this field, with an average citations per paper to be 5.3, and the proportion of highly-cited papers to be 1.1%. India has output 3895 papers and 22 highly-cited papers in this field, with an average citations per paper to be 3.6, and the proportion of highly-cited papers to be 0.6%. The 5-year CAGR of the US, China and India in the period of 2015~2019 were 16.0%, 25.4% and 2.4%, respectively. At the institutional level, most of these indicators were significantly better for the US institutions than for Chinese and Indian ones. There were four research hotspots in this field, namely medical imaging technology, health big data mining, disease prediction with biomarkers and genetic information, and early diagnosis of neurological disease. The three countries focused on different hotspots, with China focusing relatively less on health big data mining, while the US and India being complementary to each other. As to international cooperation, the average links per paper to other countries were 0.60, 0.40 and 0.20, respectively, for the US, China and India. Conclusions: In the field of AI in medicine, the US, with a number of competitive institutions in AI and medical researches, is taking a definitely leading role, having conducted many innovative researches and cooperated extensively with other countries. China is taking the second leading role at the country level, with top institutions somewhat less productive than those in the US. India is the third productive country, with top institutions obvious less productive than those in the US, and with research hotspots exactly complementary to the US.},
booktitle = {Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences},
pages = {257–264},
numpages = {8},
keywords = {Development status, Information technology, Medical sciences, Artificial intelligence, Bibliometric analysis},
location = {Beijing, China},
series = {ISAIMS 2020}
}

@inproceedings{10.1145/3291064.3291074,
author = {H G, Monika Rani and R, Sapna and Mishra, Shakti},
title = {An Investigative Study on the Quality Aspects of Linked Open Data},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291074},
doi = {10.1145/3291064.3291074},
abstract = {Linked Open Data refers to a set of best practices that empowers enterprises to publish and interlink their data using existing ontologies on the Semantic Web. The focus of linked open data is to move from document-based Web to a Web of interlinked data, created by typed links between data from different data sources. Linked open data expert group has taken cognizance of data quality importance, as the amount of linked data publications grown on the Web substantially. Measures have been taken to check the linked data quality. But, these measures are diverse in nature with respect to quality terms. This makes the comparison and evaluation difficult, leading to an incorrect selection of accurate data sources based on quality requirements. In this paper, we carried out an analysis on linked data, the quality of linked data, the frameworks to assess the quality of linked data and the challenges to achieve the quality of linked open data.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {33–39},
numpages = {7},
keywords = {Semantic Web, Quality of linked open data, Linked open data},
location = {Singapore, Singapore},
series = {CCIOT 2018}
}

@inproceedings{10.1145/3486611.3491133,
author = {Chowdhury, Tahiya and Ding, Qizhen and Mandel, Ilan and Ju, Wendy and Ortiz, Jorge},
title = {Tracking Urban Heartbeat and Policy Compliance through Vision and Language-Based Sensing},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3491133},
doi = {10.1145/3486611.3491133},
abstract = {Sensing activities at the city scale using big data can enable applications to improve the quality of citizen life. While there are approaches to sense the urban heartbeat using sound, vision, radio frequency (RF), and other sensors, capturing changes at urban scale using such sensing modalities is challenging. Due to the enormous amount of data they produce and the associated annotation and processing requirement, such data can be of limited use. In this paper, we present a vision-to-language modeling approach to capture patterns and transitions that occur in New York City from March 2020 to August 2020. We use the model on ~1 million street images captured by dashcams over 6 months. We then use the captions to train a language model based on Latent Dirichlet Allocation [4] and compare models from different periods using probabilistic distance measures. We observe distribution shifts in the model that correlate well with social distancing policies and are corroborated by different data sources, such as mobility traces. This language-based sensing introduces a new sensing modality to capture dynamics in the city with lower storage requirements and privacy concerns.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {302–306},
numpages = {5},
keywords = {COVID-19, urban sensing, computer vision and language},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3543106.3543113,
author = {Meiryani, Meiryani and Aprilia, Kanaya Regina and Warganegara, Dezie Leonarda and Yanti, Yanti},
title = {Challenges of the Accounting Profession in the Era of the Industrial Revolution 4.0},
year = {2022},
isbn = {9781450397162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543106.3543113},
doi = {10.1145/3543106.3543113},
abstract = {In the current industrial era, namely 4.0, it greatly facilitates the industrial world and human work and brings changes in human work adjustments. Where there are many changes in human life, for example in communication and data processing, both for individuals and for companies [1]. The characteristic of the industrial revolution 4.0 is the emergence of many applied technologies, such as the internet of things, cloud computing, Big Data, and artificial intelligence which as a whole can change business models and production patterns, especially in various industrial sectors [4]. In the current era of the industrial revolution, it is a challenge that is not easy and difficult for accountants as there is a lot of information technology present because financial transactions do not only use cash, but also use digital money [1]. The research objectives in this paper are to determine the role of accountants in the era of the industrial revolution 4.0 and to find out how accountants challenge the industrial revolution 4.0. The data in this study were obtained from library sources such as collecting journals, websites, and related party documents, and sources from other media that can support the completeness of research data so that this research can run correctly and according to reality.},
booktitle = {Proceedings of the 2022 International Conference on E-Business and Mobile Commerce},
pages = {39–46},
numpages = {8},
keywords = {Accountants, Challenges, Industrial revolution 4.0},
location = {Seoul, Republic of Korea},
series = {ICEMC '22}
}

@inproceedings{10.1145/3274192.3274219,
author = {Strey, Mateus Rambo and Pereira, Roberto and de Castro Salgado, Luciana C.},
title = {Human Data-Interaction: A Systematic Mapping},
year = {2018},
isbn = {9781450366014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274192.3274219},
doi = {10.1145/3274192.3274219},
abstract = {Big Data, e-Science and Internet of Things have contributed to increase the production, processing and storage of data, changing the way people deal and live with data. Although the problem is not new, the "human aspect" of data and the possible impact of Human-Data Interaction (HDI) in human life have been explored and discussed as an emerging research area. On the one hand, HDI offers plenty of opportunities for research and development, and on the other hand it demands characterization, grounding, critical discussions, empirical results and thinking tools to support research and practice. This paper presents a Systematic Mapping of Literature on HDI in Computer Science, identifying the different definitions for the area, elements or objects of investigation, contexts of application, stakeholders, etc. Based on 28 selected papers, results point out to a lack of definition or agreement on what HDI is, but suggest that there are different aspects that can characterize it, and allow identifying concerns and objects of study, such as privacy, ownership and transparency. Results suggest a demand for theoretical and methodological frameworks to support the understanding, design and evaluation of HDI via computing systems.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {27},
numpages = {12},
keywords = {Systematic Mapping Review, Human-Data Interaction, Human-Computer Interaction},
location = {Bel\'{e}m, Brazil},
series = {IHC 2018}
}

@inproceedings{10.1145/2791347.2791371,
author = {Efros, Pavel and Buchmann, Erik and Englhardt, Adrian and B\"{o}hm, Klemens},
title = {How to Quantify the Impact of Lossy Transformations on Change Detection},
year = {2015},
isbn = {9781450337090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791347.2791371},
doi = {10.1145/2791347.2791371},
abstract = {To ease the proliferation of big data, it frequently is transformed, be it by compression, be it by anonymization. Such transformations however modify characteristics of the data, such as changes in the case of time series. Changes however are important for subsequent analyses. The impact of those modifications depends on the application scenario, and quantifying it is far from trivial. This is because a transformation can shift or modify existing changes or introduce new ones. In this paper, we propose MILTON, a flexible and robust Measure for quantifying the Impact of Lossy Transformations on subsequent change detectiON. MILTON is applicable to any lossy transformation technique on time-series data and to any general-purpose change-detection approach. We have evaluated it with three real-world use cases. Our evaluation shows that MILTON allows to quantify the impact of lossy transformations and to choose the best one from a class of transformation techniques for a given application scenario.},
booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
articleno = {17},
numpages = {11},
location = {La Jolla, California},
series = {SSDBM '15}
}

@article{10.1145/3353401.3353406,
author = {Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel},
title = {Recognizing Experts on Social Media: A Heuristics-Based Approach},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/3353401.3353406},
doi = {10.1145/3353401.3353406},
abstract = {Knowing who is an expert on social media is a challenging yet important task, especially in a world where misleading information is commonplace and where social media is an important information source for knowledge seekers. In this paper we investigate expertise heuristics by comparing features of experts versus non-experts in big data settings. We employ a large set of features to classify experts and non-experts using data collected on two social media platform (Twitter and reddit). Our results show a good ability to predict who is an expert, especially using language-based features, validating that heuristics can be developed to differentiate experts from novices organically, based on social media use. Our results contribute to the development of expertise location and identification systems as well as our understanding on how experts present themselves on social media.},
journal = {SIGMIS Database},
month = {jul},
pages = {66–84},
numpages = {19},
keywords = {social media, data analytics., expertise location}
}

@article{10.14778/2824032.2824070,
author = {Dasu, Tamraparni and Shkapenyuk, Vladislav and Srivastava, Divesh and Swayne, Deborah F.},
title = {FIT to Monitor Feed Quality},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824070},
doi = {10.14778/2824032.2824070},
abstract = {While there has been significant focus on collecting and managing data feeds, it is only now that attention is turning to their quality. In this paper, we propose a principled approach to online data quality monitoring in a dynamic feed environment. Our goal is to alert quickly when feed behavior deviates from expectations.We make contributions in two distinct directions. First, we propose novel enhancements to permit a publish-subscribe approach to incorporate data quality modules into the DFMS architecture. Second, we propose novel temporal extensions to standard statistical techniques to adapt them to online feed monitoring for outlier detection and alert generation at multiple scales along three dimensions: aggregation at multiple time intervals to detect at varying levels of sensitivity; multiple lengths of data history for varying the speed at which models adapt to change; and multiple levels of monitoring delay to address lagged data arrival.FIT, or Feed Inspection Tool, is the result of a successful implementation of our approach. We present several case studies outlining the effective deployment of FIT in real applications along with user testimonials.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1728–1739},
numpages = {12}
}

@inproceedings{10.1145/3544109.3544151,
author = {Zhang, Jun and Liu, Longlong},
title = {Research on Recommendation Strategy of E-Commerce User Portrait Based on User Dynamic Interest Factor Hybrid Recommendation Algorithm},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544151},
doi = {10.1145/3544109.3544151},
abstract = {In modern society with the good and fast development of mobile e-commerce, the commodity information and the user behavior data accompanying it show an explosive and sudden growth trend, which also leads to the emergence of information overload on the e-commerce platform, and the proposed personalized recommendation system for e-commerce users largely alleviates this problem mentioned above. The personalized recommendation system for e-commerce users aims to solve the information overload of e-commerce platform by analyzing the user behavior data of e-commerce platform, so as to explore the interest preference of e-commerce platform users and make active recommendation of advertising content related to e-commerce platform. Although the research on recommendation algorithms for e-commerce platforms has made great progress, there are still challenges in terms of sparse data, static user features and interpretability of e-commerce platform recommendation results in terms of big data feature recognition. Therefore, in this paper, a hybrid recommendation algorithm based on the forgetting curve of e-commerce platform and the automatic feature construction of e-commerce platform is studied in the e-commerce scenario of e-commerce platform, combined with the e-commerce data collected in real field, for the sparsity of e-commerce platform data, the interpretability of recommendation results and the static nature of e-commerce platform user features.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {225–230},
numpages = {6},
location = {Dalian, China},
series = {IPEC '22}
}

@inproceedings{10.1145/2442952.2442955,
author = {Mehta, Paras and Voisard, Agn\`{e}s},
title = {Analysis of User Mobility Data Sources for Multi-User Context Modeling},
year = {2012},
isbn = {9781450316941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442952.2442955},
doi = {10.1145/2442952.2442955},
abstract = {Finding the right data source for research is a challenge that many of us face. Although we live in times where 'Open Data' and 'Big Data' have become buzzwords, getting hold of a reasonable size and quality dataset is often hard. When it comes to user data such as mobility data, this becomes even tougher due to privacy-related concerns. This paper briefly explains our research in the area of multi-user context modeling and presents some criteria that we believe are important while selecting a dataset for testing different approaches in this domain. To find the right dataset, some relevant publicly available human mobility datasets are examined using these criteria. The following are the datasets that have been analyzed: Microsoft Research GeoLife Trajectory Dataset, Tracking Delft I Pedestrian Trajectory Dataset, MIT Media Lab Reality Mining Dataset and LifeMap Dataset. Besides these, some other useful data sources for researchers have been cited.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Crowdsourced and Volunteered Geographic Information},
pages = {9–14},
numpages = {6},
keywords = {multi-user, context, situation, mobility, model, dataset},
location = {Redondo Beach, California},
series = {GEOCROWD '12}
}

@inproceedings{10.1145/2525314.2525455,
author = {McKenzie, Grant and Janowicz, Krzysztof and Adams, Benjamin},
title = {Weighted Multi-Attribute Matching of User-Generated Points of Interest},
year = {2013},
isbn = {9781450325219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525314.2525455},
doi = {10.1145/2525314.2525455},
abstract = {To a large degree, the attraction of Big Data lies in the variety of its heterogeneous multi-thematic and multi-dimensional data sources and not merely its volume. To fully exploit this variety, however, requires conflation. This is a two step process. First, one has to establish identity relations between information entities across the different data sources; and second, attribute values have to be merged according to certain procedures which avoid logical contradictions. The first step, also called matching, can be thought of as a weighted combination of common attributes according to some similarity measures. In this work, we propose such a matching based on multiple attributes of Points of Interests (POI) from the Location-based Social Network Foursquare and the Yelp local directory service. While both contain overlapping attributes that can be use for matching, they have specific strengths and weaknesses which makes their conflation desirable. We present a weighted multi-attribute matching strategy and evaluate its performance. Our strategy can automatically match 97% of randomly selected Yelp POI to their corresponding Foursquare entities.},
booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {440–443},
numpages = {4},
keywords = {volunteered geographic information, similarity, point of interest, location-based services, conflation, POI},
location = {Orlando, Florida},
series = {SIGSPATIAL'13}
}

@inproceedings{10.1145/3442381.3449956,
author = {Yang, Longqi and Zhang, Liangliang and Tang, Yuhua},
title = {Scalable Auto-Weighted Discrete Multi-View Clustering},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449956},
doi = {10.1145/3442381.3449956},
abstract = {Multi-view clustering has been widely studied in machine learning, which uses complementary information to improve clustering performance. However, challenges remain when handling large-scale multi-view data due to the traditional approaches’ high time complexity. Besides, the existing approaches suffer from parameter selection. Due to the lack of labeled data, parameter selection in practical clustering applications is difficult, especially in big data. In this paper, we propose a novel approach for large-scale multi-view clustering to overcome the above challenges. Our approach focuses on learning the low-dimensional binary embedding of multi-view data, preserving the samples’ local structure during binary embedding, and optimizing the embedding and clustering in a unified framework. Furthermore, we proposed to learn the parameters using a combination of data-driven and heuristic approaches. Experiments on five large-scale multi-view datasets show that the proposed method is superior to the state-of-the-art in terms of clustering quality and running time.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3269–3278},
numpages = {10},
keywords = {multi-view clustering, graph regularization, parameter selection, binary coding},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/2685352,
author = {Aalst, Wil Van Der and Zhao, J. Leon and Wang, Harry Jiannan},
title = {Editorial: “Business Process Intelligence: Connecting Data and Processes”},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2685352},
doi = {10.1145/2685352},
abstract = {This introduction to the special issue on Business Process Intelligence (BPI) discusses the relation between data and processes. The recent attention for Big Data illustrates that organizations are aware of the potential of the torrents of data generated by today's information systems. However, at the same time, organizations are struggling to extract value from this overload of data. Clearly, there is a need for data scientists able to transform event data into actionable information. To do this, it is crucial to take a process perspective. The ultimate goal of BPI is not to improve information systems or the recording of data; instead the focus should be in improving the process. For example, we may want to aim at reducing costs, minimizing response times, and ensuring compliance. This requires a “confrontation” between process models and event data. Recent advances in process mining allow us to automatically learn process models showing the bottlenecks from “raw” event data. Moreover, given a normative model, we can use conformance checking to quantify and understand deviations. Automatically learned models may also be used for prediction and recommendation. BPI is rapidly developing as a field linking data science to business process management. This article aims to provide an overview thereby paving the way for the other contributions in this special issue.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {apr},
articleno = {18e},
numpages = {7},
keywords = {compliance checking, Process mining, process modeling, business process intelligence, performance analysis}
}

@article{10.1145/3145623,
author = {Wang, Chang and Zhu, Yongxin and Shi, Weiwei and Chang, Victor and Vijayakumar, P. and Liu, Bin and Mao, Yishu and Wang, Jiabao and Fan, Yiping},
title = {A Dependable Time Series Analytic Framework for Cyber-Physical Systems of IoT-Based Smart Grid},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3145623},
doi = {10.1145/3145623},
abstract = {With the emergence of cyber-physical systems (CPS), we are now at the brink of next computing revolution. The Smart Grid (SG) built on top of IoT (Internet of Things) is one of the foundations of this CPS revolution, which involves a large number of smart objects connected by networks. The volume of time series of SG equipment is tremendous and the raw time series are very likely to contain missing values because of undependable network transferring. The problem of storing a tremendous volume of raw time series thereby providing a solid support for precise time series analytics now becomes tricky. In this article, we propose a dependable time series analytics (DTSA) framework for IoT-based SG. Our proposed DTSA framework is capable of providing a dependable data transforming from CPS to the target database with an extraction engine to preliminary refining raw data and further cleansing the data with a correction engine built on top of a sensor-network-regularization-based matrix factorization method. The experimental results reveal that our proposed DTSA framework is capable of effectively increasing the dependability of raw time series transforming between CPS and the target database system through the online lightweight extraction engine and the offline correction engine. Our proposed DTSA framework would be useful for other industrial big data practices.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {aug},
articleno = {7},
numpages = {18},
keywords = {cyber-physical-systems, dependable time series analytics, sensor-network-regularization-based matrix factorization, IoT-based smart grid}
}

@inproceedings{10.1145/3447568.3448537,
author = {Sang, Go Muan and Xu, Lai and de Vrieze, Paul and Bai, Yuewei and Pan, Fangyu},
title = {Predictive Maintenance in Industry 4.0},
year = {2020},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448537},
doi = {10.1145/3447568.3448537},
abstract = {In the context of Industry 4.0, the manufacturing related processes have shifted from conventional processes within one organization to collaborative processes cross different organizations, for example, product design processes, manufacturing processes, and maintenance processes across different factories and enterprises. The application of Internet of things, i.e. smart devices and sensors increases collection and availability of diverse data. Advanced technologies such as big data analytics and cloud computing offer new opportunities for effective optimization of manufacturing related processes, e.g. predictive maintenance. Predictive maintenance provides a detailed examination of the detection, location and diagnosis of faults in related machineries using various analyses. RAMI4.0 is a framework for thinking about the various efforts that constitute Industry 4.0. It spans the entire product life cycle &amp; value stream axis, hierarchical structure axis and functional classification axis. The Industrial Data Space (now International Data Space) is a virtual data space using standards and common governance models to facilitate the secure exchange and easy linkage of data in business ecosystems. It thereby provides a basis for creating and using smart services and innovative business processes, while at the same time ensuring digital sovereignty of data owners. This paper looks at how to support predictive maintenance in the context of Industry 4.0? Especially, applying RAMI 4.0 architecture supports the predictive maintenance using FIWARE framework, which leads to deal with data exchanging among different organizations with different security requirements as well as modularizing of related functions.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {29},
numpages = {11},
keywords = {Blockchain, Collaborative business process, Industrial data space, FIWARE, Industry 4.0, Predictive maintenance},
location = {Lecce, Italy},
series = {ICIST '20}
}

@inproceedings{10.1145/3548785.3548797,
author = {Guyot, Alexis and Gillet, Annabelle and Leclercq, Eric and Cullot, Nadine},
title = {A Formal Framework for Data Lakes Based on Category Theory},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548785.3548797},
doi = {10.1145/3548785.3548797},
abstract = {The management of Big Data requires flexible systems to handle the heterogeneity of data models as well as the complexity of analytical workflows. Traditional systems like data warehouses have reached their limits due to their rigid schema-on-write paradigm, that requires well identified and defined use cases to ingest data. Data lakes, with their schema-on-read paradigm, have been proposed as more flexible systems in which raw data are directly stored in their original format associated with metadata, to be accessed and transformed only when users need to process or analyze them. Thus, it is necessary to define and control the different levels of abstraction and the dependencies among functionalities of a data lake to use it efficiently. In this article, we present a formal framework aiming to define a data lake pattern and to unify the interactions among the functionalities. We use the category theory as theoretical foundations to benefit from its high level of abstraction and its compositionality. By relying on different categories and functors, we ensure the navigation among the functionalities and allow the composition of multiples operations, while keeping track of the entire lineage of data. We also show how our framework can be applied on a simple example of data lake.},
booktitle = {Proceedings of the 26th International Database Engineered Applications Symposium},
pages = {75–83},
numpages = {9},
keywords = {Category Theory, Architecture Pattern, Data Lakes},
location = {Budapest, Hungary},
series = {IDEAS '22}
}

@inproceedings{10.1145/3396868.3402495,
author = {Kumar, Santosh},
title = {Sensitivity, Specificity, Generalizability, and Reusability Aspirations for Machine Learning (ML) Models in MHealth},
year = {2020},
isbn = {9781450380126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396868.3402495},
doi = {10.1145/3396868.3402495},
abstract = {Mobile sensor big data collected from smartphones, smartwatches, fitness trackers, and other wearables can be mined for signatures (called mHealth biomarkers) of subtle changes in daily behaviors (e.g., mobility, gait, sleep, etc.) and/or physiology (e.g., heart function, breathing, sweating, etc.). Clinical adoption of these mHealth biomarkers can lead to potent temporally-precise interventions, enabling patients to initiate and sustain the healthy lifestyle choices and treatment regimes that are necessary to prevent and/or successfully manage the growing burden of multiple chronic conditions.However, for any new biomarker to be successfully used for clinical diagnosis or treatment, its clinical utility must be established. mHealth biomarkers are usually derived by training a machine learning (ML) algorithm on mobile sensor data. The published models differ in feature construction (domain-derived features fed to a supervised ML model vs. data-driven features discovered by a deep learning (DL) model), data collection setting (lab vs. field), data selection and preparation (covering all twenty-four hours of the day vs. awake hours, vs. only when performing certain tasks), data labeling (retrospective self-reported aggregate labels vs. time-synchronized labels from first-person video), data size and diversity (e.g., number of participants, gender, ethnicity, age group, number of days, hours per day, etc.), experiment design (cross-validation vs. cross-subject validation), and performance (e.g., accuracy, F1 score, confusion matrix, AUC, etc.). As a result, there is wide diversity in published models on their potential for reusability, generalizability, and eventual clinical utility.This talk will describe an aspirational framework for specificity, sensitivity, generalizability, and reusability of mHealth biomarkers with some concrete performance targets so that they have a higher chance of widespread clinical utility. It will draw upon the presenter's decade-long transdisciplinary research experience in developing machine learning models to detect a wide variety of daily behaviors such as stress, speaking, smoking, and brushing from wearable physiological and inertial sensors.The talk will use the analogy of five nines (i.e., 99.999%) paradigm in the area of service-level agreements to quantify high-availability. Similar to how these managed services are expected to be available 24-7-365, with the downtime limited to 5.26 minutes per year, we can express the performance requirements of mHealth biomarkers that are expected to detect subtle signs of health and behaviour deterioration anytime and anywhere. Five nines guarantee for the detection of a health event (e.g., fall, stress) translates to one false positive every 100 person-days, if the model runs on 1,000 minutes of sensor data collected each day. Achieving five nines to claim the detection of non-event (e.g., smoking abstinence) is even more challenging, as there are several other failure scenarios for missing an event, in addition to model failure, such as the non-wearing of sensors when performing the event of interest, poor data quality, mismatch of model to where (on the body) and how the sensor is worn (e.g., smartwatch on non-dominant hand), battery failure, and data loss. To achieve generalizability, the model performance must be achieved on independent test data that covers all aspects of daily life, without any data selection. Finally, for the model to be used by others for real-life, the model should not only be accessible to the community, it should also be possible to train and test the model on different datasets by independent non-ML-expert researchers.},
booktitle = {Proceedings of Deep Learning for Wellbeing Applications Leveraging Mobile Devices and Edge Computing},
pages = {1},
numpages = {1},
keywords = {Machine Learning Models, Mobile Health (mHealth)},
location = {Toronto, ON, Canada},
series = {HealthDL'20}
}

@inproceedings{10.1145/3388176.3388210,
author = {Duan, Xuliang and Guo, Bing and Shen, Yan and Shen, Yuncheng and Dong, Xiangqian and Zhang, Hong},
title = {Research on Parallel Data Currency Rule Algorithms},
year = {2020},
isbn = {9781450377256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388176.3388210},
doi = {10.1145/3388176.3388210},
abstract = {Data currency is a temporal reference of data, which is related to the value of data and affects the results of data analysis and mining. The currency rules that reflect the time series features of data can be used not only for data repairing, but also for data quality evaluation. However, with the rapid growth and dynamic update of data volume, both the forms and algorithms of basic currency rule are facing severe challenges in application. Therefore, based on the research on data currency repairing, we extended the basic currency rule form, and proposed rule extraction and incremental updating algorithms that can run in parallel on dynamic data set. The experimental results show that, compared with non-parallel methods, the efficiency of parallel algorithms is significantly improved.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Information Science and System},
pages = {24–28},
numpages = {5},
keywords = {data currency rule, dynamic data, Data currency, parallel algorithm},
location = {Cambridge, United Kingdom},
series = {ICISS 2020}
}

@inproceedings{10.1145/3548785.3548793,
author = {Endres, Markus and Mannarapotta Venugopal, Asha and Tran, Tung Son},
title = {Synthetic Data Generation: A Comparative Study},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548785.3548793},
doi = {10.1145/3548785.3548793},
abstract = {Generating synthetic data similar to realistic data is a crucial task in data augmentation and data production. Due to the preservation of authentic data distribution, synthetic data provide concealment of sensitive information and therefore enable Big Data acquisition for model training without facing privacy challenges. Nevertheless, the obstacles arise starting with acquiring real-world open-source data to effectively synthesizing new samples as genuine as possible. In this paper, a comparative study is conducted by considering the efficacy of different generative models like Generative Adversarial Networks (GAN), Variational Autoencoder (VAE), Synthetic Minority Oversampling Technique (SMOTE), Data Synthesizer (DS), Synthetic Data Vault with Gaussian Copula (SDV-G), Conditional Generative Adversarial Networks (SDV-GAN), and SynthPop Non-Parametric (SP-NP) approach to synthesize data with regard to various datasets. We used the pairwise correlation and Synthetic Data (SD) metrics as utility measures respectively between real data and generated data for evaluation. Accordingly, this paper investigates the effects of various data generation models, and the processing time of every model is included as one of the evaluation metrics.},
booktitle = {Proceedings of the 26th International Database Engineered Applications Symposium},
pages = {94–102},
numpages = {9},
keywords = {Generative Models, Neural Networks, Synthetic Data},
location = {Budapest, Hungary},
series = {IDEAS '22}
}

@article{10.1145/3176648,
author = {Skorin-Kapov, Lea and Varela, Mart\'{\i}n and Ho\ss{}feld, Tobias and Chen, Kuan-Ta},
title = {A Survey of Emerging Concepts and Challenges for QoE Management of Multimedia Services},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3176648},
doi = {10.1145/3176648},
abstract = {Quality of Experience (QoE) has received much attention over the past years and has become a prominent issue for delivering services and applications. A significant amount of research has been devoted to understanding, measuring, and modelling QoE for a variety of media services. The next logical step is to actively exploit that accumulated knowledge to improve and manage the quality of multimedia services, while at the same time ensuring efficient and cost-effective network operations. Moreover, with many different players involved in the end-to-end service delivery chain, identifying the root causes of QoE impairments and finding effective solutions for meeting the end users’ requirements and expectations in terms of service quality is a challenging and complex problem. In this article, we survey state-of-the-art findings and present emerging concepts and challenges related to managing QoE for networked multimedia services. Going beyond a number of previously published survey articles addressing the topic of QoE management, we address QoE management in the context of ongoing developments, such as the move to softwarized networks, the exploitation of big data analytics and machine learning, and the steady rise of new and immersive services (e.g., augmented and virtual reality). We address the implications of such paradigm shifts in terms of new approaches in QoE modeling and the need for novel QoE monitoring and management infrastructures.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {may},
articleno = {29},
numpages = {29},
keywords = {crowdsourcing, encrypted traffic, SDN, QoE monitoring, QoE modeling, QoE management, monitoring probes, NFV, data analytics}
}

@inproceedings{10.1145/2882903.2904442,
author = {Zhang, Ce and Shin, Jaeho and R\'{e}, Christopher and Cafarella, Michael and Niu, Feng},
title = {Extracting Databases from Dark Data with DeepDive},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2904442},
doi = {10.1145/2882903.2904442},
abstract = {DeepDive is a system for extracting relational databases from dark data: the mass of text, tables, and images that are widely collected and stored but which cannot be exploited by standard relational tools. If the information in dark data --- scientific papers, Web classified ads, customer service notes, and so on --- were instead in a relational database, it would give analysts access to a massive and highly-valuable new set of "big data" to exploit.DeepDive is distinctive when compared to previous information extraction systems in its ability to obtain very high precision and recall at reasonable engineering cost; in a number of applications, we have used DeepDive to create databases with accuracy that meets that of human annotators. To date we have successfully deployed DeepDive to create data-centric applications for insurance, materials science, genomics, paleontologists, law enforcement, and others. The data unlocked by DeepDive represents a massive opportunity for industry, government, and scientific researchers.DeepDive is enabled by an unusual design that combines large-scale probabilistic inference with a novel developer interaction cycle. This design is enabled by several core innovations around probabilistic training and inference.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {847–859},
numpages = {13},
keywords = {dark data, information extraction, data integration, knowledge base construction},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3330204.3330259,
author = {Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel},
title = {Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330259},
doi = {10.1145/3330204.3330259},
abstract = {In the last decade the (big) data-driven science paradigm became a wide-spread reality. However, this approach has some limitations such as a performance dependency on the quality of the data and the lack of reproducibility of the results. In order to enable this reproducibility, many tools such as Workflow Management Systems were developed to formalize process pipelines and capture execution traces. However, interoperating data generated by these solutions became a problem, since most systems adopted proprietary data models. To support interoperability across heterogeneous provenance data, we propose a Service Oriented Architecture with a polystore storage design in which provenance is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover, we propose a query layer that provides location and access transparency to users. Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios. Firstly, we illustrate how two research groups can compare their processes and results. Secondly, we show how our architecture can be used as a queriable provenance repository. We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology. Finally, we show our solution usability and extensibility appeal by comparing it to similar approaches.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {49},
numpages = {8},
keywords = {heterogeneous provenance data integration, polystore, Workflows interoperability},
location = {Aracaju, Brazil},
series = {SBSI'19}
}

@inproceedings{10.1145/3357777.3357781,
author = {Lv, Zhining and Hu, Ziheng and Ning, Baifeng and Li, Wei and Yan, Gangfeng and Ding, Lifu and Shi, Xiasheng and Guo, Ningxuan},
title = {Safety Monitoring of Power Industrial Control Terminals Based on Data Cleaning},
year = {2019},
isbn = {9781450372312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357777.3357781},
doi = {10.1145/3357777.3357781},
abstract = {Stable and high-quality electric energy is the main driving force for the development of social science, technology, and the national economic leap. The assessment and monitoring of electrical safety rely on the generation, collection and statistics of large amounts of data by the power system. For the possible problems and impurities in these data, this paper uses the 'local Chebyshev theorem' and the 'near data averaging method' for the attribute values. The error is cleaned, and the 'sorting neighbor algorithm' is used to clean the duplicate data, thereby improving the data quality and realizing the accuracy of the safety monitoring of the power grid of the smart grid.},
booktitle = {Proceedings of the 2019 the International Conference on Pattern Recognition and Artificial Intelligence},
pages = {7–11},
numpages = {5},
keywords = {Chebyshev theory, Power monitoring, Proximity data averaging, Data cleaning},
location = {Wenzhou, China},
series = {PRAI '19}
}

@article{10.1145/2893482,
author = {Aiken, Peter},
title = {EXPERIENCE: Succeeding at Data Management—BigCo Attempts to Leverage Data},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2893482},
doi = {10.1145/2893482},
abstract = {In a manner similar to most organizations, BigCompany (BigCo) was determined to benefit strategically from its widely recognized and vast quantities of data. (U.S. government agencies make regular visits to BigCo to learn from its experiences in this area.) When faced with an explosion in data volume, increases in complexity, and a need to respond to changing conditions, BigCo struggled to respond using a traditional, information technology (IT) project-based approach to address these challenges. As BigCo was not data knowledgeable, it did not realize that traditional approaches could not work. Two full years into the initiative, BigCo was far from achieving its initial goals. How much more time, money, and effort would be required before results were achieved? Moreover, could the results be achieved in time to support a larger, critical, technology-driven challenge that also depended on solving the data challenges? While these questions remain unaddressed, these considerations increase our collective understanding of data assets as separate from IT projects. Only by reconceiving data as a strategic asset can organizations begin to address these new challenges. Transformation to a data-driven culture requires far more than technology, which remains just one of three required “stool legs” (people and process being the other two). Seven prerequisites to effectively leveraging data are necessary, but insufficient awareness exists in most organizations—hence, the widespread misfires in these areas, especially when attempting to implement the so-called big data initiatives. Refocusing on foundational data management practices is required for all organizations, regardless of their organizational or data strategies.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {8},
numpages = {35},
keywords = {CDO, data warehousing, IT management, BigCo, enterprise architecture, analytics, business intelligence, data architecture, data governance, enterprise data executive, CIO, information systems, strategy, chief data officer, chief information officer, organizational design, policy, conceptual modeling, Data management, data integration, data, data stewardship}
}

@article{10.1145/3461839,
author = {Wang, Jingjing and Jiang, Wenjun and Li, Kenli and Wang, Guojun and Li, Keqin},
title = {Incremental Group-Level Popularity Prediction in Online Social Networks},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3461839},
doi = {10.1145/3461839},
abstract = {Predicting the popularity of web contents in online social networks is essential for many applications. However, existing works are usually under non-incremental settings. In other words, they have to rebuild models from scratch when new data occurs, which are inefficient in big data environments. It leads to an urgent need for incremental prediction, which can update previous results with new data and conduct prediction incrementally. Moreover, the promising direction of group-level popularity prediction has not been well treated, which explores fine-grained information while keeping a low cost. To this end, we identify the problem of incremental group-level popularity prediction, and propose a novel model IGPP to address it. We first predict the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental prediction, we propose three strategies to restart the CP decomposition. To the best of our knowledge, this is the first work that identifies and solves the problem of incremental group-level popularity prediction. Extensive experimental results show significant improvements of the IGPP method over other works both in the prediction accuracy and the efficiency.},
journal = {ACM Trans. Internet Technol.},
month = {sep},
articleno = {20},
numpages = {26},
keywords = {information diffusion, tensor analysis, popularity prediction, Group level, online social networks, incremental approach}
}

@inproceedings{10.1145/2559206.2560469,
author = {Meyer, Jochen and Simske, Steven and Siek, Katie A. and Gurrin, Cathal G. and Hermens, Hermie},
title = {Beyond Quantified Self: Data for Wellbeing},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2560469},
doi = {10.1145/2559206.2560469},
abstract = {Sustaining our health and wellbeing requires lifelong efforts for prevention and healthy living. Continuously observing ourselves is one of the fundamental measures to be taken. While many devices support monitoring and quantifying our health behavior and health state, they all are facing the same trade-off: the higher the data quality is the higher are the efforts of data acquisition. However, for lifelong use, minimizing efforts for the user is crucial. Nowadays, few devices find a good balance between cost and value. In this interdisciplinary workshop we discuss how this trade-off can be approached by addressing three topics: understanding the user's information needs, exploring options for data acquisition, and discussing potential designs for life-long use.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {95–98},
numpages = {4},
keywords = {wellbeing, data analysis, user oriented design},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@article{10.14778/3282495.3282496,
author = {Bleifu\ss{}, Tobias and Bornemann, Leon and Johnson, Theodore and Kalashnikov, Dmitri V. and Naumann, Felix and Srivastava, Divesh},
title = {Exploring Change: A New Dimension of Data Analytics},
year = {2018},
issue_date = {October 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3282495.3282496},
doi = {10.14778/3282495.3282496},
abstract = {Data and metadata in datasets experience many different kinds of change. Values are inserted, deleted or updated; rows appear and disappear; columns are added or repurposed, etc. In such a dynamic situation, users might have many questions related to changes in the dataset, for instance which parts of the data are trustworthy and which are not? Users will wonder: How many changes have there been in the recent minutes, days or years? What kind of changes were made at which points of time? How dirty is the data? Is data cleansing required? The fact that data changed can hint at different hidden processes or agendas: a frequently crowd-updated city name may be controversial; a person whose name has been recently changed may be the target of vandalism; and so on. We show various use cases that benefit from recognizing and exploring such change.We envision a system and methods to interactively explore such change, addressing the variability dimension of big data challenges. To this end, we propose a model to capture change and the process of exploring dynamic data to identify salient changes. We provide exploration primitives along with motivational examples and measures for the volatility of data. We identify technical challenges that need to be addressed to make our vision a reality, and propose directions of future work for the data management community.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {85–98},
numpages = {14}
}

@article{10.14778/3407790.3407802,
author = {Fan, Ju and Chen, Junyou and Liu, Tongyu and Shen, Yuwei and Li, Guoliang and Du, Xiaoyong},
title = {Relational Data Synthesis Using Generative Adversarial Networks: A Design Space Exploration},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407802},
doi = {10.14778/3407790.3407802},
abstract = {The proliferation of big data has brought an urgent demand for privacy-preserving data publishing. Traditional solutions to this demand have limitations on effectively balancing the tradeoff between privacy and utility of the released data. Thus, the database community and machine learning community have recently studied a new problem of relational data synthesis using generative adversarial networks (GAN) and proposed various algorithms. However, these algorithms are not compared under the same framework and thus it is hard for practitioners to understand GAN's benefits and limitations. To bridge the gaps, we conduct so far the most comprehensive experimental study that investigates applying GAN to relational data synthesis. We introduce a unified GAN-based framework and define a space of design solutions for each component in the framework, including neural network architectures and training strategies. We conduct extensive experiments to explore the design space and compare with traditional data synthesis approaches. Through extensive experiments, we find that GAN is very promising for relational data synthesis, and provide guidance for selecting appropriate design solutions. We also point out limitations of GAN and identify future research directions.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1962–1975},
numpages = {14}
}

@article{10.1145/3522591,
author = {Xiao, Houping and Wang, Shiyu},
title = {Toward Quality of Information Aware Distributed Machine Learning},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3522591},
doi = {10.1145/3522591},
abstract = {In the era of big data, data are usually distributed across numerous connected computing and storage units (i.e., nodes or workers). Under such an environment, many machine learning problems can be reformulated as a consensus optimization problem, which consists of one objective and constraint terms splitting into N parts (each corresponds to a node). Such a problem can be solved efficiently in a distributed manner via Alternating Direction Method of Multipliers (ADMM). However, existing consensus optimization frameworks assume that every node has the same quality of information (QoI), i.e., the data from all the nodes are equally informative for the estimation of global model parameters. As a consequence, they may lead to inaccurate estimates in the presence of nodes with low QoI. To overcome this challenge, in this article, we propose a novel consensus optimization framework for distributed machine-learning that incorporates the crucial metric, QoI. Theoretically, we prove that the convergence rate of the proposed framework is linear to the number of iterations, but has a tighter upper bound compared with ADMM. Experimentally, we show that the proposed framework is more efficient and effective than existing ADMM-based solutions on both synthetic and real-world datasets due to its faster convergence rate and higher accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {109},
numpages = {28},
keywords = {quality of information, Distributed machine learning}
}

@article{10.1145/3275520,
author = {Sangogboye, Fisayo Caleb and Jia, Ruoxi and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun},
title = {A Framework for Privacy-Preserving Data Publishing with Enhanced Utility for Cyber-Physical Systems},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3–4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3275520},
doi = {10.1145/3275520},
abstract = {Cyber-physical systems have enabled the collection of massive amounts of data in an unprecedented level of spatial and temporal granularity. Publishing these data can prosper big data research, which, in turn, helps improve overall system efficiency and resiliency. The main challenge in data publishing is to ensure the usefulness of published data while providing necessary privacy protection. In our previous work&nbsp;(Jia et al. 2017a), we presented a privacy-preserving data publishing framework (referred to as PAD hereinafter), which can guarantee k-anonymity while achieving better data utility than traditional anonymization techniques. PAD learns the information of interest to data users or features from their interactions with the data publishing system and then customizes data publishing processes to the intended use of data. However, our previous work is only applicable to the case where the desired features are linear in the original data record. In this article, we extend PAD to nonlinear features. Our experiments demonstrate that for various data-driven applications, PAD can achieve enhanced utility while remaining highly resilient to privacy threats.},
journal = {ACM Trans. Sen. Netw.},
month = {nov},
articleno = {30},
numpages = {22},
keywords = {smart buildings, deep learning, k-anonymity, cyber-physical systems, Privacy preservation}
}

@article{10.1145/3564276,
author = {Srinivasan, Karthik and Currim, Faiz and Ram, Sudha},
title = {A Human-in-the-Loop Segmented Mixed-Effects Modeling Method For Analyzing Wearables Data},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2158-656X},
url = {https://doi.org/10.1145/3564276},
doi = {10.1145/3564276},
abstract = {Wearables are an important source of big data as they provide real-time high-resolution data logs of health indicators of individuals. Higher-order associations between pairs of variables is common in wearables data. Representing higher-order association curves as piece-wise linear segments in a regression model makes them more interpretable. However, existing methods for identifying the change points for segmented modeling either overfit or have low external validity for wearables data containing repeated measures. Therefore, we propose a human-in-the-loop method for segmented modeling of higher-order pairwise associations between variables in wearables data. Our method uses the smooth function estimated by a generalized additive mixed model to allow the analyst to annotate change point estimates for a segmented mixed-effects model, and thereafter employs the Brent's constrained optimization procedure to fine-tuning the manually provided estimates. We validate our method using three real-world wearables datasets. Our method not only outperforms state-of-the-art modeling methods in terms of prediction performance but also provides more interpretable results. Our study contributes to health data science in terms of developing a new method for interpretable modeling of wearables data. Our analysis uncovers interesting insights on higher order associations for health researchers.},
note = {Just Accepted},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {sep},
keywords = {explainability, human-in-the-loop method, smart health, wearables, interpretable modeling, segmented mixed-effects regression}
}

@inproceedings{10.1145/2964284.2976761,
author = {Tang, Mengfan and Pongpaichet, Siripen and Jain, Ramesh},
title = {Research Challenges in Developing Multimedia Systems for Managing Emergency Situations},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2976761},
doi = {10.1145/2964284.2976761},
abstract = {With an increasing amount of diverse heterogeneous data and information, the methodology of multimedia analysis has become increasingly relevant in solving challenging societal problems such as managing emergency situations during disasters. Using cybernetic principles combined with multimedia technology, researchers can develop effective frameworks for using diverse multimedia (including traditional multimedia as well as diverse multimodal) data for situation recognition, and determining and communicating appropriate actions to people stranded during disasters. We present known issues in disaster management and then focus on emergency situations. We show that an emergency management problem is fundamentally a multimedia information assimilation problem for situation recognition and for connecting people's needs to available resources effectively, efficiently, and promptly. Major research challenges for managing emergency situations are identified and discussed. We also present a intelligently detecting evolving environmental situations, and discuss the role of multimedia micro-reports as spontaneous participatory sensing data streams in emergency responses. Given enormous progress in concept recognition using machine learning in the last few years, situation recognition may be the next major challenge for learning approaches in multimedia contextual big data. The data needed for developing such approaches is now easily available on the Web and many challenging research problems in this area are ripe for exploration in order to positively impact our society during its most difficult times.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {938–947},
numpages = {10},
keywords = {disaster, micro-reports, situation recognition, eventshop, situation prediction},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.5555/3204979.3204980,
author = {Nie, Yu and Talburt, John and Li, Xinming and Xiao, Zhongdong},
title = {Chief Data Officer (CDO) Role and Responsibility Analysis},
year = {2018},
issue_date = {May 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {33},
number = {5},
issn = {1937-4771},
abstract = {While the number of organizations creating the role of Chief Data Officer (CDO) is increasing each year, the nature of the role is still emerging. CDO management responsibilities can vary widely from company to company. The study focuses on the various management responsibilities of the CDO role and their commonalities across organizations. After collecting and analyzing CDO job description from 411 organizations, we came to the following conclusions. Data analytics and business management are the most often cited and thus the most important management responsibilities for the CDO. Second is the management of data quality and data governance programs. Third, the CDO should keep abreast of new information technologies that could help firms design and execute an enterprise data strategy that coordinates the firm's business intelligence processes, leads to the development of new products, and acquires new customers through new data media.},
journal = {J. Comput. Sci. Coll.},
month = {may},
pages = {4–12},
numpages = {9},
keywords = {role and responsibility analysis, chief data officer (CDO), data analytics, business management}
}

@article{10.1145/3470918,
author = {Karmaker (“Santu”), Shubhra Kanti and Hassan, Md. Mahadi and Smith, Micah J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan},
title = {AutoML to Date and Beyond: Challenges and Opportunities},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3470918},
doi = {10.1145/3470918},
abstract = {As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML’s main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually—generally by a data scientist—and explain how this limits domain experts’ access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {175},
numpages = {36},
keywords = {democratization of artificial intelligence, interactive data science, Automated machine learning, predictive analytics}
}

@inproceedings{10.1145/2939672.2945365,
author = {Mierswa, Ingo},
title = {The Wisdom of Crowds: Best Practices for Data Prep &amp; Machine Learning Derived from Millions of Data Science Workflows},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945365},
doi = {10.1145/2939672.2945365},
abstract = {With hundreds of thousands of users, RapidMiner is the most frequently used visual workflow platform for machine learning. It covers the full spectrum of analytics from data preparation to machine learning and model validation. In this presentation, I will take you on a tour of machine learning which spans the last 15 years of research and industry applications and share key insights with you about how data scientists perform their daily analysis tasks. These patterns are extracted from mining millions of analytical workflows that have been created with RapidMiner over the past years. This talk will address important questions around the data mining process such as: What are the most frequently used solutions for typical data quality problems? How often are analysts using decision trees or neural networks? And does this behavior change over time or depend on the users experience level?},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {411},
numpages = {1},
keywords = {wisdom of the crowds, analytics, visual workflow, machine learning tools, data visualization},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/2832087.2832090,
author = {Lopez, M. Graham and Young, Jeffrey and Meredith, Jeremy S. and Roth, Philip C. and Horton, Mitchel and Vetter, Jeffrey S.},
title = {Examining Recent Many-Core Architectures and Programming Models Using SHOC},
year = {2015},
isbn = {9781450340090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832087.2832090},
doi = {10.1145/2832087.2832090},
abstract = {The Scalable HeterOgeneous Computing (SHOC) benchmark suite was released in 2010 as a tool to evaluate the stability and performance of emerging heterogeneous architectures and to compare different programming models for compute devices used in those architectures. Since then, high-performance computing (HPC) system architectures have increasingly incorporated both discrete and fused multi-core and many-core processors. The TOP500 list illustrates this trend: heterogeneous systems grew from a 3.4% to 18.0% share of the list between June 2010 and June 2015. Not only are there more heterogeneous systems on the TOP500 list today, those machines are responsible for a disproportionately large percentage of list's aggregate performance: as of June 2015, the performance share for heterogeneous systems has grown to 33.7%.Part of this shift toward heterogeneous architectures has stemmed from new products in the hardware accelerator market, such as Intel's Xeon Phi coprocessor, and improvements in the approaches for programming such accelerators. Existing approaches such as CUDA and OpenCL have become more powerful and easy to use, and directive-based programming models such as OpenACC, OpenMP 4.0, and Intel's Language Extensions for Offload (LEO) are rapidly gaining user acceptance. The benefits of these hardware and software advances are not limited to HPC; other problem domains such as "big data" are reaping the rewards also.The original SHOC benchmarks had adequate support for CUDA and OpenCL for graphics processing units, but did not support more recent programming models and devices. We extended SHOC to support evaluation of recent heterogeneous architectures and programming models such as OpenACC and LEO, and we added new benchmarks to increase SHOC's application domain coverage. In this paper, we describe our modifications to the stock SHOC distribution and present several examples of using our augmented version of SHOC for evaluation of recent heterogeneous architectures and programming models.},
booktitle = {Proceedings of the 6th International Workshop on Performance Modeling, Benchmarking, and Simulation of High Performance Computing Systems},
articleno = {3},
numpages = {12},
keywords = {accelerators, performance, benchmarking},
location = {Austin, Texas},
series = {PMBS '15}
}

@inproceedings{10.1145/2882903.2903736,
author = {Rheinl\"{a}nder, Astrid and Lehmann, Mario and Kunkel, Anja and Meier, J\"{o}rg and Leser, Ulf},
title = {Potential and Pitfalls of Domain-Specific Information Extraction at Web Scale},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2903736},
doi = {10.1145/2882903.2903736},
abstract = {In many domains, a plethora of textual information is available on the web as news reports, blog posts, community portals, etc. Information extraction (IE) is the default technique to turn unstructured text into structured fact databases, but systematically applying IE techniques to web input requires highly complex systems, starting from focused crawlers over quality assurance methods to cope with the HTML input to long pipelines of natural language processing and IE algorithms. Although a number of tools for each of these steps exists, their seamless, flexible, and scalable combination into a web scale end-to-end text analytics system still is a true challenge. In this paper, we report our experiences from building such a system for comparing the "web view" on health related topics with that derived from a controlled scientific corpus, i.e., Medline. The system combines a focused crawler, applying shallow text analysis and classification to maintain focus, with a sophisticated text analytic engine inside the Big Data processing system Stratosphere. We describe a practical approach to seed generation which led us crawl a corpus of ~1 TB web pages highly enriched for the biomedical domain. Pages were run through a complex pipeline of best-of-breed tools for a multitude of necessary tasks, such as HTML repair, boilerplate detection, sentence detection, linguistic annotation, parsing, and eventually named entity recognition for several types of entities. Results are compared with those from running the same pipeline (without the web-related tasks) on a corpus of 24 million scientific abstracts and a third corpus made of ~250K scientific full texts. We evaluate scalability, quality, and robustness of the employed methods and tools. The focus of this paper is to provide a large, real-life use case to inspire future research into robust, easy-to-use, and scalable methods for domain-specific IE at web scale.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {759–771},
numpages = {13},
keywords = {massively parallel data analysis, focused crawling, information extraction},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3469213.3470246,
author = {Wu, Yang and Zou, Wentao and Liu, Shuangquan and Jiang, Yan and Shao, Qizhuan and Zhou, Han},
title = {Rule-Based Data Verification Method in Electricity Spot Market},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470246},
doi = {10.1145/3469213.3470246},
abstract = {In the electric power spot market, input data quality is critical to an accurate and reliable clearing result. Missing and abnormal data will lead to the result that the clearing algorithm diverges or the results mismatch reality. This would seriously affect power system security and reduce the efficiency of the market. Therefore, to ensure a smooth convergence of the algorithm and reasonable results, this paper proposes a rule-based verification method for the input data in the power spot market. Data integrity and logical verification rules are established. During the verification process, information will be divided into different warning levels and displayed so that users can modify relevant data according to the actual situation.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {46},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3334480.3382864,
author = {Sharbatdar, Nasim and Lamine, Yassine and Milord, Brigitte and Morency, Catherine and Cheng, Jinghui},
title = {Capturing the Practices, Challenges, and Needs of Transportation Decision-Makers},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382864},
doi = {10.1145/3334480.3382864},
abstract = {Transportation decision-makers from government agencies play an important role in addressing the traffic network conditions, which in turn, have a major impact on the well-being of citizens. The practices, challenges, and needs of this group of practitioners are less represented in the HCI literature. We address this gap through an interview study with 19 practitioners from Transports Qu\'{e}bec, a government agency responsible for transportation infrastructures in Qu\'{e}bec, Canada. We found that this group of decision-makers can most benefit from research about data analysis tools and platforms that (1) provide information to support data quality awareness, (2) are interoperable with other tools in the complex workflow of the practitioners, and (3) support intuitive and customizable visual analytics. These implications can also be informative to the design of tools supporting other decision-making tasks and domains.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {decision-making, decision-maker, transportation management and planning, user study, persona},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@article{10.1145/3439873,
author = {Neto, Nelson Novaes and Madnick, Stuart and Paula, Anchises Moraes G. De and Borges, Natasha Malara},
title = {Developing a Global Data Breach Database and the Challenges Encountered},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3439873},
doi = {10.1145/3439873},
abstract = {If the mantra “data is the new oil” of our digital economy is correct, then data leak incidents are the critical disasters in the online society. The initial goal of our research was to present a comprehensive database of data breaches of personal information that took place in 2018 and 2019. This information was to be drawn from press reports, industry studies, and reports from regulatory agencies across the world. This article identified the top 430 largest data breach incidents among more than 10,000 data breach incidents.In the process, we encountered many complications, especially regarding the lack of standardization of reporting. This article should be especially interesting to the readers of JDIQ because it describes both the range of data quality and consistency issues found as well as what was learned from the database created.The database that was created, available at https://www.databreachdb.com, shows that the number of data records breached in those top 430 incidents increased from around 4B in 2018 to more than 22B in 2019. This increase occurred despite the strong efforts from regulatory agencies across the world to enforce strict rules on data protection and privacy, such as the General Data Protection Regulation (GDPR) that went into effect in Europe in May 2018. Such regulatory effort could explain the reason why there is such a large number of data breach cases reported in the European Union when compared to the U.S. (more than 10,000 data breaches publicly reported in the U.S. since 2018, while the EU reported more than 160,0001 data breaches since May 2018). However, we still face the problem of an excessive number of breach incidents around the world.This research helps to understand the challenges of proper visibility of such incidents on a global scale. The results of this research can help government entities, regulatory bodies, security and data quality researchers, companies, and managers to improve the data quality of data breach reporting and increase the visibility of the data breach landscape around the world in the future.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {3},
numpages = {33},
keywords = {Cyber security, data aggregation, data breach, privacy, semantics of data}
}

@inproceedings{10.1145/3511716.3511730,
author = {Yang, Jie and Cao, Yong},
title = {The Classification of Gene Sequencer Based on Machine Learning},
year = {2022},
isbn = {9781450395687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511716.3511730},
doi = {10.1145/3511716.3511730},
abstract = {Abstract: Biological sequencing plays a very important role in life science, especially with the improvement of sequencing technology and the development of sequencing instruments, and a large number of biological sequencing quality data are produced every day. Because of different sequencers, the quality of sequencing is different. In the process of sequencing quality control, the model of sequencer can be deduced according to the quality of gene sequence. Therefore, in this paper, five sequencers of Illumina HiSeq series, Illumina HiSeq 2000, Illumina HiSeq 2500, Illumina HiSeq 3000, Illumina HiSeq 4000 and Illumina HiSeq XTen, are selected as the classification objects. Firstly, the sequencing quality data of the five sequencers are preprocessed. Then, the classification model is trained by three machine learning algorithms: decision tree, logistic regression and support vector machine. The experimental results show that the accuracy rates of the three machine learning algorithms are 96.67%, 97.50% and 97.50% respectively. These algorithms are very good to solve the problem of using biological sequencing data quality to classify sequencer.},
booktitle = {2021 4th International Conference on E-Business, Information Management and Computer Science},
pages = {84–88},
numpages = {5},
keywords = {Machine learning, Classification of gene Sequencer, Quality of sequencing},
location = {Hong Kong, China},
series = {EBIMCS 2021}
}

@inproceedings{10.1145/3312714.3312717,
author = {Li, Pei and Dai, Chaofan and Wang, Wenqian},
title = {Application of Attribute Correlation in Unsupervised Data Cleaning},
year = {2019},
isbn = {9781450362351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312714.3312717},
doi = {10.1145/3312714.3312717},
abstract = {Referring to the supervised learning and unsupervised learning in machine learning, we divide the data cleaning processes into supervised and unsupervised two forms too, and then, we reclassify the data quality problems into canonicalization error, redundancy error, strong logic error and weak logic error according to the characteristics of unsupervised cleaning. For the weak logic errors, we propose a repair framework AC-Framework and an algorithm AC-Repair based on the attribute correlation. When repairing, we first establish a priority queue(PQ) for elements to be repaired according to the minimum cost idea and take the corresponding conflict-free data set(Icf) as a training set to learn the correlation among attributes. Then, we select the first element in PQ list as the candidate element to repair, and recompute the PQ list after one repair round to improve the efficiency. Finally, in order to prevent the algorithm from endless loops, we set a label flag to mark the repaired elements, in this way, every error element will be repaired at most once. In the experimental part, we compare the AC-Repair algorithm with the interpolation-based repair algorithm to verify its validity.},
booktitle = {Proceedings of the 5th International Conference on E-Society, e-Learning and e-Technologies},
pages = {45–51},
numpages = {7},
keywords = {attribute correlation, minimum repair cost, Unsupervised data cleaning, machine learning, weak logic errors},
location = {Vienna, Austria},
series = {ICSLT '19}
}

@inproceedings{10.1145/3110025.3110161,
author = {Ahmadov, Ahmad and Thiele, Maik and Lehner, Wolfgang and Wrembel, Robert},
title = {Context Similarity for Retrieval-Based Imputation},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110161},
doi = {10.1145/3110025.3110161},
abstract = {Completeness as one of the four major dimensions of data quality is a pervasive issue in modern databases. Although data imputation has been studied extensively in the literature, most of the research is focused on inference-based approach. We propose to harness Web tables as an external data source to effectively and efficiently retrieve missing data while taking into account the inherent uncertainty and lack of veracity that they contain.Existing approaches mostly rely on standard retrieval techniques and out-of-the-box matching methods which result in a very low precision, especially when dealing with numerical data. We, therefore, propose a novel data imputation approach by applying numerical context similarity measures which results in a significant increase in the precision of the imputation procedure, by ensuring that the imputed values are of the same domain and magnitude as the local values, thus resulting in an accurate imputation.We use Dresden Web Table Corpus which is comprised of more than 125 million web tables extracted from the Common Crawl as our knowledge source. The comprehensive experimental results demonstrate that the proposed method well outperforms the default out-of-the-box retrieval approach.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {1017–1024},
numpages = {8},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3336191.3371871,
author = {Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin and Mukherjee, Avijit},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371871},
doi = {10.1145/3336191.3371871},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the "why" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [11, 14]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 [23]. It was attended by around 150 participants.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {877–880},
numpages = {4},
keywords = {user experience evaluation, a/b testing, online metrics, controlled experiments},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@article{10.1145/3552490.3552494,
author = {Dave, Dev and Celestino, Angelica and Varde, Aparna S. and Anu, Vaibhav},
title = {Management of Implicit Requirements Data in Large SRS Documents: Taxonomy and Techniques},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3552490.3552494},
doi = {10.1145/3552490.3552494},
abstract = {Implicit Requirements (IMR) identification is part of the Requirements Engineering (RE) phase in Software Engineering during which data is gathered to create SRS (Software Requirements Specifications) documents. As opposed to explicit requirements clearly stated, IMRs constitute subtle data and need to be inferred. Research has shown that IMRs are crucial to the success of software development. Many software systems can encounter failures due to lack of IMR data management. SRS documents are large, often hundreds of pages, due to which manually identifying IMRs by human software engineers is not feasible. Moreover, such data is evergrowing due to the expansion of software systems. It is thus important to address the crucial issue of IMR data management. This article presents a survey on IMRs in SRS documents with the definition and overview of IMR data, detailed taxonomy of IMRs with explanation and examples, practices in managing IMR data, and tools for IMR identification. In addition to reviewing classical and state-of-the-art approaches, we highlight trends and challenges and point out open issues for future research. This survey article is interesting based on data quality, hidden information retrieval, veracity and salience, and knowledge discovery from large textual documents with complex heterogeneous data.},
journal = {SIGMOD Rec.},
month = {jul},
pages = {18–29},
numpages = {12}
}

@article{10.14778/2536274.2536304,
author = {Civili, Cristina and Console, Marco and De Giacomo, Giuseppe and Lembo, Domenico and Lenzerini, Maurizio and Lepore, Lorenzo and Mancini, Riccardo and Poggi, Antonella and Rosati, Riccardo and Ruzzi, Marco and Santarelli, Valerio and Savo, Domenico Fabio},
title = {Mastro Studio: Managing Ontology-Based Data Access Applications},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536274.2536304},
doi = {10.14778/2536274.2536304},
abstract = {Ontology-based data access (OBDA) is a novel paradigm for accessing large data repositories through an ontology, that is a formal description of a domain of interest. Supporting the management of OBDA applications poses new challenges, as it requires to provide effective tools for (i) allowing both expert and non-expert users to analyze the OBDA specification, (ii) collaboratively documenting the ontology, (iii) exploiting OBDA services, such as query answering and automated reasoning over ontologies, e.g., to support data quality check, and (iv) tuning the OBDA application towards optimized performances. To fulfill these challenges, we have built a novel system, called MASTRO STUDIO, based on a tool for automated reasoning over ontologies, enhanced with a suite of tools and optimization facilities for managing OBDA applications. To show the effectiveness of MASTRO STUDIO, we demonstrate its usage in one OBDA application developed in collaboration with the Italian Ministry of Economy and Finance.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1314–1317},
numpages = {4}
}

@article{10.14778/3317315.3317318,
author = {Fan, Wenfei and Lu, Ping and Tian, Chao and Zhou, Jingren},
title = {Deducing Certain Fixes to Graphs},
year = {2019},
issue_date = {March 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3317315.3317318},
doi = {10.14778/3317315.3317318},
abstract = {This paper proposes to deduce certain fixes to graphs G based on data quality rules Σ and ground truth Γ (i.e., validated attribute values and entity matches). We fix errors detected by Σ in G such that the fixes are assured correct as long as Σand Γ are correct. We deduce certain fixes in two paradigms. (a) We interact with users and "incrementally" fix errors online. Whenever users pick a small set V0 of nodes in G, we fix all errors pertaining to V0 and accumulate ground truth in the process. (b) Based on accumulated Γ, we repair the entire graph G offline; while this may not correct all errors in G, all fixes are guaranteed certain.We develop techniques for deducing certain fixes. (1) We define data quality rules to support conditional functional dependencies, recursively defined keys and negative rules on graphs, such that we can deduce fixes by combining data repairing and object identification. (2) We show that deducing certain fixes is Church-Rosser, i.e., the deduction converges at the same fixes regardless of the order of rules applied. (3) We establish the complexity of three fundamental problems associated with certain fixes. (4) We provide (parallel) algorithms for deducing certain fixes online and offline, and guarantee to reduce running time when given more processors. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of our methods.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {752–765},
numpages = {14}
}

@inproceedings{10.1145/3366424.3383117,
author = {Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383117},
doi = {10.1145/3366424.3383117},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the “why” behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program 11, 14. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 23. It was attended by around 150 participants. This tutorial has also been accepted for the WSDM 2020 conference.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {317–319},
numpages = {3},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3187009.3177739,
author = {Lin, Xueling and Chen, Lei},
title = {Domain-Aware Multi-Truth Discovery from Conflicting Sources},
year = {2018},
issue_date = {January 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.1145/3187009.3177739},
doi = {10.1145/3187009.3177739},
abstract = {In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {635–647},
numpages = {13}
}

@article{10.1145/3177732.3177739,
author = {Lin, Xueling and Chen, Lei},
title = {Domain-Aware Multi-Truth Discovery from Conflicting Sources},
year = {2018},
issue_date = {January 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.1145/3177732.3177739},
doi = {10.1145/3177732.3177739},
abstract = {In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {635–647},
numpages = {13}
}

@inproceedings{10.1145/3129416.3129441,
author = {Cohen, L.},
title = {Impacts of Business Intelligence on Population Health: A Systematic Literature Review},
year = {2017},
isbn = {9781450352505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129416.3129441},
doi = {10.1145/3129416.3129441},
abstract = {"Business Intelligence" is an area of Information Technology (IT) that involves the collection, analysis and presentation of large amounts of data. BI has been successfully applied to promote good decision making in a variety of environments, and has high potential to make a significant impact in the domain of population health. The promotion of population health is a key concern of government authorities and various health institutions and officials making decisions about interventions that may impact on population health would benefit from the use of information on population health. BI could clearly be a facilitator in this regard, but evidence of its current application and impact in this field is not easily accessible to policy makers. This systematic literature review explored the literature and provided a synthesis of information available on the current use of BI in this area, and evidence of the impact of its use on population health. An array of applications of BI for population health were found, including data warehouses, analytics, reports, data warehouse browsers, OLAP, GIS, Dashboards and Alerts. Evidence of the impact of these applications on population health was mainly anecdotal, with only one empirical study found. Issues and challenges encountered in the development and use of BI are Privacy and Security, Data Quality and Development and Maintenance of BI infrastructure},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists},
articleno = {9},
numpages = {9},
keywords = {population health, business intelligence, systematic literature review},
location = {Thaba 'Nchu, South Africa},
series = {SAICSIT '17}
}

@inproceedings{10.1145/3514221.3522567,
author = {Nargesian, Fatemeh and Asudeh, Abolfazl and Jagadish, H. V.},
title = {Responsible Data Integration: Next-Generation Challenges},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3522567},
doi = {10.1145/3514221.3522567},
abstract = {Data integration has been extensively studied by the data management community and is a core task in the data pre-processing step of ML pipelines. When the integrated data is used for analysis and model training, responsible data science requires addressing concerns about data quality and bias. We present a tutorial on data integration and responsibility, highlighting the existing efforts in responsible data integration along with research opportunities and challenges. In this tutorial, we encourage the community to audit data integration tasks with responsibility measures and develop integration techniques that optimize the requirements of responsible data science. We focus on three critical aspects: (1) the requirements to be considered for evaluating and auditing data integration tasks for quality and bias; (2) the data integration tasks that elicit attention to data responsibility measures and methods to satisfy these requirements; and, (3) techniques, tasks, and open problems in data integration that help achieve data responsibility.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {2458–2464},
numpages = {7},
keywords = {data integration, data collection, data equity, responsible ai, distribution tailoring, fair ML},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3284179.3284322,
author = {Dorn, Amelie and Wandl-Vogt, Eveline and Palfinger, Thomas and D\'{\i}az, Jos\'{e} Luis Preza and Piringer, Barbara and Schatek, Alexander and Zoubek, Rainer},
title = {Applying Commercial Computer Vision Tools to Cope with Uncertainties in a Citizen-Driven Archive: The Case Study Topothek@exploreAT!},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284322},
doi = {10.1145/3284179.3284322},
abstract = {Uncertainties in data, e.g., incomplete data sets, data quality issues or inconsistencies in annotations, are a common phenomenon across disciplines. How to address these issues is context dependent. In this paper, we address uncertainties in the citizen-driven archive Topotheque as a concrete use-case in the Digital Humanities project exploreAT!, and demonstrate, how to deal with uncertainties by benchmarking a set of selected commercial computer vision (CV) tools. The approach aims to enrich Topotheque's data to enable better access, connectivity and analysis for both researchers and citizens. Results show that by applying CV, existing uncertainties are noticeably reduced, but new ones also introduced. Better grounds for semantic structuring are provided, enabling higher connectivity and linking within Topotheque, but also across other data sets. Ultimately, the enrichment of the archive is for the benefit of both researchers and citizens enabled by addressing and tackling apparent uncertainties.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {845–851},
numpages = {7},
keywords = {uncertainty, Digital Humanities, AI, computer vision, citizen-driven archive},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@article{10.5555/3344081.3344082,
author = {Ye, Yumeng and Talburt, John R.},
title = {Generating Synthetic Data to Support Entity Resolution Education and Research},
year = {2019},
issue_date = {April 2019},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {7},
issn = {1937-4771},
abstract = {Almost all organizations use some type of Entity Resolution (ER) methods to uniquely identify their customers and vendors across different channels of contact. In the case of persons, this requires the use of personally identifying information (PII) such as name, address, phone number, and email address. Because of the growing concerns over data privacy and identity theft, organizations are reluctant to release personally-identifiable customer information even for education and research purposes. An alternative is to generate synthetic data to use in student exercises and for research related to entity resolution methods and techniques. One advantage of synthetically generated data for ER is it can be fully annotated with the correct linking making it very easy to calculate the precision and recall of linking operations. This paper discusses a simple method to generate synthetic data as input for ER processes. The method allows the user to randomly assign certain types and levels of data quality errors along with other types of non-error variations to the data, such as nicknames, different date formats, and changes in address. For ER research in particular, the method can create introduce data redundancy by copying records referencing the same person into the same file or into different files with different record layouts.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {12–19},
numpages = {8}
}

@inproceedings{10.1145/3491396.3506519,
author = {Li, Xiang and Zhang, Zhaoqian and Zhao, Zhigang and Wu, Lu and Huo, Jidong and Zhang, Jian and Wang, Yinglong},
title = {ECNN: One Online Deep Learning Model for Streaming Ocean Data Prediction},
year = {2022},
isbn = {9781450391603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491396.3506519},
doi = {10.1145/3491396.3506519},
abstract = {Despite been extensively explored, current techniques in sequential data modeling and prediction are generally designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient but also poorly scalable in real-world applications, especially for real-time intelligent ocean data quality control (QC), where the data arrives sequentially and the QC should be conducted in real time. This paper investigates the online learning for ocean data streams by resolving two main challenges: (i) how to develop a deep learning model to capture the complex ocean data distribution that could evolve dynamically, namely tackling the 'concept drift' problem for non-stationary time series; (ii) how to develop a deep learning model that can dynamically adapt its structure from shallow to deep with the inflow of the data to overcome under-fitting problem, namely tackling the 'model selection' problem. To tackle these challenges, we propose one Evolutive Convolutional Neural Network (ECNN) that dynamically re-weighting the sub-structure of the model from data streams in a sequential or online learning fashion, by which the capacity scalability and sustainability are introduced into the model. The experiments on real ocean observation data verify the effectiveness of our model. As far as we know, it is the first work that introduce online deep learning techniques into ocean data prediction research.},
booktitle = {Proceedings of the 2021 ACM International Conference on Intelligent Computing and Its Emerging Applications},
pages = {170–175},
numpages = {6},
keywords = {Time Series Prediction, CNN, Attention Network, Ocean Data, Online Learning},
location = {Jinan, China},
series = {ACM ICEA '21}
}

@inproceedings{10.1145/3438872.3439085,
author = {Wu, Yi and Song, Yan and Yang, Hongshan},
title = {Intelligent Distributed Web Crawler Based on Attention Mechanism},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439085},
doi = {10.1145/3438872.3439085},
abstract = {With the rapid development of the Internet, webpages' content has become the central platform for people to publish and retrieve information. Recently, web crawlers could quickly and accurately find the information users need from the massive network information resources. There have been many different types of web crawlers in the literature, developed for data retrieval. However, most of the existing web crawlers have significant limitations. For example, they focus on the effective overall architecture instead of paying attention to the actual data's complexity. Moreover, the advertising links in the news and the public platform's promotional content have become ubiquitous noise. The existing web crawler collection strategy lacks sufficient identification of advertising information. The degree of automation to detect advertisements is low, so it isn't easy to form a complete and deployable large-scale distributed data crawling system. Therefore, the research and improvement of distributed web crawlers that intelligently distinguish advertisements is a work of practical significance. The distributed intelligent web crawler system designed and implemented in this paper solves low manual crawler efficiency and poor data quality. The crawler system can effectively identify and eliminate advertising information and significantly improve the automatically extracted data in the distributed crawler system from the experimental results.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {229–233},
numpages = {5},
keywords = {Deep Learning, Intelligent Web Crawler, Artificial Intelligence, Distributed Framework},
location = {Shanghai, China},
series = {RICAI 2020}
}

@inproceedings{10.1145/3292500.3332297,
author = {Shi, Xiaolin and Dmitriev, Pavel and Gupta, Somit and Fu, Xin},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3332297},
doi = {10.1145/3292500.3332297},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the "why" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [18, 22]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3189–3190},
numpages = {2},
keywords = {a/b testing, controlled experiments, user experience evaluation, online metrics},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@book{10.1145/3310205,
author = {Ilyas, Ihab F. and Chu, Xu},
title = {Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.}
}

@article{10.1145/3326164,
author = {Cong, Phan Thanh and Tam, Nguyen Thanh and Yin, Hongzhi and Zheng, Bolong and Stantic, Bela and Hung, Nguyen Quoc Viet},
title = {Efficient User Guidance for Validating Participatory Sensing Data},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3326164},
doi = {10.1145/3326164},
abstract = {Participatory sensing has become a new data collection paradigm that leverages the wisdom of the crowd for big data applications without spending cost to buy dedicated sensors. It collects data from human sensors by using their own devices such as cell phone accelerometers, cameras, and GPS devices. This benefit comes with a drawback: human sensors are arbitrary and inherently uncertain due to the lack of quality guarantee. Moreover, participatory sensing data are time series that exhibit not only highly irregular dependencies on time but also high variance between sensors. To overcome these limitations, we formulate the problem of validating uncertain time series collected by participatory sensors. In this article, we approach the problem by an iterative validation process on top of a probabilistic time series model. First, we generate a series of probability distributions from raw data by tailoring a state-of-the-art dynamical model, namely <u>G</u>eneralised <u>A</u>uto <u>R</u>egressive <u>C</u>onditional <u>H</u>eteroskedasticity (GARCH), for our joint time series setting. Second, we design a feedback process that consists of an adaptive aggregation model to unify the joint probabilistic time series and an efficient user guidance model to validate aggregated data with minimal effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness of our approach on both real data and synthetic data. Highlights from our experiences include the fast running time of a probabilistic model, the robustness of an aggregation model to outliers, and the significant effort saving of a guidance model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jul},
articleno = {37},
numpages = {30},
keywords = {trust management, Participatory sensing, probabilistic database}
}

@article{10.1145/3530991,
author = {Killeen, Patrick and Kiringa, Iluju and Yeap, Tet},
title = {Unsupervised Dynamic Sensor Selection for IoT-Based Predictive Maintenance of a Fleet of Public Transport Buses},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2691-1914},
url = {https://doi.org/10.1145/3530991},
doi = {10.1145/3530991},
abstract = {In recent years, big data produced by the Internet of Things has enabled new kinds of useful applications. One such application is monitoring a fleet of vehicles in real time to predict their remaining useful life. The consensus self-organized models (COSMO) approach is an example of a predictive maintenance system. The present work proposes a novel Internet of Things based architecture for predictive maintenance that consists of three primary nodes: the vehicle node, the server leader node, and the root node, which enable on-board vehicle data processing, heavy-duty data processing, and fleet administration, respectively. A minimally viable prototype of the proposed architecture was implemented and deployed to a local bus garage in Gatineau, Canada.The present work proposes improved consensus self-organized models (ICOSMO), a fleet-wide unsupervised dynamic sensor selection algorithm. To analyze the performance of ICOSMO, a fleet simulation was implemented. The J1939 data gathered from a hybrid bus was used to generate synthetic data in the simulations. Simulation results that compared the performance of the COSMO and ICOSMO approaches revealed that in general ICOSMO improves the average area under the curve of COSMO by approximately 1.5% when using the Cosine distance and 0.6% when using Hellinger distance.},
journal = {ACM Trans. Internet Things},
month = {jul},
articleno = {21},
numpages = {36},
keywords = {fleet management, predictive maintenance, Controller Area Network, Internet of Things, sensor selection, predictive analytics, machine learning, J1939}
}

@inproceedings{10.1145/3447548.3469441,
author = {Zhu, Feida and Pei, Jian},
title = {The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD2021): Joint Workshop with SIGKDD 2021 Trust Day},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469441},
doi = {10.1145/3447548.3469441},
abstract = {Today's computing is characterized by an increasing degree of complexity, comprehensiveness and collaboration. The complexity can be observed by the wide application of gigantic models with a huge number of parameters and structures of an unprecedented level of sophistication. The comprehensiveness is best illustrated by the high heterogeneity of data both in terms of format and source. The collaboration, finally, becomes an obvious trend when computing systems grow more open and decentralized in which various entities interact to achieve collective intelligence with the presence of potentially malicious behavior. Trust, therefore, has become critical at multiple levels: At model level to assure its integrity, fairness and interpretability; At data level to safeguard data quality, compliance and privacy; At system level to govern resilience, performance and incentive. Moreover, the notion of trust has long been discussed in different domains in both academia and industry with different definition and understanding. The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD'21) will be held as a joint workshop with the special-themed "Trust Day" of KDD 2021, which has therefore aimed to bring together researchers, practitioners and experts from various communities to exchange and explore ideas, frontiers, opportunities and challenges under the broad theme of "trust" in a highly interdisciplinary manner.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4185–4186},
numpages = {2},
keywords = {data auditing, data pricing, data governance, privacy, distributed ledger technology, data asset},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

