@article{10.1145/3312750,
author = {Bertino, Elisa and Kundu, Ahish and Sura, Zehra},
title = {Data Transparency with Blockchain and AI Ethics},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3312750},
doi = {10.1145/3312750},
abstract = {Providing a 360° view of a given data item especially for sensitive data is essential toward not only protecting the data and associated privacy but also assuring trust, compliance, and ethics of the systems that use or manage such data. With the advent of General Data Protection Regulation, California Data Privacy Law, and other such regulatory requirements, it is essential to support data transparency in all such dimensions. Moreover, data transparency should not violate privacy and security requirements. In this article, we put forward a vision for how data transparency would be achieved in a de-centralized fashion using blockchain technology.},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {16},
numpages = {8},
keywords = {accountability, Big data, data provenance, privacy}
}

@inproceedings{10.1145/3322134.3322155,
author = {Polpinij, Jantima and Namee, Khanista},
title = {Internet Usage Patterns Mining from Firewall Event Logs},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322155},
doi = {10.1145/3322134.3322155},
abstract = {Understanding users' behavior of internet usage is essential for the quality of service (QoS) analysis on the internet. If the internet providers can better understand their users, they may be able to provide better service, and also enhance the quality of the service. In general, the information about users' behavior is stored as the internet access log files, called event logs, on the server. To have the patterns of users' behavior from the event logs, this work aims to extract an interesting pattern of inappropriate user behaviors through the method of internet usage patterns mining. The primary mechanism of the proposed method is the Generalized Sequential Pattern (GSP) algorithm, which is an algorithm of sequential pattern mining. This study uses real event logs from an organization in Thailand. The results have identified exciting findings that have made possible to propose some improvements and increasing the QoS of the internet service.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {93–97},
numpages = {5},
keywords = {Event logs, Sequential pattern mining, Inappropriate user pattern, Generalized Sequential Pattern, Internet usage, Data mining},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/3185768.3186307,
author = {Cabrera, Anthony M. and Faber, Clayton J. and Cepeda, Kyle and Derber, Robert and Epstein, Cooper and Zheng, Jason and Cytron, Ron K. and Chamberlain, Roger D.},
title = {DIBS: A Data Integration Benchmark Suite},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186307},
doi = {10.1145/3185768.3186307},
abstract = {As the generation of data becomes more prolific, the amount of time and resources necessary to perform analyses on these data increases. What is less understood, however, is the data preprocessing steps that must be applied before any meaningful analysis can begin. This problem of taking data in some initial form and transforming it into a desired one is known as data integration. Here, we introduce the Data Integration Benchmarking Suite (DIBS), a suite of applications that are representative of data integration workloads across many disciplines. We apply a comprehensive characterization to these applications to better understand the general behavior of data integration tasks. As a result of our benchmark suite and characterization methods, we offer insight regarding data integration tasks that will guide other researchers designing solutions in this area.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {25–28},
numpages = {4},
keywords = {big data, data integration, data wrangling},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1145/2663715.2669610,
author = {Silva, M\'{a}rio J. and Rijo, Pedro and Francisco, Alexandre},
title = {Evaluating the Impact of Anonymization on Large Interaction Network Datasets},
year = {2014},
isbn = {9781450315838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663715.2669610},
doi = {10.1145/2663715.2669610},
abstract = {We address the publication of a large academic information dataset addressing privacy issues. We evaluate anonymization techniques achieving the intended protection, while retaining the utility of the anonymized data. The released data could help infer behaviors and subsequently find solutions for daily planning activities, such as cafeteria attendance, cleaning schedules or student performance, or study interaction patterns among an academic population. However, the nature of the academic data is such that many implicit social interaction networks can be derived from the anonymized datasets, raising the need for researching how anonymity can be assessed in this setting.},
booktitle = {Proceedings of the First International Workshop on Privacy and Secuirty of Big Data},
pages = {3–10},
numpages = {8},
keywords = {privacy-preserving data publishing, privacy of big data, interaction network inference, academic data publishing},
location = {Shanghai, China},
series = {PSBD '14}
}

@inproceedings{10.1145/3159450.3159483,
author = {Saltz, Jeffrey S. and Dewar, Neil I. and Heckman, Robert},
title = {Key Concepts for a Data Science Ethics Curriculum},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3159483},
doi = {10.1145/3159450.3159483},
abstract = {Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {952–957},
numpages = {6},
keywords = {big data, ethics, computing education, data science},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/2957276.2957283,
author = {Verma, Nitya and Voida, Amy},
title = {On Being Actionable: Mythologies of Business Intelligence and Disconnects in Drill Downs},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2957283},
doi = {10.1145/2957276.2957283},
abstract = {We present results from a case study of the use of business intelligence systems in a human services organization. We characterize four mythologies of business intelligence that informants experience as shared organizational values and are core to their trajectory towards a "culture of data": data-driven, predictive and proactive, shared accountability, and inquisitive. Yet, for each mythology, we also discuss the ways in which being actionable is impeded by a disconnect between the aggregate views of data that allows them to identify areas of focus for decision making and the desired "drill down" views of data that would allow them to understand how to act in a data-driven context. These findings contribute initial empirical evidence for the impact of business intelligence's epistemological biases on organizations and suggest implications for the design of technologies to better support data-driven decision making.},
booktitle = {Proceedings of the 19th International Conference on Supporting Group Work},
pages = {325–334},
numpages = {10},
keywords = {big data, business intelligence, data analytics, mythology},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@inproceedings{10.1145/2896338.2896341,
author = {Vandervort, David},
title = {Medical Device Data Goes to Court},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896338.2896341},
doi = {10.1145/2896338.2896341},
abstract = {Advances in mobile and computer technology are combining to create massive changes in the way data about human health and well-being are gathered and used. As the trend toward wearable and ubiquitous health tracking devices moves forward, the sheer quantity of new data from a wide variety of devices presents challenges for analysts. In the coming years, this data will inevitably be used in the criminal and civil justice systems. However, the tools to make full use of it are currently lacking. This paper discusses scenarios where data collected from health and fitness related devices may intersect with legal requirements such as investigations into insurance fraud or even murder. The conclusion is that there is much work to be done to enable reliable investigations. This should include at least the establishment of an organization to promote development of the field, development of cross-disciplinary education materials, and the creation of an open data bank for information sharing.},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
pages = {23–27},
numpages = {5},
keywords = {ehealth, crime, forensics, big data, wearables, law},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}

@inproceedings{10.1145/3329189.3329204,
author = {Rahman, Md Mahbubur and Nathan, Viswam and Nemati, Ebrahim and Vatanparvar, Korosh and Ahmed, Mohsin and Kuang, Jilong},
title = {Towards Reliable Data Collection and Annotation to Extract Pulmonary Digital Biomarkers Using Mobile Sensors},
year = {2019},
isbn = {9781450361262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329189.3329204},
doi = {10.1145/3329189.3329204},
abstract = {Proliferation of sensors embedded in smartphones and smartwatches helps capture rich dataset for machine learning algorithms to extract meaningful digital bio-markers on consumer devices for monitoring disease progression and treatment response. However, development and validation of machine learning algorithms depend on gathering high fidelity sensor data and reliable ground-truth. We conduct a study, called mLungStudy, with 131 subjects with varying pulmonary conditions to collect mobile sensor data including audio, accelerometer, gyroscope using a smartphone and a smartwatch, in order to extract pulmonary biomarkers such as breathing, coughs, spirometry, and breathlessness. Our study shows that commonly used breathing ground-truth data from chestband may not always be reliable as a gold-standard. Our analysis shows that breathlessness biomarkers such as pause time and pause frequency from 2.15 minutes of audio can be as reliable as those extracted from 5 minutes' worth of speech data. This finding can be useful for future studies to trade-off between the reliability of breathlessness data and patient comfort in generating continuous speech data. Furthermore, we use crowdsourcing techniques to annotate pulmonary sound events for developing signal processing and machine learning algorithms. In this paper, we highlight several practical challenges to collect and annotate physiological data and acoustic symptoms from chronic pulmonary patients and ways to improve data quality. We show that the waveform visualization of the audio signal improves annotation quality which leads to a 6.59% increase in cough classification accuracy and a 6% increase in spirometry event classification accuracy. Findings from this study inform future studies focusing on developing explainable machine learning models to extract pulmonary digital bio-markers using mobile sensors.},
booktitle = {Proceedings of the 13th EAI International Conference on Pervasive Computing Technologies for Healthcare},
pages = {179–188},
numpages = {10},
keywords = {Data Quality, Breathing, Breathlessness, Crowdsourced Annotation, mHealth, Digital Biomarkers, Cough},
location = {Trento, Italy},
series = {PervasiveHealth'19}
}

@inproceedings{10.1145/3131672.3131694,
author = {Hossain, Syed Monowar and Hnat, Timothy and Saleheen, Nazir and Nasrin, Nusrat Jahan and Noor, Joseph and Ho, Bo-Jhang and Condie, Tyson and Srivastava, Mani and Kumar, Santosh},
title = {MCerebrum: A Mobile Sensing Software Platform for Development and Validation of Digital Biomarkers and Interventions},
year = {2017},
isbn = {9781450354592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131672.3131694},
doi = {10.1145/3131672.3131694},
abstract = {The development and validation studies of new multisensory biomarkers and sensor-triggered interventions requires collecting raw sensor data with associated labels in the natural field environment. Unlike platforms for traditional mHealth apps, a software platform for such studies needs to not only support high-rate data ingestion, but also share raw high-rate sensor data with researchers, while supporting high-rate sense-analyze-act functionality in real-time. We present mCerebrum, a realization of such a platform, which supports high-rate data collections from multiple sensors with realtime assessment of data quality. A scalable storage architecture (with near optimal performance) ensures quick response despite rapidly growing data volume. Micro-batching and efficient sharing of data among multiple source and sink apps allows reuse of computations to enable real-time computation of multiple biomarkers without saturating the CPU or memory. Finally, it has a reconfigurable scheduler which manages all prompts to participants that is burden- and context-aware. With a modular design currently spanning 23+ apps, mCerebrum provides a comprehensive ecosystem of system services and utility apps. The design of mCerebrum has evolved during its concurrent use in scientific field studies at ten sites spanning 106,806 person days. Evaluations show that compared with other platforms, mCerebrum's architecture and design choices support 1.5 times higher data rates and 4.3 times higher storage throughput, while causing 8.4 times lower CPU usage.},
booktitle = {Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems},
articleno = {7},
numpages = {14},
keywords = {mHealth, mobile sensor big data, software architecture},
location = {Delft, Netherlands},
series = {SenSys '17}
}

@inproceedings{10.1145/2494091.2499223,
author = {Romualdo-Suzuki, Larissa and Finkelstein, Anthony and Gann, David},
title = {A Middleware Framework for Urban Data Management},
year = {2013},
isbn = {9781450322157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494091.2499223},
doi = {10.1145/2494091.2499223},
abstract = {The domain of inquiry of this research is the collection, organization, integration, distribution and consumption of knowledge derived from urban open data, and how it can be best offered to application cities' stakeholders through a software middleware. We argue that the extensive investigation proposed in this research will contribute to a growing body of knowledge about data integration and application in smart cities, and offer opportunities to re-think an integrated urban infrastructure.},
booktitle = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
pages = {1359–1362},
numpages = {4},
keywords = {smart cities, value chain., big data, software architecture},
location = {Zurich, Switzerland},
series = {UbiComp '13 Adjunct}
}

@inproceedings{10.1145/3078081.3078109,
author = {Kir\'{a}ly, P\'{e}ter},
title = {Towards an Extensible Measurement of Metadata Quality},
year = {2017},
isbn = {9781450352659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078081.3078109},
doi = {10.1145/3078081.3078109},
abstract = {This paper describes the structure of an extensible metadata quality assessment framework, which supports multiple metadata schemas, and is flexible enough to work with new schemas. The software has to be scalable to be able to process huge amount of metadata records within a reasonable time. Fundamental requirements that need to be considered during the design of such a software are i) the abstraction of the metadata schema (in the context of the measurement process), ii) how to address distinct parts within metadata records, iii) the workflow of the measurement, iv) a common and powerful interface for the individual metrics, and v) interoperability with Java and REST APIs.},
booktitle = {Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage},
pages = {111–115},
numpages = {5},
keywords = {big data, metadata quality, REST API, design patterns},
location = {G\"{o}ttingen, Germany},
series = {DATeCH2017}
}

@inproceedings{10.1145/2790755.2790774,
author = {Hamdi, Sana and Bouazizi, Emna and Faiz, Sami},
title = {A New QoS Management Approach in Real-Time GIS with Heterogeneous Real-Time Geospatial Data Using a Feedback Control Scheduling},
year = {2015},
isbn = {9781450334143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790755.2790774},
doi = {10.1145/2790755.2790774},
abstract = {Geographic Information System (GIS) is a computer system designed to capture, store, manipulate, analyze, manage, and present all types of spatial data. Spatial data, whether captured through remote sensors or large scale simulations becomes more and big and heterogenous. As a result, structured data and unstructured content are simultaneously accessed via an integrated user interface. The issue of real-time and heterogeneity is extremely important for taking effective decision. Thus, heterogeneous real-time spatial data management is a very active research domain nowadays. Existing research are interested in querying of real-time spatial data and their updates without taking into account the heterogeneity of real-time geospatial data. In this paper, we propose the use of the real-time Spatial Big Data and we define a new architecture called FCSA-RTSBD (Feedback Control Scheduling Architecture for Real-Time Spatial Big Data). The main objectives of this architecture are the following: take in account the heterogeneity of data, guarantee the data freshness, enhance the deadline miss ratio even in the presence of conflicts and finally satisfy the requirements of users by the improving of the quality of service (QoS).},
booktitle = {Proceedings of the 19th International Database Engineering &amp; Applications Symposium},
pages = {174–179},
numpages = {6},
keywords = {Real-Time Spatial Big Data, Geographic Information System, Feedback Control Scheduling, Quality of Service, Heterogeneous Real-Time Geospatial Data, Transaction},
location = {Yokohama, Japan},
series = {IDEAS '15}
}

@inproceedings{10.1145/3286606.3286794,
author = {Noussair, Lazrak and Jihad, Zahir and Hajar, Mousannif},
title = {Responsive Cities and Data Gathering: Challenges and Opportunities},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286794},
doi = {10.1145/3286606.3286794},
abstract = {For the last two decades, data driven cities have emerged as an efficient way of improving the city performance, enhancing life quality, and providing more choices to city planners and decision makers. A significant change in data driven cities in recent years is that much more data are collected from a variety of sources and can be processed into various forms for different stakeholders. The availability of a large amount of data can potentially lead to a revolution in city development, changing the city operation system from a conventional technology-driven system into a more powerful multifunctional data-driven intelligent system. But with more data collected the more questions raised about the optimization and space saving methods, then the quality of data collected and the efficiency of the its treatment. In this paper, we provide a survey on the data driven cities requirements, and the tools made available for the responsive cities to maintain its data.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {17},
numpages = {8},
keywords = {Responsive cities, Big Data, Quality of data, Data gathering},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/3194696.3194700,
author = {Palacio, Ana Le\'{o}n and L\'{o}pez, \'{O}scar Pastor},
title = {Towards an Effective Medicine of Precision by Using Conceptual Modelling of the Genome: Short Paper},
year = {2018},
isbn = {9781450357340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194696.3194700},
doi = {10.1145/3194696.3194700},
abstract = {The continuous improvement in our understanding of the human genome is leading to an increasing viable and effective Precision Medicine. Its intention is to provide a personalized solution to any individual health problem. Nevertheless, three main issues must be considered to make Precision Medicine a reality: i) the understanding of the huge amount of genomic data, spread out in hundreds of genome data sources, with different formats and contents, whose semantic interoperability is a must; ii) the development of information systems intended to guide the search of relevant genomic repositories related with a disease, the identification of significant information for its prevention, diagnosis and/or treatment and its management in an efficient software platform; iii) the high variability in the quality of the publicly available information. This paper presents a conceptual framework for solving these problems by i) using a precise conceptual schema of the human genome, and ii) introducing a method to search, identify, load and adequately interpret the required data, assuring its quality during the entire process.},
booktitle = {Proceedings of the International Workshop on Software Engineering in Healthcare Systems},
pages = {14–17},
numpages = {4},
keywords = {conceptual modelling, data quality, precision medicine},
location = {Gothenburg, Sweden},
series = {SEHS '18}
}

@inproceedings{10.1145/3549843.3549859,
author = {Lee, Angela S.H and Bengeri, Atul and Kan, Chong Chin},
title = {Artificial Intelligence Adoption in QSR Industry},
year = {2022},
isbn = {9781450397216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549843.3549859},
doi = {10.1145/3549843.3549859},
abstract = {According to research conducted by Allied Market Research (Kale and Deshmukh, 2020), the Quick-service restaurant (QSR) industry has been enjoying significant growth over the last several years and the trend is expected to continue at least over the next five years. This is due to the agility of most QSR brands in adapting to consumer needs, quality food products and service innovation. Offering the convenience of brands reach for their consumers such as a restaurant within the reach of the customer, as well as providing online ordering and food delivery convenience.The convenience of the omnichannel digital services offered by typical QSR restaurant brands to their customer provides convenience for service delivery and at the same time provides an opportunity to collect, collate, check, consume and analyse the data to examine, explore and provide hindsight, insight and foresight for the QSR industry to be ahead of the competition.Not only are these digital customer channels a great way of providing a better customer experience, but they also help QSR operators in capturing massive data such as sales transactions, customer details, product performance, and daypart (breakfast, lunch and dinner) sales insights.With the ability to analyse and perform analytics on the data and from a data strategy perspective, it is paramount to have a robust big data platform, data warehouse and ability to consolidate data from all the IT systems. This is to enable the development of BI/dashboards to provide better business insights, such as real-time and daypart product sales. Data could also be used to develop new digital products for more positive customer engagements such as upselling and loyalty programmes or for implementing of “smart kitchen” that is able to connect various datasets for maximum kitchen efficiency and cost optimisation.Due to a lack of data science approach and machine learning capability, most QSR brands are missing this sales uplifting opportunity.To develop an effective A.I. and machine learning in the QSR business, there is a need to evaluate and select the best machine learning techniques to address the business challenges.Our objective is to explore and recommend the various machine learning techniques that can be applied to the QSR industry to gain insights and provide the necessary analysis and analytics to ensure quality service and also address changing business challenges in the competitive and ever-changing market by deploying various machine learning models for prediction and pattern analysis through classification and association approaches.},
booktitle = {Proceedings of the 2022 6th International Conference on E-Education, E-Business and E-Technology},
pages = {102–110},
numpages = {9},
keywords = {Data Warehousing, Machine Learning, Classification, Prediction, Data Management, Information systems applications Data Mining, Association, Big Data, Data Analytics},
location = {Beijing, China},
series = {ICEBT '22}
}

@inproceedings{10.1145/3485447.3512104,
author = {Lin, Zihan and Tian, Changxin and Hou, Yupeng and Zhao, Wayne Xin},
title = {Improving Graph Collaborative Filtering with Neighborhood-Enriched Contrastive Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512104},
doi = {10.1145/3485447.3512104},
abstract = {Recently, graph collaborative filtering methods have been proposed as an effective recommendation approach, which can capture users’ preference over items by modeling the user-item interaction graphs. Despite the effectiveness, these methods suffer from data sparsity in real scenarios. In order to reduce the influence of data sparsity, contrastive learning is adopted in graph collaborative filtering for enhancing the performance. However, these methods typically construct the contrastive pairs by random sampling, which neglect the neighboring relations among users&nbsp;(or items) and fail to fully exploit the potential of contrastive learning for recommendation. To tackle the above issue, we propose a novel contrastive learning approach, named Neighborhood-enriched Contrastive Learning, named NCL, which explicitly incorporates the potential neighbors into contrastive pairs. Specifically, we introduce the neighbors of a user&nbsp;(or an item) from graph structure and semantic space respectively. For the structural neighbors on the interaction graph, we develop a novel structure-contrastive objective that regards users&nbsp;(or items) and their structural neighbors as positive contrastive pairs. In implementation, the representations of users&nbsp;(or items) and neighbors correspond to the outputs of different GNN layers. Furthermore, to excavate the potential neighbor relation in semantic space, we assume that users with similar representations are within the semantic neighborhood, and incorporate these semantic neighbors into the prototype-contrastive objective. The proposed NCL can be optimized with EM algorithm and generalized to apply to graph collaborative filtering methods. Extensive experiments on five public datasets demonstrate the effectiveness of the proposed NCL, notably with 26% and 17% performance gain over a competitive graph collaborative filtering base model on the Yelp and Amazon-book datasets, respectively. Our implementation code is available at: https://github.com/RUCAIBox/NCL.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2320–2329},
numpages = {10},
keywords = {Contrastive Learning, Recommender System, Graph Neural Network, Collaborative Filtering},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3428080,
author = {Wang, Guang and Fang, Zhihan and Xie, Xiaoyang and Wang, Shuai and Sun, Huijun and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng},
title = {Pricing-Aware Real-Time Charging Scheduling and Charging Station Expansion for Large-Scale Electric Buses},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3428080},
doi = {10.1145/3428080},
abstract = {We are witnessing a rapid growth of electrified vehicles due to the ever-increasing concerns on urban air quality and energy security. Compared to other types of electric vehicles, electric buses have not yet been prevailingly adopted worldwide due to their high owning and operating costs, long charging time, and the uneven spatial distribution of charging facilities. Moreover, the highly dynamic environment factors such as unpredictable traffic congestion, different passenger demands, and even the changing weather can significantly affect electric bus charging efficiency and potentially hinder the further promotion of large-scale electric bus fleets. To address these issues, in this article, we first analyze a real-world dataset including massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then, we investigate the electric bus network to understand its operating and charging patterns, and further verify the necessity and feasibility of a real-time charging scheduling. With such understanding, we design busCharging, a pricing-aware real-time charging scheduling system based on Markov Decision Process to reduce the overall charging and operating costs for city-scale electric bus fleets, taking the time-variant electricity pricing into account. To show the effectiveness of busCharging, we implement it with the real-world data from Shenzhen, which includes GPS data of electric buses, the metadata of all bus lines and bus stops, combined with data of 376 charging stations for electric buses. The evaluation results show that busCharging dramatically reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously. Finally, we design a scheduling-based charging station expansion strategy to verify our busCharging is also effective during the charging station expansion process.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {nov},
articleno = {13},
numpages = {26},
keywords = {charging scheduling, charging pattern, MDP, Electric bus, data driven}
}

@inproceedings{10.1145/3437963.3441747,
author = {Wu, Jinze and Huang, Zhenya and Liu, Qi and Lian, Defu and Wang, Hao and Chen, Enhong and Ma, Haiping and Wang, Shijin},
title = {Federated Deep Knowledge Tracing},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441747},
doi = {10.1145/3437963.3441747},
abstract = {Knowledge tracing is a fundamental task in intelligent education for tracking the knowledge states of students on necessary concepts. In recent years, Deep Knowledge Tracing (DKT) utilizes recurrent neural networks to model student learning sequences. This approach has achieved significant success and has been widely used in many educational applications. However, in practical scenarios, it tends to suffer from the following critical problems due to data isolation: 1) Data scarcity. Educational data, which is usually distributed across different silos (e.g., schools), is difficult to gather. 2) Different data quality. Students in different silos have different learning schedules, which results in unbalanced learning records, meaning that it is necessary to evaluate the learning data quality independently for different silos. 3) Data incomparability. It is difficult to compare the knowledge states of students with different learning processes from different silos. Inspired by federated learning, in this paper, we propose a novel Federated Deep Knowledge Tracing (FDKT) framework to collectively train high-quality DKT models for multiple silos. In this framework, each client takes charge of training a distributed DKT model and evaluating data quality by leveraging its own local data, while a center server is responsible for aggregating models and updating the parameters for all the clients. In particular, in the client part, we evaluate data quality incorporating different education measurement theories, and we construct two quality-oriented implementations based on FDKT, i.e., FDKTCTT and FDKTIRT-where the means of data quality evaluation follow Classical Test Theory and Item Response Theory, respectively. Moreover, in the server part, we adopt hierarchical model interpolation to uptake local effects for model personalization. Extensive experiments on real-world datasets demonstrate the effectiveness and superiority of the FDKT framework.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {662–670},
numpages = {9},
keywords = {federated learning, knowledge tracing, intelligent education, data quality evaluation, data isolation},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/2659532.2659594,
author = {Jaakkola, Hannu and M\"{a}kinen, Timo and Etel\"{a}aho, Anna},
title = {Open Data: Opportunities and Challenges},
year = {2014},
isbn = {9781450327534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659532.2659594},
doi = {10.1145/2659532.2659594},
abstract = {Open data is seen as a promising source of new business, especially in the SME sector, in the form of new products, services and innovative solutions. High importance is seen also in fostering citizens' participation in political and social life and increasing the transparency of public authorities. The forerunners of the open data movement in the public sector are the USA and the UK, which started to open their public data resources in 2009. The first European Union open data related directive was drawn up as early as 2003; however progress in putting the idea into practice has been slow and adoptions by the wider member states are placed in the early 2010s. The beneficial use of open data in real applications has progressed hand in hand with the improvement of other ICT-related technologies. The (raw) data itself has no high value. The economic value comes from a balanced combination of high quality open (data) resources combined with the related value chain. This paper builds up a "big picture" of the role of open data in current society. The approach is analytical and it clarifies the topic from the viewpoints of both opportunities and challenges. The paper covers both general aspects related to open data and results of the research and regional development project conducted by the authors.},
booktitle = {Proceedings of the 15th International Conference on Computer Systems and Technologies},
pages = {25–39},
numpages = {15},
keywords = {open data, data analysis, public data, networking, big data},
location = {Ruse, Bulgaria},
series = {CompSysTech '14}
}

@inproceedings{10.1145/3523111.3523119,
author = {Meng, Xianyu and Ma, Liangli and Zhou, Yingxue},
title = {Analysis and Example Implementation of Data Visualization Technology},
year = {2022},
isbn = {9781450395670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523111.3523119},
doi = {10.1145/3523111.3523119},
abstract = {Abstract: The development of visualization technology and data mining technology provides powerful means and tools for the visual analysis of diversified data. In this era of information and data explosion, various information resources are very rich, but due to the large amount of data, the characteristics of the data are not so obvious. This requires us to study the corresponding methods and means to extract the characteristics of data. Data visualization technology has developed rapidly under this background. This paper introduces the basic theory of data visualization technology, and selects the GDP data of the world's major economies over the years as a visualization example.},
booktitle = {2022 the 5th International Conference on Machine Vision and Applications (ICMVA)},
pages = {56–60},
numpages = {5},
keywords = {Big data, Data analysis, Data space, Data mining, Data visualization},
location = {Singapore, Singapore},
series = {ICMVA 2022}
}

@inproceedings{10.1145/3503928.3503930,
author = {Zeng, Xian and Han, Minglei and Li, Ning and Liu, Peng},
title = {Research on Real-Time Data Warehouse Technology for Sea Battlefield},
year = {2021},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503930},
doi = {10.1145/3503928.3503930},
abstract = {Aiming at the data governance problems in the sea battlefield, this paper proposes a real-time data warehouse construction method for naval battlefields, which realizes the functions of storage, analysis and mining of battle data. This paper completes the construction of the data warehouse from the aspects of real-time data life cycle, real-time data application scenarios, data warehouse real-time safeguard measures, and data warehouse theme design. It can effectively provide data support for naval combat forces and provide auxiliary decision-making for commanders.},
booktitle = {2021 the 6th International Conference on Information Systems Engineering},
pages = {13–20},
numpages = {8},
keywords = {Data Warehouse, Real-time, Sea Battlefield, Big Data},
location = {Shanghai, China},
series = {ICISE 2021}
}

@inproceedings{10.1145/3047273.3047296,
author = {Choudhury, Pranab Ranjan and Behera, Manoj Kumar},
title = {Using Administrative Data for Monitoring and Improving Land Policy and Governance in India},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047296},
doi = {10.1145/3047273.3047296},
abstract = {Demands for production and dissemination of reliable data is growing with increasing demand from public policies to monitor, compare and improve global and national developmental status and targets. Implementation of intentionally agreed commitments like Millennium Development Goals (MDGs), Sustainable development Goals (SDGs) are influencing data production and availability, and the development of national statistical capacities. They also trigger challenges and opportunities in production of internationally comparable data to induce fair comparability among nations. Being a signatory to major international treaties, India has considerably improved data production, accessibility and availability over the years to ensure proper alignment of national level statistics and induce international comparison. However, very little efforts have been made to assess India's progress around data production and dissemination around growingly important land governance. This assessment attempts to identify key opportunities and challenges at the country level to improve data availability, access, timeliness and quality.India has made many progressive reforms around land laws and institutions to make land governance more inclusive and equitable; however its assessment with respect to global best practices through World Bank's Land Governance Assessment Framework (LGAF) indicate the need of improvements around different land dimensions. Movement towards good land governance outcomes is incumbent upon robust and regular monitoring mechanism of land indicators across spatial (viz. administrative boundaries, land being a state subject in India) and temporal scales.India has traditions of collecting, maintaining and reporting land information through nation-wide surveys, census, administrative and judicial reports/ databases. Its flagship program Digital India Land Record Modernization Program (DILRMP), has been supporting universal digitization of spatial and textual land records by the states. Together, these administrative and survey-derived datasets provide seamless opportunity for routine generation of data on key land indicators at low cost on a regularbasis. Land is a state subject in India. Monitoring and reporting land-indicators at state levels would help in systematically discovering and identifying good practice that can then be documented and disseminated across states, manage change, and gradually move towards a more performance-based approach to improving land governance in India. However, there have been lack of institutionalized attempts, so far, to report land-indicators at national scale.We have tried to assess the state of data in India, particularly to track and report two critical land governance indicators viz. women land rights and forest rights, critical to ensure equity and sustainability in terms of public policy. With UN's SDG, defining similar indicators, we also attempt aligning them around SDG indicators. Status of these two parameters were analyzed using nation-wide datasets collecting whole population data, through legitimate institutions following robust processes and reporting them open access.Census (human population) data and Forest Survey of India (FSI) data were used to assess village-wise forest areas eligible for recognition of rights under India's historic Forest Rights Act, 2005. Using the FSI data and meta-analysis of census data, we calculated the estimated population (150 million including 90 million tribal) living in villages that have forest land within administrative revenue boundaries, potential area (40 million ha) that can be recognized under FRA and number of villages (0.17 million) that are eligible to initiate the claim. These data were made available across administrative boundaries of state, district and village, providing opportunities for relevant Government Ministries at Central and State level and civil society to expedite the forest rights recognition under India's largest land reform process.In order to assess women's land rights (WLR) in India in the context of the SDGs, after examining the existing data sets, we used Agricultural Census data, conducted by Government of India every fifth year following the guidelines of World Census on Agriculture (WCA). Using Agricultural census data, we have developed atlas of women land rights (based on operational holdings) in India with state and district wise granularity with further disaggregation across ethnicity (caste) and other socio-economic parameters. The study also attempted to analyze the link between the inter-regional and temporal variability of WLR and relevant policies and legal-institutional frameworks among the states to see if the correlations can better inform public policy and also induce healthy competition among states to appreciate and follow best practices. This paper presents the process, methodology and results of the data-analysis for these two land indicators while delving into the scope and challenges of dealing with existing and upcoming big datasets in India to report the land governance indicators and the potential policy spinoffs.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {127–135},
numpages = {9},
keywords = {Women Land Rights, Forest Rights, India, SDGs, Big Data},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.5555/2857070.2857186,
author = {Blake, Catherine and Souden, Maria and Anderson, Caryn L. and Twidale, Michael and Stelmack, Jenifer E.},
title = {Online Question Answering Practices to Support Healthcare Data Re-Use},
year = {2015},
isbn = {087715547X},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Institutional data collection practices inevitably evolve over time, especially in a distributed clinical setting. Clinical and administrative data can improve health and healthcare, but only if researchers ensure that the data is well-aligned to their reuse goals and that they have adequately accounted for changes in data collection practices over time. Our goal is to understand information behaviors of health services data users as they bridge the gap between the historical data and their intended data reuse goals. This project leverages more than a decade of listserv posts related to the use of clinical and administrative data by US Department of Veterans Affairs (VA) employees, providing longitudinal insight into data reuse practices in both research and operational settings. In this paper we report the results of a pilot study that highlighted questions raised in the use of data and the knowledge engaged to answer them.},
booktitle = {Proceedings of the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community},
articleno = {116},
numpages = {4},
keywords = {forums, social question answering, big data, health, communities of practice},
location = {St. Louis, Missouri},
series = {ASIST '15}
}

@inproceedings{10.1145/3411764.3445518,
author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
title = {“Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes AI},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445518},
doi = {10.1145/3411764.3445518},
abstract = {AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {39},
numpages = {15},
keywords = {data collectors, data quality, Nigeria, data cascades, data politics, high-stakes AI, Ghana, developers, USA, Kenya, Data, India, raters, application-domain experts, Uganda, AI, ML},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3356991.3365474,
author = {Palumbo, Rachel and Thompson, Laura and Thakur, Gautam},
title = {SONET: A Semantic Ontological Network Graph for Managing Points of Interest Data Heterogeneity},
year = {2019},
isbn = {9781450369602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356991.3365474},
doi = {10.1145/3356991.3365474},
abstract = {Scalability, standardization, and management are important issues when working with very large Volunteered Geographic Information (VGI). VGI is a rich and valuable source of Points of Interest (POI) information, but its inherent heterogeneity in content, structure, and scale across sources present major challenges for interlinking data sources for analysis. To be useful at scale, the raw information needs to be transformed into a standardized schema that can be easily and reliably used by data analysts. In this work, we tackle the problem of unifying POI categories (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving land use maps and population distribution estimation as well as support data analysts wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform or working with projects that are already configured in the OSM schema and wish to add additional sources of information. Graph theory and its implementation through the SONET graph database, provides a programmatic way to organize, store, and retrieve standardized POI categories at multiple levels of abstraction. Additionally, it addresses category heterogeneity across data sources by standardizing and managing categories in a way that makes cross-domain analysis possible.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities},
articleno = {6},
numpages = {6},
keywords = {ontology, graph database, big data, points of interest, openstreetmap},
location = {Chicago, Illinois},
series = {GeoHumanities '19}
}

@inproceedings{10.1145/3378539.3393864,
author = {Birkel, Hendrik and Kopyto, Matthias and Lutz, Corinna},
title = {Challenges of Applying Predictive Analytics in Transport Logistics},
year = {2020},
isbn = {9781450371308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378539.3393864},
doi = {10.1145/3378539.3393864},
abstract = {The field of Predictive Analytics (PA) provides the possibility to utilize large amounts of data to improve forecasting, data-driven decision-making, and competitive advantage. Especially the transport logistics sector, which is characterized by high business-related uncertainties, time-sensitivity, and volatility, highly benefits from accurate resource and production planning. While success factors and framework conditions of applying PA are well-investigated on a theoretical SCM level, findings on internal and external challenges of transport logistics organizations remain scarce. Therefore, based on a multiple case approach, this study offers in-depth insights into six real-world cases of freight forwarders, ocean carriers, and air carriers. The results uncover both internal and external challenges. From the internal perspective, the biggest challenges are related to the technical implementation including the acquisition of globally generated, internal and external data and its harmonization. In addition, stakeholder management and target setting impede the development of PA. Regarding external challenges, relational and external conditions hamper the application. Therefore, especially actions of third-party institutions in terms of standardization and security enhancements are required. This study contributes to the existing literature in various ways as the systematic identification addresses real-world issues of PA in the neglected but crucial area of transport logistics, discussing urgent research needs and highlighting potential solutions. Additionally, the results offer valuable guidance for managers when implementing PA in transport logistics.},
booktitle = {Proceedings of the 2020 on Computers and People Research Conference},
pages = {144–151},
numpages = {8},
keywords = {big data, challenges, predictive analytics, supply chain management, transport logistics},
location = {Nuremberg, Germany},
series = {SIGMIS-CPR'20}
}

@inproceedings{10.1145/2755492.2755494,
author = {Huang, Qunying and Cao, Guofeng and Wang, Caixia},
title = {From Where Do Tweets Originate? A GIS Approach for User Location Inference},
year = {2014},
isbn = {9781450331401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755492.2755494},
doi = {10.1145/2755492.2755494},
abstract = {A number of natural language processing and text-mining algorithms have been developed to extract the geospatial cues (e.g., place names) to infer locations of content creators from publicly available information, such as text content, online social profiles, and the behaviors or interactions of users from social networks. These studies, however, can only successfully infer user locations at city levels with relatively decent accuracy, while much higher resolution is required for meaningful spatiotemporal analysis in geospatial fields. Additionally, geographical cues exploited by current text-based approaches are hidden in the unreliable, unstructured, informal, ungrammatical, and multilingual data, and therefore are hard to extract and make meaningful correctly. Instead of using such hidden geographic cues, this paper develops a GIS approach that can infer the true origin of tweets down to the zip code level by using and mining spatial (geo-tags) and temporal (timestamps when a message was posted) information recorded on user digital footprints. Further, individual major daily activity zones and mobility can be successfully inferred and predicted. By integrating GIS data and spatiotemporal clustering methods, this proposed approach can infer individual daily physical activity zones with spatial resolution as high as 20 m by 20 m or even higher depending on the number of digit footprints collected for social media users. The research results with detailed spatial resolution are necessary and useful for various applications such as human mobility pattern analysis, business site selection, disease control, or transportation systems improvement.},
booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Location-Based Social Networks},
pages = {1–8},
numpages = {8},
keywords = {geography, spatial clustering, spatiotemporal clustering, human mobility, big data},
location = {Dallas/Fort Worth, Texas},
series = {LBSN '14}
}

@inproceedings{10.1145/3433996.3434008,
author = {Guo, Xusheng and Liang, Likeng and Liu, Yuanxia and Weng, Heng and Hao, Tianyong},
title = {The Construction of a Diabetes-Oriented Frequently Asked Question Corpus for Automated Question-Answering Services},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434008},
doi = {10.1145/3433996.3434008},
abstract = {In recent years, the prevalence of diabetes has been increasing rapidly worldwide. With the advancement of information technology, automated question-answering services for healthcare, which are commonly based on annotated corpus in health domain, have positive effects on health knowledge spread and daily health management for high-risk populations. This paper proposes to construct a large scale diabetes corpus of frequently-asked questions for automated question-answering services and evaluations. Concentrating on the characteristics of diabetes-related factors that reflect conditions of diabetes, this work establishes an annotated dataset containing professional question &amp; answer pairs about diabetes and their annotated question target categories. The corpus is applicable for various question-answering applications, supporting users to retrieve needed information, arrange diets, adhere to scientific medication as well as prevent and control disease complications.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {60–66},
numpages = {7},
keywords = {frequently-asked questions, visualization, Diabetes, corpus construction},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/2882903.2912574,
author = {Chu, Xu and Ilyas, Ihab F. and Krishnan, Sanjay and Wang, Jiannan},
title = {Data Cleaning: Overview and Emerging Challenges},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2912574},
doi = {10.1145/2882903.2912574},
abstract = {Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the field, we will first present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-the-art techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efficiency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2201–2206},
numpages = {6},
keywords = {statistical cleaning, integrity constraints, sampling, data quality, data cleaning},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3447513,
author = {Deng, Song and Chen, Fulin and Dong, Xia and Gao, Guangwei and Wu, Xindong},
title = {Short-Term Load Forecasting by Using Improved GEP and Abnormal Load Recognition},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3447513},
doi = {10.1145/3447513},
abstract = {Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-term load forecasting algorithms in terms of the convergence speed, MAE, MAPE, RSME, and R2.},
journal = {ACM Trans. Internet Technol.},
month = {jul},
articleno = {95},
numpages = {28},
keywords = {adaptive evolution, probability distribution, power load forecasting, abnormal load recognition, Gene expression programming}
}

@inproceedings{10.1145/3460866.3461769,
author = {Nesen, Alina and Bhargava, Bharat},
title = {Towards Situational Awareness with Multimodal Streaming Data Fusion: Serverless Computing Approach},
year = {2021},
isbn = {9781450384650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460866.3461769},
doi = {10.1145/3460866.3461769},
abstract = {The availability of large quantities of data has given an impulse for methods and techniques to extract unseen useful knowledge and process it in a fast and scalable manner. In order to extract the most complete possible knowledge from the continuous data stream, it is necessary to use the heterogeneous data sources and process information from multiple modalities. The systems that utilize multimodal data must take advantage of the up-to-date approaches for data storage, usage, cleaning and storage. Neural networks and machine learning approaches are widely used for data-heavy software where pattern extractions and predictions need to be conducted while serverless computing frameworks are being increasingly used for machine learning solutions to optimize cost and speed of such systems. This work presents a framework for processing data from multimodal sources where the task of feature and pattern extraction is performed on a serverless computing platform. The use cases for public safety solutions to increase situational awareness are described and compared with other implementation approaches.},
booktitle = {Proceedings of the International Workshop on Big Data in Emergent Distributed Environments},
articleno = {5},
numpages = {6},
keywords = {serverless computing, function-as-a-service, multimodal machine learning},
location = {Virtual Event, China},
series = {BiDEDE '21}
}

@article{10.1145/3345551,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Large-Scale Semantic Integration of Linked Data: A Survey},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3345551},
doi = {10.1145/3345551},
abstract = {A large number of published datasets (or sources) that follow Linked Data principles is currently available and this number grows rapidly. However, the major target of Linked Data, i.e., linking and integration, is not easy to achieve. In general, information integration is difficult, because (a) datasets are produced, kept, or managed by different organizations using different models, schemas, or formats, (b) the same real-world entities or relationships are referred with different URIs or names and in different natural languages,<!--?brk?-->(c) datasets usually contain complementary information, (d) datasets can contain data that are erroneous, out-of-date, or conflicting, (e) datasets even about the same domain may follow different conceptualizations of the domain, (f) everything can change (e.g., schemas, data) as time passes. This article surveys the work that has been done in the area of Linked Data integration, it identifies the main actors and use cases, it analyzes and factorizes the integration process according to various dimensions, and it discusses the methods that are used in each step. Emphasis is given on methods that can be used for integrating several datasets. Based on this analysis, the article concludes with directions that are worth further research.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {103},
numpages = {40},
keywords = {Data integration, RDF, big data, data discovery, semantic web}
}

@inproceedings{10.1145/2882903.2899414,
author = {Agrawal, Divy and Ba, Lamine and Berti-Equille, Laure and Chawla, Sanjay and Elmagarmid, Ahmed and Hammady, Hossam and Idris, Yasser and Kaoudi, Zoi and Khayyat, Zuhair and Kruse, Sebastian and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge-Arnulfo and Tang, Nan and Zaki, Mohammed J.},
title = {Rheem: Enabling Multi-Platform Task Execution},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2899414},
doi = {10.1145/2882903.2899414},
abstract = {Many emerging applications, from domains such as healthcare and oil &amp; gas, require several data processing systems for complex analytics. This demo paper showcases system, a framework that provides multi-platform task execution for such applications. It features a three-layer data processing abstraction and a new query optimization approach for multi-platform settings. We will demonstrate the strengths of system by using real-world scenarios from three different applications, namely, machine learning, data cleaning, and data fusion.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2069–2072},
numpages = {4},
keywords = {data analytics, cross-platform execution, big data},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3436817,
author = {Harley, Kelsey and Cooper, Rodney},
title = {Information Integrity: Are We There Yet?},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3436817},
doi = {10.1145/3436817},
abstract = {The understanding and promotion of integrity in information security has traditionally been underemphasized or even ignored. From implantable medical devices and electronic voting to vehicle control, the critical importance of information integrity to our well-being has compelled review of its treatment in the literature. Through formal information flow models, the data modification view, and the relationship to data quality, information integrity will be surveyed. Illustrations are given for databases and information trustworthiness. Integrity protection is advancing but lacks standardization in terminology and application. Integrity must be better understood, and pursued, to achieve devices and systems that are beneficial and safe for the future.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {33},
numpages = {35},
keywords = {information flow, information trustworthiness, noninterference, information integrity, data quality, Integrity, information security, quality dimension, Clark-Wilson model, quality assessment, Biba’s model, security requirements, information quality}
}

@inproceedings{10.5555/2873021.2873031,
author = {Ferguson, Holly T. and Vardeman, Charles F. and Buccellato, Aimee P. C.},
title = {Capturing an Architectural Knowledge Base Utilizing Rules Engine Integration for Energy and Environmental Simulations},
year = {2015},
isbn = {9781510801042},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The era of "Big Data" presents new challenges and opportunities to impact how the built environment is designed and constructed. Modern design tools and material databases should be more scalable, reliable, and accessible to take full advantage of the quantity of available building data. New approaches providing well-structured information can lead to robust decision support for architectural simulations earlier in the design process; rule-based decision engines and knowledge bases are the link between current data and useful decision frameworks. Integrating distributed API-based systems means that material data silos existing in modern tools can become enriched and extensible for future use with additional data from building documents, other databases, and the minds of design professionals. The PyKE rules engine extension to the Green Scale (GS) Tool improves material searches, creates the opportunity for incorporating additional rules via a REST interface, and enables integration with the Semantic Web via Linked Data principles.},
booktitle = {Proceedings of the Symposium on Simulation for Architecture &amp; Urban Design},
pages = {67–74},
numpages = {8},
keywords = {big data, verification, sustainable data, SWIRL, OWL, knowledge based rules, green scale tool, PyKE, RIF, reliability, performance, REST, design, machine learning, algorithms, SPARAQL, linked data, expert systems, experimentation, SPIN, semantic web, standardization, HCI, ontological knowledge engine},
location = {Alexandria, Virginia},
series = {SimAUD '15}
}

@inproceedings{10.1145/3341105.3373989,
author = {Alaa, Mostafa and Bolock, Alia El and Abas, Mostafa and Abdennadher, Slim and Herbert, Cornelia},
title = {AppGen: A Framework for Automatic Generation of Data Collection Apps},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373989},
doi = {10.1145/3341105.3373989},
abstract = {Data, and its collection, is one core aspect of technology and research, nowadays. Various scientific disciplines are interested in collecting human data in practically any context (at home, at work, during leisure time). For example, experts from the field of Psychology design studies for reliable and valid data collection in the laboratory and in the wild. We propose a generic platform for data-collection software development to be used by scientists without a programming background. This is done by adapting a basic Unity project through a configuration file provided by the platform users through an easy to use user interface. The scientific user can adapt and rearrange pre-defined data collection modules targeting a desired research question, implement it as application within the data collection platform and use and manage the application for data collection and later data analysis. As a proof of concept, the platform was embedded with build-in application modules for wide-spread Psychology data collection experiments. The versatility of the platform was tested by creating three diverse prototypical applications. Finally, the usability of the proposed platform evaluated using the System Usability Scale obtained high usability results. The robust module-based nature of the platform architecture makes is possible to create a various range of of psychologically-proven applications with different features to be decided by the researcher. This holds true for both the development phase of the applications, as well as, for after deployment.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1906–1913},
numpages = {8},
keywords = {psychology, data collection, software framework, character computing, big data},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3173574.3173710,
author = {Bowyer, Alex and Montague, Kyle and Wheater, Stuart and McGovern, Ruth and Lingam, Raghu and Balaam, Madeline},
title = {Understanding the Family Perspective on the Storage, Sharing and Handling of Family Civic Data},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173710},
doi = {10.1145/3173574.3173710},
abstract = {Across social care, healthcare and public policy, enabled by the "big data" revolution (which has normalized large-scale data-based decision-making), there are moves to "join up" citizen databases to provide care workers with holistic views of families they support. In this context, questions of personal data privacy, security, access, control and (dis-)empowerment are critical considerations for system designers and policy makers alike. To explore the family perspective on this landscape of what we call Family Civic Data, we carried out ethnographic interviews with four North-East families. Our design-game-based interviews were effective for engaging both adults and children to talk about the impact of this dry, technical topic on their lives. Our findings, delivered in the form of design guidelines, show support for dynamic consent: families would feel most empowered if involved in an ongoing co-operative relationship with state welfare and civic authorities through shared interaction with their data.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {healthcare, ubicomp, personal data, ethnographic interviews, dynamic consent, big data, civic data, social care, family, boundary objects, family research, design games, user-centered design, family design games, data privacy, data sharing, data security},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3219819.3219916,
author = {Xin, SHEN and Yang, Hongxia and Xian, Weizhao and Ester, Martin and Bu, Jiajun and Wang, Zhongyao and Wang, Can},
title = {Mobile Access Record Resolution on Large-Scale Identifier-Linkage Graphs},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219916},
doi = {10.1145/3219819.3219916},
abstract = {The e-commerce era is witnessing a rapid increase of mobile Internet users. Major e-commerce companies nowadays see billions of mobile accesses every day. Hidden in these records are valuable user behavioral characteristics such as their shopping preferences and browsing patterns. And, to extract these knowledge from the huge dataset, we need to first link records to the corresponding mobile devices. This Mobile Access Records Resolution (MARR) problem is confronted with two major challenges: (1) device identifiers and other attributes in access records might be missing or unreliable; (2) the dataset contains billions of access records from millions of devices. To the best of our knowledge, as a novel challenge industrial problem of mobile Internet, no existing method has been developed to resolve entities using mobile device identifiers in such a massive scale. To address these issues, we propose a SParse Identifier-linkage Graph (SPI-Graph) accompanied with the abundant mobile device profiling data to accurately match mobile access records to devices. Furthermore, two versions (unsupervised and semi-supervised) of Parallel Graph-based Record Resolution (PGRR) algorithm are developed to effectively exploit the advantages of the large-scale server clusters comprising of more than 1,000 computing nodes. We empirically show superior performances of PGRR algorithms in a very challenging and sparse real data set containing 5.28 million nodes and 31.06 million edges from 2.15 billion access records compared to other state-of-the-arts methodologies.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {886–894},
numpages = {9},
keywords = {mobile access record resolution, scalable algorithms, big data, graph algorithms},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/2910674.2935861,
author = {Bj\"{o}rk, Kaj-Mikael and Eirola, Emil and Miche, Yoan and Lendasse, Amaury},
title = {A New Application of Machine Learning in Health Care},
year = {2016},
isbn = {9781450343374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910674.2935861},
doi = {10.1145/2910674.2935861},
abstract = {In our ever more complex world, the field of analytics has dramatically increased its importance. Gut feeling is no longer sufficient in decision making, but intuition has to be combined with support from the huge amount of data available today. Even if the amount of data is enormous, the quality of the data is not always good. Problems arise in at least two situations: i) the data is imprecise by nature and ii) the data is incomplete (or there are missing parts in the data set). Both situations are problematic and need to be addressed appropriately. If these problems are solved, applications are to be found in various interesting fields. We aim at achieving significant methodology development as well as creative solutions in the domain of medicine, information systems and risk management. This paper sets focus especially on missing data problems in the field of medicine when presenting a new project in its very first phase.},
booktitle = {Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {49},
numpages = {4},
keywords = {Health care, Machine Learning, Missing values, Huntington's disease, Big Data},
location = {Corfu, Island, Greece},
series = {PETRA '16}
}

@inproceedings{10.1145/3478905.3478920,
author = {Fei, Yiming and Yuan, Xiaoyue and Ren, Mengmeng and Fan, Shuhai},
title = {Research on Horizontal Integration Scheme for Mass Customization Data Quantity and Quality Problem: Horizontal Integration Scheme for MC},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478920},
doi = {10.1145/3478905.3478920},
abstract = {To solve the problem of Mass Customization Data Quantity and Quality Problem, a Horizontal Integration Scheme of MC is proposed. By using LiDAR technology to scan and identify parts information, the machining route and required parts of the workpiece are automatically planned by comparing and matching with the documents using STEP-NC standard, to realize the efficient acquisition and utilization of MC data and ensure the automation of enterprise production.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {69–73},
numpages = {5},
keywords = {Mass Customization, Data Quality, LiDAR Camera Technology, Horizontal Integration},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/3340531.3414077,
author = {Ukil, Arijit and Marin, Leandro and Jara, Antonio and Farserotu, John},
title = {On the Knowledge-Driven Analytics and Systems Impacting Human Quality of Life},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3414077},
doi = {10.1145/3340531.3414077},
abstract = {The present scenario of Covid-19 pandemic has disrupted the human life to a larger extent. In such context, human-centric applications and systems that endeavor to positively impact the human quality of life is of utmost importance. Knowledge-driven analytics that help to build such intelligent systems play important role to construct the required eco-system on the macro-scale. It is worth mentioning that Knowledge-Driven Analytics and Systems Impacting Human Quality of Life (KDAH) workshop in ACM International Conference on Information and Knowledge Management (CIKM), attempts to bring out the intricate research direction for enabling a sustainable human society through the positive co-existence of human beings and intelligent systems.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3539–3540},
numpages = {2},
keywords = {sensors, artificial intelligence, human life, deep learning, security, privacy, big data, knowledge},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3468791.3468841,
author = {Bechny, Michal and Sobieczky, Florian and Zeindl, J\"{u}rgen and Ehrlinger, Lisa},
title = {Missing Data Patterns: From Theory to an Application in the Steel Industry},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3468841},
doi = {10.1145/3468791.3468841},
abstract = {Missing data (MD) is a prevalent problem and can negatively affect the trustworthiness of data analysis. In industrial use cases, faulty sensors or errors during data integration are common causes for systematically missing values. The majority of MD research deals with imputation, i.e., the replacement of missing values with “best guesses”. Most imputation methods require missing values to occur independently, which is rarely the case in industry. Thus, it is necessary to identify missing data patterns (i.e., systematically missing values) prior to imputation (1) to understand the cause of the missingness, (2) to gain deeper insight into the data, and (3) to choose the proper imputation technique. However, in literature, there is a wide varity of MD patterns without a common formalization. In this paper, we introduce the first formal definition of MD patterns. Building on this theory, we developed a systematic approach on how to automatically detect MD patterns in industrial data. The approach has been developed in cooperation with voestalpine Stahl GmbH, where we applied it to real-world data from the steel industry and demonstrated its efficacy with a simulation study.},
booktitle = {33rd International Conference on Scientific and Statistical Database Management},
pages = {214–219},
numpages = {6},
keywords = {Steel Industry., Pattern Detection, Missing Data, Data Quality},
location = {Tampa, FL, USA},
series = {SSDBM 2021}
}

@inproceedings{10.1145/3216122.3216148,
author = {Chabin, Jacques and Gomes-Jr., Luiz and Halfeld-Ferrari, Mirian},
title = {A Context-Driven Querying System for Urban Graph Analysis},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216148},
doi = {10.1145/3216122.3216148},
abstract = {This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing it to capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {297–301},
numpages = {5},
keywords = {data quality, Query language, constraints, smart city, data graph},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@article{10.1145/2822898,
author = {Coletti, Paolo and Murgia, Maurizio},
title = {Design and Construction of a Historical Financial Database of the Italian Stock Market 1973--2011},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2822898},
doi = {10.1145/2822898},
abstract = {This article presents the technical aspects of designing and building a historical database of the Italian Stock Market. The database contains daily market data from 1973 to 2011 and is constructed by merging two main digital sources and several other hand-collected data sources. We analyzed and developed semiautomatic tools to deal with problems related to time-series matchings, quality of data, and numerical errors. We also developed a concatenation structure to allow the handling of company name changes, mergers, and spin-offs without artificially altering numerical series. At the same time, we maintained the transparency of the historical information on each individual company listed. Thanks to the overlapping of digital and hand-collected data, the completed database has a very high level of detail and accuracy. The dataset is particularly suited for any empirical research in financial economics and for more practically oriented numerical applications and forecasting simulations.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {16},
numpages = {23},
keywords = {stock market, Financial database, data integration, data quality}
}

@inproceedings{10.1145/2948992.2949009,
author = {Martins, Pedro and Cec\'{\i}lio, Jos\'{e} and Abbasi, Maryam and Furtado, Pedro},
title = {GPII: A Benchmark for Generic Purpose Image Information},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949009},
doi = {10.1145/2948992.2949009},
abstract = {The growing number of different models and approaches for Geographic Information Systems (GIS) brings high complexity when we want to develop new approaches and compare a new GIS algorithm. In order to test and compare different processing models and approaches, in a simple way, we identified the need of defining uniform testing methods, able to compare processing algorithms in terms of performance and accuracy regarding large image processing, algorithms for GIS pattern-detection.Taking into account, for instance, images collected during a done flight or a satellite, it is important to know the processing cost to extract data when applying different processing models and approaches, as well as their accuracy (compare execution time vs. extracted data quality). In this work, we propose a GIS Benchmark (GPII), a benchmark that allows evaluating different approaches to detect/extract selected features from a GIS dataset. Considering a given dataset (or two data-sets, from different years, of the same region), it provides linear methods to compare different performance parameters regarding GIS information, making possible to access the most relevant information in terms of features and processing efficiency. Moreover, our approach to test algorithms makes possible to change the data-set in order to support different purpose algorithms.},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {119–122},
numpages = {4},
keywords = {Big-data, spatio-temporal databases, Benchmark, performance, pattern-detection, experimentation, algorithms, GIS},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.1145/2939672.2939799,
author = {Xu, Tong and Zhu, Hengshu and Zhao, Xiangyu and Liu, Qi and Zhong, Hao and Chen, Enhong and Xiong, Hui},
title = {Taxi Driving Behavior Analysis in Latent Vehicle-to-Vehicle Networks: A Social Influence Perspective},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939799},
doi = {10.1145/2939672.2939799},
abstract = {With recent advances in mobile and sensor technologies, a large amount of efforts have been made on developing intelligent applications for taxi drivers, which provide beneficial guide and opportunity to improve the profit and work efficiency. However, limited scopes focus on the latent social interaction within cab drivers, and corresponding social propagation scheme to share driving behaviors has been largely ignored. To that end, in this paper, we propose a comprehensive study to reveal how the social propagation affects for better prediction of cab drivers' future behaviors. To be specific, we first investigate the correlation between drivers' skills and their mutual interactions in the latent vehicle-to-vehicle network, which intuitively indicates the effects of social influences. Along this line, by leveraging the classic social influence theory, we develop a two-stage framework for quantitatively revealing the latent driving pattern propagation within taxi drivers. Comprehensive experiments on a real-word data set collected from the New York City clearly validate the effectiveness of our proposed framework on predicting future taxi driving behaviors, which also support the hypothesis that social factors indeed improve the predictability of driving behaviors.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1285–1294},
numpages = {10},
keywords = {social influence, mobile data mining, taxi trajectories},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3290607.3313002,
author = {Hohmann, Matthias R. and Hackl, Michelle and Wirth, Brian and Zaman, Talha and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\"{o}lkopf, Bernhard},
title = {MYND: A Platform for Large-Scale Neuroscientific Studies},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3313002},
doi = {10.1145/3290607.3313002},
abstract = {We present a smartphone application for at-home participation in large-scale neuroscientific studies. Our goal is to establish user-experience design as a paradigm in basic neuroscientific research to overcome the limits of current studies, especially in rare neurological disorders.The presented application guides users through the fitting procedure of the EEG headset and automatically encrypts and uploads recorded data to a remote server. User-feedback and neurophysiological data from a pilot study with eighteen subjects indicate that the application can be used outside of a laboratory, without the need for external guidance. We hope to inspire future work on the intersection between basic neuroscience and human-computer interaction as a promising paradigm to accelerate research on rare neurological diseases and assistive neurotechnology.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {big data, electrophysiology, smartphone application, user-centered design, medical studies, neuroscience, wearable sensors},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@article{10.1145/3432247,
author = {Garriga, Martin and Aarns, Koen and Tsigkanos, Christos and Tamburri, Damian A. and Heuvel, Wjan Van Den},
title = {DataOps for Cyber-Physical Systems Governance: The Airport Passenger Flow Case},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3432247},
doi = {10.1145/3432247},
abstract = {Recent advancements in information technology have ushered a new wave of systems integrating Internet technology with sensing, wireless communication, and computational resources over existing infrastructures. As a result, myriad complex, non-traditional Cyber-Physical Systems (CPS) have emerged, characterized by interaction among people, physical facilities, and embedded sensors and computers, all generating vast amounts of complex data. Such a case is encountered within a contemporary airport hall setting: passengers roaming, information systems governing various functions, and data being generated and processed by cameras, phones, sensors, and other Internet of Things technology. This setting has considerable potential of contributing to goals entertained by the CPS operators, such as airlines, airport operators/owners, technicians, users, and more. We model the airport setting as an instance of such a complex, data-intensive CPS where multiple actors and data sources interact, and generalize a methodology to support it and other similar systems. Furthermore, this article instantiates the methodology and pipeline for predictive analytics for passenger flow, as a characteristic manifestation of such systems requiring a tailored approach. Our methodology also draws from DataOps principles, using multi-modal and real-life data to predict the underlying distribution of the passenger flow on a flight-level basis (improving existing day-level predictions), anticipating when and how the passengers enter the airport and move through the check-in and baggage drop-off process. This allows to plan airport resources more efficiently while improving customer experience by avoiding passenger clumping at check-in and security. We demonstrate results obtained over a case from a major international airport in the Netherlands, improving up to 60% upon predictions of daily passenger flow currently in place.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {36},
numpages = {25},
keywords = {cyber-physical systems, big data, DataOps, Data-intensive systems, airport management, systems governance}
}

@article{10.1007/s00778-021-00704-2,
author = {Alva Principe, Renzo Arturo and Maurino, Andrea and Palmonari, Matteo and Ciavotta, Michele and Spahiu, Blerina},
title = {ABSTAT-HD: A Scalable Tool for Profiling Very Large Knowledge Graphs},
year = {2022},
issue_date = {Sep 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00704-2},
doi = {10.1007/s00778-021-00704-2},
abstract = {Processing large-scale and highly interconnected Knowledge Graphs (KG) is becoming crucial for many applications such as recommender systems, question answering, etc. Profiling approaches have been proposed to summarize large KGs with the aim to produce concise and meaningful representation so that they can be easily managed. However, constructing profiles and calculating several statistics such as cardinality descriptors or inferences are resource expensive. In this paper, we present ABSTAT-HD, a highly distributed profiling tool that supports users in profiling and understanding big and complex knowledge graphs. We demonstrate the impact of the new architecture of ABSTAT-HD by presenting a set of experiments that show its scalability with respect to three dimensions of the data to be processed: size, complexity and workload. The experimentation shows that our profiling framework provides informative and concise profiles, and can process and manage very large KGs.},
journal = {The VLDB Journal},
month = {sep},
pages = {851–876},
numpages = {26},
keywords = {Data quality, Data profiling, Distributed processing engine, Data management, Knowledge graph}
}

@inproceedings{10.1145/3338906.3338931,
author = {Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei},
title = {Robust Log-Based Anomaly Detection on Unstable Log Data},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338931},
doi = {10.1145/3338906.3338931},
abstract = {Logs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {807–817},
numpages = {11},
keywords = {Log Instability, Deep Learning, Log Analysis, Data Quality, Anomaly Detection},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3386723.3387850,
author = {Maqboul, Jaouad and Jaouad, Bouchaib Bounabat},
title = {Contribution of Artificial Neural Network in Predicting Completeness Through the Impact and Complexity of Its Improvement},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387850},
doi = {10.1145/3386723.3387850},
abstract = {The technological evolution and the immensity of the data produced, circulated into company makes these data, the real capital of the companies to the detriment of the customers. The erroneous data put the knockout to relationships with customers, the company must address this problem and identify the quality projects on which it must make an effort. In this article, we will present an approach based on qualitative and quantitative analysis to help the decision-makers to target data by its impacts and complexities of process improvement. The Qualitative study will be a survey and a quantitative to learn from survey data to decide the prediction and the completeness of data.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {31},
numpages = {8},
keywords = {cost/benefit analysis, Data quality improvement project, artificial neural network, data quality assessment and improvement, cost of data quality},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@article{10.1145/3483423,
author = {Shraga, Roee and Gal, Avigdor},
title = {PoWareMatch: A Quality-Aware Deep Learning Approach to Improve Human Schema Matching},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3483423},
doi = {10.1145/3483423},
abstract = {Schema matching is a core task of any data integration process. Being investigated in the fields of databases, AI, Semantic Web, and data mining for many years, the main challenge remains the ability to generate quality matches among data concepts (e.g., database attributes). In this work, we examine a novel angle on the behavior of humans as matchers, studying match creation as a process. We analyze the dynamics of common evaluation measures (precision, recall, and f-measure), with respect to this angle and highlight the need for unbiased matching to support this analysis. Unbiased matching, a newly defined concept that describes the common assumption that human decisions represent reliable assessments of schemata correspondences, is, however, not an inherent property of human matchers. In what follows, we design PoWareMatch that makes use of a deep learning mechanism to calibrate and filter human matching decisions adhering to the quality of a match, which are then combined with algorithmic matching to generate better match results. We provide an empirical evidence, established based on an experiment with more than 200 human matchers over common benchmarks, that PoWareMatch predicts well the benefit of extending the match with an additional correspondence and generates high-quality matches. In addition, PoWareMatch outperforms state-of-the-art matching algorithms.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {16},
numpages = {27},
keywords = {deep learning, data quality, Human-in-the-loop}
}

@inproceedings{10.1145/3404555.3404621,
author = {Zhu, Ruyi},
title = {Traffic Condition Prediction of Urban Roads Based on Neural Network},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404621},
doi = {10.1145/3404555.3404621},
abstract = {Real-time and reliable traffic flow estimation is the basis of urban traffic management and control. However, the existing research focuses on how to use the historical data of surveillance intersection to predict future traffic conditions. As we know, there are few effective algorithms to infer the real-time traffic state of non-surveillance intersections from limited road surveillance by using traffic information in the urban road system. In this paper, we introduce a new solution to solve the prediction task of traffic flow analysis by using traffic data, especially taxi historical data, traffic network data and intersection historical data. The proposed solution takes advantage of GCN and CGAN, and we improved the Unet to realize an important part of the generator. Then, we capture the relationship between the intersections with surveillance and the intersections without surveillance by floating taxi-cabs covered in the whole city. The framework of CGAN can adjust the weights and enhance the inference ability to generate complete traffic status under current conditions. The experimental results show that our method is superior to other methods on the accuracy of traffic volume inference.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {30–36},
numpages = {7},
keywords = {forecasting, Urban road system, video surveillance system, big data, real-time traffic condition},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1145/3495018.3495097,
author = {Chen, Zhangbin and Liu, Yang},
title = {Research and Construction of University Data Governance Platform Based on Smart Campus Environment},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495097},
doi = {10.1145/3495018.3495097},
abstract = {Campus data is a subset of education big data. It is a variety of data generated by teachers and students in the life, teaching, scientific research, management and service process, as well as various school affairs management status data. It has the characteristics of a wide variety of data. Contains great information value, and giving full play to its role is an indispensable part of achieving the school's strategic goals. Taking the opportunity of building a smart campus, using advanced technologies such as cloud computing, big data, Internet of Things, and artificial intelligence, through a big data management platform, the full collection of existing business data inside and outside the school is provided, and a normal data governance model is provided to eliminate data islands. Realize normal data sharing services. The article conducts research on the data governance platform, and builds a data governance platform system with functions such as a normalized data quality monitoring system, information resource catalog system and full-link data monitoring to realize university data integration, business collaboration, service upgrades and assistance Decision-making provides a solid foundation for the application and expansion of smart campuses in universities and provides a reference for smart campus builders in universities.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {450–455},
numpages = {6},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1007/s00778-021-00720-2,
author = {Sadiq, Shazia and Aryani, Amir and Demartini, Gianluca and Hua, Wen and Indulska, Marta and Burton-Jones, Andrew and Khosravi, Hassan and Benavides-Prado, Diana and Sellis, Timos and Someh, Ida and Vaithianathan, Rhema and Wang, Sen and Zhou, Xiaofang},
title = {Information Resilience: The Nexus of Responsible and Agile Approaches to Information Use},
year = {2022},
issue_date = {Sep 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00720-2},
doi = {10.1007/s00778-021-00720-2},
abstract = {The appetite for effective use of information assets has been steadily rising in both public and private sector organisations. However, whether the information is used for social good or commercial gain, there is a growing recognition of the complex socio-technical challenges associated with balancing the diverse demands of regulatory compliance and data privacy, social expectations and ethical use, business process agility and value creation, and scarcity of data science talent. In this vision paper, we present a series of case studies that highlight these interconnected challenges, across a range of application areas. We use the insights from the case studies to introduce Information Resilience, as a scaffold within which the competing requirements of responsible and agile approaches to information use can be positioned. The aim of this paper is to develop and present a manifesto for Information Resilience that can serve as a reference for future research and development in relevant areas of responsible data management.},
journal = {The VLDB Journal},
month = {sep},
pages = {1059–1084},
numpages = {26},
keywords = {Data quality, Value creation, Effective information use, Information Resilience, Responsible data science}
}

@inproceedings{10.1145/3328905.3332513,
author = {Frischbier, Sebastian and Paic, Mario and Echler, Alexander and Roth, Christian},
title = {A Real-World Distributed Infrastructure for Processing Financial Data at Scale},
year = {2019},
isbn = {9781450367943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328905.3332513},
doi = {10.1145/3328905.3332513},
abstract = {Financial markets are event- and data-driven to an extremely high degree. For making decisions and triggering actions stakeholders require notifications about significant events and reliable background information that meet their individual requirements in terms of timeliness, accuracy, and completeness. As one of Europe's leading providers of financial data and regulatory solutions vwd: processes an average of 18 billion event notifications from 500+ data sources for 30 million symbols per day. Our large-scale distributed event-based systems handle daily peak rates of 1+ million event notifications per second and additional load generated by singular pivotal events with global impact.In this poster we give practical insights into our IT systems. We outline the infrastructure we operate and the event-driven architecture we apply at vwd. In particular we showcase the (geo)distributed publish/subscribe broker network we operate across locations and countries to provide market data to our customers with varying quality of information (QoI) properties.},
booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems},
pages = {254–255},
numpages = {2},
keywords = {broker network, big data, quality of information, publish/subscribe, stream-processing, infrastructure, Event-processing, financial data},
location = {Darmstadt, Germany},
series = {DEBS '19}
}

@article{10.1145/2906149,
author = {Khan, Suleman and Gani, Abdullah and Wahab, Ainuddin Wahid Abdul and Bagiwa, Mustapha Aminu and Shiraz, Muhammad and Khan, Samee U. and Buyya, Rajkumar and Zomaya, Albert Y.},
title = {Cloud Log Forensics: Foundations, State of the Art, and Future Directions},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2906149},
doi = {10.1145/2906149},
abstract = {Cloud log forensics (CLF) mitigates the investigation process by identifying the malicious behavior of attackers through profound cloud log analysis. However, the accessibility attributes of cloud logs obstruct accomplishment of the goal to investigate cloud logs for various susceptibilities. Accessibility involves the issues of cloud log access, selection of proper cloud log file, cloud log data integrity, and trustworthiness of cloud logs. Therefore, forensic investigators of cloud log files are dependent on cloud service providers (CSPs) to get access of different cloud logs. Accessing cloud logs from outside the cloud without depending on the CSP is a challenging research area, whereas the increase in cloud attacks has increased the need for CLF to investigate the malicious activities of attackers. This paper reviews the state of the art of CLF and highlights different challenges and issues involved in investigating cloud log data. The logging mode, the importance of CLF, and cloud log-as-a-service are introduced. Moreover, case studies related to CLF are explained to highlight the practical implementation of cloud log investigation for analyzing malicious behaviors. The CLF security requirements, vulnerability points, and challenges are identified to tolerate different cloud log susceptibilities. We identify and introduce challenges and future directions to highlight open research areas of CLF for motivating investigators, academicians, and researchers to investigate them.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {7},
numpages = {42},
keywords = {cloud log forensics, correlation of cloud logs, big data, Cloud computing, confidentiality, authenticity, integrity}
}

@article{10.1145/3379445,
author = {Bonifati, Angela and Holubov\'{a}, Irena and Prat-P\'{e}rez, Arnau and Sakr, Sherif},
title = {Graph Generators: State of the Art and Open Challenges},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379445},
doi = {10.1145/3379445},
abstract = {The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties or gauging the effectiveness of graph algorithms, techniques, and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers, and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {36},
numpages = {30},
keywords = {Big data management, benchmarks, generators, synthetic data, graph data}
}

@article{10.5555/3546258.3546276,
author = {Jiang, Gaoxia and Wang, Wenjian and Qian, Yuhua and Liang, Jiye},
title = {A Unified Sample Selection Framework for Output Noise Filtering: An Error-Bound Perspective},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {The existence of output noise will bring difficulties to supervised learning. Noise filtering, aiming to detect and remove polluted samples, is one of the main ways to deal with the noise on outputs. However, most of the filters are heuristic and could not explain the filtering in uence on the generalization error (GE) bound. The hyper-parameters in various filters are specified manually or empirically, and they are usually unable to adapt to the data environment. The filter with an improper hyper-parameter may overclean, leading to a weak generalization ability. This paper proposes a unified framework of optimal sample selection (OSS) for the output noise filtering from the perspective of error bound. The covering distance filter (CDF) under the framework is presented to deal with noisy outputs in regression and ordinal classification problems. Firstly, two necessary and sufficient conditions for a fixed goodness of fit in regression are deduced from the perspective of GE bound. They provide the unified theoretical framework for determining the filtering effectiveness and optimizing the size of removed samples. The optimal sample size has the adaptability to the environmental changes in the sample size, the noise ratio, and noise variance. It offers a choice of tuning the hyper-parameter and could prevent filters from overcleansing. Meanwhile, the OSS framework can be integrated with any noise estimator and produces a new filter. Then the covering interval is proposed to separate low-noise and high-noise samples, and the effectiveness is proved in regression. The covering distance is introduced as an unbiased estimator of high noises. Further, the CDF algorithm is designed by integrating the cover distance with the OSS framework. Finally, it is verified that the CDF not only recognizes noise labels correctly but also brings down the prediction errors on real apparent age data set. Experimental results on benchmark regression and ordinal classification data sets demonstrate that the CDF outperforms the state-of-the-art filters in terms of prediction ability, noise recognition, and efficiency.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {18},
numpages = {66},
keywords = {supervised learning, output noise, covering distance filtering, optimal sample selection, generalization error bound}
}

@inproceedings{10.1145/3383583.3398539,
author = {Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta, Amit and Jah, Moriba},
title = {Modeling Data Curation to Scientific Inquiry: A Case Study for Multimodal Data Integration},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398539},
doi = {10.1145/3383583.3398539},
abstract = {Scientific data publications may include interactive data applications designed by scientists to explore a scientific problem. Defined as knowledge systems, their development is complex when data are aggregated from multiple sources over time. Multimodal data are created, encoded, and maintained differently, and even when reporting about identical phenomena, fields and their values may be inconsistent across datasets. To assure the validity and accuracy of the application, the data has to abide by curation requirements similar to those ruling digital libraries. We present a novel, inquiry-driven curation approach aimed to optimize multimodal datasets curation and maximize data reuse by domain researchers. We demonstrate the method through the ASTRIAGraph project, in which multiple data sources about near earth space objects are aggregated into a central knowledge system. The process involves multidisciplinary collaboration, resulting in the design of a data model as the backbone for both data curation and scientific inquiry. We demonstrate a) how data provenance information is needed to assess the uncertainty of the results of scientific inquiries involving multiple data sources, and b) that continuous curation of integrated datasets is facilitated when undertaken as integral to the research project. The approach provides flexibility to support expansion of scientific inquiries and data in the knowledge system, and allows for transparent and explainable results.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {235–242},
numpages = {8},
keywords = {space traffic management, data model, data curation, graph database, data integration, big data, knowledge system},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3340017.3340022,
author = {Wieczorkowski, Jundefineddrzej},
title = {Barriers to Using Open Government Data},
year = {2019},
isbn = {9781450362375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340017.3340022},
doi = {10.1145/3340017.3340022},
abstract = {The article describes the issues of Open Government Data (OGD) and problems with the use of such data. Good quality and proper publishing of OGD enable (apart from the control function) their business use. This affects the economic benefits. The author has identified the main problems of data publication based on Central Repositories for Public Information (CRPI) in Poland, the USA, the UK and Germany. The article focuses on the maturity of data formats, automated processing with Application Programming Interface (API), using the concept of Linked Open Data (LOD). The aim of the article is to identify barriers to the implementation of OGD-based solutions and to indicate recommendations to overcome these barriers. The research shows that the methods of sharing OGD differ significantly between countries despite common guidelines. The main problem is the use of unstructured data, unsuitable for the use of LOD.},
booktitle = {Proceedings of the 2019 3rd International Conference on E-Commerce, E-Business and E-Government},
pages = {15–20},
numpages = {6},
keywords = {Linked Open Data, LOD, Open Government Data, Big Data, OGD, CRPI, Linked Data, E-government, Open Data, Central Repository for Public Information},
location = {Lyon, France},
series = {ICEEG 2019}
}

@inproceedings{10.5555/2740769.2740814,
author = {Borgman, Christine L. and Darch, Peter T. and Sands, Ashley E. and Wallis, Jillian C. and Traweek, Sharon},
title = {The Ups and Downs of Knowledge Infrastructures in Science: Implications for Data Management},
year = {2014},
isbn = {9781479955695},
publisher = {IEEE Press},
abstract = {The promise of technology-enabled, data-intensive scholarship is predicated upon access to knowledge infrastructures that are not yet in place. Scientific data management requires expertise in the scientific domain and in organizing and retrieving complex research objects. The Knowledge Infrastructures project compares data management activities of four large, distributed, multidisciplinary scientific endeavors as they ramp their activities up or down; two are big science and two are small science. Research questions address digital library solutions, knowledge infrastructure concerns, issues specific to individual domains, and common problems across domains. Findings are based on interviews (n=113 to date), ethnography, and other analyses of these four cases, studied since 2002. Based on initial comparisons, we conclude that the roles of digital libraries in scientific data management often depend upon the scale of data, the scientific goals, and the temporal scale of the research projects being supported. Digital libraries serve immediate data management purposes in some projects and long-term stewardship in others. In small science projects, data management tools are selected, designed, and used by the same individuals. In the multi-decade time scale of some big science research, data management technologies, policies, and practices are designed for anticipated future uses and users. The need for library, archival, and digital library expertise is apparent throughout all four of these cases. Managing research data is a knowledge infrastructure problem beyond the scope of individual researchers or projects. The real challenges lie in designing digital libraries to assist in the capture, management, interpretation, use, reuse, and stewardship of research data.},
booktitle = {Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {257–266},
numpages = {10},
keywords = {data management, digital libraries, sensor networks, small science, knowledge infrastructures, astronomy, big data, biology, little science, big science},
location = {London, United Kingdom},
series = {JCDL '14}
}

@article{10.14778/3007263.3007320,
author = {Chu, Xu and Ilyas, Ihab F.},
title = {Qualitative Data Cleaning},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007320},
doi = {10.14778/3007263.3007320},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions.Data cleaning exercise often consist of two phases: error detection and error repairing. Error detection techniques can either be quantitative or qualitative; and error repairing is performed by applying data transformation scripts or by involving human experts, and sometimes both.In this tutorial, we discuss the main facets and directions in designing qualitative data cleaning techniques. We present a taxonomy of current qualitative error detection techniques, as well as a taxonomy of current data repairing techniques. We will also discuss proposals for tackling the challenges for cleaning "big data" in terms of scale and distribution.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1605–1608},
numpages = {4}
}

@article{10.1145/3301284,
author = {Silva, Thiago H. and Viana, Aline Carneiro and Benevenuto, Fabr\'{\i}cio and Villas, Leandro and Salles, Juliana and Loureiro, Antonio and Quercia, Daniele},
title = {Urban Computing Leveraging Location-Based Social Network Data: A Survey},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3301284},
doi = {10.1145/3301284},
abstract = {Urban computing is an emerging area of investigation in which researchers study cities using digital data. Location-Based Social Networks (LBSNs) generate one specific type of digital data that offers unprecedented geographic and temporal resolutions. We discuss fundamental concepts of urban computing leveraging LBSN data and present a survey of recent urban computing studies that make use of LBSN data. We also point out the opportunities and challenges that those studies open.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {17},
numpages = {39},
keywords = {big data, city dynamics, Urban computing, urban sensing, urban societies, location-based social networks, urban informatics}
}

@inproceedings{10.1145/3447548.3467129,
author = {Deng, Alex and Li, Yicheng and Lu, Jiannan and Ramamurthy, Vivek},
title = {On Post-Selection Inference in A/B Testing},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467129},
doi = {10.1145/3447548.3467129},
abstract = {When interpreting A/B tests, we typically focus only on the statistically significant results and take them by face value. This practice, termed post-selection inference in the statistical literature, may negatively affect both point estimation and uncertainty quantification, and therefore hinder trustworthy decision making in A/B testing. To address this issue, in this paper we explore two seemingly unrelated paths, one based on supervised machine learning and the other on empirical Bayes, and propose post-selection inferential approaches that combine the strengths of both. Through large-scale simulated and empirical examples, we demonstrate that our proposed methodologies stand out among other existing ones in both reducing post-selection biases and improving confidence interval coverage rates, and discuss how they can be conveniently adjusted to real-life scenarios.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2743–2752},
numpages = {10},
keywords = {randomization, winner's curse, post-selection inference, regression, bias correction, A/B testing, online metrics, machine learning, empirical Bayes, big data},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3410992.3411027,
author = {Kamel, Mohammed B. M. and Wallis, Kevin and Ligeti, Peter and Reich, Christoph},
title = {Distributed Data Validation Network in IoT: A Decentralized Validator Selection Model},
year = {2020},
isbn = {9781450387583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410992.3411027},
doi = {10.1145/3410992.3411027},
abstract = {The generated real-time data on the Internet of Things (IoT) and the ability to gather and manipulate them are positively affecting various fields. One of the main concerns in IoT is how to provide trustworthy data. The data validation network ensures that the generated data by data sources in the IoT are trustworthy. However, the existing data validation network depends on a centralized entity for the selection of data validators. In this paper, a decentralized validator selection model is proposed. The proposed model creates multiple clusters using the distributed hash table (DHT) technique. The selection process of data validators from different clusters in the model is done randomly in a decentralized scheme. It provides a global method of assignment, selection, and verification of the selected validators in the network.},
booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
articleno = {12},
numpages = {8},
keywords = {data validation, industrial internet of things, internet of things, data validation network, distributed hash table, big data, cluster-based data \^{A}\u{a}Validation},
location = {Malm\"{o}, Sweden},
series = {IoT '20}
}

@inproceedings{10.1145/3323878.3325806,
author = {Lehmberg, Oliver and Bizer, Christian},
title = {Profiling the Semantics of N-Ary Web Table Data},
year = {2019},
isbn = {9781450367660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323878.3325806},
doi = {10.1145/3323878.3325806},
abstract = {The Web contains millions of relational HTML tables, which cover a multitude of different, often very specific topics. This rich pool of data has motivated a growing body of research on methods that use web table data to extend local tables with additional attributes or add missing facts to knowledge bases. Nearly all existing approaches for these tasks build upon the assumption that web table data consists of binary relations, meaning that an attribute value depends on a single key attribute, and that the key attribute value is contained in the HTML table. Inspecting randomly chosen tables on the Web, however, quickly reveals that both assumptions are wrong for a large fraction of the tables. In order to better understand the potential of non-binary web table data for downstream applications, this papers analyses a corpus of 5 million web tables originating from 80 thousand different web sites with respect to how many web table attributes are non-binary, what composite keys are required to correctly interpret the semantics of the non-binary attributes, and whether the values of these keys are found in the table itself or need to be extracted from the page surrounding the table. The profiling of the corpus shows that at least 38% of the relations are non-binary. Recognizing these relations requires information from the title or the URL of the web page in 50% of the cases. We find that different websites use keys of varying length for the same dependent attribute, e.g. one cluster of websites presents employment numbers depending on time, another cluster presents them depending on time and profession. By identifying these clusters, we lay the foundation for selecting Web data sources according to the specificity of the keys that are used to determine specific attributes.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {5},
numpages = {6},
keywords = {key detection, n-ary relations, web tables, data profiling},
location = {Amsterdam, Netherlands},
series = {SBD '19}
}

@inproceedings{10.1145/3442555.3442579,
author = {Maziku, Hellen},
title = {Improved Data Accuracy Assessment Tool for Information Management Systems},
year = {2020},
isbn = {9781450388092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442555.3442579},
doi = {10.1145/3442555.3442579},
abstract = {Developing countries are increasingly taking advantage of the rapid advancement in ICT to replace paper-based operations with Information Management Systems (IMS) such as District Health Information Software (DHIS). While the adoption of IMS presents significant benefits, challenges exist in the quality of IMS data. Inaccurate, incomplete or redundant data in IMS has misled organisations into making incorrect decisions leading to customer dissatisfaction and high cost implications. There is an urgent need for IMS stakeholders to have mechanisms of assessing the quality of data prior to data analysis, data sharing or decision making. Data Accuracy Assessment Tool (DAAT) assesses and identifies errors in a pair of context related datasets. DAAT provides ability for Data Managers to easily compare datasets by choosing attributes of their interest from a pool of diverse attributes that define the data. Through reports and visualization, the tool reveals the accuracy of data in real time using metrics such as validity, completeness and duplication of data. DAAT is scalable since it can be integrated with any IMS such as DHIS. The tool has been tested using four years Voluntary Medical Male Circumcision (VMMC) program data from JHPIEGO's AIDSFree project in Tanzania.},
booktitle = {2020 the 6th International Conference on Communication and Information Processing},
pages = {148–152},
numpages = {5},
keywords = {Information Management Systems, Human Centered Design, Data Quality Assessment, Accuracy},
location = {Tokyo, Japan},
series = {ICCIP 2020}
}

@inproceedings{10.1145/2457317.2457382,
author = {Jiang, Yu and Deng, Dong and Wang, Jiannan and Li, Guoliang and Feng, Jianhua},
title = {Efficient Parallel Partition-Based Algorithms for Similarity Search and Join with Edit Distance Constraints},
year = {2013},
isbn = {9781450315999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457317.2457382},
doi = {10.1145/2457317.2457382},
abstract = {The quantity of data in real-world applications is growing significantly while the data quality is still a big problem. Similarity search and similarity join are two important operations to address the poor data quality problem. Although many similarity search and join algorithms have been proposed, they did not utilize the abilities of modern hardware with multi-core processors. It calls for new parallel algorithms to enable multi-core processors to meet the high performance requirement of similarity search and join on big data. To this end, in this paper we propose parallel algorithms to support efficient similarity search and join with edit-distance constraints. We adopt the partition-based framework and extend it to support parallel similarity search and join on multi-core processors. We also develop two novel pruning techniques. We have implemented our algorithms and the experimental results on two real datasets show that our parallel algorithms achieve high performance and obtain good speedup.},
booktitle = {Proceedings of the Joint EDBT/ICDT 2013 Workshops},
pages = {341–348},
numpages = {8},
keywords = {similarity join, similarity search, content filter, parallel algorithms},
location = {Genoa, Italy},
series = {EDBT '13}
}

@inproceedings{10.1109/CCGrid.2015.24,
author = {Yan, Cairong and Song, Yalong and Wang, Jian and Guo, Wenjing},
title = {Eliminating the Redundancy in MapReduce-Based Entity Resolution},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.24},
doi = {10.1109/CCGrid.2015.24},
abstract = {Entity resolution is the basic operation of data quality management, and the key step to find the value of data. The parallel data processing framework based on MapReduce can deal with the challenge brought by big data. However, there exist two important issues, avoiding redundant pairs led by the multi-pass blocking method and optimizing candidate pairs based on the transitive relations of similarity. In this paper, we propose a multi-signature based parallel entity resolution method, called multi-sig-er, which supports unstructured data and structured data. Two redundancy elimination strategies are adopted to prune the candidate pairs and reduce the number of similarity computation without affecting the resolution accuracy. Experimental results on real-world datasets show that our method tends to handle large datasets and it is more suitable for complex similarity computation than simple object matching.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {1233–1236},
numpages = {4},
keywords = {MapReduce, entity resolution, blocking, redundancy elimination},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3340531.3414073,
author = {Bowles, Juliana and Broccia, Giovanna and Nanni, Mirco},
title = {DataMod2020: 9th International Symposium "From Data to Models and Back"},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3414073},
doi = {10.1145/3340531.3414073},
abstract = {DataMod 2020 aims to bring together practitioners and researchers from academia, industry and research institutions interested in the combined application of computational modelling methods with data-driven techniques from the areas of knowledge management, data mining and machine learning. Modelling methodologies of interest include automata, agents, Petri nets, process algebras and rewriting systems. Application domains include social systems, ecology, biology, medicine, smart cities, governance, security, education, software engineering, and any other field that deals with complex systems and large amounts of data. Papers can present research results in any of the themes of interest for the symposium as well as application experiences, tools and promising preliminary ideas. Papers dealing with synergistic approaches that integrate modelling and knowledge management/discovery or that exploit knowledge management/discovery to develop/syntesise system models are especially welcome.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3531–3532},
numpages = {2},
keywords = {processing mining, machine learning, big data analytics, text mining, process calculi, formal methods, deep learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3397536.3422232,
author = {Soliman, Aiman and Terstriep, Jeffrey},
title = {Leveraging Geospatial Data Gateways to Support the Operational Application of Deep Learning Models: Vision Paper},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422232},
doi = {10.1145/3397536.3422232},
abstract = {Geospatial data providers have adopted a variety of science gateways as the primary method for accessing remote geospatial data. Early systems provided little more than a simple file transfer mechanism but over the past decade, advanced features were incorporated to allow users to retrieve data seamlessly without concern for native file formats, data resolution, or even spatial projections. However, the recent growth in Deep Learning models in the geospatial domains has exposed additional requirements for accessing geospatial repositories. In this paper we discussed the major data accessibility challenges faced by the Deep Learning community namely: (1) reproducibility of data preprocessing workflows, (2) optimizing data transfer between gateways and computational environments, and (3) minimizing local storage requirements using on-the-fly augmentation. In this paper, we present our vision of spatial data generators to act as middleware between geospatial data gateways and Deep Learning models. We propose advanced features for spatial data generators and describe how they could satisfy the data accessibility requirements of the geospatial Deep Learning community. Lastly, we argue that satisfying these data accessibility requirements will not only enhance the reproducibility of Deep Learning workflows and speed their development but will also improve the quality of training and prediction of operational Deep Learning models.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {593–596},
numpages = {4},
keywords = {Scientific Reproducibility, Remote Sensing, Image Preprocessing, Geospatial Data Gateway, Geospatial Big Data, Deep Learning},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1145/2806416.2806418,
author = {Zhou, Xiaofang and Zheng, Kai and Jueng, Hoyoung and Xu, Jiajie and Sadiq, Shazia},
title = {Making Sense of Spatial Trajectories},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806418},
doi = {10.1145/2806416.2806418},
abstract = {Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. Daily increases of real-time trajectory data have also been phenomenal in recent years. More and more new applications have emerged to derive business values from both trajectory data warehouses and real-time trajectory data. Due to their very large volumes, their nature of streaming, their highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data becomes one of the crucial areas for big data analytics. In this paper we will present a review of the extensive work in spatiotemporal data management and trajectory mining, and discuss new challenges and new opportunities in the context of new applications, focusing on recent advances in trajectory data management and trajectory mining from their foundations to high performance processing with modern computing infrastructure.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {671–672},
numpages = {2},
keywords = {spatiotemporal database, trajectory mining, trajectory data management},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@article{10.1145/3397462,
author = {Caruccio, Loredana and Cirillo, Stefano},
title = {Incremental Discovery of Imprecise Functional Dependencies},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3397462},
doi = {10.1145/3397462},
abstract = {Functional dependencies (fds) are one of the metadata used to assess data quality and to perform data cleaning operations. However, to pursue robustness with respect to data errors, it has been necessary to devise imprecise versions of functional dependencies, yielding relaxed functional dependencies (rfds). Among them, there exists the class of rfds relaxing on the extent, i.e., those admitting the possibility that an fd holds on a subset of data. In the literature, several algorithms to automatically discover rfds from big data collections have been defined. They achieve good performances with respect to the inherent problem complexity. However, most of them are capable of discovering rfds only by batch processing the entire dataset. This is not suitable in the era of big data, where the size of a database instance can grow with high-velocity, and the insertion of new data can invalidate previously holding rfds. Thus, it is necessary to devise incremental discovery algorithms capable of updating the set of holding rfds upon data insertions, without processing the entire dataset. To this end, in this article we propose an incremental discovery algorithm for rfds relaxing on the extent. It manages the validation of candidate rfds and the generation of possibly new rfd candidates upon the insertion of the new tuples, while limiting the size of the overall search space. Experimental results show that the proposed algorithm achieves extremely good performances on real-world datasets.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {19},
numpages = {25},
keywords = {tuple insertions, incremental discovery, parallelism, Functional dependency, discovery algorithm}
}

@article{10.1145/3328747,
author = {Colborne, Adrienne and Smit, Michael},
title = {Characterizing Disinformation Risk to Open Data in the Post-Truth Era},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3328747},
doi = {10.1145/3328747},
abstract = {Curated, labeled, high-quality data is a valuable commodity for tasks such as business analytics and machine learning. Open data is a common source of such data—for example, retail analytics draws on open demographic data, and weather forecast systems draw on open atmospheric and ocean data. Open data is released openly by governments to achieve various objectives, such as transparency, informing citizen engagement, or supporting private enterprise. Critical examination of ongoing social changes, including the post-truth phenomenon, suggests the quality, integrity, and authenticity of open data may be at risk. We introduce this risk through various lenses, describe some of the types of risk we expect using a threat model approach, identify approaches to mitigate each risk, and present real-world examples of cases where the risk has already caused harm. As an initial assessment of awareness of this disinformation risk, we compare our analysis to perspectives captured during open data stakeholder consultations in Canada.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {13},
numpages = {13},
keywords = {post-truth, Open data, fake news, risk mitigation, data quality assurance, risk identification}
}

@article{10.1145/3316416.3316425,
author = {Singh, Lisa and Deshpande, Amol and Zhou, Wenchao and Banerjee, Arindam and Bowers, Alex and Friedler, Sorelle and Jagadish, H.V. and Karypis, George and Obradovic, Zoran and Vullikanti, Anil and Zuo, Wangda},
title = {NSF BIGDATA PI Meeting - Domain-Specific Research Directions and Data Sets},
year = {2019},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3316416.3316425},
doi = {10.1145/3316416.3316425},
abstract = {In March 2017, PIs and co-PIs funded through the NSF BIGDATA program were brought together along with selected industry and government invitees to discuss current research, identify current challenges, discuss promising future directions, foster new collaborations, and share accomplishments, at BDPI-2017. Given that two recent NITRD [2] and NSF [1] meeting reports contained a set of recommendations, grand challenges, and high impact priorities for Big Data, the organizers of this meeting shifted the focus of the breakout sessions to discuss problems and available data sets that exist in five application domains - policy, health, education, economy &amp; finance, and environment &amp; energy. These domains were selected based on a survey of the PIs/co-PIs and should not be interpreted as being more important than others. Slides that were presented by the different breakout group leaders are available at https://www.bi.vt.edu/ nsf-big-data/. We hope this report will serve as a blueprint for promising big data research in five application domains.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {32–35},
numpages = {4}
}

@inproceedings{10.1145/2567574.2567582,
author = {Piety, Philip J. and Hickey, Daniel T. and Bishop, M. J.},
title = {Educational Data Sciences: Framing Emergent Practices for Analytics of Learning, Organizations, and Systems},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567582},
doi = {10.1145/2567574.2567582},
abstract = {In this paper, we develop a conceptual framework for organizing emerging analytic activities involving educational data that can fall under broad and often loosely defined categories, including Academic/Institutional Analytics, Learning Analytics/Educational Data Mining, Learner Analytics/Personalization, and Systemic Instructional Improvement. While our approach is substantially informed by both higher education and K-12 settings, this framework is developed to apply across all educational contexts where digital data are used to inform learners and the management of learning. Although we can identify movements that are relatively independent of each other today, we believe they will in all cases expand from their current margins to encompass larger domains and increasingly overlap. The growth in these analytic activities leads to the need to find ways to synthesize understandings, find common language, and develop frames of reference to help these movements develop into a field.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {193–202},
numpages = {10},
keywords = {learning analytics, big data, analytic approaches, theories and theoretical concepts for understanding learning, learner analytics, educational data mining, methods, tools for sense-making in learning analytics, educational data science, data-driven decisions},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2513549.2513552,
author = {Wei, Qin},
title = {Information Fusion in Taxonomic Descriptions},
year = {2013},
isbn = {9781450324151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513549.2513552},
doi = {10.1145/2513549.2513552},
abstract = {Providing a single access point to an information system from multiple documents is helpful for biodiversity researchers as it is true in many fields. It not only saves the time for going back and forth from different sources but also provides the opportunity to generate new information out of the complementary information in different sources and levels of description. This paper investigates the potential of information fusion techniques in biodiversity area since the researchers in this domain desperately need information from different sources to verify their decision. In another sense, there are massive amounts of collections in this area. It is not easy or even possible for the researcher to manually collect information from different places. The proposed system contains 4 steps: Text segmentation and Taxonomic Name Identification, Organ-level and Sub-organ level Information Extraction, Relationship Identification, and Information fusion. Information fusion is based on the seven out of the twenty-four relationships in CST (Cross-document Sentence Theory). We argue that this kind of information fusion system might not only save the researchers the time for going back and forth from different sources but also provides the opportunity to generate new information out of the complementary information in different sources and levels.},
booktitle = {Proceedings of the 2013 International Workshop on Mining Unstructured Big Data Using Natural Language Processing},
pages = {11–14},
numpages = {4},
keywords = {natural language processing, information fusion},
location = {San Francisco, California, USA},
series = {UnstructureNLP '13}
}

@inproceedings{10.1145/3098593.3098601,
author = {Fiadino, Pierdomenico and Ponce-Lopez, Victor and Antonio, Juan and Torrent-Moreno, Marc and D'Alconzo, Alessandro},
title = {Call Detail Records for Human Mobility Studies: Taking Stock of the Situation in the "Always Connected Era"},
year = {2017},
isbn = {9781450350549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098593.3098601},
doi = {10.1145/3098593.3098601},
abstract = {The exploitation of cellular network data for studying human mobility has been a popular research topic in the last decade. Indeed, mobile terminals could be considered ubiquitous sensors that allow the observation of human movements on large scale without the need of relying on non-scalable techniques, such as surveys, or dedicated and expensive monitoring infrastructures. In particular, Call Detail Records (CDRs), collected by operators for billing purposes, have been extensively employed due to their rather large availability, compared to other types of cellular data (e.g., signaling). Despite the interest aroused around this topic, the research community has generally agreed about the scarcity of information provided by CDRs: the position of mobile terminals is logged when some kind of activity (calls, SMS, data connections) occurs, which translates in a picture of mobility somehow biased by the activity degree of users. By studying two datasets collected by a Nation-wide operator in 2014 and 2016, we show that the situation has drastically changed in terms of data volume and quality. The increase of flat data plans and the higher penetration of "always connected" terminals have driven up the number of recorded CDRs, providing higher temporal accuracy for users' locations.},
booktitle = {Proceedings of the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
pages = {43–48},
numpages = {6},
keywords = {human mobility, mobile networks, call detail records},
location = {Los Angeles, CA, USA},
series = {Big-DAMA '17}
}

@article{10.14778/3415478.3415562,
author = {Whang, Steven Euijong and Lee, Jae-Gil},
title = {Data Collection and Quality Challenges for Deep Learning},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415562},
doi = {10.14778/3415478.3415562},
abstract = {Software 2.0 refers to the fundamental shift in software engineering where using machine learning becomes the new norm in software with the availability of big data and computing infrastructure. As a result, many software engineering practices need to be rethought from scratch where data becomes a first-class citizen, on par with code. It is well known that 80--90% of the time for machine learning development is spent on data preparation. Also, even the best machine learning algorithms cannot perform well without good data or at least handling biased and dirty data during model training. In this tutorial, we focus on data collection and quality challenges that frequently occur in deep learning applications. Compared to traditional machine learning, there is less need for feature engineering, but more need for significant amounts of data. We thus go through state-of-the-art data collection techniques for machine learning. Then, we cover data validation and cleaning techniques for improving data quality. Even if the data is still problematic, hope is not lost, and we cover fair and robust training techniques for handling data bias and errors. We believe that the data management community is well poised to lead the research in these directions. The presenters have extensive experience in developing machine learning platforms and publishing papers in top-tier database, data mining, and machine learning venues.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3429–3432},
numpages = {4}
}

@inproceedings{10.1109/WI-IAT.2014.139,
author = {Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu},
title = {Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.139},
doi = {10.1109/WI-IAT.2014.139},
abstract = {In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 02},
pages = {495–502},
numpages = {8},
keywords = {privacy preserving data mining, data publishing, algorithm},
series = {WI-IAT '14}
}

@inproceedings{10.1145/2335484.2335488,
author = {Artikis, Alexander and Etzion, Opher and Feldman, Zohar and Fournier, Fabiana},
title = {Event Processing under Uncertainty},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335488},
doi = {10.1145/2335484.2335488},
abstract = {Big data is recognized as one of the three technology trends at the leading edge a CEO cannot afford to overlook in 2012. Big data is characterized by volume, velocity, variety and veracity ("data in doubt"). As big data applications, many of the emerging event processing applications must process events that arrive from sources such as sensors and social media, which have inherent uncertainties associated with them. Consider, for example, the possibility of incomplete data streams and streams including inaccurate data. In this tutorial we classify the different types of uncertainty found in event processing applications and discuss the implications on event representation and reasoning. An area of research in which uncertainty has been studied is Artificial Intelligence. We discuss, therefore, the main Artificial Intelligence-based event processing systems that support probabilistic reasoning. The presented approaches are illustrated using an example concerning crime detection.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {32–43},
numpages = {12},
keywords = {uncertainty, event recognition, event processing, pattern matching, artificial intelligence},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1145/3442381.3450066,
author = {Fang, Minghong and Sun, Minghao and Li, Qi and Gong, Neil Zhenqiang and Tian, Jin and Liu, Jia},
title = {Data Poisoning Attacks and Defenses to Crowdsourcing Systems},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450066},
doi = {10.1145/3442381.3450066},
abstract = {A key challenge of big data analytics is how to collect a large volume of (labeled) data. Crowdsourcing aims to address this challenge via aggregating and estimating high-quality data (e.g., sentiment label for text) from pervasive clients/users. Existing studies on crowdsourcing focus on designing new methods to improve the aggregated data quality from unreliable/noisy clients. However, the security aspects of such crowdsourcing systems remain under-explored to date. We aim to bridge this gap in this work. Specifically, we show that crowdsourcing is vulnerable to data poisoning attacks, in which malicious clients provide carefully crafted data to corrupt the aggregated data. We formulate our proposed data poisoning attacks as an optimization problem that maximizes the error of the aggregated data. Our evaluation results on one synthetic and two real-world benchmark datasets demonstrate that the proposed attacks can substantially increase the estimation errors of the aggregated data. We also propose two defenses to reduce the impact of malicious clients. Our empirical results show that the proposed defenses can substantially reduce the estimation errors of the data poisoning attacks.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {969–980},
numpages = {12},
keywords = {truth discovery, crowdsourcing, Data poisoning attacks},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3520084.3520099,
author = {Li, Yunze and Wu, Yuxuan and Tang, Ruisen},
title = {Data Aggregation and Anomaly Detection System for Isomerism and Heterogeneous Data},
year = {2022},
isbn = {9781450395519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520084.3520099},
doi = {10.1145/3520084.3520099},
abstract = {With the development of big data technology, data accessed by big data platforms maintain the features of mass, isomerism, heterogeneous, and streaming. Therefore, how to access the varied data sources of isomerism and heterogeneous data and how to process and analyze the data become the current challenges. In this paper, we design and implement a data aggregation and anomaly detection system for isomerism and heterogeneous data. The system proposes a novel isomerism and heterogeneous data access sub-system. The sub-system applies improved Avro as the unified data description format and presents different storage algorithms for data serialization to raise the data adaption efficiency. The system adopts Kafka as the message middleware for data aggregation and distribution. Also, we design the anomaly detection and alarming sub-system for detecting the anomalies of streaming data on time and notifying the users. The data aggregation and anomaly detection system has passed all the tests and applied in small and medium-sized enterprises.},
booktitle = {2022 The 5th International Conference on Software Engineering and Information Management (ICSIM)},
pages = {95–99},
numpages = {5},
keywords = {Kafka, anomaly detection, isomerism and heterogeneous data, serialization and deserialization},
location = {Yokohama, Japan},
series = {ICSIM 2022}
}

@inproceedings{10.1145/3208352.3208357,
author = {Madkour, Amgad and Aref, Walid G. and Prabhakar, Sunil and Ali, Mohamed and Bykau, Siarhei},
title = {TrueWeb: A Proposal for Scalable Semantically-Guided Data Management and Truth Finding in Heterogeneous Web Sources},
year = {2018},
isbn = {9781450357791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208352.3208357},
doi = {10.1145/3208352.3208357},
abstract = {We envision a responsible web environment, termed TrueWeb, where a user should be able to find out whether any sentence he or she encounters in the web is true or false. The user should be able to track the provenance of any sentence or paragraph in the web. The target of TrueWeb is to compose factual knowledge from Internet resources about any subject of interest and present the collected knowledge in chronological order and distribute facts spatially and temporally as well as assign some belief factor for each fact. Another important target of TrueWeb is to be able to identify whether a statement in the Internet is true or false. The aim is to create an Internet infrastructure that, for each piece of published information, will be able to identify the truthfulness (or the degree of truthfulness) of that piece of information.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {5},
numpages = {6},
keywords = {Linked Data, RDF, Truth detection, Data Management},
location = {Houston, TX, USA},
series = {SBD'18}
}

@inproceedings{10.1145/3495018.3495398,
author = {Zhang, Mingjie and Sheng, Yan and Tian, Nuo and Liu, Wei and Wang, Hui and Zhu, Longzhu and Xu, Qing},
title = {Exploring and Analyzing Data Mining Algorithm Technology in Internet Customer Ranking},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495398},
doi = {10.1145/3495018.3495398},
abstract = {Based on the characteristics of online customers, such as user characteristics, interaction behavior, frequency of visits and business queries, this paper uses big data analysis mining algorithm to conduct exploratory analysis on each business data, and builds a weight division model (Entropy Value Method) to achieve online customer ranking.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1352–1360},
numpages = {9},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/2910896.2926735,
author = {Fox, Edward A. and Xie, Zhiwu and Klein, Martin},
title = {WADL 2016: Third International Workshop on Web Archiving and Digital Libraries},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2926735},
doi = {10.1145/2910896.2926735},
abstract = {This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {293–294},
numpages = {2},
keywords = {web archiving, internet archive},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.5555/3200334.3200410,
author = {Fox, Edward A. and Xie, Zhiwu and Klein, Martin},
title = {Web Archiving and Digital Libraries (WADL)},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {352–353},
numpages = {2},
keywords = {web archiving, internet archive},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.1145/3482632.3482675,
author = {Sun, Wen},
title = {Cloud Service Context and Feedback Fusion of Product Design Creative Demand Perception Technology},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482675},
doi = {10.1145/3482632.3482675},
abstract = {After experiencing the stages of document delivery service, information service and knowledge service, the traditional product design creative demand perception technology has gradually transformed to the cloud service stage driven by new technologies such as big data and cloud computing. With the advent of the era of big data and artificial intelligence, multi-source heterogeneous and massive data resources have specific fusion characteristics and application trends. The generation of new artificial intelligence technologies and methods is to respond to the above characteristics and trends. Multi-source heterogeneous resources and vast amounts of data driven product design requirements perception from the user requirements perception, perception technology content and creative product design requirements capturing perception technology scenario-based push these three core function implementation requirements perception technology cloud service mode, implement the product design requirements perception technology innovation process. Of this study was to study the cloud service situation and feedback the fusion of product design requirements perception technology, cloud service components analysis demand perception technology and its features, with case studies to explore data driven era product design demand perception technology to the cloud service model transformation of ideas, requirements for perception technology transformation provides scientific theory and practice.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {207–213},
numpages = {7},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3423603.3424004,
author = {Darmont, J\'{e}r\^{o}me and Favre, C\'{e}cile and Loudcher, Sabine and No\^{u}s, Camille},
title = {Data Lakes for Digital Humanities},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424004},
doi = {10.1145/3423603.3424004},
abstract = {Traditional data in Digital Humanities projects bear various formats (structured, semi-structured, textual) and need substantial transformations (encoding and tagging, stemming, lemmatization, etc.) to be managed and analyzed. To fully master this process, we propose the use of data lakes as a solution to data siloing and big data variety problems. We describe data lake projects we currently run in close collaboration with researchers in humanities and social sciences and discuss the lessons learned running these projects.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {6},
numpages = {4},
keywords = {metadata, digital humanities, data lakes},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/3093241.3093268,
author = {Smith, Jeffrey and Rege, Manjeet},
title = {The Data Warehousing (R) Evolution: Where's It Headed Next?},
year = {2017},
isbn = {9781450352413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093241.3093268},
doi = {10.1145/3093241.3093268},
abstract = {This paper provides an overview of the history and current state of data warehousing and corporate analytics. It begins with a quick review of the history of the data warehouse and then does a deeper dive into subsets of this space including data integration, the DBMS, business intelligence and analytics, advanced analytics, and information stewardship. It finishes with a quick review of some of the leading trends in data warehousing including Big Data and the Logical Data Warehouse, Hybrid Transaction Analytical Processing and In-Memory Computing.},
booktitle = {Proceedings of the International Conference on Compute and Data Analysis},
pages = {104–108},
numpages = {5},
keywords = {ETL, business, Data, intelligence, warehouse},
location = {Lakeland, FL, USA},
series = {ICCDA '17}
}

@inproceedings{10.1145/3144826.3145387,
author = {Moreira, Fernando and Gon\c{c}alves, Ramiro and Martins, Jos\'{e} and Branco, Frederico and Au-Yong-Oliveira, Manuel},
title = {Learning Analytics as a Core Component for Higher Education Disruption: Governance Stakeholder},
year = {2017},
isbn = {9781450353861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144826.3145387},
doi = {10.1145/3144826.3145387},
abstract = {Higher education institutions are at this stage, on the one hand, faced with challenges never seen before and, on the other hand, their action is moving very rapidly into digital learning spaces. These challenges are increasingly complex because of the global competition for resources, students and teachers. In addition, the amount of data produced inside and outside higher education institutions has grown exponentially, so more and more institutions are exploring the potential of Big Data to meet these challenges. In this context, higher education institutions and key stakeholders (students, teachers, and governance) can derive multiple benefits from learning analytics using different data analysis strategies to produce summative, real-time and predictive information and recommendations. However, it may be questioned whether institutions, academic administrative staff as well as including those with responsibility for governance, are prepared for learning analytics? As a response to the question raised in this paper is presented an extension of a disruptive conceptual approach to higher education, using information gathered by IoT and based on Big Data &amp; Cloud Computing and Learning Analytics analysis tools, with the main focus on the stakeholder governance.},
booktitle = {Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality},
articleno = {37},
numpages = {8},
keywords = {Learning Analytics, Governance, Higher Education Institutions, Disruption},
location = {C\'{a}diz, Spain},
series = {TEEM 2017}
}

@inproceedings{10.1145/3106426.3106436,
author = {Saberi, Morteza and Hussain, Omar K. and Chang, Elizabeth},
title = {An Online Statistical Quality Control Framework for Performance Management in Crowdsourcing},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106436},
doi = {10.1145/3106426.3106436},
abstract = {The big data research topic has grown rapidly for the past decade due to the advent of the "data deluge". Recent advancements in the literature leverage human computing power known as crowdsourcing to manage and harness big data for various applications. However, human involvement in the completion of crowdsourcing tasks is an error-prone process that affects the overall performance of the crowd. Thus, controlling the quality of workers is an essential step for crowdsourcing systems, which due to unavailability of ground-truth data for any task at hand becomes increasingly challenging. To propose a solution to this problem, in this study, we propose OSQC (Online Statistical Quality Control Framework) for managing the performance of workers in crowdsourcing. OSQC ascertains the worker's performance by using a statistical model and then leverages the traditional statistical control techniques to decide whether to retain a worker for crowdsourcing or to evict him. We evaluate our proposed framework on a real dataset and demonstrate how OSQC assists crowdsourcing to maintain its accuracy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {476–482},
numpages = {7},
keywords = {crowd workers, crowdsourcing management, statistical quality control, multiple choice HIT},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.5555/2602339.2602400,
author = {Petrou, Lambros and Larkou, George and Laoudias, Christos and Zeinalipour-Yazti, Demetrios and Panayiotou, Christos G.},
title = {Demonstration Abstract: Crowdsourced Indoor Localization and Navigation with Anyplace},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {In this demonstration paper, we present the Anyplace system that relies on the abundance of sensory data on smartphones (e.g., WiFi signal strength and inertial measurements) to deliver reliable indoor geolocation information. Our system features two highly desirable properties, namely crowdsourcing and scalability. Anyplace implements a set of crowdsourcing-supportive mechanisms to handle the enormous amount of crowdsensed data, filter incorrect user contributions and exploit WiFi data from heterogeneous mobile devices. Moreover, Anyplace follows a big-data architecture for efficient and scalable storage and retrieval of localization and mapping data.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {331–332},
numpages = {2},
keywords = {crowdsourcing, navigation, indoor localization},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3183713.3183753,
author = {Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao},
title = {Catching Numeric Inconsistencies in Graphs},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183753},
doi = {10.1145/3183713.3183753},
abstract = {Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we propose to extend graph functional dependencies with linear arithmetic expressions and comparison predicates, referred to as NGDs. We study fundamental problems for NGDs. We show that their satisfiability, implication and validation problems are Σ 2 p-complete, ¶II2 p-complete and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity.To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs, in response to updates Δ G to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in Δ G instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {381–393},
numpages = {13},
keywords = {numeric errors, graph dependencies, incremental validation},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3500931.3501016,
author = {Gao, Mengke and Zhang, Yan and Gao, Yue},
title = {Research Progress of User Portrait Technology in Medical Field},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3501016},
doi = {10.1145/3500931.3501016},
abstract = {In recent years, due to the rise of big data mining and intelligent recommendation, user portrait technology has gradually become a hot topic. As a new data analysis method, user portrait technology aims to mine user characteristics from a large number of user behavior data, and complete the user information panorama of the monomer or group through information mining, so as to prepare for the realization of precision service and personalized recommendation in various industries. It has been widely used in personalized recommendation and precision recommendation, personalized service and intelligent service, group characteristics analysis, prediction analysis and auxiliary decision-making. User portrait technology in the domestic research started late, currently in e-commerce, commercial precision marketing, book recommendation management, network personalized search, video entertainment and other fields of application is relatively mature. However, the application of user portrait technology in the field of medical and health is still in the preliminary exploration stage, and the combination of medical big data and user portrait technology can vividly depict the portraits of patients, doctors, residents and other different groups in promoting health and disease prevention. According to the different characteristics extracted, it can provide reference for meeting the needs of patients and achieving precision medicine. Therefore, this study reviews the concept, elements, implementation process and application of user portrait technology in the medical field, and provides reference for the subsequent application research of user portrait technology in the medical field},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {500–504},
numpages = {5},
keywords = {User portrait, Medical treatment, Review},
location = {Beijing, China},
series = {ISAIMS 2021}
}

@inproceedings{10.1145/3230833.3233288,
author = {Jirovsk\'{y}, V\'{a}clav and Pastorek, Andrej and M\"{u}hlh\"{a}user, Max and Tundis, Andrea},
title = {Cybercrime and Organized Crime},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233288},
doi = {10.1145/3230833.3233288},
abstract = {The way of live in the modern society has changed radically over the past few decades. In particular, thanks to the strong use of information technology, many activities have moved from the real world to the digital world. This has obviously introduced advantages in terms of data management and communication efficiency. Nevertheless, it has given also to the criminals the possibility to move into cybernetic space and, as a consequence, to exploit all the technological advantages available for carrying out their activities. In this context the paper provide an overview on the cybercrime and organized crime by focusing on the concept of crime as a service as well as the main issues related to big data by highlighting the social aspects.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {61},
numpages = {5},
keywords = {Cyber-security, Data Security, Privacy, Cyber-crime},
location = {Hamburg, Germany},
series = {ARES 2018}
}

@inproceedings{10.1145/3079856.3080241,
author = {Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung},
title = {APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080241},
doi = {10.1145/3079856.3080241},
abstract = {The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {666–677},
numpages = {12},
keywords = {Approximate Computing, Data Compression, Networks-On-Chip},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1145/3140659.3080241,
author = {Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung},
title = {APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080241},
doi = {10.1145/3140659.3080241},
abstract = {The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {666–677},
numpages = {12},
keywords = {Approximate Computing, Networks-On-Chip, Data Compression}
}

@inproceedings{10.1145/3340531.3418506,
author = {Huang, Ruihong},
title = {Approximate Event Pattern Matching over Heterogeneous and Dirty Sources},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3418506},
doi = {10.1145/3340531.3418506},
abstract = {Pattern matching is an important task in the field of Complex Event Processing (CEP). However, exact event pattern matching methods could suffer from low hit rate and loss for meaningful events identification due to the heterogeneous and dirty sources in the big data era. Since both events and patterns could be imprecise, the actual event trace may have different event names as well as structures from the pre-defined pattern. The low-quality data even intensifies the difficulty of matching. In this work, we propose to learn embedding representations for patterns and event traces separately and calculate their similarity as the scores for approximate matching.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3237–3240},
numpages = {4},
keywords = {cep, low-quality data, approximate match, heterogeneous source, dirty source, complex event processing},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/2851581.2892379,
author = {Verma, Nitya and Voida, Amy},
title = {Mythologies of Business Intelligence},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892379},
doi = {10.1145/2851581.2892379},
abstract = {We present results from a case study of the use of business intelligence (BI) systems in a human services organization. In their organizational trajectory towards a "culture of data," our informants perceived four values associated with BI: data-driven, predictive and proactive, shared accountability, and inquisitive. Each value corresponds to a mythology of big data and BI. For each, we highlight the ways in which the enactment of the mythology is problematized by disconnects between aggregate and drill-down views of data that often impede the desired actionability. Our findings contribute initial empirical evidence of the ways in which the epistemological biases of BI systems influence organizations. We suggest design implications for better enabling data-driven decision making.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2341–2347},
numpages = {7},
keywords = {values, mythology, business intelligence, analytics},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

