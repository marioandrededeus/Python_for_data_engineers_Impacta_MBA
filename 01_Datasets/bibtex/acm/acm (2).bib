@inproceedings{10.1145/3331453.3361308,
author = {Jing, Furong and Cao, Yongsheng and Fang, Wei and Chen, Yanqing},
title = {Construction and Implementation of Big Data Framework for Crop Germplasm Resources},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3361308},
doi = {10.1145/3331453.3361308},
abstract = {Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {27},
numpages = {7},
keywords = {Crop germplasm resources, Data analysis, Big data architecture, Data management},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3510003.3510619,
author = {Gote, Christoph and Mavrodiev, Pavlin and Schweitzer, Frank and Scholtes, Ingo},
title = {Big Data = Big Insights? Operationalising Brooks' Law in a Massive GitHub Data Set},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510619},
doi = {10.1145/3510003.3510619},
abstract = {Massive data from software repositories and collaboration tools are widely used to study social aspects in software development. One question that several recent works have addressed is how a software project's size and structure influence team productivity, a question famously considered in Brooks' law. Recent studies using massive repository data suggest that developers in larger teams tend to be less productive than smaller teams. Despite using similar methods and data, other studies argue for a positive linear or even super-linear relationship between team size and productivity, thus contesting the view of software economics that software projects are diseconomies of scale.In our work, we study challenges that can explain the disagreement between recent studies of developer productivity in massive repository data. We further provide, to the best of our knowledge, the largest, curated corpus of GitHub projects tailored to investigate the influence of team size and collaboration patterns on individual and collective productivity. Our work contributes to the ongoing discussion on the choice of productivity metrics in the operationalisation of hypotheses about determinants of successful software projects. It further highlights general pitfalls in big data analysis and shows that the use of bigger data sets does not automatically lead to more reliable insights.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {262–273},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3134271.3134296,
author = {Peng, Michael Yao-Ping and Tuan, Sheng-Hwa and Liu, Feng-Chi},
title = {Establishment of Business Intelligence and Big Data Analysis for Higher Education},
year = {2017},
isbn = {9781450352765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134271.3134296},
doi = {10.1145/3134271.3134296},
abstract = {The applications on business intelligence and big data analysis to extract useful information are getting more mature, but the development and operations in higher education institutions are still be lack. This study aims to explore how HEIs employ business intelligence to analysis and mining student learning and HEIs' operation data from database. The outcomes are benefit for universities to support the management of decision-making.},
booktitle = {Proceedings of the International Conference on Business and Information Management},
pages = {121–125},
numpages = {5},
keywords = {Database, Business Intelligence, Institutional Research, Big data},
location = {Bei Jing, China},
series = {ICBIM 2017}
}

@inproceedings{10.1145/3349341.3349371,
author = {Li, Jiale and Liao, Shunbao},
title = {Quality Control Framework of Big Data for Early Warning of Agricultural Meteorological Disasters},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349371},
doi = {10.1145/3349341.3349371},
abstract = {Agricultural meteorological disasters, including floods, droughts, dry hot winds, low temperature chills, typhoons, hail and continuous rain, can lead to significant reduction in agricultural output. Big data platform for early warning of agricultural meteorological disaster is the basis of business operation system for early warning of agricultural meteorological disasters, and the data quality is an important guarantee for success of the early warning. Quality control of big data for early warning of agricultural meteorological disaster involves names of data sets, metadata, data documents and content of data sets. The quality control for contents of data sets is divided into quality control of attribute data and that of spatial data, and quality control of spatial data is divided into quality control of vector data and that of raster data. Methods for data quality control are divided into fully automatic, semi-automatic and full manual control methods.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {74–78},
numpages = {5},
keywords = {big data, agro-meteorological disasters, early warning, framework, quality control},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3366030.3366044,
author = {Cuzzocrea, Alfredo},
title = {Big Data Management and Analytics in Intelligent Smart Environments: State-of-the-Art Analysis and Future Research Directions},
year = {2019},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366044},
doi = {10.1145/3366030.3366044},
abstract = {This paper focuses on big data management and analytics in intelligent smart environments, with particular regards to intelligent transportation and logistics systems, and provides relevant research directions that may represent a milestone for future years.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {5–7},
numpages = {3},
keywords = {Intelligent smart environments, Big data analytics, Big data management},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3379247.3379282,
author = {Liyao, Zhou and Xiaofang, Liu and Chunyu, Hu},
title = {Evaluation Method of Equipment Combat Effectiveness Based On Big Data Mining},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379282},
doi = {10.1145/3379247.3379282},
abstract = {In the evaluation of equipment combat effectiveness, it is necessary to comprehensively analyze the data of outfield test and infield test, including a variety of audio-visual, image and other combat test data. These data can be classified, extracted, stored and managed by building data model through big data mining technology. The evaluation method of equipment combat effectiveness based on big data mining is based on massive data, through machine learning, statistical analysis, neural network, database and other methods to analyze and process the data, mining the correlation between test data, evaluation index and evaluation conclusion, and extracting useful information and finding new knowledge from it to realize the evaluation of the combat effectiveness of the tested system.},
booktitle = {Proceedings of 2020 the 6th International Conference on Computing and Data Engineering},
pages = {131–135},
numpages = {5},
keywords = {combat effectiveness evaluation, Big data mining, combat test},
location = {Sanya, China},
series = {ICCDE 2020}
}

@article{10.1145/2992786,
author = {Debattista, Jeremy and Auer, S\"{O}ren and Lange, Christoph},
title = {Luzzu—A Methodology and Framework for Linked Data Quality Assessment},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2992786},
doi = {10.1145/2992786},
abstract = {The increasing variety of Linked Data on the Web makes it challenging to determine the quality of this data and, subsequently, to make this information explicit to data consumers. Despite the availability of a number of tools and frameworks to assess Linked Data Quality, the output of such tools is not suitable for machine consumption, and thus consumers can hardly compare and rank datasets in the order of fitness for use. This article describes a conceptual methodology for assessing Linked Datasets, and Luzzu; a framework for Linked Data Quality Assessment. Luzzu is based on four major components: (1) an extensible interface for defining new quality metrics; (2) an interoperable, ontology-driven back-end for representing quality metadata and quality problems that can be re-used within different semantic frameworks; (3) scalable dataset processors for data dumps, SPARQL endpoints, and big data infrastructures; and (4) a customisable ranking algorithm taking into account user-defined weights. We show that Luzzu scales linearly against the number of triples in a dataset. We also demonstrate the applicability of the Luzzu framework by evaluating and analysing a number of statistical datasets against a variety of metrics. This article contributes towards the definition of a holistic data quality lifecycle, in terms of the co-evolution of linked datasets, with the final aim of improving their quality.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {4},
numpages = {32},
keywords = {Data quality, linked data, quality assessment}
}

@article{10.1145/2968332,
author = {Geisler, Sandra and Quix, Christoph and Weber, Sven and Jarke, Matthias},
title = {Ontology-Based Data Quality Management for Data Streams},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2968332},
doi = {10.1145/2968332},
abstract = {Data Stream Management Systems (DSMS) provide real-time data processing in an effective way, but there is always a tradeoff between data quality (DQ) and performance. We propose an ontology-based data quality framework for relational DSMS that includes DQ measurement and monitoring in a transparent, modular, and flexible way. We follow a threefold approach that takes the characteristics of relational data stream management for DQ metrics into account. While (1) Query Metrics respect changes in data quality due to query operations, (2) Content Metrics allow the semantic evaluation of data in the streams. Finally, (3) Application Metrics allow easy user-defined computation of data quality values to account for application specifics. Additionally, a quality monitor allows us to observe data quality values and take counteractions to balance data quality and performance. The framework has been designed along a DQ management methodology suited for data streams. It has been evaluated in the domains of transportation systems and health monitoring.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {18},
numpages = {34},
keywords = {data quality assessment, ontologies, Data streams, data quality control}
}

@inproceedings{10.1145/2389686.2389688,
author = {Megler, V. M. and Maier, David},
title = {When Big Data Leads to Lost Data},
year = {2012},
isbn = {9781450317191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2389686.2389688},
doi = {10.1145/2389686.2389688},
abstract = {For decades, scientists bemoaned the scarcity of observational data to analyze and against which to test their models. Exponential growth in data volumes from ever-cheaper environmental sensors has provided scientists with the answer to their prayers: "big data". Now, scientists face a new challenge: with terabytes, petabytes or exabytes of data at hand, stored in thousands of heterogeneous datasets, how can scientists find the datasets most relevant to their research interests? If they cannot find the data, then they may as well never have collected it; that data is lost to them. Our research addresses this challenge, using an existing scientific archive as our test-bed. We approach this problem in a new way: by adapting Information Retrieval techniques, developed for searching text documents, into the world of (primarily numeric) scientific data. We propose an approach that uses a blend of automated and "semi-curated" methods to extract metadata from large archives of scientific data. We then perform searches over the extracted metadata, returning results ranked by similarity to the query terms. We briefly describe an implementation performed at an ocean observatory to validate the proposed approach. We propose performance and scalability research to explore how continued archive growth will affect our goal of interactive response, no matter the scale.},
booktitle = {Proceedings of the 5th Ph.D. Workshop on Information and Knowledge},
pages = {1–8},
numpages = {8},
keywords = {ranked data search, scientific data},
location = {Maui, Hawaii, USA},
series = {PIKM '12}
}

@inproceedings{10.1145/3512576.3512618,
author = {Sardjono, Wahyu and Retnowardhani, Astari and Emil Kaburuan, Robert and Rahmasari, Aninda},
title = {Artificial Intelligence and Big Data Analysis Implementation in Electronic Medical Records},
year = {2021},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512618},
doi = {10.1145/3512576.3512618},
abstract = {Industry 4.0 is the pioneer of the Internet of Things (IoT). The Internet of Things (IoT) are often heard and successful in business revolution from all sectors. The IoT are widely used in numerous sectors including medical services and have become the rise of Internet of Medical Things (IoMT). One of the implementations is the Electronic Health Record (EHR) systems. Previously the health records were used in traditional manner such as print-out health record of a patient and stored to an archive room. With the innovation of EHR, patients’ health records are digitalized which provides advantages from space efficiency and paperless forms. EHR helps medical service management to provide better healthcare services. With the integration of Artificial Intelligence (AI) and Big Data Analysis in EHR, healthcare services provide more accurate and reliable diagnosis.},
booktitle = {2021 The 9th International Conference on Information Technology: IoT and Smart City},
pages = {231–237},
numpages = {7},
keywords = {Analysis, big data, internet of things, artificial intelligence, electronic medical records},
location = {Guangzhou, China},
series = {ICIT 2021}
}

@inproceedings{10.1145/2910019.2910033,
author = {Netten, Niels and van den Braak, Susan and Choenni, Sunil and van Someren, Maarten},
title = {A Big Data Approach to Support Information Distribution in Crisis Response},
year = {2016},
isbn = {9781450336406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910019.2910033},
doi = {10.1145/2910019.2910033},
abstract = {Crisis response organizations operate in very dynamic environments, in which it is essential for responders to acquire all information critical to their task execution in time. In reality, the responders are often faced with information overload, incomplete information, or a combination of both. This hampers their decision-making process, workflow, situational awareness and, consequently, effective execution of collaborative crisis response. Therefore, getting the right information to the right person at the right time is of crucial importance.The task of processing all data during crisis response situations and determining for whom at a particular moment the information is relevant is not straightforward. When developing an information system to support this task, some important challenges have to be taken into account. These challenges relate to the structure and truthfulness of the used data, the assessment of information relevance, and the dissemination of relevant information in time. While methods and techniques from big data can be used to collect and integrate data, machine learning can be used to build a model for relevance assessments. An example implementation of such a framework of big data is the TAID software system that collects and integrates data communicated between first responders and may send information to crisis responders that were not addressed in the initial communication. As an example of the impact of TAID on crisis response, we show its effect in a simulated crisis response scenario.},
booktitle = {Proceedings of the 9th International Conference on Theory and Practice of Electronic Governance},
pages = {266–275},
numpages = {10},
keywords = {Information Distribution, Machine Learning, Relevance Assessments, Big Data, Crisis Response for Public Safety},
location = {Montevideo, Uruguay},
series = {ICEGOV '15-16}
}

@inproceedings{10.1145/3331453.3360973,
author = {Dong, Wei and Xiao, Litian and Niu, Shengfen and Niu, Jianjun and Wang, Fei},
title = {Application Research of Big Data for Launch Support System at Space Launch Site},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3360973},
doi = {10.1145/3331453.3360973},
abstract = {At the space launch site, the big data of the launch support system comes from the construction of the launch site, the ground service, the comprehensive support process, and launch mission organization and command. The big data is extensive sources, various types, large scale, and rapid growth. The big data application can improve the data processing and management efficiency for the launch support system. Then the application can enhance the support capability of flight mission and success rate. This paper analyzes the existing data application of launch support system. The challenges and requirements of big data application are studied by the construction of intelligent launch site. The application pattern and target are put forward from four aspects of launch mission organization and command, mission application, comprehensive support, and information security. The classification of big data is proposed for a launch support system. The architecture of big data application system is designed, which meets the application pattern and target. It lays a foundation for the future big data project at the launch site.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {23},
numpages = {6},
keywords = {Big data, Space launch site, Launch support system, Application research},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3371425.3371435,
author = {Liu, He and Wang, Xiaohui and Lei, Shuya and Zhang, Xi and Liu, Weiwei and Qin, Ming},
title = {A Rule Based Data Quality Assessment Architecture and Application for Electrical Data},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371435},
doi = {10.1145/3371425.3371435},
abstract = {Data quality assessment plays an important role in electricity consumption big data. It can help business people master the overall data situation, which can provide a strong guarantee for subsequent data improvement, analysis and decision. According to the electrical data quality issues, we design a rule-based data quality assessment architecture for electrical big data. It includes six types of data quality assessment indexes (such as comprehensiveness, accuracy, completeness), and the related data quality rules (such as non-empty rule and range rule), which can be used to guide the electrical data quality inspection. Meanwhile, for the accuracy, we propose an outlier detection method based on time time-relevant k-means, which is used to detect the voltage, curve and power data issues in electricity data. The experimental and simulation results show that the proposed architecture and method can work well for the comprehensive data quality assessment of electrical data.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {40},
numpages = {6},
keywords = {outlier, quality assessment, electrical data, data quality},
location = {Sanya, China},
series = {AIIPCC '19}
}

@inproceedings{10.1145/3268891.3268892,
author = {Liu, Zhao-ge and Li, Xiang-yang},
title = {Full View Scenario Model of Big Data Governance in Community Safety Service},
year = {2018},
isbn = {9781450365024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268891.3268892},
doi = {10.1145/3268891.3268892},
abstract = {In community safety service, big data governance is the prime mode to achieve community safety big data sharing and service value increasing. Although existing researches have preliminarily established the general big data governance framework, identification and governance of specific sharing problems lack comprehensive and systematic scenario description. Applying software engineering method, this paper proposes a kind of scenario expression model of big data governance in community safety service. Considering the common features of big data governance scenarios, construct the meta-scenario model of big data governance in community safety services. Considering the scenario expression difference under different levels, scales and particle sizes, construct the full view scenario model of big data governance in community safety services by meta-models nesting to complete the scenario expression under different applying situation. Finally, a use case is proposed to verify the rationality and effectiveness of the scenario expression models.},
booktitle = {Proceedings of the 8th International Conference on Information Communication and Management},
pages = {44–49},
numpages = {6},
keywords = {big data governance, safety service, community safety, scenario model},
location = {Edinburgh, United Kingdom},
series = {ICICM '18}
}

@inproceedings{10.1145/3529190.3529222,
author = {Pleimling, Xavier and Shah, Vedant and Lourentzou, Ismini},
title = {[Data] Quality Lies In The Eyes Of The Beholder},
year = {2022},
isbn = {9781450396318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529190.3529222},
doi = {10.1145/3529190.3529222},
abstract = {As large-scale machine learning models become more prevalent in assistive and pervasive technologies, the research community has started examining limitations and challenges that arise from training data, e.g., fairness, bias, and interpretability issues. To this end, data-centric approaches are increasingly prevailing over time, showing that high-quality data is a critical component in many applications. Several studies explore methods to define and improve data quality, however, no uniform definition exists. In this work, we present an empirical analysis of the multifaceted problem of evaluating data quality. Our work aims at identifying data quality challenges that are most commonly observed by data users and practitioners. Inspired by the need for generally applicable methods, we select a representative set of quality indicators, that covers a broad spectrum of issues, and investigate the utility of these indicators on a broad range of datasets through inter-annotator agreement analysis. Our work provides insights and presents open challenges in designing improved data life cycles.},
booktitle = {Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {118–124},
numpages = {7},
keywords = {datasets, incomplete data, data quality metrics, user survey, data annotation, inconsistent data, data utility, data quality, duplicate data, incorrect data},
location = {Corfu, Greece},
series = {PETRA '22}
}

@inproceedings{10.1109/CCGrid.2016.63,
author = {Ordonez, Carlos and Garc\'{\i}a-Garc\'{\i}a, Javier},
title = {Managing Big Data Analytics Workflows with a Database System},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.63},
doi = {10.1109/CCGrid.2016.63},
abstract = {A big data analytics workflow is long and complex, with many programs, tools and scripts interacting together. In general, in modern organizations there is a significant amount of big data analytics processing performed outside a database system, which creates many issues to manage and process big data analytics workflows. In general, data preprocessing is the most time-consuming task in a big data analytics workflow. In this work, we defend the idea of preprocessing, computing models and scoring data sets inside a database system. In addition, we discuss recommendations and experiences to improve big data analytics workflows by pushing data preprocessing (i.e. data cleaning, aggregation and column transformation) into a database system. We present a discussion of practical issues and common solutions when transforming and preparing data sets to improve big data analytics workflows. As a case study validation, based on experience from real-life big data analytics projects, we compare pros and cons between running big data analytics workflows inside and outside the database system. We highlight which tasks in a big data analytics workflow are easier to manage and faster when processed by the database system, compared to external processing.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {649–655},
numpages = {7},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/3543106.3543115,
author = {Li, Kai},
title = {A Study on the Economic Model of Volume in the Age of Big Data},
year = {2022},
isbn = {9781450397162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543106.3543115},
doi = {10.1145/3543106.3543115},
abstract = {With the rapid development of China's economy, the degree of integration of economics and management is deepening. Based on the statistical analysis method of big data, the trend and law of economic development can be obtained. Big data statistical methods are widely used in the field of economics and improve work efficiency. Effective statistical analysis of data can not only reflect the operation of the product in time, but also reflect the market demand for the product. Therefore, this paper studies the role of big data statistical analysis methods in the field of economic management. This paper first classifies and sorts out the representative quantitative research methods and models in the era of big data, and then based on the BP neural network model and combines 36 indicator data to make multivariate forecasts for China's consumer price index (CPI). The research results show that the prediction results of the BP neural network are good.},
booktitle = {Proceedings of the 2022 International Conference on E-Business and Mobile Commerce},
pages = {54–58},
numpages = {5},
keywords = {BP neural network, Quantitative economy, CPI, Big data},
location = {Seoul, Republic of Korea},
series = {ICEMC '22}
}

@inproceedings{10.1145/3149572.3149575,
author = {Francisco, Maritza M. C. and Alves-Souza, Solange N. and Campos, Edit G. L. and De Souza, Luiz S.},
title = {Total Data Quality Management and Total Information Quality Management Applied to Costumer Relationship Management},
year = {2017},
isbn = {9781450353373},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149572.3149575},
doi = {10.1145/3149572.3149575},
abstract = {Data quality (DQ) is an important issue for modern organizations, mainly for decision-making based on information, using solutions such as CRM, Business Analytics, and Big Data. In order to obtain quality data, it is necessary to implement methods, processes, and specific techniques that handle information as a product, with well established, controlled, and managed production processes. The literature provides several types of quality data management methodologies that treat structured data, and few treating semi- and non-structured data. Choosing the methodology to be adopted is one the major issues faced by organizations, when challenged to treat the data quality in a systematic manner. This paper makes a comparative analysis between TDQM -- Total Data Quality Management and TIQM -- Total Information Quality Management approaches, focusing on data quality problems in the context of a CRM -- Costumer Relationship Management application. Such analysis identifies the strengths and weaknesses of each methodology and suggests the most suitable for the CRM scenario.},
booktitle = {Proceedings of the 9th International Conference on Information Management and Engineering},
pages = {40–45},
numpages = {6},
keywords = {data quality methodology, Data quality, data quality problems, data quality dimensions, data quality management},
location = {Barcelona, Spain},
series = {ICIME 2017}
}

@inproceedings{10.1145/2723372.2742784,
author = {G.C., Paul Suganthan and Sun, Chong and K., Krishna Gayatri and Zhang, Haojun and Yang, Frank and Rampalli, Narasimhan and Prasad, Shishir and Arcaute, Esteban and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay and Doan, AnHai},
title = {Why Big Data Industrial Systems Need Rules and What We Can Do About It},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742784},
doi = {10.1145/2723372.2742784},
abstract = {Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to address rule management, given the tens of thousands of rules industrial systems often manage today in an ad-hoc fashion.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {265–276},
numpages = {12},
keywords = {rule management, big data, classification},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/2661829.2661837,
author = {Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng},
title = {Cleanix: A Big Data Cleaning Parfait},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661837},
doi = {10.1145/2661829.2661837},
abstract = {In this demo, we present Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate - the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. We demonstrate that Cleanix is a practical tool that supports effective and efficient data cleaning at the large scale.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {2024–2026},
numpages = {3},
keywords = {data quality, data cleaning, big data},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/3356998.3365776,
author = {Zhang, Mingke and Guo, Danhuai and Hu, Jinyong and Jin, Wei},
title = {Risk Prediction and Assessment of Foodborne Disease Based on Big Data},
year = {2019},
isbn = {9781450369657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356998.3365776},
doi = {10.1145/3356998.3365776},
abstract = {In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
articleno = {8},
numpages = {6},
keywords = {foodborne disease, risk assessment, big data, machine learning},
location = {Chicago, Illinois},
series = {EM-GIS '19}
}

@inproceedings{10.1145/3341069.3341086,
author = {Pengxi, Li},
title = {The Construction Study of College Informationization Teaching Service System under the Background of Big Data},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3341086},
doi = {10.1145/3341069.3341086},
abstract = {Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of "big data assisted employment", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {185–189},
numpages = {5},
keywords = {Teaching informatization, Service system, Big data},
location = {Guangzhou, China},
series = {HPCCT 2019}
}

@inproceedings{10.1145/3399205.3399228,
author = {Moumen, Aniss},
title = {Adoption of Big Data, Cloud Computing &amp; IoT in Morocco Perception of Public Administrations Collaborators},
year = {2020},
isbn = {9781450375788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399205.3399228},
doi = {10.1145/3399205.3399228},
abstract = {The integration of new technologies such as big data, cloud computing, and the Internet of Things (IoT), constitute a new challenge for existing information systems of local administrations.In this work, we present the results of interviews conducted with the public administrations in the South-eastern region of Morocco, and as conclusion we expose two proposed models induced by this study.},
booktitle = {Proceedings of the 4th Edition of International Conference on Geo-IT and Water Resources 2020, Geo-IT and Water Resources 2020},
articleno = {21},
numpages = {4},
keywords = {Information System, IoT, Big data, Cloud Computing},
location = {Al-Hoceima, Morocco},
series = {GEOIT4W-2020}
}

@inproceedings{10.1145/3445945.3445962,
author = {Zhao, Liangbin and Fu, Xiuju},
title = {A Visual Method for Ship Close Encounter Pattern Recognition Based on Fuzzy Theory and Big Data Intelligence},
year = {2020},
isbn = {9781450387750},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445945.3445962},
doi = {10.1145/3445945.3445962},
abstract = {As an important application of big data technology in the maritime field, big data driven visualization of ship encounter patterns helps to intuitively understand the risk situation in the water traffic. However traditional methods based on fixed thresholds do not consider the fuzziness of classification on ship encounter situations. We proposed a visual method to visualize the risk situations caused by the interaction between vessel traffic flows in more detail based on fuzzy theory and big data intelligence on large scale AIS data. A case study is conducted to verify the applicability based on the AIS data from Singapore Strait. Visualization results of density in grids show that the proposed method can effectively reflect the ship encounter patterns, which are consistent with the real situation and can show more valuable details for the safety assessment of water traffic.},
booktitle = {2020 the 4th International Conference on Big Data Research (ICBDR'20)},
pages = {94–100},
numpages = {7},
keywords = {Ship encounter, Fuzzy theory, Visualization, AIS data},
location = {Tokyo, Japan},
series = {ICBDR 2020}
}

@inproceedings{10.1145/3483816.3483836,
author = {Febiri, Frank and Yihum Amare, Meseret and Hub, Miloslav},
title = {Fusion from Big Data to Smart Data to Enhance Quality of Information Systems},
year = {2021},
isbn = {9781450390545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483816.3483836},
doi = {10.1145/3483816.3483836},
abstract = {The term “smartness” in the data framework indicates relevancy based on the intended purpose of data. The Internet of Things (IoT) and advancements in technology have resulted in an ever-increasing pool of data available to all institutions to derive meaning and make sound decisions from them. The research presented in this paper explored the role smart data play in information systems quality through a qualitative study of how using the large pool of data (big data) and fusing it to smart data organizations can make sound and intelligent decisions using the available techniques. We use an existing architecture for a public institution to analyze how data ingestion can be achieved with minimum challenges. The findings suggest that even though there is a large pool of data for most organizations, it is becoming more challenging to use this data to make organizational sense due to the challenges posed by such data. The realization of smart data and its benefits in information systems helps improve the quality of information systems, reducing cost and promoting the smartness agenda of today's organization.},
booktitle = {2021 8th International Conference on Management of E-Commerce and e-Government},
pages = {112–117},
numpages = {6},
keywords = {Quality measures, Smart data, Big Data, Information systems},
location = {Jeju, Republic of Korea},
series = {ICMECG 2021}
}

@article{10.14778/3352063.3352128,
author = {Chen, Zhimin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit},
title = {Customizable and Scalable Fuzzy Join for Big Data},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352128},
doi = {10.14778/3352063.3352128},
abstract = {Fuzzy join is an important primitive for data cleaning. The ability to customize fuzzy join is crucial to allow applications to address domain-specific data quality issues such as synonyms and abbreviations. While efficient indexing techniques exist for single-node implementations of customizable fuzzy join, the state-of-the-art scale-out techniques do not support customization, and exhibit poor performance and scalability characteristics. We describe the design of a scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing (LSH) based signature scheme, and introduce optimizations that result in significant speed up with negligible impact on recall. We evaluate our implementation on the Azure Databricks version of Spark using several real-world and synthetic data sets. We observe speedups exceeding 50X compared to the best-known prior scale-out technique, and close to linear scalability with data size and number of nodes.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2106–2117},
numpages = {12}
}

@inproceedings{10.1145/3281375.3281386,
author = {Rinaldi, Antonio M. and Russo, Cristiano},
title = {A Semantic-Based Model to Represent Multimedia Big Data},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281386},
doi = {10.1145/3281375.3281386},
abstract = {The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {31–38},
numpages = {8},
keywords = {semantics, multimedia ontologies, semantic bigdata},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1145/3393527.3393532,
author = {Shi, Bin and YabinXu},
title = {Research on Copyright Protection Method of Big Data Based on Nash Equilibrium and Constraint Optimization},
year = {2020},
isbn = {9781450375344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393527.3393532},
doi = {10.1145/3393527.3393532},
abstract = {Data watermarking technology is an effective means to protect the copyright of big data. In order to embed robust and highly available data watermarks, firstly, based on the game theory, a Nash equilibrium model between watermark robustness and data quality is established to solve the optimal number of data partitioning. Then, the mapping relationship between data partitioning and watermark bit is established by using secure hash algorithm. Finally, under the constraint of data usability, the improved particle swarm optimization algorithm is used to calculate the optimal solution of data change for each data partitioning, and then the data is changed accordingly to complete the embedding of watermark bit. In order to verify the copyright ownership of big data, this paper also gives the corresponding watermark extraction method. Watermark extraction is the inverse process of watermark embedding. First, traverse all partitions and extract the possible embedded bit values in each data partitioning. Then, the actual embedded watermark bit is finally determined by majority voting strategies. The experimental results show that our proposed method can not only detect watermarks under different attack conditions, ensure the robustness of big data watermarks, but also achieve better data quality, and the comprehensive effect of data watermarks is better than the existing methods.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
pages = {21–25},
numpages = {5},
keywords = {Particle swarm optimization algorithm, Nash equilibrium, Majority voting strategy, Copyright protection, Big data, Data watermarking, Constrained optimization},
location = {Hefei, China},
series = {ACM TURC'20}
}

@article{10.1145/3362121,
author = {Firmani, Donatella and Tanca, Letizia and Torlone, Riccardo},
title = {Ethical Dimensions for Data Quality},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3362121},
doi = {10.1145/3362121},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {2},
numpages = {5},
keywords = {source selection, knowledge extraction, Data integration}
}

@article{10.1145/3297720,
author = {M\"{u}ller, Daniel and Jain, Pratiksha and Te, Yieh-Funk},
title = {Augmenting Data Quality through High-Precision Gender Categorization},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3297720},
doi = {10.1145/3297720},
abstract = {Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies’ outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations’ records, if the gender attribute is missing or unreliable.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {8},
numpages = {18},
keywords = {patenting, gender name mapping, record completion, Data quality improvement}
}

@inproceedings{10.1145/3401895.3402092,
author = {Silva, Rodrigo Dantas da and de Ara\'{u}jo, Jean Jar Pereira and de Paiva, \'{A}lvaro Ferreira Pires and de Medeiros Valentim, Ricardo Alexsandro and Coutinho, Karilany Dantas and de Paiva, Jailton Carlos and Roussanaly, Azim and Boyer, Anne},
title = {A Big Data Architecture to a Multiple Purpose in Healthcare Surveillance: The Brazilian Syphilis Case},
year = {2020},
isbn = {9781450377119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401895.3402092},
doi = {10.1145/3401895.3402092},
abstract = {For many decades society did need to monitor and assess the standard of living of the population. In the 1950s, the United Nations (UN) saw this need and proposed 12 areas that should be evaluated, the first of which is listed under "Health and Demography", which focuses on what is expressed as the level of a population's health. Decades have passed and great results have been gained from similar initiatives such as reducing mortality from infectious diseases and even eradicating some others. In the age of the digital society, needs have grown. Monitoring demands that once perished from data to become concrete now suffer from the opposite effect, the excess of data from everywhere. Healthcare systems around the world use many different information systems, collecting and generating hundreds of data at unimaginable speed. We are billions of people on the planet and most of us are connected to the virtual world, sharing information, experiences and events with some kind of cloud. In this information age, the ability to aggregate and process this data is a major factor in raising public health to a new level. The development of tools capable of analyzing a large volume of data in seconds and producing knowledge for targeted decision making can help in the fight against specific diseases, in the process of continuing education of professionals, in the formation of new professionals, in the elaboration of new policies. with the specific locoregional look, in the analysis of hidden trends in front of so much information faced in everyday life and other possibilities. The present work proposes an architecture capable of storing and manipulating seeking to standardize the variables in order to allow to correlate this large amount of data in a systematic way, providing to several services and researchers the possibility of consuming health, social, economic and educational data for the promotion of public health.},
booktitle = {Proceedings of the 10th Euro-American Conference on Telematics and Information Systems},
articleno = {58},
numpages = {6},
keywords = {healthcare surveillance, epidemiology, big data, syphilis},
location = {Aveiro, Portugal},
series = {EATIS '20}
}

@inproceedings{10.1145/3209582.3209599,
author = {Gong, Xiaowen and Shroff, Ness},
title = {Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing},
year = {2018},
isbn = {9781450357708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209582.3209599},
doi = {10.1145/3209582.3209599},
abstract = {Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the "wisdom" of a potentially large crowd of "workers" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest "virtual valuation" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.},
booktitle = {Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {161–170},
numpages = {10},
keywords = {data quality, incentive mechanism, Mobile data crowdsourcing},
location = {Los Angeles, CA, USA},
series = {Mobihoc '18}
}

@article{10.1145/3418896,
author = {Christophides, Vassilis and Efthymiou, Vasilis and Palpanas, Themis and Papadakis, George and Stefanidis, Kostas},
title = {An Overview of End-to-End Entity Resolution for Big Data},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3418896},
doi = {10.1145/3418896},
abstract = {One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows&nbsp;for&nbsp;Big Data, critically review the pros and cons of existing methods, and conclude with the main open research&nbsp;directions.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {127},
numpages = {42},
keywords = {Entity blocking and matching, batch and incremental entity resolution workflows, deep learning, block processing, strongly and nearly similar entities, crowdsourcing}
}

@article{10.1145/3469890,
author = {Lv, Zhihan and Lou, Ranran and Feng, Hailin and Chen, Dongliang and Lv, Haibin},
title = {Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3469890},
doi = {10.1145/3469890},
abstract = {Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {oct},
articleno = {7},
numpages = {21},
keywords = {lightGBM, Machine learning, intelligent support information system, accuracy rate, big data analysis}
}

@article{10.1145/3449052,
author = {Aljawarneh, Shadi and Lara, Juan A.},
title = {Editorial: Special Issue on Quality Assessment and Management in Big Data—Part I},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3449052},
doi = {10.1145/3449052},
journal = {J. Data and Information Quality},
month = {may},
articleno = {6},
numpages = {3},
keywords = {quality management, Quality assessment, big data}
}

@article{10.1145/3449056,
author = {Aljawarneh, Shadi and Lara, Juan A.},
title = {Editorial: Special Issue on Quality Assessment and Management in Big Data—Part II},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3449056},
doi = {10.1145/3449056},
journal = {J. Data and Information Quality},
month = {may},
articleno = {13},
numpages = {3},
keywords = {big data, Quality assessment, quality management}
}

@inproceedings{10.1145/3301551.3301610,
author = {Li, Yonghong and Zhang, Shuwen and Jia, Nan},
title = {Research on the Transformation and Upgrading Path and Selection of Traditional Industries from the Perspective of Big Data},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301610},
doi = {10.1145/3301551.3301610},
abstract = {With the emergence of a new generation of information technology, big data has become an important driving force for current social development. Digitalization has become the main direction of the transformation and upgrading of traditional industries. As the product of current informatization, big data includes data quantity, data quality and data analysis ability. It is used as two different ways to interpret the value creation of big data, making it clear that it can promote the transformation and upgrading of traditional industries through value creation. Then, it puts forward the traditional industrial transformation and upgrading path from the perspective of big data, namely the linear path of "traditional industry + digital" and the transitional non-linear "digital + traditional industry". Its path selection will be analyzed by combining external and internal factors.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {54–59},
numpages = {6},
keywords = {Big data, The path, Traditional industries, Transformation and upgrading},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@inproceedings{10.1145/3539781.3539795,
author = {Satheesan, Sandeep Puthanveetil and Bhavya and Davies, Adam and Craig, Alan B. and Zhang, Yu and Zhai, ChengXiang},
title = {Toward a Big Data Analysis System for Historical Newspaper Collections Research},
year = {2022},
isbn = {9781450394109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539781.3539795},
doi = {10.1145/3539781.3539795},
abstract = {The availability and generation of digitized newspaper collections have provided researchers in several domains with a powerful tool to advance their research. More specifically, digitized historical newspapers give us a magnifying glass into the past. In this paper, we propose a scalable and customizable big data analysis system that enables researchers to study complex questions about our society as depicted in news media for the past few centuries by applying cutting-edge text analysis tools to large historical newspaper collections. We discuss our experience with building a preliminary version of such a system, including how we have addressed the following challenges: processing millions of digitized newspaper pages from various publications worldwide, which amount to hundreds of terabytes of data; applying article segmentation and Optical Character Recognition (OCR) to historical newspapers, which vary between and within publications over time; retrieving relevant information to answer research questions from such data collections by applying human-in-the-loop machine learning; and enabling users to analyze topic evolution and semantic dynamics with multiple compatible analysis operators. We also present some preliminary results of using the proposed system to study the social construction of juvenile delinquency in the United States and discuss important remaining challenges to be tackled in the future.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {12},
numpages = {11},
keywords = {historical newspapers, data visualization, juvenile delinquency, text analysis, image analysis, social science research, social construction, newspaper article segmentation, natural language processing, big data analysis system, information retrieval},
location = {Basel, Switzerland},
series = {PASC '22}
}

@inproceedings{10.1145/3495018.3495345,
author = {Chen, Xin and Yang, Lirong and Sun, Yanzhi},
title = {Human Resource Information System Performance Test under Big Data Technology},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495345},
doi = {10.1145/3495018.3495345},
abstract = {The era of big data has quietly arrived, which is a revolution that determines the development and future destiny of enterprises. Any enterprises that are not ready for this revolution will be eliminated by the era. This paper mainly studies the construction, analysis and management of human resource system in the era of big data. Based on the actual needs, this paper analyzes the business process and functional requirements of human resource management, completes the system architecture design, function module design, database design, realizes the system function module, and completes the test of the system function. The functional modules realized in this paper include: core personnel management, salary management and comprehensive inquiry. The human resource information system designed in this paper ensures the scientific nature, security, availability and portability of the system, meets the demand of data sharing, and plays a positive role in the whole human resource management cycle.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1107–1111},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3282278.3282282,
author = {Casado-Vara, Roberto and de la Prieta, Fernando and Prieto, Javier and Corchado, Juan M.},
title = {Blockchain Framework for IoT Data Quality via Edge Computing},
year = {2018},
isbn = {9781450360500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282278.3282282},
doi = {10.1145/3282278.3282282},
abstract = {Smart home presents a challenge in control and monitoring of its wireless sensors networks (WSN) and the internet of things (IoT) devices which form it. The current IoT architectures are centralized, complex, with poor security in its communications and with upstream communication channels mainly. As a result, there are problems with data reliability. These problems include data missing, malicious data inserted, communications network overload, and overload of computing power at the central node. In this paper a new architecture is presented. This architecture based in blockchain introduce the edge computing layer and a new algorithm to improve data quality and false data detection.},
booktitle = {Proceedings of the 1st Workshop on Blockchain-Enabled Networked Sensor Systems},
pages = {19–24},
numpages = {6},
keywords = {WSN, non linear control, edge computing, IoT, data quality false data detection, Blockchain},
location = {Shenzhen, China},
series = {BlockSys'18}
}

@inproceedings{10.1145/3286606.3286788,
author = {Bibri, Simon Elias and Krogstie, John},
title = {The Big Data Deluge for Transforming the Knowledge of Smart Sustainable Cities: A Data Mining Framework for Urban Analytics},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286788},
doi = {10.1145/3286606.3286788},
abstract = {There has recently been much enthusiasm about the possibilities created by the big data deluge to better understand, monitor, analyze, and plan modern cities to improve their contribution to the goals of sustainable development. Indeed, much of our knowledge of urban sustainability has been gleaned from studies that are characterized by data scarcity. Therefore, this paper endeavors to develop a systematic framework for urban sustainability analytics based on a cross-industry standard process for data mining. The intention is to enable well-informed decision-making and enhanced insights in relation to diverse urban domains. We argue that there is tremendous potential to transform and advance the knowledge of smart sustainable cities through the creation of a big data deluge that seeks to provide much more sophisticated, wider-scale, finer-grained, real-time understanding, and control of various aspects of urbanity in the undoubtedly upcoming Exabyte Age.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {11},
numpages = {10},
keywords = {Smart sustainable cities, data mining, big data analytics},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/2663876.2663885,
author = {Freudiger, Julien and Rane, Shantanu and Brito, Alejandro E. and Uzun, Ersin},
title = {Privacy Preserving Data Quality Assessment for High-Fidelity Data Sharing},
year = {2014},
isbn = {9781450331517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663876.2663885},
doi = {10.1145/2663876.2663885},
abstract = {In a data-driven economy that struggles to cope with the volume and diversity of information, data quality assessment has become a necessary precursor to data analytics. Real-world data often contains inconsistencies, conflicts and errors. Such dirty data increases processing costs and has a negative impact on analytics. Assessing the quality of a dataset is especially important when a party is considering acquisition of data held by an untrusted entity. In this scenario, it is necessary to consider privacy risks of the stakeholders.This paper examines challenges in privacy-preserving data quality assessment. A two-party scenario is considered, consisting of a client that wishes to test data quality and a server that holds the dataset. Privacy-preserving protocols are presented for testing important data quality metrics: completeness, consistency, uniqueness, timeliness and validity. For semi-honest parties, the protocols ensure that the client does not discover any information about the data other than the value of the quality metric. The server does not discover the parameters of the client's query, the specific attributes being tested and the computed value of the data quality metric. The proposed protocols employ additively homomorphic encryption in conjunction with condensed data representations such as counting hash tables and histograms, serving as efficient alternatives to solutions based on private set intersection.},
booktitle = {Proceedings of the 2014 ACM Workshop on Information Sharing &amp; Collaborative Security},
pages = {21–29},
numpages = {9},
keywords = {cryptographic protocols, data quality assessment, privacy and confidentiality},
location = {Scottsdale, Arizona, USA},
series = {WISCS '14}
}

@inproceedings{10.5555/3374138.3374194,
author = {Bawatna, Mohammed and Green, Bertram and Kovalev, Sergey and Deinert, Jan-Christoph and Knodel, Oliver and Spallek, Rainer G.},
title = {Research and Implementation of Efficient Parallel Processing of Big Data at TELBE User Facility},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {In recent years, improvements in high-speed Analog-to-Digital Converters (ADC) and sensor technology has encouraged researchers to improve the performance of Data Acquisition (DAQ) systems for scientific experiments which require high speed and continuous data measurements --- in particular, measuring the electronic and magnetic properties of materials using pump-probe experiments at high repetition rates. Experiments at TELBE are capable of acquiring almost 100 Gigabytes of raw data every ten minutes. The DAQ system used at TELBE partitions the raw data into various subdirectories for further parallel processing utilizing the multicore structure of modern CPUs.Furthermore, several other types of processors that accelerate data processing like the GPU and FPGA have emerged to solve the challenges of processing the massive amount of raw data. However, the memory and network bottlenecks become a significant challenge in big data processing, and new scalable programming techniques are needed to solve these challenges. In this contribution, we will outline the design and implementation of our practical software approach for efficient parallel processing of our large data sets at the TELBE user facility.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {56},
numpages = {6},
keywords = {data processing pipeline, data acquisition systems, signal processing, big data, data analytics},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@techreport{10.5555/2849516,
author = {Markus, M. Lynne and Topi, Heikki},
title = {Big Data, Big Decisions for Science, Society, and Business: Report on a Research Agenda Setting Workshop},
year = {2015},
publisher = {National Science Foundation},
address = {USA},
abstract = {The report from the workshop, "Big Data, Big Decisions for Government, Business and Society," makes a number of astute contributions. There is no need to replicate them in this foreword - they are in the report. What might be missed comes between the lines, where provocative points are made. Big Data means big opinions and big stakes. Those who think Big Data important want to be proven right, those who think Big Data a passing fad want Big Data to fade, and those who think Big Data will bring profound change hope for change. Big Data, like everything important, is political.}
}

@inproceedings{10.1145/3444370.3444614,
author = {Zhang, Yong},
title = {Human Resource Data Quality Management Based on Multiple Regression Analysis},
year = {2020},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444614},
doi = {10.1145/3444370.3444614},
abstract = {The essence of human resource management informatization is data. Firstly, human information is transformed into data, and then the data can be used. This requires that the comprehensive, complete, timely and effective human resource data is entered into the system, and then the data is analyzed by using the human resource management system to provide decision support for the management. Due to the complexity of the internal law of objective things and the limitation of people's cognition, it is impossible to analyze the internal causality of the actual object and establish a mathematical model in accordance with the mechanism law. Therefore, when some mathematical models cannot be established by mechanism analysis, we usually adopt the method of collecting a large amount of data, and establish the model based on the statistical analysis of the data. Among them, the most widely used random model is statistical regression model.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {465–470},
numpages = {6},
keywords = {data quality, human resources, Multiple regression analysis},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@article{10.1145/2481528.2481537,
author = {Stonebraker, Michael and Madden, Sam and Dubey, Pradeep},
title = {Intel "Big Data" Science and Technology Center Vision and Execution Plan},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/2481528.2481537},
doi = {10.1145/2481528.2481537},
abstract = {Intel has moved to a collaboration model with universities consisting of "Science and Technology Centers" (ISTCs). These are located at a "hub" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of "Big Data". This paper presents the big data vision of this technology center and the execution plan for the first few years.},
journal = {SIGMOD Rec.},
month = {may},
pages = {44–49},
numpages = {6}
}

@inproceedings{10.1145/3209415.3209427,
author = {Androutsopoulou, Aggeliki and Charalabidis, Yannis},
title = {A Framework for Evidence Based Policy Making Combining Big Data, Dynamic Modelling and Machine Intelligence},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209427},
doi = {10.1145/3209415.3209427},
abstract = {Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {575–583},
numpages = {9},
keywords = {impact assessment, evidence based policy making, dynamic simulation, data mining, behavioural patterns, Big data, policy Modelling},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inbook{10.1145/3310205.3310211,
title = {Data Quality Rule Definition and Discovery},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310211},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3377812.3390811,
author = {Khalajzadeh, Hourieh and Simmons, Andrew and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang and Ratnakanthan, Prasanna and Zia, Adil and Law, Meng},
title = {A Practical, Collaborative Approach for Modeling Big Data Analytics Application Requirements},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3390811},
doi = {10.1145/3377812.3390811},
abstract = {Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices - e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {256–257},
numpages = {2},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3210752,
author = {Orenga-Rogl\'{a}, Sergio and Chalmeta, Ricardo},
title = {Framework for Implementing a Big Data Ecosystem in Organizations},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3210752},
doi = {10.1145/3210752},
abstract = {Featuring the various dimensions of data management, it guides organizations through implementation fundamentals.},
journal = {Commun. ACM},
month = {dec},
pages = {58–65},
numpages = {8}
}

@inproceedings{10.1145/3358331.3358336,
author = {Jia, Dong-Ming and Yuan, Cun-Feng and Guo, Song and Jiang, Zu-Zhen and Xu, Ding and Wang, Da-An},
title = {Application of "Artificial Intelligence and Big Data" in Sports Rehabilitation for Chinese Judicial Administrative Drug Addicts},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358336},
doi = {10.1145/3358331.3358336},
abstract = {Under the background of "Wisdom Drug Rehabilitation", we introduced "Artificial Intelligence and Big Data" into "exercise rehabilitation" work of drug addicts in judicial administrative system. It is a practical innovation of drug treatment in China. This article will elaborate this innovation of the construction and application of "Exercise Rehabilitation" intelligence platform system. This system will improve mental status, alleviate physical and psychological symptoms, ensure safety in places, lighten the burden of professional police officers, make rapid analysis, make accurate decisions and improve the integrity rate of the addict.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {5},
numpages = {5},
keywords = {artificial intelligence, big data, judicial administrative, Sports rehabilitation, rehabilitation training},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/2896825.2896831,
author = {Scavuzzo, Marco and Tamburri, Damian A. and Di Nitto, Elisabetta},
title = {Providing Big Data Applications with Fault-Tolerant Data Migration across Heterogeneous NoSQL Databases},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896831},
doi = {10.1145/2896825.2896831},
abstract = {The recent growing interest on highly-available data-intensive applications sparked the need for flexible and portable storage technologies, e.g., NoSQL databases. Unfortunately, the lack of standard interfaces and architectures for NoSQLs makes it difficult and expensive to create portable applications, which results in vendor lock-in. Building on previous work, we aim at providing guaranteed fault-tolerant techniques and supporting architectures to port or migrate data to and across heterogeneous NoSQL technology. To prove the effectiveness of our approach we evaluate it on an industrial case-study. We conclude that our method and supporting architecture offer an efficient and fault-tolerant mechanism for NoSQL portability and interoperation.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {26–32},
numpages = {7},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/2737909.2737912,
author = {Fox, Geoffrey C. and Jha, Shantenu and Qiu, Judy and Luckow, Andre},
title = {Towards an Understanding of Facets and Exemplars of Big Data Applications},
year = {2014},
isbn = {9781450330312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737909.2737912},
doi = {10.1145/2737909.2737912},
abstract = {We study many Big Data applications from a variety of research and commercial areas and suggest a set of characteristic features and possible kernel benchmarks that stress those features for data analytics. We draw conclusions for the hardware and software architectures that are suggested by this analysis.},
booktitle = {Proceedings of the 20 Years of Beowulf Workshop on Honor of Thomas Sterling's 65th Birthday},
pages = {7–16},
numpages = {10},
location = {Annapolis, MD, USA},
series = {Beowulf '14}
}

@inproceedings{10.1145/3341620.3341624,
author = {Jia, Fengsheng and Gao, Yang and Wang, Yuming},
title = {Study on Standard System of Aerospace Quality Data Resources Integration under the Background of Big Data},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341624},
doi = {10.1145/3341620.3341624},
abstract = {The integration and application of aerospace product quality data resources is an important way to carry out quality improvement, quality evaluation and precise management. Standardization is the basis for promoting quality data resources integration. The unified and normative standard system is the guarantee for efficient development of integration standards. Firstly, we analyzed the features of quality data resources according to the status quo of integration. Integration structure of quality data resources in terms of vertical and horizontal integration was proposed by adopting the methods of "decomposition-integration" and "classification-association". Secondly, we constructed a three-dimensions architecture of quality data resource integration using the method of system engineering methodology, from the layer dimension (basis, common, special), technical dimension (description, collection, storage, transmission, processing, comprehensive management) and category dimension (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard by adopting the top-down approach. Some standard development suggestions are proposed based on the characteristics of quality data resources and standard research strategies. Finally, we applied the quality problem data resource standard construction and application to verify the proposed method.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {16–22},
numpages = {7},
keywords = {system planning, standard system, quality data resource integration, aerospace products},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3314527.3314537,
author = {Cabanban-Casem, Christianne Lynnette},
title = {Analytical Visualization of Higher Education Institutions' Big Data for Decision Making},
year = {2019},
isbn = {9781450366212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314527.3314537},
doi = {10.1145/3314527.3314537},
abstract = {Education is an important element towards learning and human development, thus, it is the key towards identifying competencies and better productivity for the workforce. As part of the Commission on Higher Education's (CHED) thrust for improving efficiency and effectiveness by simplifying the collection process for all the stakeholders, the developed system will drastically improve the availability of data for making informed decisions and efficient generation of reports.This research outlines opportunities and challenges associated with the implementation and governance of Big Data in higher education through development and implementation of data analytics tool.},
booktitle = {Proceedings of the 2019 Asia Pacific Information Technology Conference},
pages = {61–64},
numpages = {4},
keywords = {Knowledge Management, Higher Education Data, Data Science},
location = {Jeju Island, Republic of Korea},
series = {APIT 2019}
}

@inproceedings{10.1145/3102254.3102272,
author = {Weichselbraun, Albert and Kuntschik, Philipp},
title = {Mitigating Linked Data Quality Issues in Knowledge-Intense Information Extraction Methods},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102272},
doi = {10.1145/3102254.3102272},
abstract = {Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications.This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {17},
numpages = {12},
keywords = {information extraction, mitigation strategies, named entity linking, applications, linked data quality, semantic technologies},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3209281.3209300,
author = {Chotvijit, Sarunkorn and Thiarai, Malkiat and Jarvis, Stephen},
title = {Big Data Analytics in Social Care Provision: Spatial and Temporal Evidence from Birmingham},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209300},
doi = {10.1145/3209281.3209300},
abstract = {There is significant national interest in tackling issues surrounding the needs of vulnerable children and adults. At the same time, UK local authorities face severe financial challenges as a result of decreasing financial settlements and increasing demands from growing urban populations. This research employs state-of-the-art data analytics and visualisation techniques to analyse six years of local government social care data for the city of Birmingham, the UK's second most populated city. This analysis identifies: (i) service cost profiles over time; (ii) geographical dimensions to service demand and delivery; (iii) patterns in the provision of services, and (iv) the extent to which data value and data protection interact. The research accesses data held by the local authority to discover patterns and insights that may assist in the understanding of service demand, support decision making and resource management, whilst protecting and safeguarding its most vulnerable citizens. The use of data in this manner could also inform the approach a local authority has to its data, its capture and use, and the potential for supporting data-led management and service improvements.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {5},
numpages = {8},
keywords = {local authority, Birmingham, service provision, data analytics, spatio-temporal analysis, social care},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3107514.3107518,
author = {Wu, Jinrong and Sinnott, Richard O. and Effendy, Jemie and Gl\"{o}ckner, Stephan and Hu, William and Li, Jiajie},
title = {Usage Patterns and Data Quality: A Case Study of a National Type-1 Diabetes Study},
year = {2017},
isbn = {9781450352246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107514.3107518},
doi = {10.1145/3107514.3107518},
abstract = {The Environmental Determinants of Islet Auto- immunity (ENDIA) project is Australia's largest study into the causes of Type-1 Diabetes (T1D). The ENDIA study is supported by a Cloud-based software platform including a clinical registry comprising extensive longitudinal information on families at risk of having a child that might go on to develop T1D. This registry includes both demographic and clinical information on families and children as well as the environmental factors that might influence the onset of T1D. A multitude of samples are obtained through the study and used to support a diverse portfolio of bioinformatics data analytics. The quality of the data in the registry is essential to the overall success of the project. This paper presents a Cloud-based log-analytics platform that supports the detailed analysis of patterns of usage of the registry by the clinical centres and collaborators involved. We explore the impact that the usage patterns have on the overall data quality. We also consider ways of improving data quality by mothers entering their own data through targeted mobile applications that have been developed for dietary data collection.},
booktitle = {Proceedings of the 1st International Conference on Medical and Health Informatics 2017},
pages = {18–27},
numpages = {10},
keywords = {Cloud, auditing, Type-1 diabetes, log analysis},
location = {Taichung City, Taiwan},
series = {ICMHI '17}
}

@inproceedings{10.1145/2695664.2695753,
author = {Nascimento, Dimas C. and Pires, Carlos Eduardo and Mestre, Demetrio Gomes},
title = {A Data Quality-Aware Cloud Service Based on Metaheuristic and Machine Learning Provisioning Algorithms},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695753},
doi = {10.1145/2695664.2695753},
abstract = {Cloud Computing as a service has become a topic of increasing interest. The outsourcing of duties and infrastructure to external parties became a crucial concept for many business models. In this paper we discuss the design and experimental evaluation of provisioning algorithms, in a Data Quality-aware Service (DQaS) context, that enables dynamic Data Quality Service Level Agreements (DQSLA) management and optimization of cloud resources. The DQaS has been designed to respond effectively to the DQSLA requirements of the service customers, by minimizing SLA penalties and provisioning the cloud infrastructure for the execution of data quality algorithms. An experimental evaluation of the proposed provisioning algorithms, carried out through simulation, has provided very encouraging results that confirm the adequacy of these algorithms in the DQaS context.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1696–1703},
numpages = {8},
keywords = {metaheuristic, machine learning, data quality, cloud computing, provisioning},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3465631.3465664,
author = {Yu, Xiaomu and Yin, Yuelin},
title = {Application Strategies of Medical Big Data in Health Economic Management},
year = {2021},
isbn = {9781450385015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465631.3465664},
doi = {10.1145/3465631.3465664},
abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.},
booktitle = {The Sixth International Conference on Information Management and Technology},
articleno = {33},
numpages = {5},
location = {Jakarta, Indonesia},
series = {ICIMTECH 21}
}

@article{10.1145/3513135,
author = {Santoro, Donatello and Thirumuruganathan, Saravanan and Papotti, Paolo},
title = {Editorial: Special Issue on Deep Learning for Data Quality},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3513135},
doi = {10.1145/3513135},
abstract = {This editorial summarizes the content of the Special Issue on Deep Learning for Data Quality of the Journal of Data and Information Quality (JDIQ).},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {14},
numpages = {3},
keywords = {data labeling, Deep learning, schema matching}
}

@inproceedings{10.1145/3365871.3365900,
author = {Papst, Franz and Saukh, Olga and R\"{o}mer, Kay and Grandl, Florian and Jakovljevic, Igor and Steininger, Franz and Mayerhofer, Martin and Duda, J\"{u}rgen and Egger-Danner, Christa},
title = {Embracing Opportunities of Livestock Big Data Integration with Privacy Constraints},
year = {2019},
isbn = {9781450372077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365871.3365900},
doi = {10.1145/3365871.3365900},
abstract = {Today's herd management undergoes a major transformation triggered by the penetration of cheap sensor solutions into cattle farms, and the promise of predictive analytics to detect animal health issues and product-related problems before they occur. The latter is particularly important to prevent disease spread, ensure animal health, animal welfare and product quality. Sensor businesses entering the market tend to build their solutions as end-to-end pipelines spanning sensors, proprietary algorithms, cloud services, and mobile apps. Since data privacy is an important issue in this industry, as a result, disconnected data silos, heterogeneity of APIs, and lack of common standards limit the value the sensor technologies could provide for herd management. In the last few years, researchers and communities proposed a number of data integration architectures to enable exchange between streams of sensor data. This paper surveys the existing efforts and outlines the opportunities they fail to address by treating sensor data as a black box. We discuss alternative solutions to the problem based on privacy-preserving collaborative learning, and provide a set of scenarios to show their benefits for both farmers and businesses.},
booktitle = {Proceedings of the 9th International Conference on the Internet of Things},
articleno = {27},
numpages = {4},
keywords = {data privacy, privacy-preserving data analysis, agriculture},
location = {Bilbao, Spain},
series = {IoT 2019}
}

@inproceedings{10.1145/3209914.3234639,
author = {Song, Zhendong},
title = {Application of Big Data and Intelligent Processing Technology in Modern Chinese Multi-Category Words Part of Speech Tagging Corpus},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3234639},
doi = {10.1145/3209914.3234639},
abstract = {The application of modern Chinese multi-category words corpus is very wide. With the development of the Internet, data from the corpus is getting bigger and bigger during collection. The data gradually develops so big that the current relational database is difficult to deal with them. This article analyzes the important role of the big data technology in corpu},
booktitle = {Proceedings of the 2018 International Conference on Information Science and System},
pages = {107–111},
numpages = {5},
keywords = {Intelligent processing, Corpus, Multi-category words, Tagging, Big data},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@article{10.1145/3420038,
author = {Liu, Yu and Wang, Yangtao and Gao, Lianli and Guo, Chan and Xie, Yanzhao and Xiao, Zhili},
title = {Deep Hash-Based Relevance-Aware Data Quality Assessment for Image Dark Data},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1922},
url = {https://doi.org/10.1145/3420038},
doi = {10.1145/3420038},
abstract = {Data mining can hardly solve but always faces a problem that there is little meaningful information within the dataset serving a given requirement. Faced with multiple unknown datasets, to allocate data mining resources to acquire more desired data, it is necessary to establish a data quality assessment framework based on the relevance between the dataset and requirements. This framework can help the user to judge the potential benefits in advance, so as to optimize the resource allocation to those candidates. However, the unstructured data (e.g., image data) often presents dark data states, which makes it tricky for the user to understand the relevance based on content of the dataset in real time. Even if all data have label descriptions, how to measure the relevance between data efficiently under semantic propagation remains an urgent problem. Based on this, we propose a Deep Hash-based Relevance-aware Data Quality Assessment framework, which contains off-line learning and relevance mining parts as well as an on-line assessing part. In the off-line part, we first design a Graph Convolution Network (GCN)-AutoEncoder hash (GAH) algorithm to recognize the data (i.e., lighten the dark data), then construct a graph with restricted Hamming distance, and finally design a Cluster PageRank (CPR) algorithm to calculate the importance score for each node (image) so as to obtain the relevance representation based on semantic propagation. In the on-line part, we first retrieve the importance score by hash codes and then quickly get the assessment conclusion in the importance list. On the one hand, the introduction of GCN and co-occurrence probability in the GAH promotes the perception ability for dark data. On the other hand, the design of CPR utilizes hash collision to reduce the scale of graph and iteration matrix, which greatly decreases the consumption of space and computing resources. We conduct extensive experiments on both single-label and multi-label datasets to assess the relevance between data and requirements as well as test the resources allocation. Experimental results show our framework can gain the most desired data with the same mining resources. Besides, the test results on Tencent1M dataset demonstrate the framework can complete the assessment with a stability for given different requirements.},
journal = {ACM/IMS Trans. Data Sci.},
month = {apr},
articleno = {11},
numpages = {26},
keywords = {CPR, GAH, relevance, data quality assessment, data mining, Resource allocation}
}

@inproceedings{10.1145/3468945.3468964,
author = {Hou, Hanfang and Fu, Qiang and Zhang, Yang},
title = {An Empirical Study on the Classification, Grading, Sharing and Opening of Healthcare Big Data Based on Current Policies and Standards},
year = {2021},
isbn = {9781450390057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468945.3468964},
doi = {10.1145/3468945.3468964},
abstract = {This paper expounds the connotations and features of healthcare big data, as well as the concepts and logical relations for its classification, grading, sharing and opening. It also summarizes overseas policies related thereto and their status quo, and emphatically introduces main policies, laws, regulations and national standards regarding the classification, grading, sharing and opening of healthcare big data in China, as well as practices in three places represented by Shandong Province, Guangdong Province and Fuzhou. Finally, principles and practical suggestions for the classification, grading, sharing and opening of healthcare big data were proposed based on current polices, regulations and standards},
booktitle = {2021 3rd International Conference on Intelligent Medicine and Image Processing},
pages = {116–121},
numpages = {6},
keywords = {Healthcare big data, Opening, Classification, Grading, Sharing},
location = {Tianjin, China},
series = {IMIP '21}
}

@inproceedings{10.1145/3437120.3437352,
author = {Markopoulos, Dimitris and Tsolakidis, Anastasios and N. Karanikolas, Nikitas and Skourlas, Christos},
title = {Towards the Design of a Conceptual Framework for the Operation of Intensive Care Units Based on Big Data Analysis},
year = {2020},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437352},
doi = {10.1145/3437120.3437352},
abstract = {The development of Big Data Analytics (BDA) technology and the maturity of the Machine Learning (ML) sector offer great opportunities for applications in Intensive Care Units (ICUs). This paper describes a Conceptual Framework and proposes its use in designing architectures and big data applications in ICUs. The Conceptual Framework is based on BDA,MLNatural Language Processing (NLP) and consists of the following subsystems: The "Big Data Integration and ICUs" module, the "ICUs and critical care services" module, the "Use of standards and ICUs" module, the "Machine Learning and ICUs" module, and the “NLP and ICUs” module. The framework is developed using Soft System Methodology (SSM) and Design Science Research Methodology (DSRM).},
booktitle = {24th Pan-Hellenic Conference on Informatics},
pages = {411–415},
numpages = {5},
keywords = {Intensive Care Unit, Conceptual Framework, Machine Learning, Big Data Analysis},
location = {Athens, Greece},
series = {PCI 2020}
}

@inproceedings{10.1145/3018009.3018040,
author = {Xu, Gang and Wu, Shunyu and Xie, Pengfei},
title = {Integration and Exchange Method of Multi-Source Heterogeneous Big Data for Intelligent Power Distribution and Utilization},
year = {2016},
isbn = {9781450348195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018009.3018040},
doi = {10.1145/3018009.3018040},
abstract = {With the development of smart grid and big data technologies, the stability and economy of distribution network operation are enhanced effectively. Intelligent power distribution and utilization (IPDU) big data platform, which exchanges operation data with other related distribution network management systems, makes decisions for demand side management, power system and distributed energy operation strategies by analyzing the big data. In order to solve the data fusion and exchange problems among all information systems, we proposed a kind of general information model for multi-source heterogeneous big data. In addition, a data fusion and exchange mechanism is established based on circle buffer to ensure the data quality. Finally, this paper demonstrates the effective of the method of IPDU big data fusion method by the example of distribution network reconfiguration. The method proposed in this paper can satisfy the data exchanging demands of future smart grid and demand side management, and it also has good confluent and extensible feature.},
booktitle = {Proceedings of the 2nd International Conference on Communication and Information Processing},
pages = {38–42},
numpages = {5},
keywords = {information model, multi-source and heterogeneous, intelligent power distribution and utilization, data fusion and exchange},
location = {Singapore, Singapore},
series = {ICCIP '16}
}

@article{10.1145/2627534.2627558,
author = {Sharma, Abhishek B. and Ivan\v{c}i\'{c}, Franjo and Niculescu-Mizil, Alexandru and Chen, Haifeng and Jiang, Guofei},
title = {Modeling and Analytics for Cyber-Physical Systems in the Age of Big Data},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627558},
doi = {10.1145/2627534.2627558},
abstract = {In this position paper we argue that the availability of "big" monitoring data on Cyber-Physical Systems (CPS) is challenging the traditional CPS modeling approaches by violating their fundamental assumptions. However, big data alsobrings unique opportunities in its wake by enabling new modeling and analytics approaches as well as facilitating novel applications. We highlight a few key challenges andopportunities, and outline research directions for addressing them. To provide a proper context, we also summarize CPS modeling approaches, and discuss how modeling and analytics for CPS differs from general purpose IT systems.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {74–77},
numpages = {4}
}

@inproceedings{10.1145/3510858.3511394,
author = {Chen, Xiaoyu},
title = {Research on Visual Analysis Method of Food Safety Big Data Based on Artificial Intelligence},
year = {2021},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511394},
doi = {10.1145/3510858.3511394},
abstract = {In the background of today's big data era, the popularization of computer network and information technology helps us to understand and disseminate the hot information in the current society more quickly. By quickly understanding these hot issues, we can better supervise and prevent these problems in our life. In recent years, the problem of food safety appears frequently in our field of vision, which makes people have to regard food safety as a hot issue in today's social development. The state and relevant food safety supervision departments are also paying attention to the food safety problems. In order to better supervise food safety issues in this era of big data, this paper will analyze and study food safety issues with the help of popular technologies in the new era, such as artificial intelligence technology and big data technology, so as to formulate a new scheme to meet the needs of people in the new era for food safety supervision. Through the research, it can be found that a series of methods proposed in this paper can effectively provide new ideas for food safety big data visual analysis research method based on artificial intelligence.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {815–819},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3398329.3398330,
author = {Qin, Yana and Yang, Haolin and Guo, Mengjie and Guo, Meicheng},
title = {An Advanced Data Science Model Based on Big Data Analytics for Urban Driving Cycle Construction in China},
year = {2020},
isbn = {9781450377713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3398329.3398330},
doi = {10.1145/3398329.3398330},
abstract = {In recent years, with the rapid growth of car ownership, Chinese road traffic conditions have changed a lot. Governments, enterprises, and the public are increasingly finding that the increasing deviation between the actual fuel consumption and the results of the regulatory certification based on NEDC (New European Driving Cycle). In addition, this deviation has seriously affected the credibility of the government, energy saving and emission reduction of automobiles and environmental pollution. Thus, need to improve urban driving cycle construction methods to adapt the Chinese traffic and automobiles driving cycles.This paper proposes an advanced data science model based on big data analysis for accurate urban driving cycle construction in Chinese cities. In addition, we conduct a lot of data analysis and statistics. Then we design a data preprocessing method for cleaning the noise data to use in driving cycle construction. Extensive experiments and analysis on real-world datasets demonstrate that the proposed methods can significantly reduce the impact of missing and abnormal data on microtrips segmentation, and thus the proposed methods can be used for driving cycle construction in China more accurately.},
booktitle = {Proceedings of the 2020 International Conference on Computing, Networks and Internet of Things},
pages = {1–7},
numpages = {7},
keywords = {Big data analytics, data preprocessing, urban driving cycle construction, PCA, feature engineering},
location = {Sanya, China},
series = {CNIOT2020}
}

@inproceedings{10.1145/2797433.2797467,
author = {Collins, Graham and Varilly, Hugh and Yoshinori, Tanabe},
title = {Pedagogical Lessons from an International Collaborative Big Data Undergraduate Research Project},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797467},
doi = {10.1145/2797433.2797467},
abstract = {This experience report covers the collaboration between UCL and NII Tokyo students in development of data analytics research projects: the challenges, contributing student pedagogy and changes to teaching. Students are often taught technology management separate from other computing modules. The teaching team designed a more coherent learning experience linking the technology management teaching more closely to engineering processes, specifically to engage students whose interest lies more with computing. This project has given rise to a re-evaluation of how technology management is taught to undergraduate students, adoption of architecture as a key aspect and inclusion of students with different levels of academic attainment within a class.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {32},
numpages = {6},
keywords = {software architecture, contributing student pedagogy, retrospectives, Data analytics, peer assessment},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/3514221.3522568,
author = {Li, Huan and Tang, Bo and Lu, Hua and Cheema, Muhammad Aamir and Jensen, Christian S.},
title = {Spatial Data Quality in the IoT Era: Management and Exploitation},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3522568},
doi = {10.1145/3514221.3522568},
abstract = {Within the rapidly expanding Internet of Things (IoT), growing amounts of spatially referenced data are being generated. Due to the dynamic, decentralized, and heterogeneous nature of the IoT, spatial IoT data (SID) quality has attracted considerable attention in academia and industry. How to invent and use technologies for managing spatial data quality and exploiting low-quality spatial data are key challenges in the IoT. In this tutorial, we highlight the SID consumption requirements in applications and offer an overview of spatial data quality in the IoT setting. In addition, we review pertinent technologies for quality management and low-quality data exploitation, and we identify trends and future directions for quality-aware SID management and utilization. The tutorial aims to not only help researchers and practitioners to better comprehend SID quality challenges and solutions, but also offer insights that may enable innovative research and applications.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {2474–2482},
numpages = {9},
keywords = {geo-sensory data, internet of things, quality management},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3325112.3325212,
author = {Hagen, Loni and Seon Yi, Hye and Pietri, Siana and E. Keller, Thomas},
title = {Processes, Potential Benefits, and Limitations of Big Data Analytics: A Case Analysis of 311 Data from City of Miami},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325212},
doi = {10.1145/3325112.3325212},
abstract = {As part of the open government movement, an increasing number of 311 call centers have made their datasets available to the public. Studies have found that 311 request patterns are associated with personal attributes and living conditions. Most of these studies used New York City 311 data. In this study, we use 311 data from the City of Miami, a smaller local government, as a case study. This study contributes to digital government research and practices by making suggestions on best practices regarding the use of big data analytics on 311 data. In addition, we discuss limitations of 311 data and analytics results. Finally, we expect our results to inform decision making within the City of Miami government and other local governments.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {1–10},
numpages = {10},
keywords = {information visualization, big data analytics, 311 data, e-government},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1109/CCGrid.2015.46,
author = {Peng, Shaoliang and Liao, Xiangke and Yang, Canqun and Lu, Yutong and Liu, Jie and Cui, Yingbo and Wang, Heng and Wu, Chengkun and Wang, Bingqiang},
title = {The Challenge of Scaling Genome Big Data Analysis Software on TH-2 Supercomputer},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.46},
doi = {10.1109/CCGrid.2015.46},
abstract = {Whole genome re-sequencing plays a crucial role in biomedical studies. The emergence of genomic big data calls for an enormous amount of computing power. However, current computational methods are inefficient in utilizing available computational resources. In this paper, we address this challenge by optimizing the utilization of the fastest supercomputer in the world - TH-2 supercomputer. TH-2 is featured by its neo-heterogeneous architecture, in which each compute node is equipped with 2 Intel Xeon CPUs and 3 Intel Xeon Phi coprocessors. The heterogeneity and the massive amount of data to be processed pose great challenges for the deployment of the genome analysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming components (up to 70% of total runtime) in a typical genome-analyzing pipeline. To optimize the whole pipeline, we first devise a number of parallel and optimization strategies for SOAP3-dp and SOAPsnp, respectively targeting each node to fully utilize all sorts of hardware resources provided both by CPU and MIC. We also employ a few scaling methods to reduce communication between different nodes. We then scaled up our method on TH-2. With 8192 nodes, the whole analyzing procedure took 8.37 hours to finish the analysis of a 300 TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {823–828},
numpages = {6},
keywords = {TH-2 supercomputer, whole genome re-sequencing, parallel optimization, SNP detection, sequence alignment},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3482632.3484095,
author = {Wu, Rui and Cheng, Qian and He, Lisong and Cao, Zhenyu},
title = {Environmental Big Data Model and Recognition of Abnormal Emission from Enterprise Data},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484095},
doi = {10.1145/3482632.3484095},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2041–2046},
numpages = {6},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3472163.3472185,
author = {Zhao, Yan and Megdiche, Imen and Ravat, Franck and Dang, Vincent-nam},
title = {A Zone-Based Data Lake Architecture for IoT, Small and Big Data},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472185},
doi = {10.1145/3472163.3472185},
abstract = {Data lakes are supposed to enable analysts to perform more efficient and efficacious data analysis by crossing multiple existing data sources, processes and analyses. However, it is impossible to achieve that when a data lake does not have a metadata governance system that progressively capitalizes on all the performed analysis experiments. The objective of this paper is to have an easily accessible, reusable data lake that capitalizes on all user experiences. To meet this need, we propose an analysis-oriented metadata model for data lakes. This model includes the descriptive information of datasets and their attributes, as well as all metadata related to the machine learning analyzes performed on these datasets. To illustrate our metadata solution, we implemented a web application of data lake metadata management. This application allows users to find and use existing data, processes and analyses by searching relevant metadata stored in a NoSQL data store within the data lake. To demonstrate how to easily discover metadata with the application, we present two use cases, with real data, including datasets similarity detection and machine learning guidance.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {94–102},
numpages = {9},
keywords = {Metadata, Technical Architecture, Zone-based, Data Lake, Stream IoT Data},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@article{10.1109/TNET.2019.2934026,
author = {Gong, Xiaowen and Shroff, Ness B.},
title = {Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2934026},
doi = {10.1109/TNET.2019.2934026},
abstract = {Mobile crowdsensing has found a variety of applications e.g., spectrum sensing, environmental monitoring by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users’ data e.g., users’ received SNRs for measuring a transmitter’s transmit signal strength. However, the quality of a user can be its private information which, e.g., may depend on the user’s location that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data’s accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data’s accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation QEE, which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user’s data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester’s optimal RO effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user’s quality and the quality’s distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.},
journal = {IEEE/ACM Trans. Netw.},
month = {oct},
pages = {1959–1972},
numpages = {14}
}

@inproceedings{10.1145/3361821.3361825,
author = {Podhoranyi, Michal and Vojacek, Lukas},
title = {Social Media Data Processing Infrastructure by Using Apache Spark Big Data Platform: Twitter Data Analysis},
year = {2019},
isbn = {9781450372411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361821.3361825},
doi = {10.1145/3361821.3361825},
abstract = {Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.},
booktitle = {Proceedings of the 2019 4th International Conference on Cloud Computing and Internet of Things},
pages = {1–6},
numpages = {6},
keywords = {Twitter, data processing architecture, Apache Spark, social network data},
location = {Tokyo, Japan},
series = {CCIOT 2019}
}

@article{10.1145/3498338,
author = {Li, Huan and Lu, Hua and Jensen, Christian S. and Tang, Bo and Cheema, Muhammad Aamir},
title = {Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3498338},
doi = {10.1145/3498338},
abstract = {With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {57},
numpages = {41},
keywords = {spatiotemporal dependencies, geo-sensory data, Internet of Things, location refinement, quality management, spatial queries, spatial computing, spatiotemporal data cleaning}
}

@inproceedings{10.1145/3482632.3484007,
author = {Li, Jicai and Liu, Dan},
title = {A Method of Constructing Distributed Big Data Analysis Model for Machine Learning Based on Cloud Computing},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484007},
doi = {10.1145/3482632.3484007},
abstract = {There are many big data analysis methods, and it is an effective method to build big data analysis model through machine learning. Big data is characterized by large data scale and long calculation cycle. In order to speed up the calculation speed and shorten the calculation cycle, distributed computing method is one of the effective methods to solve the above problems. With the wide application and rapid development of information technology, cloud computing as a new business computing model has attracted more and more attention. However, the security of cloud computing data storage model lacks reliability. Under the mainstream cloud computing and big data basic environment, building a better model from resource aggregation to analysis and mining, and modeling distributed big data analysis can provide high-reliability, high-security, high-efficiency analysis services for practical analysis and mining applications such as intelligence judgment, information deployment and control, stakeholder analysis and intelligent decision-making.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1634–1638},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3495018.3495364,
author = {Ding, Gaohu},
title = {Optimization of Modern Teaching System with Computer Technology under the Background of Big Data},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495364},
doi = {10.1145/3495018.3495364},
abstract = {The rapid development of science and technology has promoted the rapid development and wide application of CT(computer Technology), especially the emergence of BDT(big data technology) in recent years. The integration and improvement of CT and it has promoted great changes in all aspects of society, and these changes are beneficial, and they have brought us great convenience and help Help. For the education industry, under the background of BDT, the integration of CT and modern teaching system can provide new development thinking and new direction for the reform of modern education. In order to study what effect the combination of the two will bring, this paper selects two universities and their students as the experimental research objects to explore how the modern teaching system will be innovated and developed under the effect of this new technology. The experimental results show that the students of a university who have applied CT in the modern teaching system have a high degree of satisfaction, and the percentage of those who are satisfied has reached 67%. Moreover, the score of teaching and research group of a university is relatively high, and the highest score is 95.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1197–1200},
numpages = {4},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3482632.3487514,
author = {Wang, Wenwen},
title = {Research on the Application of Big Data Cloud Cleaning System in Physical Function Sports Training Management},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487514},
doi = {10.1145/3482632.3487514},
abstract = {The main purpose of physical function training is to solve the problems of many injuries, incorrect movement patterns, movement compensation and so on. In the era of big data, the scientific research field of sports training is gradually beginning to adopt the big data model for development, which will better form a new insight after data generation, collection, analysis and transformation. Therefore, this paper analyzes the application of big data intelligent cleaning system based on cloud computing in physical function training management. Students can quickly query running results through WEB query system, and administrators can download and modify students' results through WEB system. By using big data analysis technology, students are grouped according to their physical fitness test scores and BMI index, and their sports plans are formulated accordingly. A high-reliability real-time wireless receiving and sending sports monitoring system for big data is adopted, which realizes the functions of networking, unmanned and intelligent sports management.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2783–2786},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3148055.3148072,
author = {Abdullah, Tariq and Ahmet, Ahmed},
title = {Genomics Analyser: A Big Data Framework for Analysing Genomics Data},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148072},
doi = {10.1145/3148055.3148072},
abstract = {Abstract Genomics data is unstructured and mostly stored on hard disks. It is both technically and culturally residing in big data domain due to the challenges of volume, velocity and variety. Huge volumes of data are generated from diverse sources in different formats and at a high frequency. Appropriate data models are required to accommodate these data formats for analysing and producing required results with a quick response time. Genomics data can be analysed for a variety of purposes. Existing genomics data analysis pipelines are disk I/O intensive and focus on optimizing data processing for individual analysis tasks. Intensive disk I/O operations and focus on optimizing individual analysis tasks are the biggest bottleneck of existing genomics analysis pipelines. Making any updates in genomics data require reading the whole data set again. In this paper, we present a genomics data analysis framework that addresses both the issues of existing genomics analysis pipelines. It reads unstructured genomics data from sources, transforms it in a structured format and stores this data into a NoSQL database. In this way, genomics data can be queried like any other data and an update in the genomics data does not require reading the whole data set. The framework also presents an efficient analysis pipeline for analysing the genomics data for a variety of purposes like genotype clustering, gene expression microarrays, chromosome variations or gene linkage analysis. A case study of genotype clustering is presented to demonstrate and evaluate the effectiveness of the presented framework. Our results show that the framework improves overall performance of the genomics data analysis pipeline by 49% from existing genomics data analysis pipelines. Furthermore, our approach is robust and is able sustain high performance with high system workloads.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {189–197},
numpages = {9},
keywords = {resource management, data analysis, population scale clustering, compute cluster, machine learning, in-memory computing, big data, algorithms},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3530050.3532928,
author = {Paasche, Simon and Groppe, Sven},
title = {Enhancing Data Quality and Process Optimization for Smart Manufacturing Lines in Industry 4.0 Scenarios},
year = {2022},
isbn = {9781450393461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530050.3532928},
doi = {10.1145/3530050.3532928},
abstract = {An essential component of today's industry is data, which is generated during manufacturing. The goal of industry 4.0 is efficient collection, processing and analysis of this data. In our work, we address these three tasks and present an extensible system to solve them. To the best of our knowledge, the combination of a consistency checker (CC) for data preparation and a digital twin (DT) for analysis activities represents a novel approach. Consistency checking in combination with a DT leads to increased data quality, which in turn has a positive effect on analyses, like reducing errors to decrease costs, identifying relevant parameters to increase the productivity, and determining the bottleneck of a manufacturing line for enhanced production planning.},
booktitle = {Proceedings of The International Workshop on Big Data in Emergent Distributed Environments},
articleno = {9},
numpages = {7},
keywords = {digital twin, consistency checking, industry 4.0},
location = {Philadelphia, Pennsylvania},
series = {BiDEDE '22}
}

@article{10.1145/3507467,
author = {Shen, Yanyan and Dinh, Anh and Jagadish, H. V.},
title = {Introduction to the Special Issue on Data Science for Next Generation Big Data},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1922},
url = {https://doi.org/10.1145/3507467},
doi = {10.1145/3507467},
journal = {ACM/IMS Trans. Data Sci.},
month = {mar},
articleno = {31},
numpages = {2}
}

@inproceedings{10.1145/3457784.3457816,
author = {Farhana Jamaludin, Ain and Najib Razali, Muhammad and jalil, Rohaya and Othman, Hajar and Adnan, Yasmin},
title = {Identification of Business Intelligence in Big Data Maintenance of Government Sector in Putrajaya},
year = {2021},
isbn = {9781450388825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457784.3457816},
doi = {10.1145/3457784.3457816},
abstract = {This paper contributes significantly, which focuses on an intelligent system that lets the government make an integral part of decision-making and can be applied horizontally to solve the problems in maintenance practice through business intelligence. Accordingly, a real-time data management system for maintenance management is proposed in this paper. It looks at a real case study highlighting the need for proper data management in the government sector. Our findings bridge the gap of information technology inserted in government office buildings, with maintenance management being the domain. This paper demonstrates the underlying structure of the developed simulation model.},
booktitle = {2021 10th International Conference on Software and Computer Applications},
pages = {201–207},
numpages = {7},
keywords = {Maintenance Management, Data Management,, Business Intelligence},
location = {Kuala Lumpur, Malaysia},
series = {ICSCA 2021}
}

@inproceedings{10.1145/3278312.3278316,
author = {Win, Thee Zin and Kham, Nang Saing Moon},
title = {Mutual Information-Based Feature Selection Approach to Reduce High Dimension of Big Data},
year = {2018},
isbn = {9781450365567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278312.3278316},
doi = {10.1145/3278312.3278316},
abstract = {As increasing the massive amount of data demands effective and efficient mining strategies, practitioners and researchers are trying to develop scalable mining algorithms, machine learning algorithms and strategies to be successful data mining in turning mountains of data into nuggets. Data of high dimension significantly increases the memory storage requirements and computational costs for data analytics. Therefore, reducing dimension can mainly improve three data mining performance: speed of learning, predictive accuracy and simplicity and comprehensibility of mined result. Feature selection, data preprocessing technique, is effective and efficient in data mining, data analytics and machine learning problems particularly in high dimension reduction. Most feature selection algorithms can eliminate only irrelevant features but redundant features. Not only irrelevant features but also redundant features can degrade learning performance. Mutual information measured feature selection is proposed in this work to remove both irrelevant and redundant features.},
booktitle = {Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence},
pages = {3–7},
numpages = {5},
keywords = {High Dimensional Data, Mutual Information, Feature Selection, Redundant Features},
location = {Ha Noi, Viet Nam},
series = {MLMI2018}
}

@inproceedings{10.1145/3468264.3468613,
author = {Wang, Zehao and Zhang, Haoxiang and Chen, Tse-Hsun (Peter) and Wang, Shaowei},
title = {Would You like a Quick Peek? Providing Logging Support to Monitor Data Processing in Big Data Applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468613},
doi = {10.1145/3468264.3468613},
abstract = {To analyze large-scale data efficiently, developers have created various big data processing frameworks (e.g., Apache Spark). These big data processing frameworks provide abstractions to developers so that they can focus on implementing the data analysis logic. In traditional software systems, developers leverage logging to monitor applications and record intermediate states to assist workload understanding and issue diagnosis. However, due to the abstraction and the peculiarity of big data frameworks, there is currently no effective monitoring approach for big data applications. In this paper, we first manually study 1,000 randomly sampled Spark-related questions on Stack Overflow to study their root causes and the type of information, if recorded, that can assist developers with motioning and diagnosis. Then, we design an approach, DPLOG, which assists developers with monitoring Spark applications. DPLOG leverages statistical sampling to minimize performance overhead and provides intermediate information and hint/warning messages for each data processing step of a chained method pipeline. We evaluate DPLOG on six benchmarking programs and find that DPLOG has a relatively small overhead (i.e., less than 10% increase in response time when processing 5GB data) compared to without using DPLOG, and reduce the overhead by over 500% compared to the baseline. Our user study with 20 developers shows that DPLOG can reduce the needed time to debug big data applications by 63% and the participants give DPLOG an average of 4.85/5 for its usefulness. The idea of DPLOG may be applied to other big data processing frameworks, and our study sheds light on future research opportunities in assisting developers with monitoring big data applications.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {516–526},
numpages = {11},
keywords = {Logging, Monitoring, Apache Spark},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3132847.3133187,
author = {Wang, Hongzhi and Ding, Xiaoou and Chen, Xiangying and Li, Jianzhong and Gao, Hong},
title = {CleanCloud: Cleaning Big Data on Cloud},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133187},
doi = {10.1145/3132847.3133187},
abstract = {We describe CleanCloud, a system for cleaning big data based on Map-Reduce paradigm in cloud. Using Map-Reduce paradigm, the system detects and repairs various data quality problems in big data. We demonstrate the following features of CleanCloud: (a) the support for cleaning multiple data quality problems in big data; (b) a visual tool for watching the status of big data cleaning process and tuning the parameters for data cleaning; (c) the friendly interface for data input and setting as well as cleaned data collection for big data. CleanCloud is a promising system that provides scalable and effect data cleaning mechanism for big data in either files or databases.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2543–2546},
numpages = {4},
keywords = {data cleaning, parallel computing, entity resolution},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3482632.3483061,
author = {Han, Caibao},
title = {The Architecture and Security Design of Big Data Platform of the Physical Teaching Information System in Colleges and Universities},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3483061},
doi = {10.1145/3482632.3483061},
abstract = {This paper constructs the logic structure of big data platform of physical teaching information system in Colleges and universities through Hadoop distributed system, which ensures the realization of management and monitoring function, data collection function, data storage and query function, data calculation function, security and management function. The security design is carried out from access control, data authorization and management, Hadoop security configuration to deal with the possible security problems of big data platform.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {960–963},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inbook{10.5555/3042094.3042407,
author = {Chien, Chen-Fu and Chen, Ying-Jen and Wu, Jei-Zheng},
title = {Big Data Analytics for Modeling WAT Parameter Variation Induced by Process Tool in Semiconductor Manufacturing and Empirical Study},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {With the feature size shrinkage in advanced technology nodes, the modeling of process variations has become more critical for troubleshooting and yield enhancement. Misalignment among equipment tools or chambers in process stages is a major source of process variations. Because a process flow contains hundreds of stages during semiconductor fabrication, tool/chamber misalignment may more significantly affect the variation of transistor parameters in a wafer acceptance test. This study proposes a big data analytic framework that simultaneously considers the mean difference between tools and wafer-to-wafer variation and identifies possible root causes for yield enhancement. An empirical study was conducted to demonstrate the effectiveness of proposed approach and obtained promising results.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {2512–2522},
numpages = {11}
}

@article{10.1145/2996198,
author = {Shankaranarayanan, G. and Blake, Roger},
title = {From Content to Context: The Evolution and Growth of Data Quality Research},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2996198},
doi = {10.1145/2996198},
abstract = {Research in data and information quality has made significant strides over the last 20 years. It has become a unified body of knowledge incorporating techniques, methods, and applications from a variety of disciplines including information systems, computer science, operations management, organizational behavior, psychology, and statistics. With organizations viewing “Big Data”, social media data, data-driven decision-making, and analytics as critical, data quality has never been more important. We believe that data quality research is reaching the threshold of significant growth and a metamorphosis from focusing on measuring and assessing data quality—content—toward a focus on usage and context. At this stage, it is vital to understand the identity of this research area in order to recognize its current state and to effectively identify an increasing number of research opportunities within. Using Latent Semantic Analysis (LSA) to analyze the abstracts of 972 peer-reviewed journal and conference articles published over the past 20 years, this article contributes by identifying the core topics and themes that define the identity of data quality research. It further explores their trends over time, pointing to the data quality dimensions that have—and have not—been well-studied, and offering insights into topics that may provide significant opportunities in this area.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {9},
numpages = {28},
keywords = {information quality, Data quality, text mining}
}

@article{10.1145/3502771.3502781,
author = {Nguyen, Phu H. and Sen, Sagar and Jourdan, Nicolas and Cassoli, Beatriz and Myrseth, Per and Armendia, Mikel and Myklebust, Odd},
title = {Software Engineering and AI for Data Quality in Cyber- Physical Systems - SEA4DQ'21 Workshop Report},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3502771.3502781},
doi = {10.1145/3502771.3502781},
abstract = {Cyber-physical systems (CPS) have been developed in many industrial sectors and application domains in which the quality requirements of data acquired are a common factor. Data quality in CPS can deteriorate because of several factors such as sensor faults and failures due to operating in harsh and uncertain environments. How can software engineering and artificial intelligence (AI) help manage and tame data quality issues in CPS? This is the question we aimed to investigate in the SEA4DQ workshop. Emerging trends in software engineering need to take data quality management seriously as CPS are increasingly datacentric in their approach to acquiring and processing data along the edge-fog-cloud continuum. This workshop provided researchers and practitioners a forum for exchanging ideas, experiences, understanding of the problems, visions for the future, and promising solutions to the problems in data quality in CPS. Examples of topics include software/hardware architectures and frameworks for data quality management in CPS; software engineering and AI to detect anomalies in CPS data or to repair erroneous CPS data. SEA4DQ 2021, which took place on August 24th, 2021 was a satellite event of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC / FSE) 2021. The workshop attracted 35 international participants and was exciting with a great keynote, six excellent presentations, and concluded on a high note with a panel discussion. SEA4DQ was motivated by the common research interests from the EU projects for Zero-Defects Manufacturing such as InterQ and Dat4.Zero.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {26–29},
numpages = {4}
}

@article{10.1145/2935694.2935702,
author = {Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng},
title = {Cleanix: A Parallel Big Data Cleaning System},
year = {2016},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2935694.2935702},
doi = {10.1145/2935694.2935702},
abstract = {For big data, data quality problem is more serious. Big data cleaning system requires scalability and the abilityof handling mixed errors. Motivated by this, we develop Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. In this paper, we show the organization, data cleaning algorithms as well as the design of Cleanix.},
journal = {SIGMOD Rec.},
month = {may},
pages = {35–40},
numpages = {6}
}

@inproceedings{10.1145/2627770.2627776,
author = {Smith, Ken and Seligman, Len and Rosenthal, Arnon and Kurcz, Chris and Greer, Mary and Macheret, Catherine and Sexton, Michael and Eckstein, Adric},
title = {"Big Metadata": The Need for Principled Metadata Management in Big Data Ecosystems},
year = {2014},
isbn = {9781450329972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627770.2627776},
doi = {10.1145/2627770.2627776},
abstract = {Current big data ecosystems lack a principled approach to metadata management. This impedes large organizations' ability to share data and data preparation and analysis code, to integrate data, and to ensure that analytic code makes compatible assumptions with the data it uses. This use-case paper describes the challenges and an in-progress effort to address them. We present a real application example, discuss requirements for "big metadata" drawn from that example as well as other U.S. government analytic applications, and briefly describe an effort to adapt an existing open source metadata manager to support the needs of big data ecosystems.},
booktitle = {Proceedings of Workshop on Data Analytics in the Cloud},
pages = {1–4},
numpages = {4},
keywords = {Big data analytics, data integration, data discovery, metadata},
location = {Snowbird, UT, USA},
series = {DanaC'14}
}

@inproceedings{10.1145/2723372.2747646,
author = {Khayyat, Zuhair and Ilyas, Ihab F. and Jindal, Alekh and Madden, Samuel and Ouzzani, Mourad and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Yin, Si},
title = {BigDansing: A System for Big Data Cleansing},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2747646},
doi = {10.1145/2723372.2747646},
abstract = {Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big datasets. This presents a serious impediment since data cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general purpose data processing platforms, ranging from DBMSs to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both declaratively and procedurally, with no requirement of being aware of the underlying distributed platform. BigDansing takes these rules into a series of transformations that enable distributed computations and several optimizations, such as shared scans and specialized joins operators. Experimental results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more than two orders of magnitude without sacrificing the quality provided by the repair algorithms.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1215–1230},
numpages = {16},
keywords = {distributed data repair, cleansing abstraction, schema constraints, distributed data cleansing},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3359115.3359119,
author = {Wu, Jian and Kim, Kunho and Giles, C. Lee},
title = {CiteSeerX: 20 Years of Service to Scholarly Big Data},
year = {2019},
isbn = {9781450371841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359115.3359119},
doi = {10.1145/3359115.3359119},
abstract = {We overview CiteSeerX, the pioneer digital library search engine, that has been serving academic communities for more than 20 years (first released in 1998), from three perspectives. The system perspective summarizes its architecture evolution in three phases over the past 20 years. The data perspective describes how CiteSeerX has created searchable scholarly big datasets and made them freely available for multiple purposes. In order to be scalable and effective, AI technologies are employed in all essential modules. To effectively train these models, a sufficient amount of data has been labeled, which can then be reused for training future models. Finally, we discuss the future of CiteSeerX. Our ongoing work is to make CiteSeerX more sustainable. To this end, we are working to ingest all open access scholarly papers, estimated to be 30-40 million. Part of the plan is to discover dataset mentions and metadata in scholarly articles and make them more accessible via search interfaces. Users will have more opportunities to explore and trace datasets that can be reused and discover other datasets for new research projects. We summarize what was learned to make a similar system more sustainable and useful.},
booktitle = {Proceedings of the Conference on Artificial Intelligence for Data Discovery and Reuse},
articleno = {1},
numpages = {4},
keywords = {CiteSeerX, digital libraries, scholarly big data, search engines},
location = {Pittsburgh, Pennsylvania},
series = {AIDR '19}
}

@article{10.14778/3503585.3503602,
author = {Sinthong, Phanwadee and Patel, Dhaval and Zhou, Nianjun and Shrivastava, Shrey and Iyengar, Arun and Bhamidipaty, Anuradha},
title = {DQDF: Data-Quality-Aware Dataframes},
year = {2021},
issue_date = {December 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3503585.3503602},
doi = {10.14778/3503585.3503602},
abstract = {Data quality assessment is an essential process of any data analysis process including machine learning. The process is time-consuming as it involves multiple independent data quality checks that are performed iteratively at scale on evolving data resulting from exploratory data analysis (EDA). Existing solutions that provide computational optimizations for data quality assessment often separate the data structure from its data quality which then requires efforts from users to explicitly maintain state-like information. They demand a certain level of distributed system knowledge to ensure high-level pipeline optimizations from data analysts who should instead be focusing on analyzing the data. We, therefore, propose data-quality-aware dataframes, a data quality management system embedded as part of a data analyst's familiar data structure, such as a Python dataframe. The framework automatically detects changes in datasets' metadata and exploits the context of each of the quality checks to provide efficient data quality assessment on ever-changing data. We demonstrate in our experiment that our approach can reduce the overall data quality evaluation runtime by 40-80% in both local and distributed setups with less than 10% increase in memory usage.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {949–957},
numpages = {9}
}

@inproceedings{10.1145/3495018.3501149,
author = {Tang, Yan},
title = {Computer Assisted Design and Practice of GNSS Survey Course of Typical Tasks through Cloud Computing and Big Data Technology},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3501149},
doi = {10.1145/3495018.3501149},
abstract = {Under the background of typical task orientation in higher vocational education, it is an inevitable trend to carry out teaching reform of GNSS Survey for engineering survey major in higher vocational education. This paper breaks the traditional theoretical teaching system, reconstructs it from the aspects of teaching content selection, combination of theory and practice teaching system, considers the actual situation of students' weak theoretical foundation in higher vocational colleges, and the core technical ability needed for employment and post, optimizes the curriculum structure, integrates the theoretical knowledge system and practice teaching links, and meets the requirements of project-oriented curriculum standards.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {2605–2608},
numpages = {4},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1145/2992787,
author = {De, Sushovan and Hu, Yuheng and Meduri, Venkata Vamsikrishna and Chen, Yi and Kambhampati, Subbarao},
title = {BayesWipe: A Scalable Probabilistic Framework for Improving Data Quality},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2992787},
doi = {10.1145/2992787},
abstract = {Recent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like Conditional Functional Dependencies (which have to be provided by domain experts or learned from a clean sample of the database). In this article, we provide a method for correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable. We evaluate our methods over both synthetic and real data.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {5},
numpages = {30},
keywords = {offline and online cleaning, Data quality, statistical data cleaning}
}

