@inproceedings{10.5555/3400397.3400442,
author = {Vieira, Ant\'{o}nio A. C. and Dias, Lu\'{\i}s M. S. and Santos, Maribel Y. and Pereira, Guilherme A. B. and Oliveira, Jos\'{e} A.},
title = {Real-Time Supply Chain Simulation: A Big Data-Driven Approach},
year = {2019},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Simulation of Supply Chains comprises huge amounts of data, resulting in numerous entities flowing in the model. These networks are highly dynamic systems, where entities' relationships and other elements evolve with time, paving the way for real-time Supply Chain decision-support tools capable of using real data. In light of this, a solution comprising of a Big Data Warehouse to store relevant data and a simulation model of an automotive plant, are being developed. The purpose of this paper is to address the modelling approach, which allowed the simulation model to automatically adapt to the data stored in a Big Data Warehouse and thus adapt to new scenarios without manual intervention. The main characteristics of the conceived solution were demonstrated, with emphasis to the real-time and the ability to allow the model to load the state of the system from the Big Data Warehouse.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {548–559},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3386723.3387851,
author = {Tounsi, Youssef and Anoun, Houda and Hassouni, Larbi},
title = {CSMAS: Improving Multi-Agent Credit Scoring System by Integrating Big Data and the New Generation of Gradient Boosting Algorithms},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387851},
doi = {10.1145/3386723.3387851},
abstract = {Credit risk is one of the main risks facing banks and credit institutions, with the current progress in machine learning, artificial intelligence and big data. Recent research has proposed several systems for improving credit rating. In this paper, a new scalable credit scoring multi-agent system called "CSMAS" is introduced for the prediction of problems in data mining of credit scoring domain. This engine is built using a seven-layer multi-agent system architecture to generate a data mining process based on the coordination of intelligent agents. CSMAS performance is based on preprocessing and data forecasting. The first layer is designed to retrieve any data from various core banking systems, payment systems, credit Bureaus and external databases and data sources and to store it in big data platform. The second layer is devoted to three different subtasks; feature engineering, pre-processing data and integrating diverse datasets. While the third layer is dedicated to dealing with missing Values and treating outliers. In the fourth layer, the techniques of dimensionality reduction are used to reduce the number of features in the original set of features. The fifth layer is dedicated to build a model using the new generation of Gradient Boosting Algorithms (XGBoost, LightGBM and CatBoost) and make predictions. The sixth layer is designed for the model's evaluation. The seventh layer is made to perform the rating of new credit applicants. The performance of CSMAS is assessed using a large dataset of Home Credit Default Risk from Kaggle Challenge (307511 records) to evaluate the risk of a loan applicant as a major problem for banks. The results show that the CSMAS give relevant results. Therefore, the results indicated that the CSMAS can be further employed as a reliable tool to predict more complicated case in credit scoring.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {32},
numpages = {7},
keywords = {Credit Scoring, XgBoost, Multi-Agent System, CatBoost, LightGBM, Big Data},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@article{10.14778/3137765.3137781,
author = {Zhang, Mingming and Wo, Tianyu and Xie, Tao and Lin, Xuelian and Liu, Yaxiao},
title = {CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137781},
doi = {10.14778/3137765.3137781},
abstract = {As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet vehicles and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based on the collected data. CarStream has been deployed and maintained for three years in industrial usage, collecting over 40 terabytes of driving data. This paper shares our experiences on designing CarStream based on large-scale driving-data streams, and the lessons learned from the process of addressing the challenges in designing and maintaining CarStream.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1766–1777},
numpages = {12}
}

@article{10.14778/1687553.1687576,
author = {Cohen, Jeffrey and Dolan, Brian and Dunlap, Mark and Hellerstein, Joseph M. and Welton, Caleb},
title = {MAD Skills: New Analysis Practices for Big Data},
year = {2009},
issue_date = {August 2009},
publisher = {VLDB Endowment},
volume = {2},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1687553.1687576},
doi = {10.14778/1687553.1687576},
abstract = {As massive data acquisition and storage becomes increasingly affordable, a wide variety of enterprises are employing statisticians to engage in sophisticated data analysis. In this paper we highlight the emerging practice of Magnetic, Agile, Deep (MAD) data analysis as a radical departure from traditional Enterprise Data Warehouses and Business Intelligence. We present our design philosophy, techniques and experience providing MAD analytics for one of the world's largest advertising networks at Fox Audience Network, using the Greenplum parallel database system. We describe database design methodologies that support the agile working style of analysts in these settings. We present dataparallel algorithms for sophisticated statistical techniques, with a focus on density methods. Finally, we reflect on database system features that enable agile design and flexible algorithm development using both SQL and MapReduce interfaces over a variety of storage mechanisms.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1481–1492},
numpages = {12}
}

@inproceedings{10.1145/2767109.2770014,
author = {Abiteboul, Serge and Dong, Luna and Etzioni, Oren and Srivastava, Divesh and Weikum, Gerhard and Stoyanovich, Julia and Suchanek, Fabian M.},
title = {The Elephant in the Room: Getting Value from Big Data},
year = {2015},
isbn = {9781450336277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2767109.2770014},
doi = {10.1145/2767109.2770014},
booktitle = {Proceedings of the 18th International Workshop on Web and Databases},
pages = {1–5},
numpages = {5},
location = {Melbourne, VIC, Australia},
series = {WebDB'15}
}

@techreport{10.5555/2582001,
author = {Marchionini, Gary and Lee, Christopher A. and Bowden, Heather and Lesk, Michael},
title = {Curating for Quality: Ensuring Data Quality to Enable New Science},
year = {2012},
publisher = {National Science Foundation},
address = {USA},
abstract = {Science is built on observations. If our observational data is bad, we are building a house on sand. Some of our data banks have quality measurements and maintenance, such as the National Climate Data Center and the National Center for Biotechnology Information; but others do not, and we do not even know which scientific data services have quality metrics or what they are.Data quality is an assertion about data properties, typically assumed within a context defined by a collection that holds the data. The assertion is made by the creator of the data. The collection context includes both metadata that describe provenance and representation information, and procedures that are able to parse and manipulate the data. However data quality from the perspective of users is defined based on the data properties that are required for use within their scientific research. The user believes data is of high quality when assertions about compliance can be shown to their research requirements.Digital data can accumulate rich contextual and derivative data as it is collected, analyzed, used, and reused, and planning for the management of this history requires new kinds of tools, techniques, standards, workflows, and attitudes. As science and industry recognize the need for digital curation, scientists and information professionals recognize that access and use of data depend on trust in the accuracy and veracity of data. In all data sets trust and reuse depend on accessible context and metadata that make explicit provenance, precision, and other traces of the datum and data life cycle. Poor data quality can be worse than missing data because it can waste resources and lead to faulty ideas and solutions, or at minimum challenges trust in the results and implications drawn from the data. Improvement in data quality can thus have significant benefits.}
}

@article{10.1145/3005395,
author = {Bizer, Christian and Dong, Luna and Ilyas, Ihab and Vidal, Maria-Esther},
title = {Editorial: Special Issue on Web Data Quality},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3005395},
doi = {10.1145/3005395},
journal = {J. Data and Information Quality},
month = {nov},
articleno = {1},
numpages = {3}
}

@inproceedings{10.1145/3486011.3486555,
author = {Rocuts, Schweitzer and Alier, Marc},
title = {A Methodology Exploration to Motivate Teachers to Place Mathematics at the Center of a Transdisciplinary Experience Using Videogames and Big Data Combined with the 21st Century Skills},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486555},
doi = {10.1145/3486011.3486555},
abstract = {Human knowledge is highly connected and under continuous and collaborative evolution, where all of us can co-create and contribute. But the execution of our standardized educational systems mostly offers a standardized experience and teaching practices that do not reflect this point. This is particularly acute in teachers and students with mathematics. In this research, I will review some strategies to help teachers to effectively use connected mathematics with the real and daily world, promoting transdisciplinary work with an emphasis on creativity and collaboration, and using the Game-Based Assessment methodology combined with the unique environment surrounding each teacher.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {721–722},
numpages = {2},
keywords = {21st Century Skills, Connected mathematics, creativity and collaborative work in education, transdisciplinary},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@article{10.1145/2629605,
author = {Rahm, Erhard},
title = {Discovering Product Counterfeits in Online Shops: A Big Data Integration Challenge},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2629605},
doi = {10.1145/2629605},
journal = {J. Data and Information Quality},
month = {sep},
articleno = {3},
numpages = {3}
}

@inproceedings{10.1145/3110025.3120958,
author = {Al-janabi, Samir and Hamid, Abubaker and Janicki, Ryszard},
title = {DatumPIPE: Data Generator and Corrupter for Multiple Data Quality Aspects},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3120958},
doi = {10.1145/3110025.3120958},
abstract = {Organizations use data to support different business processes. Data may become unclean because of corruptions in the central quality aspects due to factors such as duplicate records, outdated data, inconsistent values, incomplete information, or inaccurate values. Real datasets are usually not available for reasons such as privacy constraints. In the existing systems that generate or corrupt synthetic data, the intrinsic characteristics of data may not satisfy the quality aspects, and the injected types of errors do not corrupt multiple data quality aspects. Also, a lack of common datasets is a primary reason that representative comparisons between algorithms of different data quality management approaches are not possible. To address these issues, we present datumPIPE, a system that allows for the generation of data that satisfies a set of integrity constraints, including functional dependencies (FDs), conditional functional dependencies (CFDs), and inclusion dependencies (INDs). Also, datumPIPE provides the functionality to generate other types of attribute values such as sensors and personal data. It also allows for the corruption of the generated data through the introduction of quality issues in the central data quality aspects.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {589–592},
numpages = {4},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@article{10.1145/3464419,
author = {Zhang, Jie and Qu, Zhihao and Chen, Chenxi and Wang, Haozhao and Zhan, Yufeng and Ye, Baoliu and Guo, Song},
title = {Edge Learning: The Enabling Technology for Distributed Big Data Analytics in the Edge},
year = {2021},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3464419},
doi = {10.1145/3464419},
abstract = {Machine Learning (ML) has demonstrated great promise in various fields, e.g., self-driving, smart city, which are fundamentally altering the way individuals and organizations live, work, and interact. Traditional centralized learning frameworks require uploading all training data from different sources to a remote data server, which incurs significant communication overhead, service latency, and privacy issues.To further extend the frontiers of the learning paradigm, a new learning concept, namely, Edge Learning (EL) is emerging. It is complementary to the cloud-based methods for big data analytics by enabling distributed edge nodes to cooperatively training models and conduct inferences with their locally cached data. To explore the new characteristics and potential prospects of EL, we conduct a comprehensive survey of the recent research efforts on EL. Specifically, we first introduce the background and motivation. We then discuss the challenging issues in EL from the aspects of data, computation, and communication. Furthermore, we provide an overview of the enabling technologies for EL, including model training, inference, security guarantee, privacy protection, and incentive mechanism. Finally, we discuss future research opportunities on EL. We believe that this survey will provide a comprehensive overview of EL and stimulate fruitful future research in this field.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {151},
numpages = {36},
keywords = {edge computing, security and privacy, federated learning, Edge learning, machine learning}
}

@inproceedings{10.1145/3524458.3547246,
author = {Bottai, Carlo and Crosato, Lisa and Domenech, Josep and Guerzoni, Marco and Liberati, Caterina},
title = {Unconventional Data for Policy: Using Big Data for Detecting Italian Innovative SMEs},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524458.3547246},
doi = {10.1145/3524458.3547246},
abstract = {The paper explores the possibility to employ the source code of corporate websites as an information source for research in innovation studies. Research in this area is generally based on studies that collect data on patents or official data sources. Our paper links the standard economic information of the firm with web-based data and joins the ongoing debate with a threefold contribution. First, whereas the majority of the literature focused on the linguistic content of web-pages, we mostly use HTML tags. Second, we propose a method to assess the quality of the linkage of Web data to firm-level information. Third, we show that the data retrieved from corporate websites can aid to identify ‘innovative SMEs’.},
booktitle = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
pages = {338–344},
numpages = {7},
keywords = {SMEs, HTML tags, Website data, Innovation},
location = {Limassol, Cyprus},
series = {GoodIT '22}
}

@inproceedings{10.1145/3291801.3291834,
author = {Deze, Wang},
title = {Application of Large Data Mining Technology in Colleges and Universities},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291834},
doi = {10.1145/3291801.3291834},
abstract = {Data mining is an advanced science and technology to process data and information. It can be extracted from a large number of complex data, or find some valuable data rules and models. Education has entered the era of big data. However, the way of data processing in university educational administration is relatively backward. Aiming at this problem, this paper applies data mining technology to college teaching management, extracts useful information from the data collected by educational administration management system, and provides correct and powerful data support and guarantee for college teaching managers to make relevant decisions.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {86–89},
numpages = {4},
keywords = {big data, data mining, college student management},
location = {Weihai, China},
series = {ICBDR 2018}
}

@article{10.1145/3174791,
author = {Geerts, Floris and Missier, Paolo and Paton, Norman},
title = {Editorial: Special Issue on Improving the Veracity and Value of Big Data},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3174791},
doi = {10.1145/3174791},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {13},
numpages = {2}
}

@inproceedings{10.1145/3010089.3010095,
author = {Sindhu, C. S. and Hegde, Nagaratna P.},
title = {TAF: Temporal Analysis Framework for Handling Data Velocity in Healthcare Analytics},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010095},
doi = {10.1145/3010089.3010095},
abstract = {We are inundated in a flood of data today. Data is being collected at a rapid scale from variety of sources like healthcare, e-commerce, social networking and so on. Decisions which were earlier made on assumptions can now be made on the data itself. It's a well known fact that volume, variety, velocity and veracity are the challenges associated in handling Big Data. The dynamic nature of the Internet and the velocity factor pose humongous challenges in retrieving patterns from the data. Coping up with noisy data which occurs at a rapid rate is still an open challenge. We have handled the issues associated with variety and veracity. After reviewing the existing system, it was found that there is no significant research model towards addressing data velocity problem exclusively taking case study of healthcare analytics.Hence, this paper presents a novel framework TAF or Temporal Analysis Framework that mainly targets at handling the incoming speed of data and redundancies in Healthcare Analytics. The proposed system uses real-time data analysis that significantly handles the data velocity along with retention of minimal error. The study outcome was assessed to find minimal algorithm complexities compared to any system that doesn't use this approach of self-adaptable real-time data analysis.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {10},
numpages = {11},
keywords = {Healthcare Analytics, Real-Time Analysis, Data Volume, Medical Data, Big Data},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@article{10.1145/3423321,
author = {Polese, Giuseppe and Deufemia, Vincenzo and Song, Shaoxu},
title = {Editorial: Special Issue on Metadata Discovery for Assessing Data Quality},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3423321},
doi = {10.1145/3423321},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {17},
numpages = {2}
}

@inproceedings{10.1145/3277644.3277803,
author = {Filonik, Daniel and Bednarz, Tomasz},
title = {Visual Analytics of Big Networks: Novel Approaches for Exploring Complex Networks in Big Data},
year = {2018},
isbn = {9781450360265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277644.3277803},
doi = {10.1145/3277644.3277803},
abstract = {Four "Paradigms" of Science• Empirical Science• Theoretical Science• Computational Science• Data Science},
booktitle = {SIGGRAPH Asia 2018 Courses},
articleno = {18},
numpages = {96},
location = {Tokyo, Japan},
series = {SA '18}
}

@inproceedings{10.1145/3090354.3090358,
author = {Tikito, I. and Souissi, N.},
title = {Data Collect Requirements Model},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090358},
doi = {10.1145/3090354.3090358},
abstract = {In Big data era, managing data requires sufficient tools, last computer science evolution and developed methodologies. To be able to satisfy customer and the big need of information, multiple methods are developed to handle the complexity as well as the huge amount of data in different phases of data lifecycle. We notice for each complicated situation in data lifecycle we focus more particularly to develop storage or Analysis processes. For this reason in this paper, we try to have a different approach to resolve basic issues on targeting the first phase of data lifecycle, which is data collect. We present it as a System of systems, since the complexity of each phase of data lifecycle. In this research, we are interested by the collect system and particularly the process of Creation/Reception of data for which we model the requirements in order to manage smart data at the first level of the cycle. To build this model, we follow a methodology that required three major steps. Starting with requirement identification to defining criterion for each requirement, and in the last step will provide requirement modeling. This research highlight the importance of managing data collect to identify and restrict the issues of big data era.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {4},
numpages = {7},
keywords = {Requirement Model, Collect system, BPMN, System of Systems, Big data, Data lifecycle, Seven views, Creation/Reception Process},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3523181.3523193,
author = {Yu, Jie and Li, Danning and Chen, Kai and Huang, Wei and Qin, Meiyuan and Qin, Xianjin},
title = {Research on Food Safety Data Sharing and Exchange Mechanism},
year = {2022},
isbn = {9781450387453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523181.3523193},
doi = {10.1145/3523181.3523193},
abstract = {With the development of the economy and the improvement of people's quality of life, the public demand for food taste has gradually changed to the demand for food safety. In order to better facilitate the government to strengthen food safety supervision and protect people's food safety, it is necessary for the government to realize information interaction with enterprises related to the food supply chain and ensure the traceability of food flowing into the market by exchanging and sharing the data of food safety information. With the rapid promotion and popularization of various mobile terminals in the Internet era, the data analysis technology based on artificial intelligence technology is more accurate, which makes the value contained in the data more and more important to people. At present, many fields need the opening and sharing of big data, but there is no reliable data sharing environment in the field of food supervision, and it is still difficult to ensure the traceability of data related to food safety. Blockchain has unique advantages of decentralization and distribution, which can help break the current obstacles of big data sharing and exchange and achieve a high degree of data sharing, interconnection and exchange. Based on the blockchain technology, this paper studies the food safety data sharing and exchange mechanism, combines the blockchain with the distributed file system, constructs the data connection model, stores the shared information on the blockchain, then introduces IPFs and zigzag coding, designs the corresponding control method, and establishes a reliable data sharing and exchange mechanism. The analysis shows that the data sharing and exchange mechanism proposed in this paper can meet the needs of food safety data sharing and exchange.},
booktitle = {2022 3rd Asia Service Sciences and Software Engineering Conference},
pages = {81–86},
numpages = {6},
location = {Macau, Macao},
series = {ASSE' 22}
}

@inproceedings{10.1145/3220199.3220206,
author = {Xie, Dingding and Li, Junyi and Yuan, Lening and Peng, Peng},
title = {Multi: Source Data Inconsistency Detection and Repair Based on CRC Algorithm},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220206},
doi = {10.1145/3220199.3220206},
abstract = {Most existing systems suffer from data quality problems. Data quality has been affected by many factors such as manual operation, software problems and hardware problems, especially data inconsistencies. As an important carrier of data, database system plays an important role in distributed systems. In order to reduce the impact of data inconsistency on data quality in distributed database systems, we design and implement a multi-source data inconsistency detection and repair method based on CRC algorithm.The idea of the proposed techniques is to use the rolling checksum in the rsync algorithm. In the process of data inconsistency detection, the method divides the table into chunks and calculates the checksums of data chunks in parallel for multiple data tables to detect and repair the inconsistent data. The experimental results show that the detection effect of this method is consistent with that of the traditional method which comparing source data with target data. The detection rate is as high as 99%, but it performs better than the traditional method, and the running time is reduced by about 20%.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Computing},
pages = {38–43},
numpages = {6},
keywords = {Distributed database system, Data quality, CRC, Data inconsistency},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@article{10.1145/3428154,
author = {Chirkova, Rada and Doyle, Jon and Reutter, Juan},
title = {Ensuring Data Readiness for Quality Requirements with Help from Procedure Reuse},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3428154},
doi = {10.1145/3428154},
abstract = {Assessing and improving the quality of data are fundamental challenges in Big-Data applications. These challenges have given rise to numerous solutions targeting transformation, integration, and cleaning of data. However, while schema design, data cleaning, and data migration are nowadays reasonably well understood in isolation, not much attention has been given to the interplay between standalone tools in these areas. In this article, we focus on the problem of determining whether the available data-transforming procedures can be used together to bring about the desired quality characteristics of the data in business or analytics processes. For example, to help an organization avoid building a data-quality solution from scratch when facing a new analytics task, we ask whether the data quality can be improved by reusing the tools that are already available, and if so, which tools to apply, and in which order, all without presuming knowledge of the internals of the tools, which may be external or proprietary.Toward addressing this problem, we conduct a formal study in which individual data cleaning, data migration, or other data-transforming tools are abstracted as black-box procedures with only some of the properties exposed, such as their applicability requirements, the parts of the data that the procedure modifies, and the conditions that the data satisfy once the procedure has been applied. As a proof of concept, we provide foundational results on sequential applications of procedures abstracted in this way, to achieve prespecified data-quality objectives, for the use case of relational data and for procedures described by standard relational constraints. We show that, while reasoning in this framework may be computationally infeasible in general, there exist well-behaved cases in which these foundational results can be applied in practice for achieving desired data-quality results on Big Data.},
journal = {J. Data and Information Quality},
month = {apr},
articleno = {15},
numpages = {15},
keywords = {and models, Big Data quality management processes, Big Data quality in business process, frameworks, data cleaning in Big Data, Big Data quality and analytics, Data and information quality, data integration in Big Data}
}

@inproceedings{10.1145/3365109.3368778,
author = {Wang, Chun-Yu and Fuh, Shih-Hao and Lo, Ta-Chun and Cheng, Qi-Jun and Chen, Yu-Cheng and Cho, Feng-Min and Chang, Jyh-Biau and Shieh, Ce-Kuen},
title = {A Data Compacting Technique to Reduce the NetFlow Size in Botnet Detection with BotCluster},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368778},
doi = {10.1145/3365109.3368778},
abstract = {Big data analytics helps us to find potentially valuable knowledge, but as the size of the dataset increases, the computing cost also grows exponentially. In our previous work, BotCluster, we had designed a pre-processing filtering pipeline, including whitelist filter and flow loss-response rate (FLR) filter, for data reduction, which intended to wipe out irrelative noises and reduce the computing overhead. However, we still face a data redundancy phenomenon in which some of the same feature vectors repeatedly emerged. In this paper, we propose a data compacting approach aimed to reduce the input volume and keep enough representative feature vectors to fit DBSCAN's (Density-based spatial clustering of applications with noise) criteria. It purges the redundant vectors according to a purging threshold and keeps the primary representatives. Experimental results have shown that the average data reduction ratio is about 81.34%, while the precision has only slightly decreased by 1.6% on average, and the results still have 99.88% of IPs overlapped with the previous system.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {81–84},
numpages = {4},
keywords = {big data, botnet, data reduction, data compression, netflow, mapreduce framework, p2p botnet, data compacting},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@inproceedings{10.1145/3207677.3278010,
author = {Zhang, Guilan and Wang, Jian and Zhou, Guomin and Liu, Jianping and Wei, Caoyuan},
title = {Scientific Data Relevance Criteria Classification and Usage},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3278010},
doi = {10.1145/3207677.3278010},
abstract = {In1 the big data era, scientific data plays a crucial role in scientific research. Data sharing, retrieval and usage has become an inevitable trend. We study how the users of scientific data select relevant data from the data sharing platform. The study was conducted in two stages. In the first stage, a total of 14 subjects were selected to obtain their relevance criteria and usage of scientific data through semi-structured interviews. In the second stage, 671 questionnaires were collected in order to classify criteria. Finally, we determined 9 relevance criteria for scientific data: topicality, availability, comprehensiveness, currency, authority, quality, convenience, standardization, and usability, and divided them to 5 groups. In order to truly make a better data search engine and improve its search efficiency, moving beyond the criteria often used by users, we need to determine those criteria that are not often used, but still very important. What's more, a more convenient data search platform needs to be considered.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {30},
numpages = {7},
keywords = {information carrier, relevance criteria, Relevance, scientific data},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/3175684.3175719,
author = {Nguyen, Minh Chau and Won, Hee Sun},
title = {Advanced Multitenant Hadoop in Smart Open Data Platform},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175719},
doi = {10.1145/3175684.3175719},
abstract = {Nowadays, there has been an immense amount of data coming from various devices sensors, social networks and IoT services. Among these data, open data is playing more and more important role in practice. Many individuals and organizations collect a broad range of different types of data in order to perform their analytic tasks. However, the current open data platforms still have many limitations. Among the drawbacks, data management, an important process of analytic service development, needs to be improved significantly. The main reason is that the emergence of massive data explosion coming from various sources has been making the process become more and more complicated and costly. Therefore, we propose here a system related to the field of data management to allow multitenant users to find and access easily their desired data as well as metadata. It also helps improve the performance of platform.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {48–51},
numpages = {4},
keywords = {Big data, Open data, Data authorization, Metadata, Advanced multitenant Hadoop},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@article{10.1145/3287168,
author = {Srivastava, Divesh and Scannapieco, Monica and Redman, Thomas C.},
title = {Ensuring High-Quality Private Data for Responsible Data Science: Vision and Challenges},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3287168},
doi = {10.1145/3287168},
abstract = {High-quality data is critical for effective data science. As the use of data science has grown, so too have concerns that individuals’ rights to privacy will be violated. This has led to the development of data protection regulations around the globe and the use of sophisticated anonymization techniques to protect privacy. Such measures make it more challenging for the data scientist to understand the data, exacerbating issues of data quality. Responsible data science aims to develop useful insights from the data while fully embracing these considerations.We pose the high-level problem in this article, “How can a data scientist develop the needed trust that private data has high quality?” We then identify a series of challenges for various data-centric communities and outline research questions for data quality and privacy researchers, which would need to be addressed to effectively answer the problem posed in this article.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {1},
numpages = {9},
keywords = {Responsible data science, quality of private data, data trust, private data}
}

@inproceedings{10.1145/3459637.3481905,
author = {Bian, Shuqing and Zhao, Wayne Xin and Zhou, Kun and Cai, Jing and He, Yancheng and Yin, Cunxiang and Wen, Ji-Rong},
title = {Contrastive Curriculum Learning for Sequential User Behavior Modeling via Data Augmentation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481905},
doi = {10.1145/3459637.3481905},
abstract = {Within online platforms, it is critical to capture the semantics of sequential user behaviors for accurately modeling user interests. However, dynamic characteristics and sparse behaviors make it difficult to train effective user representations for sequential user behavior modeling.Inspired by the recent progress in contrastive learning, we propose a novel Contrastive Curriculum Learning framework for producing effective representations for modeling sequential user behaviors. We make important technical contributions in two aspects, namely data quality and sample ordering. Firstly, we design a model-based data generator by generating high-quality samples confirming to users' attribute information. Given a target user, it can leverage the fused attribute semantics for generating more close-to-real sequences. Secondly, we propose a curriculum learning strategy to conduct contrastive learning via an easy-to-difficult learning process. The core component is a learnable difficulty evaluator, which can score augmented sequences, and schedule them in curriculums. Extensive results on both public and industry datasets demonstrate the effectiveness of our approach on downstream tasks.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3737–3746},
numpages = {10},
keywords = {contrastive curriculum learning, user behavior modeling},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3513142.3513235,
author = {Zhang, Le and Ren, Junda and Yang, Zhi and Yin, Zenan and Chen, Yiting and Gu, Yiming},
title = {Analysis of The Advancement of Rpa Technology and Its Application in the Financial Field of Electric Power Enterprises},
year = {2021},
isbn = {9781450386494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3513142.3513235},
doi = {10.1145/3513142.3513235},
abstract = {Under the background of the new technology era of cloud, big things, mobile intelligence, RPA (RoboticsProcessAutomation) technology, as an important and mature application in the field of artificial intelligence, can help financial personnel to free themselves from a large number of simple and complex transactional work and invest in Financial analysis, scientific decision-making and other high value-added work. At present, financial robot products based on RPA technology can be extended to be compatible with OCR, voice, intelligent customer service, deep learning and other functions, supporting the establishment of risk management and control systems and intelligent application scenarios, and ultimately improve the cross-business collaboration capabilities and operation automation efficiency of financial management. Effectively control financial risks, improve the efficiency of data asset use and financial analysis and decision-making capabilities, and provide power companies with good management and economic benefits. This article first analyzes the advantages and technical characteristics of RPA technology, then summarizes the practical application of financial robotics technology in power companies, explores the role of RPA technology in financial digital transformation, and studies its risk management and control models, which are of great significance to improving the comprehensive management level of power grid companies.},
booktitle = {The 4th International Conference on Information Technologies and Electrical Engineering},
articleno = {91},
numpages = {5},
keywords = {digitization, financial robot, intelligence, RPA, electric power enterprise},
location = {Changde, Hunan, China},
series = {ICITEE2021}
}

@article{10.14778/2824032.2824136,
author = {Gao, Jing and Li, Qi and Zhao, Bo and Fan, Wei and Han, Jiawei},
title = {Truth Discovery and Crowdsourcing Aggregation: A Unified Perspective},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824136},
doi = {10.14778/2824032.2824136},
abstract = {In the era of Big Data, data entries, even describing the same objects or events, can come from a variety of sources, where a data source can be a web page, a database or a person. Consequently, conflicts among sources become inevitable. To resolve the conflicts and achieve high quality data, truth discovery and crowdsourcing aggregation have been studied intensively. However, although these two topics have a lot in common, they are studied separately and are applied to different domains. To answer the need of a systematic introduction and comparison of the two topics, we present an organized picture on truth discovery and crowdsourcing aggregation in this tutorial. They are compared on both theory and application levels, and their related areas as well as open questions are discussed.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2048–2049},
numpages = {2}
}

@article{10.14778/3229863.3236224,
author = {Li, Huan and Lu, Hua and Shi, Feichao and Chen, Gang and Chen, Ke and Shou, Lidan},
title = {TRIPS: A System for Translating Raw Indoor Positioning Data into Visual Mobility Semantics},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3236224},
doi = {10.14778/3229863.3236224},
abstract = {The rapid accumulation of indoor positioning data is increasingly booming the interest in indoor mobility analyses. As a fundamental analysis, it is highly relevant to translate raw indoor positioning data into mobility semantics that describe what, where and when in a more concise and semantics-oriented way. Such a translation is challenging as multiple data sources are involved, raw indoor positioning data is of low quality, and translation results are hard to assess. We demonstrate a system TRIPS that streamlines the entire translation process by three functional components. The Configurator provides a standard but concise means to configure multiple input sources, including the indoor positioning data, indoor space information, and relevant contexts. The Translator cleans the indoor positioning data and exports reliable mobility semantics without manual interventions. The Viewer offers a suite of flexible operations to trace the input, output and intermediate data involved in the translation. Data analysts can interact with TRIPS to obtain the desired mobility semantics in a visual and convenient way.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1918–1921},
numpages = {4}
}

@inproceedings{10.1145/2928294.2928306,
author = {Wu, Jian and Liang, Chen and Yang, Huaiyu and Giles, C. Lee},
title = {CiteSeerX Data: Semanticizing Scholarly Papers},
year = {2016},
isbn = {9781450342995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2928294.2928306},
doi = {10.1145/2928294.2928306},
abstract = {Scholarly big data is, for many, an important instance of Big Data. Digital library search engines have been built to acquire, extract, and ingest large volumes of scholarly papers. This paper provides an overview of the scholarly big data released by CiteSeerX, as of the end of 2015, and discusses various aspects such as how the data is acquired, its size, general quality, data management, and accessibility. Preliminary results on extracting semantic entities from body text of scholarly papers with Wikifier show biases towards general terms appearing in Wikipedia and against domain specific terms. We argue that the latter will play a more important role in extracting important facts from scholarly papers.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {2},
numpages = {6},
keywords = {digital library search engine, CiteSeerX, scholarly big data, citation graph, semantic entity extraction},
location = {San Francisco, California},
series = {SBD '16}
}

@inproceedings{10.1145/3331453.3360954,
author = {Liu, Jianping and Wang, Jian and Zhou, Guomin and Zhang, Guilan and Cui, Yunpeng and Liu, Juan},
title = {The Cognitive Enhancement Process of Scientific Data Retrieval},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3360954},
doi = {10.1145/3331453.3360954},
abstract = {Is there a stable cognitive structure of scientific data retrieval process? Based on the theory and method of user relevance research, this study explores the cognitive characteristics of user scientific data query and retrieval. The semi-structured interview method used to collect relevant data, and the content analysis method used to encode and analyze the cognitive process of users' scientific data query and retrieval. The results show that (1) users scientific data relevance judgment not only depend on topicality, but also use accessibility, quality, authority and usefulness. (2) There are 7 combination patterns for the use of user's scientific data relevance criteria, and (3) different patterns correspond to different user relevance types and different user information need states. These 7 criteria usage patterns reveal the cognitive enhancement of user scientific data relevance judgment. The research results have a great inspiration for the development of interactive scientific data retrieval system based on user cognitive enhancement characteristics.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {1},
numpages = {7},
keywords = {Scientific data retrieval, User relevance, Relevance criteria},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/2463676.2465309,
author = {Cao, Yang and Fan, Wenfei and Yu, Wenyuan},
title = {Determining the Relative Accuracy of Attributes},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465309},
doi = {10.1145/2463676.2465309},
abstract = {The relative accuracy problem is to determine, given tuples t1 and t2 that refer to the same entity e, whether t1[A] is more accurate than t2A, i.e., t1A is closer to the true value of the A attribute of e than t2A. This has been a longstanding issue for data quality, and is challenging when the true values of e are unknown. This paper proposes a model for determining relative accuracy. (1) We introduce a class of accuracy rules and an inference system with a chase procedure, to deduce relative accuracy. (2) We identify and study several fundamental problems for relative accuracy. Given a set Ie of tuples pertaining to the same entity e and a set of accuracy rules, these problems are to decide whether the chase process terminates, is Church-Rosser, and leads to a unique target tuple te composed of the most accurate values from Ie for all the attributes of e. (3) We propose a framework for inferring accurate values with user interaction. (4) We provide algorithms underlying the framework, to find the unique target tuple te whenever possible; when there is no enough information to decide a complete te, we compute top-k candidate targets based on a preference model. (5) Using real-life and synthetic data, we experimentally verify the effectiveness and efficiency of our method.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {565–576},
numpages = {12},
keywords = {data accuracy, data cleaning},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3532213.3532249,
author = {Liu, Haitao and Ye, Bo and Qin, Zhi and Zhang, Jing},
title = {The High-Performance Solution with Federated Learning in Supply Chain System},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532249},
doi = {10.1145/3532213.3532249},
abstract = {Supply chain is an important commercial pattern which used in industry, agriculture and service widely. However different parties have different information even though they are all in the same supply chain. That would results in information silo and the extra increasing on cost. On one hand, information could not transfer to other parties for itself benefits or intellectual property. On the other hand, other parties want to obtain supplementary information in order to make equitable trade and lower trade risk. A skillful method would be designed and employed for solve above problems in this paper. Federated Learning technology was introduced in order to solve the puzzle between information privacy and information sharing. In the concretely implement process, Vertical Federated Learning (VFL) model was constructed and trained for resolving several problems specially because the overlap of parties are more but the features are small in supply chain forward. Gradient descent methods and loss computation ways were also used in training process in order to advance the performance. A series of experiments were used to evaluate VFL in supply chain. Experimental results revealed that VFL used in supply chain was effective.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {241–245},
numpages = {5},
keywords = {Federated Learning, Block chain, High-performance, Supply Chain},
location = {Tianjin, China},
series = {ICCAI '22}
}

@article{10.1145/3138806,
author = {Truong, Hong-Linh and Murguzur, Aitor and Yang, Erica},
title = {Challenges in Enabling Quality of Analytics in the Cloud},
year = {2018},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3138806},
doi = {10.1145/3138806},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {9},
numpages = {4},
keywords = {big data analytics, Cloud computing, service management, data quality}
}

@article{10.1145/3165713,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3165713},
doi = {10.1145/3165713},
abstract = {Although the ultimate objective of Linked Data is linking and integration, it is not currently evident how connected the current Linked Open Data (LOD) cloud is. In this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference, i.e., for obtaining complete information about a set of entities, including provenance information; (c) Data Quality Assessment and Improvement, i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d) Dataset Visualizations; and various other tasks. Since it would be prohibitively expensive to perform all these measurements in a na\"{\i}ve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. For enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. Finally, we report measurements about the connectivity of the (billion triples-sized) LOD cloud that have never been carried out so far.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {15},
numpages = {49},
keywords = {linked data, Data quality, dataset discovery, mapreduce, big data, dataset selection, spark, connectivity, lattice of measurements}
}

@inproceedings{10.1145/2808797.2809367,
author = {Tekieh, Mohammad Hossein and Raahemi, Bijan},
title = {Importance of Data Mining in Healthcare: A Survey},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2809367},
doi = {10.1145/2808797.2809367},
abstract = {In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1057–1062},
numpages = {6},
keywords = {health big data, data quality, data mining applications, predictive modelling, data mining, health data analysis},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3445815.3445820,
author = {Jin, Liya and Wang, Ronghui and Wang, Xuan},
title = {Research on Architecture of Big Date Analysis Platform in Cloud Environment},
year = {2020},
isbn = {9781450388436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445815.3445820},
doi = {10.1145/3445815.3445820},
abstract = {The rapid development of big data has attracted extensive attention at home and abroad. Scientific and effective analysis and processing of big data is the core issue in the field of big data. The construction of a big data analysis platform in cloud environment can process complex data structures and highly correlated data, timely respond to user requests, realize intelligent and efficient data analysis, and mine more valuable data, providing technical support for the rapid construction of big data services.},
booktitle = {2020 4th International Conference on Computer Science and Artificial Intelligence},
pages = {29–33},
numpages = {5},
keywords = {Data mining, Analysis platform, Cloud computing, Big data},
location = {Zhuhai, China},
series = {CSAI 2020}
}

@inproceedings{10.1145/3500931.3500993,
author = {Wang, Yiyang},
title = {A Comparison of Machine Learning Algorithms in Blood Glucose Prediction for People with Type 1 Diabetes},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500993},
doi = {10.1145/3500931.3500993},
abstract = {Diabetes is a metabolic disease with the characteristic of hyperglycemia. The pathogenic principle is derived from the defect of insulin secretion or the impairment of biological effects, or both. We use machine learning models and deep learning models for forecasting future blood glucose levels in this paper, and study the efficiency of detecting hypoglycemia and hyperglycemia events. The data set used is in-silico data generated from the UVA/PADOVA type 1 diabetes simulator. We aim to compare support vector machines, random forests, linear regression, K-Nearest Neighbors regression (KNN), XGBoosted trees and other deep learning models in terms of Root Mean Squared Error (RMSE), and other several evaluation metrics to study their effectiveness in predicting future blood sugar, and the accuracy rate of predicting hypoglycemia and hyperglycemia events. In this work, we found a bidirectional long-short term memory (LSTM) model with the best prediction effect, which can predict the blood glucose level of simulated patients with leading accuracy within 30 minutes (RMSE = 7.55±0.19 [mg/dl], R2-SCORE=0.96). The hopeful results show that this method could have practical application value for self-management of blood glucose in patients with type 1 diabetes.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {351–360},
numpages = {10},
keywords = {deep neural networks, artificial intelligence, machine learning, type 1 diabetes, deep learning, reinforcement learning},
location = {Beijing, China},
series = {ISAIMS 2021}
}

@inproceedings{10.1145/3414752.3414772,
author = {Qin, Haiqing and Liu, Xiaohan and Qin, Haiqi and Zhu, Jiang},
title = {How to Be an Enabler of Digital Transformation for Media Organizations?},
year = {2020},
isbn = {9781450388016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414752.3414772},
doi = {10.1145/3414752.3414772},
abstract = {Big data brings opportunities for the development of the media industry and also poses serious challenges. During the digital transformation period, media organizations have used third-party big data to achieve digital transformation and build a media digital ecology, which has become the focus of strategic development at this stage. This article takes the application of the data assets of China Unicom Big Data Co., Ltd. (UBD) in the media industry as an example, and explores the difficulties and attempts made by UBD. through in-depth interviews. The countermeasures are summarized from different aspects such as technology and data governance, which provide a reference for the digital transformation of media organizations.},
booktitle = {2020 The 11th International Conference on E-Business, Management and Economics},
pages = {153–156},
numpages = {4},
keywords = {Case study, Data empowerment, Media organization, Big data},
location = {Beijing, China},
series = {ICEME 2020}
}

@inproceedings{10.1145/3437075.3437087,
author = {Baijens, Jeroen and Helms, Remko and Kusters, Rob},
title = {Data Analytics Project Methodologies: Which One to Choose?},
year = {2020},
isbn = {9781450375061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437075.3437087},
doi = {10.1145/3437075.3437087},
abstract = {Developments in big data have led to an increase in data analytics projects conducted by organizations. Such projects aim to create value by improving decision making or enhancing business processes. However, many data analytics projects still fail to deliver the expected value. The use of process models or methodologies is recommended to increase the success rate of these projects. Nevertheless, organizations are hardly using them because they are considered too rigid and hard to implement. The existing methodologies often do not fit the specific project characteristics. Therefore, this research suggests grouping different project characteristics to identify the most appropriate project methodology for a specific type of project. More specifically, this research provides a structured description that helps to determine what type of project methodology works for different types of data analytics projects. The results of six different case studies show that continuous projects would benefit from an iterative methodology.},
booktitle = {Proceedings of the 2020 International Conference on Big Data in Management},
pages = {41–47},
numpages = {7},
keywords = {Project Methodologies, Project characteristics, Data Analytics},
location = {Manchester, United Kingdom},
series = {ICBDM 2020}
}

@inproceedings{10.1145/3331184.3331218,
author = {Lu, Shuqi and Dou, Zhicheng and Jun, Xu and Nie, Jian-Yun and Wen, Ji-Rong},
title = {PSGAN: A Minimax Game for Personalized Search with Limited and Noisy Click Data},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331218},
doi = {10.1145/3331184.3331218},
abstract = {Personalized search aims to adapt document ranking to user's personal interests. Traditionally, this is done by extracting click and topical features from historical data in order to construct a user profile. In recent years, deep learning has been successfully used in personalized search due to its ability of automatic feature learning. However, the small amount of noisy personal data poses challenges to deep learning models to learn the personalized classification boundary between relevant and irrelevant results. In this paper, we propose PSGAN, a Generative Adversarial Network (GAN) framework for personalized search. By means of adversarial training, we enforce the model to pay more attention to training data that are difficult to distinguish. We use the discriminator to evaluate personalized relevance of documents and use the generator to learn the distribution of relevant documents. Two alternative ways to construct the generator in the framework are tested: based on the current query or based on a set of generated queries. Experiments on data from a commercial search engine show that our models can yield significant improvements over state-of-the-art models.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {555–564},
numpages = {10},
keywords = {generative adversarial network, personalized web search},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/2882903.2899391,
author = {Farid, Mina and Roatis, Alexandra and Ilyas, Ihab F. and Hoffmann, Hella-Franziska and Chu, Xu},
title = {CLAMS: Bringing Quality to Data Lakes},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2899391},
doi = {10.1145/2882903.2899391},
abstract = {With the increasing incentive of enterprises to ingest as much data as they can in what is commonly referred to as "data lakes", and with the recent development of multiple technologies to support this "load-first" paradigm, the new environment presents serious data management challenges. Among them, the assessment of data quality and cleaning large volumes of heterogeneous data sources become essential tasks in unveiling the value of big data. The coveted use of unstructured and semi-structured data in large volumes makes current data cleaning tools (primarily designed for relational data) not directly adoptable.We present CLAMS, a system to discover and enforce expressive integrity constraints from large amounts of lake data with very limited schema information (e.g., represented as RDF triples). This demonstration shows how CLAMS is able to discover the constraints and the schemas they are defined on simultaneously. CLAMS also introduces a scale-out solution to efficiently detect errors in the raw data. CLAMS interacts with human experts to both validate the discovered constraints and to suggest data repairs.CLAMS has been deployed in a real large-scale enterprise data lake and was experimented with a real data set of 1.2 billion triples. It has been able to spot multiple obscure data inconsistencies and errors early in the data processing stack, providing huge value to the enterprise.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2089–2092},
numpages = {4},
keywords = {data quality, data lakes, RDF},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3538950.3538962,
author = {Zhou, Wei and Xue, Xiaorui and Luo, Danxue},
title = {Credit Card Fraud Detection Using Boundary Reconstruction and Integrated Classification},
year = {2022},
isbn = {9781450395632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538950.3538962},
doi = {10.1145/3538950.3538962},
abstract = {With the popularity of electronic payment, it not only brings great convenience, but also increases the risk of fraudulent transactions. At present, there are two problems in the identification of credit card fraud. The number of fraud and normal transactions is extremely unbalanced and the classification boundary is fuzzy. In order to solve these problems, this paper proposes an integrated classification framework for boundary reconstruction, which uses different machine learning algorithms as base learners to compare the original data with the data modeling after boundary reconstruction. The research shows that data boundary reconstruction can not only effectively alleviate the deviation caused by data imbalance to machine learning. It can also improve the data quality, so as to improve the accuracy of model classification; The integrated classification method can accurately identify credit card transactions, and the prediction effect of decision tree is the best. The proposed model is also applicable in other abnormal situations.},
booktitle = {Proceedings of the 4th International Conference on Big Data Engineering},
pages = {86–93},
numpages = {8},
keywords = {Credit card fraud detection, Boundary reconstruction, Machine learning, Integrated learning},
location = {Beijing, China},
series = {BDE '22}
}

@inproceedings{10.1145/3446999.3447017,
author = {Wang, Mo and Wang, Jing and Song, Yulun},
title = {A Map Matching Method for Restoring Movement Routes with Cellular Signaling Data},
year = {2020},
isbn = {9781450388559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446999.3447017},
doi = {10.1145/3446999.3447017},
abstract = {Cellular signaling data is a valuable and abundant data source to explore human mobility. Yet challenges remain to restore movement routes from signaling data due to its coarse positioning information. We propose an efficient map matching method based on road network topology. First, a customized spatial-temporal clustering algorithm ST-DBSCAN was employed to find stationary point clusters, which were later used to segment trips into sub-trips. The search space was then clipped with a fixed buffer zone along the line that connects the whole trip. Two optional strategies were provided to find the best matching routes with distance costs. Experiments on real-world data showed that both strategies achieved high map matching accuracies (88.2% and 94.3%). With Deep Mode, the method reached higher accuracy, while with longer computation time. The proposed method has the potential in solving practical problems, in the sense that it could be easily parallelized to deal with mass data.},
booktitle = {2020 The 8th International Conference on Information Technology: IoT and Smart City},
pages = {94–99},
numpages = {6},
keywords = {human mobility, road networks, signaling data, map matching},
location = {Xi'an, China},
series = {ICIT 2020}
}

@inproceedings{10.1145/2623330.2623716,
author = {Dasu, Tamraparni and Loh, Ji Meng and Srivastava, Divesh},
title = {Empirical Glitch Explanations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623716},
doi = {10.1145/2623330.2623716},
abstract = {Data glitches are unusual observations that do not conform to data quality expectations, be they logical, semantic or statistical. By applying data integrity constraints, potentially large sections of data could be flagged as being noncompliant. Ignoring or repairing significant sections of the data could fundamentally bias the results and conclusions drawn from analyses. In the context of Big Data where large numbers and volumes of feeds from disparate sources are integrated, it is likely that significant portions of seemingly noncompliant data are actually legitimate usable data.In this paper, we introduce the notion of Empirical Glitch Explanations - concise, multi-dimensional descriptions of subsets of potentially dirty data - and propose a scalable method for empirically generating such explanatory characterizations. The explanations could serve two valuable functions: (1) Provide a way of identifying legitimate data and releasing it back into the pool of clean data. In doing so, we reduce cleaning-related statistical distortion of the data; (2) Used to refine existing data quality constraints and generate and formalize domain knowledge.We conduct experiments using real and simulated data to demonstrate the scalability of our method and the robustness of explanations. In addition, we use two real world examples to demonstrate the utility of the explanations where we reclaim over 99% of the suspicious data, keeping data repair related statistical distortion close to 0.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {572–581},
numpages = {10},
keywords = {crossover subsampling, data quality, glitch explanations, quantitative data cleaning},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1145/3396850,
author = {Luckner, Marcin and Grzenda, Maciej and Kunicki, Robert and Legierski, Jaroslaw},
title = {IoT Architecture for Urban Data-Centric Services and Applications},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3396850},
doi = {10.1145/3396850},
abstract = {In this work, we describe an urban Internet of Things (IoT) architecture, grounded in big data patterns and focused on the needs of cities and their key stakeholders. First, the architecture of the dedicated platform USE4IoT (Urban Service Environment for the Internet of Things), which gathers and processes urban big data and extends the Lambda architecture, is proposed. We describe how the platform was used to make IoT an enabling technology for intelligent transport planning. Moreover, key data processing components vital to provide high-quality IoT data streams in a near-real-time manner are defined. Furthermore, tests showing how the IoT platform described in this study provides a low-latency analytical environment for smart cities are included.},
journal = {ACM Trans. Internet Technol.},
month = {jul},
articleno = {29},
numpages = {30},
keywords = {data processing, big data, Data stream, public transport}
}

@inproceedings{10.1145/3335484.3335541,
author = {Liang, Yu and Duan, Xuliang and Ding, Yuanjun and Kou, Xifeng and Huang, Jingcheng},
title = {Data Mining of Students' Course Selection Based on Currency Rules and Decision Tree},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335541},
doi = {10.1145/3335484.3335541},
abstract = {The currency of data can ensure that data is not obsolete and outdated. As one of the important bases for evaluating data quality, it plays an important role in the availability of data. Data currency rules can effectively discriminate the temporal relationship between data sets. The decision tree can availably classify and predict the data, and can test the attribute values very well. In this paper, the currency rules are combined with the C4.5 algorithm in the decision tree, and the improved algorithm is applied to the college elective data in recent years. Through experiments, the algorithm used in this paper can extract the statute rules from the student elective database. According to the currency rules, the college teaching plan can be planned in advance and the curriculum resources can be allocated reasonably.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {247–252},
numpages = {6},
keywords = {currency rules, course selection information, data mining, decision tree},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@inproceedings{10.1145/2896387.2900326,
author = {Cuzzocrea, Alfredo and Gaber, Mohamed Medhat and Lattimer, Staci and Grasso, Giorgio Mario},
title = {Clustering-Based Spatio-Temporal Analysis of Big Atmospheric Data},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900326},
doi = {10.1145/2896387.2900326},
abstract = {This paper proposes a comprehensive approach for supporting clustering-based spatio-temporal analysis of big atmospheric data via specializing on the interesting applicative setting represented by Greenhouse Gas Emissions (GGEs), a relevant instance of Big Data that empathize the Variety aspect of the well-known 3V Big Data axioms. In particular, in our research we consider GGEs from three EU countries, namely UK, France and Italy. The deriving Big Data Mining model turns to be useful for decision support processes in both the governmental and industrial contexts.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {74},
numpages = {8},
keywords = {Big Data Mining, Clustering-Based Spatio-Temporal Analysis of Big Data, Big Environmental and Atmospheric Data},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/3545801.3545803,
author = {Liu, Junhong and Ren, Tianqi and Huang, Ziyue and Tu, Yan},
title = {Regional Water Resources Allocation Based on Fuzzy Data Mining},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545801.3545803},
doi = {10.1145/3545801.3545803},
abstract = {To focus on the problem of water shortage and the contradictory relationship between water supply and demand in China, the water supply case sets of each administrative region in Hubei Province were studied in order to provide reference for the policy formulation of regional water allocation. Based on the construction of a multi-dimensional data index system, this paper applied the fuzzy hierarchical clustering method to cluster and characterize the water distribution cases in administrative regions, and used the Xie_beni index to test the validity of the clustering results. The results show that water allocation in different administrative regions of Hubei province by industry is divided into four categories, showing different industry-oriented water allocation characteristics. By Xie_Beni clustering validity index test, the minimum value of Xie_Beni index is 12.26, and the relative error of irrigation area water demand prediction is less than 15%, which confirms the validity of the method. Xie_Beni index can better test the validity of clustering results. However, there are still some drawbacks. In the case of increasing the number of clusters, the Xie_beni index will gradually lose its judgment ability. Besides, the method requires high data quality of the research object, and requires each index to be quantifiable and rich historical data.},
booktitle = {Proceedings of the 7th International Conference on Big Data and Computing},
pages = {6–13},
numpages = {8},
keywords = {Correlation analysis, Xie_Beni index, Fuzzy data mining, Cluster analysis, Water resources allocation},
location = {Shenzhen, China},
series = {ICBDC '22}
}

@inproceedings{10.1109/CCGrid.2016.79,
author = {Abdellaoui, Sabrina and Bellatreche, Ladjel and Nader, Fahima},
title = {A Quality-Driven Approach for Building Heterogeneous Distributed Databases: The Case of Data Warehouses},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.79},
doi = {10.1109/CCGrid.2016.79},
abstract = {Data Warehouse (DW) is a collection of data, consolidated from several heterogeneous sources, used to perform data analysis and support decision-making in an organization. Extract-Transform-Load (ETL) phase plays a crucial role in designing DW. To overcome the complexity of the ETL phase, different studies have recently proposed the use of ontologies. Ontology-based ETL approaches have been used to reduce heterogeneity between data sources and ensure automation of the ETL process. Existing studies in semantic ETL have largely focused on fulfilling functional requirements. However, the ETL process quality dimension has not been sufficiently considered by these studies. As the amount of data has exploded with the advent of big data era, dealing with quality challenges in the early stages of designing the process become more important than ever. To address this issue, we propose to keep data quality requirements at the center of the ETL phase design. We present in this paper an approach, defining the ETL process at the ontological level. We define a set of quality indicators and quantitative measures that can anticipate data quality problems and identify causes of deficiencies. Our approach checks the quality of data before loading them into the target data warehouse to avoid the propagation of corrupted data. Finally, our proposal is validated through a case study, using Oracle Semantic DataBase sources (SDBs), where each source references the Lehigh University BenchMark ontology (LUBM).},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {631–638},
numpages = {8},
keywords = {ontologies, data warehouse, data quality, semantic database sources, ETL design},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/2668930.2688055,
author = {Arlitt, Martin and Marwah, Manish and Bellala, Gowtham and Shah, Amip and Healey, Jeff and Vandiver, Ben},
title = {IoTAbench: An Internet of Things Analytics Benchmark},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688055},
doi = {10.1145/2668930.2688055},
abstract = {The commoditization of sensors and communication networks is enabling vast quantities of data to be generated by and collected from cyber-physical systems. This ``Internet-of-Things" (IoT) makes possible new business opportunities, from usage-based insurance to proactive equipment maintenance. While many technology vendors now offer ``Big Data" solutions, a challenge for potential customers is understanding quantitatively how these solutions will work for IoT use cases. This paper describes a benchmark toolkit called IoTAbench for IoT Big Data scenarios. This toolset facilitates repeatable testing that can be easily extended to multiple IoT use cases, including a user's specific needs, interests or dataset. We demonstrate the benchmark via a smart metering use case involving an eight-node cluster running the HP Vertica analytics platform. The use case involves generating, loading, repairing and analyzing synthetic meter readings. The intent of IoTAbench is to provide the means to perform ``apples-to-apples" comparisons between different sensor data and analytics platforms. We illustrate the capabilities of IoTAbench via a large experimental study, where we store 22.8 trillion smart meter readings totaling 727 TB of data in our eight-node cluster.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {133–144},
numpages = {12},
keywords = {performance evaluation, internet of things, big data, benchmarking},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@article{10.1145/3317573,
author = {Ding, Junhua and Li, Xinchuan and Kang, Xiaojun and Gudivada, Venkat N.},
title = {A Case Study of the Augmentation and Evaluation of Training Data for Deep Learning},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3317573},
doi = {10.1145/3317573},
abstract = {Deep learning has been widely used for extracting values from big data. As many other machine learning algorithms, deep learning requires significant training data. Experiments have shown both the volume and the quality of training data can significantly impact the effectiveness of the value extraction. In some cases, the volume of training data is not sufficiently large for effectively training a deep learning model. In other cases, the quality of training data is not high enough to achieve the optimal performance. Many approaches have been proposed for augmenting training data to mitigate the deficiency. However, whether the augmented data are “fit for purpose” of deep learning is still a question. A framework for comprehensively evaluating the effectiveness of the augmented data for deep learning is still not available. In this article, we first discuss a data augmentation approach for deep learning. The approach includes two components: the first one is to remove noisy data in a dataset using a machine learning based classification to improve its quality, and the second one is to increase the volume of the dataset for effectively training a deep learning model. To evaluate the quality of the augmented data in fidelity, variety, and veracity, a data quality evaluation framework is proposed. We demonstrated the effectiveness of the data augmentation approach and the data quality evaluation framework through studying an automated classification of biology cell images using deep learning. The experimental results clearly demonstrated the impact of the volume and quality of training data to the performance of deep learning and the importance of the data quality evaluation. The data augmentation approach and the data quality evaluation framework can be straightforwardly adapted for deep learning study in other domains.},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {20},
numpages = {22},
keywords = {deep learning, machine learning, Data quality, support vector machine, convolutional neural network, diffraction image}
}

@inproceedings{10.1145/3357292.3357306,
author = {Min, Han Xue and Feng, Zheng Gao and Xi, Liu Peng and Dong, Wang Xu and Ping, Xu Zhong},
title = {Application Research of Power Grid Full-Business Monitoring and Analysis Based on Multi-Source Business and Data Fusion},
year = {2019},
isbn = {9781450371445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357292.3357306},
doi = {10.1145/3357292.3357306},
abstract = {With the development of power enterprise informationization after more than ten years of development, Achieve full coverage of the company's business, effectively supporting the full-business operation of the power grid, and the accumulated business data has exploded. However, there are still problems such as low data quality, insufficient integration of multi-source business and data fusion, which makes it difficult to monitor and analyze the full-business of the power grid. This paper will combine the big data technology to study how to conduct monitoring and analysis of power grid full-business operation based on multi-source business and data fusion, and realize the three-layer architecture of business and data combination layer, business and data integration layer and business and data aggregation layer. Different levels of analysis and application, such as indicator monitoring analysis, subject monitoring analysis, and special monitoring analysis, effectively support enterprise management analysis and analytical decision.},
booktitle = {Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences},
pages = {49–53},
numpages = {5},
keywords = {mining analysis, Big data technology, business and data fusion},
location = {Chengdu, China},
series = {IMMS 2019}
}

@inproceedings{10.1145/3468920.3468942,
author = {Aljohani, Asmaa and Jones, James},
title = {Conducting Malicious Cybersecurity Experiments on Crowdsourcing Platforms},
year = {2021},
isbn = {9781450389426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468920.3468942},
doi = {10.1145/3468920.3468942},
abstract = {Evaluating the effectiveness of defense technologies mandates the inclusion of a human element, specifically if these technologies target human cognition and emotions. One of the biggest challenges that face researchers in the realm of behavioral cybersecurity is participant recruitment. Researchers often rely on college students, the general public, real-world hackers, or a hard-to-reach population (e.g., professional red teamers) to test the effectiveness of cybersecurity defense techniques. However, recruiting participants from these populations has drawbacks, including but not limited to: high cost, time constraints, and manageability and accessibility issues. This research explored the applicability of using two popular crowdsourcing platforms, Amazon Mechanical Turk and Prolific, to conduct web hacking experiments. Our study is the first to use crowdsourcing platforms to run hacking experiments for scientific purposes. While the recruitment is challenging, the paradigm of existing crowdsourcing platforms can be useful for understanding adversarial behavior, as it facilitates access to a diverse set of participants and allows researchers to conduct longitudinal and cross-cultural assessments. In particular, crowdsourcing platforms offer a great opportunity for cybersecurity researchers to investigate the Oppositional Human Factors (OHFs) in a manageable and flexible way.},
booktitle = {The 2021 3rd International Conference on Big Data Engineering},
pages = {150–161},
numpages = {12},
keywords = {Cybersecurity, Participants, Recruitment, Crowdsourcing, Online experiments},
location = {Shanghai, China},
series = {BDE 2021}
}

@inproceedings{10.1145/3320154.3320165,
author = {Lawrenz, Sebastian and Sharma, Priyanka and Rausch, Andreas},
title = {Blockchain Technology as an Approach for Data Marketplaces},
year = {2019},
isbn = {9781450362689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320154.3320165},
doi = {10.1145/3320154.3320165},
abstract = {In the digital Economy 'Data is the new oil. In the last decade technology has disrupted every filed imaginable. One such booming technology is Blockchain. A blockchain is essentially a distributed database of records or public ledger of all transactions or digital events that have been executed and shared among participating parties. And once entered, the information is immutable. Ongoing projects and prior work in the fields of big data, data mining and data science has revealed how relevant data can be used to enhance products and services. There are uncountable applications and advantages of relevant data. The most valuable companies of today treat data as a commodity, which they trade and earn revenues.But use of relevant data has also drawn attention by the other non-conventional organizations and domains. To facilitate such trading, data marketplaces have emerged. In this paper we present a global data marketplace for users to easily buy and share data. The main focus of this research is to have a central data sharing platform for the recycling industry. This paper is a part of the research project "Recycling 4.0" which is focusing on sustainably improving the recycling process through exchange of information. We identify providing secure platform, data integrity and data quality as some major challenges for a data marketplace. In this paper we also explore how global data marketplace could be implemented using blockchain and similar technologies.},
booktitle = {Proceedings of the 2019 International Conference on Blockchain Technology},
pages = {55–59},
numpages = {5},
keywords = {Smart Contracts, Security, Data marketplaces, Data quality, Blockchain},
location = {Honolulu, HI, USA},
series = {ICBCT 2019}
}

@inproceedings{10.1145/3404687.3404703,
author = {Wu, Mingming},
title = {Multi-Task Representation Learning Network for Trajectory Recovery},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404703},
doi = {10.1145/3404687.3404703},
abstract = {Trajectory recovery can benefit many applications such as migration pattern studies of animal and finding hot routes in the urban city. It is necessary to recover trajectory with limited trajectory points to utilize collected trajectory data in a reasonable and efficient way and to provide the better location based service for users. However, the trajectory data involves complex and nonlinear spatial-temporal impacts which cannot be captured by traditional trajectory recovery methods. Moreover, the existing methods consider little about the correlations between trajectory and traffic pattern in the urban city. The superiority of deep neural network makes it possible to recover trajectory with low data quality. We propose a Multi-Task Representation Learning Network (MRL-Net) framework which models the complex nonlinear spatial-temporal correlations in trajectory data with representation learning technique and capture the dependencies of trajectory points with recurrent neural networks. To the best of our knowledge, it is the first paper to address the trajectory recovery problem with representation learning and multi-task learning. Experiments on real-world trajectory data show that our model is superior to state-of-the-art methods.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {79–83},
numpages = {5},
keywords = {Trajectory recovery, Representation learning, Multi-task learning},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.5555/2820489.2820507,
author = {Casale, G. and Ardagna, D. and Artac, M. and Barbier, F. and Nitto, E. Di and Henry, A. and Iuhasz, G. and Joubert, C. and Merseguer, J. and Munteanu, V. I. and P\'{e}rez, J. F. and Petcu, D. and Rossi, M. and Sheridan, C. and Spais, I. and Vladu\v{s}i\v{c}, D.},
title = {DICE: Quality-Driven Development of Data-Intensive Cloud Applications},
year = {2015},
publisher = {IEEE Press},
abstract = {Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.},
booktitle = {Proceedings of the Seventh International Workshop on Modeling in Software Engineering},
pages = {78–83},
numpages = {6},
keywords = {quality assurance, big data, model-driven engineering},
location = {Florence, Italy},
series = {MiSE '15}
}

@inproceedings{10.1145/3478905.3478999,
author = {Huang, Yongliang and Yang, Shulin and Li, Xiang and Peng, Jiao and Zhou, Meiqi},
title = {Research on Publisher Topic Selection Based on Data Mining},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478999},
doi = {10.1145/3478905.3478999},
abstract = {As one of the important traditional industries in China, the printing and publishing industry is facing the current situation of the Internet era with the explosion of information and people's demands tend to be personalized and diversified.How to achieve accurate topic selection is the key.In this context, this paper combines the most popular big data technology with the traditional printing industry, improves the quality of the original data of the publishing house through data preprocessing technology, classifies different types of data by decision tree classifier, and finally completes the data mining.It provides a new thought and method for the topic planning of publishing industry.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {448–452},
numpages = {5},
keywords = {Decision tree, Big data, Data mining, Publishing topics},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/2723372.2742800,
author = {Deshpande, Mukund and Ray, Dhruva and Dixit, Sameer and Agasti, Avadhoot},
title = {ShareInsights: An Unified Approach to Full-Stack Data Processing},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742800},
doi = {10.1145/2723372.2742800},
abstract = {The field of data analysis seeks to extract value from data for either business or scientific benefit. This field has seen a renewed interest with the advent of big data technologies and a new organizational role called data scientist. Even with the new found focus, the task of analyzing large amounts of data is still challenging and time-consuming.The essence of data analysis involves setting up data pipe-lines which consists of several operations that are chained together - starting from data collection, data quality checks, data integration, data analysis and data visualization (including the setting up of interaction paths in that visualization).In our opinion, the challenges stem from from the technology diversity at each stage of the data pipeline as well as the lack of process around the analysis.In this paper we present a platform that aims to significantly reduce the time it takes to build data pipelines. The platform attempts to achieve this in following ways. Allow the user to describe the entire data pipeline with a single language and idioms - all the way from data ingestion to insight expression (via visualization and end-user interaction).Provide a rich library of parts that allow users to quickly assemble a data analysis pipeline in the language.Allow for a collaboration model that allows multiple users to work together on a data analysis pipeline as well as leverage and extend prior work with minimal effort.We studied the efficacy of the platform for a data hackathon competition conducted in our organization. The hackathon provided us with a way to study the impact of the approach. Rich data pipelines which traditionally took weeks to build were constructed and deployed in hours. Consequently, we believe that the complexity of designing and running the data analysis pipeline can be significantly reduced; leading to a marked improvement in the productivity of data analysts/data scientists.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1925–1940},
numpages = {16},
keywords = {data pipeline, big data, data analysis, data visualization},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/2459976.2459991,
author = {Howes, Joshua and Solderitsch, James and Chen, Ignatius and Craighead, Jont\'{e}},
title = {Enabling Trustworthy Spaces via Orchestrated Analytical Security},
year = {2013},
isbn = {9781450316873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2459976.2459991},
doi = {10.1145/2459976.2459991},
abstract = {Cyberspaces require both the implementation of customized functional requirements and the enforcement of policy constraints to be trustworthy. In tailored, distributed and adaptive environments (spaces), monitoring to ensure this enforcement is especially difficult given the wide spectrum of activities performed and the evolving range of threats. Spaces must be monitored from a multitude of perspectives, each of which will generate a vast quantity of disparate information, including structured, semi-structured and unstructured data. However, existing security toolsets and offerings are not yet well equipped to analyze these kinds of data with the necessary speed and agility. Big Data technologies, such as Hadoop, enable the analysis of large and unstructured data sources. We propose security operations teams extend their existing security infrastructure with emerging Big Data analytics and Complex Event Processing platforms. To help them do so, we introduce a conceptual blueprint for the analytics solution. We also present an Orchestrated Analytical Security operational and organizational framework that helps organizations understand how analytical security not only provides monitoring but also creates actionable intelligence from data.},
booktitle = {Proceedings of the Eighth Annual Cyber Security and Information Intelligence Research Workshop},
articleno = {13},
numpages = {4},
keywords = {analytics, security, intelligence, big data, Hadoop, trust, orchestration, complex event processing},
location = {Oak Ridge, Tennessee, USA},
series = {CSIIRW '13}
}

@inproceedings{10.1145/3090354.3090382,
author = {El Bacha, Oussama and Jmad, Othmane and El Bouzekri El Idrissi, Younes and Hmina, Nabil},
title = {Exploiting Open Data to Improve the Business Intelligence &amp; Business Discovery Experience},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090382},
doi = {10.1145/3090354.3090382},
abstract = {The extent to which data mining tools are able to make efficient use of an open data oriented strategy in a smart city is limited. In a sense that it is not fully automated, incompatible or has to be supervised. These sets of tools may offer the possibility to import a dataset in a certain predefined standardized format, still, they do not make it a part of their workflow and algorithms in a fully unsupervised manner (i.e without ongoing human guidance). In a departure from previous research works, in this paper, we present a middleware architecture that exploits open data as background knowledge by acting as a bridge between data mining tools and open data resources.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {27},
numpages = {6},
keywords = {linked, business discovery, knowledge discovery in databases, open data, middleware, data mining, business intelligence, smart city},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@article{10.1145/3177873,
author = {Esteves, Diego and Rula, Anisa and Reddy, Aniketh Janardhan and Lehmann, Jens},
title = {Toward Veracity Assessment in RDF Knowledge Bases: An Exploratory Analysis},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3177873},
doi = {10.1145/3177873},
abstract = {Among different characteristics of knowledge bases, data quality is one of the most relevant to maximize the benefits of the provided information. Knowledge base quality assessment poses a number of big data challenges such as high volume, variety, velocity, and veracity. In this article, we focus on answering questions related to the assessment of the veracity of facts through Deep Fact Validation (DeFacto), a triple validation framework designed to assess facts in RDF knowledge bases. Despite current developments in the research area, the underlying framework faces many challenges. This article pinpoints and discusses these issues and conducts a thorough analysis of its pipeline, aiming at reducing the error propagation through its components. Furthermore, we discuss recent developments related to this fact validation as well as describing advantages and drawbacks of state-of-the-art models. As a result of this exploratory analysis, we give insights and directions toward a better architecture to tackle the complex task of fact-checking in knowledge bases.},
journal = {J. Data and Information Quality},
month = {feb},
articleno = {16},
numpages = {26},
keywords = {linked data, fact checking, benchmark, trustworthiness, exploratory data analysis, DeFacto, data quality}
}

@inproceedings{10.1145/3289100.3289123,
author = {Korachi, Zineb and Bounabat, Bouchaib},
title = {Data Driven Maturity Model for Assessing Smart Cities},
year = {2018},
isbn = {9781450365079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289100.3289123},
doi = {10.1145/3289100.3289123},
abstract = {Smart cities provide the ability to improve the quality of the citizen's life. Transformation into a smart city consists of defining the way ICT (Information and Communication Technologies) can be used to improve the weaker aspects of the city and improve the quality of services provided by public sectors (education, health, transportation...). The growth of the urban population implies the growing needs of urban services (health, education...) and resources (water, energy...). ICT can be used to meet the growing population needs and solve many of today's problems in the private and public sectors (health, transportation, school...). Using mobile phones all citizens produce data and information every day and everywhere, this data will be used to improve the quality of services provided by the city. The quality of the generated data presents the key element that will impact the success of the transformation into a smart city.This paper describes the proposed data quality driven smart cities model. The proposed model, called DQSC-MM (Data Quality Driven Smart Cities Maturity Model). DQSC-MM is used to evaluate the maturity of a smart city based on the quality of produced and consumed data. It suggests a way to measure the importance of data quality in a city's transformation into a smart city. The paper describes how the model was conceived, designed and developed. It describes also a JEE application conceived to support DQSC-MM. The developed application provides the ability to measure data quality and use these measurements for smart city evaluation.},
booktitle = {Proceedings of the 2nd International Conference on Smart Digital Environment},
pages = {140–147},
numpages = {8},
keywords = {Smart city, data quality, maturity model, DQSC-MM, data, data quality measurement, smart city evaluation},
location = {Rabat, Morocco},
series = {ICSDE'18}
}

@inproceedings{10.1145/3358961.3358970,
author = {Posadas, Brianna B. and Hanumappa, Mamatha and Gilbert, Juan E.},
title = {Opinions Concerning Crowdsourcing Applications in Agriculture in D.C.},
year = {2019},
isbn = {9781450376792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358961.3358970},
doi = {10.1145/3358961.3358970},
abstract = {As big data has become increasingly necessary in modern farming techniques, the dependence on high quality and quantity of ground truthing data has risen. Collecting ground truthing data is one of the most labor-intensive aspects of the research process. A crowdsourcing platform application to aid laypeople in completing ground truthing data can improve the quality and quantity of data for growers and agricultural researchers. Focus groups were conducted to gauge opinions on crowdsourcing initiatives in agriculture to inform the design of the platform. Preliminary results demonstrate that the greatest motivation for the participants was having opportunities to develop their skills and access to educational resources. They also discussed having a finite timeframe for collecting the data, feeling appreciated by the researchers, and being informed on the context and next steps of the research. The results of these focus groups will be used to develop design prototypes for the crowdsourcing platform.},
booktitle = {Proceedings of the IX Latin American Conference on Human Computer Interaction},
articleno = {3},
numpages = {4},
keywords = {urban agriculture, focus groups, precision agriculture, big data},
location = {Panama City, Panama},
series = {CLIHC '19}
}

@article{10.1145/3469028,
author = {Tian, Haiman and Presa-Reyes, Maria and Tao, Yudong and Wang, Tianyi and Pouyanfar, Samira and Miguel, Alonso and Luis, Steven and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja Sitharama},
title = {Data Analytics for Air Travel Data: A Survey and New Perspectives},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3469028},
doi = {10.1145/3469028},
abstract = {From the start, the airline industry has remarkably connected countries all over the world through rapid long-distance transportation, helping people overcome geographic barriers. Consequently, this has ushered in substantial economic growth, both nationally and internationally. The airline industry produces vast amounts of data, capturing a diverse set of information about their operations, including data related to passengers, freight, flights, and much more. Analyzing air travel data can advance the understanding of airline market dynamics, allowing companies to provide customized, efficient, and safe transportation services. Due to big data challenges in such a complex environment, the benefits of drawing insights from the air travel data in the airline industry have not yet been fully explored. This article aims to survey various components and corresponding proposed data analysis methodologies that have been identified as essential to the inner workings of the airline industry. We introduce existing data sources commonly used in the papers surveyed and summarize their availability. Finally, we discuss several potential research directions to better harness airline data in the future. We anticipate this study to be used as a comprehensive reference for both members of the airline industry and academic scholars with an interest in airline research.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {167},
numpages = {35},
keywords = {Airline, big data, revenue management}
}

@inproceedings{10.1145/3463677.3463687,
author = {Li, Hongqin and Zhai, Jun},
title = {Research on Suggestions of Improving Chinese Open Government Data in Innovation of Public Governance},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463687},
doi = {10.1145/3463677.3463687},
abstract = {This paper collects a large number of cases and makes a comparative analysis of the typical application of Chinese and American open government data for public governance. Through comparison, this paper finds the gap between China's open government data and the United States, and then analyzes the reasons. On this basis, through the investigation of advanced experience, this paper puts forward the suggestions of open government data to innovate public governance, including data catalogue compilation, data standard formulating, data quality assessment and open government data sharing cooperation, in order to improve Chinese open government data to innovate the public governance level.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {142–152},
numpages = {11},
keywords = {Data quality, Sharing and cooperation},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@inproceedings{10.1145/2882903.2899389,
author = {Hai, Rihan and Geisler, Sandra and Quix, Christoph},
title = {Constance: An Intelligent Data Lake System},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2899389},
doi = {10.1145/2882903.2899389},
abstract = {As the challenge of our time, Big Data still has many research hassles, especially the variety of data. The high diversity of data sources often results in information silos, a collection of non-integrated data management systems with heterogeneous schemas, query languages, and APIs. Data Lake systems have been proposed as a solution to this problem, by providing a schema-less repository for raw data with a common access interface. However, just dumping all data into a data lake without any metadata management, would only lead to a 'data swamp'. To avoid this, we propose Constance, a Data Lake system with sophisticated metadata management over raw data extracted from heterogeneous data sources. Constance discovers, extracts, and summarizes the structural metadata from the data sources, and annotates data and metadata with semantic information to avoid ambiguities. With embedded query rewriting engines supporting structured data and semi-structured data, Constance provides users a unified interface for query processing and data exploration. During the demo, we will walk through each functional component of Constance. Constance will be applied to two real-life use cases in order to show attendees the importance and usefulness of our generic and extensible data lake system.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2097–2100},
numpages = {4},
keywords = {data integration, data quality, data lake},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3064173,
author = {Abdellaoui, Sabrina and Nader, Fahima and Chalal, Rachid},
title = {QDflows: A System Driven by Knowledge Bases for Designing Quality-Aware Data Flows},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3064173},
doi = {10.1145/3064173},
abstract = {In the big data era, data integration is becoming increasingly important. It is usually handled by data flows processes that extract, transform, and clean data from several sources, and populate the data integration system (DIS). Designing data flows is facing several challenges. In this article, we deal with data quality issues such as (1) specifying a set of quality rules, (2) enforcing them on the data flow pipeline to detect violations, and (3) producing accurate repairs for the detected violations. We propose QDflows, a system for designing quality-aware data flows that considers the following as input: (1) a high-quality knowledge base (KB) as the global schema of integration, (2) a set of data sources and a set of validated users’ requirements, (3) a set of defined mappings between data sources and the KB, and (4) a set of quality rules specified by users. QDflows uses an ontology to design the DIS schema. It offers the ability to define the DIS ontology as a module of the knowledge base, based on validated users’ requirements. The DIS ontology model is then extended with multiple types of quality rules specified by users. QDflows extracts and transforms data from sources to populate the DIS. It detects violations of quality rules enforced on the data flows, constructs repair patterns, searches for horizontal and vertical matches in the knowledge base, and performs an automatic repair when possible or generates possible repairs. It interactively involves users to validate the repair process before loading the clean data into the DIS. Using real-life and synthetic datasets, the DBpedia and Yago knowledge bases, we experimentally evaluate the generality, effectiveness, and efficiency of QDflows. We also showcase an interactive tool implementing our system.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {14},
numpages = {39},
keywords = {Data flows, knowledge bases, graph-based repairing, data quality}
}

@article{10.1145/2874239.2874248,
author = {Gotterbarn, Don},
title = {The Creation of Facts in the Cloud: A Fiction in the Making},
year = {2016},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/2874239.2874248},
doi = {10.1145/2874239.2874248},
abstract = {Like most significant changes in technology, Cloud Computing and Big Data along with their associated analytic techniques are claimed to provide us with new insights unattainable by any previous knowledge techniques. It is believed that the quantity of virtual data now available requires new knowledge production strategies. Although they have yielded significant results, there are problems with advocated processes and resulting facts. The primary process treats "pattern recognition" as a final result rather than using "pattern recognition" to lead to yet to be tested testable hypotheses. In data analytics, the discovery of a pattern is treated as knowledge rather than going further to understand the possible causes of those patterns. When this is used as the primary approach to knowledge acquisition unjustified inferences are made - "fact generation". These pseudo-facts are used to generate new pseudo-facts as those initial inferences are fed back into analytic engines as established facts. The approach of generating "facts from data analytics" is introducing highly risky scenarios where "fiction becomes fact" very quickly. These "facts" are then given elevated epistemic status and get used in decision making. This, misleading approach is inconsistent with the moral duty of computing professionals embodied in their Codes of Ethics. There are some ways to mitigate the problems generated by this single path approach to knowledge generation.},
journal = {SIGCAS Comput. Soc.},
month = {jan},
pages = {60–67},
numpages = {8},
keywords = {data misuse, data integrity, big data, professional responsibility}
}

@inproceedings{10.1145/3422713.3422718,
author = {Qi, Yajie and Guo, Chunwei},
title = {Deep Learning-Based Hourly Temperature Prediction: A Case Study of Mega-Cites in North China},
year = {2020},
isbn = {9781450387859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422713.3422718},
doi = {10.1145/3422713.3422718},
abstract = {Accurate prediction of temperature is an important part of fine weather forecast services (such as heating energy consumption in winter, Winter Olympic Games, etc.). Therefore, the accurate prediction of hourly temperature is very significant in the management of human health and the decision-making of government. In this study, a long short term (LSTM) memory model was proposed and used to predict the next hour's temperature in mega-cites in North China. It was fully considered for the historic temperature and meteorological condition. As a result, the predictor secured a fast and accurate prediction performance by fully reflecting the long-term historic process of input time series data through LSTM. The meteorological data from Beijing, Tianjin, Shijiazhuang and Taiyuan which represents the mega-cites of North China during October 1 to December 31 in 2016-2018 were used to verify the validity of the proposed method. In conclusion, the proposed method was proved to have a good prediction performance in cooling and turning warming processes, making up for the poor performance of turning weather prediction in the previous research. It confirmed that the forward supplement LSTM model has the best prediction ability for hourly temperature prediction in Beijing among mega-cites in North China. The results also indicate great potential of the machine learning method in improving local weather forecast and the potential to serve the 2022 Winter Olympics.},
booktitle = {Proceedings of the 2020 3rd International Conference on Big Data Technologies},
pages = {93–96},
numpages = {4},
keywords = {Deep learning, LSTM, North China, Hourly temperature prediction},
location = {Qingdao, China},
series = {ICBDT 2020}
}

@inproceedings{10.1145/3269206.3271809,
author = {Chen, Lihan and Liang, Jiaqing and Xie, Chenhao and Xiao, Yanghua},
title = {Short Text Entity Linking with Fine-Grained Topics},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271809},
doi = {10.1145/3269206.3271809},
abstract = {A wide range of web corpora are in the form of short text, such as QA queries, search queries and news titles. Entity linking for these short texts is quite important. Most of supervised approaches are not effective for short text entity linking. The training data for supervised approaches are not suitable for short text and insufficient for low-resourced languages. Previous unsupervised methods are incapable of handling the sparsity and noisy problem of short text. We try to solve the problem by mapping the sparse short text to a topic space. We notice that the concepts of entities have rich topic information and characterize entities in a very fine-grained granularity. Hence, we use the concepts of entities as topics to explicitly represent the context, which helps improve the performance of entity linking for short text. We leverage our linking approach to segment the short text semantically, and build a system for short entity text recognition and linking. Our entity linking approach exhibits the state-of-the-art performance on several datasets for the realistic short text entity linking problem.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {457–466},
numpages = {10},
keywords = {concepts, entity linking, fine-grained topics, short text},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/2987386.2987413,
author = {Alabduljabbar, Reham and Al-Dossari, Hmood},
title = {A Task Ontology-Based Model for Quality Control in Crowdsourcing Systems},
year = {2016},
isbn = {9781450344555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987386.2987413},
doi = {10.1145/2987386.2987413},
abstract = {In the era of big data, a vast amount of data is created every day. Crowdsourcing systems have recently gained significance as an interesting practice in managing and performing big data operations. Crowdsourcing has facilitated the process of performing tasks that cannot be adequately solved by machines including image labeling, transcriptions, data validation and sentiment analysis. However, quality control remains one of the biggest challenges for crowdsourcing. Current crowdsourcing systems use the same quality control mechanism for evaluating different types of tasks. In this paper, we argue that quality mechanisms vary by task type. We propose a task ontology-based model to identify the most appropriate quality mechanism for a given task. The proposed model has been enriched by a reputation system to collect requesters' feedback on quality mechanisms. Accordingly, the reputation of each mechanism can be established and used for mapping between tasks and mechanisms. Description of the model's framework, algorithms, and its components' interaction are presented.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {22–28},
numpages = {7},
keywords = {Ontology, MTurk, Big Data, Crowdsourcing, Crowd Computing, Reputation, Human Computation, Quality Control, HITs},
location = {Odense, Denmark},
series = {RACS '16}
}

@inproceedings{10.1145/3372938.3372950,
author = {Chahidi, Hamza and Omara, Hicham and Lazaar, Mohamed and Al Achhab, Mohammed},
title = {Impact of Neural Network Architectures on Arabic Sentiment Analysis},
year = {2019},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372950},
doi = {10.1145/3372938.3372950},
abstract = {Sentiment Analysis (SA), commonly known as opinion mining, during last couple of years, it becomes the fastest growing research areas in computer science. Conventionally, it helps to automatically detect if a text express is a positive, negative or neutral opinion. It enables us to identify and extract subjective information in a piece of writing, and this leads to gain an overview of wider public opinions or attitudes toward topics, products or services. Many researches have been done in this area, but most of them have focused on English and other Indo-European languages. Insufficient studies have actually accosted Sentiment Analysis in morphologically rich language such as Arabic. Regardless, given the increasing number of Arabic users and the exponential growth of online content, SA in this language has gained the attention of many researches last years, since Arabic raises many challenges because of its derivational, inflectional and agglutinative morphology. The objective of this paper is to promote the performance of Arabic Sentiment Analysis (ASA) by using Deep learning techniques. For that we implement Multi-Layer perceptron model in order to process and classify a dataset (Tweets). In fact, the experimental results prove that MLP as a deep learning model has a better performance for ASA than classical approaches.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {12},
numpages = {6},
keywords = {Multi-layer Perceptron, Sentiment Analysis, Arabic, Deep Learning, Machine Learning},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/3469968.3469992,
author = {Pan, Zhengjun and Zhao, Lianfen and Zhong, Xingyu and Xia, Zitong},
title = {Application of Collaborative Filtering Recommendation Algorithm in Internet Online Courses},
year = {2021},
isbn = {9781450389808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469968.3469992},
doi = {10.1145/3469968.3469992},
abstract = {Aiming at the problem that the overload of online education platform course resources leads to the difficulty of user selection, this paper mainly studies the improvement and application of collaborative filtering algorithm based on online course recommendation system, which organically combines personalized recommendation technology and online course system to meet the needs of users and online education platform. In the process of recommendation, firstly, user preferences are collected to establish a data model, and user login information and learning behavior information are used as implicit characteristics of user preferences. The loss rate of users in the computing platform is defined, the popularity of each course is calculated, and the relationship between users and courses is constructed, and the correlation and comparative analysis are carried out, Then, the traditional collaborative filtering algorithm is improved by introducing the implicit features after analysis, and the cosine similarity method is used to calculate the course similarity. Finally, the topN recommendation list is generated to get the recommendation results. Based on the desensitization data of an education platform, the experimental results show that the improved recommendation model can improve the precision of recommendation by introducing implicit features.},
booktitle = {Proceedings of the 6th International Conference on Big Data and Computing},
pages = {142–147},
numpages = {6},
keywords = {Collaborative filtering algorithm, online course, recommendation system, education platform},
location = {Shenzhen, China},
series = {ICBDC '21}
}

@inproceedings{10.1145/3372938.3372970,
author = {El Kafhali, Said and Chahir, Chorouk and Hanini, Mohamed and Salah, Khaled},
title = {Architecture to Manage Internet of Things Data Using Blockchain and Fog Computing},
year = {2019},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372970},
doi = {10.1145/3372938.3372970},
abstract = {In this paper, we propose a novel architecture that utilizes features of Blockchain, fog computing, and cloud computing to manage IoT data. Blockchain allows to have a distributed peer-to-peer network in which non-trusting participants can interact with each other without a trusted intermediary or third party. We evaluate how this mechanism works to face the challenges of IoT with respect to multiple accessibility to IoT devises. We consider a Blockchain architecture in presence of edge computing layer. With fog or fog computing, the sensitive data can be analyzed locally instead of sending it to the cloud for analysis. Edge nodes can also keep track and control of the IoT devices that collect, analyze and store data. We show that this control can be better executed when Software Defined Network (SDN) and Network Functions Virtualization (NFV) are integrated into our process for optimal resource management. In this paper, we present our system architecture with a detailed description of the different interactions. We remark that the integration of Blockchain, IoT, and edge computing when coupled with SDN and NFV-enabled cloud infrastructure can bring to more superior and efficient platform for accessing, managing, and processing the huge influx of IoT data.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {32},
numpages = {8},
keywords = {Smart Contracts, SDN, NFV, Internet of Things, Fog computing, Edge Computing, Blockchain},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/3090354.3090404,
author = {Ennajjar, Ibtissam and Tabii, Youness and Benkaddour, Abdelhamid},
title = {Securing Data in Cloud Computing by Classification},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090404},
doi = {10.1145/3090354.3090404},
abstract = {Cloud computing is a wide architecture based on diverse models for providing different services of software and hardware. Cloud computing paradigm attracts different users because of its several benefits such as high resource elasticity, expense reduction, scalability and simplicity which provide significant preserving in terms of investment and work force. However, the new approaches introduced by the cloud, related to computation outsourcing, distributed resources, multi-tenancy concept, high dynamism of the model, data warehousing and the nontransparent style of cloud increase the security and privacy concerns and makes building and handling trust among cloud service providers and consumers a critical security challenge. This paper proposes a new approach to improve security of data in cloud computing. It suggests a classification model to categorize data before being introduced into a suitable encryption system according to the category. Since data in cloud has not the same sensitivity level, encrypting it with the same algorithms can lead to a lack of security or of resources. By this method we try to optimize the resources consumption and the computation cost while ensuring data confidentiality.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {49},
numpages = {5},
keywords = {Data Security, Cloud Computing, Cloud Storage, Classification},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3004725.3004733,
author = {Werner, Martin and Kiermeier, Marie},
title = {A Low-Dimensional Feature Vector Representation for Alignment-Free Spatial Trajectory Analysis},
year = {2016},
isbn = {9781450345828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3004725.3004733},
doi = {10.1145/3004725.3004733},
abstract = {Trajectory analysis is a central problem in the era of big data due to numerous interconnected mobile devices generating unprecedented amounts of spatio-temporal trajectories. Unfortunately, datasets of spatial trajectories are quite difficult to analyse because of the computational complexity of the various existing distance measures. A significant amount of work in comparing two trajectories stems from calculating temporal alignments of the involved spatial points. With this paper, we propose an alignment-free method of representing spatial trajectories using low-dimensional feature vectors by summarizing the combinatorics of shape-derived string sequences. Therefore, we propose to translate trajectories into strings describing the evolving shape of each trajectory, and then provide a sparse matrix representation of these strings using frequencies of adjacencies of characters (n-grams). The final feature vectors are constructed by approximating this matrix with low-dimensional column space using singular value decomposition. New trajectories can be projected into this geometry for comparison. We show that this construction leads to low-dimensional feature vectors with surprising expressive power. We illustrate the usefulness of this approach in various datasets.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
pages = {19–26},
numpages = {8},
keywords = {big data, trajectory, multi-modal trajectory, moving objects},
location = {Burlingame, California},
series = {MobiGIS '16}
}

@inproceedings{10.1145/3322134.3322145,
author = {Al Fanah, Muna and Ansari, Muhammad Ayub},
title = {Understanding E-Learners' Behaviour Using Data Mining Techniques},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322145},
doi = {10.1145/3322134.3322145},
abstract = {The information from Higher Education Institutions (HEIs) is primarily relevant for decision maker and educators. This study tackles e-learners behaviour using machine learning, particularly association rules and classifiers. Learners are characterized by a set of behaviours and attitudes that determine their learning abilities and skills. Learning from data generated by online learners may have significant impacts, however, few studies cover this resource from machine learning perspectives. We examine different data mining techniques including Random Forests, Logistic Regressions and Bayesian Networks as classifiers used for predicting e-learners' classes (High, Medium and Low). The novelty of this study is that it explores and compares classifiers performance on the behaviour of online learners on four variables: raise hands, visiting IT resources, view announcement and discussion impact on e-learners. The results of this study indicate an 80% accuracy level obtained by Bayesian Networks; in contrast, the Random Forests have only 63% accuracy level and Logistic Regressions for 58%.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {59–65},
numpages = {7},
keywords = {Bayesian Networks, Radom Forests, Precision, Accuracy, Association Rules},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/2818314.2818330,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {Bringing the Innovations in Data Management to CS Education: An Educational Reconstruction Approach},
year = {2015},
isbn = {9781450337533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818314.2818330},
doi = {10.1145/2818314.2818330},
abstract = {This paper describes the application of the research framework educational reconstruction for investigating the field data management under a CS education perspective. Like the many other innovations in CS, Big Data and the field data management have strong influences on students' daily lives. In contrast, school does not yet sufficiently prepare students to handle the arising challenges. In this paper we will describe how we apply an educational reconstruction approach to prepare the teaching of essential data management competencies. We will summarize the main goals and principles of educational reconstruction and discuss the application of the framework to the topic data management, as well as first outcomes. Just as educational reconstruction is suitable for finding the essential aspects for teaching data management and for designing classes/courses on this topic, it also seems promising for the curricular development of other CS innovations as well.},
booktitle = {Proceedings of the Workshop in Primary and Secondary Computing Education},
pages = {88–91},
numpages = {4},
keywords = {Big Data, Data Management, CS Education, Educational Reconstruction, Secondary Schools},
location = {London, United Kingdom},
series = {WiPSCE '15}
}

@inproceedings{10.1145/2675316.2675321,
author = {Dashdorj, Zolzaya and Sobolevsky, Stanislav and Serafini, Luciano and Ratti, Carlo},
title = {Human Activity Recognition from Spatial Data Sources},
year = {2014},
isbn = {9781450331425},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675316.2675321},
doi = {10.1145/2675316.2675321},
abstract = {Recent availability of big data of digital traces of human activity boosted research on human behavior. However, in most of the datasets such as mobile phone data or GPS traces, an important layer of information is typically missing: providing an extensive information of when and where people go typically does not allow understanding of what they do there. Predicting the context of human behavior in such cases where such information is not directly available from the data is a complex task that addresses context recognition problems. To fill in the contextual information for such data, we developed an ontological and stochastic model (HRBModel) that interprets semantic (high-level) human behaviors from geographical maps like OpenStreetMap, analyzing the distribution of Points of Interest(POIs), in a given region and time period. The semantic human behaviors are human activities that are accompanied by their likelihood, depending on their location and time. In this paper, we perform an extended evaluation of this model based on other qualitative data source, namely a country-wide anonymized bank card transaction data in Spain, which contains contextual information about the locations and the types of business categories where transactions occurred. This allows us to validate the model, by matching our predicted activity patterns with the actually observed ones, so that it can be later applied to the cases where such information is unavailable. This extended evaluation aimed to define the applicability of the predictive model, HRBModel, taking various type of spatial and temporal factors into account.},
booktitle = {Proceedings of the Third ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
pages = {18–25},
numpages = {8},
keywords = {context recognition, geo-spatial data and knowledge, bank card transactions, statistical matching, big data, urban and environmental planning, spatial data quality and uncertainty, human activity recognition},
location = {Dallas, Texas},
series = {MobiGIS '14}
}

@inproceedings{10.1145/3291801.3291826,
author = {Sun, Donglei and Zeng, Jun and Zhu, Yi and Cao, Xiangyang and Wang, Yiqun and Yang, Bo and Yang, Bin and Wang, Nan and Bo, Qibin and Fu, Yimu and Wei, Jia and Liu, Dong},
title = {Mechanism Design for Unified Management of Power Grid Planning Data},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291826},
doi = {10.1145/3291801.3291826},
abstract = {In order to deal with diversity of massive data structures and the variety of information formats, a novel mechanism is designed for unified management of power grid planning data. By integrating many business systems including production management system (PMS), geographic information system (GIS), energy management system (EMS), distribution network information acquisition system in the power supply company's system and adopting various technical means such as data warehouse technology (e.g. ETL, Extract-Transform-Load) and incremental capture, data structure that supports the whole process management of power grid business is designed, data correlation analysis and integration &amp; migration are carried out, and efficient access and deep fusion of massive relational data, file-type data, distributed data and spatial data are realized. Besides, through computing modes such as diagnostic analysis, load analysis and spatial analysis, the integrated database for power grid planning that integrates data fusion, storage, mining, modeling, computing, analysis and intelligent perception is finally constructed based on the designed data management mechanism, which could provide the comprehensive model and data support for power grid development. Field application shows the engineering benefit of the designed data management mechanism.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {21–26},
numpages = {6},
keywords = {data management, power grid planning, data fusion, mechanism design},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/3314545.3314566,
author = {Mbah, Raymond Blanch K. and Rege, Manjeet and Misra, Bhabani},
title = {Using Spark and Scala for Discovering Latent Trends in Job Markets},
year = {2019},
isbn = {9781450366342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314545.3314566},
doi = {10.1145/3314545.3314566},
abstract = {Job markets are experiencing an exponential growth in data alongside the recent explosion of big data in various domains including health, security and finance. Staying current with job market trends entails collecting, processing and analyzing huge amounts of data. A typical challenge with analyzing job listings is that they vary drastically with regards to verbiage, for instance a given job title or skill can be referred to using different words or industry jargons. As a result, it becomes incumbent to go beyond words present in job listings and carry out analysis aimed at discovering latent structures and trends in job listings. In this paper, we present a systematic approach of uncovering latent trends in job markets using big data technologies (Apache Spark and Scala) and distributed semantic techniques such as latent semantic analysis (LSA). We show how LSA can uncover patterns/relationships/trends that will otherwise remain hidden if using traditional text mining techniques that rely only on word frequencies in documents.},
booktitle = {Proceedings of the 2019 3rd International Conference on Compute and Data Analysis},
pages = {55–62},
numpages = {8},
keywords = {Big Data, Spark, Natural Language Processing(NLP), Latent Semantic Analysis(LSA), Singular Value Decomposition (SVD), Scala},
location = {Kahului, HI, USA},
series = {ICCDA 2019}
}

@article{10.1145/3076253,
author = {Cao, Longbing},
title = {Data Science: A Comprehensive Overview},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3076253},
doi = {10.1145/3076253},
abstract = {The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {43},
numpages = {42},
keywords = {data DNA, statistics, data science, data education, data analytics, big data analytics, computing, data service, informatics, data scientist, data innovation, data profession, data industry, advanced analytics, data engineering, data economy, Big data, data analysis}
}

@inproceedings{10.1145/3285957.3285962,
author = {Li, Xiao-Tong and Zhai, Jun and Zheng, Gui-Fu and Yuan, Chang-Feng},
title = {Quality Assessment for Open Government Data in China},
year = {2018},
isbn = {9781450364898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3285957.3285962},
doi = {10.1145/3285957.3285962},
abstract = {With the development in research of government open data, the issue of data quality becomes more prominent. It's important to accurately judge the data quality before using it. The microcosmic quality assessment not only provides criteria for users to pick up dataset, but also establishes standards for providers' data quality management. In this paper, it sums up 16 types of quality problems through the investigation of three Chinese local government datasets in Beijing, Guangzhou and Harbin, and then constructs 7 quality dimensions and metrics at different granular level to score three cities. The evaluation results reflect that overall score of completeness, accuracy and consistency is low, which will affect the availability of data and mislead users to make wrong decision. On the basis of this evaluation, government could take measures to overcome the weaknesses observed in the open data quality, addressing specific lower score quality aspects.},
booktitle = {Proceedings of the 2018 10th International Conference on Information Management and Engineering},
pages = {110–114},
numpages = {5},
keywords = {open government data, quality dimension, quality assessment, quality metric, Data quality},
location = {Salford, United Kingdom},
series = {ICIME 2018}
}

@inproceedings{10.1145/3357729.3357742,
author = {Belghait, Fodil and April, Alain and Hamet, Pavel and Tremblay, Johanne and Desrosiers, Christian},
title = {A Large-Scale and Extensible Platform for Precision Medicine Research},
year = {2019},
isbn = {9781450372084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357729.3357742},
doi = {10.1145/3357729.3357742},
abstract = {The massive adoption of high-throughput genomics, deep sequencing technologies and big data technologies have made possible the era of precision medicine. However, the volume of data and its complexity remain important challenges for precision medicine research, hindering development in this field. The literature on precision medicine research describes a few platforms to support specific types of studies, but none of these offer researchers the level of customization required to meet their specific needs [1]. Methods: We propose to design and develop a platform able to import and integrate a very large volume of genetics, clinical, demographical and environmental data in a cloud computing infrastructure. In our previous publication, we presented an approach that can customize existing data models to fit any precision medicine research data requirement [1] and the requirement for future large-scale precision medicine platforms, in terms of data extensibility and the scalability of processing on demand. We also proposed a framework to meet the specific requirement of any precision medicine research [2]. In this paper, we describe how this new framework was implemented and trialed by the precision medicine researchers at the Centre Hospitalier Universitaire de l'Universit\'{e} de Montr\'{e}al (CHUM). Results: The data analysis simulations showed that the random forest algorithm presents better accuracy results. We obtained an F1-Score of 72% for random forest, 69% using linear regression and 62% using the neural network algorithm. Conclusion: The results suggest that the proposed precision medicine data analysis platform allows researchers to configure, prepare the analysis environment and customize the platform data model to their specific research in very optimal delays, at very low cost and with minimal technical skills.},
booktitle = {Proceedings of the 9th International Conference on Digital Public Health},
pages = {47–54},
numpages = {8},
keywords = {clinical databases, bioinformatics, precision medicine, genomics, big data},
location = {Marseille, France},
series = {DPH2019}
}

@inproceedings{10.1145/3424978.3425146,
author = {Si, Yaqing and Qin, Siyao and Su, Jing and Wang, Mingyue},
title = {Research on Factors Influencing the Value of Data Products and Pricing Models},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425146},
doi = {10.1145/3424978.3425146},
abstract = {The article focuses on the analysis of data product value influencing factors and establishes a data product pricing model based on value factors. The research reviews the existing research on the value evaluation of data assets, and summarizes the characteristics of data products in the data product trading system based on the alliance chain [1], and then obtains factors for data product value evaluation. Combined with the dynamics and personalized requirements of data products, a value-based three-stage dynamic pricing model for data products is proposed.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {161},
numpages = {5},
keywords = {Pricing strategy, Value factor, Data product, Pricing model},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/3105831.3105834,
author = {Ara\'{u}jo, Tiago Brasileiro and Cappiello, Cinzia and Kozievitch, Nadia Puchalski and Mestre, Demetrio Gomes and Pires, Carlos Eduardo Santos and Vitali, Monica},
title = {Towards Reliable Data Analyses for Smart Cities},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105834},
doi = {10.1145/3105831.3105834},
abstract = {As cities are becoming green and smart, public information systems are being revamped to adopt digital technologies. There are several sources (official or not) that can provide information related to a city. The availability of multiple sources enables the design of advanced analyses for offering valuable services to both citizens and municipalities. However, such analyses would fail if the considered data were affected by errors and uncertainties: Data Quality is one of the main requirements for the successful exploitation of the available information. This paper highlights the importance of the Data Quality evaluation in the context of geographical data sources. Moreover, we describe how the Entity Matching task can provide additional information to refine the quality assessment and, consequently, obtain a better evaluation of the reliability data sources. Data gathered from the public transportation and urban areas of Curitiba, Brazil, are used to show the strengths and effectiveness of the presented approach.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {304–308},
numpages = {5},
keywords = {Data Quality, Smart Cities, Entity Matching, Data Analysis},
location = {Bristol, United Kingdom},
series = {IDEAS '17}
}

@article{10.1145/3097570,
author = {Lin, Jimmy and Milligan, Ian and Wiebe, Jeremy and Zhou, Alice},
title = {Warcbase: Scalable Analytics Infrastructure for Exploring Web Archives},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3097570},
doi = {10.1145/3097570},
abstract = {Web archiving initiatives around the world capture ephemeral Web content to preserve our collective digital memory. However, unlocking the potential of Web archives for humanities scholars and social scientists requires a scalable analytics infrastructure to support exploration of captured content. We present Warcbase, an open-source Web archiving platform that aims to fill this need. Our platform takes advantage of modern open-source “big data” infrastructure, namely Hadoop, HBase, and Spark, that has been widely deployed in industry. Warcbase provides two main capabilities: support for temporal browsing and a domain-specific language that allows scholars to interrogate Web archives in several different ways. This work represents a collaboration between computer scientists and historians, where we have engaged in iterative codesign to build tools for scholars with no formal computer science training. To provide guidance, we propose a process model for scholarly interactions with Web archives that begins with a question and proceeds iteratively through four main steps: filter, analyze, aggregate, and visualize. We call this the FAAV cycle for short and illustrate with three prototypical case studies. This article presents the current state of the project and discusses future directions.},
journal = {J. Comput. Cult. Herit.},
month = {jul},
articleno = {22},
numpages = {30},
keywords = {Apache HBase, Big data, WARC, ARC, Apache Hadoop, Apache Spark}
}

@inproceedings{10.1145/3230905.3230932,
author = {Maqboul, Jaouad and Bounabat, Bouchaib},
title = {An Approach of Data-Driven Framework Alignment to Knowledge Base},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230932},
doi = {10.1145/3230905.3230932},
abstract = {When we talk about quality, we cannot do without mentioning the cost of quality and non-quality, the cost increases if the quality also increases; to maintain quality in small data is easier than huge data like big data or knowledge base.Companies tend to use the knowledge base to perfect and facilitate their work, thus satisfying the end customer, however the non-quality of these bases will penalize the company, so it is necessary to improve the quality, the question is when and why to improve quality, our proposal is based on the cost and impact of this improvement, if the impact is greater than the cost then it is recommended to improve completeness in our case study.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {40},
numpages = {5},
keywords = {framework, knowledge Base, Knowledge, complexity, impact, completeness, prediction, data quality, java EE, Business process},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@inproceedings{10.1145/3448016.3457250,
author = {Song, Jie and He, Yeye},
title = {Auto-Validate: Unsupervised Data Validation Using Data-Domain Patterns Inferred from Data Lakes},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457250},
doi = {10.1145/3448016.3457250},
abstract = {Complex data pipelines are increasingly common in diverse applications such as BI reporting and ML modeling. These pipelines often recur regularly (e.g., daily or weekly), as BI reports need to be refreshed, and ML models need to be retrained. However, it is widely reported that in complex production pipelines, upstream data feeds can change in unexpected ways, causing downstream applications to break silently that are expensive to resolve. Data validation has thus become an important topic, as evidenced by notable recent efforts from Google and Amazon, where the objective is to catch data quality issues early as they arise in the pipelines. Our experience on production data suggests, however, that on string-valued data, these existing approaches yield high false-positive rates and frequently require human intervention. In this work, we develop a corpus-driven approach to auto-validate machine-generated data by inferring suitable data-validation "patterns'' that accurately describe the underlying data-domain, which minimizes false-positives while maximizing data quality issues caught. Evaluations using production data from real data lakes suggest that sj is substantially more effective than existing methods. Part of this technology ships as an Auto-Tag feature in Microsoft Azure Purview.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1678–1691},
numpages = {14},
keywords = {data lake, data quality, data pipelines, data validation},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1145/3422669,
author = {Fraihat, Salam and Salameh, Walid A. and Elhassan, Ammar and Tahoun, Bushra Abu and Asasfeh, Maisa},
title = {Business Intelligence Framework Design and Implementation: A Real-Estate Market Case Study},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3422669},
doi = {10.1145/3422669},
abstract = {This article builds on previous work in the area of real-world applications of Business Intelligence (BI) technology. It illustrates the analysis, modeling, and framework design of a BI solution with high data quality to provide reliable analytics and decision support in the Jordanian real estate market. The motivation is to provide analytics dashboards to potential investors about specific segments or units in the market. The article ekxplains the design of a BI solution, including background market and technology investigation, problem domain requirements, solution architecture modeling, design and testing, and the usability of descriptive and predictive features. The resulting framework provides an effective BI solution with user-friendly market insights for investors with little or no market knowledge. The solution features predictive analytics based on established Machine Learning modeling techniques, analyzed and contrasted to select the optimum methodology and model combination for predicting market behavior to empower inexperienced users.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {10},
numpages = {16},
keywords = {predictive analytics, real estate, Business intelligence, data quality}
}

@inproceedings{10.1145/3501247.3539504,
author = {Sen, Indira and Fr\"{o}hling, Leon and Weller, Katrin},
title = {Documenting Web Data for Social Research (#DocuWeb22): A Participatory Workshop for Developing Structured and Reusable Practices},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3539504},
doi = {10.1145/3501247.3539504},
abstract = {With this half-day, in-person workshop, we attempt to collaboratively discover best practices as well as frequent pitfalls encountered when working with Web data. Participants will first be presented with different perspectives on the significance of data quality in this specific context and familiarized with existing, structured approaches for the critical reflection on and documentation of data collection processes, before being invited to share their own experiences with the collection, use and documentation of Web data. We hope to thereby inspire participants to further integrate data documentation practices into their research processes, and for us to learn from the participants’ experiences in order to improve upon existing documentation frameworks for Web data. More details of the workshop, including the planned activities can be found at&nbsp;https://frohleon.github.io/DocuWeb22/.},
booktitle = {14th ACM Web Science Conference 2022},
pages = {478–479},
numpages = {2},
keywords = {guidelines, web data, data collection, dataset documentation, data quality},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@inproceedings{10.1109/MICRO.2018.00056,
author = {Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
title = {CounterMiner: Mining Big Performance Data from Hardware Counters},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00056},
doi = {10.1109/MICRO.2018.00056},
abstract = {Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events1. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running ina"24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance.In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {613–626},
numpages = {14},
keywords = {data mining, computer architecture, performance counters, big data},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1145/2484712.2484715,
author = {de Mendon\c{c}a, Rogers Reiche and da Cruz, S\'{e}rgio Manuel Serra and De La Cerda, Jonas F. S. M. and Cavalcanti, Maria Cl\'{a}udia and Cordeiro, Kelli Faria and Campos, Maria Luiza M.},
title = {LOP: Capturing and Linking Open Provenance on LOD Cycle},
year = {2013},
isbn = {9781450321945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484712.2484715},
doi = {10.1145/2484712.2484715},
abstract = {The Web of Data has emerged as a means to expose, share, reuse, and connect information on the Web identified by URIs using RDF as a data model, following Linked Data Principles. However, the reuse of third party data can be compromised without proper data quality assessments. In this context, important questions emerge: how can one trust on published data and links? Which manipulation, modification and integration operations have been applied to the data before its publication? What is the nature of comparisons or transformations applied to data during the interlinking process? In this scenario, provenance becomes a fundamental element. In this paper, we describe an approach for generating and capturing Linked Open Provenance (LOP) to support data quality and trustworthiness assessments, which covers preparation and format transformation of traditional data sources, up to dataset publication and interlinking. The proposed architecture takes advantage of provenance agents, orchestrated by an ETL workflow approach, to collect provenance at any specified level and also link it with its corresponding data. We also describe a real use case scenario where the architecture was implemented to evaluate the proposal.},
booktitle = {Proceedings of the Fifth Workshop on Semantic Web Information Management},
articleno = {3},
numpages = {8},
keywords = {linked open data, linked data, interoperability, data quality, provenance, ETL},
location = {New York, New York},
series = {SWIM '13}
}

@inproceedings{10.1145/3545801.3545816,
author = {Bei, Yijun and Gao, Kewei and Wang, Linxin},
title = {Supervised Information Extraction of Chinese Equipment Maintenance Documents},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545801.3545816},
doi = {10.1145/3545801.3545816},
abstract = {The service data of equipment maintenance is scattered, complex and uncorrelated, and depends on the experience of experts. This paper focuses on the knowledge extraction technology of equipment maintenance documents. However, equipment maintenance documents are characterized by no obvious boundary symbols, many professional terms, and rich context information. Therefore, Convolution Neural Network - Bi-directional Long Short-Term Memory - Conditional Random Field(CNN-BiLSTM-CRF) entity extraction model is designed in this paper on the basis of Bi-directional Long Short-Term Memory – Conditional Random Field(BiLSTM-CRF) model. Aiming at the normative and implicit characteristics of equipment documents, this paper designed a relationship extraction method based on the fusion of pattern and CNN. Experimental results show that both models have good results.},
booktitle = {Proceedings of the 7th International Conference on Big Data and Computing},
pages = {102–108},
numpages = {7},
keywords = {Knowledge Extraction, Entity Extraction, Equipment Maintenance, Knowledge Graph, Relation Extraction},
location = {Shenzhen, China},
series = {ICBDC '22}
}

@inproceedings{10.1145/3152723.3152730,
author = {Chaofan, Dai and Ran, Zhang and Pei, Li and Wenqian, Wang},
title = {Design of ETL Provenance Tool Based on Minimal Attribute Set},
year = {2017},
isbn = {9781450353564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152723.3152730},
doi = {10.1145/3152723.3152730},
abstract = {For the ETL process, this paper designs a provenance tool based on inversible transformation, and describes the meta-information of ETL and data provenance process in two ways: one is to take the database two-dimensional table to describe the relevant information in logical level, easy to record; the other is the use of PROV model information on the xml description, and shows the ETL and the provenance process in the directed acyclic graph, easy to understand.},
booktitle = {Proceedings of the 2017 International Conference on Big Data Research},
pages = {57–61},
numpages = {5},
keywords = {PROV, ETL, Metadata, minimal attribute set, data provenance},
location = {Osaka, Japan},
series = {ICBDR 2017}
}

@inproceedings{10.1145/3437075.3437091,
author = {Majthoub, Manar and Odeh, Yousra and Hijjawi, Mohammed},
title = {Non-Functional Requirements Classification for Aligning Business with Information Systems},
year = {2020},
isbn = {9781450375061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437075.3437091},
doi = {10.1145/3437075.3437091},
abstract = {Non-Functional Requirements (NFR) are defined as the desired quality requirements, such as availability, that restrict software product being developed where some external restrictions may apply. Since information systems have been introduced, organizations in the business world align their functional activities with systems without paying attention to quality-based alignment. Few research works have been conducted in order to classify and integrate the NFR with business or system models. But these classifications and integrations are only confined to either the business side or the system side, which in turn have caused in having a gap in mapping the classifications between the two sides. Because business models and system models mutually affect each other in many ways, their NFR integration and classification should be aligned with each other. Having a NFR alignment-based classification between business and information systems contributes to assist the stakeholders in reflecting the quality requirements at the business side for a particular task on the related tasks integrated with NFRs at the systems side. Also having an alignment-oriented classification contributes to trace quality/NFR-based changes from the business organization to its systems and vice versa.In this research, we propose a NFR classification for aligning quality requirements in business with their NFRs in information systems. The work in business side is represented through business process models designed using Business Process Model and Notation (BPMN) where the use case models represents the system side in this research. The proposed classification is demonstrated in both business and systems using the academic advising and registration case study at Applied Science University in Jordan.},
booktitle = {Proceedings of the 2020 International Conference on Big Data in Management},
pages = {84–89},
numpages = {6},
keywords = {Quality Requirements, System Model, Business Process Model, Non-Functional Requirements, Business/IT Alignment, Business Models, Use Case},
location = {Manchester, United Kingdom},
series = {ICBDM 2020}
}

@article{10.14778/3401960.3401965,
author = {Tan, Zijing and Ran, Ai and Ma, Shuai and Qin, Sheng},
title = {Fast Incremental Discovery of Pointwise Order Dependencies},
year = {2020},
issue_date = {June 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3401960.3401965},
doi = {10.14778/3401960.3401965},
abstract = {Pointwise order dependencies (PODs) are dependencies that specify ordering semantics on attributes of tuples. POD discovery refers to the process of identifying the set Σ of valid and minimal PODs on a given data set D. In practice D is typically large and keeps changing, and it is prohibitively expensive to compute Σ from scratch every time. In this paper, we make a first effort to study the incremental POD discovery problem, aiming at computing changes ΔΣ to Σ such that Σ ⊕ ΔΣ is the set of valid and minimal PODs on D with a set ΔD of tuple insertion updates. (1) We first propose a novel indexing technique for inputs Σ and D. We give algorithms to build and choose indexes for Σ and D, and to update indexes in response to ΔD. We show that POD violations w.r.t. Σ incurred by ΔD can be efficiently identified by leveraging the proposed indexes, with a cost dependent on log(|D|). (2) We then present an effective algorithm for computing ΔΣ, based on Σ and identified violations caused by ΔD. The PODs in Σ that become invalid on D + ΔD are efficiently detected with the proposed indexes, and further new valid PODs on D + ΔD are identified by refining those invalid PODs in Σ on D + ΔD. (3) Finally, using both real-life and synthetic datasets, we experimentally show that our approach outperforms the batch approach that computes from scratch, up to orders of magnitude.},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {1669–1681},
numpages = {13}
}

@inproceedings{10.1145/3226116.3226135,
author = {Huang, Huijun and Zheng, Jiguang},
title = {Quality Earned Value Analysis Based on IFPUG Method in Software Project},
year = {2018},
isbn = {9781450364270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3226116.3226135},
doi = {10.1145/3226116.3226135},
abstract = {Earned value method is an important tool to evaluate and control the schedule and cost of the project. It is widely used in engineering construction projects, but rarely used in software projects. Due to the characteristics of the software project, the accuracy of the calculation of the basic parameters of the earned value analysis is low, which leads to the fact that the credibility of the result of the earned value analysis becomes very low or even meaningless. In order to make Earned value method applied to software projects better , IFPUG function points method is used to measure the actual completion of software projects, then used earned value method on the basis of IFPUG function point method. This can improved the accuracy of earned value analysis better. In order to analyze the cost and schedule of software projects better, the quality factors of software are also taken into account in the analysis of earned value, this can monitor the cost and schedule of software projects accurately.},
booktitle = {Proceedings of 2018 International Conference on Big Data Technologies},
pages = {101–108},
numpages = {8},
keywords = {software quality, software project, function point method, earned value analysis},
location = {Hangzhou, China},
series = {ICBDT '18}
}

@inproceedings{10.1145/3357223.3362727,
author = {Teoh, Jason and Gulzar, Muhammad Ali and Xu, Guoqing Harry and Kim, Miryung},
title = {PerfDebug: Performance Debugging of Computation Skew in Dataflow Systems},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362727},
doi = {10.1145/3357223.3362727},
abstract = {Performance is a key factor for big data applications, and much research has been devoted to optimizing these applications. While prior work can diagnose and correct data skew, the problem of computation skew---abnormally high computation costs for a small subset of input data---has been largely overlooked. Computation skew commonly occurs in real-world applications and yet no tool is available for developers to pinpoint underlying causes.To enable a user to debug applications that exhibit computation skew, we develop a post-mortem performance debugging tool. PerfDebug automatically finds input records responsible for such abnormalities in a big data application by reasoning about deviations in performance metrics such as job execution time, garbage collection time, and serialization time. The key to PerfDebug's success is a data provenance-based technique that computes and propagates record-level computation latency to keep track of abnormally expensive records throughout the pipeline. Finally, the input records that have the largest latency contributions are presented to the user for bug fixing. We evaluate PerfDebug via in-depth case studies and observe that remediation such as removing the single most expensive record or simple code rewrite can achieve up to 16X performance improvement.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {465–476},
numpages = {12},
keywords = {data provenance, data intensive scalable computing, fault localization, Performance debugging, big data systems},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

