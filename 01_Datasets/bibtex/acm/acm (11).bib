@article{10.1145/3057266,
author = {Perera, Charith and Qin, Yongrui and Estrella, Julio C. and Reiff-Marganiec, Stephan and Vasilakos, Athanasios V.},
title = {Fog Computing for Sustainable Smart Cities: A Survey},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3057266},
doi = {10.1145/3057266},
abstract = {The Internet of Things (IoT) aims to connect billions of smart objects to the Internet, which can bring a promising future to smart cities. These objects are expected to generate large amounts of data and send the data to the cloud for further processing, especially for knowledge discovery, in order that appropriate actions can be taken. However, in reality sensing all possible data items captured by a smart object and then sending the complete captured data to the cloud is less useful. Further, such an approach would also lead to resource wastage (e.g., network, storage, etc.). The Fog (Edge) computing paradigm has been proposed to counterpart the weakness by pushing processes of knowledge discovery using data analytics to the edges. However, edge devices have limited computational capabilities. Due to inherited strengths and weaknesses, neither Cloud computing nor Fog computing paradigm addresses these challenges alone. Therefore, both paradigms need to work together in order to build a sustainable IoT infrastructure for smart cities. In this article, we review existing approaches that have been proposed to tackle the challenges in the Fog computing domain. Specifically, we describe several inspiring use case scenarios of Fog computing, identify ten key characteristics and common features of Fog computing, and compare more than 30 existing research efforts in this domain. Based on our review, we further identify several major functionalities that ideal Fog computing platforms should support and a number of open challenges toward implementing them, to shed light on future research directions on realizing Fog computing for building sustainable smart cities.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {32},
numpages = {43},
keywords = {fog computing, Internet of things, sustainability, smart cities}
}

@inproceedings{10.1145/3548785.3548811,
author = {Passi, Kalpdrum and Shah, Aanan},
title = {Distinguishing Fake and Real News of Twitter Data with the Help of Machine Learning Techniques},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548785.3548811},
doi = {10.1145/3548785.3548811},
abstract = {News articles have an influence on people's belief and views about various circumstances. In this regard, some news publishers with political or ideological bias try to spread news which are distorted or totally wrong. Natural language processing was used to preprocess the text. Some general features like, number of words, sentences, stopwords, non-alphabetic words, verbs, nouns, and adjectives were identified. Word positioning was labeled to distinguish a word as a noun, a pronoun, an adjective or a verb in the sentences. Preprocessing was followed by feature extraction methods namely, count vectorizer, Term Frequency-Inverse Document Frequency (TF-IDF) vectorizer and word2vec embedding. It was observed that the results obtained by TF-IDF feature extraction method were superior compared with the other two methods. Various machine learning models were used for training the model namely, Naive Bayes, Logistic Regression, Random Forest, K-nearest neighbors (KNN), Support Vector Machine (SVM) and Recurrent Neural Network (RNN) as a deep learning model. The models were successfully tested on two datasets. On the first dataset, SVM achieved an accuracy of 98.5% and RNN achieved an accuracy of 98.03% which is much improvement over the best results of Agarwalla et al., 2019 (83.16 % accuracy). On the second dataset, SVM achieved an accuracy of 97.76%, RNN achieved 97.1% and Logistic Regression achieved 97.50% which is an improvement over the best results of Vijayraghavan et al. 2020 (94.88% accuracy).},
booktitle = {Proceedings of the 26th International Database Engineered Applications Symposium},
pages = {1–8},
numpages = {8},
location = {Budapest, Hungary},
series = {IDEAS '22}
}

@inproceedings{10.1145/3325112.3325243,
author = {Chen, Tao and Ran, Longya and Gao, Xian},
title = {AI Innovation for Advancing Public Service: The Case of China's First Administrative Approval Bureau},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325243},
doi = {10.1145/3325112.3325243},
abstract = {The adoption of artificial intelligence (AI) is becoming increasingly popular in the public sector, but there is a severe lack of relevant theoretical research. The government of China also has high expectations for AI innovation. This paper proposes a four-stage model for AI development in public sectors to help public administrators think about the impact of AI on their organizations. We empirically investigate a case of AI adoption for delivering public services in local government in China. The findings improve our understanding of not only the status of AI innovation but also the factors motivating and challenging public sectors that are intending to adopt AI. Given that AI application in public sectors is still in its infancy, this study provides us with an opportunity to conduct longitudinal tracking of AI innovation in local government in China. This could help public administrators to think more comprehensively about the changes and transformations that AI may bring to the public sector.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {100–108},
numpages = {9},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1145/2740908.2742563,
author = {Deng, Alex},
title = {Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742563},
doi = {10.1145/2740908.2742563},
abstract = {As A/B testing gains wider adoption in the industry, more people begin to realize the limitations of the traditional frequentist null hypothesis statistical testing (NHST). The large number of search results for the query ``Bayesian A/B testing'' shows just how much the interest in the Bayesian perspective is growing. In recent years there are also voices arguing that Bayesian A/B testing should replace frequentist NHST and is strictly superior in all aspects. Our goal here is to clarify the myth by looking at both advantages and issues of Bayesian methods. In particular, we propose an objective Bayesian A/B testing framework for which we hope to bring the best from Bayesian and frequentist methods together. Unlike traditional methods, this method requires the existence of historical A/B test data to objectively learn a prior. We have successfully applied this method to Bing, using thousands of experiments to establish the priors.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {923–928},
numpages = {6},
keywords = {controlled experiments, objective bayes, a/b testing, prior, optional stopping, multiple testing, empirical bayes, bayesian statistics},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@article{10.1145/3545797,
author = {Zhang, Hang and Yang, Yajun and Wang, Xin and Gao, Hong and Hu, Qinghua},
title = {MLI: A Multi-Level Inference Mechanism for User Attributes in Social Networks},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3545797},
doi = {10.1145/3545797},
abstract = {In the social network, each user has attributes for self-description called user attributes which are semantically hierarchical. Attribute inference has become an essential way for social platforms to realize user classifications and targeted recommendations. Most existing approaches mainly focus on the flat inference problem neglecting the semantic hierarchy of user attributes which will cause serious inconsistency in multi-level tasks. In this article, we propose a multi-level model MLI, where information propagation part collects attribute information by mining the global graph structure, and the attribute correction part realizes the mutual correction between different levels of attributes. Further, we put forward the concept of generalized semantic tree, a way of representing the hierarchical structure of user attributes, whose nodes are allowed to have multiple parent nodes unlike the regular tree. Both regular and generalized semantic tree are commonly used in practice, and can be handled by our model. Besides, by making the inference start from sub-networks with sufficient attribute information, we design a “Ripple” algorithm to improve the efficiency and effectiveness of our model. For evaluation purposes, we conduct extensive verification experiments on DBLP datasets. The experimental results show the superior effect of MLI, compared with the state-of-the-art methods.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = {jun},
keywords = {attribute inference, social network, hierarchical inference}
}

@article{10.1145/3527546.3527555,
author = {Ahlers, Dirk and Wilde, Erik and Spaniol, Marc and Baeza-Yates, Ricardo and Alonso, Omar},
title = {Report on the 11th International Workshop on Location and the Web (LocWeb 2021) and the 11th Temporal Web Analytics Workshop (TempWeb2021) at WWW2021},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3527546.3527555},
doi = {10.1145/3527546.3527555},
abstract = {LocWeb and TempWeb 2021 were the eleventh events in their workshop series and took place co-located on 12th April 2021 in conjunction with The Web Conference WWW 2021. They were intended to be held in Ljubljana, Slovenia as a potentially hybrid event, but due to the pandemic, were fully moved online.LocWeb and TempWeb were held as one colocated session with a merged programme and shared topics to explore similarities and introduce attendees to the two related and complementary areas. LocWeb 2021 explored the intersection of location-based analytics and Web architecture with a focus on on Web-scale services and location-aware information access. TempWeb 2021 discussed temporal analytics at a Web scale with experts from science and industry.Date: 12 April, 2021.Websites: https://dhere.de/locweb/locweb2021 and http://temporalweb.net/.},
journal = {SIGIR Forum},
month = {mar},
articleno = {6},
numpages = {7}
}

@inproceedings{10.1145/3091478.3091521,
author = {Xu, Jiejun and Xie, Daniel and Lu, Tsai-Ching and Cafeo, John},
title = {EDSV: Emerging Defect Surveillance for Vehicles},
year = {2017},
isbn = {9781450348966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091478.3091521},
doi = {10.1145/3091478.3091521},
abstract = {We present early findings on building a proof of concept for an automated system to identify emerging trends regarding vehicle defects. The proposed system functions by continuously collecting and monitoring publicly available data from several heterogeneous channels ranging from online social media to vehicle enthusiast forums and consumer reporting sites. By mining the collected data, the system would provide real-time detection of ongoing consumer issues with vehicles. In addition, our system has special emphasis on detecting early signals prior to the widespread knowledge of the general public. One of the system components involves estimating a baseline statistical distribution governing the frequency of observing specific types of vehicle defective complaints from our data sources and subsequently identifying irregular deviations from this distribution. A web interface is made available to visualize descriptive statistics derived from various channels, with the intent to provide timely insights for human analysts.},
booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
pages = {219–222},
numpages = {4},
keywords = {business intelligence, quality management, measurement, online social media, user generated content},
location = {Troy, New York, USA},
series = {WebSci '17}
}

@inproceedings{10.1145/2666310.2666471,
author = {Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang},
title = {HMM-Based Address Parsing: Efficiently Parsing Billions of Addresses on MapReduce},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666471},
doi = {10.1145/2666310.2666471},
abstract = {Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches to build models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses with Hadoop.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {433–436},
numpages = {4},
keywords = {large-scale data, address parsing, record linkage},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/3269206.3271798,
author = {Kuo, Yu-Hsuan and Li, Zhenhui and Kifer, Daniel},
title = {Detecting Outliers in Data with Correlated Measures},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271798},
doi = {10.1145/3269206.3271798},
abstract = {Advances in sensor technology have enabled the collection of large-scale datasets. Such datasets can be extremely noisy and often contain a significant amount of outliers that result from sensor malfunction or human operation faults. In order to utilize such data for real-world applications, it is critical to detect outliers so that models built from these datasets will not be skewed by outliers. In this paper, we propose a new outlier detection method that utilizes the correlations in the data (e.g., taxi trip distance vs. trip time). Different from existing outlier detection methods, we build a robust regression model that explicitly models the outliers and detects outliers simultaneously with the model fitting. We validate our approach on real-world datasets against methods specifically designed for each dataset as well as the state of the art outlier detectors. Our outlier detection method achieves better performances, demonstrating the robustness and generality of our method. Last, we report interesting case studies on some outliers that result from atypical events.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {287–296},
numpages = {10},
keywords = {contextual outlier detection, robust regression},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1145/3352020.3352033,
author = {Du, Yifan and Issarny, Val\'{e}rie and Sailhan, Fran\c{c}oise},
title = {When the Power of the Crowd Meets the Intelligence of the Middleware: The Mobile Phone Sensing Case},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/3352020.3352033},
doi = {10.1145/3352020.3352033},
abstract = {The data gluttony of AI is well known: Data fuels the artificial intelligence. Technologies that help to gather the needed data are then essential, among which the IoT. However, the deployment of IoT solutions raises significant challenges, especially regarding the resource and financial costs at stake. It is our view that mobile crowdsensing, aka phone sensing, has a major role to play because it potentially contributes massive data at a relatively low cost. Still, crowdsensing is useless, and even harmful, if the contributed data are not properly analyzed. This paper surveys our work on the development of systems facing this challenge, which also illustrates the virtuous circles of AI. We specifically focus on how intelligent crowdsensing middleware leverages on-device machine learning to enhance the reported physical observations. Keywords: Crowdsensing, Middleware, Online learning.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jul},
pages = {85–90},
numpages = {6}
}

@article{10.1145/2795403.2795412,
author = {Alonso, Omar and Kamps, Jaap and Karlgren, Jussi},
title = {Report on the Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14)},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/2795403.2795412},
doi = {10.1145/2795403.2795412},
abstract = {There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remained to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side---the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues---and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in "graph search" change the classic division between searchers and information and lead to extreme personalization---are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects---how to not creep out users.There was a strong feeling that we made substantial progress. Specifically, the discussion contributed to our understanding of the way forward. First, for notable (head, shoulder, but not tail) entities in semantic search we have reached the level of quality at minimal costs allowing for deployment in major web search engines---the dream has become a reality. Second, entity detection is moving fast into domain specific, personal, and business domains, and has become a vital component for a range of applications. Third, semantic web has exchanged logic for machine learning approaches, and machine learning is the natural unification of semantic web and information retrieval approaches.},
journal = {SIGIR Forum},
month = {jun},
pages = {27–34},
numpages = {8}
}

@inproceedings{10.1145/2187980.2188029,
author = {Soldatos, John and Draief, Moez and Macdonald, Craig and Ounis, Iadh},
title = {Multimedia Search over Integrated Social and Sensor Networks},
year = {2012},
isbn = {9781450312301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187980.2188029},
doi = {10.1145/2187980.2188029},
abstract = {This paper presents work in progress within the FP7 EU-funded project SMART to develop a multimedia search engine over content and information stemming from the physical world, as derived through visual, acoustic and other sensors. Among the unique features of the search engine is its ability to respond to social queries, through integrating social networks with sensor networks. Motivated by this innovation, the paper presents and discusses the state-of-the-art in participatory sensing and other technologies blending social and sensor networks.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {283–286},
numpages = {4},
keywords = {multimedia, sensors, search engine},
location = {Lyon, France},
series = {WWW '12 Companion}
}

@inproceedings{10.1145/3510858.3510863,
author = {Sun, Fangyu},
title = {Manufacturing Audit Quality Analysis Model Based on Data Mining Technology},
year = {2021},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510863},
doi = {10.1145/3510858.3510863},
abstract = {As our country's economic development enters a new normal, the original manufacturing development model can no longer meet the needs of current economic development, and it is urgent to accelerate the transformation of the manufacturing industry. At present, the country's supply-side structural reforms are deepening, and listed manufacturing companies are the most important backbone in terms of scale and innovation opportunities. Data mining technology is used to study the impact of quality control on corporate performance. Listed companies have a positive impact on the further realization of transformation and upgrading and the improvement of corporate performance. This article aims to study the manufacturing audit quality analysis model based on data mining technology, and adopts the analysis method of the combination of supervisory research and empirical analysis, from the perspective of supervisory research, summarizes the theory of internal audit quality and company performance in the research process, and summarizes predecessors' research results and research ideas. The experimental data in this article shows that the average quality of internal audit information disclosure is 3.1156, indicating that the audit disclosure status of listed companies selected by the Shenzhen Stock Exchange is good, and to a certain extent reflects the quality level of some internal controls.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {1–6},
numpages = {6},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3209978.3210194,
author = {Kumar Chandrasekaran, Muthu and Jaidka, Kokil and Mayr, Philipp},
title = {Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018)},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210194},
doi = {10.1145/3209978.3210194},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Information retrieval~(IR), bibliometric and natural language processing (NLP) techniques could enhance scholarly search, retrieval and user experience but are not yet widely used. To this purpose, we propose the third iteration of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL). The workshop is intended to stimulate IR, NLP researchers and Digital Library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop will incorporate multiple invited talks, paper sessions, a poster session and the 4th edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {1415–1418},
numpages = {4},
keywords = {text mining, citation analysis, digital libraries, natural language processing, bibliometrics, information extraction, information retrieval},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3107091.3107093,
author = {Truong, Hong-Linh and Berardinelli, Luca},
title = {Testing Uncertainty of Cyber-Physical Systems in IoT Cloud Infrastructures: Combining Model-Driven Engineering and Elastic Execution},
year = {2017},
isbn = {9781450351126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107091.3107093},
doi = {10.1145/3107091.3107093},
abstract = {Today's cyber-physical systems (CPS) span IoT and cloud-based datacenter infrastructures, which are highly heterogeneous with various types of uncertainty. Thus, testing uncertainties in these CPS is a challenging and multidisciplinary activity. We need several tools for modeling, deployment, control, and analytics to test and evaluate uncertainties for different configurations of the same CPS. In this paper, we explain why using state-of-the art model-driven engineering (MDE) and model-based testing (MBT) tools is not adequate for testing uncertainties of CPS in IoT Cloud infrastructures. We discus how to combine them with techniques for elastic execution to dynamically provision both CPS under test and testing utilities to perform tests in various IoT Cloud infrastructures.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Testing Embedded and Cyber-Physical Systems},
pages = {5–8},
numpages = {4},
keywords = {Cloud, uncertainty, IoT, MDE, MBT, elasticity, testing},
location = {Santa Barbara, CA, USA},
series = {TECPS 2017}
}

@article{10.1145/3522592,
author = {Cai, Jianghui and Yang, Yuqing and Yang, Haifeng and Zhao, Xujun and Hao, Jing},
title = {ARIS: A Noise Insensitive Data Pre-Processing Scheme for Data Reduction Using Influence Space},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3522592},
doi = {10.1145/3522592},
abstract = {The extensive growth of data quantity has posed many challenges to data analysis and retrieval. Noise and redundancy are typical representatives of the above-mentioned challenges, which may reduce the reliability of analysis and retrieval results and increase storage and computing overhead. To solve the above problems, a two-stage data pre-processing framework for noise identification and data reduction, called ARIS, is proposed in this article. The first stage identifies and removes noises by the following steps: First, the influence space (IS) is introduced to elaborate data distribution. Second, a ranking factor (RF) is defined to describe the possibility that the points are regarded as noises, then, the definition of noise is given based on RF. Third, a clean dataset (CD) is obtained by removing noise from the original dataset. The second stage learns representative data and realizes data reduction. In this process, CD is divided into multiple small regions by IS. Then the reduced dataset is formed by collecting the representations of each region. The performance of ARIS is verified by experiments on artificial and real datasets. Experimental results show that ARIS effectively weakens the impact of noise and reduces the amount of data and significantly improves the accuracy of data analysis within a reasonable time cost range.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {110},
numpages = {39},
keywords = {Data pre-processing scheme, noise identification, ranking factor, influence space, data representation}
}

@article{10.1145/3533381,
author = {Adhikari, Deepak and Jiang, Wei and Zhan, Jinyu and Zhiyuan He and Rawat, Danda B. and Aickelin, Uwe and Khorshidi, Hadi A.},
title = {A Comprehensive Survey on Imputation of Missing Data in Internet of Things},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533381},
doi = {10.1145/3533381},
abstract = {Internet of Things (IoT) is enabled by the latest developments in smart sensors, communication technologies, and Internet protocols with broad applications. Collecting data from IoT and generating information from these data become tedious tasks in real-life applications when missing data is encountered in datasets. It is of critical importance to deal with the missing data timely for intelligent decision making. Hence, this survey attempts to provide a structured and comprehensive overview of the research on the imputation of incomplete data in IoT. The paper starts by providing an overview of incomplete data based on the architecture of IoT. Then, it discusses the various strategies to handle the missing data, the assumptions used, the computing platform, and the issues related to them. The paper also explores the application of imputation in the area of IoT. We encourage researchers and data analysts to use known imputation techniques and discuss various issues and challenges. Finally, potential future directions regarding the method are suggested. We believe this survey will provide a better understanding of the research of incomplete data and serve as a guide for future research.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {apr},
keywords = {Internet of Things, Machine Learning, Computing Platform for Incomplete data, Multiple Imputations, Deep Learning, Imputation of Missing Data}
}

@inproceedings{10.1145/3102254.3102268,
author = {Lee, Rich C. and Cuzzocrea, Alfredo and Lee, Wookey and Leung, Carson K.},
title = {An Innovative Majority Voting Mechanism in Interactive Social Network Clustering},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102268},
doi = {10.1145/3102254.3102268},
abstract = {We describe a new method of voting system in social networks environment1. We suggest a sequence of continuous support via a social network after electing representatives or exemplars in the network that is different from the typical majority voting. In other words, this paper suggests the method of elected representatives using network clustering approach to counts voting. On the network structure, sending messages from each node reflects the influence or importance to the representative and that can be readjusted and send back to each node. Where the representatives can be clustered within which the selectivity can be decided through the graph edges. In the experiment our algorithm outperformed conventional approaches in social network synthetic dataset as well as real dataset.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {10},
keywords = {vote, majority, social networks, graph clustering},
location = {Amantea, Italy},
series = {WIMS '17}
}

@article{10.1145/3310230,
author = {Fan, Wenfei},
title = {Dependencies for Graphs: Challenges and Opportunities},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3310230},
doi = {10.1145/3310230},
abstract = {What are graph dependencies? What do we need them for? What new challenges do they introduce? This article tackles these questions. It aims to incite curiosity and interest in this emerging area of research.},
journal = {J. Data and Information Quality},
month = {feb},
articleno = {5},
numpages = {12},
keywords = {validation, Dependencies, dependency discovery, implication, graphs, satisfiability, certain fixes, error detection}
}

@article{10.1145/3436239,
author = {Liu, Zhicheng and Zhang, Yang and Huang, Ruihong and Chen, Zhiwei and Song, Shaoxu and Wang, Jianmin},
title = {EXPERIENCE: Algorithms and Case Study for Explaining Repairs with Uniform Profiles over IoT Data},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3436239},
doi = {10.1145/3436239},
abstract = {IoT data with timestamps are often found with outliers, such as GPS trajectories or sensor readings. While existing systems mostly focus on detecting temporal outliers without explanations and repairs, a decision maker may be more interested in the cause of the outlier appearance such that subsequent actions would be taken, e.g., cleaning unreliable readings or repairing broken devices or adopting a strategy for data repairs. Such outlier detection, explanation, and repairs are expected to be performed in either offline (batch) or online modes (over streaming IoT data with timestamps). In this work, we present TsClean, a new prototype system for detecting and repairing outliers with explanations over IoT data. The framework defines uniform profiles to explain the outliers detected by various algorithms, including the outliers with variant time intervals, and take approaches to repair outliers. Both batch and streaming processing are supported in a uniform framework. In particular, by varying the block size, it provides a tradeoff between computing the accurate results and approximating with efficient incremental computation. In this article, we present several case studies of applying TsClean in industry, e.g., how this framework works in detecting and repairing outliers over excavator water temperature data, and how to get reasonable explanations and repairs for the detected outliers in tracking excavators.},
journal = {J. Data and Information Quality},
month = {apr},
articleno = {18},
numpages = {17},
keywords = {outlier repairs, Outlier explanation, time series, data profiling}
}

@inproceedings{10.1145/2663713.2664430,
author = {Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang},
title = {HMM-Based Address Parsing with Massive Synthetic Training Data Generation},
year = {2014},
isbn = {9781450314596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663713.2664430},
doi = {10.1145/2663713.2664430},
abstract = {Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches of synthetic training data generation to build robust models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses.},
booktitle = {Proceedings of the 4th International Workshop on Location and the Web},
pages = {33–36},
numpages = {4},
keywords = {large-scale data, record linkage, address parsing},
location = {Shanghai, China},
series = {LocWeb '14}
}

@article{10.1145/3480968,
author = {Tahir, Madiha and Halim, Zahid and Rahman, Atta Ur and Waqas, Muhammad and Tu, Shanshan and Chen, Sheng and Han, Zhu},
title = {Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3480968},
doi = {10.1145/3480968},
abstract = {The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human–computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {61},
numpages = {24},
keywords = {data-driven decision-making, affective states, Affective computing, pattern recognition, machine learning}
}

@inproceedings{10.1145/3132847.3132894,
author = {Zhao, Yan and Li, Yang and Wang, Yu and Su, Han and Zheng, Kai},
title = {Destination-Aware Task Assignment in Spatial Crowdsourcing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132894},
doi = {10.1145/3132847.3132894},
abstract = {With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. Our empirical studies demonstrate that our proposed technique is quite effective and settle the problem nicely.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {297–306},
numpages = {10},
keywords = {spatial crowdsourcing, user mobility, spatial task assignment},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3543434.3543438,
author = {Marmier, Auriane},
title = {The Impact of Data Governance on OGD Publication – An Ethnographic Odyssey},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543438},
doi = {10.1145/3543434.3543438},
abstract = {Over the past decade, Open Government Data (OGD) strategies have become a continuing concern in administrative services. This is even truer than at any time. Given the current situation, data management, specifically consistent data publication, has been central to public institutions. The Covid-19 pandemic has shown that data collected by public administrations could make valuable contributions. However, in Switzerland, the pandemic has highlighted the limitations of public organizations' capability to lead the publication of their data. Based on an ethnography and a literature review, this paper explores how data governance components impact OGD publication process and presents a model of OGD governance. For this purpose, we identify key data governance components necessary to OGD publication - structural, procedural, and relational - and illustrate how OGD challenges rarely arise from the publication of OGD or the open nature of data itself, but a lack of data governance.},
booktitle = {DG.O 2022: The 23rd Annual International Conference on Digital Government Research},
pages = {235–243},
numpages = {9},
location = {Virtual Event, Republic of Korea},
series = {dg.o 2022}
}

@article{10.1109/TASLP.2015.2512041,
author = {Lin, Zheng and Jin, Xiaolong and Xu, Xueke and Wang, Yuanzhuo and Cheng, Xueqi and Wang, Weiping and Meng, Dan},
title = {An Unsupervised Cross-Lingual Topic Model Framework for Sentiment Classification},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2512041},
doi = {10.1109/TASLP.2015.2512041},
abstract = {Sentiment classification aims to determine the sentiment polarity expressed in a text. In online customer reviews, the sentiment polarities of words are usually dependent on the corresponding aspects. For instance, in mobile phone reviews, we may expect the long battery time but not enjoy the long response time of the operating system. Therefore, it is necessary and appealing to consider aspects when conducting sentiment classification. Probabilistic topic models that jointly detect aspects and sentiments have gained much success recently. However, most of the existing models are designed to work well in a language with rich resources. Directly applying those models on poor-quality corpora often leads to poor results. Consequently, a potential solution is to use the cross-lingual topic model to improve the sentiment classification for a target language by leveraging data and knowledge from a source language. However, the existing cross-lingual topic models are not suitable for sentiment classification because sentiment factors are not considered therein. To solve these problems, we propose for the first time a novel cross-lingual topic model framework which can be easily combined with the state-of-the-art aspect/sentiment models. Extensive experiments in different domains and multiple languages demonstrate that our model can significantly improve the accuracy of sentiment classification in the target language.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {432–444},
numpages = {13},
keywords = {topic model, sentiment classification, cross-language}
}

@inproceedings{10.1145/3014087.3014112,
author = {Nikiforov, Alexander and Singireja, Anastasija},
title = {Open Data and Crowdsourcing Perspectives for Smart City in the United States and Russia},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014112},
doi = {10.1145/3014087.3014112},
abstract = {In this research paper we describe the transformation of open data strategy and implementation of crowdsourcing technologies for the city E-government services. Analysis of smart city projects provides the role of open data and crowdsourcing for smart city vision in United States and Russia. We define challenges and perspectives for collaboration of open data and crowdsourcing in smart city projects.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {171–177},
numpages = {7},
keywords = {crowdsourcing, civic issue tracker, smart city, government 2.0, open innovations, e-government, open data},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inproceedings{10.1145/2951894.2951901,
author = {Fan, Liju and Flood, Mark D.},
title = {An Ontology of Form PF},
year = {2016},
isbn = {9781450344074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2951894.2951901},
doi = {10.1145/2951894.2951901},
abstract = {Form PF, mandated by the 2010 Dodd-Frank Act, is financial regulators' primary source for supervisory data on the risk exposures of hedge funds. Investment advisers use the Securities and Exchange Commission's (SEC) Form ADV to register, and then submit Form PF to report on each of the funds that they advise. These forms embed significant internal structure that is amenable to knowledge representation via formal ontologies, which would facilitate key tasks, such as data integration and the assurance of data integrity. We argue that ontologies should be a core and integral component of information management for financial regulatory reporting. We have tested the approach by designing, developing, and integrating ontologies in OWL/RDF in prototype to consistently describe Form ADV and Form PF with precise semantics. Preliminary results indicate that this technique is feasible in practice for data search and analysis, and will yield useful functionality. We also outline directions for future research.},
booktitle = {Proceedings of the Second International Workshop on Data Science for Macro-Modeling},
articleno = {9},
numpages = {6},
keywords = {data integration, hedge funds, supervisory data, Financial regulation, data integrity, knowledge representation, investment advisers, ontologies},
location = {San Francisco, CA, USA},
series = {DSMM'16}
}

@inproceedings{10.1145/3209415.3209459,
author = {Millard, Jeremy and Thomasen, Louise and Pastrovic, Goran and Cvetkovic, Bojan},
title = {A Roadmap for E-Participation and Open Government: Empirical Evidence from the Western Balkans},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209459},
doi = {10.1145/3209415.3209459},
abstract = {This paper describes why and how a conceptual framework for e-participation and open government has been developed and applied to six aspirant EU countries in the Western Balkans. It provides a rationale and background, and then examines the main academic and other relevant sources used. This is followed by an overview of the conceptual framework and a description of its main elements. Finally, the paper examines international data on e-participation covering the Western Balkan countries, uses this to examine the results of applying the conceptual framework in each country, and then provides conclusions and recommendations.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {191–198},
numpages = {8},
keywords = {Participation, E-government, Policy, Open government, Collaboration, Transparency, E-Participation},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inproceedings{10.1145/1710035.1710077,
author = {Liu, Jing and Cho, Sungchol and Han, Sunyoung and Kim, Keecheon and Ha, YoungGuk and Choe, Jongwon and Kamolphiwong, Sinchai and Choo, Hyunseung and Shin, Yongtae and Kim, Chinchol},
title = {Establishment and Traffic Measurement of Overlay Multicast Testbed in KOREN, THaiREN and TEIN2},
year = {2009},
isbn = {9781605585369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1710035.1710077},
doi = {10.1145/1710035.1710077},
abstract = {Nowadays not only many of research works with various international networks are increasing more and more but also commercial works are increasing with different international networks. In this paper, we have constructed the overlay multicast testbed with KOREN and TEIN2 network, and then we also analyze and research many works with the data got from the testbed experiments, and research works for speed of transmission and transmission security when the data is forwarded to several various international network. We work out the process of problem based on several data of experiments. We analyze these problems and propose the research way to other researchers in overlay multicast area, and we also provide these useful results to other researchers in this area.},
booktitle = {Proceedings of the 6th International Conference on Mobile Technology, Application &amp; Systems},
articleno = {42},
numpages = {7},
keywords = {overlay multicast, measurement, overlay, KOREN, UniNet, TEIN2, multicast},
location = {Nice, France},
series = {Mobility '09}
}

@inproceedings{10.1145/3243082.3264607,
author = {Costa, Daniel G.},
title = {On the Development of Visual Sensors with Raspberry Pi},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3264607},
doi = {10.1145/3243082.3264607},
abstract = {The increasing interest for Internet of Things (IoT) technologies has brought a lot of attention to microelectronics and sensors development. With the availability of affordable embedded platforms for countless applications, it is possible to develop low-cost programmable sensors to provide different types of data, benefiting applications in the IoT world. When cameras can be integrated to such development platforms, visual sensors can be easily created, supporting monitoring and controls functions based on the processing of images and videos. In this context, some of the most relevant details concerning the development of visual sensors with the Raspberry Pi platform are described herein, bringing fundamentals for the creation of highly programmable visual sensors.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {19–22},
numpages = {4},
keywords = {Visual sensors, Raspberry Pi, Internet of things, Camera, Wireless sensor networks},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inproceedings{10.1145/3524458.3547121,
author = {Casini, Luca and Orr\`{u}, Valentina and Roccetti, Marco and Marchetti, Nicol\`{o}},
title = {When Machines Find Sites for the Archaeologists: A Preliminary Study with Semantic Segmentation Applied on Satellite Imagery of the Mesopotamian Floodplain},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524458.3547121},
doi = {10.1145/3524458.3547121},
abstract = {In the perspective of landscape archaeology, remote sensing is a very important tool that allows to recognize and locate potential sites, which will then be “groundtruthed” through a surface survey. Remote sensing is, unfortunately, a very time-consuming process that scales terribly with the size of the area under investigation. In this paper we explore the possibility of using semantic segmentation models to detect and highlight the presence of archaeological sites present in the Mesopotamian floodplain. Whereas archaeologists usually combine information from a variety of basemaps, including aerial and satellite photos taken from the 1950s onwards, we investigated the possibility of using an easily accessible online maps (in our case, Bing Maps). Trying to build an accessible and lightweight system also dictated the choice of trying pretrained segmentation models and use transfer learning. The preliminary results obtained (from different models and parameters choices), as well as the dataset, its idiosyncrasies and how we can deal with them are discussed in this paper.},
booktitle = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
pages = {378–383},
numpages = {6},
keywords = {human-in-the-loop, archaeology, semantic segmentation, mesopotamian floodplain},
location = {Limassol, Cyprus},
series = {GoodIT '22}
}

@inproceedings{10.1145/3208159.3208187,
author = {Gavrilova, Marina L.},
title = {Machine Learning for Social Behavior Understanding},
year = {2018},
isbn = {9781450364010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208159.3208187},
doi = {10.1145/3208159.3208187},
abstract = {Human brain has an ability to perform a massive processing of auxiliary information such as visual cues, cognitive and social interactions, contextual and spatio-temporal data. Similarly to a human brain, social behavioral cues can aid the reliable decision-making of a biometric security system. Being an integral part of human behavior, social interactions are likely to possess unique behavioral patterns. This state-of-the-art review paper discusses an emerging person recognition approach based on the in-depth analysis of individuals' social behavior in order to enhance the performance of a traditional biometric system. The social behavioral information can be mined from their offline or online interactions, and can be identified as a set of Social Behavioral Biometric (SBB) features. These features could be used on their own or further combined with other behavioral and physiological patters, and classification can be enhanced by the use of machine learning approaches. An overview of open problems and challenges as well as applications of studying social behavior in various domains concludes this paper.},
booktitle = {Proceedings of Computer Graphics International 2018},
pages = {247–252},
numpages = {6},
keywords = {machine learning, decision-making, Human behavior recognition, virtual worlds, social behavioral biometrics, online networks},
location = {Bintan, Island, Indonesia},
series = {CGI 2018}
}

@inbook{10.1145/3310205.3310208,
title = {Outlier Detection},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310208},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3480001.3480017,
author = {Jin, Ying and Gao, Ming and Yu, Jixiang},
title = {A Transformer Based Sales Prediction of Smart Container in New Retail Era},
year = {2021},
isbn = {9781450390163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3480001.3480017},
doi = {10.1145/3480001.3480017},
abstract = {With the advent of the new retail era, the value of unmanned smart container is increasingly prominent. Fast and flexible self-service is favored by consumers. How to use accumulated historical sales data to predict sales in the future is an important part of smart container operation management. Reasonable sales prediction can not only reduce the inventory cost, but also reduce the shortage loss of the container. Based on the smart container sales data of Dalian Xiaode New Retail Co., Ltd., through detailed exploratory analysis in many aspects, this paper carries out the feature selection of sales prediction, and uses random forest, XGBoost, Transformer and other algorithms to predict sales. The experimental results show that the prediction accuracy of Transformer is better than traditional algorithms, whose MAPE is 14.67% lower than that of the worst one. Transformer can be well applied in the field of sales prediction of smart container. And in this experiment, compared with Transformer using sine and cosine functions for positional encoding, Transformer encoded by position index has better prediction performance and stronger stability.},
booktitle = {2021 5th International Conference on Deep Learning Technologies (ICDLT)},
pages = {46–53},
numpages = {8},
keywords = {Smart container, Transformer, Sales prediction},
location = {Qingdao, China},
series = {ICDLT 2021}
}

@inproceedings{10.1145/3535508.3545565,
author = {Hornback, Andrew and Shi, Wenqi and Giuste, Felipe O. and Zhu, Yuanda and Carpenter, Ashley M. and Hilton, Coleman and Bijanki, Vinieth N. and Stahl, Hiram and Gottesman, Gary S. and Purnell, Chad and Iwinski, Henry J. and Wattenbarger, J. Michael and Wang, May D.},
title = {Development of a Generalizable Multi-Site and Multi-Modality Clinical Data Cloud Infrastructure for Pediatric Patient Care},
year = {2022},
isbn = {9781450393867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535508.3545565},
doi = {10.1145/3535508.3545565},
abstract = {World-renowned pediatric patient care in scoliosis, craniofacial, orthopedic, and other life-altering conditions is provided at the international Shriners Children's hospital system. The impact of scoliosis can be extreme with significant curvature of the spine that often progresses during childhood periods of growth and development. Gauging the impact of treatment is vital throughout the diagnostic and treatment process and is achieved using radiographic imaging and patient reported feedback surveys. Surgeons from multiple clinical centers have amassed a wealth of patient data from more than 1,000 scoliosis patients. However, these data are difficult to access due to data heterogeneity and poor interoperability between complex hospital systems. These barriers significantly decrease the value of these data to improve patient care. To solve these challenges, we create a generalizable multi-site and multi-modality cloud infrastructure for managing the clinical data of multiple diseases. First, we establish a standardized and secure research data repository using the Fast Health Interoperability Resources (FHIR) standard to harmonize multi-modal clinical data from different hospital sites. Additionally, we develop a SMART-on-FHIR application with a user-friendly graphical user interface (GUI) to enable non-technical users to access the harmonized clinical data. We demonstrate the generalizability of our solution by expanding it to also facilitate craniofacial microsomia and pediatric bone disease imaging research. Ultimately, we present a generalized framework for multi-site, multimodal data harmonization, which can efficiently organize and store data for clinical research to improve pediatric patient care.},
booktitle = {Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {23},
numpages = {10},
keywords = {SMART-on-FHIR, FHIR, data interoperability, data repository, pediatric patient care, user interface, ETL},
location = {Northbrook, Illinois},
series = {BCB '22}
}

@article{10.1109/TCBB.2019.2903804,
author = {Chen, Yiyuan and Wang, Yufeng and Cao, Liang and Jin, Qun},
title = {CCFS: A Confidence-Based Cost-Effective Feature Selection Scheme for Healthcare Data Classification},
year = {2021},
issue_date = {May-June 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2903804},
doi = {10.1109/TCBB.2019.2903804},
abstract = {Feature selection (FS) is one of the fundamental data processing techniques in various machine learning algorithms, especially for classification of healthcare data. However, it is a challenging issue due to the large search space. Binary Particle Swarm Optimization (BPSO) is an efficient evolutionary computation technique, and has been widely used in FS. In this paper, we proposed a Confidence-based and Cost-effective feature selection (CCFS) method using BPSO to improve the performance of healthcare data classification. Specifically, first, CCFS improves search effectiveness by developing a new updating mechanism that designs the feature confidence to explicitly take into account the fine-grained impact of each dimension in the particle on the classification performance. The feature confidence is composed of two measurements: the correlation between feature and categories, and historically selected frequency of each feature. Second, considering the fact that the acquisition costs of different features are naturally different, especially for medical data, and should be fully taken into account in practical applications, besides the classification performance, the feature cost and the feature reduction ratio are comprehensively incorporated into the design of fitness function. The proposed method has been verified in various UCI public datasets and compared with various benchmark schemes. The thoroughly experimental results show the effectiveness of the proposed method, in terms of accuracy and feature selection cost.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {may},
pages = {902–911},
numpages = {10}
}

@article{10.1145/3484945,
author = {Bazai, Sibghat Ullah and Jang-Jaccard, Julian and Alavizadeh, Hooman},
title = {A Novel Hybrid Approach for Multi-Dimensional Data Anonymization for Apache Spark},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {2471-2566},
url = {https://doi.org/10.1145/3484945},
doi = {10.1145/3484945},
abstract = {Multi-dimensional data anonymization approaches (e.g., Mondrian) ensure more fine-grained data privacy by providing a different anonymization strategy applied for each attribute. Many variations of multi-dimensional anonymization have been implemented on different distributed processing platforms (e.g., MapReduce, Spark) to take advantage of their scalability and parallelism supports. According to our critical analysis on overheads, either existing iteration-based or recursion-based approaches do not provide effective mechanisms for creating the optimal number of and relative size of resilient distributed datasets (RDDs), thus heavily suffer from performance overheads. To solve this issue, we propose a novel hybrid approach for effectively implementing a multi-dimensional data anonymization strategy (e.g., Mondrian) that is scalable and provides high-performance. Our hybrid approach provides a mechanism to create far fewer RDDs and smaller size partitions attached to each RDD than existing approaches. This optimal RDD creation and operations approach is critical for many multi-dimensional data anonymization applications that create tremendous execution complexity. The new mechanism in our proposed hybrid approach can dramatically reduce the critical overheads involved in re-computation cost, shuffle operations, message exchange, and cache management.},
journal = {ACM Trans. Priv. Secur.},
month = {nov},
articleno = {5},
numpages = {25},
keywords = {data anonymization, resilient distributed dataset (RDD), multi-dimensional data, Mondrian, Spark}
}

@inproceedings{10.1145/3340531.3412105,
author = {Ranbaduge, Thilina and Schnell, Rainer},
title = {Securing Bloom Filters for Privacy-Preserving Record Linkage},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412105},
doi = {10.1145/3340531.3412105},
abstract = {Privacy-preserving record linkage (PPRL) facilitates the matching of records that correspond to the same real-world entities across different databases while preserving the privacy of the individuals in these databases. A Bloom filter (BF) is a space efficient probabilistic data structure that is becoming popular in PPRL as an efficient privacy technique to encode sensitive information in records while still enabling approximate similarity computations between attribute values. However, BF encoding is susceptible to privacy attacks which can re-identify the values that are being encoded. In this paper we propose two novel techniques that can be applied on BF encoding to improve privacy against attacks. Our techniques use neighbouring bits in a BF to generate new bit values. An empirical study on large real databases shows that our techniques provide high security against privacy attacks, and achieve better similarity computation accuracy and linkage quality compared to other privacy improvements that can be applied on BF encoding.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2185–2188},
numpages = {4},
keywords = {random sampling, perturbation, xor, sliding window, hardening},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3233347.3233373,
author = {Hagemann, Simon and Stark, Rainer},
title = {Automated Body-in-White Production System Design: Data-Based Generation of Production System Configurations},
year = {2018},
isbn = {9781450364720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233347.3233373},
doi = {10.1145/3233347.3233373},
abstract = {Design processes for production systems (PS) in the automotive body-in-white (BIW) sector tie up tremendous resources. Current challenges like the continuous increase of product variants and product complexity have direct impact on the required planning effort in production system design (PSD), which is currently increasing significantly. Analysis of these design processes have revealed a high potential for process automatization. In order to achieve this, suitable methods are required as well as a data basis of reasonable quality. Both methods and data basis are deeply investigated in this paper. The investigations' results create a solid basis for further research in the young field of automated BIW PSD.},
booktitle = {Proceedings of the 4th International Conference on Frontiers of Educational Technologies},
pages = {192–196},
numpages = {5},
keywords = {production system design, process automatization, knowledge-engineering, optimization, automotive, ruled-based algorithms, data mining, body-in-white, artificial intelligence},
location = {Moscow, Russian Federation},
series = {ICFET '18}
}

@article{10.1109/TCBB.2016.2535251,
author = {Ma, Tianle and Zhang, Aidong},
title = {Omics Informatics: From Scattered Individual Software Tools to Integrated Workflow Management Systems},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2535251},
doi = {10.1109/TCBB.2016.2535251},
abstract = {Omic data analyses pose great informatics challenges. As an emerging subfield of bioinformatics, omics informatics focuses on analyzing multi-omic data efficiently and effectively, and is gaining momentum. There are two underlying trends in the expansion of omics informatics landscape: the explosion of scattered individual omics informatics tools with each of which focuses on a specific task in both single- and multi- omic settings, and the fast-evolving integrated software platforms such as workflow management systems that can assemble multiple tools into pipelines and streamline integrative analysis for complicated tasks. In this survey, we give a holistic view of omics informatics, from scattered individual informatics tools to integrated workflow management systems. We not only outline the landscape and challenges of omics informatics, but also sample a number of widely used and cutting-edge algorithms in omics data analysis to give readers a fine-grained view. We survey various workflow management systems WMSs, classify them into three levels of WMSs from simple software toolkits to integrated multi-omic analytical platforms, and point out the emerging needs for developing intelligent workflow management systems. We also discuss the challenges, strategies and some existing work in systematic evaluation of omics informatics tools. We conclude by providing future perspectives of emerging fields and new frontiers in omics informatics.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {926–946},
numpages = {21}
}

@article{10.1145/3434173,
author = {Hockenhull, Michael and Cohn, Marisa Leavitt},
title = {Speculative Data Work &amp; Dashboards: Designing Alternative Data Visions},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3434173},
doi = {10.1145/3434173},
abstract = {This paper studies data work in an organizational context, and suggests speculative data work as a useful concept and the speculative dashboard as a design concept, to better understand and support cooperative work. Drawing on fieldwork in a Danish public sector organisation, the paper identifies and conceptualizes the speculative data work performed around processes of digitalization and the push to become data-driven. The speculative dashboard is proposed as a design concept and opportunity for design, using practices from speculative design and research to facilitate speculation about data?its sources, visualizations, practices and infrastructures. It does so by hacking the 'genre' of the business intelligence data dashboard, and using it as a framework for the juxtaposition of different kinds of data, facilitating and encouraging speculation on alternative visions for data types and use. The paper contributes an empirical study of organizational use of and attitudes towards data, informing a novel design method and concept for co-speculating on alternative visions of and for organizational data.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {264},
numpages = {31},
keywords = {business intelligence, ethnography, data visualization, speculative design, data work}
}

@inproceedings{10.1145/3368089.3417055,
author = {Chen, Zhuangbin and Kang, Yu and Li, Liqun and Zhang, Xu and Zhang, Hongyu and Xu, Hui and Zhou, Yangfan and Yang, Li and Sun, Jeffrey and Xu, Zhangwei and Dang, Yingnong and Gao, Feng and Zhao, Pu and Qiao, Bo and Lin, Qingwei and Zhang, Dongmei and Lyu, Michael R.},
title = {Towards Intelligent Incident Management: Why We Need It and How We Make It},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417055},
doi = {10.1145/3368089.3417055},
abstract = {The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two critical challenges (namely, incomplete service/resource dependencies and imprecise resource health assessment) and investigate the underlying reasons from the perspective of cloud system design and operations. We also present IcM BRAIN, our AIOps framework towards intelligent incident management, and show its practical benefits conveyed to the cloud services of Microsoft.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1487–1497},
numpages = {11},
keywords = {Incident Management, Cloud Computing, AIOps},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inbook{10.1145/3447404.3447406,
author = {Eslambolchilar, Parisa and Komninos, Andreas and Dunlop, Mark D.},
title = {Introduction},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447406},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {1–13},
numpages = {13}
}

@inproceedings{10.1145/3384544.3384611,
author = {Hartner, Raphael and Mezhuyev, Vitaliy and Tschandl, Martin and Bischof, Christian},
title = {Digital Shop Floor Management: A Practical Framework For Implementation},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384611},
doi = {10.1145/3384544.3384611},
abstract = {In the context of manufacturing, shop floor management (SFM) is employed to ensure efficient production operations and workflows. Advanced technologies and methods can be used to improve the SFM and achieve close to real-time responsiveness. Even though there is a number of research available for the digitalized SFM (DSFM), a supportive framework for implementation purposes was not considered yet. Consequently, this paper utilizes concepts from related disciplines and research areas to derive an architectural framework for a DSFM. This particular architecture is then implemented to ensure its practicability and foster the understanding of challenges and opportunities. The proposed multi-layer framework and supportive methods can be employed by manufacturing companies to implement a DSFM focused on interoperability, security and low-latency.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {41–45},
numpages = {5},
keywords = {Shop Floor Management, Retrofitting, Industry 4.0, Lean Production, Mist Computing, Fog Computing, Cloud Computing, Smart Production, Middleware},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@article{10.1145/3502736,
author = {Varde, Aparna S.},
title = {Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3502736},
doi = {10.1145/3502736},
abstract = {Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {86},
numpages = {52},
keywords = {graphical data mining, classification, Applied research, domain knowledge, machine learning, clustering, scientific applications, estimation, predictive analytics}
}

@inproceedings{10.1145/2494091.2499576,
author = {Blunck, Henrik and Bouvin, Niels Olof and Franke, Tobias and Gr\o{}nb\ae{}k, Kaj and Kjaergaard, Mikkel B. and Lukowicz, Paul and W\"{u}stenberg, Markus},
title = {On Heterogeneity in Mobile Sensing Applications Aiming at Representative Data Collection},
year = {2013},
isbn = {9781450322157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494091.2499576},
doi = {10.1145/2494091.2499576},
abstract = {Gathering representative data using mobile sensing to answer research questions is becoming increasingly popular, driven by growing ubiquity and sensing capabilities of mobile devices. However, there are pitfalls along this path, which introduce heterogeneity in the gathered data, and which are rooted in the diversity of the involved device platforms, hardware, software versions and participants. Thus, we, as a research community, need to establish good practices and methodologies for addressing this issue in order to help ensure that, e.g., scientific results and policy changes based on collective, mobile sensed data are valid. In this paper, we aim to inform researchers and developers about mobile sensing data heterogeneity and ways to combat it. We do so via distilling a vocabulary of underlying causes, and via describing their effects on mobile sensing---building on experiences from three projects within citizen science, crowd awareness and trajectory tracking.},
booktitle = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
pages = {1087–1098},
numpages = {12},
keywords = {representativeness, data collection, heterogeneity, mobile sensing},
location = {Zurich, Switzerland},
series = {UbiComp '13 Adjunct}
}

@inproceedings{10.1145/3085504.3085505,
author = {Gorenflo, Christian and Golab, Lukasz and Keshav, Srinivasan},
title = {Managing Sensor Data Streams: Lessons Learned from the WeBike Project},
year = {2017},
isbn = {9781450352826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085504.3085505},
doi = {10.1145/3085504.3085505},
abstract = {We present insights on data management resulting from a field deployment of approximately 30 sensor-equipped electric bicycles (e-bikes) at the University of Waterloo. The trial has been in operation for the last two-and-a-half years, and we have collected and analyzed more than 150 gigabytes of data. We discuss best practices for the entire data management process, spanning data collection, extract-transform-load, data cleaning, and choosing a suitable data management ecosystem. We also comment on how our experiences will inform the design of a future large-scale field trial involving several thousand fully-instrumented e-bikes.},
booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
articleno = {1},
numpages = {11},
keywords = {Time series data management, Data management for the Internet of Things (IoT), Data feed management},
location = {Chicago, IL, USA},
series = {SSDBM '17}
}

@inproceedings{10.1145/3357492.3358628,
author = {Dominguez, Hector and Mowry, Judith and Perez, Elisabeth and Kendrick, Christine and Martin, Kevin},
title = {Privacy and Information Protection for a New Generation of City Services},
year = {2019},
isbn = {9781450369787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357492.3358628},
doi = {10.1145/3357492.3358628},
abstract = {This paper will showcase the work that the City of Portland has done around developing Privacy and Information Protection Principles considering the current state of technology, the social digital age, and advance inference algorithms like machine learning or other Artificial Intelligence tools. By creating more responsible data stewardship in the public sector, municipalities are set to build trusted information networks involving communities and complex social issues. Particularly, the promotion of data privacy can lead to the emergence of anti-poverty and economic development strategies.The City of Portland has developed seven Privacy and Information Protection Principles: Transparency and accountability, full lifecycle stewardship, equitable data management, ethical and non-discriminatory use of data, data openness, automated decision systems, and data utility. These principles have implications in social equity and the future of technology management in smart cities projects. Principle implementation involves the collaboration of different agencies, particularly focused on ethics and human rights supporting sustainable development.This work is part of emergent strategies for a new generation of city services based on data and information, which aim to improve civic engagement, social benefits to communities in city neighborhoods and better collaboration with partners and other government agencies.},
booktitle = {Proceedings of the 2nd ACM/EIGSCC Symposium on Smart Cities and Communities},
articleno = {5},
numpages = {6},
keywords = {Digital Inclusion, government services, Privacy, Automatic decision systems, Digital equity},
location = {Portland, OR, USA},
series = {SCC '19}
}

@inproceedings{10.1145/3437800.3439203,
author = {Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena},
title = {High Performance Computing Education: Current Challenges and Future Directions},
year = {2020},
isbn = {9781450382939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437800.3439203},
doi = {10.1145/3437800.3439203},
abstract = {High Performance Computing (HPC) is the ability to process data and perform complex calculations at extremely high speeds. Current HPC platforms can achieve calculations on the order of quadrillions of calculations per second, with quintillions on the horizon. The past three decades witnessed a vast increase in the use of HPC across different scientific, engineering, and business communities on problems such as sequencing the genome, predicting climate changes, designing modern aerodynamics, or establishing customer preferences. Although HPC has been well incorporated into science curricula such as bioinformatics, the same cannot be said for most computing programs. Computing educators are only now beginning to recognize the need for HPC Education (HPCEd). Building on earlier work, this working group explored how HPCEd can make inroads into computing education, focusing on the undergraduate level. This paper presents the background of HPC and HPCEd, identifies several of the needed core HPC competencies for students, identifies the support needed by educators for HPCEd, and explores the symbiosis between HPCEd and computing education in contemporary areas such as artificial intelligence and data science, as well as how HPCEd can be applied to benefit diverse non-computing domains such as atmospheric science, biological sciences and critical infrastructure protection. Finally, the report makes several recommendations to improve and facilitate HPC education in the future.},
booktitle = {Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {51–74},
numpages = {24},
keywords = {iticse working group, high performance computing, computer science education, high-performance computing curricula, contemporary computing education, hpc education},
location = {Trondheim, Norway},
series = {ITiCSE-WGR '20}
}

@inproceedings{10.1145/3469213.3470272,
author = {Hu, Yerong and He, Xiangzhen and Zhang, Yihao and Zeng, Jia and Yang, Huaiyuan and Zhou, Shuaihang},
title = {Research and Application of Digital Collection Method of Human Movement},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470272},
doi = {10.1145/3469213.3470272},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {71},
numpages = {6},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3003733.3003761,
author = {Gatziolis, Kleanthis and Boucouvalas, Anthony C.},
title = {User Profile Extraction Engine},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003761},
doi = {10.1145/3003733.3003761},
abstract = {The Internet is overwhelmed by a huge amount of information every day and every user has different interests from another. It is therefore important that this information is filtered and sorted according to their preferences. Thus, the profiling systems exploit particularities and preferences of each user and finally they can be studied or used by other applications or humans. This paper analyzes the methods of collecting data (data gathering), and the ways in which this information can be used - filtered so as to create knowledge. A user profile extraction engine is presented and analyzed.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {41},
numpages = {6},
keywords = {E-Commerce, Mobile shopping, Retailing, User Profiling, e-Shopping},
location = {Patras, Greece},
series = {PCI '16}
}

@article{10.1145/3485875,
author = {Yang, Qiang},
title = {Toward Responsible AI: An Overview of Federated Learning for User-Centered Privacy-Preserving Computing},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3–4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3485875},
doi = {10.1145/3485875},
abstract = {With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {oct},
articleno = {32},
numpages = {22},
keywords = {privacy-preserving computing, Federated learning, blockchain, user privacy, responsible AI, data security, decentralized AI, machine learning}
}

@inproceedings{10.1145/3313831.3376485,
author = {Gathani, Sneha and Lim, Peter and Battle, Leilani},
title = {Debugging Database Queries: A Survey of Tools, Techniques, and Users},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376485},
doi = {10.1145/3313831.3376485},
abstract = {Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16},
keywords = {debugging databases, survey, visualization, literature review, empirical study},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3400903.3400908,
author = {Schuler, Robert and Czajkowski, Karl and D'Arcy, Mike and Tangmunarunkit, Hongsuda and Kesselman, Carl},
title = {Towards Co-Evolution of Data-Centric Ecosystems},
year = {2020},
isbn = {9781450388146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400903.3400908},
doi = {10.1145/3400903.3400908},
abstract = {Database evolution is a notoriously difficult task, and it is exacerbated by the necessity to evolve database-dependent applications. As science becomes increasingly dependent on sophisticated data management, the need to evolve an array of database-driven systems will only intensify. In this paper, we present an architecture for data-centric ecosystems that allows the components to seamlessly co-evolve by centralizing the models and mappings at the data service and pushing model-adaptive interactions to the database clients. Boundary objects fill the gap where applications are unable to adapt and need a stable interface to interact with the components of the ecosystem. Finally, evolution of the ecosystem is enabled via integrated schema modification and model management operations. We present use cases from actual experiences that demonstrate the utility of our approach.},
booktitle = {32nd International Conference on Scientific and Statistical Database Management},
articleno = {4},
numpages = {12},
keywords = {schema evolution, application-database co-evolution, model management, software ecosystems},
location = {Vienna, Austria},
series = {SSDBM 2020}
}

@inproceedings{10.1145/3209281.3209326,
author = {Haak, Elise and Ubacht, Jolien and Van den Homberg, Marc and Cunningham, Scott and Van den Walle, Bartel},
title = {A Framework for Strengthening Data Ecosystems to Serve Humanitarian Purposes},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209326},
doi = {10.1145/3209281.3209326},
abstract = {The incidence of natural disasters worldwide is increasing. As a result, a growing number of people is in need of humanitarian support, for which limited resources are available. This requires an effective and efficient prioritization of the most vulnerable people in the preparedness phase, and the most affected people in the response phase of humanitarian action. Data-driven models have the potential to support this prioritization process. However, the applications of these models in a country requires a certain level of data preparedness. To achieve this level of data preparedness on a large scale we need to know how to facilitate, stimulate and coordinate data-sharing between humanitarian actors. We use a data ecosystem perspective to develop success criteria for establishing a "humanitarian data ecosystem". We first present the development of a general framework with data ecosystem governance success criteria based on a systematic literature review. Subsequently, the applicability of this framework in the humanitarian sector is assessed through a case study on the "Community Risk Assessment and Prioritization toolbox" developed by the Netherlands Red Cross. The empirical evidence led to the adaption the framework to the specific criteria that need to be addressed when aiming to establish a successful humanitarian data ecosystem.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {85},
numpages = {9},
keywords = {governance, humanitarian sector, data preparedness, data ecosystem, framework},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@article{10.1145/3502288,
author = {Barth, Susanne and Ionita, Dan and Hartel, Pieter},
title = {Understanding Online Privacy—A Systematic Review of Privacy Visualizations and Privacy by Design Guidelines},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3502288},
doi = {10.1145/3502288},
abstract = {Privacy visualizations help users understand the privacy implications of using an online service. Privacy by Design guidelines provide generally accepted privacy standards for developers of online services. To obtain a comprehensive understanding of online privacy, we review established approaches, distill a unified list of 15 privacy attributes and rank them based on perceived importance by users and privacy experts. We then discuss similarities, explain notable differences, and examine trends in terms of the attributes covered. Finally, we show how our results provide a foundation for user-centric privacy visualizations, inspire best practices for developers, and give structure to privacy policies.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {63},
numpages = {37},
keywords = {privacy labels, Privacy attributes, privacy icons, privacy by design, privacy factors}
}

@inproceedings{10.1145/3488560.3498421,
author = {Bhargav, Samarth and Sidiropoulos, Georgios and Kanoulas, Evangelos},
title = { 'It's on the Tip of My Tongue': A New Dataset for Known-Item Retrieval},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498421},
doi = {10.1145/3488560.3498421},
abstract = {The tip of the tongue known-item retrieval (TOT-KIR) task involves the 'one-off' retrieval of an item for which a user cannot recall a precise identifier. The emergence of several online communities where users pose known-item queries to other users indicates the inability of existing search systems to answer such queries. Research in this domain is hampered by the lack of large, open or realistic datasets. Prior datasets relied on either annotation by crowd workers, which can be expensive and time-consuming, or generating synthetic queries, which can be unrealistic. Additionally, small datasets make the application of modern (neural) retrieval methods unviable, since they require a large number of data-points. In this paper, we collect the largest dataset yet with 15K query-item pairs in two domains, namely, Movies and Books, from an online community using heuristics, rendering expensive annotation unnecessary while ensuring that queries are realistic. We show that our data collection method is accurate by conducting a data study. We further demonstrate that methods like BM25 fall short of answering such queries, corroborating prior research. The size of the dataset makes neural methods feasible, which we show outperforms lexical baselines, indicating that neural/dense retrieval is superior for the TOT-KIR task.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {48–56},
numpages = {9},
keywords = {known item retrieval, tip of the tongue known item retrieval},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3325112.3325222,
author = {S. Bargh, Mortaza and Meijer, Ronald and Vink, Marco and van Den Braak, Susan and Schirm, Walter and Choenni, Sunil},
title = {Opening Privacy Sensitive Microdata Sets in Light of GDPR},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325222},
doi = {10.1145/3325112.3325222},
abstract = {To enhance the transparency, accountability and efficiency of the Dutch Ministry of Justice and Security, the ministry has set up an open data program to proactively stimulate sharing its (publicly funded) data sets with the public. Disclosure of personal data is considered as one of the main threats for data opening. In this contribution we argue that, according to Dutch laws, the criminal data within the Dutch justice domain are sensitive data in GDPR terms and that the criminal data can only be opened if these sensitive data are transformed to have no personal information. Subsequently, having no personal information in data sets is related to two GDPR concepts: the data being anonymous in its GDPR sense or the data being pseudonymized in its GDPR sense. These two GDPR concepts, i.e., being anonymous data or pseudonymized data in a GDPR sense, can be distinguished in our setting based on whether the data controller cannot or can revert the data protection process, respectively. (Note that the terms anonymous and pseudonymized are interpreted differently in the technical domain.) We examine realizing these GDPR concepts with the Statistical Disclosure Control (SDC) technology and subsequently argue that pseudonymized data in a GDPR sense delivers a better data utility than the other. At the end, we present a number of the consequences of adopting either of these concepts, which can inform legislators and policymakers to define their strategy for opening privacy sensitive microdata sets, like those pertaining to the Dutch criminal justice domain.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {314–323},
numpages = {10},
keywords = {Justice domain data, GDPR, Criminal justice data, Microdata, Open data, Privacy, Data protection},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1145/3479645.3479661,
author = {Kusnandar, Toni and Surendro, Kridanto},
title = {Camera-Based Vegetation Index from Unmanned Aerial Vehicles},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479661},
doi = {10.1145/3479645.3479661},
abstract = {Agriculture assumes a vital role in human life because it provides food, feed for livestock, and bioenergy. The agricultural sector is expected to meet the needs of secure and nutritious food for the community at all times to boost productivity. Providing nutrition, water and light precisely and measuredly is an important effort in plant cultivation to produce quality. This effort can be materialized by implementing smart farming involving devices and information technology. Vast field surveillance or monitoring is made easy with the advent of unmanned aerial vehicle (UAV). Detection of plant condition can be achieved by obtaining Vegetation Index (VI) through camera imaging in UAVs which are more economic compared to multispectral or hyperspectral cameras. This study aims to obtain VI that is accurate but still economical, so that it can be utilized even by small-scale agriculture. The work that will be done is to conduct repair experiments at several stages of image processing to produce a new, more accurate VI. The research stages started from experiments on previous research, to finding new research opportunities in VI. Furthermore, the experiment was carried out with the addition of white balance value parameters and other UAV sensor parameters at the Pre-Processing stage to improve its quality. The hypothesis of adding white balance parameters should prove to be more accurate in correcting shooting in various light conditions. Next, try to modify the feature extraction algorithm using Color Extraction Edge Detection. Followed by modifying it using Back Propagation Neural Network to increase accuracy at the image processing stage. After synthesizing some of these experiments, a new formula or model VI using the camera on the UAV is expected to be produced. This research will contribute to the modification of methods or algorithms at the image processing stage to produce a corrected image in producing a new VI that is more accurate using a camera on a more economical UAV.},
booktitle = {6th International Conference on Sustainable Information Engineering and Technology 2021},
pages = {173–178},
numpages = {6},
keywords = {Unmanned Aerial Vehicle, Vegetation Index, Image Processing, Precission Agriculture},
location = {Malang, Indonesia},
series = {SIET '21}
}

@inproceedings{10.1145/2905055.2905184,
author = {Jangra, Ajay and Singh, Niharika and Lakhina, Upasana},
title = {VIP: Verification and Identification Protective Data Handling Layer Implementation to Achieve MVCC in Cloud Computing},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905184},
doi = {10.1145/2905055.2905184},
abstract = {Over transactional database systems MultiVersion concurrency control is maintained for secure, fast and efficient access to the shared data file implementation scenario. An effective coordination is supposed to be set up between owners and users also the developers &amp; system operators, to maintain inter-cloud &amp; intra-cloud communication Most of the services &amp; application offered in cloud world are real-time, which entails optimized compatibility service environment between master and slave clusters. In the paper, offered methodology supports replication and triggering methods intended for data consistency and dynamicity. Where intercommunication between different clusters is processed through middleware besides slave intra-communication is handled by verification &amp; identification protection. The proposed approach incorporates resistive flow to handle high impact systems that identifies and verifies multiple processes. Results show that the new scheme reduces the overheads from different master and slave servers as they are co-located in clusters which allow increased horizontal and vertical scalability of resources.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {124},
numpages = {6},
keywords = {Data version validation, Verification &amp; Identification, Serializability, MVCC, Transaction Manager, Cloud computing},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3473465.3473478,
author = {Sun, Xiyan and Xiao, Yu and Ji, Yuanfa and Huang, Jianhua and Bai, Yang},
title = {Multi Scale UNet Encoder-Decoder Network for Building Extraction},
year = {2021},
isbn = {9781450389884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473465.3473478},
doi = {10.1145/3473465.3473478},
abstract = {Buildings in remote sensing images have large scale differences and complex shapes. And there are often distractors with visual features similar to buildings in complex scenes. The traditional methods used to extract buildings are limited by the ability of feature representation, resulting in low accuracy and low universality. The semantic segmentation network based on the Encoder-Decoder structure can automatically learn multi-level building feature representation from the data set, and achieve end-to-end building extraction. UNet is a typical semantic segmentation Encoder-Decoder network, but UNet cannot explore enough building information. Small buildings are easy to be missed, large buildings with complex colors and shapes are incompletely extracted, boundary segmentation is inaccurate. And the network is easily affected by roads, trees, shadows and other distractors. Therefore, this article improves UNet and proposes a multi-scale Encoder-Decoder network to learn multi-scale and distinguishable features to better identify buildings and backgrounds. We experiment with the improved network and the classic U-Net on two data sets, and show that the multi-scale Encoder-Decoder network can effectively improve the accuracy of building extraction.},
booktitle = {2021 3rd International Conference on Information Technology and Computer Communications},
pages = {73–79},
numpages = {7},
location = {Guangzhou, China},
series = {ITCC 2021}
}

@inproceedings{10.1145/3139958.3140013,
author = {Lin, Yijun and Chiang, Yao-Yi and Pan, Fan and Stripelis, Dimitrios and Ambite, Jose Luis and Eckel, Sandrah P. and Habre, Rima},
title = {Mining Public Datasets for Modeling Intra-City PM2.5 Concentrations at a Fine Spatial Resolution},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3140013},
doi = {10.1145/3139958.3140013},
abstract = {Air quality models are important for studying the impact of air pollutant on health conditions at a fine spatiotemporal scale. Existing work typically relies on area-specific, expert-selected attributes of pollution emissions (e,g., transportation) and dispersion (e.g., meteorology) for building the model for each combination of study areas, pollutant types, and spatiotemporal scales. In this paper, we present a data mining approach that utilizes publicly available OpenStreetMap (OSM) data to automatically generate an air quality model for the concentrations of fine particulate matter less than 2.5 μm in aerodynamic diameter at various temporal scales. Our experiment shows that our (domain-) expert-free model could generate accurate PM2.5 concentration predictions, which can be used to improve air quality models that traditionally rely on expert-selected input. Our approach also quantifies the impact on air quality from a variety of geographic features (i.e., how various types of geographic features such as parking lots and commercial buildings affect air quality and from what distance) representing mobile, stationary and area natural and anthropogenic air pollution sources. This approach is particularly important for enabling the construction of context-specific spatiotemporal models of air pollution, allowing investigations of the impact of air pollution exposures on sensitive populations such as children with asthma at scale.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {25},
numpages = {10},
keywords = {Air Quality Modeling, Geospatial Data Mining, PM2.5 Concentration Prediction},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1145/3428502.3428514,
author = {Papadopoulos, Theodoros and Charalabidis, Yannis},
title = {What Do Governments Plan in the Field of Artificial Intelligence? Analysing National AI Strategies Using NLP},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428514},
doi = {10.1145/3428502.3428514},
abstract = {The primary goal of this paper is to explore how Natural Language Processing techniques (NLP) can assist in reviewing, understanding, and drawing conclusions from text datasets. We explore NLP techniques for the analysis and the extraction of useful information from the text of twelve national strategies on artificial intelligence (AI). For this purpose, we are using a set of machine learning algorithms in order to (a) extract the most significant keywords and summarize each strategy document, (b) discover and assign topics to each document, and (c) cluster the strategies based on their pair-wise similarity. Using the results of the analysis, we discuss the findings and highlight critical issues that emerge from the national strategies for artificial intelligence, such as the importance of the data ecosystem for the development of AI, the increasing considerations about ethical and safety issues, as well as the growing ambition of many countries to lead in the AI race. Utilizing the LDA topic model, we were able to reveal the distributions of thematic sub-topics among the strategic documents. The topic modelling distributions were then used along with other document similarity measures as an input for the clustering of the strategic documents into groups. The results revealed three clusters of countries with a visible differentiation between the strategies of China and Japan on the one hand and the Scandinavian strategies (plus the German and the Luxemburgish) one on the other. The former promote technology and innovation-driven development plans in order to integrate AI with the economy, while the latter share a common view regarding the role of the public sector both as a promoter and investor but also as a user and beneficiary of AI, and give a higher priority to the ethical &amp; safety issues that are connected to the development of AI.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {100–111},
numpages = {12},
keywords = {Automated Text Analysis, NLP, machine learning, document similarity, topic modelling, AI strategies},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/2631775.2631806,
author = {Zhang, Kunpeng and Bhattacharyya, Siddhartha and Ram, Sudha},
title = {Empirical Analysis of Implicit Brand Networks on Social Media},
year = {2014},
isbn = {9781450329545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631775.2631806},
doi = {10.1145/2631775.2631806},
abstract = {This paper investigates characteristics of implicit brand networks extracted from a large dataset of user historical activities on a social media platform. To our knowledge, this is one of the first studies to comprehensively examine brands by incorporating user-generated social content and information about user interactions. This paper makes several important contributions. We build and normalize a weighted, undirected network representing interactions among users and brands. We then explore the structure of this network using modified network measures to understand its characteristics and implications. As a part of this exploration, we address three important research questions: (1) What is the structure of a brand-brand network? (2) Does an influential brand have a large number of fans? (3) Does an influential brand receive more positive or more negative comments from social users? Experiments conducted with Facebook data show that the influence of a brand has (a) high positive correlation with the size of a brand, meaning that an influential brand can attract more fans, and, (b) low negative correlation with the sentiment of comments made by users on that brand, which means that negative comments have a more powerful ability to generate awareness of a brand than positive comments. To process the large-scale datasets and networks, we implement MapReduce-based algorithms.},
booktitle = {Proceedings of the 25th ACM Conference on Hypertext and Social Media},
pages = {190–199},
numpages = {10},
keywords = {marketing intelligence, mapreduce, network analysis, sentiment identification, social media},
location = {Santiago, Chile},
series = {HT '14}
}

@inproceedings{10.1145/3477314.3506986,
author = {Stach, Christoph and Gritti, Cl\'{e}mentine and Przytarski, Dennis and Mitschang, Bernhard},
title = {Can Blockchains and Data Privacy Laws Be Reconciled? A Fundamental Study of How Privacy-Aware Blockchains Are Feasible},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3506986},
doi = {10.1145/3477314.3506986},
abstract = {Due to the advancing digitalization, the importance of data is constantly increasing. Application domains such as smart cars, smart cities, or smart healthcare rely on the permanent availability of large amounts of data to all parties involved. As a result, the value of data increases, making it a lucrative target for cyber-attacks. Particularly when human lives depend on the data, additional protection measures are therefore important for data management and provision. Blockchains, i. e., decentralized, immutable, and tamper-proof data stores, are becoming increasingly popular for this purpose. Yet, from a data protection perspective, the immutable and tamper-proof properties of blockchains pose a privacy concern. In this paper, we therefore investigate whether blockchains are in compliance with the General Data Protection Regulation (GDPR) if personal data are involved. To this end, we elaborate which articles of the GDPR are relevant in this regard and present technical solutions for those legal requirements with which blockchains are in conflict. We further identify open research questions that need to be addressed in order to achieve a privacy-by-design blockchain system.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1218–1227},
numpages = {10},
keywords = {tamper-proof, privacy assessment, blockchains, immutable, GDPR},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3313831.3376662,
author = {Shipman, Frank M. and Marshall, Catherine C.},
title = {Ownership, Privacy, and Control in the Wake of Cambridge Analytica: The Relationship between Attitudes and Awareness},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376662},
doi = {10.1145/3313831.3376662},
abstract = {Has widespread news of abuse changed the public's perceptions of how user-contributed content from social networking sites like Facebook and LinkedIn can be used? We collected two datasets that reflect participants' attitudes about content ownership, privacy, and control, one in April 2018, while Cambridge Analytica was still in the news, and another in February 2019, after the event had faded from the headlines, and aggregated the data according to participants' awareness of the story, contrasting the attitudes of those who reported the greatest awareness with those who reported the least. Participants with the greatest awareness of the news story's details have more polarized attitudes about reuse, especially the reuse of content as data. They express a heightened desire for data mobility, greater concern about networked privacy rights, increased skepticism of algorithmically targeted advertising and news, and more willingness for social media platforms to demand corrections of inaccurate or deceptive content.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {privacy, social media attitudes, ownership, cambridge analytica, data monetization, facebook, data use, linkedin},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1145/2602204.2602217,
author = {Kenneally, Erin and Bailey, Michael},
title = {Cyber-Security Research Ethics Dialogue &amp; Strategy Workshop},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2602204.2602217},
doi = {10.1145/2602204.2602217},
abstract = {The inaugural Cyber-security Research Ethics Dialogue &amp; Strategy Workshop was held on May 23, 2013, in conjunction with the IEEE Security Privacy Symposium in San Francisco, California. CREDS embraced the theme of "ethics-by-design" in the context of cyber security research, and aimed to: Educate participants about underlying ethics principles and applications;Discuss ethical frameworks and how they are applied across the various stakeholders and respective communities who are involved;Impart recommendations about how ethical frameworks can be used to inform policymakers in evaluating the ethical underpinning of critical policy decisions;Explore cyber security research ethics techniques,tools,standards and practices so researchers can apply ethical principles within their research methodologies; andDiscuss specific case vignettes and explore the ethical implications of common research acts and omissions.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {apr},
pages = {76–79},
numpages = {4},
keywords = {trust, network measurement, cyber security, law, ethics}
}

@inproceedings{10.1145/2600821.2600847,
author = {Moazeni, Ramin and Link, Daniel and Boehm, Barry},
title = {COCOMO II Parameters and IDPD: Bilateral Relevances},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600847},
doi = {10.1145/2600821.2600847},
abstract = {The phenomenon called Incremental Development Productivity Decline (IDPD) is presumed to be present in all incremental soft-ware projects to some extent. COCOMO II is a popular parametric cost estimation model that has not yet been adapted to account for the challenges that IDPD poses to cost estimation. Instead, its cost driver and scale factors stay constant throughout the increments of a project. While a simple response could be to make these parameters variable per increment, questions are raised as to whether the existing parameters are enough to predict the behavior of an incrementally developed project even in that case. Individual COCOMO II parameters are evaluated with regard to their development over the course of increments and how they influence IDPD. The reverse is also done. In light of data collected in recent experimental projects, additional new variable parameters that either extend COCOMO II or could stand on their own are proposed.},
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {20–24},
numpages = {5},
keywords = {cost drivers, Parametric cost estimation, incremental development, IDPD, scale factors},
location = {Nanjing, China},
series = {ICSSP 2014}
}

@article{10.1145/3523059,
author = {Zhang, Lin and Fan, Lixin and Luo, Yong and Duan, Ling-Yu},
title = {Intrinsic Performance Influence-Based Participant Contribution Estimation for Horizontal Federated Learning},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3523059},
doi = {10.1145/3523059},
abstract = {The rapid development of modern artificial intelligence technique is mainly attributed to sufficient and high-quality data. However, in the data collection, personal privacy is at risk of being leaked. This issue can be addressed by federated learning, which is proposed to achieve efficient model training among multiple data providers without direct data access and aggregation. To encourage more parties owning high-quality data to participate in the federated learning, it is important to evaluate and reward the participant contribution in a reasonable, robust, and efficient manner. To achieve this goal, we propose a novel contribution estimation method: Intrinsic Performance Influence-based Contribution Estimation (IPICE). In particular, the class-level intrinsic performance influence is adopted as the contribution estimation criteria in IPICE, and a neural network is employed to exploit the non-linear relationship between the performance change and estimated contribution. Extensive experiments are conducted on various datasets, and the results demonstrate that IPICE is more accurate and stable than the counterpart in various data distribution settings. The computational complexity is significantly reduced in our IPICE, especially when a new party joins the federation. IPICE assigns small contributions to bad/garbage data and thus prevent them from participating and deteriorating the learning ecosystem.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {sep},
articleno = {88},
numpages = {24},
keywords = {Federated learning, participant contribution estimation, neural network}
}

@inproceedings{10.1145/3307772.3328285,
author = {Kriechbaumer, Thomas and Jorde, Daniel and Jacobsen, Hans-Arno},
title = {Waveform Signal Entropy and Compression Study of Whole-Building Energy Datasets},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3328285},
doi = {10.1145/3307772.3328285},
abstract = {Electrical energy consumption has been an ongoing research area since the coming of smart homes and Internet of Things. Consumption characteristics and usages profiles are directly influenced by building occupants and their interaction with electrical appliances. Data analysis together with machine learning models can be utilized to extract valuable information for the benefit of occupants themselves (conserve energy and increase comfort levels), power plants (maintenance), and grid operators (stability). Public energy datasets provide a scientific foundation to develop and benchmark these algorithms and techniques. With datasets exceeding tens of terabytes, we present a novel study of five whole-building energy datasets with high sampling rates, their signal entropy, and how a well-calibrated measurement can have a significant effect on the overall storage requirements. We show that some datasets do not fully utilize the available measurement precision, therefore leaving potential accuracy and space savings untapped. We benchmark a comprehensive list of 365 file formats, transparent data transformations, and lossless compression algorithms. The primary goal is to reduce the overall dataset size while maintaining an easy-to-use file format and access API. We show that with careful selection of file format and encoding scheme, we can reduce the size of some datasets by up to 73%.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {58–67},
numpages = {10},
keywords = {Energy dataset, file format, waveform compression, electricity aggregate, non-intrusive load monitoring, high sampling rate},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@article{10.1145/3422158,
author = {Kumar, Devender and Jeuris, Steven and Bardram, Jakob E. and Dragoni, Nicola},
title = {Mobile and Wearable Sensing Frameworks for MHealth Studies and Applications: A Systematic Review},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3422158},
doi = {10.1145/3422158},
abstract = {With the widespread use of smartphones and wearable health sensors, a plethora of mobile health (mHealth) applications to track well-being, run human behavioral studies, and clinical trials have emerged in recent years. However, the design, development, and deployment of mHealth applications is challenging in many ways. To address these challenges, several generic mobile sensing frameworks have been researched in the past decade. Such frameworks assist developers and researchers in reducing the complexity, time, and cost required to build and deploy health-sensing applications. The main goal of this article is to provide the reader with an overview of the state-of-the-art of health-focused generic mobile and wearable sensing frameworks. This review gives a detailed analysis of functional and non-functional features of existing frameworks, the health studies they were used in, and the stakeholders they support. Additionally, we also analyze the historical evolution, uptake, and maintenance after the initial release. Based on this analysis, we suggest new features and opportunities for future generic mHealth sensing frameworks.},
journal = {ACM Trans. Comput. Healthcare},
month = {dec},
articleno = {8},
numpages = {28},
keywords = {mobile sensing, mobile sensing frameworks, mHealth frameworks, mHealth sensing, wearable sensing}
}

@article{10.1145/3450518,
author = {Wei, Ziheng and Link, Sebastian},
title = {Embedded Functional Dependencies and Data-Completeness Tailored Database Design},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3450518},
doi = {10.1145/3450518},
abstract = {We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.},
journal = {ACM Trans. Database Syst.},
month = {may},
articleno = {7},
numpages = {46},
keywords = {functional dependency, normal form, third normal form, database design, redundancy, key, updates, decomposition, missing value, synthesis, Boyce-Codd normal form}
}

@inproceedings{10.1145/3428502.3428548,
author = {Osorio-Sanabria, Mariutsi Alexandra and Amaya-Fern\'{a}ndez, Ferney and Gonz\'{a}lez-Zabala, Mayda Patricia},
title = {Developing a Model to Readiness Assessment of Open Government Data in Public Institutions in Colombia},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428548},
doi = {10.1145/3428502.3428548},
abstract = {Open data is a movement that has gained worldwide political relevance as a strategy that supports active transparency, access to public information, and the generation of public, social, and economic value. To know the progress and results of open data initiatives, governments, working groups, international organizations, and researchers have proposed indexes and evaluation models. These measurements focus on the evaluation of aspects of the preparation, implementation, and impact of open data initiatives. In Colombia, the national government within the framework of its digital government policy defined the open data project. The progress in data openings is monitored through international indexes and the open government index, which focuses solely on the publication and use of open government data. This research deals with the evaluation of the preparation for the opening of data, in public entities that have not implemented an open data initiative. The study gives a general description of the evaluation of open data at the international and national level, identifies aspects to be considered to measure the preparation, and proposes a conceptual model of evaluation to measure the preparation in open data of a public sector entity. This proposal can be considered as a tool that generates information that supports the design and implementation of an effective open data initiative.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {334–340},
numpages = {7},
keywords = {e-government, digital government, open government data, readiness assessment, Open data},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/3543434.3543442,
author = {Bartolomucci, Federico and Bresolin, Gianluca},
title = {Fostering Data Collaboratives’ Systematisation through Models’ Definition and Research Priorities Setting},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543442},
doi = {10.1145/3543434.3543442},
abstract = {Data collaboratives (DC) [12, 18] have gained increasing attention in recent years benefitting and nurturing the momentum around the use of Data for Good [11]. However, research on the topic, derived and built upon the fields of collaborative governance, information sharing, and open data [9, 17] is still unmature, lacking a systematic body of knowledge, grounded in empirical evidence [11]. Except few studies, specifically referred to DC [31, 36, 37, 40, 42] [15, 18, 19, 24], most of the literature used in the field is encompassing broader concepts as such DataSharing, DataforGood, or Cross Sectoral Partnership.Given that the empirical field has matured sufficiently to permit more quantitative analysis, the research seeks to go beyond existing qualitative classifications and inductively define data collaborative archetypes, emphasizing their distinctions and peculiarities as a foundation for future research on the topic.The research started from a literature review on DCs, their definition and the dimensions identifying different DC's models. The dataset provided on datacollaboratives.org has been filtered based on the literature review, excluding those instances that do not meet the DCs criteria or for whom online data collection is not feasible. Once the empirical setting was defined, a phase of variables selection and population has been conducted according to different variables. The evaluation of different clustering solutions, using both qualitative and quantitative methodologies, brought to identify five mutually exclusive clusters.Each cluster is described according to 18 variables, allowing the emergence of cluster's specific peculiarities and challenges. Findings are consistent with prior classifications and taxonomies [18, 29] with additional views afforded by a larger number of instances, the use of quantitative methodologies and the analysis of additional variables. Findings demonstrate the coexistence of quite different entities under the concept of DC, each of whose challenges and progress should be examined independently by researchers.Responding to the objective to foster DCs long term sustainability, different research priorities are specified according to identified clusters and an empirical setting for conducting this research is made available. From a practitioner perspective, research's findings may enable those interested in the topic to obtain more comprehensive information about benchmark examples, which is a valuable resource for industry growth. Additionally, the research illustrates&nbsp;the efficacy of categorical variable clustering analysis for inductive exploratory studies in a novel field of research.},
booktitle = {DG.O 2022: The 23rd Annual International Conference on Digital Government Research},
pages = {35–40},
numpages = {6},
keywords = {Data for Good, Cluster Analysis, Data Collaboratives},
location = {Virtual Event, Republic of Korea},
series = {dg.o 2022}
}

@article{10.1145/3093895,
author = {Longo, Antonella and Zappatore, Marco and Bochicchio, Mario and Navathe, Shamkant B.},
title = {Crowd-Sourced Data Collection for Urban Monitoring via Mobile Sensors},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3093895},
doi = {10.1145/3093895},
abstract = {A considerable amount of research has addressed Internet of Things and connected communities. It is possible to exploit the sensing capabilities of connected communities, by leveraging the continuously growing use of cloud computing solutions and mobile devices. The pervasiveness of mobile sensors also enables the Mobile Crowd Sensing (MCS) paradigm, which aims at using mobile-embedded sensors to extend monitoring of multiple (environmental) phenomena in expansive urban areas. In this article, we discuss our approach with a cloud-based platform to pave the way for applying crowd sensing in urban scenarios. We have implemented a complete solution for environmental monitoring of several pollutants, like noise, air, electromagnetic fields, and so on in an urban area based on this paradigm. Through extensive experimentation, specifically on noise pollution, we show how the proposed infrastructure exhibits the ability to collect data from connected communities, and enables a seamless support of services needed for improving citizens’ quality of life and eventually helps city decision makers in urban planning.},
journal = {ACM Trans. Internet Technol.},
month = {oct},
articleno = {5},
numpages = {21},
keywords = {social sensing, Mobile crowed sensing, smart cities}
}

@inproceedings{10.1145/3374587.3374631,
author = {Luo, Jingtang and Yao, Shiying and Gou, Jijun and Shuai, Lisha and Cao, Yu},
title = {A Secure Transmission Scheme of Sensitive Power Information in Ubiquitous Power IoT},
year = {2019},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374631},
doi = {10.1145/3374587.3374631},
abstract = {As one of the national key infrastructures, the ubiquitous power Internet of Things (IoT) provides a convenient method for large-scale power information collection. The widespread transmission of massive power information using data mining techniques for large amounts of data can yield valuable information. Therefore, hacker attacks are endless, posing a threat to the security of the state, society, collectives and individuals. In this paper, we propose a secure transmission scheme of power information, named "SSD" (Split &amp; Signature &amp; Disturbing). In the scheme, after the data is split, it will be anonymized and selected for different paths to be transferred to the destination node. After recombination, the data will be restored. The SSD ensures the indistinguishability and security of the sensitive data by data splitting and disturbing method, and protects the anonymity of individual identities by group signature. The experimental results show that the individual prediction/actual data similarity approaches 0%, and the similarity ratio of the category data (three types in the experiment) is 37.32%, which can be judged to be basically non-correlated.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {252–257},
numpages = {6},
keywords = {Data Splitting, Information Security, Ubiquitous Power IoT},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.1145/3183713.3197387,
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
title = {Data Integration and Machine Learning: A Natural Synergy},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3197387},
doi = {10.1145/3183713.3197387},
abstract = {There is now more data to analyze than ever before. As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1645–1650},
numpages = {6},
keywords = {data enrichment, machine learning, data integration},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3397166.3413465,
author = {Bazzi, Alessandro and Campolo, Claudia and Masini, Barbara M. and Molinaro, Antonella},
title = {How to Deal with Data Hungry V2X Applications?},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3413465},
doi = {10.1145/3397166.3413465},
abstract = {Current vehicular communication technologies were designed for a so-called phase 1, where cars needed to advise of their presence. Several projects, research activities and field tests have proved their effectiveness to this scope. But entering the phase 2, where awareness needs to be improved with non-connected objects and vulnerable road users, and even more with phases 3 and 4, where also coordination is foreseen, the spectrum scarcity becomes a critical issue. In this work, we provide an overview of various 5G and beyond solutions currently under investigation that will be needed to tackle the challenge. We first recall the undergoing activities at the access layer aimed to satisfy capacity and bandwidth demands. We then discuss the role that emerging networking paradigms can play to improve vehicular data dissemination, while preventing congestion and better exploiting resources. Finally, we give a look into edge computing and machine learning techniques that will be determinant to efficiently process and mine the massive amounts of sensor data.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {333–338},
numpages = {6},
keywords = {connected and automated vehicles, cooperative sensing, 5G, vehicle-to-everything},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@inproceedings{10.1145/3083187.3083189,
author = {Pogorelov, Konstantin and Eskeland, Sigrun Losada and de Lange, Thomas and Griwodz, Carsten and Randel, Kristin Ranheim and Stensland, H\r{a}kon Kvale and Dang-Nguyen, Duc-Tien and Spampinato, Concetto and Johansen, Dag and Riegler, Michael and Halvorsen, P\r{a}l},
title = {A Holistic Multimedia System for Gastrointestinal Tract Disease Detection},
year = {2017},
isbn = {9781450350020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3083187.3083189},
doi = {10.1145/3083187.3083189},
abstract = {Analysis of medical videos for detection of abnormalities and diseases requires both high precision and recall, but also real-time processing for live feedback and scalability for massive screening of entire populations. Existing work on this field does not provide the necessary combination of retrieval accuracy and performance.; AB@In this paper, a multimedia system is presented where the aim is to tackle automatic analysis of videos from the human gastrointestinal (GI) tract. The system includes the whole pipeline from data collection, processing and analysis, to visualization. The system combines filters using machine learning, image recognition and extraction of global and local image features. Furthermore, it is built in a modular way so that it can easily be extended. At the same time, it is developed for efficient processing in order to provide real-time feedback to the doctors. Our experimental evaluation proves that our system has detection and localisation accuracy at least as good as existing systems for polyp detection, it is capable of detecting a wider range of diseases, it can analyze video in real-time, and it has a low resource consumption for scalability.},
booktitle = {Proceedings of the 8th ACM on Multimedia Systems Conference},
pages = {112–123},
numpages = {12},
keywords = {Evaluation, Performance, Medicine, Medical Multimedia System, Interactive, Gastrointestinal Tract},
location = {Taipei, Taiwan},
series = {MMSys'17}
}

@article{10.1145/3426866,
author = {Meng, Linhao and Wei, Yating and Pan, Rusheng and Zhou, Shuyue and Zhang, Jianwei and Chen, Wei},
title = {VADAF: Visualization for Abnormal Client Detection and Analysis in Federated Learning},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3–4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3426866},
doi = {10.1145/3426866},
abstract = {Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model’s visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {aug},
articleno = {26},
numpages = {23},
keywords = {visual analytics, anomaly detection, Federated learning}
}

@article{10.1145/3502852,
author = {Yu, Hao and Hu, Xing and Li, Ge and Li, Ying and Wang, Qianxiang and Xie, Tao},
title = {Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3502852},
doi = {10.1145/3502852},
abstract = {In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {62},
numpages = {25},
keywords = {deep learning, dataset collection, Code clone detection}
}

@inproceedings{10.1145/3459955.3460617,
author = {Gosh, Saptarshi and EL Boudani, Brahim and Dagiuklas, Tasos and Iqbal, Muddesar},
title = {SO-KDN: A Self-Organised Knowledge Defined Networks Architecture for Reliable Routing},
year = {2021},
isbn = {9781450389136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459955.3460617},
doi = {10.1145/3459955.3460617},
abstract = {“When you are destined for an important appoint-ment, you would obviously opt for the most reliable route instead of the shortest in order to be well prepared”. Modern networking is presently undergoing through a quantum leap. To cope up with ambitious demands and user expectations, it is becoming more complex both structurally and functionally. Software Defined Networking (SDN) happens to be an instance of such advancements. It has significantly leveraged the network programmability, abstraction, and automation. Eventually, with acceptance form all major network infrastructure such as 5G and Cloud, SDN is becoming the standard of future networking. Likewise, Machine Learning (ML) has become the trendiest skill-in-demand recently. With its superiority of analyzing data, makes it applicable for almost every possible domain. The attempt to applying the power of ML in networking has not been too long, it allows the network to be more intelligent and capable enough to take optimal decisions to address some of its native problems. This gives rise to Self- Organized Networking (SON). In this article, Routing using Deep Neural Network (DNN) on top of SDN is addressed. We proposed a Self-organized Knowledge Defined Network (SO-KDN) architecture and an intelligent routing algorithm, that reactively finds the most reliable route, i.e., a route having least probability of fluctuation. This reduces network overhead due to re-routing and optimizes traffic congestion. Experimental data show a mean 90% accurate forecast in reliability prediction.},
booktitle = {2021 The 4th International Conference on Information Science and Systems},
pages = {160–166},
numpages = {7},
keywords = {Deep Learning, SON, SDN, Routing},
location = {Edinburgh, United Kingdom},
series = {ICISS 2021}
}

@article{10.1145/3409473,
author = {Maiolo, Sof\'{\i}a and Etcheverry, Lorena and Marotta, Adriana},
title = {Data Profiling in Property Graph Databases},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3409473},
doi = {10.1145/3409473},
abstract = {Property Graph databases are being increasingly used within the industry as a powerful and flexible way to model real-world scenarios. With this flexibility, a great challenge appears regarding profiling tasks due to the need of adapting them to these new models while taking advantage of the Property Graphs’ particularities. This article proposes a set of data profiling tasks by integrating existing methods and techniques and an taxonomy to classify them. In addition, an application pipeline is provided while a formal specification of some tasks is defined.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {20},
numpages = {27},
keywords = {Property Graph, data profiling}
}

@article{10.1145/2738210.2738211,
author = {Kenneally, Erin},
title = {How to Throw the Race to the Bottom: Revisiting Signals for Ethical and Legal Research Using Online Data},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0095-2737},
url = {https://doi.org/10.1145/2738210.2738211},
doi = {10.1145/2738210.2738211},
abstract = {With research using data available online, researcher conduct is not fully prescribed or proscribed by formal ethical codes of conduct or law because of ill-fitting "expectations signals" -- indicators of legal and ethical risk. This article describes where these ordering forces breakdown in the context of online research and suggests how to identify and respond to these grey areas by applying common legal and ethical tenets that run across evolving models. It is intended to advance the collective dialogue work-in-progress toward a path that revisits and harmonizes more appropriate ethical and legal signals for research using online data between and among researchers, oversight entities, policymakers and society.},
journal = {SIGCAS Comput. Soc.},
month = {feb},
pages = {4–10},
numpages = {7},
keywords = {security research ethics, law}
}

@inproceedings{10.1145/2961111.2962605,
author = {Sun, Yan and Wang, Qing and Li, Mingshu},
title = {Understanding the Contribution of Non-Source Documents in Improving Missing Link Recovery: An Empirical Study},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962605},
doi = {10.1145/2961111.2962605},
abstract = {Background: Links between issue reports and their fixing commits play an important role in software maintenance. Such link data are often missing in practice and many approaches have been proposed in order to recover them automatically. Most of existing approaches focus on comparing log messages and source code files in commits with issues reports. Besides the two kinds of data in commits, non-source documents (NSDs) such as change logs usually record the fixing activities and sometimes share similar texts as those in issue reports. However, few discussions have been made on the role of NSDs in designing link recovery approaches.Aims: This paper aims at understanding whether and how NSDs affect the performance of link recovery approaches.Method: An empirical study is conducted to evaluate the role of NSDs in link recovery approaches in 18 open source projects with 6370 issues and 22761 commits.Results: With the inclusion of NSDs, link recovery approaches can get an average increase in F-Measure ranging from 2.76% - 25.63%. Further examinations show NSDs contribute to the performance improvement in 15 projects and have exceptions in 3 projects. The performance improvement in the 15 projects is mainly from the filtering of noisy links. On average, 23.59% - 76.30% false links can be excluded by exploiting NSDs in the link recovery approach. We also analyze the 3 projects in which NSDs cannot improve the performance. Our finding shows sophisticated data selection in NSDs is necessary.Conclusions: Our preliminary findings demonstrate that involving NSDs can improve the performance of link recovery approaches in most cases.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {39},
numpages = {10},
keywords = {Software Maintenance, Non-Source Documents, Missing Link Recovery, Mining Software Repositories},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/3098954.3105822,
author = {Stupka, V\'{a}clav and Hor\'{a}k, Martin and Hus\'{a}k, Martin},
title = {Protection of Personal Data in Security Alert Sharing Platforms},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3105822},
doi = {10.1145/3098954.3105822},
abstract = {In order to ensure confidentiality, integrity and availability (so called CIA triad) of data within network infrastructure, it is necessary to be able to detect and handle cyber security incidents. For this purpose, it is vital for Computer Security Incident Response Teams (CSIRT) to have enough data on relevant security events and threats. That is why CSIRTs share security alerts and incidents data using various sharing platforms. Even though they do so primarily to protect data and privacy of users, their use also lead to additional processing of personal data, which may cause new privacy risks. European data protection law, especially with the adoption of the new General data protection regulation, sets out very strict rules on processing of personal data which on one hand leads to greater protection of individual's rights, but on the other creates great obstacles for those who need to share any personal data. This paper analyses the General Data Protection Regulation (GDPR), relevant case-law and analyses by the Article 29 Working Party to propose optimal methods and level of personal data processing necessary for effective use of security alert sharing platforms, which would be legally compliant and lead to appropriate balance between risks.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {65},
numpages = {8},
keywords = {Information sharing, Cyber security, Privacy, Personal data, Intrusion detection, Alert sharing platform},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/3274895.3274899,
author = {Oliver, Dev and Hoel, Erik G.},
title = {A Trace Framework for Analyzing Utility Networks: A Summary of Results (Industrial Paper)},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274899},
doi = {10.1145/3274895.3274899},
abstract = {Given a utility network and one or more starting points that define where analysis should begin, the problem of analyzing utility networks entails assembling a subset of network elements that meet some specified criteria. Analyzing utility network data has several applications and provides tremendous business value to utilities. For example, analysis may answer questions about the current state of the network (e.g., what valves need to be closed to shut off water flow to a location of a pipe leak), help to design future facilities (e.g., how many houses are fed by a transformer and can the transformer supply another house without overloading its capacity?), and help to organize business practices (e.g., create circuit maps for work crews to facilitate damage assessment after an ice storm). Analyzing utility networks is a challenging problem due to 1) the size of the data, which could have many tens of millions of network elements per utility, and billions of elements at the nationwide or continental scale, 2) modeling and analyzing utility assets at high fidelity (level of detail), and 3) the different analysis requirements across utility domains (e.g., water, wastewater, sewer, district heating, gas, electric, fiber, and telecom). This paper describes the trace framework for utility network analysis that has been implemented in ArcGIS Pro 2.1/ArcGIS Enterprise 10.6. The trace framework features algorithms in a services-based architecture for addressing analysis tasks across a wide array of utility domains. Previous approaches have focused on solving specific problems in specific domains whereas the trace framework provides a more general, scalable solution. We present experiments that demonstrate the scalability of the trace framework and a case study that highlights its value in performing a wide variety of analytics on utility networks.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {249–258},
numpages = {10},
keywords = {utility networks, graphs and networks, spatial databases, GIS, graph algorithms},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@article{10.1145/3494582,
author = {Shen, Cong and Qian, Zhaozhi and Huyuk, Alihan and Van Der Schaar, Mihaela},
title = {MARS: Assisting Human with Information Processing Tasks Using Machine Learning},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2691-1957},
url = {https://doi.org/10.1145/3494582},
doi = {10.1145/3494582},
abstract = {This article studies the problem of automated information processing from large volumes of unstructured, heterogeneous, and sometimes untrustworthy data sources. The main contribution is a novel framework called Machine Assisted Record Selection (MARS). Instead of today’s standard practice of relying on human experts to manually decide the order of records for processing, MARS learns the optimal record selection via an online learning algorithm. It further integrates algorithm-based record selection and processing with human-based error resolution to achieve a balanced task allocation between machine and human. Both fixed and adaptive MARS algorithms are proposed, leveraging different statistical knowledge about the existence, quality, and cost associated with the records. Experiments using semi-synthetic data that are generated from real-world patients record processing in the UK national cancer registry are carried out, which demonstrate significant (3 to 4 fold) performance gain over the fixed-order processing. MARS represents one of the few examples demonstrating that machine learning can assist humans with complex jobs by automating complex triaging tasks.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {21},
numpages = {19},
keywords = {human-in-the-loop decision support system, online learning, Data entry}
}

@inproceedings{10.1145/2750858.2807526,
author = {Hovsepian, Karen and al'Absi, Mustafa and Ertin, Emre and Kamarck, Thomas and Nakajima, Motohiro and Kumar, Santosh},
title = {CStress: Towards a Gold Standard for Continuous Stress Assessment in the Mobile Environment},
year = {2015},
isbn = {9781450335744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2750858.2807526},
doi = {10.1145/2750858.2807526},
abstract = {Recent advances in mobile health have produced several new models for inferring stress from wearable sensors. But, the lack of a gold standard is a major hurdle in making clinical use of continuous stress measurements derived from wearable sensors. In this paper, we present a stress model (called cStress) that has been carefully developed with attention to every step of computational modeling including data collection, screening, cleaning, filtering, feature computation, normalization, and model training. More importantly, cStress was trained using data collected from a rigorous lab study with 21 participants and validated on two independently collected data sets --- in a lab study on 26 participants and in a week-long field study with 20 participants. In testing, the model obtains a recall of 89% and a false positive rate of 5% on lab data. On field data, the model is able to predict each instantaneous self-report with an accuracy of 72%.},
booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {493–504},
numpages = {12},
keywords = {mobile health (mHealth), wearable sensors, stress, modeling},
location = {Osaka, Japan},
series = {UbiComp '15}
}

@article{10.1145/3524104,
author = {Qu, Youyang and Uddin, Md Palash and Gan, Chenquan and Xiang, Yong and Gao, Longxiang and Yearwood, John},
title = {Blockchain-Enabled Federated Learning: A Survey},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3524104},
doi = {10.1145/3524104},
abstract = {Federated learning (FL) is experiencing fast booming in recent years, which is jointly promoted by the prosperity of machine learning and Artificial Intelligence along with the emerging privacy issues. In the FL paradigm, a central server and local end devices maintain the same model by exchanging model updates instead of raw data, with which the privacy of data stored on end devices is not directly revealed. In this way, the privacy violation caused by the growing collection of sensitive data can be mitigated. However, the performance of FL with a central server is reaching a bottleneck while new threats are emerging simultaneously. There are various reasons, among which the most significant ones are centralized processing, data falsification, and lack of incentives. To accelerate the proliferation of FL, blockchain-enabled FL has attracted substantial attention from both academia and industry. A considerable number of novel solutions are devised to meet the emerging demands of diverse scenarios. Blockchain-enabled FL provides both theories and techniques to improve the performances of FL from various perspectives. In this survey, we will comprehensively summarize and evaluate existing variants of blockchain-enabled FL, identify the emerging challenges, and propose potentially promising research directions in this under-explored domain.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {feb},
keywords = {Countermeasures., Attacks, Blockchain, Federated Learning}
}

@inproceedings{10.1145/3498851.3498929,
author = {Dautaras, Justas and Matskin, Mihhail},
title = {Mobile Crowdsensing with Imagery Tasks},
year = {2021},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498929},
doi = {10.1145/3498851.3498929},
abstract = {The amount of gadgets connected to the internet has grown rapidly in the recent years. These human owned devices can potentially be used to gather sensor data without active involvement of their owners. One of the types of platforms that contribute to the utilisation of these devices are mobile crowdsensing systems. These systems can be used for different tasks including different types of community support. While these systems are quite widely used, yet little research has been done for integration of imagery data into them which require also human involvement. This paper considers a mobile crowdsensing system where gathering data from sensors is supported by crowdsourcing human intelligence for providing both textual and visual information. We also explore the best settings for such a system. Imagery processing is integrated into an already existing mobile crowdsensing platform CrowdS. The solution was evaluated both by a limited number of real life users and by conducting simulations. The simulations represent complex scenarios with multi-level variables. The results of simulation allow suggest an efficient configuration for the parameters and characteristics of the environment used in imagery integration.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {54–61},
numpages = {8},
keywords = {mobile sensing devices, crowdsourcing, mobile crowdsensing},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@article{10.1145/2935634.2935641,
author = {Bajpai, Vaibhav and Berger, Arthur W. and Eardley, Philip and Ott, J\"{o}rg and Sch\"{o}nw\"{a}lder, J\"{u}rgen},
title = {Global Measurements: Practice and Experience (Report on Dagstuhl Seminar #16012)},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2935634.2935641},
doi = {10.1145/2935634.2935641},
abstract = {This article summarises a 2.5 day long Dagstuhl seminar on Global Measurements: Practice and Experience held in January 2016. This seminar was a followup of the seminar on Global Measurement Frameworks held in 2013, which focused on the development of global Internet measurement platforms and associated metrics. The second seminar aimed at discussing the practical experience gained with building these global Internet measurement platforms. It brought together people who are actively involved in the design and maintenance of global Internet measurement platforms and who do research on the data delivered by such platforms. Researchers in this seminar have used data derived from global Internet measurement platforms in order to manage networks or services or as input for regulatory decisions. The entire set of presentations delivered during the seminar is made publicly available at [1].},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {32–39},
numpages = {8},
keywords = {network management, quality of experience, traffic engineering, internet measurements}
}

@article{10.1109/TCBB.2019.2953908,
author = {Wang, Bing and Mei, Changqing and Wang, Yuanyuan and Zhou, Yuming and Cheng, Mu-Tian and Zheng, Chun-Hou and Wang, Lei and Zhang, Jun and Chen, Peng and Xiong, Yan},
title = {Imbalance Data Processing Strategy for Protein Interaction Sites Prediction},
year = {2021},
issue_date = {May-June 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2953908},
doi = {10.1109/TCBB.2019.2953908},
abstract = {Protein-protein interactions play essential roles in various biological progresses. Identifying protein interaction sites can facilitate researchers to understand life activities and therefore will be helpful for drug design. However, the number of experimental determined protein interaction sites is far less than that of protein sites in protein-protein interaction or protein complexes. Therefore, the negative and positive samples are usually imbalanced, which is common but bring result bias on the prediction of protein interaction sites by computational approaches. In this work, we presented three imbalance data processing strategies to reconstruct the original dataset, and then extracted protein features from the evolutionary conservation of amino acids to build a predictor for identification of protein interaction sites. On a dataset with 10,430 surface residues but only 2,299 interface residues, the imbalance dataset processing strategies can obviously reduce the prediction bias, and therefore improve the prediction performance of protein interaction sites. The experimental results show that our prediction models can achieve a better prediction performance, such as a prediction accuracy of 0.758, or a high F-measure of 0.737, which demonstrated the effectiveness of our method.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {may},
pages = {985–994},
numpages = {10}
}

@inproceedings{10.1145/2390045.2390062,
author = {B\"{a}r, Arian and Golab, Lukasz},
title = {Towards Benchmarking Stream Data Warehouses},
year = {2012},
isbn = {9781450317214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390045.2390062},
doi = {10.1145/2390045.2390062},
abstract = {Data management systems are facing two challenges driven by the requirements of emerging data-intensive applications: more data and less time to process the data. Data volumes continue to increase as new sources and data collecting mechanisms appear. At the same time, these sources tend to be highly dynamic and generate data in the form of a stream, which requires quick reaction to newly arrived data. Traditional data warehouses enable scalable data storage and analytics, including the ability to define nested levels of materialized views. However, views are typically refreshed during downtimes---e.g., every night---which does not meet the latency requirements of many applications. Stream data warehousing is a new data management technology that allows nearly-continuous view refresh as new data arrive, which enables seamless integration of real-time monitoring and business intelligence with long-term data mining. In this paper, we argue that a new benchmark is required for stream warehouses, which should focus on measuring the property that determines the utility of these systems, namely how well they can keep up with the incoming data and guarantee the "freshness" of materialized views.},
booktitle = {Proceedings of the Fifteenth International Workshop on Data Warehousing and OLAP},
pages = {105–112},
numpages = {8},
keywords = {materialized view maintenance, data warehouse benchmarking, stream data warehousing},
location = {Maui, Hawaii, USA},
series = {DOLAP '12}
}

@inbook{10.1145/3447404.3447411,
author = {McMenemy, David},
title = {Ethics and Statistics},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447411},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {101–103},
numpages = {3}
}

@inbook{10.1145/3310205.3310210,
title = {Data Transformation},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310210},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3490099.3511115,
author = {Dodge, Jonathan and Anderson, Andrew A. and Olson, Matthew and Dikkala, Rupika and Burnett, Margaret},
title = {How Do People Rank Multiple Mutant Agents?},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511115},
doi = {10.1145/3490099.3511115},
abstract = {Faced with several AI-powered sequential decision-making systems, how might someone choose on which to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (i.e., too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) Explanation Resolution, a quantifiable direct measurement concept; 2) a new XAI empirical task to measure explanations: “the Ranking Task”; and 3) a new strategy for inducing controllable agent variations—Mutant Agent Generation. In support of those main contributions, it also presents 4) novel explanations for sequential decision-making agents; 5) an adaptation to the AAR/AI assessment process; and 6) a qualitative study around these devices with 10 participants to investigate how they performed the Ranking Task on our mutant agents, using our explanations, and structured by AAR/AI. From an XAI researcher perspective, just as mutation testing can be applied to any code, mutant agent generation can be applied to essentially any neural network for which one wants to evaluate an assessment process or explanation type. As to an XAI user’s perspective, the participants ranked the agents well overall, but showed the importance of high explanation resolution for close differences between agents. The participants also revealed the importance of supporting a wide diversity of explanation diets and agent “test selection” strategies.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {191–211},
numpages = {21},
keywords = {After-Action Review, Explainable AI},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.14778/3342263.3342626,
author = {Wei, Ziheng and Link, Sebastian},
title = {Embedded Functional Dependencies and Data-Completeness Tailored Database Design},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342626},
doi = {10.14778/3342263.3342626},
abstract = {We establish a robust schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing many redundant data value occurrences. We establish axiomatic and algorithmic foundations for reasoning about embedded functional dependencies. These foundations allow us to establish generalizations of Boyce-Codd and Third normal forms that do not permit any redundancy in any future application data, or minimize their redundancy across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate our framework, and the effectiveness and efficiency of our algorithms, but also provide quantified insight into database schema design trade-offs.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1458–1470},
numpages = {13}
}

@inproceedings{10.1145/3461702.3462605,
author = {Kelley, Patrick Gage and Yang, Yongwei and Heldreth, Courtney and Moessner, Christopher and Sedley, Aaron and Kramm, Andreas and Newman, David T. and Woodruff, Allison},
title = {Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462605},
doi = {10.1145/3461702.3462605},
abstract = {As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {627–637},
numpages = {11},
keywords = {public perception, artificial intelligence},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1145/3371906,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Combemale, Benoit and Bastin, Lucy and Bencomo, Nelly and Bruel, Jean-Michel and Becker, Christoph and Betz, Stefanie and Chitchyan, Ruzanna and Cheng, Betty H. C. and Klingert, Sonja and Paige, Richard F. and Penzenstadler, Birgit and Seyff, Norbert and Syriani, Eugene and Venters, Colin C.},
title = {Toward Model-Driven Sustainability Evaluation},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3371906},
doi = {10.1145/3371906},
abstract = {Exploring the vision of a model-based framework that may enable broader engagement with and informed decision making about sustainability issues.},
journal = {Commun. ACM},
month = {feb},
pages = {80–91},
numpages = {12}
}

