@article{10.1145/3543508,
author = {Opdahl, Andreas L. and Al-Moslmi, Tareq and Dang-Nguyen, Duc-Tien and Gallofr\'{e} Oca\~{n}a, Marc and Tessem, Bj\o{}rnar and Veres, Csaba},
title = {Semantic Knowledge Graphs for the News: A Review},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3543508},
doi = {10.1145/3543508},
abstract = {ICT platforms for news production, distribution, and consumption must exploit the ever-growing availability of digital data. These data originate from different sources and in different formats; they arrive at different velocities and in different volumes. Semantic knowledge graphs (KGs) is an established technique for integrating such heterogeneous information. It is therefore well-aligned with the needs of news producers and distributors, and it is likely to become increasingly important for the news industry. This paper reviews the research on using semantic knowledge graphs for production, distribution, and consumption of news. The purpose is to present an overview of the field; to investigate what it means; and to suggest opportunities and needs for further research and development.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {may},
keywords = {Knowledge Graphs, Literature Review, Semantic Technologies, News Distribution, News, News Consumption, Semantic Web, Linked Open Data, Ontology, News Production, Linked Data, Journalism}
}

@article{10.1145/3448888,
author = {Koesten, Laura and Simperl, Elena},
title = {UX of Data: Making Data Available Doesn't Make It Usable},
year = {2021},
issue_date = {March - April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1072-5520},
url = {https://doi.org/10.1145/3448888},
doi = {10.1145/3448888},
abstract = {This forum provides a space to engage with the challenges of designing for intelligent algorithmic experiences. We invite articles that tackle the tensions between research and practice when integrating AI and UX design. We welcome interdisciplinary debate, artful critique, forward-looking research, case studies of AI in practice, and speculative design explorations. --- Juho Kim and Henriette Cramer, Editors},
journal = {Interactions},
month = {mar},
pages = {97–99},
numpages = {3}
}

@article{10.1145/1978542.1978562,
author = {Chaudhuri, Surajit and Dayal, Umeshwar and Narasayya, Vivek},
title = {An Overview of Business Intelligence Technology},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/1978542.1978562},
doi = {10.1145/1978542.1978562},
abstract = {BI technologies are essential to running today's businesses and this technology is going through sea changes.},
journal = {Commun. ACM},
month = {aug},
pages = {88–98},
numpages = {11}
}

@inproceedings{10.1145/3384772.3385138,
author = {H. Gyldenkaerne, Christopher and From, Gustav and M\o{}nsted, Troels and Simonsen, Jesper},
title = {PD and The Challenge of AI in Health-Care},
year = {2020},
isbn = {9781450376068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384772.3385138},
doi = {10.1145/3384772.3385138},
abstract = {In its promise to contribute to considerable cost savings and improved patient care through efficient analysis of the tremendous amount of data stored in electronic health records (EHR), there is currently a strong push for the proliferation of artificial intelligence (AI) in health-care. We identify, through a study of AI being used to predict patient no-show’s, that for the AI to gain full potential there lies a need to balance the introduction of AI with a proper focus on the patients and the clinicians’ interests. We call for a Participatory Design (PD) approach to understand and reconfigure the socio-technical setup in health-care, especially where AI is being used on EHR data that are manually being submitted by health-care personnel.},
booktitle = {Proceedings of the 16th Participatory Design Conference 2020 - Participation(s) Otherwise - Volume 2},
pages = {26–29},
numpages = {4},
keywords = {Primary- and Secondary Use data, Participatory Design, precision medicine, Artificial Intelligence, Electronic Health Record data},
location = {Manizales, Colombia},
series = {PDC '20}
}

@inbook{10.1145/3310205.3310215,
title = {References},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310215},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1145/2935752,
author = {Morstatter, Fred and Liu, Huan},
title = {Replacing Mechanical Turkers? Challenges in the Evaluation of Models with Semantic Properties},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935752},
doi = {10.1145/2935752},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {15},
numpages = {4},
keywords = {Artificial intelligence, automation, data mining, evaluation, crowdsourcing}
}

@inproceedings{10.1145/3297280.3297477,
author = {Maamar, Zakaria and Baker, Thar and Faci, Noura and Ugljanin, Emir and Khafajiy, Mohammed Al and Bur\'{e}gio, Vanilson},
title = {Towards a Seamless Coordination of Cloud and Fog: Illustration through the Internet-of-Things},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297477},
doi = {10.1145/3297280.3297477},
abstract = {With the increasing popularity of the Internet-of-Things (IoT), organizations are revisiting their practices as well as adopting new ones so they can deal with an ever-growing amount of sensed and actuated data that IoT-compliant things generate. Some of these practices are about the use of cloud and/or fog computing. The former promotes "anything-as-a-service" and the latter promotes "process data next to where it is located". Generally presented as competing models, this paper discusses how cloud and fog could work hand-in-hand through a seamless coordination of their respective "duties". This coordination stresses out the importance of defining where the data of things should be sent (either cloud, fog, or cloud&amp;fog concurrently) and in what order (either cloud then fog, fog then cloud, or fog&amp;cloud concurrently). Applications' concerns with data such as latency, sensitivity, and freshness dictate both the appropriate recipients and the appropriate orders. For validation purposes, a healthcare-driven IoT application along with an in-house testbed, that features real sensors and fog and cloud platforms, have permitted to carry out different experiments that demonstrate the technical feasibility of the coordination model.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2008–2015},
numpages = {8},
keywords = {fog, internet-of-things, cloud, coordination, healthcare},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3209581,
author = {Baeza-Yates, Ricardo},
title = {Bias on the Web},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3209581},
doi = {10.1145/3209581},
abstract = {Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.},
journal = {Commun. ACM},
month = {may},
pages = {54–61},
numpages = {8}
}

@inproceedings{10.1145/3277593.3277642,
author = {Truong, Hong-Linh},
title = {Dynamic IoT Data, Protocol, and Middleware Interoperability with Resource Slice Concepts and Tools: Tutorial},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277642},
doi = {10.1145/3277593.3277642},
abstract = {Dealing with interoperability in the IoT domain is a complex matter that requires various techniques for tackling data, protocol and middleware interoperability. We cannot solve IoT interoperability problems by just developing (new) software components and (semantic) data models. In this tutorial, we will present interoperability techniques for complex IoT Cloud applications by leveraging dynamic solutions of provisioning and reconfiguring of IoT data processing pipelines, protocol bridges, IoT middleware and cloud services. First, the tutorial will examine cross-layered, cross-system inter-operability issues and present a DevOps IoT Interoperability approach for defining metadata, selecting resources and software artifacts, and provisioning and connecting resources to create various potential solutions for IoT Cloud interoperability using resource slice concepts. Second, the tutorial will present techniques for dynamically provisioning data pipelines, middleware services, protocol adapters and custom solutions to address cross-layered, cross-system interoperability for IoT Cloud applications. Such solutions also allow dynamic reconfiguration of resources to add/remove interoperability support. We will present the concepts and techniques with hands-on examples using our research tools rsiHub and IoTCloudSamples.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {48},
numpages = {4},
keywords = {resource slice, IoT interoperability, cloud computing},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/2661829.2663539,
author = {Alonso, Omar and Kamps, Jaap and Karlgren, Jussi},
title = {Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14): CIKM 2014 Workshop},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2663539},
doi = {10.1145/2661829.2663539},
abstract = {There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remains to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side - the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues - and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in "graph search" change the classic division between searchers and information and lead to extreme personalization - are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects - how to not creep out users?},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {2094–2095},
numpages = {2},
keywords = {query suggest, graph search, semantic annotation},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/3371425.3371459,
author = {Tan, Wenan and Jiang, Zihui},
title = {A Novel Experience-Based Incentive Mechanism for Mobile Crowdsensing System},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371459},
doi = {10.1145/3371425.3371459},
abstract = {While sensor networks have been pervasively deployed in the real world, more and more mobile crowdsensing (MCS) applications have come into realization to collaboratively detect events and collect data. This paper aims to design a novel incentive mechanism to achieve good services for mobile crowdsensing applications. Responding to insufficient participants, we propose a novel Experience-Based incentive mechanism using Reverse Auction (EBRA). Additionally, it can also guarantee fair competition while maximizing the total profit of the service platform. Through strictly proving, our proposed EBRA incentive mechanism satisfies four properties: computational efficiency, individual rationality, profitability, and truthfulness. The extensive simulations show that the proposed EBRA method has a better performance over 20% than other benchmark mechanisms.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {70},
numpages = {6},
keywords = {incentive mechanism, mobile crowdsensing, fairness competition, sensor network},
location = {Sanya, China},
series = {AIIPCC '19}
}

@inproceedings{10.1145/3472163.3472173,
author = {Zouari, Firas and Kabachi, Nadia and Boukadi, Khouloud and Ghedira Guegan, Chirine},
title = {Data Management in the Data Lake: A Systematic Mapping},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472173},
doi = {10.1145/3472163.3472173},
abstract = {The computer science community is paying more and more attention to data due to its crucial role in performing analysis and prediction. Researchers have proposed many data containers such as files, databases, data warehouses, cloud systems, and recently data lakes in the last decade. The latter enables holding data in its native format, making it suitable for performing massive data prediction, particularly for real-time application development. Although data lake is well adopted in the computer science industry, its acceptance by the research community is still in its infancy stage. This paper sheds light on existing works for performing analysis and predictions on data placed in data lakes. Our study reveals the necessary data management steps, which need to be followed in a decision process, and the requirements to be respected, namely curation, quality evaluation, privacy-preservation, and prediction. This study aims to categorize and analyze proposals related to each step mentioned above.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {280–284},
numpages = {5},
keywords = {Systematic mapping, Data management, Data lake},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3487553.3524214,
author = {Gao, Liming and Liao, Dongliang and Li, Gongfu and Xu, Jin and Zhuo, Hankz Hankui},
title = {Semantic IR Fused Heterogeneous Graph Model in Tag-Based Video Search},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524214},
doi = {10.1145/3487553.3524214},
abstract = {With the rapid growth of video resources on the Internet, text-video retrieval has become a common requirement. Scholars handled text-video retrieval tasks with two-broad-category: concept-based methods and neural semantics match networks. Besides deep neural semantics matching models, some scholars mined queries and videos relationships from click-graphs, which express the users’ implicit judgments on relevance relations. However, bad generalization of click-based or concept-based models hardly capture semantic information from short queries, which stunt existing methods to fully utilize the methods to enhance the IR performance. In this paper, we propose a framework ETHGS to combine the abilities of concept-based, click-based and semantic-based models in IR and publish a new video retrieval dataset QVT from a real-world video search engine. In ETHGS, we make use of tags (i.e. concept) to construct a heterogeneous graph to alleviate the sparsity of click-through data. And we also overcome the problem of long-tailed query representation without graph information by fusing tag embeddings to represent queries. ETHGS leverages semantic embeddings to review deviant semantic information from graph nodes information. Finally, we evaluate our model ETHGS on the QVT.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {94–98},
numpages = {5},
keywords = {gaze detection, neural networks, datasets, text tagging},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3360322.3360836,
author = {Baasch, Gaby and Wicikowski, Adam and Faure, Ga\"{e}lle and Evins, Ralph},
title = {Comparing Gray Box Methods to Derive Building Properties from Smart Thermostat Data},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360836},
doi = {10.1145/3360322.3360836},
abstract = {The development of quantitative techniques for determining the amount of heat lost through the building envelope is essential for targeted retrofits. This type of evaluation is traditionally a resource intensive process that involves onsite appraisal and in-situ measurements. In order to build more efficient and scalable methods for retrofit analysis, new sources of data could be used. Smart thermostat data, for example, provide a valuable resource, however they often lack detailed information about the building characteristics and energy loads. This paper presents and compares three methods for assessing heating characteristics of households using a dataset that does not contain heating power. The three methods are based on: (1) balance point plots, (2) the extraction of indoor temperature decay curves, and (3) the classic differential equation for indoor temperature. These methods all take a gray box approach in which physics-based and machine learning models are combined. The dataset used for this study consists of over 4,000 houses in Ontario and New York. The three methods are applied to each building and the resulting data is analyzed to determine whether the results are statistically sound. It is found that there is a positive linear correlation between characteristics derived for each method, although there is uncertainty about absolute values. This result indicates that the methods can be used to ascertain relative values for the thermal characteristics of a building. The methods suggested in this paper may therefore be used to filter heating profiles to target potential retrofit measures or other stock-level decisions.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {223–232},
numpages = {10},
keywords = {smart thermostats, thermal characteristics, Buildings, gray box models},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@inproceedings{10.1145/2815833.2816944,
author = {Rizzo, Giuseppe and Corcho, Oscar and Troncy, Rapha\"{e}l and Plu, Julien and Hermida, Juan Carlos Ballesteros and Assaf, Ahmad},
title = {The 3cixty Knowledge Base for Expo Milano 2015: Enabling Visitors to Explore the City},
year = {2015},
isbn = {9781450338493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815833.2816944},
doi = {10.1145/2815833.2816944},
abstract = {In this paper, we present the 3cixty Knowledge Base, which collects and harmonizes descriptions of events, places, transportation facilities and user-generated data such as reviews of the city and Expo site of Milan. This knowledge base is used by a set of web and mobile applications to guide Expo Milano 2015 visitors in the city and in the exhibit, allowing them to find places, satellite events and transportation facilities around Milan. As of July 24th, 2015 the knowledge base contains 18665 unique events, 225821 unique places, 94789 reviews, and 9343 transportation facilities, collected from several static, near- and real time local and global data providers, including Expo Milano 2015 official services and numerous social media platforms. The ontologies used as a backbone for structuring the knowledge base follow a rigorous development method where the design principle has generally been to re-use existing ontologies when they exist. We think that the lessons learned from this development will be useful for similar endeavors in other cities or large events around the world with a similar ecosystem of data provisioning services.},
booktitle = {Proceedings of the 8th International Conference on Knowledge Capture},
articleno = {18},
numpages = {4},
keywords = {Smart City, Expo 2015, Data Reconciliation, 3cixty, Knowledge base, Data Integration},
location = {Palisades, NY, USA},
series = {K-CAP 2015}
}

@article{10.1007/s00778-017-0477-2,
author = {Ali, Syed Muhammad and Wrembel, Robert},
title = {From Conceptual Design to Performance Optimization of ETL Workflows: Current State of Research and Open Problems},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0477-2},
doi = {10.1007/s00778-017-0477-2},
abstract = {In this paper, we discuss the state of the art and current trends in designing and optimizing ETL workflows. We explain the existing techniques for: (1) constructing a conceptual and a logical model of an ETL workflow, (2) its corresponding physical implementation, and (3) its optimization, illustrated by examples. The discussed techniques are analyzed w.r.t. their advantages, disadvantages, and challenges in the context of metrics such as autonomous behavior, support for quality metrics, and support for ETL activities as user-defined functions. We draw conclusions on still open research and technological issues in the field of ETL. Finally, we propose a theoretical ETL framework for ETL optimization.},
journal = {The VLDB Journal},
month = {dec},
pages = {777–801},
numpages = {25},
keywords = {ETL conceptual design, ETL optimization, ETL workflow, ETL logical design, ETL physical implementation}
}

@article{10.1145/3446373,
author = {Li, Xi and Wang, Zehua and Leung, Victor C. M. and Ji, Hong and Liu, Yiming and Zhang, Heli},
title = {Blockchain-Empowered Data-Driven Networks: A Survey and Outlook},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446373},
doi = {10.1145/3446373},
abstract = {The paths leading to future networks are pointing towards a data-driven paradigm to better cater to the explosive growth of mobile services as well as the increasing heterogeneity of mobile devices, many of which generate and consume large volumes and variety of data. These paths are also hampered by significant challenges in terms of security, privacy, services provisioning, and network management. Blockchain, which is a technology for building distributed ledgers that provide an immutable log of transactions recorded in a distributed network, has become prominent recently as the underlying technology of cryptocurrencies and is revolutionizing data storage and processing in computer network systems. For future data-driven networks (DDNs), blockchain is considered as a promising solution to enable the secure storage, sharing, and analytics of data, privacy protection for users, robust, trustworthy network control, and decentralized routing and resource managements. However, many important challenges and open issues remain to be addressed before blockchain can be deployed widely to enable future DDNs. In this article, we present a survey on the existing research works on the application of blockchain technologies in computer networks and identify challenges and potential solutions in the applications of blockchains in future DDNs. We identify application scenarios in which future blockchain-empowered DDNs could improve the efficiency and security, and generally the effectiveness of network services.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {58},
numpages = {38},
keywords = {networking technologies, Data-driven networks, blockchain-empowered data-driven networks, blockchain}
}

@inbook{10.1145/3310205.3310214,
title = {Conclusion and Future Thoughts},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310214},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1145/3191750,
author = {Lin, Yuxiang and Dong, Wei and Chen, Yuan},
title = {Calibrating Low-Cost Sensors by a Two-Phase Learning Approach for Urban Air Quality Measurement},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191750},
doi = {10.1145/3191750},
abstract = {Urban air quality information, e.g., PM2.5 concentration, is of great importance to both the government and society. Recently, there is a growing interest in developing low-cost sensors, installed on moving vehicles, for fine-grained air quality measurement. However, low-cost mobile sensors typically suffer from low accuracy and thus need careful calibration to preserve a high measurement quality. In this paper, we propose a two-phase data calibration method consisting of a linear part and a nonlinear part. We use MLS (multiple least square) to train the linear part, and use RF (random forest) to train the nonlinear part. We propose an automatic feature selection algorithm based on AIC (Akaike information criterion) for the linear model, which helps avoid overfitting due to the inclusion of inappropriate features. We evaluate our method extensively. Results show that our method outperforms existing approaches, achieving an overall accuracy improvement of 16.4% in terms of PM2.5 levels compared with state-of-the-art approach.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {18},
numpages = {18},
keywords = {Mobile sensor network, Low-cost sensors, Air quality, Sensor calibration}
}

@inproceedings{10.1145/3283207.3283210,
author = {Jilani, Musfira and Corcoran, Padraig and Bertolotto, Michela},
title = {A Multi-Layer CRF Based Methodology for Improving Crowdsourced Street Semantics},
year = {2018},
isbn = {9781450360371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3283207.3283210},
doi = {10.1145/3283207.3283210},
abstract = {This paper presents an intuitive and novel method for improving the semantic quality of streets in crowdsourced maps. Two factors negatively affecting the quality are incorrect and ambiguous semantics. Toward overcoming these, a multi-layer CRF based model is proposed that performs a simultaneous hierarchical classification of streets into fine-grained (crowdsourced; therefore, rich but ambiguous) and coarse-grained (familiar and standard) semantics. Inference is performed using Lazy Flipper algorithm which is fast for street network consisting of several hundred thousand streets. The model achieves a classification accuracy of 61% for fine-grained classification and 77% for coarse-grained classification respectively.},
booktitle = {Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
pages = {29–38},
numpages = {10},
keywords = {OpenStreetMap, Semantics, Street Networks, Conditional Random Fields, Hierarchical Classification},
location = {Seattle, WA, USA},
series = {IWCTS'18}
}

@inproceedings{10.1145/2938503.2938519,
author = {Caruccio, Loredana and Deufemia, Vincenzo and Polese, Giuseppe},
title = {On the Discovery of Relaxed Functional Dependencies},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938519},
doi = {10.1145/2938503.2938519},
abstract = {Functional dependencies (fds) express important relationships among data, which can be used for several goals, including schema normalization and data cleansing. However, to solve several issues in emerging application domains, such as the identification of data inconsistencies or patterns of semantically related data, it has been necessary to relax the fd definition through the introduction of approximations in data comparison and/or validity. Moreover, while fds were originally specified at design time, with the availability of massive data and computational power many algorithms have been devised to automatically discover them from data, including algorithms for discovering some types of relaxed fds. In this paper we present a technique that exploits lattice-based algorithms for the discovery of fds from data, in order to detect relaxed fds. Moreover, we introduce an algorithm to determine a proper distance threshold for a given relaxed fd holding over the entire database.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {53–61},
numpages = {9},
keywords = {discovery algorithm, functional dependency, approximate match, Database integration},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@article{10.1145/3085580,
author = {Razzaque, M. A. and Hira, Muta Tah and Dira, Mukta},
title = {QoS in Body Area Networks: A Survey},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3085580},
doi = {10.1145/3085580},
abstract = {Body Area Networks (BANs) are becoming increasingly popular and have shown great potential in real-time monitoring of the human body. With the promise of being cost-effective and unobtrusive and facilitating continuous monitoring, BANs have attracted a wide range of monitoring applications, including medical and healthcare, sports, and rehabilitation systems. Most of these applications are real time and life critical and require a strict guarantee of Quality of Service (QoS) in terms of timeliness, reliability, and so on. Recently, there has been a number of proposals describing diverse approaches or frameworks to achieve QoS in BANs (i.e., for different layers or tiers and different protocols). This survey put these individual efforts into perspective and presents a more holistic view of the area. In this regard, this article identifies a set of QoS requirements for BAN applications and shows how these requirements are linked in a three-tier BAN system and presents a comprehensive review of the existing proposals against those requirements. In addition, open research issues, challenges, and future research directions in achieving these QoS in BANs are highlighted.},
journal = {ACM Trans. Sen. Netw.},
month = {aug},
articleno = {25},
numpages = {46},
keywords = {QoS, medical care, Body area networks, cloud computing, healthcare}
}

@inproceedings{10.1145/3077136.3082060,
author = {Deng, Alex and Dmitriev, Pavel and Gupta, Somit and Kohavi, Ron and Raff, Paul and Vermeer, Lukas},
title = {A/B Testing at Scale: Accelerating Software Innovation},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3082060},
doi = {10.1145/3077136.3082060},
abstract = {The Internet provides developers of connected software, including web sites, applications, and devices, an unprecedented opportunity to accelerate innovation by evaluating ideas quickly and accurately using controlled experiments, also known as A/B tests. From front-end user-interface changes to backend algorithms, from search engines (e.g., Google, Bing, Yahoo!) to retailers (e.g., Amazon, eBay, Etsy) to social networking services (e.g., Facebook, LinkedIn, Twitter) to travel services (e.g., Expedia, Airbnb, Booking.com) to many startups, online controlled experiments are now utilized to make data-driven decisions at a wide range of companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher's experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and evaluation of online controlled experiments at scale (100's of concurrently running experiments) across variety of web sites, mobile apps, and desktop applications presents many pitfalls and new research challenges. In this tutorial we will give an introduction to A/B testing, share key lessons learned from scaling experimentation at Bing to thousands of experiments per year, present real examples, and outline promising directions for future work. The tutorial will go beyond applications of A/B testing in information retrieval and will also discuss on practical and research challenges arising in experimentation on web sites and mobile and desktop apps. Our goal in this tutorial is to teach attendees how to scale experimentation for their teams, products, and companies, leading to better data-driven decisions. We also want to inspire more academic research in the relatively new and rapidly evolving field of online controlled experimentation.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1395–1397},
numpages = {3},
keywords = {experimentation, a/b testing},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/2523429.2523458,
author = {Nyk\"{a}nen, Ossi},
title = {Datamap Visualization Technique for Interactively Visualizing Large Datasets},
year = {2013},
isbn = {9781450319928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523429.2523458},
doi = {10.1145/2523429.2523458},
abstract = {This article describes the novel datamap visualization technique which enables visualizing large datasets interactively and fairly, inspired by geographic maps and microscopes. The main contributions include introducing the datamap metaphor and datamap visualization architecture, specifying efficient methods of approximate rendering, and illustrating the basic concepts in terms of example applications.},
booktitle = {Proceedings of International Conference on Making Sense of Converging Media},
pages = {52–58},
numpages = {7},
keywords = {Visualization techniques and methodologies, Interactive data exploration and discovery, Datamap visualization, Data and knowledge visualization},
location = {Tampere, Finland},
series = {AcademicMindTrek '13}
}

@inproceedings{10.1145/3485768.3485770,
author = {Guo, Yuanyuan},
title = {Data Protection Measures in E-Society: Policy Implications of British Data Protection Act to China},
year = {2021},
isbn = {9781450390156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485768.3485770},
doi = {10.1145/3485768.3485770},
booktitle = {2021 5th International Conference on E-Society, E-Education and E-Technology},
pages = {171–176},
numpages = {6},
keywords = {Data protection, Private information, E-society, Public policy},
location = {Taipei, Taiwan},
series = {ICSET 2021}
}

@inproceedings{10.1145/3141880.3141886,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {Key Concepts of Data Management: An Empirical Approach},
year = {2017},
isbn = {9781450353014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141880.3141886},
doi = {10.1145/3141880.3141886},
abstract = {When preparing new topics for teaching, it is important to identify their central aspects. Sets of fundamental ideas, great principles or big ideas have already been described for several parts of computer science. Yet, existing catalogs of ideas, principles and concepts of computer science only consider the field data management marginally. However, we assume that several concepts of data management are fundamental to CS and, despite the significant changes in this field in recent years, have long-term relevance. In order to provide a comprehensive overview of the key concepts of data management and to bring relevant parts of this field to school, we describe and use an empirical approach to determine such central aspects systematically. This results in a model of key concepts of data management. On the basis of examples, we show how the model can be interpreted and used in different contexts and settings.},
booktitle = {Proceedings of the 17th Koli Calling International Conference on Computing Education Research},
pages = {30–39},
numpages = {10},
keywords = {practices, key concepts, mechanics, CS education, data management, principles, model, core technologies},
location = {Koli, Finland},
series = {Koli Calling '17}
}

@article{10.1145/2676566,
author = {MacLean, Diana Lynn},
title = {Gathering People to Gather Data},
year = {2014},
issue_date = {Winter 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/2676566},
doi = {10.1145/2676566},
abstract = {An interview with Paul Wicks, Vice President of Innovation at PatientsLikeMe, a patient network and real-time research platform.},
journal = {XRDS},
month = {dec},
pages = {18–22},
numpages = {5}
}

@inproceedings{10.1145/3210586.3210587,
author = {Baker, Karen S. and Karasti, Helena},
title = {Data Care and Its Politics: Designing for Local Collective Data Management as a Neglected Thing},
year = {2018},
isbn = {9781450363716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210586.3210587},
doi = {10.1145/3210586.3210587},
abstract = {In this paper, we think with Puig de la Bellacasa's 'matters of care' about how to support data care and its politics. We use the notion to reflect on participatory design activities in two recent case studies of local collective data management in ecological research. We ask "How to design for data care?" and "How to account for the politics of data care in design?" Articulation of data care together with ethically and politically significant data issues in design, reveals in these cases the invisible labors of care by local data advocates and a 'partnering designer'. With digital data work in the sciences increasing and data infrastructures for research under development at a variety of large scales, the local level is often considered merely a recipient of services rather than an active participant in design of data practices and infrastructures. We identify local collective data management as a 'neglected thing' in infrastructure planning and speculate on how things could be different in the data landscape.},
booktitle = {Proceedings of the 15th Participatory Design Conference: Full Papers - Volume 1},
articleno = {10},
numpages = {12},
keywords = {partnering designer, data care, information infrastructure, matters of care, politics, infrastructuring, local collective data management, information management, science and technology studies, participatory design},
location = {Hasselt and Genk, Belgium},
series = {PDC '18}
}

@inproceedings{10.1145/3487664.3487783,
author = {Diamantini, Claudia and Potena, Domenico and Storti, Emanuele},
title = {A Semantic Data Lake Model for Analytic Query-Driven Discovery},
year = {2021},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487783},
doi = {10.1145/3487664.3487783},
abstract = {Data Lake (DL) architectures have recently emerged as an effective solution to the problem of data analytics with big, highly heterogeneous, and quickly changing data sources. However, novel challenges arise too, including how to make sense of disparate raw data and how to identify the sources that satisfy a data need. In the paper, we introduce a semantic model for a Data Lake aimed to support data discovery and integration in data analytics scenarios. By formally modeling indicators of interest, their computation formulas, and dimensions of analysis in a knowledge graph, and by seamlessly mapping them to relevant source metadata, the framework is suited for identifying the sources and the required transformation steps according to the analytical request.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {183–186},
numpages = {4},
keywords = {query-driven discovery, ontology, indicator, Data Lake},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3485314.3485323,
author = {Su, Hang and He, Qian and Guo, Biao},
title = {KPI Anomaly Detection Method for Data Center AIOps Based on GRU-GAN},
year = {2021},
isbn = {9781450384957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485314.3485323},
doi = {10.1145/3485314.3485323},
abstract = {The system architecture and application services of the data center are becoming increasingly large. To ensure the stable operation of the systems and businesses carried by the data center, the operations engineer needs to collect and monitor the generating KPIs during the operation of the systems and services. Traditional KPI anomaly detection methods are faced with the challenges of the huge amount of KPIs and constantly changing data characteristics, which are gradually no longer suitable for highly dynamic systems and services. With the popularity of artificial intelligence algorithms, machine learning and deep learning methods have also begun to be applied in operation and maintenance scenarios, that is the emergence of Artificial Intelligence for IT Operations (AIOps). KPI anomaly detection is the underlying core technology of AIOps. This paper proposes a hybrid model based on GRU-GAN (GGAN) for KPI anomaly detection in data center AIOps. The Gated Recurrent Unit (GRU) network is selected as the generator and discriminator of Generative adversarial network (GAN) in this model, which get the time correlation and data distribution of KPI through the adversarial training between the generator and the discriminator to make use of the reconstruction ability of the generator and the discriminant ability of the discriminator at the same time. At the anomaly detection stage, the anomaly score is formed by integrating reconstruction difference and discrimination loss to complete the anomaly detection task. Experimental results show that the proposed method can more accurately capture the variable data characteristics of KPI compared with the traditional KPI anomaly detection method and the general unsupervised method, as well as achieve better performance in the KPI anomaly detection task.},
booktitle = {2021 10th International Conference on Internet Computing for Science and Engineering},
pages = {23–29},
numpages = {7},
keywords = {AIOps, GAN, Data Center, GRU, KPI Anomaly Detection},
location = {Guilin, China},
series = {ICICSE 2021}
}

@inproceedings{10.1145/3465336.3475107,
author = {Nurmikko-Fuller, Terhi and Pickering, Paul},
title = {Reductio Ad Absurdum?: From Analogue Hypertext to Digital Humanities},
year = {2021},
isbn = {9781450385510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465336.3475107},
doi = {10.1145/3465336.3475107},
abstract = {In this paper we report on a complex and complete archive of historical primary sources that map the political landscape of the anglophone world in the mid-to late 1800s. The ruthless pragmatism applied to the construction of the initial Humanities dataset resulted in an analogue equivalent of a hypertext system, which has already resulted in published academic books and articles. Here, we describe the processes of a current project, which consists of the translation of this analogue information aggregation system into a graph database using Linked Data and semantic Web technologies.},
booktitle = {Proceedings of the 32nd ACM Conference on Hypertext and Social Media},
pages = {245–250},
numpages = {6},
keywords = {political history, australian history, linked data, hypertext, information aggregation},
location = {Virtual Event, USA},
series = {HT '21}
}

@inproceedings{10.1145/3085228.3085283,
author = {Brolch\'{a}in, Niall \'{O} and Porwol, Lukasz and Ojo, Adegboyega and Wagner, Tilman and Lopez, Eva Tamara and Karstens, Eric},
title = {Extending Open Data Platforms with Storytelling Features},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085283},
doi = {10.1145/3085228.3085283},
abstract = {1Research into Data-Driven Storytelling using Open Data has led to considerable discussion into many possible futures for storytelling and journalism in a Data-Driven world, in particular, into the Open Data directives framed by various governments across the globe as a means of facilitating governments, transparency enabled citizens and journalists to get more insights into government actions and enable deeper and easier monitoring of governments' work. While progress in the development of Open Data platforms (usually funded by national and local governments) has been significant, it is only now that we are beginning to see the emergence of more practical and more applied use of Open Data platforms. Previous works have highlighted the potential for storytelling using Open Data as a source of information for journalistic stories. Nevertheless, there is a paucity of studies into Open Data platform affordances to support Data-Driven Storytelling. In this paper, we elaborate on existing Open Data platforms in terms of support for storytelling and analyse feedback from stakeholder focus groups, to discover what methods and tools can introduce or facilitate the storytelling capabilities of Open Data platforms.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {48–53},
numpages = {6},
keywords = {Open Data, Usable Open Data Platform, Journalism, YDS Platform, Data-Driven Journalism, Data-Driven Storytelling},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@article{10.14778/3352063.3352066,
author = {Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Li, Zijue and Li, Jianzhong and Gao, Hong},
title = {Cleanits: A Data Cleaning System for Industrial Time Series},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352066},
doi = {10.14778/3352063.3352066},
abstract = {The great amount of time series generated by machines has enormous value in intelligent industry. Knowledge can be discovered from high-quality time series, and used for production optimization and anomaly detection in industry. However, the original sensors data always contain many errors. This requires a sophisticated cleaning strategy and a well-designed system for industrial data cleaning. Motivated by this, we introduce Cleanits, a system for industrial time series cleaning. It implements an integrated cleaning strategy for detecting and repairing three kinds of errors in industrial time series. We develop reliable data cleaning algorithms, considering features of both industrial time series and domain knowledge. We demonstrate Cleanits with two real datasets from power plants. The system detects and repairs multiple dirty data precisely, and improves the quality of industrial time series effectively. Cleanits has a friendly interface for users, and result visualization along with logs are available during each cleaning process.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1786–1789},
numpages = {4}
}

@inproceedings{10.1145/2721956.2721968,
author = {Leibold, Christian F. and Spies, Marcus},
title = {Towards a Pattern Language for Cognitive Systems Integration},
year = {2014},
isbn = {9781450334167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2721956.2721968},
doi = {10.1145/2721956.2721968},
abstract = {This paper discusses the influence of recent advances in cognitive computing systems on enterprise software architecture and design/development. Specifically, building on key features and capabilities of cognitive computing systems, we propose a new schema of enterprise application integration patterns in the tradition of the design pattern literature. Our schema has three groups of patterns addressing essential scoping, security and service integration issues related to cognitive components in enterprise architecture. While some patterns are modifications or refinements of known Enterprise Application Integration patterns, some of them are new and require dedicated consideration by enterprise architects and software designers.},
booktitle = {Proceedings of the 19th European Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {9},
keywords = {requirements engineering, pattern language, social and ethical impact, cognitive systems, scope identification, enterprise integration pattern},
location = {Irsee, Germany},
series = {EuroPLoP '14}
}

@inproceedings{10.1145/3093338.3104170,
author = {Pugmire, David and Bozda\u{g}, Ebru and Lefebvre, Matthieu and Tromp, Jeroen and Komatitsch, Dmitri and Peter, Daniel and Podhorszki, Norbert and Hill, Judith},
title = {Pillars of the Mantle: Imaging the Interior of the Earth with Adjoint Tomography},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3104170},
doi = {10.1145/3093338.3104170},
abstract = {In this work, we investigate global seismic tomographic models obtained by spectral-element simulations of seismic wave propagation and adjoint methods. Global crustal and mantle models are obtained based on an iterative conjugate-gradient type of optimization scheme. Forward and adjoint seismic wave propagation simulations, which result in synthetic seismic data to make measurements and data sensitivity kernels to compute gradient for model updates, respectively, are performed by the SPECFEM3D_GLOBE package [1] [2] at the Oak Ridge Leadership Computing Facility (OLCF) to study the structure of the Earth at unprecedented levels. Using advances in solver techniques that run on the GPUs on Titan at the OLCF, scientists are able to perform large-scale seismic inverse modeling and imaging. Using seismic data from global and regional networks from global CMT earthquakes, scientists are using SPECFEM3D_GLOBE to understand the structure of the mantle layer of the Earth. Visualization of the generated data sets provide an effective way to understand the computed wave perturbations which define the structure of mantle in the Earth.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {75},
numpages = {4},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

@article{10.1145/2641383.2641390,
author = {Agosti, Maristella and Fuhr, Norbert and Toms, Elaine and Vakkari, Pertti},
title = {Evaluation Methodologies in Information Retrieval Dagstuhl Seminar 13441},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/2641383.2641390},
doi = {10.1145/2641383.2641390},
journal = {SIGIR Forum},
month = {jun},
pages = {36–41},
numpages = {6}
}

@inproceedings{10.5555/3017447.3017493,
author = {Bishop, Bradley Wade and Hank, Carolyn},
title = {Data Curation Profiling of Biocollections},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {In the contexts of the data deluge and open data, scientists studying biodiversity benefit from online access to global datasets of existing vouchered biological and paleontological collections. Using biocollections collected over time across the world allows for the advancement of scientific knowledge concerning evolution in process as well as species poleward migrations, an indicator of climate change. This study's purpose was to validate and expand the Data Curation Profiles (DCP) to digital biocollections and inform a DCP framework for worldwide biota. Ten biocollection producers, curating various types of specimens affiliated with the project building the United States' national biodiversity infrastructure, were interviewed using the DCP questionnaire. Results indicate there is extreme diversity in the curation of biocollections and additional DCP questions should be added to reflect the complicated approaches to biological data curation. Although discipline specific metadata creation tools, standards, and practices enable long-term sustainability of the U.S. digitization effort, some scientists would benefit from further clarification and guidance on the information needs of consumers beyond designated communities of expert users, and the long-term preservation of biocollections.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology},
articleno = {46},
numpages = {9},
keywords = {biocollections, data curation profiles, data curation, biology, data provenance},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1145/3465481.3470055,
author = {P\"{o}hn, Daniela and Seeber, Sebastian and Hanauer, Tanja and Ziegler, Jule A. and Schmitz, David},
title = {Towards Improving Identity and Access Management with the IdMSecMan Process Framework},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470055},
doi = {10.1145/3465481.3470055},
abstract = {In today’s networks, administrative access to Linux servers is commonly managed by Privileged Access Management (PAM). It is not only important to monitor these privileged accounts, but also to control segregation of duty and detect keys as well as accounts that potentially bypass PAM. Unprohibited access can become a business risk. In order to improve the security in a controlled manner, we establish IdMSecMan, a security management process tailored for identity and access management (IAM). Security management processes typically use the Deming Cycle or an adaption for continuous improvements of products, services, or processes within the network infrastructure. We adjust a security management process with visualization for IAM, which also shifts the focus from typical assets to the attacker. With the controlled cycles, the maturity of IAM is measured and can continually advance. This paper presents and applies the work in progress IdMSecMan to a motivating scenario in the field of Linux server. We evaluate our approach in a controlled test environment with first steps to roll it out in our data center. Last but not least, we discuss challenges and future work.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {89},
numpages = {10},
keywords = {Server, Identity Management, Security Management, Security},
location = {Vienna, Austria},
series = {ARES 21}
}

@inproceedings{10.5555/3522802.3522903,
author = {Hunker, Joachim and Wuttke, Alexander and Scheidler, Anne Antonia and Rabe, Markus},
title = {A Farming-for-Mining-Framework to Gain Knowledge in Supply Chains},
year = {2021},
publisher = {IEEE Press},
abstract = {Gaining knowledge from a given data basis is a complex challenge. One of the frequently used methods in the context of a supply chain (SC) is knowledge discovery in databases (KDD). For a purposeful and successful knowledge discovery, valid and preprocessed input data are necessary. Besides preprocessing collected observational data, simulation can be used to generate a data basis as an input for the knowledge discovery process. The process of using a simulation model as a data generator is called data farming. This paper investigates the link between data farming and data mining. We developed a Farming-for-Mining-Framework, where we highlight requirements of knowledge discovery techniques and derive how the simulation model for data generation can be configured accordingly, e.g., to meet the required data accuracy. We suggest that this is a promising approach and is worth further research attention.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {130},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3340964.3340975,
author = {Xu, Jianqiu and Lu, Hua and G\"{u}ting, Ralf Hartmut},
title = {Understanding Human Mobility: A Multi-Modal and Intelligent Moving Objects Database},
year = {2019},
isbn = {9781450362801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340964.3340975},
doi = {10.1145/3340964.3340975},
abstract = {The research field of moving objects has been quite active in the past 20 years. The recording of position data becomes easy and huge amounts of mobile data are collected. Moving objects databases represent time-dependent objects and support queries with spatial and temporal constraints. In this paper we provide the vision of a multi-model and intelligent moving objects database. The goal is to enhance the data management of moving objects by providing extensive data models for different applications and fusing artificial intelligence techniques. Toward this goal, we propose how to develop corresponding modules and integrate them into the system to achieve the next-generation moving objects database.},
booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
pages = {222–225},
numpages = {4},
location = {Vienna, Austria},
series = {SSTD '19}
}

@inproceedings{10.1145/3209281.3209297,
author = {Yoon, Sang-Pil and Joo, Moon-Ho and Kwon, Hun-Yeong},
title = {Role of Law as a Guardian of the Right to Use Public Sector Information: Case Study of Korean Government},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209297},
doi = {10.1145/3209281.3209297},
abstract = {With data revolution, data is emerging as a new raw material. As the importance of data has increased, interest in the availability of public sector information (PSI) has also grown. PSI created in the public sector comprises public attributes and directly impacts national administration and citizen's lives.Korea has almost the highest level of Information and Communication Technology (ICT) infrastructure and considerable data through government-led policies. As a result of such policies, Korea has demonstrated excellent results in the United Nation's e-government survey, ITU's ICT development index, and OECD's public data openness index. Paradoxically, however, this history and experience is a stumbling block to a new era. PSI, which is a basic resource for realizing the value of openness, sharing, cooperation, and communication, should be actively managed and opened by government to provide support for reuse in the sense that the government is its main producer and manager. However, no matter how good the quality of PSI through data management is and how excellent policies and institutions are established, if the private sector cannot actively use it, it is useless. What is the role of government and law in the context of changing the way data is managed and blurring sectoral boundaries?This paper aims to propose core challenges by analyzing the case of Korea in order to derive a basis for discussions to coordinate public and private cooperation and legal relations in the process. To begin with, we analyze the changes in the management environment of data and PSI and identify the role of government and law in responding to changes in the legal rights. Then, we discuss how Korea responds to change, examines related policies by function and discussions on the data law, which seem to have the greatest effect on government's role, and suggests essential tasks to change its role accordingly.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {83},
numpages = {10},
keywords = {reuse of PSI, openness, public sector information, data management, legal right management, public-private cooperation, the right to know, public data},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@article{10.14778/3484224.3484233,
author = {Ammerlaan, Remmelt and Antonius, Gilbert and Friedman, Marc and Hossain, H M Sajjad and Jindal, Alekh and Orenberg, Peter and Patel, Hiren and Qiao, Shi and Ramani, Vijay and Rosenblatt, Lucas and Roy, Abhishek and Shaffer, Irene and Srinivasan, Soundarajan and Weimer, Markus},
title = {PerfGuard: Deploying ML-for-Systems without Performance Regressions, Almost!},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484233},
doi = {10.14778/3484224.3484233},
abstract = {Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called PerfGuard, that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3362–3375},
numpages = {14}
}

@inproceedings{10.1145/3303772.3303821,
author = {Mitra, Ritayan and Chavan, Pankaj},
title = {DEBE Feedback for Large Lecture Classroom Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303821},
doi = {10.1145/3303772.3303821},
abstract = {Learning Analytics (LA) research has demonstrated the potential of LA in detecting and monitoring cognitive-affective parameters and improving student success. But most of it has been applied to online and computerized learning environments whereas physical classrooms have largely remained outside the scope of such research. This paper attempts to bridge that gap by proposing a student feedback model in which they report on the difficult/easy and engaging/boring aspects of their lecture. We outline the pedagogical affordances of an aggregated time-series of such data and discuss it within the context of LA research.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {426–430},
numpages = {5},
keywords = {learning analytics, live feedback, Large lectures, quantified self, mobile application},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2912160.2912206,
author = {Li, Hongqin and Zhai, Jun},
title = {Constructing Investment Open Data of Chinese Listed Companies Based on Linked Data},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912206},
doi = {10.1145/2912160.2912206},
abstract = {Linked Data can provide the data according to user's demand, promote the availability of half structured and unstructured data on the network, improve the interoperability of open data. With the development of Linked Data, Linked Enterprise Data becomes a research hotspot. This article draw lessons from foreign investment Linked Data research of listed companies, selected the listed company information and XBRL reports from Shanghai stock exchange and Shenzhen stock exchange, industry information from the China securities regulatory commission and daily stock price from Flush as data source, built investment open data of Chinese listed companies based on Linked Data. This work could promote the internationalization of the Chinese data and the commercial use of open government data, lay the foundation for the global data ecological system, at the same time prepare for the challenge of Shanghai-Hong Kong Stock Connect and Shenzhen-Hong Kong Stock Connect even the challenge of international investment.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {475–480},
numpages = {6},
keywords = {Linked Enterprise Data, Linked Data, XBRL, Open Data},
location = {Shanghai, China},
series = {dg.o '16}
}

@inproceedings{10.1145/3487075.3487147,
author = {Zhang, Jiapeng and Zhuang, Cunbo and Liu, Jianhua and Yuan, Kun and Zhang, Jin and Liu, Juan},
title = {Digital Twin-Based Three-Dimensional Visual and Global Monitoring of Assembly Shop-Floor},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487147},
doi = {10.1145/3487075.3487147},
abstract = {Aiming at the requirements of rapid response and production efficiency improvement in the assembly shop-floor, a three-dimensional (3D) visual and global monitoring method for the assembly shop-floor based on the digital twin is proposed. The paper analyzes the monitoring objectives, objects, and methods of assembly shop-floor, and constructs a global monitoring framework of digital twin-based assembly shop-floor. Then, three key technologies of realizing global monitoring shop-floor are described: current-time data perception and collection, current-time information-driven digital twin generation, and state monitoring and optimization based on digital twin. Finally, a global monitoring prototype system is designed and developed to verify the effectiveness of the proposed method.},
booktitle = {The 5th International Conference on Computer Science and Application Engineering},
articleno = {72},
numpages = {7},
keywords = {Digital twin, Global monitoring, 3D visual monitoring, Assembly shop-floor},
location = {Sanya, China},
series = {CSAE 2021}
}

@inproceedings{10.1145/3316782.3322785,
author = {Haescher, Marian and Matthies, Denys J. C. and Krause, Silvio and Bieber, Gerald},
title = {Presenting a Data Imputation Concept to Support the Continuous Assessment of Human Vital Data and Activities},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3322785},
doi = {10.1145/3316782.3322785},
abstract = {Data acquisition of mobile tracking devices often suffers from invalid and non-continuous input data streams. This issue especially occurs with current wearables tracking the user's activity and vital data. Typical reasons include the short battery life and the fact that the body-worn tracking device may be doffed. Other reasons, such as technical issues, can corrupt the data and render it unusable. In this paper, we introduce a data imputation concept which complements and thus fixes incomplete datasets by using a new merging approach that is particularly suitable for assessing activities and vital data. Our technique enables the dataset to become coherent and comprehensive so that it is ready for further analysis. In contrast to previous approaches, our technique enables the controlled creation of continuous data sets that also contain information on the level of uncertainty for possible reconversions, approximations, or later analysis.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {587–592},
numpages = {6},
keywords = {controlled data creation, data fusion, mobile device, data imputation, sensor fusion, accelerometer, smartwatch, coherent database},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/3512826.3512840,
author = {Yang, Xiaohui and Guo, Chenxi and Ren, Huan and Dong, Ming},
title = {Research on Anomaly Detection Method of Online Monitoring Data of Dissolved Gas in Transformer Oil},
year = {2022},
isbn = {9781450395489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512826.3512840},
doi = {10.1145/3512826.3512840},
abstract = {Dissolved gases in oil analysis has been a significant conventional condition detection method for condition evaluation for power transformers. But false data and wrong data do exist in DGA on-line monitoring system, which often lead to misjudgment. To handle this problem, the monitoring system often uses a threshold method based on data distribution statistics to determine the authenticity of the data. However, it is difficult to grasp the rules of the data distribution in advance, resulting in the problem of generally low detection rate of abnormal data. In this paper, according to the time series characteristics of on-line monitoring data of DGA, an abnormal data detection method based on condensed hierarchical clustering is proposed. First, the sliding time window is used to preprocess a variety of oil gas monitoring data to obtain a time series set of monitoring data, and comprehensively apply statistical indicators to classify them and establish Typical time series map; on this basis, the agglomerated hierarchical clustering model is used to perform similarity clustering on the distance between different characteristic data points and the typical abnormal map to determine the abnormal type of the monitoring data. The verification of the application of actual monitoring data shows that this method can detect data anomalies in the online monitoring data stream and determine its type in real time.},
booktitle = {2022 The 3rd International Conference on Artificial Intelligence in Electronics Engineering},
pages = {65–69},
numpages = {5},
keywords = {Anomaly detection, Hierarchical agglomerative cluster, Dissolved gases in oil, Time series, On-line monitoring},
location = {Bangkok, Thailand},
series = {AIEE 2022}
}

@article{10.1145/3349629,
author = {Verma, Neeta and Dawar, Savita},
title = {Digital Transformation in the Indian Government},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3349629},
doi = {10.1145/3349629},
journal = {Commun. ACM},
month = {oct},
pages = {50–53},
numpages = {4}
}

@article{10.1145/2590989.2591002,
author = {Pedersen, Torben Bach and Lehner, Wolfgang},
title = {Report on the Second International Workshop on Energy Data Management (EnDM 2013)},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2590989.2591002},
doi = {10.1145/2590989.2591002},
journal = {SIGMOD Rec.},
month = {feb},
pages = {70–72},
numpages = {3}
}

@article{10.1145/3503780.3503792,
author = {Kondylakis, Haridimos and Stefanidis, Kostas and Rao, Praveen},
title = {Report on the Third International Workshop on Semantic Web Meets Health Data Management (SWH 2020)},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3503780.3503792},
doi = {10.1145/3503780.3503792},
abstract = {Creating a holistic view of patient data comes with many challenges but also brings many benefits for disease prediction, prevention, diagnosis, and treatment. Especially in the COVID-19 era, this is more important than ever before. The third International Workshop on Semantic Web Meets Health Data Management (SWH) was aimed at bringing together an interdisciplinary audience who was interested in the fields of Semantic Web, data management, and health informatics. The workshop goal was to discuss the challenges in healthcare data management and to propose new solutions for the next generation of data-driven healthcare systems. In this article, we summarize the outcomes of the workshop, and we present a number of key observations and research directions that emerged from presentations.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {32–35},
numpages = {4}
}

@inproceedings{10.1145/2912160.2912179,
author = {Hu, Yanhua and Bai, Xianyang and Sun, Shuyang},
title = {Readiness Assessment of Open Government Data Programs: A Case of Shenzhen},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912179},
doi = {10.1145/2912160.2912179},
abstract = {More and more cities in China are implementing various open government data initiatives for improving their governance. Little research, however, has been done in evaluating the readiness of individual governments in pursuing such initiatives. This paper presents a case study of the readiness assessment on the adoption of open data programs in Shenzhen based on the open data readiness assessment framework of the World Bank. The result shows that there are several issues including developing an action plan, providing privacy and ownership solutions, designating a unified administration, and implementing consistent data management policies and standards that need to be adequately addressed for the effective adoption of the open data program in the city.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {97–103},
numpages = {7},
keywords = {Readiness assessment, Open government data, Open data},
location = {Shanghai, China},
series = {dg.o '16}
}

@inproceedings{10.1145/2851613.2851757,
author = {Vilela, J\'{e}ssyka and Gon\c{c}alves, Enyo and Holanda, Ana and Figueiredo, Bruno and Castro, Jaelson},
title = {Retrospective, Relevance, and Trends of SAC Requirements Engineering Track},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851757},
doi = {10.1145/2851613.2851757},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: This study aims to investigate how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 79 papers over the 8 previous SAC RE-Track editions, which were analyzed and discussed.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1264–1269},
numpages = {6},
keywords = {requirements engineering, trends, SAC, symposium on applied computing, scoping study, relevance, systematic mapping study, retrospective},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3414274.3414509,
author = {Liu, Xingchen and Zhao, Boyu and Qian, Haotian and Liu, Yuhang},
title = {Multidimensional Data Mining on the Early Scientific Talents of China},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414509},
doi = {10.1145/3414274.3414509},
abstract = {The cultivation of high-level talents in either scientific or engineering domain is of significant importance to the development of a country. From the perspective of data science, this paper takes the group of academicians of the Chinese Academy of Sciences and the Chinese Academy of Engineering as an example to explore the method of cultivating high-end talents. Through multidimensional data analysis, it is of great significance to explore the spatial pattern and time evolution characteristics of the first generation of top natural science talents in China, to deepen the understanding of the growth and education laws of talents, optimize top-level design, and implement targeted scientific policies. It is found that Chinese culture and Chinese native universities have made significant contributions for the early scientific talents of China, and the analysis method of data science can be used to facilitate the education innovation.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {239–246},
numpages = {8},
keywords = {Education, Cultivation of high-level talents, Data-mining},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/2935663.2935664,
author = {Zhu, Chengang and Cheng, Guang and Guo, Xiaojun and Wang, Yuxiang},
title = {RBAS: A Real-Time User Behavior Analysis System for Internet TV in Cloud Computing},
year = {2016},
isbn = {9781450341813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935663.2935664},
doi = {10.1145/2935663.2935664},
abstract = {The characteristic of Internet TV user behavior is quite essential for designers to optimize resource schedule and improve user experience. With the rapid development of Internet, both Internet TV users and STB (set top boxes) models are booming. This brings a large amount of behavior data which requires matching computing and storage resource to process. Therefore, scalable Internet TV user behavior analysis becomes more difficult. As a solution, cloud computing framework such as Hive is emerged. But limited by performance, it's not an appropriate choice for interactive analysis or real-time data exploration. In this paper, we present a real-time Internet TV user behavior analysis system with advantages of high concurrency, low latency and good transportability. Firstly, we design an event capture scheme, consisted of agents embedded in STBs and capture server clusters, to capture every manipulation performed by users. Secondly, we develop a SQL-on-Hadoop engine with distributed transactional management to decrease the response time. The engine has excellent query performance and ability to interactively query various data sources in different Hadoop formats. Lastly, we evaluate RBAS in a commercial Internet TV platform of 16 million registered users. The results show that, with a 32-node cluster, the system can effectively process 10.2 TB of behavior data every day, which is about 40x faster than original Hive-based system.},
booktitle = {Proceedings of the 11th International Conference on Future Internet Technologies},
pages = {36–42},
numpages = {7},
keywords = {SQL-on-Hadoop, User behavior analysis, cloud computing, Internet TV},
location = {Nanjing, China},
series = {CFI '16}
}

@inproceedings{10.1145/3211954.3211955,
author = {Seabolt, Ed and Kandogan, Eser and Roth, Mary},
title = {Contextual Intelligence for Unified Data Governance},
year = {2018},
isbn = {9781450358514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211954.3211955},
doi = {10.1145/3211954.3211955},
abstract = {Current data governance techniques are very labor-intensive, as teams of data stewards typically rely on best practices to transform business policies into governance rules. As data plays an increasingly key role in today's data-driven enterprises, current approaches do not scale to the complexity and variety present in the data ecosystem of an enterprise as an increasing number of data requirements, use cases, applications, tools and systems come into play. We believe techniques from artificial intelligence and machine learning have potential to improve discoverability, quality and compliance in data governance. In this paper, we propose a framework for 'contextual intelligence', where we argue for (1) collecting and integrating contextual metadata from variety of sources to establish a trusted unified repository of contextual data use across users and applications, and (2) applying machine learning and artificial intelligence techniques over this rich contextual metadata to improve discoverability, quality and compliance in governance practices. We propose an architecture that unifies governance across several systems, with a graph serving as a core repository of contextual metadata, accurately representing data usage across the enterprise and facilitating machine learning, We demonstrate how our approach can enable ML-based recommendations in support of governance best practices.},
booktitle = {Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {2},
numpages = {9},
keywords = {Graph, Analytics, Data Governance, Context},
location = {Houston, TX, USA},
series = {aiDM'18}
}

@inproceedings{10.1145/3512850.3512861,
author = {Mei, Songzhu and Liu, Cong and Wang, Qinglin and Su, Huayou},
title = {Model Provenance Management in MLOps Pipeline},
year = {2022},
isbn = {9781450395717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512850.3512861},
doi = {10.1145/3512850.3512861},
booktitle = {2022 The 8th International Conference on Computing and Data Engineering},
pages = {45–50},
numpages = {6},
keywords = {Machine learning engineering, Model provenance, MLOps, Artificial Intelligence},
location = {Bangkok, Thailand},
series = {ICCDE 2022}
}

@article{10.1145/3131780,
author = {Lukyanenko, Roman and Samuel, Binny M.},
title = {Are All Classes Created Equal? Increasing Precision of Conceptual Modeling Grammars},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3131780},
doi = {10.1145/3131780},
abstract = {Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {sep},
articleno = {14},
numpages = {15},
keywords = {Conceptual modeling, database design}
}

@inproceedings{10.1145/3284103.3284114,
author = {Wang, Rongxiao and Chen, Bin and Wang, Yiduo and Zhu, Zhengqiu and Ma, Liang and Qiu, Xiaogang},
title = {The Air Contaminant Dispersion Prediction by the Integration of the Neural Network and AermodSystem},
year = {2018},
isbn = {9781450360449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284103.3284114},
doi = {10.1145/3284103.3284114},
abstract = {Air pollution caused by industrial production has become a serious problem for public health. This challenging problem promotes the development of the research in the air contaminant dispersion (ADS) prediction, for the management of the emission and leak accident. However, conventional ADS models can hardly meet the requirement of both accuracy and efficiency. The data model, like the artificial neural network (ANN) provides a feasible way of forecasting the dispersion with high accuracy and efficiency. However, the construction of the ANN for prediction needs plenty of data, which is impractical to obtain in most emission cases. To address this problem, an ADS simulation software AermodSystem is applied to build the simulated dispersion scenarios and provide synthetic dataset for the model training and test. Based on the synthetic data set, the ANN prediction model is established, and evaluated on the test set, as well as the Gaussian model. Further, these two models are served as the forward dispersion model and combined with the Particle Swarm Optimization (PSO) for source estimation. The results verify the effectiveness of the proposed model and indicate that the ANN together with the AermodSystem as the data generator is feasible in the air contaminant dispersion forecast and the source estimation of a particular case.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience},
articleno = {11},
numpages = {5},
keywords = {Particle Swarm Optimization, Artificial neural network, AermodSystem, Source estimation, Atmospheric dispersion prediction},
location = {Seattle, WA, USA},
series = {Safety and Resilience'18}
}

@article{10.1145/3446679,
author = {Celes, Clayson and Boukerche, Azzedine and Loureiro, Antonio A. F.},
title = {Mobility Trace Analysis for Intelligent Vehicular Networks: Methods, Models, and Applications},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446679},
doi = {10.1145/3446679},
abstract = {Intelligent vehicular networks emerge as a promising technology to provide efficient data communication in transportation systems and smart cities. At the same time, the popularization of devices with attached sensors has allowed the obtaining of a large volume of data with spatiotemporal information from different entities. In this sense, we are faced with a large volume of vehicular mobility traces being recorded. Those traces provide unprecedented opportunities to understand the dynamics of vehicular mobility and provide data-driven solutions. In this article, we give an overview of the main publicly available vehicular mobility traces; then, we present the main issues for preprocessing these traces. Also, we present the methods used to characterize and model mobility data. Finally, we review existing proposals that apply the hidden knowledge extracted from the mobility trace for vehicular networks. This article provides a survey on studies that use vehicular mobility traces and provides a guideline for the proposition of data-driven solutions in the domain of vehicular networks. Moreover, we discuss open research problems and give some directions to undertake them.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {49},
numpages = {38},
keywords = {routing, mobility, vanet, survey, Vehicular networks, data analysis, data mining, topology}
}

@inproceedings{10.1109/MSR.2019.00031,
author = {Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and Mockus, Audris},
title = {World of Code: An Infrastructure for Mining the Universe of Open Source VCS Data},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00031},
doi = {10.1109/MSR.2019.00031},
abstract = {Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {143–154},
numpages = {12},
keywords = {software supply chain, software mining, software ecosystem},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.1145/3488717,
author = {Stoyanovich, Julia and Abiteboul, Serge and Howe, Bill and Jagadish, H. V. and Schelter, Sebastian},
title = {Responsible Data Management},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3488717},
doi = {10.1145/3488717},
abstract = {Perspectives on the role and responsibility of the data-management research community in designing, developing, using, and overseeing automated decision systems.},
journal = {Commun. ACM},
month = {may},
pages = {64–74},
numpages = {11}
}

@article{10.1145/1462571.1462573,
author = {Agrawal, Rakesh and Ailamaki, Anastasia and Bernstein, Philip A. and Brewer, Eric A. and Carey, Michael J. and Chaudhuri, Surajit and Doan, AnHai and Florescu, Daniela and Franklin, Michael J. and Garcia-Molina, Hector and Gehrke, Johannes and Gruenwald, Le and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Korth, Hank F. and Kossmann, Donald and Madden, Samuel and Magoulas, Roger and Ooi, Beng Chin and O'Reilly, Tim and Ramakrishnan, Raghu and Sarawagi, Sunita and Stonebraker, Michael and Szalay, Alexander S. and Weikum, Gerhard},
title = {The Claremont Report on Database Research},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/1462571.1462573},
doi = {10.1145/1462571.1462573},
abstract = {In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.},
journal = {SIGMOD Rec.},
month = {sep},
pages = {9–19},
numpages = {11}
}

@inproceedings{10.1145/3510003.3510057,
author = {Biswas, Sumon and Wardat, Mohammad and Rajan, Hridesh},
title = {The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines in Theory, in-the-Small, and in-the-Large},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510057},
doi = {10.1145/3510003.3510057},
abstract = {Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large. Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2091–2103},
numpages = {13},
keywords = {data science processes, data science pipelines, descriptive, predictive},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3458027,
author = {Bromander, Siri and Swimmer, Morton and Muller, Lilly Pijnenburg and J\o{}sang, Audun and Eian, Martin and Skj\o{}tskift, Geir and Borg, Fredrik},
title = {Investigating Sharing of Cyber Threat Intelligence and Proposing A New Data Model for Enabling Automation in Knowledge Representation and Exchange},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2692-1626},
url = {https://doi.org/10.1145/3458027},
doi = {10.1145/3458027},
abstract = {For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.},
journal = {Digital Threats},
month = {oct},
articleno = {6},
numpages = {22},
keywords = {ontology, Cyber threat intelligence, security, knowledge graph}
}

@inproceedings{10.1145/3368089.3417039,
author = {Nguyen-Duc, Anh and Abrahamsson, Pekka},
title = {Continuous Experimentation on Artificial Intelligence Software: A Research Agenda},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417039},
doi = {10.1145/3368089.3417039},
abstract = {Moving from experiments to industrial level AI software development requires a shift from understanding AI/ ML model attributes as a standalone experiment to know-how integrating and operating AI models in a large-scale software system. It is a growing demand for adopting state-of-the-art software engineering paradigms into AI development, so that the development efforts can be aligned with business strategies in a lean and fast-paced manner. We describe AI development as an “unknown unknown” problem where both business needs and AI models evolve over time. We describe a holistic view of an iterative, continuous approach to develop industrial AI software basing on business goals, requirements and Minimum Viable Products. From this, five areas of challenges are presented with the focus on experimentation. In the end, we propose a research agenda with seven questions for future studies.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1513–1516},
numpages = {4},
keywords = {Continuous Experimentation, AI software, Industrial Artificial Intelligence, AI system},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2345396.2345413,
author = {Mehta, R. Vasanth Kumar and Sankarasubramaniam, B. and Rajalakshmi, S.},
title = {An Algorithm for Fuzzy-Based Sentence-Level Document Clustering for Micro-Level Contradiction Analysis},
year = {2012},
isbn = {9781450311960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345396.2345413},
doi = {10.1145/2345396.2345413},
abstract = {Contradiction Analysis is one of the popular text-mining operations in which a document whose content is contradictory to the theme of a set of documents is identified. It is a means to identifying Outlier documents that do not confirm to the overall sense conveyed by other documents. Most of the existing techniques perform document-level comparisons, ignoring the sentence-level semantics, often leading to loss of vital information. Applications in domains like Defence and Healthcare require high levels of accuracy and identification of micro-level contradictions are vital. In this paper, we propose an algorithm for identifying contradictory documents using sentence-level clustering technique along with an optimization feature. A novel visualization scheme is also suggested to present the results to an end-user.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
pages = {102–105},
numpages = {4},
keywords = {information-retrieval, contradiction analysis, document clustering},
location = {Chennai, India},
series = {ICACCI '12}
}

@inproceedings{10.1145/3486635.3491073,
author = {Rao, Jinmeng and Gao, Song and Zhu, Xiaojin},
title = {VTSV: A Privacy-Preserving Vehicle Trajectory Simulation and Visualization Platform Using Deep Reinforcement Learning},
year = {2021},
isbn = {9781450391207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486635.3491073},
doi = {10.1145/3486635.3491073},
abstract = {Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {43–46},
numpages = {4},
keywords = {transportation, vehicle trajectory, privacy protection, data visualization, reinforcement learning},
location = {Beijing, China},
series = {GEOAI '21}
}

@article{10.1109/TNET.2019.2944984,
author = {Wu, Haiqin and Wang, Liangmin and Xue, Guoliang and Tang, Jian and Yang, Dejun},
title = {Enabling Data Trustworthiness and User Privacy in Mobile Crowdsensing},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2944984},
doi = {10.1109/TNET.2019.2944984},
abstract = {Ubiquitous mobile devices with rich sensors and advanced communication capabilities have given rise to mobile crowdsensing systems. The diverse reliabilities of mobile users and the openness of sensing paradigms raise concerns for data trustworthiness, user privacy, and incentive provision. Instead of considering these issues as isolated modules in most existing researches, we comprehensively capture both conflict and inner-relationship among them. In this paper, we propose a holistic solution for trustworthy and privacy-aware mobile crowdsensing with no need of a trusted third party. Specifically, leveraging cryptographic technologies, we devise a series of protocols to enable benign users to request tasks, contribute their data, and earn rewards anonymously without any data linkability. Meanwhile, an anonymous trust/reputation model is seamlessly integrated into our scheme, which acts as reference for our fair incentive design, and provides evidence to detect malicious users who degrade the data trustworthiness. Particularly, we first propose the idea of limiting the number of issued pseudonyms which serves to efficiently tackle the anonymity abuse issue. Security analysis demonstrates that our proposed scheme achieves stronger security with resilience against possible collusion attacks. Extensive simulations are presented which demonstrate the efficiency and practicality of our scheme.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {2294–2307},
numpages = {14}
}

@inproceedings{10.1145/3170427.3170632,
author = {Thelisson, Eva and Sharma, Kshitij and Salam, Hanan and Dignum, Virginia},
title = {The General Data Protection Regulation: An Opportunity for the HCI Community?},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3170632},
doi = {10.1145/3170427.3170632},
abstract = {With HCI, researchers conduct studies in interdisciplinary projects involving massive volume of data, artificial intelligence and machine learning capabilities. Awareness of the responsibility is emerging as a key concern for the HCI community.This Community will be impacted by the General Data Protection Regulation (GDPR) [5], that will enter into force on the 25th of May 2018. From that date, each data controller and data processor will face an increase of its legal obligations (in particular its accountability) under certain conditions.The GDPR encourages the adoption of Soft Law mechanisms, approved by the national competent authority on data protection, to demonstrate the compliance to the Regulation. Approved Guidelines, Codes of Conducts, Labeling, Marks and Seals dedicated to data protection, as well as certification mechanisms are some of the options proposed by the GDPR.There may be discrepancies between the realities of HCI fieldwork and the formal process of obtaining Soft Law approval by Competent Authorities dedicated to data protection. Given these issues, it is important for researchers to reflect on legal and ethical encounters in HCI research as a community.This workshop will provide a forum for researchers to share experiences about Soft Law they have put in place to increase Trust, Transparency and Accountability among the shareholders. These discussions will be used to develop a white paper of practical Soft Law mechanisms (certification, labeling, marks, seals...) emerging in HCI research with the aim to demonstrate that the GDPR may be an opportunity for the HCI community.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {responsible innovation, quality standards, privacy, labeling, design, soft law, codes of conduct},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.5555/2346696.2346710,
author = {Wang, BingQiang and See, Simon},
title = {Accelerate High Throughput Analysis for Genome Sequencing with GPU},
year = {2012},
isbn = {9781450316446},
publisher = {A*STAR Computational Resource Centre},
address = {SGP},
booktitle = {Proceedings of the ATIP/A*CRC Workshop on Accelerator Technologies for High-Performance Computing: Does Asia Lead the Way?},
articleno = {11},
numpages = {46},
location = {Buona Vista, Singapore},
series = {ATIP '12}
}

@inproceedings{10.1145/3207677.3277983,
author = {Li, Pei and Dai, Chaofan and Wang, Wenqian},
title = {Inconsistent Data Detection Based on Maximum Dependency Set},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3277983},
doi = {10.1145/3207677.3277983},
abstract = {For1 the incomplete detection of inconsistent data by conditional functional dependency(CFDs), this paper proposes a dependency lifting algorithm (DLA) based on maximum dependency set (MDS), which detects inconsistent data in data set by acquiring recessive conditional functional dependencies (RCFDs) in CFDs. Presenting the dynamic domain adjustment, setting forward and backward pointers of numerical change to improve the enumeration process in original algorithm, the applicability of the algorithm to the continuous attributes is raised too. Then, this paper provides the algorithm flow and pseudo code of dynamic domain adjustment and the DLA, analyses the convergence and time complexity of them. Finally, the validity of the DLA is verified by comparing the detection accuracy and time-cost.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {63},
numpages = {8},
keywords = {inconsistent data, dynamic domain adjustment, maximum dependency set (MDS), Conditional functional dependency (CFDs)},
location = {Hohhot, China},
series = {CSAE '18}
}

@article{10.1145/2590989.2590995,
author = {Naumann, Felix},
title = {Data Profiling Revisited},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2590989.2590995},
doi = {10.1145/2590989.2590995},
abstract = {Data profiling comprises a broad range of methods to efficiently analyze a given data set. In a typical scenario, which mirrors the capabilities of commercial data profiling tools, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, keys and foreign keys, and occasionally functional dependencies and association rules. Individual research projects have proposed several additional profiling tasks, such as the discovery of inclusion dependencies or conditional functional dependencies.Data profiling deserves a fresh look for two reasons: First, the area itself is neither established nor defined in any principled way, despite significant research activity on individual parts in the past. Second, more and more data beyond the traditional relational databases are being created and beg to be profiled. The article proposes new research directions and challenges, including interactive and incremental profiling and profiling heterogeneous and non-relational data.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {40–49},
numpages = {10}
}

@article{10.1145/2756547,
author = {Shekhar, Shashi and Feiner, Steven K. and Aref, Walid G.},
title = {Spatial Computing},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/2756547},
doi = {10.1145/2756547},
abstract = {Knowing where you are in space and time promises a deeper understanding of neighbors, ecosystems, and the environment.},
journal = {Commun. ACM},
month = {dec},
pages = {72–81},
numpages = {10}
}

@inproceedings{10.1145/3328519.3329133,
author = {Rezig, El Kindi and Ouzzani, Mourad and Elmagarmid, Ahmed K. and Aref, Walid G. and Stonebraker, Michael},
title = {Towards an End-to-End Human-Centric Data Cleaning Framework},
year = {2019},
isbn = {9781450367912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328519.3329133},
doi = {10.1145/3328519.3329133},
abstract = {Data Cleaning refers to the process of detecting and fixing errors in the data. Human involvement is instrumental at several stages of this process such as providing rules or validating computed repairs. There is a plethora of data cleaning algorithms addressing a wide range of data errors (e.g., detecting duplicates, violations of integrity constraints, and missing values). Many of these algorithms involve a human in the loop, however, this latter is usually coupled to the underlying cleaning algorithms. In a real data cleaning pipeline, several data cleaning operations are performed using different tools. A high-level reasoning on these tools, when combined to repair the data, has the potential to unlock useful use cases to involve humans in the cleaning process. Additionally, we believe there is an opportunity to benefit from recent advances in active learning methods to minimize the effort humans have to spend to verify data items produced by tools or humans. There is currently no end-to-end data cleaning framework that systematically involves humans in the cleaning pipeline regardless of the underlying cleaning algorithms. In this paper, we present opportunities that this framework could offer, and highlight key challenges that need to be addressed to realize this vision. We present a design vision and discuss scenarios that motivate the need for this framework to judiciously assist humans in the cleaning process.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {1},
numpages = {7},
location = {Amsterdam, Netherlands},
series = {HILDA'19}
}

@inproceedings{10.1145/3381991.3395603,
author = {Momen, Nurul and Bock, Sven and Fritsch, Lothar},
title = {Accept - Maybe - Decline: Introducing Partial Consent for the Permission-Based Access Control Model of Android},
year = {2020},
isbn = {9781450375689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3381991.3395603},
doi = {10.1145/3381991.3395603},
abstract = {The consent to personal data sharing is an integral part of modern access control models on smart devices. This paper examines the possibility of registering conditional consent which could potentially increase trust in data sharing. We introduce an indecisive state of consenting to policies that will enable consumers to evaluate data services before fully committing to their data sharing policies. We address technical, regulatory, social, individual and economic perspectives for inclusion of partial consent within an access control mechanism. Then, we look into the possibilities to integrate it within the access control model of Android by introducing an additional button in the interface--Maybe. This article also presents a design for such implementation and demonstrates feasibility by showcasing a prototype built on Android platform. Our effort is exploratory and aims to shed light on the probable research direction.},
booktitle = {Proceedings of the 25th ACM Symposium on Access Control Models and Technologies},
pages = {71–80},
numpages = {10},
keywords = {access control, partial consent, data protection, privacy},
location = {Barcelona, Spain},
series = {SACMAT '20}
}

@article{10.1145/3552490.3552504,
author = {Fekete, Alan D. and R\"{o}hm, Uwe},
title = {Teaching about Data and Databases: Why, What, How?},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3552490.3552504},
doi = {10.1145/3552490.3552504},
abstract = {The panel on data(base) education at VLDB2021 [13] drew attention to important challenges in choosing how database classes are constructed for students in a world where data is being used in novel and impactful settings. This paper aims to present one view of a process for making these pedagogy decisions. We don't aim to present a best-possible design of the subject, rather we want to illuminate the space of possibilities, to encourage reasoned choices rather than simply teaching the subject as it was previously offered, or spending time on the latest innovations without considering the "opportunity cost" of doing so. We hope to guide the perplexed instructor or departmental curriculum committee.},
journal = {SIGMOD Rec.},
month = {jul},
pages = {52–60},
numpages = {9}
}

@inproceedings{10.1145/2729104.2729129,
author = {Lipuntsov, Yuri P.},
title = {Three Types of Data Exchange in the Open Government Information Projects},
year = {2014},
isbn = {9781450334013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729104.2729129},
doi = {10.1145/2729104.2729129},
abstract = {Open government as a new system of public administration requires a qualitatively new level of information support for digital interactions of agencies, as well as between government and citizens, experts, and businesses. This article examines three categories of government counterparties, interactions with which are based on different principles: community of interest; subject areas; a loosely coupled environment. The public sector has now many information projects, and most of them deal with the data exchange, data delivery from and to the environment. The understanding of an environment is various for different projects. The categorization of projects depends on nature and the level of intellectual interaction with the external environment needed for the correct definition of project objectives, assessment of effectiveness. With the growing number of users and the transition to an open world, semantic principles are becoming more significant. Given the shift from systems integration to the semantic method, the role of subject matter expert is growing substantially.},
booktitle = {Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {88–94},
numpages = {7},
keywords = {Linked Data, Open Data, Core Component, Open Government, Data Standardization, Shared Environment},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '14}
}

@inproceedings{10.1145/3308558.3313683,
author = {Altenburger, Kristen M. and Ho, Daniel E.},
title = {Is Yelp Actually Cleaning Up the Restaurant Industry? A Re-Analysis on the Relative Usefulness of Consumer Reviews},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313683},
doi = {10.1145/3308558.3313683},
abstract = {Social media provides the government with novel methods to improve regulation. One leading case has been the use of Yelp reviews to target food safety inspections. While previous research on data from Seattle finds that Yelp reviews can predict unhygienic establishments, we provide a more cautionary perspective. First, we show that prior results are sensitive to what we call “Extreme Imbalanced Sampling”: extreme because the dataset was restricted from roughly 13k inspections to a sample of only 612 inspections with only extremely high or low inspection scores, and imbalanced by not accounting for class imbalance in the population. We show that extreme imbalanced sampling is responsible for claims about the power of Yelp information in the original classification setup. Second, a re-analysis that utilizes the full dataset of 13k inspections and models the full inspection score (regression instead of classification) shows that (a) Yelp information has lower predictive power than prior inspection history and (b) Yelp reviews do not significantly improve predictions, given existing information about restaurants and inspection history. Contrary to prior claims, Yelp reviews do not appear to aid regulatory targeting. Third, this case study highlights critical issues when using social media for predictive models in governance and corroborates recent calls for greater transparency and reproducibility in machine learning.},
booktitle = {The World Wide Web Conference},
pages = {2543–2550},
numpages = {8},
keywords = {replication, Yelp, consumer reviews, food safety, regulation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{10.1145/3398020,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398020},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {82},
numpages = {47},
keywords = {deep learning, IoT, orchestration, machine learning}
}

@inproceedings{10.1145/3424771.3424821,
author = {Zimmermann, Olaf and Pautasso, Cesare and L\"{u}bke, Daniel and Zdun, Uwe and Stocker, Mirko},
title = {Data-Oriented Interface Responsibility Patterns: Types of Information Holder Resources},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424821},
doi = {10.1145/3424771.3424821},
abstract = {Remote Application Programming Interfaces (APIs) are used in almost any distributed system today, for instance in microservices-based systems, and are thus enablers for many digitalization efforts. API design not only impacts whether software provided as a service is easy and efficient to develop applications with, but also affects the long term evolution of the software system. In general, APIs are responsible for providing remote and controlled access to the functionality provided as services; however, APIs often are also used to expose and share information. We focus on such data-related aspects of microservice APIs in this paper. Depending on the life cycle of the information published through the API, its mutability and the endpoint role, data-oriented APIs can be designed following patterns such as Operational Data Holder, Master Data Holder, Reference Data Holder, Data Transfer Holder, and Link Lookup Resource. Known uses and examples of the patterns are drawn from public Web APIs as well as application development and integration projects we have been involved in.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {11},
numpages = {25},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@inproceedings{10.1145/2896377.2901461,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits},
year = {2016},
isbn = {9781450342667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896377.2901461},
doi = {10.1145/2896377.2901461},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.},
booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
pages = {249–260},
numpages = {12},
keywords = {differential privacy, mechanism design, game theory, incentive mechanism, strategic data subjects},
location = {Antibes Juan-les-Pins, France},
series = {SIGMETRICS '16}
}

@article{10.1145/2964791.2901461,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2964791.2901461},
doi = {10.1145/2964791.2901461},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {249–260},
numpages = {12},
keywords = {game theory, mechanism design, differential privacy, incentive mechanism, strategic data subjects}
}

@article{10.1145/3009973,
author = {Gotz, David and Sun, Shun and Cao, Nan and Kundu, Rita and Meyer, Anne-Marie},
title = {Adaptive Contextualization Methods for Combating Selection Bias during High-Dimensional Visualization},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3009973},
doi = {10.1145/3009973},
abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user’s visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {nov},
articleno = {17},
numpages = {23},
keywords = {selection bias, intelligent visual interfaces, Visualization, visual analytics, exploratory analysis}
}

@inproceedings{10.1145/3428757.3429972,
author = {Draheim, Dirk},
title = {On Architecture of E-Government Ecosystems: From e-Services to e-Participation: [IiWAS'2020 Keynote]},
year = {2020},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429972},
doi = {10.1145/3428757.3429972},
abstract = {The "digital transformation" is perceived as the key enabler for increasing wealth and well-being by many in politics, media and among the citizens alike. In the same vein, e-Government steadily received and receives more and more attention. e-Government gives rise to complex, large-scale system landscapes consisting of many players and technological systems - and we call such system landscapes e-Government ecosystems. In this talk, we are interested in the architecture of e-Government ecosystems. "Form ever follows function." Now, what is the function that determines e-Government? And what is the form in which it manifests? After briefly reviewing the purpose of e-Government from a democratic as well as a technocratic viewpoint, we will discover the primacy of the state's institutional design in the architecture of e-Government ecosystems. From there, we will arrive at the notion of data governance architecture, which provides the core of all system design efforts in e-Government. A data governance architecture maps data assets to accountable legal entities and represents the essence of co-designing institutions and technological systems. Against the background of what has been achieved, we review a series of established and emerging technologies that have been explicitly designed for or are otherwise relevant for building e-Government systems.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {3–10},
numpages = {8},
keywords = {consent management, data exchange layers, digital transformation, e-governance, Fiware, e-government, data governance, persistent messaging, GAIA-X, public key infrastructures, X-Road},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3291078.3291083,
author = {Zhicheng, Dai and Feng, Liu},
title = {Evaluation of the Smart Campus Information Portal},
year = {2018},
isbn = {9781450365772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291078.3291083},
doi = {10.1145/3291078.3291083},
abstract = {As the internet wave swept the world, "Internet plus education" came into being. Smart campus design and construction has since become a research hotspot. The Campus Information Portal (CIP) plays an increasingly important role in the management of smart campuses. That is why, conducting a comprehensive evaluation study on the construction level of campus information portals is necessary. By combining CIP's own characteristics and incorporating intelligent needs, a comprehensive evaluation index system for CIP was developed. An Analytic Hierarchy Process (AHP) was used to determine index weights, while a Fuzzy Comprehensive Evaluation (FCE) was used to calculate the quantitative scores of the evaluation objects. We selected 10 representative Chinese universities for a comprehensive CIP evaluation and experimental analysis. We analyze the final results of the study, evaluate the validity of our process and methods and finally provide guidance for the construction of a smart campus information portal.},
booktitle = {Proceedings of the 2018 2nd International Conference on Education and E-Learning},
pages = {73–79},
numpages = {7},
keywords = {Comprehensive evaluation, Smart campus, Index system, Campus information portal},
location = {Bali, Indonesia},
series = {ICEEL 2018}
}

@inproceedings{10.1145/3170427.3170636,
author = {Russell, Daniel M. and Convertino, Gregorio and Kittur, Aniket and Pirolli, Peter and Watkins, Elizabeth Anne},
title = {Sensemaking in a Senseless World: 2018 Workshop Abstract},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3170636},
doi = {10.1145/3170427.3170636},
abstract = {Sensemaking is a common activity in the analysis of a large or complex amount of information. This active area of HCI research asks how DO people come to understand such difficult sets of information? The information workplace is increasing dominated by high velocity, high volume, complex information streams. At the same time, understanding how sensemaking operates has become an urgent need in an era of increasingly unreliable news and information sources. While there has been a huge amount of work in this space, the research involved is scattered over a number of different domains with differing approaches. This workshop will focus on the most recent work in sensemaking, the activities, technologies and behaviors that people do when making sense of their complex information spaces. In the second part of the workshop we will work to synthesize a cross-disciplinary view of how sensemaking works in people, along with the human behaviors, biases, proclivities, and technologies required to support it.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {information understanding, collaborative work, sensemaking},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3488838.3488873,
author = {Cai, Hongxia and Tan, Qiqi},
title = {Blockchain-Based Data Control for Complex Product Assembly Collaboration Process},
year = {2021},
isbn = {9781450384094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488838.3488873},
doi = {10.1145/3488838.3488873},
abstract = {The collaborative development model of complex products brings the challenge to the data interaction management. There are many manufacturers and suppliers involved in the whole life cycle of the assembly process, which makes it difficult to ensure the data security and traceability. The Hyperledger-Fabric architecture in blockchain technology has modular design, pluggable architecture, complete authority control and security, which can well solve the data security and traceability management in the collaborative development of complex products. Therefore, this paper proposes a framework based on the Hyperledger-Fabric architecture of blockchain for the whole life cycle data management of complex products. We also demonstrate the effectiveness of our proposed new framework integrating blockchain technology through the case of quality data control during the aircraft final assembly collaborative process.},
booktitle = {2021 The 3rd World Symposium on Software Engineering},
pages = {205–209},
numpages = {5},
keywords = {Collaborative Production, Blockchain, Complex product assembly},
location = {Xiamen, China},
series = {WSSE 2021}
}

@inproceedings{10.1145/3336499.3338012,
author = {Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius and Gomez, Mauricio Soto and Elves\ae{}ter, Brian and Roman, Dumitru},
title = {Modelling and Linking Company Data in the EuBusinessGraph Platform},
year = {2019},
isbn = {9781450368230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336499.3338012},
doi = {10.1145/3336499.3338012},
abstract = {In the business environment, knowledge of company data is essential for a variety of tasks. The European funded project euBusinessGraph enables the establishment of a company data platform where data providers and consumers can publish and access company data. The core of the platform is the semantic data model that is the conceptual representation of company data in a common way so that it is easier to share and interlink company data. In this paper we show how the unified model and Grafterizer, a tool for manipulating and transforming raw data into Linked Data, support the linking challenge proposed in FEIII 2019. Results show that geographical enrichment of RDF data supports the interlinking process between company entities in different datasets.},
booktitle = {Proceedings of the 5th Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {12},
numpages = {6},
keywords = {RDF, Entity Matching, Record Linkage, Company data},
location = {Amsterdam, Netherlands},
series = {DSMM'19}
}

@inproceedings{10.1145/3132218.3132226,
author = {Esnaola-Gonzalez, Iker and Berm\'{u}dez, Jes\'{u}s and Fern\'{a}ndez, Izaskun and Fern\'{a}ndez, Santiago and Arnaiz, Aitor},
title = {Towards a Semantic Outlier Detection Framework in Wireless Sensor Networks},
year = {2017},
isbn = {9781450352963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132218.3132226},
doi = {10.1145/3132218.3132226},
abstract = {Outlier detection in the preprocessing phase of Knowledge Discovery in Databases (KDD) processes has been a widely researched topic for many years. However, identifying the potential outlier cause still remains an unsolved challenge even though it could be very helpful for determining what actions to take after detecting it. Furthermore, conventional outlier detection methods might still overlook outliers in certain complex contexts. In this article, Semantic Technologies are used to contribute overcoming these problems by proposing the SemOD (Semantic Outlier Detection) Framework. This framework guides the data-scientist towards the detection of certain types of outliers in WSNs (Wireless Sensor Network). Feasibility of the approach has been tested in outdoor temperature sensors and results show that the proposed approach is generic enough to apply it to different sensors, even improving the accuracy, specificity and sensitivity of outlier detection as well as spotting their potential cause.},
booktitle = {Proceedings of the 13th International Conference on Semantic Systems},
pages = {152–159},
numpages = {8},
keywords = {Knowledge Discovery in Databases, Wireless Sensor Network, Semantic Technologies, Outlier Detection},
location = {Amsterdam, Netherlands},
series = {Semantics2017}
}

@inproceedings{10.1145/3448016.3457542,
author = {Li, Guoliang and Zhou, Xuanhe and Cao, Lei},
title = {AI Meets Database: AI4DB and DB4AI},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457542},
doi = {10.1145/3448016.3457542},
abstract = {Database and Artificial Intelligence (AI) can benefit from each other. On one hand, AI can make database more intelligent (AI4DB). For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, learning-based techniques can alleviate this problem. On the other hand, database techniques can optimize AI models (DB4AI). For example, AI is hard to deploy, because it requires developers to write complex codes and train complicated models. Database techniques can be used to reduce the complexity of using AI models, accelerate AI algorithms and provide AI capability inside databases. DB4AI and AI4DB have been extensively studied recently. In this tutorial, we review existing studies on AI4DB and DB4AI. For AI4DB, we review the techniques on learning-based database configuration, optimization, design, monitoring, and security. For DB4AI, we review AI-oriented declarative language, data governance, training acceleration, and inference acceleration. Finally, we provide research challenges and future directions in AI4DB and DB4AI.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2859–2866},
numpages = {8},
keywords = {machine learning, database, models, AI},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/2856767.2856779,
author = {Gotz, David and Sun, Shun and Cao, Nan},
title = {Adaptive Contextualization: Combating Bias During High-Dimensional Visualization and Data Selection},
year = {2016},
isbn = {9781450341370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856767.2856779},
doi = {10.1145/2856767.2856779},
abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. In this paper, we present Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. Our approach (1) monitors and models a user's visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. We also share results from a user study which demonstrate the effectiveness of our technique.},
booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
pages = {85–95},
numpages = {11},
keywords = {visualization, visual analytics, exploratory analysis, intelligent visual interfaces},
location = {Sonoma, California, USA},
series = {IUI '16}
}

@article{10.1145/3511666,
author = {Schmeck, Hartmut and Monti, Antonello and Hagenmeyer, Veit},
title = {Energy Informatics: Key Elements for Tomorrow's Energy System},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3511666},
doi = {10.1145/3511666},
journal = {Commun. ACM},
month = {mar},
pages = {58–63},
numpages = {6}
}

@inproceedings{10.1145/3473714.3473788,
author = {Zhang, Jiamin},
title = {Potential Energy Saving Estimation for Retrofit Building with ASHRAE-Great Energy Predictor III Using Machine Learning},
year = {2021},
isbn = {9781450390231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473714.3473788},
doi = {10.1145/3473714.3473788},
abstract = {Energy is material basis for social development and conflicting issue of economic development around the world. With the continuous urbanization, the energy consumption of buildings will be further increased, accounting for about 40% of the total energy consumption eventually. Thus, enhancement of energy efficiency of the buildings has become an essential issue to reduce the amount of gas emission as well as fossil fuel consumption. Delivering a high-quality built environment in an energy efficient way is the crucial key to energy conservation. Energy efficiency retrofit for buildings is considered to be a promising way to achieve energy savings. Machine learning provides the ability to learn from data using multiple computer algorithms. This paper introduces several algorithms, including random forest and Light GBM, to analyze building energy consumption based on the data from Kaggle competition, providing discussion of improvement in model efficiency and economic analysis by simulating different scenarios. In addition, sensitivity analysis is conducted to show the influence of different parameters in models and metrics to quantify the accuracy of prediction are proposed. The results of this paper can help people understand quantitative influence of different variables on energy use and energy baseline models. Future works will incorporate more data type in order to enhance the performance of prediction.},
booktitle = {Proceedings of the 2021 International Conference on Control and Intelligent Robotics},
pages = {425–429},
numpages = {5},
keywords = {green architecture, energy saving, retrofit building, machine learning},
location = {Guangzhou, China},
series = {ICCIR 2021}
}

@article{10.1145/2857274.2886105,
author = {Diakopoulos, Nicholas},
title = {Accountability in Algorithmic Decision-Making: A View from Computational Journalism},
year = {2015},
issue_date = {November-December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {9},
issn = {1542-7730},
url = {https://doi.org/10.1145/2857274.2886105},
doi = {10.1145/2857274.2886105},
abstract = {Every fiscal quarter automated writing algorithms churn out thousands of corporate earnings articles for the AP (Associated Press) based on little more than structured data. Companies such as Automated Insights, which produces the articles for AP, and Narrative Science can now write straight news articles in almost any domain that has clean and well-structured data: finance, sure, but also sports, weather, and education, among others. The articles aren’t cardboard either; they have variability, tone, and style, and in some cases readers even have difficulty distinguishing the machine-produced articles from human-written ones.},
journal = {Queue},
month = {nov},
pages = {126–149},
numpages = {24}
}

@inproceedings{10.1145/2565585.2565608,
author = {Welbourne, Evan and Wu, Pang and Bao, Xuan and Munguia-Tapia, Emmanuel},
title = {Crowdsourced Mobile Data Collection: Lessons Learned from a New Study Methodology},
year = {2014},
isbn = {9781450327428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2565585.2565608},
doi = {10.1145/2565585.2565608},
abstract = {In this paper we explore a scalable data collection methodology that simultaneously achieves low cost and a high degree of control. We use popular online crowdsourcing platforms to recruit 63 subjects for a 90-day data collection that resulted in over 75K hours of data. The total cost of data collection was dramatically lower than for alternative methodologies, with total subject compensation under $3.5K US, and a total of less than 10 hours/week spent by researchers managing the study. At the same time, our methodology enhances control and enables richer study protocols by allowing direct contact with subjects. We were able to conduct surveys, exchange messages, and debug remotely with feedback from subjects. In addition to reporting on study details, we also discuss interesting findings and offer lessons learned.},
booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
articleno = {2},
numpages = {6},
location = {Santa Barbara, California},
series = {HotMobile '14}
}

@inproceedings{10.1145/3423455.3430316,
author = {Seto, Toshikazu and Sekimoto, Yoshihide and Asahi, Kosuke and Endo, Takahiro},
title = {Constructing a Digital City on a Web-3D Platform: Simultaneous and Consistent Generation of Metadata and Tile Data from a Multi-Source Raw Dataset},
year = {2020},
isbn = {9781450381659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423455.3430316},
doi = {10.1145/3423455.3430316},
abstract = {In this study, we develop a platform that can display approximately 20 types of data via a web browser to realize a digital twin of a wider area, including a detailed reading display of block units and individual three-dimensional point cloud data (point cloud) of a city. Using actual data, we examine if the data model and visualization design correspond with the zoom level. Owing to the comparative examination of the wide-area display performance and the map representation design in a JavaScript-based open-source library, we were able to develop a platform with light architecture and an easily customizable display. Furthermore, prototyping, based on Mapbox GL JS and Deck.GL, and the display of spatiotemporal flow layers, such as background maps, point cloud data in many places, dozens of layer display types, and the General Transit Feed Specification (GTFS) allowed for the seamless transition from the local government to the wide-area display in the prefecture unit in approximately 10-20 s.It is recommended that this digital smart city platform should be standardized by other local governments, especially in areas where higher-order data visualization is yet to advance. To display this digital city in a lightweight environment, we consider the digital data situation of local governments in Japan. It is necessary to define the visualized design for each zoom level according to the characteristics of the data. We then arranged the display model of each zoom level for 20 types of urban infrastructure data related to the digital smart city by referring to the style schema of the tile form. Through these tasks, we organized the commonality and optimization of data models and formats.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
pages = {1–9},
numpages = {9},
keywords = {Deck.GL, digital city, Mapbox GL JS, infrastructure 3D tiles},
location = {Seattle, Washington},
series = {ARIC '20}
}

@article{10.1145/2844110,
author = {Diakopoulos, Nicholas},
title = {Accountability in Algorithmic Decision Making},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2844110},
doi = {10.1145/2844110},
abstract = {A view from computational journalism.},
journal = {Commun. ACM},
month = {jan},
pages = {56–62},
numpages = {7}
}

@article{10.1007/s00779-019-01217-0,
author = {Jin, Yong and Qian, Zhenjiang and Chen, Shunjiang},
title = {Data Collection Scheme with Minimum Cost and Location of Emotional Recognition Edge Devices},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3–4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-019-01217-0},
doi = {10.1007/s00779-019-01217-0},
abstract = {This paper develops a real-time and reliable data collection system for big scale emotional recognition systems. Based on the data sample set collected in the initialization stage and by considering the dynamic migration of emotional recognition data, we design an adaptive Kth average device clustering algorithm for migration perception. We define a sub-modulus weight function, which minimizes the sum of the weights of the subsets covered by a cover to achieve high-precision device positioning. Combining the energy of the data collection devices and the energy of the wireless emotional device, we balance the data collection efficiency and energy consumption, and define a minimum access number problem based on energy and storage space constraints. By designing an approximate algorithm to solve the approximate minimum Steiner point problem, the continuous collection of emotional recognition data and the connectivity of data acquisition devices are guaranteed under the energy constraint of wireless devices. We validate the proposed algorithms through simulation experiments using different emotional recognition systems and different data scale. Furthermore, we analyze the proposed algorithms in terms of topology for devices classification, location accuracy, and data collection efficiency by comparing with the Bayesian classifier-based expectation maximization algorithm, the background difference-based moving target detection arithmetic averaging algorithm, and the Hungarian algorithm for solving the assignment problem.},
journal = {Personal Ubiquitous Comput.},
month = {jul},
pages = {595–606},
numpages = {12},
keywords = {Collection cost, Emotional recognition, Edge devices, Data acquisition, Location, Data collection}
}

@inbook{10.1145/3447404.3447415,
author = {McMenemy, David},
title = {Ethical Issues in Machine Learning},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447415},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {195–196},
numpages = {2}
}

@inproceedings{10.1145/2702123.2702528,
author = {Mauriello, Matthew Louis and Norooz, Leyla and Froehlich, Jon E.},
title = {Understanding the Role of Thermography in Energy Auditing: Current Practices and the Potential for Automated Solutions},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702528},
doi = {10.1145/2702123.2702528},
abstract = {The building sector accounts for 41% of primary energy consumption in the US, contributing an increasing portion of the country's carbon dioxide emissions. With recent sensor improvements and falling costs, auditors are increasingly using thermography-infrared (IR) cameras-to detect thermal defects and analyze building efficiency. Research in automated thermography has grown commensurately, aimed at reducing manual labor and improving thermal models. Though promising, we could find no prior work exploring the professional auditor's perspectives of thermography or reactions to emerging automation. To address this gap, we present results from two studies: a semi-structured interview with 10 professional energy auditors, which includes design probes of five automated thermography scenarios, and an observational case study of a residential audit. We report on common perspectives, concerns, and benefits related to thermography and summarize reactions to our automated scenarios. Our findings have implications for thermography tool designers as well as researchers working on automated solutions in robotics, computer science, and engineering.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1993–2002},
numpages = {10},
keywords = {design probes, robotics, energy audits, human-robotic interaction, formative inquiry, thermography, sustainable hci},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@article{10.1145/3415212,
author = {Feger, Sebastian S. and Wozniak, Pawe\l{} W. and Lischke, Lars and Schmidt, Albrecht},
title = { 'Yes, I Comply!': Motivations and Practices around Research Data Management and Reuse across Scientific Fields},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415212},
doi = {10.1145/3415212},
abstract = {As science becomes increasingly data-intensive, the requirements for comprehensive Research Data Management (RDM) grow. This often overwhelms scientists, requiring more workload and training. The failure to conduct effective RDM leads to producing research artefacts that cannot be reproduced or reused. Past research placed high value on supporting data science workers, but focused mainly on data production, collection, processing, and sensemaking. In order to understand practices and needs of data science workers in relation to documentation, preservation, sharing, and reuse, we conducted a cross-domain study with 15 scientists and data managers from diverse scientific domains. We identified five core concepts which describe requirements, drivers, and boundaries in the development of commitment for RDM, essential for generating reproducible research artefacts: Practice, Adoption, Barriers, Education, and Impact. Based on those concepts, we introduce a stage-based model of personal RDM commitment evolution. The model can be used to drive the design of future systems that support a transition to open science. We discuss infrastructure, policies, and motivations involved at the stages and transitions in the model. Our work supports designers in understanding the constraints and challenges involved in designing for reproducibility in an age of data-driven science.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {141},
numpages = {26},
keywords = {reuse, research data management, human data interventions, reproducibility, data-processing science, motivation}
}

