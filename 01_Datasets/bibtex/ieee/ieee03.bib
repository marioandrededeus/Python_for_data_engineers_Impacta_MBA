@INPROCEEDINGS{8669595,
author={Jiang, Ying and Zhang, Na and Fang, Ying},
booktitle={2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={The Analysis and Design of Ship Monitoring System Based on Hybrid Replication Technology},
year={2019},
volume={},
number={},
pages={456-459},
abstract={As the core of informatization, data has a huge significance to the development of information-based enterprises. Data replication technology is an important approach to solve the problem of enterprise data sharing based on distributed database system. It plays a crucial role in promoting business integration of enterprises and institutions, improving data quality, enhancing data sharing and improving the application level of back-end big data analysis [1]. It is necessary to do research for making a good data management of the distributed database application system, synchronizing the data to the data, preventing data conflicting and being able to synchronize or asynchronous replication. Combining with the database design model of the ship monitoring and control system, this paper mainly described how to complete the construction of distributed database system using Oracle, based on advanced replication technology named as the combination of multi-agent replication and materialized views hybrid replication technology.},
keywords={Distributed databases;Database systems;Synchronization;Business;Monitoring;Marine vehicles;Distributed database;advanced replication;materialized view;data conflict},
doi={10.1109/ICITBS.2019.00118},
ISSN={},
month={Jan},}
@INPROCEEDINGS{9107804,
author={Ju, Xingang and Lian, Feiyu and Zhang, Yuan},
booktitle={2019 6th International Conference on Information Science and Control Engineering (ICISCE)},
title={Data Cleaning Optimization for Grain Big Data Processing using Task Merging},
year={2019},
volume={},
number={},
pages={225-233},
abstract={Data quality has exerted important influence over the application of grain big data, so data cleaning is a necessary and important work. In MapReduce frame, we can use parallel technique to execute data cleaning in high scalability mode, but due to the lack of effective design there are amounts of computing redundancy in the process of data cleaning, which results in lower performance. In this research, we found some tasks often are carried out multiple times on same input files, or require same operation results in the process of data cleaning. For this problem, we proposed a new optimization technique that is based on task merge. By merging simple or redundancy computations on same input files, the number of the loop computation in MapReduce can be reduced greatly. The experiment shows, by this means, the overall system runtime is significantly reduced, which proves that the process of data cleaning is optimized. In this paper, we optimized several modules of data cleaning such as entity identification, inconsistent data restoration, and missing value filling. Experimental results show that the proposed method in this paper can increase efficiency for grain big data cleaning.},
keywords={grain big data;data cleaning;task merging;hadoop;mapReduce},
doi={10.1109/ICISCE48695.2019.00053},
ISSN={},
month={Dec},}
@ARTICLE{9845398,
author={Meritxell, Gómez-Omella and Sierra, Basilio and Ferreiro, Susana},
journal={IEEE Access},
title={On the Evaluation, Management and Improvement of Data Quality in Streaming Time Series},
year={2022},
volume={10},
number={},
pages={81458-81475},
abstract={The Internet of Things (IoT) technologies plays a key role in the Fourth Industrial Revolution (Industry 4.0). This implies the digitisation of the industry and its services to improve productivity. To obtain the necessary information throughout the different processes, useful data streams are obtained to provide Artificial Intelligence and Big Data algorithms. However, strategic decision-making based on these algorithms may not be successful if they have been developed based on inadequate low-quality data. This research work proposes a set of metrics to measure Data Quality (DQ) in streaming time series, and implements and validates a set of techniques and tools that allow monitoring and improving the quality of the information. These techniques allow the early detection of problems that arise in relation to the quality of the data collected; and, in addition, they provide some mechanisms to solve these problems. Later, as part of the work, a use case related to industrial field is presented, where these techniques and tools have been deployed into a data management, monitoring and data analysis platform. This integration provides additional functionality to the platform, a Decision Support System (DSS) named DQ-REMAIN (Data Quality REport MAnagement and ImprovemeNt), for decision-making regarding the quality of data obtained from streaming time series.},
keywords={Measurement;Time series analysis;Data integrity;Decision support systems;Standards;Monitoring;Internet of Things;Data quality;streaming time series;decision support system},
doi={10.1109/ACCESS.2022.3195338},
ISSN={2169-3536},
month={},}
@ARTICLE{9884973,
author={Byabazaire, John and O’Hare, Gregory and Delaney, Declan T.},
journal={IEEE Sensors Journal},
title={End-to-End Data Quality Assessment Using Trust for Data Shared IoT Deployments},
year={2022},
volume={},
number={},
pages={1-1},
abstract={Continued development of communication technologies has led to the widespread Internet of Things (IoT) integration into various domains, including health, manufacturing, automotive and precision agriculture. This has further led to the increased sharing of data amongst such domains to abet innovation. Most of these IoT deployments, however, are based on heterogeneous, pervasive sensors, which can lead to quality issues in the recorded data. This can lead to sharing of inaccurate or inconsistent data. There is a significant need to assess the quality of the collected data, should it be shared with multiple application domains, as inconsistencies in the data could have financial or health ramifications. This paper builds on the recent research on trust metrics and presents a framework to integrate such metrics into the IoT data cycle for real-time data quality assessment. Critically, this paper adopts a mechanism to facilitate end user parameterisation of a trust metric tailoring it’s use in the framework. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd-sourced or other unreliable data collection techniques such as that in IoT.The paper further discusses how the trust based framework eliminates the requirement for a gold standard and provides visibility into data quality assessment throughout the big data model.To qualify the use of trust as a measure of quality, an experiment is conducted using data collected from an IoT deployment of sensors to measure air quality in which low cost sensors were co-located with a gold standard reference sensor. The calculated trust metric is compared with two well understood metrics for data quality, Root Mean Squares Error (RMSE) and Mean Absolute Error (MAE). A strong correlation between the trust metric and the comparison metrics shows that trust may be used as an indicative quality metric for data quality. The metric incorporates the additional benefit of its ability for use in low context scenarios, as opposed to RMSE and MAE, which require a reference for comparison.},
keywords={Data integrity;Measurement;Internet of Things;Big Data;Data models;Sensor phenomena and characterization;Quality assessment;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning},
doi={10.1109/JSEN.2022.3203853},
ISSN={1558-1748},
month={},}
@INPROCEEDINGS{6984221,
author={Bruballa, Eva and Taboada, Manel and Cabrera, Eduardo and Rexachs, Dolores and Luque, Emilio},
booktitle={2014 International Conference on Future Internet of Things and Cloud},
title={Simulation and Big Data: A Way to Discover Unusual Knowledge in Emergency Departments: Work-in-Progress Paper},
year={2014},
volume={},
number={},
pages={367-372},
abstract={Here a work in progress is reported on within research that aims to obtain knowledge about variables which may influence a hospital emergency department's performance and quality of service. Knowledge discovery will be achieved through the analysis of intensive data generated by the simulation of any possible scenario in the real system. The challenge is to provide knowledge of critical, non-usual or extreme situations. Simulation is the only way to obtain information about these kinds of situations, as it is not possible to test such scenarios in the real system. We show how simulation of the real system through advanced computing is a source of big data, as it allows rapid and massive data generation. The potential of high performance computing makes it possible to generate a very large amount of data within a reasonable time, store this data, then process and analyze it to obtain knowledge. We describe the methodology proposed for this goal, which is based on the use of the simulator as a sensor of the real system, and so as the main source of data. The application of data mining techniques will open the doors to knowledge. To verify that the proposed methodology works, we propose a case study in which the aim is to obtain knowledge from a set of data already available, obtained from the simulation of a reduced set of scenarios of the real system.},
keywords={Data models;Data mining;Computational modeling;Hospitals;Analytical models;Big data;Agent-Based Modeling and Simulation (ABMS);Big Data;Data Mining (DM);Decision Support Systems (DSS);Emergency Department (ED);Knowledge Discovery},
doi={10.1109/FiCloud.2014.65},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8456103,
author={Liu, Kai-Cheng and Kuo, Chuan-Wei and Liao, Wen-Chiuan and Wang, Pang-Chieh},
booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)},
title={Optimized Data de-Identification Using Multidimensional k-Anonymity},
year={2018},
volume={},
number={},
pages={1610-1614},
abstract={In the globalized knowledge economy, big data analytics have been widely applied in diverse areas. A critical issue in big data analysis on personal information is the possible leak of personal privacy. Therefore, it is necessary to have an anonymization-based de-identification method to avoid undesirable privacy leak. Such method can prevent published data form being traced back to personal privacy. Prior empirical researches have provided approaches to reduce privacy leak risk, e.g. Maximum Distance to Average Vector (MDAV), Condensation Approach and Differential Privacy. However, previous methods inevitably generate synthetic data of different sizes and is thus unsuitable for general use. To satisfy the need of general use, k-anonymity can be chosen as a privacy protection mechanism in the de-identification process to ensure the data not to be distorted, because k-anonymity is strong in both protecting privacy and preserving data authenticity. Accordingly, this study proposes an optimized multidimensional method for anonymizing data based on both the priority weight-adjusted method and the mean difference recommending tree method (MDR tree method). The results of this study reveal that this new method generate more reliable anonymous data and reduce the information loss rate.},
keywords={Data privacy;Privacy;Numerical models;Big Data;Measurement;Data models;Greedy algorithms;privacy preserving;k-anonymity;de-identification;data quality;information loss},
doi={10.1109/TrustCom/BigDataSE.2018.00235},
ISSN={2324-9013},
month={Aug},}
@INPROCEEDINGS{8248534,
author={Zhang, Heng and Liu, Guohua and Zhao, Wenfeng and Ni, Mengfei},
booktitle={2017 4th International Conference on Systems and Informatics (ICSAI)},
title={Incomplete relation revision method based on template},
year={2017},
volume={},
number={},
pages={1563-1567},
abstract={Data quality is the core problem in the field of big data application. In practical applications, data are often derived from multi-source databases, which can cause named conflict and similar duplicate record problems. This paper proposed a method to solve the named conflict problem and similar duplicate record based on a given template. In order to find the relationship between the template data and the data to be unified, we segmented the data that to be unified, then built an extensional B tree based on these data. At the same time, we construct a NFA for each template value. By using these NFA, we can get the language pattern for each column in the template relation. Finally, we search each template value in the extensional B tree, if the template value can be found and the corresponding data to be unified can be accepted by the NFA based on the template value, we can use the template value to replace the data that need to be recovered. Then, the data can be consolidated and integrated to ensure the consistency and integrity of the data.},
keywords={Algorithm design and analysis;Cleaning;Maintenance engineering;Feature extraction;Sorting;Classification algorithms;data quality;template;NFA;extensional B tree},
doi={10.1109/ICSAI.2017.8248534},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8622786,
author={Zheng, Ningxin and Chen, Quan and Chen, Chen and Guo, Minyi},
booktitle={2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
title={CLIBE: Precise Cluster-Level I/O Bandwidth Enforcement in Distributed File System},
year={2018},
volume={},
number={},
pages={124-131},
abstract={A distributed file system (DFS) is a core component to implement big data applications. On the one hand, a DFS is capable of managing a large volume of data with desirable properties that strike the balance between high availability, reliability, and so on. On the other hand, a DFS relies on underlying storage systems (e.g., hard drives, solid state drives, etc.) and suffer from slow read/write operations. In big data era, large-scale data processing applications start to leverage the in-memory processing to improve the performance by reducing the inhibitive cost of I/O operations. However, it is still inevitable to read input data from or write outputs to the storage system. Slow I/O operations are often the main bottleneck of emerging big data applications. In particular, while these applications often use DFSs to store their results for the high availability and reliability, the unmanaged I/O bandwidth contention results in the QoS violation of high priority applications when multiple applications share the same DFS. To enable I/O management and allocation on big-data platforms, we propose a Cluster-Level I/O Bandwidth Enforcement (CLIBE) approach that consists of a cluster-level I/O bandwidth quota manager, multiple node-level I/O bandwidth controllers, and a feedback-based quota reallocator. The quota manager splits and distributes the I/O bandwidth quota of an application to the active nodes that are serving this application. The bandwidth controller on a node ensures that the I/O bandwidth used by an application would not exceed its bandwidth quota on the node. For an application affected by slow or overloaded nodes, the quota reallocator reallocates the idle I/O bandwidth on underloaded nodes to this application to guarantee its throughput. Our experiment on a real-system cluster shows that CLIBE is able to precisely control the I/O bandwidth used by an application at the cluster level, with the deviation smaller than 2.51%.},
keywords={Bandwidth;Quality of service;Big Data applications;Throughput;Writing;Reliability;Distributed file system;I/O bandwidth enforcement;HDFS},
doi={10.1109/HPCC/SmartCity/DSS.2018.00048},
ISSN={},
month={June},}
@INPROCEEDINGS{9574505,
author={Zeng, Hui and Zhao, Xiaoyong and Wang, Lei},
booktitle={2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)},
title={Multivariate Time Series Anomaly Detection On Improved HTM Model},
year={2021},
volume={},
number={},
pages={759-763},
abstract={In recent years, industrial big data has attracted much attention as the key technical support of “Intelligent Manufacturing” and “Industrial Internet”. And as the dependence of intelligent manufacturing on digitalization continues to increase, data quality problems caused by device and system failures, harsh environment, improper scheduling and management, duplication or missing of data fields, etc., have more significant impacts on industrial processes. Therefore, the anomaly detection of industrial big data is particularly important. Among the methods onto time series data for anomaly detection, HTM(Hierarchical Temporal Memory) algorithm performs well in the unsupervised univariate time series data anomaly detection, but the capability of original HTM model for detecting multivariate time series anomaly data is insufficient. However, the multivariate data anomaly detection is common in industry and the performance requirements for data anomaly detection are relatively high. Thus, this paper proposes an improved HTM algorithm model - MSP-HTM(Multiple Spatial Poolers HTM) model. The MSP-HTM model respectively encode the value of different dimensions at the same time, and then put the result from encoder into spatial pooler respectively, finally the temporal memory layer merge result from spatial poolers, and predict future data. Experiments show that the MSP-HTM model can improve performance by processing the multivariate time series data in parallel and improve the effect of data anomaly detection.},
keywords={Performance evaluation;Job shop scheduling;Computational modeling;Time series analysis;Big Data;Predictive models;Prediction algorithms;Multivariate time series;Anomaly detection;Hierarchical temporal memory;Industrial bigdata},
doi={10.1109/CEI52496.2021.9574505},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8780271,
author={Yu, Wenjin and Dillon, Tharam and Mostafa, Fahed and Rahayu, Wenny and Liu, Yuehua},
booktitle={2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS)},
title={Implementation of Industrial Cyber Physical System: Challenges and Solutions},
year={2019},
volume={},
number={},
pages={173-178},
abstract={The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards.},
keywords={Big Data;Manufacturing;Industries;Sensors;Data integrity;Real-time systems;Cloud computing;Cyber-Physical System;Internet of Things;Industry 4.0;cloud computing;big data ecosystem;data quality},
doi={10.1109/ICPHYS.2019.8780271},
ISSN={},
month={May},}
@INPROCEEDINGS{7802106,
author={Kalan, Reza Shokri and Ünalir, Murat Osman},
booktitle={2016 6th International Conference on Computer and Knowledge Engineering (ICCKE)},
title={Leveraging big data technology for small and medium-sized enterprises (SMEs)},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Wisdom aligns with technology is the key factor for sustainable business development. By increasing amount of public and private data, organizations need to find new solutions to manage data and information which lead to knowledge, better decision making, and value. In the big data-bang, smart organization surfing on-line technology and start planning big data strategy. However, many organizations do not yet have a big data strategy. A challenge facing SMEs is that they may not have the same capacity as large companies to analysis new data sets. Also, traditional data processing tools are not capable for SMEs decision making because of volume, velocity and variety if data. For address this problem we need new leveraging technology, tools and talent. SMEs which have risen to leveraging the value of big data are using advantage of cloud computing and open-source software to realize various goals. The main goal of this investment is about value as a new concept in a big data era. In this study, we focus on emerging trends and future requirement: technology and tools for SMEs.},
keywords={Organizations;Parallel processing;Computer architecture;Security;Privacy;big data;data analytic;data quality;SMEs;business intelligence;cloud computing},
doi={10.1109/ICCKE.2016.7802106},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7518511,
author={Ahmad, Awais and Paul, Anand and Rathore, M. Mazhar and Rho, Seungmin},
booktitle={2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)},
title={Big Data Analytical Architecture Using Divide-and-Conquer Approach in Machine-to-Machine Communication},
year={2015},
volume={},
number={},
pages={1819-1824},
abstract={Machine-to-Machine (M2M) technology unremittingly motivates any time-place-objects connectivity of the devices in and around the world. Every day, a rapid growth of large M2M networks and digital storage technology, lead to a massive heterogeneous data depository, in which the M2M data are captured and warehoused in the diverse database frameworks as a magnitude of heterogeneous data sources. Hence, the M2M that handles Big Data might perform poorly or not according to the goals of their operator due to massive heterogeneous data sources may face various incompatibilities, such as data quality, processing and computational efficiency, analysis and feature extraction applications. Therefore, to address the aforementioned constraints, this paper presents a Big Data Analytical architecture based on Divide-and-Conquer approach. The designed system architecture exploits divide-and-conquer approach, where big data sets are first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using filtration and load balancing algorithms. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.},
keywords={Servers;Satellites;Big data;Computer architecture;Algorithm design and analysis;Decision making;Feature extraction;Big Data;divide-and-conquer;machine ID;efficiency},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.330},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9724934,
author={Feng, Xinyi},
booktitle={2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM)},
title={Power Data Quality Optimization and Evaluation Based on BPNN},
year={2021},
volume={},
number={},
pages={505-509},
abstract={With the continuous improvement of the information technology and communications of Smart Grid, the electric power big data environment has been formed. The data shows diversity and multi-source characteristics. How to ensure the quality of power data in the computer organization under the condition of heterogeneity is the premise of making relevant decisions. This paper firstly gives the definition of Data Space of power enterprises, analyzes the factors affecting the quality of data in the computer environment, and gives the relevant architecture of processing power data in the data space. Secondly, based on business flow and Petri net in the computer environment, this paper constructs the data flow and quality control model of the front and back platforms. The former represents the data flow in the power business and abstracts it to form Petri net computer information flow, so that the data can achieve the effect of cleaning while flowing in the business process. Finally, an evaluation index system is built and back-propagation neural network (BPNN) is used to determine the weight, a case study is given to verify the effectiveness of the proposed method.},
keywords={Data integrity;Petri nets;Computer architecture;Aerospace electronics;Data processing;Data models;Cleaning;data space;data quality;business flow;petri net;BPNN},
doi={10.1109/AIAM54119.2021.00106},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8900190,
author={Han, Weiguo and Jochum, Matthew},
booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium},
title={Practices and Experiences in High Volumes of Satellite Data Management},
year={2019},
volume={},
number={},
pages={4364-4367},
abstract={High volumes of satellite data management within an organization is still challenging and daunting in the era of big data. The increasing information technology costs and limited budgets, growing satellite data needs, data availability across multiple teams and projects, strategic goals of organization, and expected project outcomes require better satellite data management mechanism and system to facilitate research and development activities. An organization level centralized satellite data repository is a practical solution to satisfy these requirements. This paper describes the best practices and experiences from building such a central satellite data repository within our organization, including data management strategy and policy, scalable and extensible system infrastructure, comprehensive data management system, and technical support and user assistance. These practices can be borrowed and applied in other organizations with similar requirements.},
keywords={Satellites;Organizations;Research and development;Databases;Monitoring;Buildings;Data integrity;Satellite Data Management;Big Data;Data Quality;Central Data Repository},
doi={10.1109/IGARSS.2019.8900190},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{8590192,
author={Fernández-Cerero, Damian and Fernández-Montes, Alejandro and Kolodziej, Joanna and Lefèvre, Laurent},
booktitle={2018 11th International Conference on the Quality of Information and Communications Technology (QUATIC)},
title={Quality of Cloud Services Determined by the Dynamic Management of Scheduling Models for Complex Heterogeneous Workloads},
year={2018},
volume={},
number={},
pages={210-219},
abstract={The quality of services in Cloud Computing (CC) depends on the scheduling strategies selected for processing of the complex workloads in the physical cloud clusters. Using the scheduler of the single type does not guarantee of the optimal mapping of jobs onto cloud resources, especially in the case of the processing of the big data workloads. In this paper, we compare the performances of the cloud schedulers for various combinations of the cloud workloads with different characteristics. We define several scenarios where the proper types of schedulers can be selected from a list of scheduling models implemented in the system, and used to schedule the concrete workloads based on the workloads' parameters and the feedback on the efficiency of the schedulers. The presented work is the first step in the development and implementation of an automatic intelligent scheduler selection system. In our simple experimental analysis, we confirm the usefulness of such a system in today's data-intensive cloud computing.},
keywords={Cloud computing;Task analysis;Processor scheduling;Dynamic scheduling;Big Data;Job shop scheduling;Servers;Big Data;Quality of Big Data;Scheduling;Cloud scheduling;Dynamic cloud scheduling},
doi={10.1109/QUATIC.2018.00039},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9364425,
author={Neves, Ricardo A. and Cruvinel, Paulo E.},
booktitle={2021 IEEE 15th International Conference on Semantic Computing (ICSC)},
title={Ontology for Structuring a Digital Databases for Decision Making in Grain Production},
year={2021},
volume={},
number={},
pages={386-392},
abstract={This paper presents an ontology for the structuring of digital databases with the objective of acting in a cloud environment and meeting big data sources in the agricultural context of grain production. Its conception is structured in three stages: the first stage presents an ontological architecture aimed at public and private cloud environments, the second stage deals with a semantic model at process level, and a pseudocode for ontological application is elaborated in the third stage, considering the technologies applied to the cloud. This work combines advanced features to support decision making from Data Lake storage solutions, semantic treatment of big data, as well as the presentation of strategies based on machine learning and data quality analysis to obtain data and metadata organized for application in a decision model. The configuration of the ontology presented meets the diversity of big data projects in the grain production context, the characteristics of which are based on interoperability in the use of heterogeneous data and its integration, elasticity of computational resources, and high availability of cloud access.},
keywords={Cloud computing;Databases;Semantics;Decision making;Production;Ontologies;Big Data;Ontology;Agriculture;Digital Database;Cloud Computing;Big Data;Decision Making},
doi={10.1109/ICSC50631.2021.00071},
ISSN={2325-6516},
month={Jan},}
@ARTICLE{7588229,
author={Kong, Linghe and Zhang, Daqiang and He, Zongjian and Xiang, Qiao and Wan, Jiafu and Tao, Meixia},
journal={IEEE Communications Magazine},
title={Embracing big data with compressive sensing: a green approach in industrial wireless networks},
year={2016},
volume={54},
number={10},
pages={53-59},
abstract={New-generation industries heavily rely on big data to improve their efficiency. Such big data are commonly collected by smart nodes and transmitted to the cloud via wireless. Due to the limited size of smart node, the shortage of energy is always a critical issue, and the wireless data transmission is extremely a big power consumer. Aiming to reduce the energy consumption in wireless, this article introduces a potential breach from data redundancy. If redundant data are no longer collected, a large amount of wireless transmissions can be cancelled and their energy saved. Motivated by this breach, this article proposes a compressive-sensing-based collection framework to minimize the amount of collection while guaranteeing data quality. This framework is verified by experiments and extensive real-trace-driven simulations.},
keywords={Big data;Compressed sensing;Wireless sensor networks;Wireless communication;Redundancy;Industrial plants;Green design},
doi={10.1109/MCOM.2016.7588229},
ISSN={1558-1896},
month={October},}
@ARTICLE{9409047,
author={Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo},
journal={IEEE Access},
title={From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction},
year={2021},
volume={9},
number={},
pages={60447-60458},
abstract={In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.},
keywords={Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation},
doi={10.1109/ACCESS.2021.3074559},
ISSN={2169-3536},
month={},}
@ARTICLE{8715359,
author={Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed},
journal={IEEE Access},
title={Intelligent Data Engineering for Migration to NoSQL Based Secure Environments},
year={2019},
volume={7},
number={},
pages={69042-69057},
abstract={In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.},
keywords={Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational databases;NoSQL;big data;data cleansing},
doi={10.1109/ACCESS.2019.2916912},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9378192,
author={Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Gondalia, Shlok and Duggan, Jerry and Kahn, Michael G.},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={An Autocorrelation-based LSTM-Autoencoder for Anomaly Detection on Time-Series Data},
year={2020},
volume={},
number={},
pages={5068-5077},
abstract={Data quality significantly impacts the results of data analytics. Researchers have proposed machine learning based anomaly detection techniques to identify incorrect data. Existing approaches fail to (1) identify the underlying domain constraints violated by the anomalous data, and (2) generate explanations of these violations in a form comprehensible to domain experts. We propose IDEAL, which is an LSTM-Autoencoder based approach that detects anomalies in multivariate time-series data, generates domain constraints, and reports subsequences that violate the constraints as anomalies. We propose an automated autocorrelation-based windowing approach to adjust the network input size, thereby improving the correctness and performance of constraint discovery over manual and brute-force approaches. The anomalies are visualized in a manner comprehensible to domain experts in the form of decision trees extracted from a random forest classifier. Domain experts can then provide feedback to retrain the learning model and improve the accuracy of the process. We evaluate the effectiveness of IDEAL using datasets from Yahoo servers, NASA Shuttle, and Colorado State University Energy Institute. We demonstrate that IDEAL can detect previously known anomalies from these datasets. Using mutation analysis, we show that IDEAL can detect different types of injected faults. We also demonstrate that the accuracy improves after incorporating domain expert feedback.},
keywords={NASA;Manuals;Big Data;Servers;Decision trees;Anomaly detection;Random forests;Anomaly detection;Autocorrelation;Data quality tests;Explainability;LSTM-Autoencoder;Time series},
doi={10.1109/BigData50022.2020.9378192},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9671443,
author={Mecati, Mariachiara and Vetrò, Antonio and Torchiano, Marco},
booktitle={2021 IEEE International Conference on Big Data (Big Data)},
title={Detecting Discrimination Risk in Automated Decision-Making Systems with Balance Measures on Input Data},
year={2021},
volume={},
number={},
pages={4287-4296},
abstract={Bias in the data used to train decision-making systems is a relevant socio-technical issue that emerged in recent years, and it still lacks a commonly accepted solution. Indeed, the "bias in-bias out" problem represents one of the most significant risks of discrimination, which encompasses technical fields, as well as ethical and social perspectives. We contribute to the current studies of the issue by proposing a data quality measurement approach combined with risk management, both defined in ISO/IEC standards. For this purpose, we investigate imbalance in a given dataset as a potential risk factor for detecting discrimination in the classification outcome: specifically, we aim to evaluate whether it is possible to identify the risk of bias in a classification output by measuring the level of (im)balance in the input data. We select four balance measures (the Gini, Shannon, Simpson, and Imbalance ratio indexes) and we test their capability to identify discriminatory classification outputs by applying such measures to protected attributes in the training set. The results of this analysis show that the proposed approach is suitable for the goal highlighted above: the balance measures properly detect unfairness of software output, even though the choice of the index has a relevant impact on the detection of discriminatory outcomes, therefore further work is required to test more in-depth the reliability of the balance measures as risk indicators. We believe that our approach for assessing the risk of discrimination should encourage to take more conscious and appropriate actions, as well as to prevent adverse effects caused by the "bias in-bias out" problem.},
keywords={Training;Ethics;Decision making;Big Data;Software;Software reliability;Software measurement;Data quality;Data bias;Data ethics;Algorithm fairness;Automated decision-making},
doi={10.1109/BigData52589.2021.9671443},
ISSN={},
month={Dec},}
@ARTICLE{9906976,
author={Murshed, Belal Abdullah Hezam and Abawajy, Jemal and Mallappa, Suresha and Saif, Mufeed Ahmed Naji and Al-Ghuribi, Sumaia Mohammed and Ghanem, Fahd A.},
journal={IEEE Access},
title={Enhancing Big Social Media Data Quality for Use in Short-Text Topic Modelling},
year={2022},
volume={},
number={},
pages={1-1},
abstract={With the emergence of microblogging platforms and social media applications, large amounts of user-generated data in the form of comments, reviews, and brief text messages are produced every day. Microblog data is typically of poor quality; hence improving the quality of the data is a significant scientific and practical challenge. In spite of the relevance of the problem, there has been not much work so far, especially in regard to microblog data quality for Short-Text Topic Modelling (STTM) purposes. This paper addresses this problem and proposes an approach called the social media data cleansing model (SMDCM) to improve data quality for STTM. We evaluate SMDCM using six topic modelling methods, namely the Latent Dirichlet Allocation (LDA), Word-Network Topic Model (WNTM), Pseudo-document-based Topic Modelling (PTM), Biterm Topic Model (BTM), Global and Local word embedding-based Topic Modeling (GLTM), and Fuzzy Topic modelling (FTM). We used the Real-world Cyberbullying Twitter (RW-CB-Twitter) and the Cyberbullying Mendeley (CB-MNDLY) datasets in the evaluation. The results proved the efficiency of the GLTM and WNTM over the other STTM models when applying the SMDCM techniques, which achieved optimum topic coherence and high accuracy values on RW-CB-Twitter and CB-MNDLY datasets.},
keywords={Social networking (online);Data models;Blogs;Data integrity;Sentiment analysis;Information integrity;Big Data;Coherence;Social Media;Big Data;Microblogging Platforms;Topic Modeling;Data Cleansing;Data Quality;Topic Coherence;Purity},
doi={10.1109/ACCESS.2022.3211396},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8972078,
author={Borrison, Reuben and Kloepper, Benjamin and Mullen, Jennifer},
booktitle={2019 IEEE 17th International Conference on Industrial Informatics (INDIN)},
title={Data Preparation for Data Mining in Chemical Plants using Big Data},
year={2019},
volume={1},
number={},
pages={1185-1191},
abstract={Data preparation for data mining in industrial applications is a key success factor which requires considerable repeated efforts. Although the required activities need to be repeated in very similar fashion across many projects, details of their implementation differ and require both application understanding and experience. As a result, data preparation is done by data mining experts with a strong domain background and a good understanding of the characteristics of the data to be analyzed. Experts with these profiles usually have an engineering background and no strong expertise in distributed programming or big data technology. Unfortunately, the amount of data can be so large that distributed algorithms are required to allow for inspection of results and iteration of preparation steps. This contribution introduces an interactive data preparation workflow for signal data from chemical plants enabling domain experts without background in distributed computing and extensive programming experience to leverage the power of big data technologies.},
keywords={Data quality;Soft sensors;Big data},
doi={10.1109/INDIN41052.2019.8972078},
ISSN={2378-363X},
month={July},}
@ARTICLE{7885523,
author={Bronselaer, Antoon and De Mol, Robin and De Tré, Guy},
journal={IEEE Transactions on Fuzzy Systems},
title={A Measure-Theoretic Foundation for Data Quality},
year={2018},
volume={26},
number={2},
pages={627-639},
abstract={In this paper, a novel framework for data quality measurement is proposed by adopting a measure-theoretic treatment of the problem. Instead of considering a specific setting in which quality must be assessed, our approach departs more formally from the concept of measurement. The basic assumption of the framework is that the highest possible quality can be described by means of a set of predicates. Quality of data is then measured by evaluating those predicates and by combining their evaluations. This combination is based on a capacity function (i.e., a fuzzy measure) that models for each combination of predicates the capacity with respect to the quality of the data. It is shown that expression of quality on an ordinal scale entails a high degree of interpretation and a compact representation of the measurement function. Within this purely ordinal framework for measurement, it is shown that reasoning about quality beyond the ordinal level naturally originates from the uncertainty about predicate evaluation. It is discussed how the proposed framework is positioned with respect to other approaches with particular attention to aggregation of measurements. The practical usability of the framework is discussed for several well known dimensions of data quality and demonstrated in a use-case study about clinical trials.},
keywords={Current measurement;Urban areas;Context;Big Data;Uncertainty;Decision making;Data quality;fuzzy measure;uncertainty modeling},
doi={10.1109/TFUZZ.2017.2686807},
ISSN={1941-0034},
month={April},}
@INPROCEEDINGS{8622185,
author={Wijerathna, Nadeesha and Matsubara, Masaki and Morishima, Atsuyuki},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Finding Evidences by Crowdsourcing},
year={2018},
volume={},
number={},
pages={3560-3563},
abstract={Crowdsourcing is a promising tool involving multiple people in completing tasks that are difficult to complete by an individual, a small team or a computer. Ensuring the quality of the results is also one of the primary problems in crowdsourcing. One of the major approaches to improve the data quality to aggregate answers from more than one workers. This study explores a different approach - we ask workers to prove facts. We devise a general framework for collecting and ranking evidence-based proofs. The experiments results show that the proposed framework works and how diverse the collected proofs are. Our results clearly indicate that the crowd-based approach to prove facts is promising.},
keywords={Task analysis;Crowdsourcing;Libraries;Media;Uniform resource locators;Web pages;Big Data;Evidence;Assumption;Crowdsourcing},
doi={10.1109/BigData.2018.8622185},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8625275,
author={Canbek, Gürol and Sagiroglu, Seref and Taskaya Temizel, Tugba},
booktitle={2018 International Congress on Big Data, Deep Learning and Fighting Cyber Terrorism (IBIGDELFT)},
title={New Techniques in Profiling Big Datasets for Machine Learning with a Concise Review of Android Mobile Malware Datasets},
year={2018},
volume={},
number={},
pages={117-121},
abstract={As the volume, variety, velocity aspects of big data are increasing, the other aspects such as veracity, value, variability, and venue could not be interpreted easily by data owners or researchers. The aspects are also unclear if the data is to be used in machine learning studies such as classification or clustering. This study proposes four techniques with fourteen criteria to systematically profile the datasets collected from different resources to distinguish from one another and see their strong and weak aspects. The proposed approach is demonstrated in five Android mobile malware datasets in the literature and in security industry namely Android Malware Genome Project, Drebin, Android Malware Dataset, Android Botnet, and Virus Total 2018. The results have shown that the proposed profiling methods reveal remarkable insight about the datasets comparatively and directs researchers to achieve big but more visible, qualitative, and internalized datasets.},
keywords={Malware;Big Data;Machine learning;Mobile applications;Genomics;Bioinformatics;Aerospace electronics;data profiling;data quality;big data;malware detection;mobile malware;machine learning;classification;Android;feature engineering},
doi={10.1109/IBIGDELFT.2018.8625275},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8784671,
author={Xinmei, Liang and Luqin},
booktitle={2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)},
title={Research on Web Service Selection Based on Parallel Skyline Algorithm},
year={2019},
volume={},
number={},
pages={1-5},
abstract={With the continuous development of the Internet, there are many web services with the same functional attributes but different functional attributes. It is urgent to find a web service that can satisfy itself quickly and efficiently from the massive web service data. This paper improves the traditional Skyline algorithm, divides the web service data set into regions, greatly reduces the data points without dominance, and saves memory usage. The improved Skyline algorithm can significantly improve the speed of Web service selection. However, the improved Skyline algorithm will still have insufficient computing resources when processing massive Web service data, resulting in a significant decrease in computing speed and even computer jam. In view of the above situation, this paper will parallelize the improved Skyline algorithm and parallelize the improved Skyline algorithm through the Spark platform. Experiments show that the parallelized Skyline algorithm can better handle massive Web service data.},
keywords={Sparks;Web services;Clustering algorithms;Big Data;Quality of service;Computer science;Parallel processing;Skyline;big data;Spark;Hadoop;parallelization},
doi={10.1109/ICEIEC.2019.8784671},
ISSN={2377-844X},
month={July},}
@ARTICLE{9457165,
author={Manogaran, Gunasekaran and Nguyen, Tu N.},
journal={IEEE Transactions on Intelligent Transportation Systems},
title={Displacement-Aware Service Endowment Scheme for Improving Intelligent Transportation Systems Data Exchange},
year={2021},
volume={},
number={},
pages={1-11},
abstract={Intelligent Transportation Systems (ITS) is a smart-transportation system for road-side assistance and data exchange support by integrating cloud and wireless networks. ITS facilitates vehicle-to-vehicle and vehicle-to-anything (V2X) data exchanges for satisfying user demands. The rate of big data granting to the vehicular users is interrupted by the fundamental attributes such as mobility and link instability of the vehicles. To address the issues in vehicular data exchange big data, this article introduces displacement-aware service endowment scheme with the benefits of data offloading. Displacement-aware big data endowment ensures responsive availability of vehicle request information despite unfavorable location and density factors. The time congruency in V2V and V2X data exchanges are adopted for minimizing data exchange dropouts. In the data offloading phase, extraneous information and big data responses are detained based on data exchange relevance to improve congestion free big data endowment. The distinct methods work in a co-operative manner to improve big data quality of fast configuring smart vehicles to provide reliable big data in smart city environments.},
keywords={Big Data;Quality of service;Delays;Data models;Vehicular ad hoc networks;Vehicle-to-everything;Optimization;ITS;mobility prediction;time synchronized data exchanges;data offloading;V2X data exchange.},
doi={10.1109/TITS.2021.3078753},
ISSN={1558-0016},
month={},}
@INPROCEEDINGS{6602615,
author={Tien, James M.},
booktitle={2013 10th International Conference on Service Systems and Service Management},
title={Big Data: Unleashing information},
year={2013},
volume={},
number={},
pages={4-4},
abstract={Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.},
keywords={Information management;Data handling;Data storage systems;Educational institutions;Physics;Security;Cameras},
doi={10.1109/ICSSSM.2013.6602615},
ISSN={2161-1904},
month={July},}
@INPROCEEDINGS{9226358,
author={Al-Sabbagh, Khaled Walid and Staron, Miroslaw and Hebig, Regina and Meding, Wilhelm},
booktitle={2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)},
title={Improving Data Quality for Regression Test Selection by Reducing Annotation Noise},
year={2020},
volume={},
number={},
pages={191-194},
abstract={Big data and machine learning models have been increasingly used to support software engineering processes and practices. One example is the use of machine learning models to improve test case selection in continuous integration. However, one of the challenges in building such models is the identification and reduction of noise that often comes in large data. In this paper, we present a noise reduction approach that deals with the problem of contradictory training entries. We empirically evaluate the effectiveness of the approach in the context of selective regression testing. For this purpose, we use a curated training set as input to a tree-based machine learning ensemble and compare the classification precision, recall, and f-score against a non-curated set. Our study shows that using the noise reduction approach on the training instances gives better results in prediction with an improvement of 37% on precision, 70% on recall, and 59% on f-score.},
keywords={Training;Testing;Annotations;Predictive models;Noise reduction;Feature extraction;Dictionaries;Annotation Noise;Regression Testing;Machine Learning Models},
doi={10.1109/SEAA51224.2020.00042},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7363743,
author={Libes, Don and Shin, Seungjun and Woo, Jungyub},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Considerations and recommendations for data availability for data analytics for manufacturing},
year={2015},
volume={},
number={},
pages={68-75},
abstract={Data analytics is increasingly becoming recognized as a valuable set of tools and techniques for improving performance in the manufacturing enterprise. However, data analytics requires data and a lack of useful and usable data has become an impediment to research in data analytics. In this paper, we describe issues that would help aid data availability including data quality, reliability, efficiency, and formats specific to data analytics in manufacturing. To encourage data availability, we present recommendations and requirements to guide future data contributions. We also describe the need for data for challenge problems in data analytics. A better understanding of these needs, recommendations, and requirements may improve the ability of researchers and other practitioners to improve research and more rapidly deploy data analytics in manufacturing.},
keywords={Data analysis;Manufacturing;Sensors;Encryption;NIST;Synchronization;big data;challenge problems;data analytics;data quality;requirements;smart manufacturing},
doi={10.1109/BigData.2015.7363743},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9378401,
author={Makkar, Himanshu and Toshniwal, Durga and Jangra, Shalini},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Closed Itemset based Sensitive Pattern Hiding for Improved Data Utility and Scalability},
year={2020},
volume={},
number={},
pages={4026-4035},
abstract={Frequent itemset mining is used to extract interesting associations and correlations between the itemsets present in transactional datasets. The frequently appearing patterns are used for various business decision making policies, for instance to increase co-purchase of products, price optimization, cross promotion etc. However, there are some sensitive patterns present in datasets that can reveal individual or organisation's specific confidential information that they would not prefer to be known since it can cause them huge social and monetary loss. Privacy Preserving Data Mining (PPDM) approaches are used to hide these sensitive patterns with maintaining the utility of the data. Heuristics-based PPDM approaches are widely adopted sensitive pattern hiding approaches due to their simplicity and lesser computational time as compared to the border-based and exact approaches. However, these approaches causes high side effects concerning the quality of datasets. In this paper, two heuristics-based algorithms, Removal of Closed Sensitive Itemsets with Maximum Support (MaxRCSI) and Removal of Closed Sensitive Itemsets with Minimum Support (MinRCSI), are proposed. In these algorithms, data sanitization is performed over closed sensitive itemsets to improve the utility of sanitized data. The proposed algorithms are parallelized on Spark parallel computing framework to deal with the massive amount of data i.e. big data. Experiments performed on real and synthetic datasets show that the proposed algorithms preserve the privacy of datasets with substantially better utility as compared to the traditional algorithms with less execution time.},
keywords={Data privacy;Itemsets;Heuristic algorithms;Scalability;Big Data;Parallel processing;Sparks;Privacy Preserving Data Mining;Data Sanitisation;Reduced Sensitive itemset;Data Quality},
doi={10.1109/BigData50022.2020.9378401},
ISSN={},
month={Dec},}
@ARTICLE{8469815,
author={Mi, Jun and Wang, Kun and Li, Peng and Guo, Song and Sun, Yanfei},
journal={IEEE Communications Magazine},
title={Software-Defined Green 5G System for Big Data},
year={2018},
volume={56},
number={11},
pages={116-123},
abstract={The 5G system has been recognized as the most promising technology to provide high-quality network services. As a huge number of networking and computing equipments that generate big data are integrated into the 5G system, energy efficiency becomes the major challenge in building a green 5G system. In this article, we propose a software-defined green 5G system for big data, which consists of three planes: the control plane, the data plane and the energy plane. The data plane contains networking and computing equipments, which can be powered by both traditional grid and renewable energy sources in the energy plane. The control plane monitors the system status and configures the corresponding equipments to achieve energy efficiency and quality-of-service. Furthermore, to reduce the overhead of this software- defined architecture, we investigate a FRS to eliminate redundant system monitoring information. To integrate features in software-defined architecture, we propose an AIFS to mine latent rules among features. Simulation results indicate that our proposals achieve higher efficiency in the green 5G system.},
keywords={Monitoring;5G mobile communication;Green products;Big Data;Renewable energy sources;Quality of service;Computer architecture},
doi={10.1109/MCOM.2017.1700048},
ISSN={1558-1896},
month={November},}
@INPROCEEDINGS{9899144,
author={Mei, Hong and Kang, Zhiwen and Li, Haiou and Liu, Yajun and Chen, Xin and Chen, Yanlei and Song, Guolin and Chao, Yuling and Jang, Wenjie and Cheng, Zhanyu and Xu, Jinpeng and Wu, Yi},
booktitle={CIBDA 2022; 3rd International Conference on Computer Information and Big Data Applications},
title={Application analysis of data quality verification system based on resource management platform},
year={2022},
volume={},
number={},
pages={1-4},
abstract={With the rapid development of China's telecom industry, the business is more and more network resources being unwound, data growth speed faster and faster, data scale is becoming more and more large, then the data quality problem is increasingly serious, how to efficient use of existing resources, to provide a set of scientific and effective data management solutions, has become a network resources competitiveness indispensable important segment. This paper takes the application of data quality verification system of resource management platform as the main research object, discusses from system design, software architecture, database design, system function application and network resource related business, and puts forward a series of feasible schemes about data governance.},
keywords={},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{7325916,
author={Shen, Yang and Wang, Yong and Lv, Haitao},
booktitle={2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
title={Thin cloud removal for Landsat 8 OLI data using independent component analysis},
year={2015},
volume={},
number={},
pages={921-924},
abstract={Using independent component analysis (ICA) coupled with the quality assessment (QA) band of Landsat 8, an approach for thin cloud removal in Landsat 8 operational land imager (OLI) data was developed. After the ICA transformation of the visible, near infrared, short-wavelength and cirrus bands of OLI data, cloud component was identified by the mixing matrix. Then, a cloud mask derived from the analysis of the QA band was formed such that an image pixel with and without cloud cover was delineated. The cloud component and cloud mask were used to remove the thin clouds. Thin clouds disappeared visually within the OLI data. Using another cloud-free image acquired in the previous overflight as the reference, we assessed the accuracy level of the cloud removal. Before and after the cloud removal, the spatial correlation coefficients increased from 0.69 to 0.83 in band 1, 0.75 to 0.86 in band 2, 0.81 to 0.88 in band 3, 0.87 to 0.91 in band 4, and no change in bands 5, 6, and 7 for pixels identified with cloud cover.},
keywords={Clouds;Remote sensing;Satellites;Earth;Image color analysis;Independent component analysis;Algorithm design and analysis;Independent component analysis (ICA);Landsat 8;Operational land imager (OLI) data;Quality assessment (QA) band;Thin clouds and their removal},
doi={10.1109/IGARSS.2015.7325916},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{9071249,
author={Liu, Hong and Sang, Zhenhua and Karali, Sameer},
booktitle={2019 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={Approximate Quality Assessment with Sampling Approaches},
year={2019},
volume={},
number={},
pages={1306-1311},
abstract={Data is useful to the extent that it can be quickly analyzed to reveal valuable information. With high-quality data, we can increase revenue, reduce cost, and reduce risk. On the other hand, the consequences of poor-quality data can be severe. It has been estimated that poor quality customer data costs U.S. businesses $611 billion annually in postage, printing, and staff overhead. These issues make data quality assessment a necessary and critical step in any data-related systems. Big data brings new challenges to data quality assessment due to the scale of data, streaming data, and different forms of data. Therefore, we proposed a sampling-based approximate quality assessment model on large data. Sampling large datasets can make all quality assessment processes cheaper and more feasible because of data reduction. The protocol of this work: First, the sample size is determined for estimating a large dataset. Next, sampling techniques are applied to collect samples. Then, these samples are used to estimate the quality of the large dataset. The objective of quality assessment in this work is to evaluate the completeness, accuracy, and timeliness of data and to return fast and approximate scores. Using different sample sizes and different sampling methods, we obtained 72 sets of data and compared them. These results show that the proposed approach is efficient and provides some insight into the quality assessment with samples.},
keywords={Data integrity;Quality assessment;Writing;Gaussian distribution;Big Data;Sampling methods;Time-frequency analysis;Data Quality, Quality Assessment, Sampling},
doi={10.1109/CSCI49370.2019.00244},
ISSN={},
month={Dec},}
@ARTICLE{9057697,
author={Lattuada, Marco and Barbierato, Enrico and Gianniti, Eugenio and Ardagna, Danilo},
journal={IEEE Transactions on Cloud Computing},
title={Optimal Resource Allocation of Cloud-Based Spark Applications},
year={2022},
volume={10},
number={2},
pages={1301-1316},
abstract={Nowadays, the big data paradigm is consolidating its central position in the industry, as well as in society at large. Lots of applications, across disparate domains, operate on huge amounts of data and offer great advantages both for business and research. According to analysts, cloud computing adoption is steadily increasing to support big data analyses and Spark is expected to take a prominent market position for the next decade. As big data applications gain more and more importance over time and given the dynamic nature of cloud resources, it is fundamental to develop an intelligent resource management system to provide Quality of Service guarantees to end-users. This article presents a set of run-time optimization-based resource management policies for advanced big data analytics. Users submit Spark applications characterized by a priority and by a hard or soft deadline. Optimization policies address two scenarios: i) identification of the minimum capacity to run a Spark application within the deadline; ii) re-balance of the cloud resources in case of heavy load, minimising the weighted soft deadline application tardiness. The solution relies on an initial non-linear programming model formulation and a search space exploration based on simulation-optimization procedures. Spark application execution times are estimated by relying on a gamut of techniques, including machine learning, approximated analyses, and simulation. The benefits of the approach are evaluated on Microsoft Azure HDInsight and on a private cloud cluster based on POWER8 by considering the TPC-DS industry benchmark and SparkBench. The results obtained in the first scenario demonstrate that the percentage error of the prediction of the optimal resource usage with respect to system measurement and exhaustive search is in the range 4–29 percent while literature-based techniques present an average error in the range 6–63 percent. Moreover, in the second scenario, the proposed algorithms can address complex problems like computing the optimal redistribution of resources among tens of applications in less than a minute with an error of 8 percent on average. On the same considered tests, literature-based approaches obtain an average error of about 57 percent.},
keywords={Cloud computing;Sparks;Big Data;Task analysis;Resource management;Computational modeling;Optimization;Big data;quality of service;elastic resource provisioning;cluster management},
doi={10.1109/TCC.2020.2985682},
ISSN={2168-7161},
month={April},}
@INPROCEEDINGS{7840737,
author={Huang, Zhichuan and Xie, Tiantian and Zhu, Ting and Wang, Jianwu and Zhang, Qingquan},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Application-driven sensing data reconstruction and selection based on correlation mining and dynamic feedback},
year={2016},
volume={},
number={},
pages={1322-1327},
abstract={As sensors spread across almost every industry, the Internet of Things (IoT) is going to trigger an era of big data. However, the abundance of available sensing data causes new challenges when building IoT applications. One main challenge is how to select proper data from large amount of sensing data for learning useful information efficiently. Existing approaches require developers to manage data for each specific application, which is very time consuming since the developers may not have enough knowledge about the dynamic changing data quality of different sensors. In this paper, we propose a data management middleware to learn the correlations between time series sensor data without prior knowledge. The learned correlation is then applied to select the useful sensor and reconstruct the incorrect data. To generalize the correlation models for each application, we utilize the dynamic feedback from the application to update the data selection and reconstruction. We evaluate our data management middleware in smart grids. The evaluation results show that our middleware can achieve better application performance with the help of dynamic feedback, data reconstruction and data selection.},
keywords={},
doi={10.1109/BigData.2016.7840737},
ISSN={},
month={Dec},}
@ARTICLE{7809119,
author={Hildebrandt, Kai and Panse, Fabian and Wilcke, Niklas and Ritter, Norbert},
journal={IEEE Transactions on Big Data},
title={Large-Scale Data Pollution with Apache Spark},
year={2020},
volume={6},
number={2},
pages={396-411},
abstract={Because of the increasing volume of autonomously collected data objects, duplicate detection is an important challenge in today's data management. To evaluate the efficiency of duplicate detection algorithms with respect to big data, large test data sets are required. Existing test data generation tools, however, are either not able to produce large test data sets or are domain-dependent which limits their usefulness to a few cases. In this paper, we describe a new framework that can be used to pollute a clean, homogeneous and large data set from an arbitrary domain with duplicates, errors and inhomogeneities. To prove its concept, we implemented a prototype which is built upon the cluster computing framework Apache Spark and evaluate its performance in several experiments.},
keywords={Big data;Pollution;Databases;Generators;Prototypes;Gold;Standards;Data quality;duplicate detection;data pollution;Apache Spark},
doi={10.1109/TBDATA.2016.2637378},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{9491733,
author={Alzyadat, Wael and AlHroob, Aysh and Almukahel, Ikhlas Hassan and Muhairat, Mohammad and Abdallah, Mohammad and Althunibat, Ahmad},
booktitle={2021 International Conference on Information Technology (ICIT)},
title={Big Data, Classification, Clustering and Generate Rules: An inevitably intertwined for Prediction},
year={2021},
volume={},
number={},
pages={149-155},
abstract={Big Data filed is an unsettled standard comparing with a traditional database, data mining, or data warehouse. Stability measure aims to acquire the quality dataset which encourages to use of preprocessing data method to handle instability that miniaturization missing data. Therefore, to increase the data quality in order to achieve an accurate prediction, significant rules are used to provide value and meaningful data. Through, three measures by support, confidence, and the lift to acquire frequently rules. These rules are used to conduct the objective extracting pattern, to estimate each browsing customer's likelihood of making a purchase, and to choose meaningful patterns from the discovered association rules.},
keywords={Databases;Data integrity;Big Data;Data warehouses;Data mining;Information technology;Standards;classification;marketing;association rules;Big Data;k-mean;prediction;preprocess},
doi={10.1109/ICIT52682.2021.9491733},
ISSN={},
month={July},}
@ARTICLE{8820081,
author={Gong, Xiaowen and Shroff, Ness B.},
journal={IEEE/ACM Transactions on Networking},
title={Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality},
year={2019},
volume={27},
number={5},
pages={1959-1972},
abstract={Mobile crowdsensing has found a variety of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users' data (e.g., users' received SNRs for measuring a transmitter's transmit signal strength). However, the quality of a user can be its private information (which, e.g., may depend on the user's location) that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data's accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data's accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation (QEE), which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user's data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester's optimal (RO) effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user's quality and the quality's distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.},
keywords={Task analysis;Transmitters;Signal to noise ratio;Data integrity;Sensors;IEEE transactions;Big Data;Crowdsensing;truthful incentive mechanism;data quality},
doi={10.1109/TNET.2019.2934026},
ISSN={1558-2566},
month={Oct},}
@ARTICLE{9302878,
author={Song, Shaoxu and Gao, Fei and Huang, Ruihong and Wang, Chaokun},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Data Dependencies Extended for Variety and Veracity: A Family Tree},
year={2022},
volume={34},
number={10},
pages={4717-4736},
abstract={Besides the conventional schema-oriented tasks, data dependencies are recently revisited for data quality applications, such as violation detection, data repairing and record matching. To address the variety and veracity issues of big data, data dependencies have been extended as data quality rules to adapt to various data types, ranging from (1) categorical data with equality relationships to (2) heterogeneous data with similarity relationships, and (3) numerical data with order relationships. In this survey, we briefly review the recent proposals on data dependencies categorized into the aforesaid types of data. In addition to (a) the concepts of these data dependency notations, we investigate (b) the extension relationships between data dependencies, e.g., conditional functional dependencies (CFDs) extend the conventional functional dependencies (FDs). It forms a family tree of extensions, mostly rooted in FDs, helping us understand the expressive power of various data dependencies. Moreover, we summarize (c) the discovery of dependencies from data, since data dependencies are often unlikely to be manually specified in a traditional way, given the huge volume and high variety of big data. We further outline (d) the applications of the extended data dependencies, in particular in data quality practice. It guides users to select proper data dependencies with sufficient expressive power and reasonable discovery cost. Finally, we conclude with several directions of future studies on the emerging data.},
keywords={Big Data;Phase frequency detectors;Lakes;Picture archiving and communication systems;Databases;Task analysis;Proposals;Integrity constraints;data dependencies},
doi={10.1109/TKDE.2020.3046443},
ISSN={1558-2191},
month={Oct},}
@INPROCEEDINGS{8113071,
author={Yu, Weiqing and Zhu, Wendong and Liu, Guangyi and Kan, Bowen and Zhao, Ting and Liu, He},
booktitle={2017 3rd International Conference on Big Data Computing and Communications (BIGCOM)},
title={Cluster-Based Best Match Scanning for Large-Scale Missing Data Imputation},
year={2017},
volume={},
number={},
pages={232-238},
abstract={High-quality data are the prerequisite for analyzing and using big data to guarantee the value of the data. Missing values in data is a common yet challenging problem in data analytics and data mining, especially in the era of big data. Amount of missing values directly affects the data quality. Therefore, it is critical to properly recover missing values in the dataset. This paper presents a new imputation algorithm called Cluster-based Best Match Scanning (CBMS) designed for Big Data. It is a modification of k-NN imputation. CBMS focuses on recovering continuous numeric missing values, and aims at balancing computational complexity and accuracy. As an imputation algorithm, it can potentially reduce the time complexity of k-NN from O(n^2*d) to O(n^1.5*d), and also reduce the space/memory usage, while perform no worse than k-NN imputation. On top of that CBMS is highly parallelizable.Simulation of CBMS is conducted on smart meter reading data. Data is manually divided into training set and testing set, and testing accuracy is evaluated by computing the mean absolute deviation. Comparison with linear interpolation and k-NN imputation is made to demonstrate the power and effectiveness of our proposed CBMS algorithm.},
keywords={Time complexity;Clustering algorithms;Algorithm design and analysis;Estimation;Big Data;Data mining;big data;cluster-based best match scanning;data imputation;k-NN},
doi={10.1109/BIGCOM.2017.48},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8258225,
author={Tayeb, Shahab and Pirouz, Matin and Cozzens, Brittany and Huang, Richard and Jay, Maxwell and Khembunjong, Kyle and Paliskara, Sahan and Zhan, Felix and Zhang, Mark and Zhan, Justin and Latifi, Shahram},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Toward data quality analytics in signature verification using a convolutional neural network},
year={2017},
volume={},
number={},
pages={2644-2651},
abstract={Many studies have been conducted on Handwritten Signature Verification. Researchers have taken many different approaches to accurately identify valid signatures from skilled forgeries, which closely resemble the real signature. The purpose of this paper is to suggest a method for validating written signatures on bank checks. This model uses a convolutional neural network (CNN) to analyze pixels from a signature image to recognize abnormalities. We believe the feature extraction capabilities of a CNN can optimize processing time and feature analysis of signature verification. Unique characteristics from signatures can be accurately and rapidly analyzed with multiple layers of receptive fields and hidden layers. Our method was able to correctly detect the validity of the inputted signature approximately 83 percent of the time. We tested our method using the SIGCOMP 2011 dataset. The main contribution of this method is to detect and decrease fraud committed, especially in the banking industry. Future uses of signature verification could include legal documents and the justice system.},
keywords={Neural networks;Image recognition;Feature extraction;Forgery;Machine learning;convolutional neural network;handwriting;deep learning;signature authentication;signature verification;machine learning;image classifier},
doi={10.1109/BigData.2017.8258225},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8791314,
author={Qiao, Lin and Chen, Shuo and Bo, Jue and Liu, Sai and Ma, Guiwei and Wang, Haixin and Yang, Junyou},
booktitle={2019 IEEE International Conference on Energy Internet (ICEI)},
title={Wind power generation forecasting and data quality improvement based on big data with multiple temporal-spatual scale},
year={2019},
volume={},
number={},
pages={554-559},
abstract={Wind energy is one of the renewable energy sources with a large number of installations in the world. The accuracy of power generation prediction using wind speed data severely challenges the regulation and safe operation of power system. Since there are many time points in the dispatching strategy of power system, which is related to the area condition. It is of great significance for power grid dispatching to be able to timely and accurately predict the generation capacity of wind turbines in a certain period. Due to the randomness and intermittency of wind speed, the accuracy of data quality will be influenced greatly. In this paper, a neural network algorithm based on combination of back propagation (BP) and Newton interpolation mathematical function method is proposed to effectively process wind speed data, so as to predict power generation. BP neural network is a kind of multi-layer feedforward neural network including a hidden layer, which can solve the learning problem of hidden layer connection weight in a multi-layer network. From the perspective of space scale, this paper studies different wind speed data at different heights in the same area. Research results show: compared with the traditional support vector machine method, the accuracy with the proposed method is improved by 3.1%.},
keywords={Wind speed;Support vector machines;Biological neural networks;Indexes;Wind turbines;Mathematical model},
doi={10.1109/ICEI.2019.00104},
ISSN={},
month={May},}
@INPROCEEDINGS{9005484,
author={Wang, Qiyao and Wang, Haiyan and Gupta, Chetan and Serita, Susumu},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Regularized Operating Envelope with Interpretability and Implementability Constraints},
year={2019},
volume={},
number={},
pages={1506-1516},
abstract={Operating envelope is an important concept in industrial operations. Accurate identification for operating envelope can be extremely beneficial to stakeholders as it provides a set of operational parameters that optimizes some key performance indicators (KPI) such as product quality, operational safety, equipment efficiency, environmental impact, etc. Given the importance, data-driven approaches for computing the operating envelope are gaining popularity. These approaches typically use classifiers such as support vector machines, to set the operating envelope by learning the boundary in the operational parameter spaces between the manually assigned `large KPI' and `small KPI' groups. One challenge to these approaches is that the assignment to these groups is often ad-hoc and hence arbitrary. However, a bigger challenge with these approaches is that they don't take into account two key features that are needed to operationalize operating envelopes: (i) interpretability of the envelope by the operator and (ii) implementability of the envelope from a practical standpoint. In this work, we propose a new definition for operating envelope which directly targets the expected magnitude of KPI (i.e., no need to arbitrarily bin the data instances into groups) and accounts for the interpretability and the implementability. We then propose a regularized `GA +penalty' algorithm that outputs an envelope where the user can tradeoff between bias and variance. The validity of our proposed algorithm is demonstrated by two sets of simulation studies and an application to a real-world challenge in the mining processes of a flotation plant.},
keywords={Genetic algorithms;Machine learning;Oils;Optimization;Search problems;Big Data;Quality assessment;Operating envelope;Genetic algorithm;Penalty approach;Generalization},
doi={10.1109/BigData47090.2019.9005484},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7870183,
author={Clarke, Roger},
booktitle={2016 European Intelligence and Security Informatics Conference (EISIC)},
title={Quality Assurance for Security Applications of Big Data},
year={2016},
volume={},
number={},
pages={1-8},
abstract={The quality of inferences drawn from data, big or small, is heavily dependent on the quality of the data and the quality of the processes applied to it. Big data analytics is emerging from laboratories and being applied to intelligence and security needs. To achieve confidence in the outcomes of these applications, a quality assurance framework is needed. This paper outlines the challenges, and draws attention to the consequences of misconceived and misapplied projects. It presents key aspects of the necessary risk assessment and risk management approaches, and suggests opportunities for research.},
keywords={Big data;Q-factor;Sociology;Statistics;Security;Reliability;Quality assurance;risk assessment;risk management;information quality;data semantics;data scrubbing;decision quality;transparency},
doi={10.1109/EISIC.2016.010},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9145071,
author={Byabazaire, John and O'Hare, Gregory and Delaney, Declan},
booktitle={2020 IEEE International Conference on Communications Workshops (ICC Workshops)},
title={Data Quality and Trust : A Perception from Shared Data in IoT},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Internet of Things devices and data sources areseeing increased use in various application areas. The pro-liferation of cheaper sensor hardware has allowed for widerscale data collection deployments. With increased numbers ofdeployed sensors and the use of heterogeneous sensor typesthere is increased scope for collecting erroneous, inaccurate orinconsistent data. This in turn may lead to inaccurate modelsbuilt from this data. It is important to evaluate this data asit is collected to determine its validity. This paper presents ananalysis of data quality as it is represented in Internet of Things(IoT) systems and some of the limitations of this representation. The paper discusses the use of trust as a heuristic to drive dataquality measurements. Trust is a well-established metric that hasbeen used to determine the validity of a piece or source of datain crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework forrepresenting data quality effectively within the big data modeland why a trust backed framework is important especially inheterogeneously sourced IoT data streams.},
keywords={Data integrity;Data models;Big Data;Biological system modeling;Ecosystems;Standards},
doi={10.1109/ICCWorkshops49005.2020.9145071},
ISSN={2474-9133},
month={June},}
@INPROCEEDINGS{8592556,
author={Hao, Jiao and Jinming, Chen and Yajuan, Guo},
booktitle={2018 China International Conference on Electricity Distribution (CICED)},
title={Data-driven lean Management for Distribution Network},
year={2018},
volume={},
number={},
pages={701-705},
abstract={This paper proposes a concept of “data-driven, lean-oriented and closed-loop” management for distribution network and explain how to implement this kind of management, as shown in fig.1 Firstly, a big data platform is constructed to integrate and combine the multi-source data. Secondly, big data analysis technologies such as data mining, machine learning and data visualization are applied to solve problems in distribution network production. For example, accurate location of the fault can be found with help of multisource information from different devices and systems. And we can also be aware of the risk points in distribution network through history data analysis. Finally, this Paper explains how to promote lean management of distribution network in the fields of asset, operation, maintenance and investment based on the big data platform and big data analysis methods. In addition, the feedback procedure sets up a bridge between application and data collecting, which further improve the data quality. Those management measure have been piloted in several cities in Jiangsu. The result proves that they can improve power supply reliability and reduce operating costs significantly. Two practical cases are given to show how they work.},
keywords={Big Data;Maintenance engineering;Data models;Investment;Fault diagnosis;Poles and towers;data-driven;lean management;closed-loop;big data analysis},
doi={10.1109/CICED.2018.8592556},
ISSN={2161-749X},
month={Sep.},}
@ARTICLE{8847467,
author={Zhao, Cong and Yang, Shusen and McCann, Julie A.},
journal={IEEE Transactions on Mobile Computing},
title={On the Data Quality in Privacy-Preserving Mobile Crowdsensing Systems with Untruthful Reporting},
year={2021},
volume={20},
number={2},
pages={647-661},
abstract={The proliferation of mobile smart devices with ever improving sensing capacities means that human-centric Mobile Crowdsensing Systems (MCSs) can economically provide a large scale and flexible sensing solution. The use of personal mobile devices is a sensitive issue, therefore it is mandatory for practical MCSs to preserve private information (the user's true identity, precise location, etc.) while collecting the required sensing data. However, well intentioned privacy protection techniques also conceal autonomous, or even malicious, behaviors of device owners (termed as self-interested), where the objectivity and accuracy of crowdsensing data can therefore be severely threatened. The issue of data quality due to untruthful reporting in privacy-preserving MCSs has been yet to produce solutions. Bringing together game theory, algorithmic mechanism design, and truth discovery, we develop a mechanism to guarantee and enhance the quality of crowdsensing data without jeopardizing the privacy of MCS participants. Together with solid theoretical justifications, we evaluate the performance of our proposal with extensive real-world MCS trace-driven simulations. Experimental results demonstrate the effectiveness of our mechanism on both enhancing the quality of the crowdsensing data and eliminating the motivation of MCS participants, even when their privacy is well protected, to report untruthfully.},
keywords={Sensors;Data integrity;Data privacy;Task analysis;Mobile handsets;Roads;Monitoring;Mobile crowdsensing systems;privacy preservation;data quality;untruthful reporting},
doi={10.1109/TMC.2019.2943468},
ISSN={1558-0660},
month={Feb},}
@INPROCEEDINGS{9251151,
author={Lu, Jian and Li, Wei and Wang, Qingren and Zhang, Yiwen},
booktitle={2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
title={Research on Data Quality Control of Crowdsourcing Annotation: A Survey},
year={2020},
volume={},
number={},
pages={201-208},
abstract={It is well known that many intelligent and computer-hard tasks cannot be effectively addressed by existing machine-based approaches, so that it is nature to think of utilizing the intelligence of human being. With the popularization of crowdsourcing concepts as well as the development of crowdsourcing platforms, as a new way of human intelligence to participate in machine computing, crowdsourcing annotation helps more and more supervised-learning-based approaches easily obtain enormous labeled data with relatively low cost. However, because of the diversity of the crowd employed by crowdsourcing platforms, how to control qualities of labels coming from the crowd plays a key role in crowdsourcing annotation. In this survey, we first present basic concepts and definitions of crowdsourcing annotation. Then, we review existing ground truth inference algorithms and learning models. After that, the advantages and distinctions among these algorithms and learning models as well as the levels of study progresses will be reported. And finally, we summarize realworld datasets widely utilized in the field of crowdsourcing annotation as well as available open source tools.},
keywords={Crowdsourcing;Training;Annotations;Big Data;Tools;Inference algorithms;Task analysis;label;truth inference;learning model;crowdsourcing},
doi={10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00044},
ISSN={},
month={Aug},}
@ARTICLE{7914196,
author={Ding, Xiaoou and Wang, Hongzhi and Gao, Yitong and Li, Jianzhong and Gao, Hong},
journal={Tsinghua Science and Technology},
title={Efficient currency determination algorithms for dynamic data},
year={2017},
volume={22},
number={3},
pages={227-242},
abstract={Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.},
keywords={Heuristic algorithms;Remuneration;Real-time systems;Databases;Big Data;data quality management; data currency; dynamic determining},
doi={10.23919/TST.2017.7914196},
ISSN={1007-0214},
month={June},}
@INPROCEEDINGS{8109143,
author={Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang},
booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)},
title={ScienceDB: A Public Multidisciplinary Research Data Repository for eScience},
year={2017},
volume={},
number={},
pages={248-255},
abstract={Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.},
keywords={Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data},
doi={10.1109/eScience.2017.38},
ISSN={},
month={Oct},}
@ARTICLE{6527249,
author={Wigan, Marcus R. and Clarke, Roger},
journal={Computer},
title={Big Data's Big Unintended Consequences},
year={2013},
volume={46},
number={6},
pages={46-53},
abstract={Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article "Big Data's Big Unintended Consequences" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.},
keywords={Information management;Data handling;Data storage systems;Government policies;Databases;Business;Legal aspects;Data privacy;policy;privacy;data;social impact;big data;private data commons},
doi={10.1109/MC.2013.195},
ISSN={1558-0814},
month={June},}
@ARTICLE{8553660,
author={Shen, Jian and Zhou, Tianqi and Wang, Kun and Peng, Xin and Pan, Li},
journal={IEEE Network},
title={Artificial Intelligence Inspired Multi-Dimensional Traffic Control for Heterogeneous Networks},
year={2018},
volume={32},
number={6},
pages={84-91},
abstract={The heterogeneous network is the foundation of next-generation networks. It aims to explore the existing network resources effectively, and providing better QoS for every kind of traffic flow as far as possible. However, the diversity and dynamic nature of heterogeneous networks will bring a huge burden and big data to the network traffic control. Therefore, how to achieve efficient and intelligent network traffic control becomes the key problem of heterogeneous networks. In this article, an AI-inspired traffic control scheme is proposed. In order to realize fine-grained traffic control in heterogeneous networks, multi-dimensional (i.e., inter-layer, intra-layer, and caching and pushing) network traffic control is introduced. It is worth noting that backpropagation in deep recurrent neural networks is applied in the intra-layer such that an intelligent traffic control scheme can be derived efficiently when facing the huge traffic load in heterogeneous networks. Moreover, DBSCAN is adopted in the inter-layer, which supports efficient classification in the inter-layer. In addition, caching and pushing is adopted to make full use of network resources and provide better QoS. Simulation results demonstrate the effectiveness and practicability of the proposed scheme.},
keywords={Heterogeneous networks;Backpropagation;Telecommunication traffic;Big Data;Neural networks;Traffic control;Intelligent networks;Big Data;Quality of service;Networked control systems;Recurrent neural networks},
doi={10.1109/MNET.2018.1800120},
ISSN={1558-156X},
month={November},}
@INPROCEEDINGS{7336197,
author={Shi, Weiwei and Zhu, Yongxin and Zhang, Jinkui and Tao, Xiang and Sheng, Gehao and Lian, Yong and Wang, Guoxing and Chen, Yufeng},
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems},
title={Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction},
year={2015},
volume={},
number={},
pages={417-422},
abstract={Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.},
keywords={Support vector machines;Data models;Predictive models;Training;Data mining;Feature extraction;Power grids;missing data prediction;machine learning;support vector machine (SVM);power transformer},
doi={10.1109/HPCC-CSS-ICESS.2015.16},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7167407,
author={Casale, Giuliano and Ardagna, Danilo and Artac, Matej and Barbier, Franck and Di Nitto, Elisabetta and Henry, Alexis and Iuhasz, Gabriel and Joubert, Christophe and Merseguer, Jose and Munteanu, Victor Ion and Perez, Juan Fernando and Petcu, Dana and Rossi, Matteo and Sheridan, Craig and Spais, Ilias and Vladuic, Daniel},
booktitle={2015 IEEE/ACM 7th International Workshop on Modeling in Software Engineering},
title={DICE: Quality-Driven Development of Data-Intensive Cloud Applications},
year={2015},
volume={},
number={},
pages={78-83},
abstract={Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.},
keywords={Unified modeling language;Big data;Data models;Computational modeling;Analytical models;Reliability;Software;Big Data;quality assurance;model-driven engineering},
doi={10.1109/MiSE.2015.21},
ISSN={2156-7891},
month={May},}
@INPROCEEDINGS{9724540,
author={Xue, Hui and Sun, Bo and Si, Chengxiang and Zhang, Wei and Fang, Jing},
booktitle={2021 IEEE 15th International Conference on Big Data Science and Engineering (BigDataSE)},
title={SDCCD: Spatiotemporal Data Cleaning based on Collision Detection},
year={2021},
volume={},
number={},
pages={128-134},
abstract={In the era of big data, data resources are becoming more and more abundant, and the quality of data is getting more and more attention. Data cleaning is the process of identifying and processing dirty data in order to improve the quality of data, which is conducive to make full use of the collected data and ensure the effectiveness and accuracy of the follow-up data analysis. Spatiotemporal data is a kind of time series data, and its cleaning has been widely studied. However, existing methods based on smoothing and statistics are often only suitable for dense spatiotemporal datasets. In this paper, we propose a general spatiotemporal data cleaning method based on collision detection (SDCCD), which is suitable for both dense and sparse spatiotemporal datasets. Experiments on real spatiotemporal datasets show that SDCCD can effectively detect and process spatiotemporal collision records in spatiotemporal datasets.},
keywords={Smoothing methods;Data analysis;Conferences;Time series analysis;Big Data;Cleaning;Spatiotemporal phenomena;collision detect;location;spatiotemporal data;data clean},
doi={10.1109/BigDataSE53435.2021.00027},
ISSN={},
month={Oct},}
@ARTICLE{9299499,
author={Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei},
journal={CSEE Journal of Power and Energy Systems},
title={A big data cleaning method based on improved CLOF and Random Forest for distribution network},
year={2020},
volume={},
number={},
pages={1-10},
abstract={In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the "misjudgment rate". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.},
keywords={Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing data imputation;LOF;DBSCAN;Random Forest},
doi={10.17775/CSEEJPES.2020.04080},
ISSN={2096-0042},
month={},}
@INPROCEEDINGS{9671677,
author={Qi, Wenting and Chelmis, Charalampos},
booktitle={2021 IEEE International Conference on Big Data (Big Data)},
title={Improving Algorithmic Decision–Making in the Presence of Untrustworthy Training Data},
year={2021},
volume={},
number={},
pages={1102-1108},
abstract={Although data quality is of paramount importance in algorithmic decision–making, most existing methods for supervised classification use training data without ever questioning their fidelity. At the same time, counterfactual explanation approaches widely used for post–hoc explanation of algorithmic decisions may result in unrealistic recommendations when left unconstrained. This work highlights a significant research problem, and introduces a novel framework to improve supervised classification in the presence of untrustworthy data, while offering actionable suggestions when an undesirable decision has been made (e.g., loan application rejection). Evaluation results spanning datasets from different domains demonstrate the superiority of the proposed approach, and its comparative advantage as the percentage of mislabeled instances increases.},
keywords={Data integrity;Conferences;Supervised learning;Training data;Machine learning;Big Data;Data models;counterfactual explanations;data quality;data science;supervised learning},
doi={10.1109/BigData52589.2021.9671677},
ISSN={},
month={Dec},}
@ARTICLE{7762161,
author={Wang, Hongbing and Wang, Lei and Yu, Qi and Zheng, Zibin},
journal={IEEE Transactions on Services Computing},
title={Learning the Evolution Regularities for BigService-Oriented Online Reliability Prediction},
year={2019},
volume={12},
number={3},
pages={398-411},
abstract={Service computing is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which regards a System as a Service, and aims at constructing a robust and value-added complex system by outsourcing external component systems through service composition. The burgeoning Big Service computing just covers the significant challenges in constructing and maintaining a stable service-oriented SoS. A service-oriented SoS runs under a volatile and uncertain environment. As a step toward big service, service fault tolerance (FT) can guarantee the run-time quality of a service-oriented SoS. To successfully deploy FT in an SoS, online reliability time series prediction, which aims at predicting the reliability in near future for a service-oriented SoS arises as a grand challenge in SoS research. In particular, we need to tackle a number of big data related issues given the large and fast increasing size of the historical data that will be used for prediction purpose. The decision-making of prediction solution space be more complex. To provide highly accurate prediction results, we tackle the prediction challenges by identifying the evolution regularities of component systems' running states via different machine learning models. We present in this paper the motifs-based Dynamic Bayesian Networks (or m_DBNs) to perform one-step-ahead online reliability time series prediction. We also propose a multi-steps trajectory DBNs (or multi_DBNs) to further improve the accuracy of future reliability prediction. Finally, a Convolutional Neural Networks (CNN)-based prediction approach is developed to deal with the big data challenges. Extensive experiments conducted on real-world Web services demonstrate that our models outperform other well-known approaches consistently.},
keywords={Reliability;Time series analysis;Web services;Computer network reliability;Meteorology;Big data;Quality of service;Temporal evolution regularities;online reliability prediction;big service;convolutional neural networks},
doi={10.1109/TSC.2016.2633264},
ISSN={1939-1374},
month={May},}
@INPROCEEDINGS{8937930,
author={Togneri, Rodrigo and Camponogara, Glauber and Soininen, Juha-Pekka and Kamienski, Carlos},
booktitle={2019 IEEE Latin-American Conference on Communications (LATINCOM)},
title={Foundations of Data Quality Assurance for IoT-based Smart Applications},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Most current scientific and industrial efforts in IoT are geared towards building integrated platforms to finally realize its potential in commercial scale applications. The IoT and Big Data contemporary context brings a number of challenges, such as providing quality assurance (defined by availability and veracity) for sensor data. Traditional signal processing approaches are no longer sufficient, requiring combined approaches in both architectural and analytical layers. This paper proposes a discussion on the adequate foundations of a new general approach aimed at increasing robustness and antifragility of IoT-based smart applications. In addition, it shows results of preliminary experiments with real data in the context of precision irrigation using multivariate methods to identify relevant situations, such as sensor failures and the mismatch of contextual sensor information due to different spatial granularities capture. Our results provide initial indications of the adequacy of the proposed framework.},
keywords={Irrigation;Quality assurance;Data integrity;Signal processing;Feature extraction;Robustness;Object recognition;Data quality;internet of things;smart applications;precision irrigation},
doi={10.1109/LATINCOM48065.2019.8937930},
ISSN={2330-989X},
month={Nov},}
@INPROCEEDINGS{9215250,
author={Peethambaran, Geetha and Naikodi, Chandrakant and Suresh, L},
booktitle={2020 International Conference on Smart Electronics and Communication (ICOSEC)},
title={An Ensemble Learning Approach for Privacy–Quality–Efficiency Trade-Off in Data Analytics},
year={2020},
volume={},
number={},
pages={228-235},
abstract={Privacy is an issue of concern in the electronic era where data has become a primary source of investment for businesses and organizations. The value generated from data is put to use in a number of ways for economic benefit. Customer profiling is one such instance, where data collected is used for targeted marketing, personalized purchase recommendations and customized product deliveries. In such applications, the risk of individual sensitive information disclosure always prevails, affecting the privacy of individuals involved. Hence privacy preserving analysis demands suppressing or transforming data before it is published for analysis, thus curbing data leak. Subsequently, data quality degrades, and operative analytics is affected. With Big data, algorithms that offer a reasonable qualityprivacy trade off need enhancements in terms of efficiency and scalability. In this paper, the work proposed uses a privacy based composite classifier model to analyze the accuracy of classification. The diverse characteristics of algorithms in the composite classifier are found to balance the classification accuracy that is likely to get affected by privacy model. Further, the model's performance with respect to execution time is then evaluated using the parallel computing framework Spark.},
keywords={Data privacy;Privacy;Support vector machines;Big Data;Classification algorithms;Data models;Data integrity;Privacy;Scalability;Big Data;Spark;Analytics;Privacy Preserving;Performance;Utility;UCI;Composite;Efficiency;Anonymization},
doi={10.1109/ICOSEC49089.2020.9215250},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8622989,
author={Micic, Natasha and Neagu, Daniel and Torgunov, Denis and Campean, Felician},
booktitle={2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
title={Exploring Methods for Comparing Similarity of Dimensionally Inconsistent Multivariate Numerical Data},
year={2018},
volume={},
number={},
pages={1528-1535},
abstract={When developing multivariate data classification and clustering methodologies for data mining, it is clear that most literature contributions only really consider data that contain consistently the same attributes. There are however many cases in current big data analytics applications where for same topic and even same source data sets there are differing attributes being measured, for a multitude of reasons (whether the specific design of an experiment or poor data quality and consistency). We define this class of data a dimensionally inconsistent multivariate data, a topic that can be considered a subclass of the Big Data Variety research. This paper explores some classification methodologies commonly used in multivariate classification and clustering tasks and considers how these traditional methodologies could be adapted to compare dimensionally inconsistent data sets. The study focuses on adapting two similarity measures: Robinson-Foulds tree distance metrics and Variation of Information; for comparing clustering of hierarchical cluster algorithms (such clusters are derived from the raw multivariate data). The results from experiments on engineering data highlight that adapting pairwise measures to exclude non-common attributes from the traditional distance metrics may not be the best method of classification. We suggest that more specialised metrics of similarity are required to address challenges presented by dimensionally inconsistent multivariate data, with specific applications for big engineering data analytics.},
keywords={Measurement;Time series analysis;Feature extraction;Phylogeny;Mutual information;Big Data;Data mining;Similarity measures;Robinson Foulds;Variation of Information;Engineering data;Multivariate numerical data;Dimensional inconsistency},
doi={10.1109/HPCC/SmartCity/DSS.2018.00251},
ISSN={},
month={June},}
@INPROCEEDINGS{9149070,
author={Yang, Wanting and Chi, Xuefen and Zhao, Linlin},
booktitle={ICC 2020 - 2020 IEEE International Conference on Communications (ICC)},
title={Proactive VoD delivery pattern reconfiguration based on temporal-spatial channel prediction},
year={2020},
volume={},
number={},
pages={1-7},
abstract={With the help of big data analytics, predictive resource allocation (PRA) techniques for video on demand (VoD) have been recognized as promising methods to save time-frequency resources, for a number of VoD packets can be transmitted in good channels in advance to avoid the predicted transmissions in bad channel conditions. With the increasing demands on a fantastic user quality of experience, a smooth playback and a low start-up delay are of equal importance to the emerging VoD with high fidelity, which inevitably leads to a critical delay requirement of VoD packets. However, the issue of resource estimation with quality of service (QoS) requirements is still an unsolved puzzle in PRA. In this paper, we propose a martingales-based physical resource block (PRB) abstraction method, where the random characteristics of the service process are embedded in the minimum PRB consumption. Based on the method, a proactive QoS-guaranteed reconfiguration algorithm is developed to optimize the multi-user delivery pattern applied in the prediction window, aiming to maximize spectrum efficiency. In this algorithm, since the delay sensitivity of VoD content transmitted in advance is dulled compared with the original VoD stream, we divide the original VoD slice into two sub-slices and derive a three-dimensional delivery pattern. The gain of resource saving and the capability of QoS guarantee brought by the reconfiguration have been demonstrated by the simulation results.},
keywords={Delays;Quality of service;Prediction algorithms;Resource management;Estimation;Probability;Big Data;channel state prediction;reconfiguration;delivery pattern;martingales;VoD;spectrum efficiency;delay-QoS},
doi={10.1109/ICC40277.2020.9149070},
ISSN={1938-1883},
month={June},}
@INPROCEEDINGS{8572515,
author={Zhang, Lichen},
booktitle={2018 17th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)},
title={Specifying and Modeling Cloud Cyber Physical Systems Based on AADL},
year={2018},
volume={},
number={},
pages={26-29},
abstract={In cyber physical systems(CPS), the physical world and the information world are merged to form a new structure that combines both hardware and software, and become the core technology system that supports and leads the transformation of a new generation of industries. With the rapid development of network technology, the data generated has also increased rapidly, which means that today's information society has entered the era of big data, and the technology of adapting to the cloud platform has gradually matured. The cloud computing platform provides flexible and relatively inexpensive storage space and computing resources for the development of big data technology. This also provides basic support for the development of big data driven CPS based on the cloud platform. In this paper, we specify and model cloud cyber physical systems based on AADL, which can specify, model, and analyze cloud cyber physical systems, finally implement cyber physical systems on cloud platforms, provide availability analysis, reliability analysis. data quality analysis, real-time performance analysis, security analysis and resource consumption analysis.},
keywords={Cloud computing;Computational modeling;Unified modeling language;Analytical models;Computer architecture;Software;Object oriented modeling;cloud;CPS;big data;AADL;specification},
doi={10.1109/DCABES.2018.00017},
ISSN={2473-3636},
month={Oct},}
@INPROCEEDINGS{9377900,
author={Tawakuli, Amal and Kaiser, Daniel and Engel, Thomas},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Synchronized Preprocessing of Sensor Data},
year={2020},
volume={},
number={},
pages={3522-3531},
abstract={Sensor data whether collected for machine learning, deep learning or other applications must be preprocessed to fit input requirements or improve performance and accuracy. Data preparation is an expensive, resource consuming and complex phase often performed centrally on raw data for a specific application. The dataflow between the edge and the cloud can be enhanced in terms of efficiency, reliability and lineage by preprocessing the datasets closer to their data sources. We propose a dedicated data preprocessing framework that distributes preprocessing tasks between a cloud stage and two edge stages to create a dataflow with progressively improving quality. The framework handles heterogenous data and dynamic preprocessing plans simultaneously targeting diverse applications and use cases from different domains. Each stage autonomously executes sensor specific preprocessing plans in parallel while synchronizing the progressive execution and dynamic updates of the preprocessing plans with the other stages. Our approach minimizes the workload on central infrastructures and reduces the resources used for transferring raw data from the edge. We also demonstrate that preprocessing data can be sensor specific rather than application specific and thus can be performed prior to knowing a specific application.},
keywords={Deep learning;Cloud computing;Data preprocessing;Big Data;Synchronization;Reliability;Task analysis;Data Quality;Data Preprocessing;Sensor Data;Edge Computing;Data Management},
doi={10.1109/BigData50022.2020.9377900},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8126976,
author={Han, Weiguo and Jochum, Matthew},
booktitle={2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
title={Latency analysis of large volume satellite data transmissions},
year={2017},
volume={},
number={},
pages={384-387},
abstract={A wide array of time-sensitive satellite data is required in the research and development activities for natural hazard assessment, storms and weather prediction, hurricane tracking, disaster and emergency response, and so on. Identifying and analyzing the latencies of large volumes of real-time and near real-time satellite data is very useful and helpful for detecting transmission issues, managing IT resources, and configuring and optimizing data management systems. This paper introduces how to monitor and collect important timestamps of data transmissions, organize them in a NoSQL database, and explore data latency via a user-friendly dashboard. Taking Sentinel series satellite data as an example, data transmission issues are illustrated and investigated further. Latency analysis and explorations help data providers and managers improve data transmission and enhance data management.},
keywords={Satellites;Data communication;Real-time systems;Databases;Browsers;Big Data;Big Data;Satellite Data;Data Latency;Data Quality;NoSQL;MongoDB},
doi={10.1109/IGARSS.2017.8126976},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{9101464,
author={Swami, Arun and Vasudevan, Sriram and Huyn, Joojay},
booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)},
title={Data Sentinel: A Declarative Production-Scale Data Validation Platform},
year={2020},
volume={},
number={},
pages={1579-1590},
abstract={Many organizations process big data for important business operations and decisions. Hence, data quality greatly affects their success. Data quality problems continue to be widespread, costing US businesses an estimated $600 billion annually. To date, addressing data quality in production environments still poses many challenges: easily defining properties of high-quality data; validating production-scale data in a timely manner; debugging poor quality data; designing data quality solutions to be easy to use, understand, and operate; and designing data quality solutions to easily integrate with other systems. Current data validation solutions do not comprehensively address these challenges. To address data quality in production environments at LinkedIn, we developed Data Sentinel, a declarative production-scale data validation platform. In a simple and well-structured configuration, users declaratively specify the desired data checks. Then, Data Sentinel performs these data checks and writes the results to an easily understandable report. Furthermore, Data Sentinel provides well-defined schemas for the configuration and report. This makes it easy for other systems to interface or integrate with Data Sentinel. To make Data Sentinel even easier to use, understand, and operate in production environments, we provide Data Sentinel Service (DSS), a complementary system to help specify data checks, schedule, deploy, and tune data validation jobs, and understand data checking results. The contributions of this paper include the following: 1) Data Sentinel, a declarative production-scale data validation platform successfully deployed at LinkedIn 2) A generic design to build and deploy similar systems for production environments 3) Experiences and lessons learned that can benefit practitioners with similar objectives.},
keywords={Data integrity;Production;LinkedIn;Big Data;Business;Debugging;Schedules},
doi={10.1109/ICDE48307.2020.00140},
ISSN={2375-026X},
month={April},}
@ARTICLE{9739736,
author={Baoyu, Li and Guoxing, Li and Guiyu, Wang and Guofeng, Zhang and Man, Yang},
journal={IEEE Access},
title={Research on CART Model of Mass Concrete Temperature Prediction Based on Big Data Processing Technology},
year={2022},
volume={10},
number={},
pages={32845-32854},
abstract={Due to the influence of temperature changes or temperature gradients in the construction process of mass concrete, temperature cracks will occur in the concrete. In order to achieve a reasonable prediction of the temperature change of the mass concrete during the construction process and accurately obtain the temperature change trend, this paper attempts to construct a CART prediction model based on the big data processing technology based on the characteristics of the temperature change of the mass concrete. This paper introduces in detail how to use data processing methods such as outlier identification, missing value filling and random error elimination to improve data quality, as well as the method for constructing the CART prediction model, and combines engineering examples to demonstrate the feasibility of the model method. The results show that the model and method can better predict the temperature change of mass concrete. It has high prediction accuracy and can provide necessary guidance for practical engineering.},
keywords={Temperature sensors;Temperature distribution;Temperature measurement;Temperature control;Predictive models;Data models;Cooling;Mass concrete temperature;big data processing technology;CART prediction model},
doi={10.1109/ACCESS.2022.3161556},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9534570,
author={Jie, Lu and Zheng, Su and Qi, Wang and Xiya, Chen},
booktitle={2021 2nd International Conference on Artificial Intelligence and Education (ICAIE)},
title={Analysis of Employment Status and Countermeasures of Biology Graduates in Local Normal Universities Based on Big Data Technology—Take the Graduates of Guangxi Normal University From 20l6 to 2020 as an Example},
year={2021},
volume={},
number={},
pages={572-578},
abstract={The data of annual reports on the employment quality f local normal universities from 2016 to 2020 were captured from the network with the help of information technology. The data related to basic situation and employment destination of biology graduates in Guangxi Normal University were analyzed and collected by data analytic technology such as Data Mining Algorithms, Data Quality Master Data Management and Predictive Analytic Capabilities. In view of the problems of single employment structure, insufficient employment skills, vague employment planning and low achievement of Innovation and Entrepreneurship, measures such as the employment policy interpretation, the talent training programs adjustment, the employment guidance services system improvement and teaching mode innovation are needed in the purpose of promoting employment, providing useful reference to the further progress of teaching reform and optimizing talent training modes and methods for graduates of biology.},
keywords={Training;Technological innovation;Employment;Big Data;Prediction algorithms;Biology;Planning;data analysis;majors concerning biology;graduates;employment quality;local normal universities},
doi={10.1109/ICAIE53562.2021.00127},
ISSN={},
month={June},}
@INPROCEEDINGS{9378343,
author={Moon, Aekyeung and Woo Son, Seung and Jung, Jiuk and Jeong Song, Yun},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Understanding Bit-Error Trade-off of Transform-based Lossy Compression on Electrocardiogram Signals},
year={2020},
volume={},
number={},
pages={3494-3499},
abstract={The growing demand for recording longer ECG signals to improve the effectiveness of IoT-enabled remote clinical healthcare is contributing large amounts of ECG data. While lossy compression techniques have shown potential in significantly lowering the amount of data, investigation on how to trade-off between data reduction and data fidelity on ECG data received relatively less attention. This paper gives insight into the power of lossy compression to ECG signals by balancing between data quality and compression ratio. We evaluate the performance of transformed-based lossy compressions on the ECG datasets collected from the Biosemi ActiveTwo devices. Our experimental results indicate that ECG data exhibit high energy compaction property through transformations like DCT and DWT, thus could improve compression ratios significantly without hurting data fidelity much. More importantly, we evaluate the effect of lossy compression on ECG signals by validating the R-peak in the QRS complex. Our method can obtain low error rates measured in PRD (as low as 0.3) and PSNR (up to 67) using only 5% of the transform coefficients. Therefore, R-peaks in the reconstructed ECG signals are almost identical to ones in the original signals, thus facilitating extended ECG monitoring.},
keywords={Performance evaluation;Measurement uncertainty;Transforms;Medical services;Electrocardiography;Big Data;Monitoring;Transform coding;Lossy compression;IoT;Health care;R-peak;data fidelity},
doi={10.1109/BigData50022.2020.9378343},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9377801,
author={Cai, Xumin and Aydin, Berkay and Maydeo, Saurabh and Ji, Anli and Angryk, Rafal},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Local Outlier Detection for Multi-type Spatio-temporal Trajectories},
year={2020},
volume={},
number={},
pages={4509-4518},
abstract={Outlier detection has become one of the core tasks in spatio-temporal data mining. It plays an essential role in data quality improvement for the machine learning models and recognizing the anomalous patterns, which may remarkably deviate from expected patterns among the trajectory datasets. In this work, we propose a clustering-based technique to detect local outliers in trajectory datasets by utilizing spatial and temporal attributes of moving objects. This local outlier detection involves three phases. In the first phase, we apply a temporal partition procedure to divide the raw trajectory into multiple trajectory segments and extract trajectory features from spatial and temporal attributes for each trajectory segment. Then, we generate template features of trajectory segments by applying a clustering schema in the second phase. Finally, we use the abnormal score - a novel dissimilarity measure, which quantifies the disparity among the query and template trajectory segments in terms of trajectory features and hence determines the local outliers based on the distribution of abnormal score. To demonstrate the effectiveness of our method, we conduct three case studies on the real-life spatio-temporal trajectory datasets from the solar astroinformatics domain (i.e., solar active regions, coronal mass ejections, polarity inversion lines (PIL)). Our experimental results show that our local outlier detection approach can effectively discover the erroneous reports from the reporting module and abnormal phenomenon in various spatio-temporal trajectory datasets.},
keywords={Machine learning;Big Data;Feature extraction;Spatial databases;Trajectory;Task analysis;Anomaly detection},
doi={10.1109/BigData50022.2020.9377801},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9671538,
author={Feric, Zlatan and Agostini, Nicolas Bohm and Beene, Daniel and Signes-Pastor, Antonio J. and Halchenko, Yuliya and Watkins, Deborah and MacKenzie, Debra and Karagas, Margaret and Manjourides, Justin and Alshawabkeh, Akram and Kaeli, David},
booktitle={2021 IEEE International Conference on Big Data (Big Data)},
title={A Secure and Reusable Software Architecture for Supporting Online Data Harmonization},
year={2021},
volume={},
number={},
pages={2801-2812},
abstract={Retrospective data harmonization across multiple research cohorts and studies is frequently done to increase statistical power, provide comparison analysis, and create a richer data source for data mining. However, when combining disparate data sources, harmonization projects face data management and analysis challenges. These include differences in the data dictionaries and variable definitions, privacy concerns surrounding health data representing sensitive populations, and lack of properly defined data models. With the availability of mature open-source web-based database technologies, developing a complete software architecture to overcome the challenges associated with the harmonization process can alleviate many roadblocks. By leveraging state-of-the-art software engineering and database principles, we can ensure data quality and enable cross-center online access and collaboration.This paper outlines a complete software architecture developed and customized using the Django web framework, leveraged to harmonize sensitive data collected from three NIH-support birth cohorts. We describe our framework and show how we successfully overcame challenges faced when harmonizing data from these cohorts. We discuss our efforts in data cleaning, data sharing, data transformation, data visualization, and analytics, while reflecting on what we have learned to date from these harmonized datasets.},
keywords={Dictionaries;Software architecture;Databases;Soft sensors;Data visualization;Big Data;Natural language processing},
doi={10.1109/BigData52589.2021.9671538},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7872993,
author={Pu, Dong-Mei and Gao, Da-Qi and Yuan, Yu-Bo},
booktitle={2016 International Conference on Machine Learning and Cybernetics (ICMLC)},
title={A dynamic data correction algorithm based on polynomial smooth support vector machine},
year={2016},
volume={2},
number={},
pages={820-824},
abstract={Data quality plays an important role in modern intelligent information system and is crucial to any data analysis task. Many imperfection-handling techniques avoid overfitting or simply remove offending portions of the data. Data correction can help to retain and recover as much information as possible from the original data resources. In this paper, we proposed a novel technique based on polynomial smooth support vector machine. The quadratic polynomial and the first degree of polynomial as the support vector machine smooth functions are investigated. At the same time, the function was used as smooth function to calculate compensation values. In order to show the procedures of our algorithm, some necessary steps need to be considered. Firstly, the original data are normalized, so as to eliminate experimental effects of dimensional problems. Secondly, the three different kinds of smooth functions need to be analysed mathematically. The difference measure are calculated to make sure the results of correction through different data correction models. The results of given noised data sets can show that the proposed the data correction method based on polynomial smooth support vector machine is effectiveness.},
keywords={Support vector machines;Heuristic algorithms;Data analysis;Aerodynamics;Cybernetics;Big data;Machine learning algorithms;Data analysis;Data correction;Support vector machine;Data Mining},
doi={10.1109/ICMLC.2016.7872993},
ISSN={2160-1348},
month={July},}
@INPROCEEDINGS{7751640,
author={Mao, Xu and Su, Fei},
booktitle={2016 16th International Symposium on Communications and Information Technologies (ISCIT)},
title={Standards compliance testing on generic data of telecom operators},
year={2016},
volume={},
number={},
pages={302-306},
abstract={The big data is bringing new opportunities to the world. Every traditional telecom operator is exploring new ways to increase revenues and profits from the explosive growth of data traffic, but few have demonstrated the data quality needed for data applications. Standards compliance testing on generic data of telecom operators is the key means to improve competitive quality of comprehensive telecom information services. With analysis of current contradiction between data supply and data demand of telecom operators, this paper defines the generic data of telecom operators and its standards compliance testing, presents the testing procedures and the testing applications, and points out a series of testing research directions.},
keywords={C# languages;Data quality;Telecom operators;Generic data;Standards compliance testing},
doi={10.1109/ISCIT.2016.7751640},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8622349,
author={Sinha, Shweta and Seys, Marcia},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={HL7 Data Acquisition & Integration: Challenges and Best Practices},
year={2018},
volume={},
number={},
pages={2453-2457},
abstract={Lack of interoperability between health data systems is a leading challenge for healthcare in the United States. This paper describes the challenges and lessons learned in the process of incorporating HL7 data and integration with Electronic Health Records and Health Information Exchanges from the perspective of a midsized Health Plan (Payer). As a Health Care Payer, Premera has a unique perspective regarding how health plans can provide the necessary data to complete the picture of care. This paper shares some of the best practices and focus areas for successful implementation of healthcare data integrations. This paper also focuses on integrating claims and clinical data using a master patient index as well as challenges faced in that process.Note that going forward `Health Plan' and `Payer' will be used interchangeably. Also, `Provider(s)', `Hospitals', `Healthcare Providers', `Clinics', `Provider Organizations' will be used interchangeably and in the context of this paper may mean the same.},
keywords={Medical services;Organizations;Standards organizations;Engines;Monitoring;Best practices;Interoperability;HL7;big data;integration;acquisition;master patient index;payer;interoperability;data quality},
doi={10.1109/BigData.2018.8622349},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7214177,
author={Zhu, Hong and Bayley, Ian and Younas, M. and Lightfoot, David and Yousef, Basel and Liu, Dongmei},
booktitle={2015 IEEE 8th International Conference on Cloud Computing},
title={Big SaaS: The Next Step beyond Big Data},
year={2015},
volume={},
number={},
pages={1131-1140},
abstract={Software-as-a-Service (SaaS) is a model of cloud computing in which software functions are delivered to the users as services. The past few years have witnessed its global flourishing. In the foreseeable future, SaaS applications will integrate with the Internet of Things, Mobile Computing, Big Data, Wireless Sensor Networks, and many other computing and communication technologies to deliver customizable intelligent services to a vast population. This will give rise to an era of what we call Big SaaS systems of unprecedented complexity and scale. They will have huge numbers of tenants/users interrelated in complex ways. The code will be complex too and require Big Data but provide great value to the customer. With these benefits come great societal risks, however, and there are other drawbacks and challenges. For example, it is difficult to ensure the quality of data and metadata obtained from crowd sourcing and to maintain the integrity of conceptual model. Big SaaS applications will also need to evolve continuously. This paper will discuss how to address these challenges at all stages of the software lifecycle.},
keywords={Software as a service;Checkpointing;Fault tolerance;Fault tolerant systems;Ontologies;Computer architecture},
doi={10.1109/CLOUD.2015.167},
ISSN={2159-6190},
month={June},}
@INPROCEEDINGS{8859426,
author={He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia},
booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={Quality Driven Judicial Data Governance},
year={2019},
volume={},
number={},
pages={66-70},
abstract={With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.},
keywords={Data integrity;Big Data;Decision making;Organizations;Standards organizations;data quality;judicial data governance;quality measurement},
doi={10.1109/QRS-C.2019.00026},
ISSN={},
month={July},}
@INPROCEEDINGS{9574197,
author={Xue, Lian},
booktitle={2021 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)},
title={Competency Evaluation System of English Teaching Post based on K-Means Clustering Algorithm},
year={2021},
volume={},
number={},
pages={887-891},
abstract={With the rapid development of big data, user data information is increasingly perfect, data warehouse integration is more reasonable, and data quality is constantly improving, so the value of data is increasing. Based on parallel processing of K-means clustering algorithm, this paper extracts ability constraint information, integrates K-means clustering algorithm, clusters and integrates various index parameters of post competency. From the final experimental results, this method improves the execution efficiency and accuracy compared with other methods, and can be used in practice.},
keywords={Electrical engineering;Data integrity;Education;Clustering algorithms;Big Data;Parallel processing;Data warehouses;English Teaching;Post Competency;Clustering Algorithm;System Assessment},
doi={10.1109/AEECA52519.2021.9574197},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9604414,
author={Lee, Jay},
booktitle={2021 IEEE World Congress on Services (SERVICES)},
title={Transformation: Case Studies and Lessons Learned : Keynote 2},
year={2021},
volume={},
number={},
pages={xxiii-xxiii},
abstract={Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Industrial AI, Big Data Analytics, Machine Learning, and Cyber Physical Systems are changing the way we design product, manufacturing, and service systems. It is clear that as more sensors and smart analytics software are integrated in the networked industrial products and manufacturing systems, predictive technologies can further learn and autonomously optimize service productivity and performance. This presentation will address the trends of Industrial AI for smart service realization. First, Industrial AI systematic approach will be introduced. Case studies on advanced predictive analytics technologies for different maintenance and service operations will be demonstrated. In addition, issues on data quality for high performance and real-time data analytics in future digital service will be discussed.},
keywords={Smart manufacturing;US Government;Science - general;Maintenance engineering;Systematics;Software;Sensor systems},
doi={10.1109/SERVICES51467.2021.00055},
ISSN={2642-939X},
month={Sep.},}
@INPROCEEDINGS{8410313,
author={Wang, Sheng and Fu, Lieyong and Yao, Jianmin and Li, Yun},
booktitle={2018 International Conference on Robots & Intelligent System (ICRIS)},
title={The Application of Deep Learning in Biomedical Informatics},
year={2018},
volume={},
number={},
pages={391-394},
abstract={The expansion of big data in biomedical and health field has driven the need of new effective analysis technology. Deep learning is a powerful machine learning method. With the contribution of rapid computational power improvement, it is becoming a promising technique to generate new knowledge, interpretation and gain insights from high-throughout, heterogeneous and complex biomedical data from different sources, such as medical imaging, clinical genomics, and electronic health records. This paper presents an overview of the application of deep learning approach in the biomedical informatics. First we introduce the development of artificial neural network and deep learning, then mainly focus on the researches applying deep learning in biomedical informatics field. We also discuss the challenges for future improvement, such as data quality and interpretability.},
keywords={Conferences;Robots;Intelligent systems;Deep Learning;Healthcare;Biomedical informatics},
doi={10.1109/ICRIS.2018.00104},
ISSN={},
month={May},}
@INPROCEEDINGS{9617238,
author={Bergès, Corinne and Bird, Jim and Shroff, Mehul D. and Rongen, René and Smith, Chris},
booktitle={2021 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA)},
title={Data analytics and machine learning: root-cause problem-solving approach to prevent yield loss and quality issues in semiconductor industry for automotive applications},
year={2021},
volume={},
number={},
pages={1-10},
abstract={Quality requirements in the semiconductor industry for automotive products are increasing rapidly with the movement to autonomous vehicles and higher levels of safety. It is no longer possible to express maximum failure requirements in parts per million (ppm). Individual failing parts observed in the field and reported by customers can trigger a significant quality response. ‘Zero-defect’ (ZD) is no longer considered a utopian ideal, but a required potentially reachable goal for semiconductor manufacturers. Projects and studies that include artificial intelligence and big data, are seen as key drivers to reach a ZD level of quality. Competing objectives targeted in any industrial project, such as quality improvement and gross margin, must also be considered. Initial projects in machine learning (ML), focusing on yield-loss issues, are being deployed within the manufacturing sites. These projects interconnect typical internal data collected from the manufacturing and assembly lines with engineering, qualification and reliability data. For a specific case study of unexpected abnormally high variability on some parameters, this paper presents a problem-solving approach in a big-data environment. Models implemented and results obtained towards root-cause problem solving for this issue, are discussed. This overall approach may be replicated in other ML projects.},
keywords={Solid modeling;Electronics industry;Predictive models;Reliability engineering;Manufacturing;Safety;Problem-solving;Automotive semiconductor industry;root-cause problem-solving;failure prevention;data analytics;machine learning;big data},
doi={10.1109/IPFA53173.2021.9617238},
ISSN={1946-1550},
month={Sep.},}
@INPROCEEDINGS{9035250,
author={Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.},
booktitle={2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA)},
title={Assessing Context-Aware Data Consistency},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.},
keywords={Feature extraction;Data analysis;Data integrity;Data models;Machine learning algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache hadoop;apache spark},
doi={10.1109/AICCSA47632.2019.9035250},
ISSN={2161-5330},
month={Nov},}
@INPROCEEDINGS{9604548,
author={Hasan, Forat Falih and Bakar, Muhamad Shahbani Abu},
booktitle={2021 5th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)},
title={Data Transformation from SQL to NoSQL MongoDB Based on R Programming Language},
year={2021},
volume={},
number={},
pages={399-403},
abstract={Owing to their high availability and scalability, NoSQL databases are becoming more popular for Big data applications in web analytics and supporting large websites. Moreover, each NoSQL system has its API which does not support industry standards like SQL and JDBC, integrating these systems with other enterprise and reporting software takes more time. The main requirements of Big data and data analytics are transforming the data from SQL databases to NoSQL data structures to represent the data. In this work, we presented a method to transform the data from different types of SQL databases to the desired NoSQL database based on the R programming language. The proposed work is based on the R environment used to handle the data from the source system to the target databases and meet the data quality requirements in data transformation. The results confirmed that the development provided a good solution for the data transformation from SQL to NoSQL by taking into account the data quality requirements.},
keywords={Structured Query Language;Computer languages;Data analysis;NoSQL databases;Data integrity;Scalability;Transforms;Big Data;Data Transformation;SABR Algorithm;NoSQL;ETL},
doi={10.1109/ISMSIT52890.2021.9604548},
ISSN={},
month={Oct},}
@ARTICLE{9058606,
author={Lin, Mengting and Zhao, Youping},
journal={China Communications},
title={Artificial intelligence-empowered resource management for future wireless communications: A survey},
year={2020},
volume={17},
number={3},
pages={58-77},
abstract={How to explore and exploit the full potential of artificial intelligence (AI) technologies in future wireless communications such as beyond 5G (B5G) and 6G is an extremely hot inter-disciplinary research topic around the world. On the one hand, AI empowers intelligent resource management for wireless communications through powerful learning and automatic adaptation capabilities. On the other hand, embracing AI in wireless communication resource management calls for new network architecture and system models as well as standardized interfaces/protocols/data formats to facilitate the large-scale deployment of AI in future B5G/6G networks. This paper reviews the state-of-art AI-empowered resource management from the framework perspective down to the methodology perspective, not only considering the radio resource (e.g., spectrum) management but also other types of resources such as computing and caching. We also discuss the challenges and opportunities for AI-based resource management to widely deploy AI in future wireless communication networks.},
keywords={Resource management;Artificial intelligence;5G mobile communication;Wireless communication;Big Data;Quality of service;Network slicing;5G;beyond 5G (B5G);6G;artificial intelligence (AI);machine learning (ML);network slicing;resource management},
doi={10.23919/JCC.2020.03.006},
ISSN={1673-5447},
month={March},}
@INPROCEEDINGS{8622435,
author={Matsubara, Masaki and Kobayashi, Masaki and Morishima, Atsuyuki},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={A Learning Effect by Presenting Machine Prediction as a Reference Answer in Self-correction},
year={2018},
volume={},
number={},
pages={3522-3528},
abstract={Can people learn from machines behavior in microtask based crowdsourcing? Can we train the machines as our mentor even without domain expertise? In this paper, we investigate how the task results improve concerning quality during and after presenting machine prediction as a reference answer in self-correction. Four reference types were examined in the experiment; Correct, Random, Machine prediction trained by correct answers, and that trained by human answers. Learning effects were observed only in presenting machine prediction, although those accuracy rates were far from correct (100%). Moreover, there were no learning effects in "Correct" and "Random". This suggests the following hypothesis: Since machine learners make some "models" for the problem, it is easier for humans to interpret the outputs of machine learners than the results without via them; it is more difficult to interpret not only random answers but also the correct answers in a case where the perfect interpretation of the problem is difficult. Furthermore, some workers answered with higher accuracy rate than machines in the post-test. Therefore, this strategy can be expected to be useful for bootstrapping solutions in the situation where unknown problems occur without expertise or at a low cost.},
keywords={Task analysis;Painting;Training data;Quality assurance;Training;Machine learning;Crowdsourcing;Quality Assurance;Self Correction;Machine Teaching},
doi={10.1109/BigData.2018.8622435},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8279590,
author={Marone, Reine Marie and Camara, Fodé and Ndiaye, Samba},
booktitle={2017 IEEE 4th International Conference on Soft Computing & Machine Intelligence (ISCMI)},
title={A large-scale filter method for feature selection based on spark},
year={2017},
volume={},
number={},
pages={16-20},
abstract={Recently, enormous volumes of data are generated in information systems. That's why data mining area is facing new challenges of transforming this “big data” into useful knowledge. In fact, “big data” relies low density of information (low data quality) and data redundancy, which negatively affect the data mining process. Therefore, when the number of variables describing the data is high, features selection methods are crucial for selecting relevant data. Features selection is the process of identifying the most relevant variables and removing those are redundant and irrelevant. In this paper, we propose a parallel, scalable feature selection algorithm based on mRMR (Max-Relevance and Min-Redundancy) in Spark, an in-memory parallel computing framework specialized in computation for large distributed datasets. Our experiments using real-world data of high dimensionality demonstrated that our proposition scale well and efficiently with large datasets.},
keywords={Feature extraction;Mutual information;Sparks;Algorithm design and analysis;Redundancy;Big Data;Classification algorithms;feature selection;filter method;parallel computing;apache spark;mRMR;SVM},
doi={10.1109/ISCMI.2017.8279590},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9687865,
author={Yan, Peipei and Li, Feng and Xiang, Zhiwei and Li, Mingxuan and Fan, Shuming},
booktitle={2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)},
title={Research and application of power data management key technology},
year={2021},
volume={2},
number={},
pages={98-102},
abstract={With the intensified application of power information systems and the advent of the “big data” era, higher requirements are put forward for power data resource management and power data security. Electric power companies have carried out research on key technologies for data management, established a three-level management system at the provincial, prefectural and county levels, built a panoramic view of data resources, a data operation management platform, a data negative list sharing mechanism, and a data security protection mechanism, which were applied to all aspects of data management and data governance. Through the support of business processes, data standards, data quality, etc., it has effectively improved the management efficiency of power data, improved the company's data management level, promoted business collaboration and efficiency.},
keywords={Visualization;Data security;Process control;Collaboration;Big Data;Power grids;Resource management;Electric power data;Data management;Data sharing;Data security},
doi={10.1109/ICIBA52610.2021.9687865},
ISSN={},
month={Dec},}
@ARTICLE{7835169,
author={Zong, Wei and Wu, Feng and Jiang, Zhengrui},
journal={IEEE Transactions on Engineering Management},
title={A Markov-Based Update Policy for Constantly Changing Database Systems},
year={2017},
volume={64},
number={3},
pages={287-300},
abstract={In order to maximize the value of an organization's data assets, it is important to keep data in its databases up-to-date. In the era of big data, however, constantly changing data sources make it a challenging task to assure data timeliness in enterprise systems. For instance, due to the high frequency of purchase transactions, purchase data stored in an enterprise resource planning system can easily become outdated, affecting the accuracy of inventory data and the quality of inventory replenishment decisions. Despite the importance of data timeliness, updating a database as soon as new data arrives is typically not optimal because of high update cost. Therefore, a critical problem in this context is to determine the optimal update policy for database systems. In this study, we develop a Markov decision process model, solved via dynamic programming, to derive the optimal update policy that minimizes the sum of data staleness cost and update cost. Based on real-world enterprise data, we conduct experiments to evaluate the performance of the proposed update policy in relation to benchmark policies analyzed in the prior literature. The experimental results show that the proposed update policy outperforms fixed interval update policies and can lead to significant cost savings.},
keywords={Database systems;Data models;Organizations;Markov processes;Big data;Data quality;data timeliness;enterprise resource planning (ERP);Markov decision process;update policy},
doi={10.1109/TEM.2017.2648516},
ISSN={1558-0040},
month={Aug},}
@ARTICLE{8746175,
author={Saberi, Morteza and Hussain, Omar Khadeer and Chang, Elizabeth},
journal={IEEE Access},
title={Quality Management of Workers in an In-House Crowdsourcing-Based Framework for Deduplication of Organizations’ Databases},
year={2019},
volume={7},
number={},
pages={90715-90730},
abstract={While organizations in the current era of big data are generating massive volumes of data, they also need to ensure that its quality is maintained for it to be useful in decision-making purposes. The problem of dirty data plagues every organization. One aspect of dirty data is the presence of duplicate data records that negatively impact the organization's operations in many ways. Many existing approaches attempt to address this problem by using traditional data cleansing methods. In this paper, we address this problem by using an in-house crowdsourcing-based framework, namely, DedupCrowd. One of the main obstacles of crowdsourcing-based approaches is to monitor the performance of the crowd, by which the integrity of the whole process is maintained. In this paper, a statistical quality control-based technique is proposed to regulate the performance of the crowd. We apply our proposed framework in the context of a contact center, where the Customer Service Representatives are used as the crowd to assist in the process of deduplicating detection. By using comprehensive working examples, we show how the different modules of the DedupCrowd work not only to monitor the performance of the crowd but also to assist in duplicate detection.},
keywords={Crowdsourcing;Databases;Task analysis;Object recognition;Monitoring;Error analysis;Big Data;Quality management;quality control;data quality;duplicate detection;in-house crowdsourcing},
doi={10.1109/ACCESS.2019.2924979},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9401161,
author={Shioiri, Satoshi and Sato, Yoshiyuki and Horaguchi, Yuta and Muraoka, Hiroaki and Nihei, Mariko},
booktitle={2021 IEEE International Symposium on Circuits and Systems (ISCAS)},
title={Quali-Informatics in the Society with Yotta Scale Data},
year={2021},
volume={},
number={},
pages={1-4},
abstract={Accumulation of information is essential for human knowledge production, and information technology has accelerated the speed of data accumulation. The increase in quantity of information with high speed does not promise high-quality knowledge production and possibly does cause problems. One big problem is lack of storage for such big data. Another critical problem in information usage is information overload, that is, deterioration of productivity by too much information. Decision accuracy decrease with amount of information beyond a certain point while it increases at the beginning. We introduce an approach for solution of these problems with an example of research along the approach.},
keywords={Productivity;Circuits and systems;Big Data;Acceleration;Information technology;data quality;human judgments},
doi={10.1109/ISCAS51556.2021.9401161},
ISSN={2158-1525},
month={May},}
@INPROCEEDINGS{9179610,
author={Chouhan, Ashish and Prabhune, Ajinkya and Prabhuraj, Paneesh and Chaudhari, Hitesh},
booktitle={2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)},
title={DWreck: A Data Wrecker Framework for Generating Unclean Datasets},
year={2020},
volume={},
number={},
pages={78-87},
abstract={In this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. In a typical data-analysis pipeline, data cleaning is the most cost-intensive, laborious, and time-consuming step. Unclean dataset or partially cleaned dataset can lead to incorrect training of machine learning models and result in wrong conclusions. Generally, data-scientists examine null, missing, or duplicate values, and the dataset is cleaned by removing the entire record or imputing the values. However, deleting the records, or imputing the values cannot be termed as comprehensive cleaning, as these cleaning techniques may result in a reduction in the population of data, and increased error in estimation due to biased values. For systematically cleaning an unclean dataset, it is necessary to comply with the data quality dimensions such as completeness, validity, consistency, accuracy, and conformity. The errors described as violations of expectations for completeness, accuracy, timeliness, consistency and other dimensions of data quality often impede the successful completion of information processing streams and consequently degrade the dependent business processes. Therefore, educating a data-scientist for comprehensively cleaning a raw-dataset acquired for analysis is an incremental learning process. Moreover, for extensive training on cleaning a dataset on different quality dimensions, it is necessary to provide a variety of datasets that are unclean on various data quality dimensions. Hence, in this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. The DWreck framework is designed on the principles of microservices architecture pattern. For allowing function-specific extensibility, the DWreck comprises four groups of microservices: (a) Dataset Profiling, (b) Data type Processing, (c) Counterproductive Dimensions, and (d) Miscellaneous. The orchestrator coordinates the different microservices in a complex workflow that is further split into three sub-workflows to generate an unclean (wrecked) dataset as an output. Finally, we evaluate the DWreck framework on twenty seed-datasets to generate corresponding wrecked datasets.},
keywords={Generators;Data integrity;XML;Cleaning;Tools;Databases;Pipelines;data generators;data quality dimensions;data cleaning;microservice architectures;data management},
doi={10.1109/BigDataService49289.2020.00020},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9662594,
author={Lu, Jianfei and Li, Suxiu and Zhang, Xinsheng},
booktitle={2021 5th International Conference on Power and Energy Engineering (ICPEE)},
title={A Study on the Business Data Evaluation Method of the Power Grid Value-Added Service},
year={2021},
volume={},
number={},
pages={288-292},
abstract={This research focuses on the data application basic technology for value-added services. In this research, the data quality and data value evaluation methods are studied. The data quality management system from data collection, storage, management and application is formed. Quality and power marketing data quality are analyzed and data value evaluation methods are established. As big data, artificial intelligence and other technologies continue to make breakthroughs, the value of data will become more and more important. Based on the State Grid’s full-service data, the full-scale data analysis across the professional, cross-business, and cross-system will promote the company’s power grid lean and intelligent management level, and will provide more value-added services for the company, government and society.},
keywords={Systematics;Data analysis;Data integrity;Storage management;Government;Data models;Power grids;Date Evaluation Method;Power Grid;Value- Added Service},
doi={10.1109/ICPEE54380.2021.9662594},
ISSN={},
month={Dec},}
@ARTICLE{9086142,
author={Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Wang, Muxian and Li, Jianzhong and Gao, Hong},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Leveraging Currency for Repairing Inconsistent and Incomplete Data},
year={2022},
volume={34},
number={3},
pages={1288-1302},
abstract={Data quality plays a key role in big data management today. With the explosive growth of data from a variety of sources, the quality of data is faced with multiple problems. Motivated by this, we study the multiple data cleaning on incompleteness and inconsistency with currency reasoning and determination in this paper. We introduce a 4-step framework, named ${\sf Imp3C}$Imp3C, for errors detection and quality improvement in incomplete and inconsistent data without timestamps. We achieve an integrated currency determining method to compute the currency orders among tuples, according to currency constraints. Thus, the inconsistent data and missing values are repaired effectively considering the temporal impact. For both effectiveness and efficiency consideration, we carry out inconsistency repair ahead of incompleteness repair. A currency-related consistency distance metric is defined to measure the similarity between dirty tuples and clean ones more accurately. In addition, currency orders are treated as an important feature in the missing imputation training process. The solution algorithms are introduced in detail with case studies. A thorough experiment on three real-life datasets verifies our method ${\sf Imp3C}$Imp3C improves the performance of data repairing with multiple quality problems. ${\sf Imp3C}$Imp3C outperforms the existing advanced methods, especially in the datasets with complex currency orders.},
keywords={Currencies;Maintenance engineering;Cleaning;Urban areas;Remuneration;Databases;Companies;Data cleaning;data quality management;currency determining;temporal data repairing},
doi={10.1109/TKDE.2020.2992456},
ISSN={1558-2191},
month={March},}
@INPROCEEDINGS{9174457,
author={Khokhlov, Igor and Reznik, Leon},
booktitle={2020 IEEE Systems Security Symposium (SSS)},
title={What is the Value of Data Value in Practical Security Applications},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Data value (DV) is a novel concept that is introduced as one of the Big Data phenomenon features. While continuing an investigation of the DV ontology and its relationship with the data quality (DQ) on the conceptual level, this paper researches possible applications and use of the DV in the practical design of security and privacy protection systems and tools. We present a novel approach to DV evaluation that maps DQ metrics into DV value. Developed methods allow DV and DQ use in a wide range of application domains. To demonstrate DQ and DV concept employment in real tasks we present two real-life scenarios. The first use case demonstrates the DV use in crowdsensing application design. It shows up how DV can be calculated by integrating various metrics characterizing data application functionality, accuracy, and security. The second one incorporates the privacy consideration into DV calculus by exploring the relationship between privacy, DQ, and DV in the defense against web-site fingerprinting in The Onion Router (TOR) networks. These examples demonstrate how our methods of the DV and DQ evaluation may be employed in the design of real systems with security and privacy consideration.},
keywords={Data quality;data value;security evaluation;privacy protection},
doi={10.1109/SSS47320.2020.9174457},
ISSN={},
month={July},}
@INPROCEEDINGS{9155928,
author={Tavakoli, Mohammadreza and Elias, Mirette and Kismihók, Gábor and Auer, Sören},
booktitle={2020 IEEE 20th International Conference on Advanced Learning Technologies (ICALT)},
title={Quality Prediction of Open Educational Resources A Metadata-based Approach},
year={2020},
volume={},
number={},
pages={29-31},
abstract={In the recent decade, online learning environments have accumulated millions of Open Educational Resources (OERs). However, for learners, finding relevant and high quality OERs is a complicated and time-consuming activity. Furthermore, metadata play a key role in offering high quality services such as recommendation and search. Metadata can also be used for automatic OER quality control as, in the light of the continuously increasing number of OERs, manual quality control is getting more and more difficult. In this work, we collected the metadata of 8,887 OERs to perform an exploratory data analysis to observe the effect of quality control on metadata quality. Subsequently, we propose an OER metadata scoring model, and build a metadata-based prediction model to anticipate the quality of OERs. Based on our data and model, we were able to detect high-quality OERs with the F1 score of 94.6%.},
keywords={Metadata;Quality control;Predictive models;Open Educational Resources;Measurement;Data analysis;OER;open educational resources;metadata quality;OER quality;Big data;data analysis;quality prediction},
doi={10.1109/ICALT49669.2020.00007},
ISSN={2161-377X},
month={July},}
@INPROCEEDINGS{9156127,
author={Kumar, Abhishek and Braud, Tristan and Tarkoma, Sasu and Hui, Pan},
booktitle={2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)},
title={Trustworthy AI in the Age of Pervasive Computing and Big Data},
year={2020},
volume={},
number={},
pages={1-6},
abstract={The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.},
keywords={Artificial intelligence;Ethics;Data privacy;Biological system modeling;Training;Pervasive computing;Robustness;Artificial Intelligence;Pervasive Computing;Ethics;Data Fusion;Transparency;Privacy;Fairness;Accountability;Federated Learning},
doi={10.1109/PerComWorkshops48775.2020.9156127},
ISSN={},
month={March},}
@INPROCEEDINGS{8024681,
author={Guang Wei and Hailong Yang and Zhongzhi Luan and Depei Qian},
booktitle={2017 IEEE Symposium on Computers and Communications (ISCC)},
title={iDPL: A scalable and flexible inter-continental testbed for data placement research and experiment},
year={2017},
volume={},
number={},
pages={1158-1163},
abstract={In this paper, we propose the China-US international data placement laboratory (iDPL) based on an inter-continental testbed for data placement research. iDPL is able to support various data placement research due to its scalability and flexibility in deploying the experiments in the real network environment. The core design of iDPL leverages reliable workflow management and lightweight I/O protocol to allow complex experiment setup and on-the-fly experiment deployment. It is also extensible to plugin different network profiling tools such as iperf. We expect the powerful measurement capability of iDPL promotes research study on the intelligent data placement policies which adapt to the uncertainty of the wide-area network and guarantee the quality of service (QoS) of the big data applications. As a case study, we setup a set of data placement experiments to measure the end-to-end network performance constantly among several sites between China and US using different data placement tools. The experiments have been running for more than one year, and its measurement data is public available (http://mickey.buaa.edu.cn:8080/). We believe the measurement data is valuable for both network and big data researchers to understand the performance disparity between the raw network and the actual data placement, which provides useful insights to design big data applications with performance awareness. We encourage more researchers to deploy their own data placement experiments on iDPL, expediting the research direction of intelligent data placement with real network environment.},
keywords={Tools;Synchronization;Data visualization;Protocols;Distributed databases;Computers;Software},
doi={10.1109/ISCC.2017.8024681},
ISSN={},
month={July},}
@INPROCEEDINGS{7724270,
author={Gopal, R. Chandangole and Bharat, A. Tidke},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={A generic tool to process mongodb or Cassandra dataset using Hadoop streaming},
year={2016},
volume={},
number={},
pages={273-276},
abstract={Now a days Bulk of data generating on the system. This data is very important for user and today's user accessing, searching and sorting the data from database is very difficult. To overcome this problem, data is distributed in different node using Hadoop technology. A system is proposed in which the collected data is to be distributed using map reduce technique for sorting the data is very easily on Hadoop environment. In this case used Cassandra and mongodb tools to storing large amount of data on Hadoop Framework. NoSQL data is stores in unstructured data format which is a key focus area for “Big Data” research. The quantity and quality of unstructured data growing high. The Hadoop Framework used to large amount of data on a different nodes in a cluster data. NoSQL databases using different structure and unstructured data of high scalability for getting high performance of system. To present the approaches solving Problem of NoSQL data to stores with MapReduce process to under in non-Java application. A Cassandra is to provide the platform for the fast and efficient data queries. In this paper presents the tools of the Cassandra and the mongodb using NoSQL database for connecting different node with the Hadoop MapReduce engine.},
keywords={Decision support systems;Handheld computers;Conferences;Hadoop Streaming;Cassandra;Mongodb;MapReduce},
doi={},
ISSN={},
month={March},}
