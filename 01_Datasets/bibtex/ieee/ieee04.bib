@INPROCEEDINGS{7724956,
author={Bhargava, Deepshikha and Poonia, Ramesh C. and Arora, Upma},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={Design and development of an intelligent agent based framework for predictive analytics},
year={2016},
volume={},
number={},
pages={3715-3718},
abstract={The arrival of World Wide Web has led to the explosive growth of information on the web. There is a sudden boom in quality of raw data/information, which is commonly referred as Big Data. Very often, this raw information contains very useful insights which are ignored most of the time and are difficult to analyze due to the enormous size of these datasets. The feasibility for human to extract this information from the vast web and build useful application on the top of it, is very low. Hence to create predictive models, there is a huge need for intelligent and autonomous software agents which can procure useful information from the large datasets of raw information. Predictive analytics models can be created from these datasets which can be further used for various applications in security, future prediction etc. This research paper gives an overview of how these software agents will become the most important tools in coming days for building predictive models.},
keywords={Intelligent agents;Predictive models;Analytical models;Market research;Conferences;Data mining;Software Agents;Intelligent Agents;Predictive Analytics},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{8616529,
author={Kong, Weichang and Qiao, Fei and Wu, Qidi},
booktitle={2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
title={Real Manufacturing Oriented Data Process Techniques with Domain Knowledge},
year={2018},
volume={},
number={},
pages={3141-3146},
abstract={In the field of manufacturing industry, it is difficult to make full use of the research results for production optimization and/or management due to the low quality of real workshop data. Typical quality problems of the real workshop data include: data conflict, missing recessive data, and false error identification. The conventional data analysis methods cannot handle most such issues because they fail to consider professional insights into and domain knowledge about the data. The real production data from an actual semiconductor manufacturing workshop are adopted as the objective data in this paper. A series of data process techniques with domain knowledge are proposed to solve those data quality problems according to specific flaws of the data respectively. The work in this paper has the potential to be further extended and applied to other big data applications beyond the manufacturing industry.},
keywords={Conferences;Big Data;Manufacturing processes;Manufacturing industries;Data analysis;Data integrity},
doi={10.1109/SMC.2018.00532},
ISSN={2577-1655},
month={Oct},}
@INPROCEEDINGS{9474741,
author={Mavrogiorgou, Argyro and Kleftakis, Spyridon and Mavrogiorgos, Konstantinos and Zafeiropoulos, Nikolaos and Menychtas, Andreas and Kiourtis, Athanasios and Maglogiannis, Ilias and Kyriazis, Dimosthenis},
booktitle={2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)},
title={beHEALTHIER: A Microservices Platform for Analyzing and Exploiting Healthcare Data},
year={2021},
volume={},
number={},
pages={283-288},
abstract={The era of big data is surrounded by plenty of challenges, concerning aspects related to data quality, data management, and data analysis. Plenty of these challenges are met in several domains, such as the healthcare domain, where the corresponding healthcare platforms not only have to deal with managing and/or analyzing a tremendous quantity of health data, but also have to accomplish these actions in the most efficient and secure way possible. Towards this direction, medical institutions are paying attention to the replacement of traditional approaches such as the Monolithic and Service Oriented Architecture (SOA), which deal with many difficulties for handling the increasing amount of healthcare data. This paper presents a platform for overcoming these issues, by adopting the Microservice Architecture (MSA), being able to efficiently manage and analyze these vast amounts of data. More specifically, the proposed platform, namely beHEALTHIER, offers the ability to construct health policies out of data of collective knowledge, by utilizing a newly proposed kind of electronic health records (i.e., eXtended Health Records (XHRs)) and their corresponding networks, through the efficient analysis and management of ingested healthcare data. In order to achieve that, beHEALTHIER is architected based upon four (4) discrete and interacting pillars, namely the Data, the Information, the Knowledge and the Actions pillars. Since the proposed platform is based on MSA, it fully utilizes MSA's benefits, achieving fast response times and efficient mechanisms for healthcare data collection, processing, and analysis.},
keywords={Machine learning algorithms;Data analysis;Software architecture;Medical services;Computer architecture;Big Data;Service-oriented architecture;Healthcare;Electronic Health Records;Data Collection;Data Analysis;Health Policies;Microservices},
doi={10.1109/CBMS52027.2021.00078},
ISSN={2372-9198},
month={June},}
@INPROCEEDINGS{8638638,
author={Xing, Xin and Dong, Bin and Ajo-Franklin, Jonathan and Wu, Kesheng},
booktitle={2018 IEEE/ACM Machine Learning in HPC Environments (MLHPC)},
title={Automated Parallel Data Processing Engine with Application to Large-Scale Feature Extraction},
year={2018},
volume={},
number={},
pages={37-46},
abstract={As new scientific instruments generate ever more data, we need to parallelize advanced data analysis algorithms such as machine learning to harness the available computing power. The success of commercial Big Data systems demonstrated that it is possible to automatically parallelize many algorithms. However, these Big Data tools have trouble handling the complex analysis operations from scientific applications. To overcome this difficulty, we have started to build an automated parallel data processing engine for science, known as ARRAYUDF. This paper provides an overview of this data processing engine, and a use case involving a feature extraction task from a large-scale seismic recording technology, called distributed acoustic sensing (DAS). The key challenge associated with DAS data sets is that they are vast in volume and noisy in data quality. The existing methods used by the DAS team for extracting useful signals like traveling seismic waves are complex and very time-consuming. Our parallel data processing engine reduces the job execution time from 10s of hours to 10s of seconds, and achieves 95% parallelization efficiency. ARRAYUDF could be used to implement more advanced data processing algorithms including machine learning, and could work with many more applications.},
keywords={Arrays;Sensors;Kernel;Data analysis;Engines;Feature extraction;ArrayUDF;distributed acoustic sensing;local similarity},
doi={10.1109/MLHPC.2018.8638638},
ISSN={},
month={Nov},}
@ARTICLE{8993839,
author={Qadri, Yazdan Ahmad and Nauman, Ali and Zikria, Yousaf Bin and Vasilakos, Athanasios V. and Kim, Sung Won},
journal={IEEE Communications Surveys & Tutorials},
title={The Future of Healthcare Internet of Things: A Survey of Emerging Technologies},
year={2020},
volume={22},
number={2},
pages={1121-1167},
abstract={The impact of the Internet of Things (IoT) on the advancement of the healthcare industry is immense. The ushering of the Medicine 4.0 has resulted in an increased effort to develop platforms, both at the hardware level as well as the underlying software level. This vision has led to the development of Healthcare IoT (H-IoT) systems. The basic enabling technologies include the communication systems between the sensing nodes and the processors; and the processing algorithms for generating an output from the data collected by the sensors. However, at present, these enabling technologies are also supported by several new technologies. The use of Artificial Intelligence (AI) has transformed the H-IoT systems at almost every level. The fog/edge paradigm is bringing the computing power close to the deployed network and hence mitigating many challenges in the process. While the big data allows handling an enormous amount of data. Additionally, the Software Defined Networks (SDNs) bring flexibility to the system while the blockchains are finding the most novel use cases in H-IoT systems. The Internet of Nano Things (IoNT) and Tactile Internet (TI) are driving the innovation in the H-IoT applications. This paper delves into the ways these technologies are transforming the H-IoT systems and also identifies the future course for improving the Quality of Service (QoS) using these new technologies.},
keywords={Internet of Things;Medical services;Edge computing;Sensors;Blockchain;Quality of service;Big Data;H-IoT;WBAN;machine learning;fog computing;edge computing;blockchain;software defined networks},
doi={10.1109/COMST.2020.2973314},
ISSN={1553-877X},
month={Secondquarter},}
@ARTICLE{7978034,
author={Xu, Xiaolong and Liu, Xinxin and Liu, Xiaoxiao and Sun, Yanfei},
journal={Journal of Systems Engineering and Electronics},
title={Truth finder algorithm based on entity attributes for data conflict solution},
year={2017},
volume={28},
number={3},
pages={617-626},
abstract={The Internet now is a large-scale platform with big data. Finding truth from a huge dataset has attracted extensive attention, which can maintain the quality of data collected by users and provide users with accurate and efficient data. However, current truth finder algorithms are unsatisfying, because of their low accuracy and complication. This paper proposes a truth finder algorithm based on entity attributes (TFAEA). Based on the iterative computation of source reliability and fact accuracy, TFAEA considers the interactive degree among facts and the degree of dependence among sources, to simplify the typical truth finder algorithms. In order to improve the accuracy of them, TFAEA combines the one-way text similarity and the factual conflict to calculate the mutual support degree among facts. Furthermore, TFAEA utilizes the symmetric saturation of data sources to calculate the degree of dependence among sources. The experimental results show that TFAEA is not only more stable, but also more accurate than the typical truth finder algorithms.},
keywords={Algorithm design and analysis;Reliability;Internet;Telecommunications;Big Data;Data models;truth finder;data reliability;entity attribute;data conflict},
doi={10.21629/JSEE.2017.03.21},
ISSN={1004-4132},
month={June},}
@INPROCEEDINGS{9095877,
author={Li, Song and Ning, Sun and Yezhou, Yao and Jingjing, Tian and Wenxue, Zhang and Liang, Chi},
booktitle={2019 2nd International Conference on Safety Produce Informatization (IICSPI)},
title={Application of Data Mining Technology in the Recall of Defective Automobile Products in China ——A Typical Case of the Construction of Digital China},
year={2019},
volume={},
number={},
pages={541-545},
abstract={According to multisource quality safety data of defective automobile products, key quality safety factors of defective automobile products are extracted, a defect information indicator system for automobile products is systematically constructed and a correlated graph is established between quality safety factors. Based on the optimization and correlation of the quality safety factor indicator system, Big Data technology is used to design a data structure for multisource quality safety information cluster, develop a data platform for the defect information analysis of automobile products and achieve information clustering and correlation analysis based on multisource quality safety data, providing technical support for the recall management of defective automobile products.},
keywords={Automobiles;Safety;Big Data;Government;Personnel;Data mining;automobile recall;data mining;information cluster},
doi={10.1109/IICSPI48186.2019.9095877},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8121809,
author={Liu, He and Chen, Jiangqi and Huang, Fupeng and Li, Han},
booktitle={2017 14th International Symposium on Pervasive Systems, Algorithms and Networks & 2017 11th International Conference on Frontier of Computer Science and Technology & 2017 Third International Symposium of Creative Computing (ISPAN-FCST-ISCC)},
title={An Electric Power Sensor Data Oriented Data Cleaning Solution},
year={2017},
volume={},
number={},
pages={430-435},
abstract={With the development of Smart Grid Technology, more and more electric power sensor data are utilized in various electric power systems. To guarantee the effectiveness of such systems, it is necessary to ensure the quality of electric power sensor data, especially when the scale of electric power sensor data is large. In the field of large-scale electric power sensor data cleaning, the computational efficiency and accuracy of data cleaning are two vital requirements. In order to satisfy these requirements, this paper presents an electric power sensor data oriented data cleaning solution, which is composed of a data cleaning framework and a data cleaning method. Based on Hadoop, the given framework is able to support large-scale electric power sensor data acquisition, storage and processing. Meanwhile, the proposed method which achieves outlier detection and reparation is implemented on the basis of a time-relevant k-means clustering algorithm in Spark. The feasibility and effectiveness of the proposed method is evaluated on a data set which originates from charging piles. Experimental results show that the proposed data cleaning method is able to improve the data quality of electric power sensor data by finding and repairing most outliers. For large-scale electric power sensor data, the proposed data cleaning method has high parallel performance and strong scalability.},
keywords={Power systems;Cleaning;Clustering algorithms;Big Data;Sparks;Data acquisition;Algorithm design and analysis;electric power senser data;data cleaning;k-means clustering;outlier;Spark},
doi={10.1109/ISPAN-FCST-ISCC.2017.29},
ISSN={2375-527X},
month={June},}
@INPROCEEDINGS{8622249,
author={Huang, Yu and Milani, Mostafa and Chiang, Fei},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={PACAS: Privacy-Aware, Data Cleaning-as-a-Service},
year={2018},
volume={},
number={},
pages={1023-1030},
abstract={Data cleaning consumes up to 80% of the data analysis pipeline. This is a significant overhead for organizations where data cleaning is still a manually driven process requiring domain expertise. Recent advances have fueled a new computing paradigm called Database-as-a-Service, where data management tasks are outsourced to large service providers. We propose a new Data Cleaning-as-a-Service model that allows a client to interact with a data cleaning provider who hosts curated, and sensitive data. We present PACAS: a Privacy-Aware data Cleaning-As-a-Service framework that facilitates communication between the client and the service provider via a data pricing scheme where clients issue queries, and the service provider returns clean answers for a price while protecting her data. We propose a practical privacy model in such interactive settings called (X,Y,L)-anonymity that extends existing data publishing techniques to consider the data semantics while protecting sensitive values. Our evaluation over real data shows that PACAS effectively safeguards semantically related sensitive values, and provides improved accuracy over existing privacy-aware cleaning techniques.},
keywords={Cleaning;Data privacy;Pricing;Data models;Semantics;Osteoarthritis;Maintenance engineering;data quality;data cleaning;data privacy},
doi={10.1109/BigData.2018.8622249},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9006289,
author={Rocha, Lais M. A. and Bessa, Aline and Chirigati, Fernando and OFriel, Eugene and Moro, Mirella M. and Freire, Juliana},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Understanding Spatio-Temporal Urban Processes},
year={2019},
volume={},
number={},
pages={563-572},
abstract={Increasingly, decisions are based on insights and conclusions derived from the results of data analysis. Thus, determining the validity of these results is of paramount importance. In this paper, we take a step towards helping users identify potential issues in spatio-temporal data and thus gain trust in the results they derived from these data. We focus on processes that are captured by relationships among datasets that serve as the data exhaust for different components of urban environments. In this scenario, debugging data involves two important challenges: the inherent complexity of spatio-temporal data, and the number of possible relationships. We propose a framework for profiling spatio-temporal relationships that automatically identifies data slices that present a significant deviation from what is expected, and thus, helps focus a user's attention on slices of the data that may have quality issues and/or that may affect the conclusions derived from the analysis' results. We describe the profiling methodology and how it derives relationships, identifies candidate deviations, assesses their statistical significance, and measures their magnitude. We also present a series of cases studies using real datasets from New York City which demonstrate the usefulness of spatio-temporal profiling to build trust on data analysis' results.},
keywords={Urban areas;Correlation;Spatial resolution;Data analysis;Mathematical model;Public transportation;Standards;data quality;data profiling;urban data},
doi={10.1109/BigData47090.2019.9006289},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8258096,
author={Appiktala, Nirupama and Chen, Miao and Natkovich, Michael and Walters, Joshua},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Demystifying dark matter for online experimentation},
year={2017},
volume={},
number={},
pages={1620-1626},
abstract={The rise of online controlled experimentation, a.k.a. A/B testing began around the turn of the millennium with the emergence of internet giants like Amazon, Bing, Facebook, Google, LinkedIn, and Yahoo. A step towards good experimental design includes the planning for sample size, confidence level, metrics to be measured and test duration. Generally, these factors impact the quality and validity of an experiment. In practice, additional factors may also impact the validity of an experiment. One such critical factor is the discrepancy between the planned bucket size and the actual bucket size. We call this hidden gap “Experimentation Dark Matter”. Experimentation dark matter is invisible to A/A or A/B validation of experimental analysis but can impact the validity of an experiment. In this paper, we have demonstrated in detail, this gap that may cause the loss of statistical power as well as the loss of representativeness and generalizability of an experiment. We have proposed a framework to monitor experimentation dark matter that may go unnoticed in a balanced AB test. We have further discussed the remediation of a recent dark matter issue using our framework. This scalable, low-latency framework is effective and applicable to similar online controlled experimentation systems.},
keywords={Sociology;Statistics;Servers;Testing;Finance;Google;Measurement;online experimentation;quality assurance;bucket size gap;dark matter;loss of traffic;data quality},
doi={10.1109/BigData.2017.8258096},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9403739,
author={Wei, Li and Dawei, Wang and Lixia, Wang},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={Research on data Traceability Method Based on blockchain Technology},
year={2020},
volume={},
number={},
pages={45-49},
abstract={Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.},
keywords={Industries;Technological innovation;Distributed databases;Blockchain;Data models;Internet;Safety;blockchain;data traceability;data quality;data security;data governance;energy Internet;huge data},
doi={10.1109/ICBASE51474.2020.00017},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9874011,
author={Qideng, Tang and Chaofan, Dai},
booktitle={2022 8th International Conference on Big Data and Information Analytics (BigDIA)},
title={An Efficient and Effective Model for Large Scale Fingerprint Matching},
year={2022},
volume={},
number={},
pages={48-52},
abstract={Fingerprint is one of the most widely used biometric features to identify a person. Fingerprint matching is to compare an input fingerprint with fingerprints within the database to find the most similar fingerprints. However, existing fingerprint matching technologies face two major challenges: data quantity and data quality. From the perspective of data quantity, the existing fingerprint database stores hundreds of millions of data, so the traditional iterative matching method is not practical due to huge time consumption. From the data quality point, existing fingerprint products have a small collection area for aesthetics and portability, resulting in a low quality of the captured finger-print image, which seriously affects the accuracy of the matching algorithms. To address the above problems, we present a two-stage fingerprint matching method, which not only improves the efficiency of fingerprint matching, but also ensures the accuracy of fingerprint matching. The first stage utilizes a convolutional neural network trained with triplet loss to extract the overall feature of a fingerprint image. The cosine similarity between features can be used to identify possible fingerprint matches. The second stage compares fingerprint matches acquired in the first stage using a minutiae points comparing algorithm to get a more precise result. The experiment result shows that our method is satisfactory in both speed and accuracy.},
keywords={Databases;Data integrity;Image matching;Fingerprint recognition;Big Data;Feature extraction;Convolutional neural networks;fingerprint matching;convolutional neural net-work;fingerprint minutiae points;point-matching},
doi={10.1109/BigDIA56350.2022.9874011},
ISSN={2771-6902},
month={Aug},}
@INPROCEEDINGS{9041478,
author={Ding, Jian and Ma, Chunlei and Fu, Bin and Liu, Bing},
booktitle={8th Renewable Power Generation Conference (RPG 2019)},
title={Active distribution network state estimation algorithm based on decision tree of self-identification},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Under the background of active distribution network, this paper proposes a state estimation algorithm of managing analysis data to solve the problem of big data, data missing and complex analysis. This paper proposes an active distribution network state estimation algorithm based on decision tree self-identification. Setting appropriate quality weight of big data based on the check rules different from traditional single-phase currents. Data in the input state estimation model is better compatible by estimating the pre-processed data, classifying and correcting the data including voltage and current. Moreover, on the premise of lacking of distributed energy measurement devices, this paper establishes a state estimation model for distributed power, which is used to correct the default data of distributed energy and improve the quality of input data in wind power and photovoltaic. The method can be verified in the actual example. Compared with the traditional state estimation, the active distribution network state estimation algorithm based on decision tree self-identification has better estimation effect and faster iteration speed. Therefore, the proposed algorithm can be effectively applied to the current state estimation of large-scale distributed energy access.},
keywords={DATA CHECK;DATA QUALITY IDENTIFICATION;DECISION TREE;STATE ESTIMATION;BIG DATA},
doi={10.1049/cp.2019.0490},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7379507,
author={Krawczyk, Bartosz and Wozniak, Michal},
booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics},
title={Weighted Naïve Bayes Classifier with Forgetting for Drifting Data Streams},
year={2015},
volume={},
number={},
pages={2147-2152},
abstract={Mining massive data streams in real-time is one of the contemporary challenges for machine learning systems. Such a domain encompass many of difficulties hidden beneath the term of Big Data. We deal with massive, incoming information that must be processed on-the-fly, with lowest possible response delay. We are forced to take into account time, memory and quality constraints. Our models must be able to quickly process large collection of data and swiftly adapt themselves to occurring changes (shifts and drifts) in data streams. In this paper, we propose a novel version of simple, yet effective Naïve Bayes classifier for mining streams. We add a weighting module, that automatically assigns an importance factor to each object extracted from the stream. The higher the weight, the bigger influence given object exerts on the classifier training procedure. We assume, that our model works in the non-stationary environment with the presence of concept drift phenomenon. To allow our classifier to quickly adapt its properties to evolving data, we imbue it with forgetting principle implemented as weight decay. With each passing iteration, the level of importance of previous objects is decreased until they are discarded from the data collection. We propose an efficient sigmoidal function for modeling the forgetting rate. Experimental analysis, carried out on a number of large data streams with concept drift prove that our weighted Naïve Bayes classifier displays highly satisfactory performance in comparison with state-of-the-art stream classifiers.},
keywords={Training;Adaptation models;Data mining;Memory management;Detectors;Data models;Probability;machine learning;data stream;concept drift;big data;incremental learning;forgetting},
doi={10.1109/SMC.2015.375},
ISSN={},
month={Oct},}
@ARTICLE{9126214,
author={Ren, Lei and Meng, Zihao and Wang, Xiaokang and Lu, Renquan and Yang, Laurence T.},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={A Wide-Deep-Sequence Model-Based Quality Prediction Method in Industrial Process Analysis},
year={2020},
volume={31},
number={9},
pages={3721-3731},
abstract={Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.},
keywords={Feature extraction;Predictive models;Data models;Quality assessment;Product design;Data mining;Analytical models;Industrial artificial intelligence (AI);industrial big data;Industrial Internet of Things;product quality prediction;wide-deep-sequence (WDS) model},
doi={10.1109/TNNLS.2020.3001602},
ISSN={2162-2388},
month={Sep.},}
@INPROCEEDINGS{6727311,
author={Martelli, Cristina and Bellini, Emanuele},
booktitle={2013 International Conference on Signal-Image Technology & Internet-Based Systems},
title={Using Value Network Analysis to Support Data Driven Decision Making in Urban Planning},
year={2013},
volume={},
number={},
pages={998-1003},
abstract={This article provides a methodology of assessing the (Big)/(Open) Data quality in Data Driven Decision Making with the Value Network Analysis approach discovering the value creation failure point(s) in the network and evaluating the impact of loss of vale of data in DDDM process.},
keywords={Cities and towns;Decision making;IEEE 802.11 Standards;Time series analysis;Context;Urban planning;Data Driven Decision Making;Value Network Analysis;urban planning},
doi={10.1109/SITIS.2013.161},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7364059,
author={Wu, Chieh-Han and Song, Yang},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Robust and distributed web-scale near-dup document conflation in microsoft academic service},
year={2015},
volume={},
number={},
pages={2606-2611},
abstract={In modern web-scale applications that collect data from different sources, entity conflation is a challenging task due to various data quality issues. In this paper, we propose a robust and distributed framework to perform conflation on noisy data in the Microsoft Academic Service dataset. Our framework contains two major components. In the offline component, we train a GBDT model to determine whether two papers from different sources should be conflated to the same paper entity. In the online component, we propose a scalable shingling algorithm that can apply our offline model to over 100 million instances. The result shows that our algorithm can conflate noisy data robustly and efficiently.},
keywords={Algorithm design and analysis;Noise measurement;Resource management;Data models;Robustness;Computational modeling;Proteins;Near-duplicate detection;shingling algorithm;n-gram;entity conflation},
doi={10.1109/BigData.2015.7364059},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9006489,
author={Park, Hyunseop and Ko, Hyunwoong and Lee, Yung-Tsun T. and Cho, Hyunbo and Witherell, Paul},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={A Framework for Identifying and Prioritizing Data Analytics opportunities in Additive Manufacturing},
year={2019},
volume={},
number={},
pages={3458-3467},
abstract={Many industries, including manufacturing, are adopting data analytics (DA) in making decisions to improve quality, cost, and on-time delivery. In recent years, more research and development efforts have applied DA to additive manufacturing (AM) decision-making problems such as part design and process planning. Though there are many AM decision-making problems, not all benefit greatly from DA. This may be due to insufficient AM data, unreliable data quality, or the fact that DA is not cost effective when it is applied to some AM problems. This paper proposes a framework to investigate DA opportunities in a manufacturing operation, specifically AM. The proposed framework identifies and prioritizes AM potential opportunities where DA can make impact. The proposed framework is presented in a five-tier architecture, including value, decision-making, data analytics, data, and data source tiers. A case study is developed to illustrate how the proposed framework identifies DA opportunities in AM.},
keywords={Decision making;Solid modeling;Mechanical variables measurement;Electric variables measurement;Shape measurement;Data analysis;Analytical models;Data analytics;opportunity identification and prioritization;architecture;additive manufacturing},
doi={10.1109/BigData47090.2019.9006489},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8258313,
author={Yang, Dazhi and Zhang, Allan N. and Yan, Wenjing},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Performing literature review using text mining, Part I: Retrieving technology infrastructure using Google Scholar and APIs},
year={2017},
volume={},
number={},
pages={3290-3296},
abstract={Technology infrastructure (TechInfra) refers to metadata describing an academic field, such as journals & conferences, authors, publications and organizations. Understanding the TechInfra is often the first step in performing a literature review on a particular topic. In this paper, a study is conducted to retrieve TechInfra for a topic in supply chain management, namely, last mile logistics. Google Scholar is used as the primary tool for data collection. The first 1,000 results returned by Google Scholar are downloaded as HTML files. Subsequently, various application programming interfaces (APIs) - e.g., ScienceDirect, IEEE, CrossRef APIs - are used to enhance the data quality. Some plots are used to provide visualization of TechInfra of last mile logistics.},
keywords={Google;Metadata;Logistics;Uniform resource locators;Transportation;Databases;Text mining;technology infrastructure;text mining;last mile logistics;Google Scholar},
doi={10.1109/BigData.2017.8258313},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7004389,
author={Fähnrich, Cindy and Schapranow, Matthieu-P. and Plattner, Hasso},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Towards integrating the detection of genetic variants into an in-memory database},
year={2014},
volume={},
number={},
pages={27-32},
abstract={Next-generation sequencing enables whole genome sequencing within a few hours at a minimum of cost, entailing advanced medical applications such as personalized treatments. However, this recent technology imposes new challenges to alignment and variant calling as subsequent analysis steps. Compared to former sequencing, both must deal with an increasing amount of data to process at a significantly lower data quality - and are currently not capable of that. In this work, we focus on addressing these challenges for identifying Single Nucleotide Polymorphisms, i.e. SNP calling, in genome data as one subtask of variant calling. We propose the application of a column-store in-memory database for efficient data processing and apply the statistical model that is provided by the Genome Analysis Toolkit's UnifiedGenotyper. Comparisons with the UnifiedGenotyper show that our approach can exploit all computational resources available and accelerates SNP calling up to a factor of 22x.},
keywords={Genomics;Bioinformatics;Databases;Sequential analysis;Runtime;Biological cells;Instruction sets;Genome Data Analysis;Variant Calling;Single Nucleotide Polymorphism;In-Memory Database Technology;Next-Generation Sequencing},
doi={10.1109/BigData.2014.7004389},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8258016,
author={Kraus, Naama and Carmel, David and Keidar, Idit},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Fishing in the stream: Similarity search over endless data},
year={2017},
volume={},
number={},
pages={964-969},
abstract={Similarity search is the task of retrieving data items that are similar to a given query. In this paper, we introduce the time-sensitive notion of similarity search over endless data-streams (SSDS), which takes into account data quality and temporal characteristics in addition to similarity. SSDS is challenging as it needs to process unbounded data, while computation resources are bounded. We propose Stream-LSH, a randomized SSDS algorithm that bounds the index size by retaining items according to their freshness, quality, and dynamic popularity attributes. We show that Stream-LSH increases recall when searching for similar items compared to alternative approaches using the same space capacity.},
keywords={Redundancy;Heuristic algorithms;Indexing;Approximation algorithms;Measurement;Runtime;Similarity search;Stream search;Retention policy;Locality sensitive hashing;Dynamic popularity},
doi={10.1109/BigData.2017.8258016},
ISSN={},
month={Dec},}
@ARTICLE{7953577,
author={Shuai, Hong-Han and Yang, De-Nian and Shen, Chih-Ya and Yu, Philip S. and Chen, Ming-Syan},
journal={IEEE Transactions on Big Data},
title={QMSampler: Joint Sampling of Multiple Networks with Quality Guarantee},
year={2018},
volume={4},
number={1},
pages={90-104},
abstract={Because Online Social Networks (OSNs) have become increasingly important in the last decade, they have motivated a great deal of research on Social Network Analysis (SNA). Currently, SNA algorithms are evaluated on real datasets obtained from large-scale OSNs, which are usually sampled by Breadth-First-Search (BFS), Random Walk (RW), or some variations of the latter. However, none of the released datasets provides any statistical guarantees on the difference between the sampled datasets and the ground truth. Moreover, all existing sampling algorithms only focus on sampling a single OSN, but each OSN is actually a sampling of a complete social network. Hence, even if the whole dataset from a single OSN is sampled, the results may still be skewed and may not fully reflect the properties of the complete social network. To address the above issues, we have made the first attempt to explore the joint sampling of multiple OSNs and propose an approach called Quality-guaranteed Multi-network Sampler (QMSampler) that can jointly sample multiple OSNs. QMSampler provides a statistical guarantee on the difference between the sampled real dataset and the ground truth (the perfect integration of all OSNs). Our experimental results demonstrate that the proposed approach generates a much smaller bias than any existing method. QMSampler has also been released as a free download.},
keywords={Facebook;Roads;Big Data;Electronic mail;LinkedIn;Measurement;Social network;graph sampler;data quality analysis;optimization},
doi={10.1109/TBDATA.2017.2715847},
ISSN={2332-7790},
month={March},}
@INPROCEEDINGS{9778872,
author={Kong, Weiyu and Wu, Yanmin and Qi, Jinli and Chen, Yanyi},
booktitle={2022 7th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA)},
title={Research on Classification Label Denoising Algorithm Based on Granular Ball},
year={2022},
volume={},
number={},
pages={102-106},
abstract={This paper presents a granular ball denoising method (GBD) which can effectively improve the accuracy and robustness of classification algorithm. GBD method first uses the self-adaptive hypersphere to cover the data space, then eliminates the data not contained in the hypersphere and the noise data in the sphere, and finally uses only the hypersphere data for training, so as to reduce the sample data and improve the data quality. Experiments show that using the data obtained by this method for training can greatly improve the accuracy of the classification algorithm. In addition, because the hypersphere has good adaptive ability, GBD also has excellent robustness. The experiment shows that GBD sampling data training can still get good results after adding noise to the data set. Therefore, GBD is an efficient and robust denoising method.},
keywords={Training;Cloud computing;Data integrity;Noise reduction;Big Data;Robustness;Classification algorithms;classification;label noise;granular ball},
doi={10.1109/ICCCBDA55098.2022.9778872},
ISSN={},
month={April},}
@INPROCEEDINGS{7129549,
author={Tang, Nan},
booktitle={2015 31st IEEE International Conference on Data Engineering Workshops},
title={Big RDF data cleaning},
year={2015},
volume={},
number={},
pages={77-79},
abstract={Without a shadow of a doubt, data cleaning has played an important part in the history of data management and data analytics. Possessing high quality data has been proven to be crucial for businesses to do data driven decision making, especially within the information age and the era of big data. Resource Description Framework (RDF) is a standard model for data interchange on the semantic web. However, it is known that RDF data is dirty, since many of them are automatically extracted from the web. In this paper, we will first revisit data quality problems appeared in RDF data. Although many efforts have been put to clean RDF data, unfortunately, most of them are based on laborious manual evaluation. We will also describe possible solutions that shed lights on (semi-)automatically cleaning (big) RDF data.},
keywords={Resource description framework;Cleaning;Ontologies;Data mining;Knowledge based systems;Conferences;Databases},
doi={10.1109/ICDEW.2015.7129549},
ISSN={},
month={April},}
@INPROCEEDINGS{9605783,
author={Franklin, Paul},
booktitle={2021 Annual Reliability and Maintainability Symposium (RAMS)},
title={Solving Problems with Rapid Data Discovery},
year={2021},
volume={},
number={},
pages={1-3},
abstract={Summary & ConclusionsThis paper describes an approach to extracting reliability data from transaction data and performing analysis on it. Organizations typically collect significant amounts of data that could be used for reliability analysis but is not.Data science is a field that offers reliability engineers insights when faced with analyzing so-called "big data." One subset of data science that can be helpful in this regard is the idea of using data cubes as the basis for analysis. Data cubes use many techniques, such as slicing, aggregation, drill-downs, and pivots [1]. These concepts are widely implemented, and most engineers use them, even if they do not explicitly name them. For example, drill-down would occur when only the data from a particular equipment model is examined; aggregation reverses this. Slicing occurs with all but two dimensions (defined below) are held constant. Pivots occur when data is "rotated" by changing the way rows and columns are selected and displayed.This paper will report on the results of using data cubes to support and drive the culture of reliability engineering:•Rapidly model large datasets to confirm or deny reliability and process measures•Drive data quality improvements•Build confidence in the way business rules are modeled•Develop metrics•Support decision making in the face of uncertaintyThe paper describes the work done and offers some recommendations for implementation.},
keywords={Analytical models;Decision making;Measurement uncertainty;Process control;Random access memory;Organizations;Reliability engineering;Data cubes;Metrics;Decision support},
doi={10.1109/RAMS48097.2021.9605783},
ISSN={2577-0993},
month={May},}
@INPROCEEDINGS{7509814,
author={Fang, Dianjun and Zhang, Yin and Spicher, Klaus},
booktitle={2016 IEEE International Conference on Big Data Analysis (ICBDA)},
title={Forecasting accuracy analysis based on two new heuristic methods and Holt-Winters-Method},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Since 1970s, many academic researchers and business practitioners have started to develop different forecasting methods and models. Most of them are still used in the IT-Systems nowadays. However, they don't perform well enough in practice. People pay much attention to data collection but ignore the data quality, which could lead to low forecasting accuracy. In this paper, we will introduce two new heuristic business forecasting techniques (Revinda and Metrix). Both methods utilize inherent structures of time series. The error analysis is based on B2C and B2B aggregated commercial data. In addition, these two methods will be compared with HOLT-WiNTERS-Methods (HWM) by using error measures MAPE, percentage better and THEIL's U2.},
keywords={Forecasting;Time series analysis;Market research;Mathematical model;Measurement;Predictive models;time series;forecasting method;data quality;error measurement;B2C/B2B forecasting},
doi={10.1109/ICBDA.2016.7509814},
ISSN={},
month={March},}
@INPROCEEDINGS{9708977,
author={Hu, Pan and Gu, Hailin and Qi, Jun and Gao, Qiang and Xia, Yu and Qu, Ruiting},
booktitle={2021 2nd International Conference on Big Data Economy and Information Management (BDEIM)},
title={Design of two-stage federal learning incentive mechanism under specific indicators},
year={2021},
volume={},
number={},
pages={475-478},
abstract={As a privacy-focused distributed machine learning, federated learning can not only train models effectively but also prevent data sets from being leaked easily. However, like crowdsensing perception and other technologies, participants often lack the motivation to learn and the quality of participation is not high. Therefore, this paper mainly designs a two-stage federal learning incentive mechanism based on the Stackelberg game model under a specific model accuracy index. Firstly, we combine data quality and data quantity to construct a federal learning incentive mechanism model under specific indicators. Then, we conduct a two-stage Stackelberg game analysis on the incentive mechanism model based on the utility function construction of the server platform and data island. In the first stage, the platform server is the leader and the data island is the follower. The second stage is a Nash equilibrium game between data islands. Finally, we construct the objective function of the server platform and data island, namely utility maximization, deduce the optimal equilibrium solution of the two-stage game, and determine the optimal strategy of the platform server and data island.},
keywords={Analytical models;Biological system modeling;Games;Machine learning;Nash equilibrium;Linear programming;Collaborative work;Federal learning;Incentive mechanism;Stackelberg game;Nash equilibrium;Specific indicators},
doi={10.1109/BDEIM55082.2021.00103},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7498306,
author={Cao, Wei and Wu, Zhengwei and Wang, Dong and Li, Jian and Wu, Haishan},
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)},
title={Automatic user identification method across heterogeneous mobility data sources},
year={2016},
volume={},
number={},
pages={978-989},
abstract={With the ubiquity of location based services and applications, large volume of mobility data has been generated routinely, usually from heterogeneous data sources, such as different GPS-embedded devices, mobile apps or location based service providers. In this paper, we investigate efficient ways of identifying users across such heterogeneous data sources. We present a MapReduce-based framework called Automatic User Identification (AUI) which is easy to deploy and can scale to very large data set. Our framework is based on a novel similarity measure called the signal based similarity (SIG) which measures the similarity of users' trajectories gathered from different data sources, typically with very different sampling rates and noise patterns. We conduct extensive experimental evaluations, which show that our framework outperforms the existing methods significantly. Our study on one hand provides an effective approach for the mobility data integration problem on large scale data sets, i.e., combining the mobility data sets from different sources in order to enhance the data quality. On the other hand, our study provides an in-depth investigation for the widely studied human mobility uniqueness problem under heterogeneous data sources.},
keywords={Trajectory;Urban areas;Buildings;Noise measurement;Mobile communication;Navigation;Education},
doi={10.1109/ICDE.2016.7498306},
ISSN={},
month={May},}
@INPROCEEDINGS{9345133,
author={Yuan, Fang and Hong, Xianbin and Yuan, Cheng and Fei, Xiang and Guan, Sheng-Uei and Liu, Dawei and Wang, Wei},
booktitle={2020 IEEE 6th International Conference on Computer and Communications (ICCC)},
title={Keywords-oriented Data Augmentation for Chinese},
year={2020},
volume={},
number={},
pages={2006-2012},
abstract={In natural language processing tasks, data is very important, but data collection is not cheap. Large volume data can well serve a series of tasks, especially for deep learning tasks. Data augmentation methods are solutions to data problems, which can work well on rising data quality and quantity, such as generating text without meaning changing and expanding the diversity of data distribution. A user-friendly method of the data augmentation is to sample words in a text then augmenting them. The sampling method is often implemented by a random probability. Although the performance of this solution has been proved over the past few years, random sampling is not the best choice for the data augmentation as it has a chance of randomly introducing some noise into initial data, like stop words. The generated data could interfere with the subsequent tasks and drop the accuracy of the tasks' solutions. Hence, this paper aims to introduce a novel data augmentation method that could avoid involving such noisy data. The strategy is keywords-oriented data augmentation for Chinese (KDA). The KDA proposed in this paper indicates a method of extracting keywords based on category labels, and an augmenting method based on the keywords. In contrast to randomness, the proposed technique firstly selects the key information data, then expands the selected data. The experimental section is compared with another two typical data augmentation techniques on three Chinese data sets for text classification tasks. The result shows that the KDA technique has a better performance in the data augmentation task than the compared two.},
keywords={Deep learning;Data integrity;Text categorization;Sampling methods;Natural language processing;Noise measurement;Task analysis;Data Augmentation;Chinese;Classification},
doi={10.1109/ICCC51575.2020.9345133},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9245416,
author={Georgieva, P. and Nikolova, E. and Orozova, D.},
booktitle={2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)},
title={Data Cleaning Techniques in Detecting Tendencies in Software Engineering},
year={2020},
volume={},
number={},
pages={1028-1033},
abstract={The world of software engineering is dynamically changing over the last decade. Providing adequate university education is one of the key goals of the academic community for ensuring advanced and up-to-date students' training. One direction in achieving this goal is to constantly monitor the trends in the Information Technology (IT) sector. A reliable source of information is the data from the annual survey on technology and programming languages, as well as on preferred learning methods and ways to enhance competencies, conducted amongst Stack Overflow users since 2011. In processing the data from the survey, the authors have faced several problems that have provoked interest in a more general data problem - data quality and data cleaning.This paper looks into data quality, tools for data cleaning and the characteristics of high-quality data. A classification of data problems is proposed in the context of analyzing the information about software developers. Additionally, the proposed process of data cleaning in illustrated with data for 2018 and 2019.},
keywords={Training;Data integrity;Tools;Strategic planning;Cleaning;Monitoring;Software engineering;Big Data Analytics;Data quality;Data cleaning;Software engineering},
doi={10.23919/MIPRO48935.2020.9245416},
ISSN={2623-8764},
month={Sep.},}
@INPROCEEDINGS{8271966,
author={Hassanein, Hossam S. and Oteafy, Sharief M. A.},
booktitle={2017 13th International Conference on Distributed Computing in Sensor Systems (DCOSS)},
title={Big Sensed Data Challenges in the Internet of Things},
year={2017},
volume={},
number={},
pages={207-208},
abstract={Internet of Things (IoT) systems are inherently built on data gathered from heterogeneous sources. In the quest to gather more data for better analytics, many IoT systems are instigating significant challenges. First, the sheer volume and velocity of data generated by IoT systems are burdening our networking infrastructure, especially at the edge. The mobility and intermittent connectivity of edge IoT nodes are further hampering real-time access and reporting of IoT data. As we attempt to synergize IoT systems to leverage resource discovery and remedy some of these challenges, the rising challenges of Quality of Information (QoI) and Quality of Resource (QoR) calibration, render many IoT interoperability attempts far-fetched. We survey a number of challenges in realizing IoT interoperability, and advocate for a uniform view of data management in IoT systems. We delve into three planes that encompass Big Sensed Data (BSD) research directions, presenting a building block for future research efforts in IoT data management.},
keywords={Sensors;Calibration;Internet of Things;Data integration;Conferences;Interoperability;Standards;Internet of Things;Big Sensed Data;Next Generation Networks;Quality of Data;Quality of Information},
doi={10.1109/DCOSS.2017.35},
ISSN={2325-2944},
month={June},}
@INPROCEEDINGS{9874246,
author={Zhang, Liang and Wang, Jia and Cui, Dandan and Fu, Anna and Liu, Xiulei and Xu, Daozhu},
booktitle={2022 8th International Conference on Big Data and Information Analytics (BigDIA)},
title={Entity Alignment in the Construction of Knowledge Graphs for Civilian Transportation Equipment},
year={2022},
volume={},
number={},
pages={104-111},
abstract={Knowledge graphs (KGs) have provided a better approach to organize and manage multi-source knowledge in the field of civilian transportation equipment. However, due to the existence of isolated data islands, there are still problems such as unidentified attribute relations in the KGs for civilian transportation equipment. An effective solution to these problems can be achieved through entity alignment. In this paper, we propose two entity alignment strategies for different purposes. Specifically, aiming at the incompleteness of single-source knowledge in the construction of KGs for civilian transportation equipment, we employ an instance alignment strategy to enable multi-source fusion of attribute relationships. In addition, aiming at the inconsistency between different attribute values of the same entity, we propose a conflict resolution strategy based on multi-source data quality assessment, and achieve the unity of attribute values. Compared with previous KGs, our proposed entity alignment method is able to moderately enrich the attribute relations in KGs for civilian transportation equipment.},
keywords={Data integrity;Knowledge based systems;Transportation;Big Data;knowledge graph;entity alignment;conflict resolution},
doi={10.1109/BigDIA56350.2022.9874246},
ISSN={2771-6902},
month={Aug},}
@ARTICLE{8372637,
author={Dong, Yongquan and Dragut, Eduard C. and Meng, Weiyi},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Normalization of Duplicate Records from Multiple Sources},
year={2019},
volume={31},
number={4},
pages={769-782},
abstract={Data consolidation is a challenging issue in data integration. The usefulness of data increases when it is linked and fused with other data from numerous (Web) sources. The promise of Big Data hinges upon addressing several big data integration challenges, such as record linkage at scale, real-time data fusion, and integrating Deep Web. Although much work has been conducted on these problems, there is limited work on creating a uniform, standard record from a group of records corresponding to the same real-world entity. We refer to this task as record normalization. Such a record representation, coined normalized record, is important for both front-end and back-end applications. In this paper, we formalize the record normalization problem, present in-depth analysis of normalization granularity levels (e.g., record, field, and value-component) and of normalization forms (e.g., typical versus complete). We propose a comprehensive framework for computing the normalized record. The proposed framework includes a suit of record normalization methods, from naive ones, which use only the information gathered from records themselves, to complex strategies, which globally mine a group of duplicate records before selecting a value for an attribute of a normalized record. We conducted extensive empirical studies with all the proposed methods. We indicate the weaknesses and strengths of each of them and recommend the ones to be used in practice.},
keywords={Data integration;Standards;Task analysis;Databases;Google;Data mining;Terminology;Record normalization;data quality;data fusion;web data integration;deep web},
doi={10.1109/TKDE.2018.2844176},
ISSN={1558-2191},
month={April},}
@INPROCEEDINGS{9799848,
author={Khoshaba, Farah and Kareem, Shahab and Awla, Hoshang and Mohammed, Chnar},
booktitle={2022 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)},
title={Machine learning algorithms in Bigdata Analysis and its applications: A review},
year={2022},
volume={},
number={},
pages={1-8},
abstract={A wide range of disparate variety of heterogeneous and even disparate data sources has been integrated into the computer science research principles through the assistance of Artificial Intelligence (AI) and Machine Learning (ML) delivering excellent results in accuracy and data quality. Even so, taking machine learning approaches to very broad and dynamic datasets is both logistically and computationally costly. Some people are generating so much data regularly, which means that advanced analytics platforms have greater importance than ever before. Spark Machine Learning has extensive capabilities for a variety of ML activities such as regression, grouping, dimension reduction, and extraction of rules. While it has numerous datasets of high quality and consistency of structure. The specification of the MapReduce programming model was intended for the massive density of large-volume computation. It accomplishes this by splitting the job into several well-defined sub-sized units. There are various sets of data mining frameworks that link to a distributed device. Hadoop serves as the infrastructure for the distributed system. Several resources claim that Mahout will scale to handle massive datasets. This article discusses and demonstrates the use of algorithms and their significant impact on Bigdata, as well as a comparison between their implementations among Apache Spark and Apache Mahout.},
keywords={Machine learning algorithms;Social networking (online);Soft sensors;Training data;Big Data;Programming;Sparks;Big data;Machine Learning;Apache Spark;Apache Mahout;Map Reduce programming},
doi={10.1109/HORA55278.2022.9799848},
ISSN={},
month={June},}
@INPROCEEDINGS{9778298,
author={Chen, Honghu and Zhou, Te and Yang, Chao and Li, Qiang and Peng, Bo and Cheng, Qing},
booktitle={2022 7th International Conference on Intelligent Computing and Signal Processing (ICSP)},
title={Cloud-edge collaborative data processing architecture for state assessment of transmission equipments},
year={2022},
volume={},
number={},
pages={458-461},
abstract={In the process of asset status assessment, the power transmission intelligent Internet of Things (IoT) with smart towers as the core IoT nodes faces many problems such as large workload of physical information, poor data quality, large data processing delay and heavy cloud computing pressure. At the same time, traditional front-end sensing equipment is limited by the actual hardware computing power level and low power consumption requirements, which makes the front-end algorithm low in intelligence and consumes a lot of manual data verification. In view of the above problems, this paper proposes a cloud-edge collaborative data processing architecture suitable for transmission asset status assessment by combining big data framework, deep learning and edge computing technology. The architecture clearly divides the functions of the cloud, edge and data terminals based on the status assessment requirements of power transmission assets, and then divides a part of the data processing and analysis operations in the cloud to the edge, which reduces the computing pressure on the cloud and enhances resources utilization rate.},
keywords={Cloud computing;Power demand;Collaboration;Power transmission;Signal processing algorithms;Computer architecture;Signal processing;cloud computing;edge computing;cloud edge collaboration;transmission assets;state assessment},
doi={10.1109/ICSP54964.2022.9778298},
ISSN={},
month={April},}
@INPROCEEDINGS{9047270,
author={Huang, Chao and Lin, Mingwei and Chen, Riqing},
booktitle={2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)},
title={Probabilistic Linguistic VIKOR Method Based on TODIM for Reliable Participant Selection Problem in Mobile Crowdsensing},
year={2019},
volume={},
number={},
pages={712-717},
abstract={In the mobile crowdsensing systems, the participants of great variety and diversity voluntarily submit their sensing data. Evaluating the participants and ranking them is a critical problem that should be solved to ensure the data quality. In this paper, we introduce the concept of probabilistic linguistic term sets (PLTSs) to model the group preference information during the process of ranking candidate participants and then propose novel VIKOR methods based on TODIM for solving the process of ranking reliable participants and selecting the best one in the mobile crowdsensing system. This proposed methods combine the advantages from the VIKOR method and TODIM method. To show the implementation process of evaluating participants and selecting the best one under the PLTS information context, a practical case is given to verify the feasibility of the proposed methods. Compared with the existing decision making methods, the proposed methods show their effectiveness.},
keywords={Linguistics;Sensors;Crowdsensing;Task analysis;Decision making;Probabilistic logic;Cloud computing;Mobile crowdsensing, Probabilistic linguistic term set, Participants ranking, TODIM, VIKOR},
doi={10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00108},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8510969,
author={Zhang, Yang and Wang, Dong},
booktitle={2018 14th International Conference on Distributed Computing in Sensor Systems (DCOSS)},
title={Poster: On Cost-Sensitive Task Allocation in Social Sensing: An Online Learning Approach},
year={2018},
volume={},
number={},
pages={115-116},
abstract={Social sensing has emerged as a new sensing paradigm where human sensors collectively report measurements about the physical world. This paper focuses on the cost-sensitive task allocation problem in social sensing where the goal is to effectively allocate sensing tasks to the human sensors to meet the desirable data quality requirement of the applications while minimizing the sensing cost. While recent progress has been made to tackle the cost-sensitive task allocation problem, an important challenge has not been well addressed, namely "real time task allocation", the task allocation schemes need to respond quickly to the potential large dynamics of the measured variables in social sensing. To address this challenge, this paper presents a Cost-Sensitive Task Allocation (CSTA) scheme inspired by techniques from online learning. The preliminary results show that our new scheme significantly outperforms the-state-of-the-art baselines.},
keywords={Sensors;Task analysis;Resource management;Real-time systems;Big Data;Dynamic scheduling;Air quality;Social Sensing;Task Allocation;Online Learning},
doi={10.1109/DCOSS.2018.00024},
ISSN={2325-2944},
month={June},}
@ARTICLE{9715073,
author={Rudnitckaia, Julia and Venkatachalam, Hari Santhosh and Essmann, Roland and Hruška, Tomáš and Colombo, Armando Walter},
journal={IEEE Access},
title={Screening Process Mining and Value Stream Techniques on Industrial Manufacturing Processes: Process Modelling and Bottleneck Analysis},
year={2022},
volume={10},
number={},
pages={24203-24214},
abstract={One major result of the Industrial Digitalization is the access to a large set of digitalized data and information, i.e. Big Data. The market of analytic tools offers a huge variety of algorithms and software to exploit big datasets. Implementing their advantages into one approach brings better results and empower possibilities for process analysis. Its application in the manufacturing industry requires a high level of effort and remains to be challenging due to product complexity, human-centric processes, and data quality. In this manuscript, the authors combine process mining and value streams methods for analyzing the data from the information management system, applying the approach to the data delivered by one specific manufacturing system. The manufacturing process to be examined is the process of assembling gas meters in the manufacture. This specific and important part of the whole supply-chain process was taken as suitable for the study due to almost full-automated line with data about each process activity of the value-stream in the information system. The paper applies process mining algorithms in discovering a descriptive process model that plays the main role as a basis for further analysis. At the same time, modern techniques of the bottleneck analysis are described, and two new comprehensible methods of bottlenecks detection (TimeLag and Confidence intervals methods), as well as their advantages, will be discussed. Achieved results can be subsequently used for other sources of big data and industrial-compliant Information Management Systems.},
keywords={Data mining;Manufacturing;Information management;Production;Manufacturing processes;Companies;Analytical models;Bottleneck analysis;manufacturing process;process mining;process modelling;information management system;value stream},
doi={10.1109/ACCESS.2022.3152211},
ISSN={2169-3536},
month={},}
@ARTICLE{7933943,
author={Islam, MD. Mofijul and Razzaque, MD. Abdur and Hassan, Mohammad Mehedi and Ismail, Walaa Nagy and Song, Biao},
journal={IEEE Access},
title={Mobile Cloud-Based Big Healthcare Data Processing in Smart Cities},
year={2017},
volume={5},
number={},
pages={11887-11899},
abstract={In recent years, the Smart City concept has become popular for its promise to improve the quality of life of urban citizens. The concept involves multiple disciplines, such as Smart health care, Smart transportation, and Smart community. Most services in Smart Cities, especially in the Smart healthcare domain, require the real-time sharing, processing, and analyzing of Big Healthcare Data for intelligent decision making. Therefore, a strong wireless and mobile communication infrastructure is necessary to connect and access Smart healthcare services, people, and sensors seamlessly, anywhere at any time. In this scenario, mobile cloud computing (MCC) can play a vital role by offloading Big Healthcare Data related tasks, such as sharing, processing, and analysis, from mobile applications to cloud resources, ensuring quality of service demands of end users. Such resource migration, which is also termed virtual machine (VM) migration, is effective in the Smart healthcare scenario in Smart Cities. In this paper, we propose an ant colony optimization-based joint VM migration model for a heterogeneous, MCC-based Smart Healthcare system in Smart City environment. In this model, the user’s mobility and provisioned VM resources in the cloud address the VM migration problem. We also present a thorough performance evaluation to investigate the effectiveness of our proposed model compared with the state-of-the-art approaches.},
keywords={Cloud computing;Medical services;Mobile communication;Smart cities;Servers;Real-time systems;Quality of service;Smart health care;smart city;big data;quality of service (QoS);virtual machine migration;ant colony optimization},
doi={10.1109/ACCESS.2017.2707439},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7403678,
author={Tekieh, Mohammad Hossein and Raahemi, Bijan},
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={Importance of data mining in healthcare: A survey},
year={2015},
volume={},
number={},
pages={1057-1062},
abstract={In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.},
keywords={Data mining;Diseases;Insurance;Data analysis;Organizations;Sociology;data mining;health data analysis;data quality;predictive modelling;health big data;data mining applications},
doi={10.1145/2808797.2809367},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9655853,
author={Lawson, Victor J. and Banerjee, Madhushri},
booktitle={2021 IEEE 5th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)},
title={Measuring the Impact of an IoT Temperature Sensor Framework for Tracking Contagious Diseases},
year={2021},
volume={},
number={},
pages={332-337},
abstract={Due to the COVID-19 pandemic, much computer science research has been dedicated to utilizing sensor readings for medical purposes. Throughout this period, the need for virus symptom tracking has become a promising area for remotely deployed sensor networks and platforms. Our research goal is to prove that the temperature readings from these sensor network platforms can be statistically linked to public record, medical case study data. The expected outcome of our project is to prove the correlation between sensor network tracking of remote human temperature data and medical records for COVID cases. The results of this study will prove that tracking human temperature can assist in tracking disease outbreaks in various populations. Our framework platform is comprised of four main modules: (1) Temperature Collection, (2) Internal Data Validation (3) Internal-External data merger, (4) Data Analytics. The temperature data are collected from internal databases, mobile sensing devices and medical health professionals. After collection, the internal data are validated by our software, TAU-FIVE, a multi-tier data quality validation system, then merged with external data sources into a data analytic based data warehouse. The data mart queries are designed to compare the location and date of temperature sensor data with known data sets from government officials. Once blended into a fully operational data warehouse, these data marts produce high quality data analysis linking remotely sensed human temperature readings to sources of disease outbreaks.},
keywords={Temperature sensors;COVID-19;Temperature measurement;Temperature distribution;Data analysis;Correlation;Databases;Data Quality;Data Analytics;Information Quality;Big Data;Data Integration},
doi={10.1109/ICITISEE53823.2021.9655853},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8965303,
author={Caihong, Zhou and Zengyuan, Wu and Chang, Liu},
booktitle={2019 IEEE International Conference on Smart Manufacturing, Industrial & Logistics Engineering (SMILE)},
title={A Study on Quality Prediction for Smart Manufacturing Based on the Optimized BP-AdaBoost Model},
year={2019},
volume={},
number={},
pages={1-3},
abstract={To accurately predict the product quality in smart manufacturing, this paper designs the BP-AdsysBoost model on the basis of BP neural network and AdaBoost algorithm. The BP-AdsysBoost model considers both the data characteristics and the technology advantages, which pays more attentions to the unqualified products wrongly predicted. To further examine the model, the 110560 data of smart manufacturing from German BOSCH company is used for this research. The proposed BP-AdsysBoost model is compared with the BP neural network and the unmodified BP-AdaBoost model according to prediction performance. The results show that the BP-AdsysBoost model has significant advantages in prediction accuracy and FDR, which proves its satisfied prediction ability for product quality in smart manufacturing.},
keywords={smart manufacturing;big data;quality prediction;BP neural network;AdaBoost algorithm;BP-AdsysBoost model},
doi={10.1109/SMILE45626.2019.8965303},
ISSN={},
month={April},}
@INPROCEEDINGS{9421436,
author={Wang, Xiaofeng and Jiang, Yong and Zhan, Gaofeng and Zhao, Tong},
booktitle={2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)},
title={Quality Analysis and Evaluation Method for Multisource Aggregation Data based on Structural Equation Model},
year={2020},
volume={},
number={},
pages={1279-1282},
abstract={In the era of big data, how to evaluate the data quality of multi-source aggregation data is very important. The reason is that uneven data quality will directly lead to inaccurate or ambiguous data in the database, and indirectly lead to the deviation of subsequent data mining and decision-making. In this paper, structural equation model(SEM) is introduced to explore the effectiveness of various data quality evaluation indicators in data aggregation and finding out internal relationship between them. A new quality evaluation method of multi-source aggregation data is proposed, based on the regression's significance analysis and factor loads of each observation index in the SEM model. The case analysis shows that the proposed method is feasible and can be used to evaluate the quality of multi-source aggregation data adaptively for a long time.},
keywords={Analytical models;Adaptation models;Numerical analysis;Data integrity;Computational modeling;Urban areas;Data aggregation;Data Aggregation;Data Analysis;Quality Evaluation;Structural Equation Model},
doi={10.1109/ICMCCE51767.2020.00280},
ISSN={},
month={Dec},}
@ARTICLE{9093050,
author={Azmy, Sherif B. and Zorba, Nizar and Hassanein, Hossam S.},
journal={IEEE Internet of Things Journal},
title={Quality Estimation for Scarce Scenarios Within Mobile Crowdsensing Systems},
year={2020},
volume={7},
number={11},
pages={10955-10968},
abstract={Mobile crowdsensing (MCS) is a paradigm that exploits the presence of a crowd of moving human participants to acquire, or generate, data from their environment. As a part of the Internet-of-Things (IoT) paradigm, MCS serves the quest for a more efficient operation of a smart city. Big data techniques employed on this data produce inferences about the participants' environment, the smart city. However, sufficient amounts of data are not always available. Sometimes, the available data are scarce as it is obtained at different times, locations, and from different MCS participants who may not be present. As a consequence, the scale of data acquired may be small and susceptible to errors. In such scenarios, the MCS system requires techniques that acquire reliable inferences from such limited data sets. To that end, we resort to small data (SD) techniques that are relevant for scarce and erroneous scenarios. In this article, we discuss SD and propose schemes to tackle the problems associated with such limited data sets, in the context of the smart city. We propose two novel quality metrics: 1) MAD quality metric (MAD-Q) and 2) MAD bootstrap quality metric (MADBS-Q), to deal with SD, focusing on evaluating the quality of a data set within MCS. We also propose an MCS-specific coverage metric that combines the spatial dimension with MAD-Q and MADBS-Q. We show the performance of all the presented techniques through closed-form mathematical expressions, with which simulation results were found to be consistent.},
keywords={Measurement;Internet of Things;Standards;Smart cities;Task analysis;Intelligent sensors;Data quality;Internet of Things (IoT);IoT architectures;IoT-based services;mobile crowdsensing (MCS);small data (SD)},
doi={10.1109/JIOT.2020.2994556},
ISSN={2327-4662},
month={Nov},}
@INPROCEEDINGS{6943406,
author={Zhi, Yanling and Liu, Gang and Wang, Huimin},
booktitle={2014 11th International Conference on Service Systems and Service Management (ICSSSM)},
title={Research on data gray correction model based on grey interval number — A case study of Chinese ecological civilization evaluation},
year={2014},
volume={},
number={},
pages={1-5},
abstract={In the age of Big Data, we must do best to economically extract value from very large volumes of a wide variety of statistics. However, because of subjective and objective reasons, it is becoming increasing clear that much data is of poor quality, which has serious effects on the research results. With the analysis on the cause and process of the low quality data, this paper introduces the concept of gray system and proposes a data-gray-correction model, which could change the original data into gray interval number and reduce the influence from the former. Assessing the quality of data with classical econometric model and correcting the data by error correction model, then find the reasonable range of the real data and instead the crisp data by interval number, which contain much more information. An example is provided to illustrate the ecological civilization evaluation process under gray interval number.},
keywords={Biological system modeling;Economic indicators;Data models;Educational institutions;Accuracy;Mathematical model;Analytical models;data quality;gray interval number;ecological civilization evaluation},
doi={10.1109/ICSSSM.2014.6943406},
ISSN={2161-1904},
month={June},}
@INBOOK{9823132,
author={Monino, Jean-Louis},
booktitle={Data Control: Major Challenge for the Digital Society},
title={From Knowledge to Strategic Business Intelligence},
year={2020},
volume={},
number={},
pages={121-149},
abstract={The exploitation of Big Data requires reconsidering the processes of data collection, processing and management. There is a need for governance to classify data and prioritize analytical priorities. The challenge of data value is to specify and determine which data are intelligently usable. Data governance combines a set of people, processes and technologies to ensure the quality and value of an organization's data. The technological part of data governance combines data quality, integration and management. All the fields that complement business intelligence, such as knowledge management, information protection, lobbying, can be grouped together in the overall concept of strategic intelligence. Strategic economic intelligence is a mode of governance whose purpose is the control and protection of strategic and relevant information for any economic actor. Economic intelligence can be seen as a new managerial practice at the service of the company's strategy, enabling it to improve its competitiveness.},
keywords={Companies;Big Data;Data visualization;Social networking (online);Decision making;Data mining;Data centers},
doi={10.1002/9781119779780.ch6},
ISSN={},
publisher={Wiley},
isbn={9781119779810},
url={https://ieeexplore.ieee.org/document/9823132},}
@INPROCEEDINGS{8455917,
author={Qiang, Li and Zhengwei, Jiang and Zeming, Yang and Baoxu, Liu and Xin, Wang and Yunan, Zhang},
booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)},
title={A Quality Evaluation Method of Cyber Threat Intelligence in User Perspective},
year={2018},
volume={},
number={},
pages={269-276},
abstract={With the widely use of cyber threat intelligence, the influence of security threats and cyber attacks have been relieved and controlled in a degree. More and more users have accepted the conception of threat intelligence and are trying to use threat intelligence in routine security protection. Then, how to choose appropriate threat intelligence vendors and services has become a crucial issue. The present research of threat intelligence evaluation mainly focused on one-sided threat intelligence contents and approaches, which was lack of comprehensiveness and effectiveness. Aiming at this situation, we propose the comprehensive evaluation architecture of threat intelligence in user perspective to evaluate threat intelligence services in several dimensions with quantitative index system. We also carried out typical experiments for threat intelligence data feeds and comprehensive situation to verify the feasibility of proposed method. The results show that the proposed evaluation method has a clear advantage in coverage and partition degree.},
keywords={Indexes;Testing;Quantization (signal);Feeds;Blacklisting;Business;Threat Intelligence, Quality Evaluation, User Perspective, Vendor},
doi={10.1109/TrustCom/BigDataSE.2018.00049},
ISSN={2324-9013},
month={Aug},}
@INPROCEEDINGS{7785346,
author={Drakopoulos, Georgios and Megalooikonomou, Vasileios},
booktitle={2016 7th International Conference on Information, Intelligence, Systems & Applications (IISA)},
title={Regularizing large biosignals with finite differences},
year={2016},
volume={},
number={},
pages={1-6},
abstract={In the biomedical analytics pipeline data preprocessing is the first and crucial step as subsequent results and visualization depend heavily on original data quality. However, the latter often contain a large number of outliers or missing values. Moreover, they may be corrupted by noise of unknown characteristics. This is in many cases aggravated by lack of sufficient information to construct a data cleaning mechanism. Regularization techniques remove erroneous values and complete missing ones while requiring little or no information regarding either data or noise dynamics. This paper examines the theory and practice of a regularization class based on finite differences and implemented through the conjugate gradient method. Moreover, it explores the connection of finite differences to the discrete Laplace operator. The results obtained from applying the proposed regularization techniques to heart rate time series from the MIT-BIH dataset are discussed.},
keywords={Time series analysis;Signal processing algorithms;Laplace equations;Cost function;Electrocardiography;Finite difference methods;Big data;Finite difference matrix;Regularization;Biosignal processing;Big data analytics;Conjugate gradient;Discrete Laplace operator;Electrocardiogram;Heartbeat rate},
doi={10.1109/IISA.2016.7785346},
ISSN={},
month={July},}
@INPROCEEDINGS{6729628,
author={Xie, Sihong and Kong, Xiangnan and Gao, Jing and Fan, Wei and Yu, Philip S.},
booktitle={2013 IEEE 13th International Conference on Data Mining},
title={Multilabel Consensus Classification},
year={2013},
volume={},
number={},
pages={1241-1246},
abstract={In the era of big data, a large amount of noisy and incomplete data can be collected from multiple sources for prediction tasks. Combining multiple models or data sources helps to counteract the effects of low data quality and the bias of any single model or data source, and thus can improve the robustness and the performance of predictive models. Out of privacy, storage and bandwidth considerations, in certain circumstances one has to combine the predictions from multiple models or data sources without accessing the raw data. Consensus-based prediction combination algorithms are effective for such situations. However, current research on prediction combination focuses on the single label setting, where an instance can have one and only one label. Nonetheless, data nowadays are usually multilabeled, such that more than one label have to be predicted at the same time. Direct applications of existing prediction combination methods to multilabel settings can lead to degenerated performance. In this paper, we address the challenges of combining predictions from multiple multilabel classifiers and propose two novel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and MLCM-a (MLCM for microAUC). These algorithms can capture label correlations that are common in multilabel classifications, and optimize corresponding performance metrics. Experimental results on popular multilabel classification tasks verify the theoretical analysis and effectiveness of the proposed methods.},
keywords={Predictive models;Correlation;Measurement;Prediction algorithms;Data models;Bipartite graph;Algorithm design and analysis;multilabel classification;ensemble},
doi={10.1109/ICDM.2013.97},
ISSN={2374-8486},
month={Dec},}
@INPROCEEDINGS{6903506,
author={Chakravorty, Antorweep and Wlodarczyk, Tomasz Wiktor and Rong, Chunming},
booktitle={2014 IEEE International Conference on Cloud Engineering},
title={A Scalable K-Anonymization Solution for Preserving Privacy in an Aging-in-Place Welfare Intercloud},
year={2014},
volume={},
number={},
pages={424-431},
abstract={Aging-in-Place solutions are becoming increasingly prevalent in our society. New age big data technologies can harness upon enormous amount of data generated from sensors in smart homes to provide enabling services. Added care and preventive services can be furnished through interoperability and bidirectional dataflow across the value chain. However the nature of the problem domain which although allows establishing better care through sharing of information also risks disclosing complete living behavior of individuals. In this paper, we introduce and evaluate a novel scalable k-anonymization solution based upon the distributed map-reduce paradigm for preserving privacy of the shared data in a welfare intercloud. Our evaluation benchmarks both information loss and data quality metrics and demonstrates better scalability/performance than any other available solutions.},
keywords={Data privacy;Partitioning algorithms;Cryptography;Smart homes;Scalability;Dictionaries;Privacy;privacy;k-anonymization;hadoop;intercloud;aging in place},
doi={10.1109/IC2E.2014.43},
ISSN={},
month={March},}
@INPROCEEDINGS{6930153,
author={Yang, Longzhi and Neagu, Daniel},
booktitle={2014 14th UK Workshop on Computational Intelligence (UKCI)},
title={Integration strategies for toxicity data from an empirical perspective},
year={2014},
volume={},
number={},
pages={1-8},
abstract={The recent development of information techniques, especially the state-of-the-art “big data” solutions, enables the extracting, gathering, and processing large amount of toxicity information from multiple sources. Facilitated by this technology advance, a framework named integrated testing strategies (ITS) has been proposed in the predictive toxicology domain, in an effort to intelligently jointly use multiple heterogeneous toxicity data records (through data fusion, grouping, interpolation/extrapolation etc.) for toxicity assessment. This will ultimately contribute to accelerating the development cycle of chemical products, reducing animal use, and decreasing development costs. Most of the current study in ITS is based on a group of consensus processes, termed weight of evidence (WoE), which quantitatively integrate all the relevant data instances towards the same endpoint into an integrated decision supported by data quality. Several WoE implementations for the particular case of toxicity data fusion have been presented in the literature, which are collectively studied in this paper. Noting that these uncertainty handling methodologies are usually not simply developed from conventional probability theory due to the unavailability of big datasets, this paper first investigates the mathematical foundations of these approaches. Then, the investigated data integration models are applied to a representative case in the predictive toxicology domain, with the experimental results compared and analysed.},
keywords={Reliability;Mathematical model;Data integration;Bayes methods;Equations;Uncertainty},
doi={10.1109/UKCI.2014.6930153},
ISSN={2162-7657},
month={Sep.},}
@INPROCEEDINGS{8002473,
author={Haneem, Faizura and Ali, Rosmah and Kama, Nazri and Basri, Sufyan},
booktitle={2017 International Conference on Research and Innovation in Information Systems (ICRIIS)},
title={Descriptive analysis and text analysis in Systematic Literature Review: A review of Master Data Management},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Systematic Literature Review (SLR) is a structured way of conducting a review of existing research works produced by the earlier researchers. The application of right data analysis technique during the SLR evaluation stage would give an insight to the researcher in achieving the SLR objective. This paper presents how descriptive analysis and text analysis can be applied to achieve one of the common SLR objectives which is to study the progress of specific research domain. These techniques have been demonstrated to synthesis the progress of Master Data Management research domain. Using descriptive analysis technique, this study has identified a trend of related literary works distribution by years, sources, and publication types. Meanwhile, text analysis shows the common terms and interest topics in the Master Data Management research which are 1) master data, 2) data quality, 3) business intelligence, 4) business process, 5) data integration, 6) big data, 7) data governance, 8) information governance, 9) data management and 10) product data. It is hoped that other researchers would be able to replicate these analysis techniques in performing SLR for other research domains.},
keywords={Text analysis;Databases;Technological innovation;Frequency-domain analysis;Text mining;Quality assessment;Systematic Literature Review;Descriptive Analysis;Text Analysis;Master Data Management},
doi={10.1109/ICRIIS.2017.8002473},
ISSN={2324-8157},
month={July},}
@INPROCEEDINGS{9136335,
author={Zhichun, Yang and Yu, Shen and Fan, Yang and Yang, LEI and Lei, Su and Fangbin, Yan},
booktitle={2020 5th Asia Conference on Power and Electrical Engineering (ACPEE)},
title={Topology identification method of low voltage distribution network based on data association analysis},
year={2020},
volume={},
number={},
pages={2226-2230},
abstract={This paper introduces a topology identification method of low-voltage distribution network based on data association analysis. The low-voltage distribution network to be identified is divided into the single distribution transformer power off station areas, multiple distribution transformer station areas caused by 10kV distribution line power outage and the distribution transformer areas without power interruption based on low-voltage distribution network blackout event, restoration power on event and geographic location information. In each type of station area, Tanimoto similarity coefficient is used to calculate the correlation and non-correlation between distribution transformer, branch box, meter box and smart meter in each group, so as to achieve the topology identification of the low-voltage distribution network. And then the identified topology can be verified by combining the topology verification rules of the same distribution transformer station area has the same of outage and live state, outage duration, geographical location, power supply radius and so on. Through the actual case, it is proved that the method proposed in this paper can solve the problems of large amount of calculation, inaccuracy of calculation results, and inability to verify based on the existing big data mining methods. It realizes the efficient and accurate identification of distribution transformer substation topology, and improves the information level and data quality of distribution network.},
keywords={Meters;Low voltage;Substations;Network topology;Power supplies;Distribution networks;Transformers;Low voltage distribution network;Internet of things;topology identification;association analysis;topology verification},
doi={10.1109/ACPEE48638.2020.9136335},
ISSN={},
month={June},}
@INPROCEEDINGS{9713501,
author={Liu, Jiaxin and Wang, Shuai and Lu, Xuchen and Li, Tong},
booktitle={2021 IEEE 5th Conference on Energy Internet and Energy System Integration (EI2)},
title={Research on Online Status Evaluation Technology for Main Equipment of Power Transmission and Transformation Based on Digital Twin},
year={2021},
volume={},
number={},
pages={3368-3373},
abstract={Traditional status evaluation for main equipment of power transmission and transformation has some shortages, such as low timeliness, low data quality and difficulty for evaluation model construction. Based on digital twin technology system, this paper presents technology of equipment status, and constructs digital twin for power transmission and transformation equipment. According to operation characteristics of power transmission and transformation equipment, fusion and cleansing of perception data is realized. Relying on big data analysis and data mining, status evaluation differentiation, accurate fault diagnosis and status prediction for power transmission and transformation equipment is realized. Further more, this paper analyzes the application of digital twin technology in on-line status evaluation for transformer equipment, expounds specific application of digital twin technology including data governance and model building, and summarizes application prospect of digital twin technology in on-line status evaluation for main equipment of power transmission and transformation.},
keywords={Digital twin;Power transmission;System integration;Maintenance engineering;Reliability engineering;Transformers;Real-time systems;digital twin;status evaluation;online evaluation;fault diagnosis;status prediction;transformer},
doi={10.1109/EI252483.2021.9713501},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9407474,
author={Huang, Haiyan and Wei, Bizhong and Dai, Jian and Ke, Wenlong},
booktitle={2020 16th International Conference on Computational Intelligence and Security (CIS)},
title={Data Preprocessing Method For The Analysis Of Incomplete Data On Students In Poverty},
year={2020},
volume={},
number={},
pages={248-252},
abstract={Data mining is the focus of big data applications in various fields. Data pre-processing is a crucial step in the data mining process. With the development of the information society and the application of databases, the educational data has seen explosive growth, and the data on poor students has become informative. However, the actual student financial aid management system collects the data on poor students which generally has problems such as missing values, attributes redundancy, and noise. To solve this problem, we proposed a novel method called DPBP to preprocess data. The proposed DPBP approach consists of four stages: the preparation of data, the scoping of characteristics, the combination of characteristics, and the filtering of missing number. Firstly, we prepare the dataset by extracting data. Next, the characteristic range is limited by choosing experimental results of feature selection algorithm. Then, third stage performs feature combination to obtain the feature decomposition sets. Finally, based on accuracy and missing number, we gain the optimal dataset. Series of experiments result show that our proposed method significantly improves the data quality and stability.},
keywords={Filtering;Databases;Data integrity;Redundancy;Data preprocessing;Feature extraction;Stability analysis;data mining;data preprocessing;feature selection},
doi={10.1109/CIS52066.2020.00060},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9842582,
author={Schiegg, Sascha and Gerl, Armin},
booktitle={2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)},
title={Trade-off between Privacy, Quality and Risk: Anonymization Strategy Evaluation for Data Warehouses},
year={2022},
volume={},
number={},
pages={1555-1560},
abstract={The transformation of big data to the cloud requires us to reconsider trust. Trust in all parties involved in the data management, the infrastructure as well as all groups with an access interest. A common way to mitigate the risk of the identification of individuals in case of privacy breaches is anonymization, which consequently also leads to information loss. Depending on the assumed level of confidence, a data processor can control the risk for privacy breaches in changing the point where anonymization gets applied. We examined anonymization points in data warehouse scenarios to evaluate their effects on utility and re-identification risk. Our evaluation showed that data quality can differ up to 4,80% while the re-identification risk is reduced by up to 16,82 %. With still improved quality, the re-identification risk differs up to 53,49 % in another configuration.},
keywords={Data privacy;Process control;Data warehouses;Privacy breach;Size measurement;Probabilistic logic;Time measurement;Cloud computing;Data warehouses;Data pri-vacy;Internet privacy;Trust management},
doi={10.1109/COMPSAC54236.2022.00247},
ISSN={0730-3157},
month={June},}
@INPROCEEDINGS{9307781,
author={López-Acosta, Araceli and García-Hernández, Alejandra and Vázquez-Reyes, Sodel and Mauricio-González, Alejandro},
booktitle={2020 8th International Conference in Software Engineering Research and Innovation (CONISOFT)},
title={A Metadata Application Profile to Structure a Scientific Database for Social Network Analysis (SNA)},
year={2020},
volume={},
number={},
pages={208-215},
abstract={There are a number of challenges associated to metadata in its different applications including data quality, data acquisition, computing resources, interoperability, and discoverability. This work presents an approach to structure metadata of scientific information for social network analysis based on an academic case study from scientific articles published by universities, to evaluate the area of risk assessment. Studying metadata for scientists' social networks helps identify authors' relevance based on their position within the network. By using Elasticsearch (ES) and Python technologies, this work addresses big data analysis issues related to data structure and volume, given ES full-text search engine capabilities for indexing and searching data, and Python's processing support. The data is obtained from the ArnetMiner (Aminer) open scientific database providing a fresh overview of scientific records up to January 2019. From a sample of 64,070 publications, a total of 45, 000 relations are graphed in a co-authorship network. Through the computation of network centrality measures, this work identifies central-positioned authors, clusters of research, and their affiliations. The results show that degree centrality is an important measure to identify prominent scientists in this co-authorship network, and closeness and betweenness centralities together are dominant measures to pinpoint the key players in the flow of information within the network. We conclude that the application of this approach allows rapid full-text search, visualizing dense co-authorship networks, and identifying central authors through centrality metrics. The results presented in this work can help researchers or research groups identify key research collaborators, multi-disciplinary areas, and international stakeholders.},
keywords={Metadata;Social networking (online);Databases;Search engines;Interoperability;Internet;Libraries;Scientific Data;Metadata;Elasticsearch;Social Network Analysis},
doi={10.1109/CONISOFT50191.2020.00038},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7724532,
author={Manjula, K. R. and Gangothri, R.},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={Hybrid model based uncertainty analysis for geospatial metadata supporting decision making for spatial exploration},
year={2016},
volume={},
number={},
pages={1575-1579},
abstract={The proliferation of Geospatial data analytics has greatly increased the usage of GIS in all smartphones and gadgets in today's Big Data environment. The footprints of GIS are found in all fields from government to business analytics. Therefore the error propagation in such a substantial need may lead to misunderstood decisions and confusions during emergency. In this paper we take up a chance to document the data quality assurance using the geospatial metadata based uncertainty analysis approach. The paper takes up an initial attempt to state that chances occur for existence of uncertainty in metadata. And it proposes a hybrid model combing ontology, standard deviation and probability density function for detecting the occurrence of uncertainties in geospatial metadata.},
keywords={Metadata;Uncertainty;Geospatial analysis;Standards;Spatial databases;Analytical models;Geospatial Metadata;GIS;ontology;standard deviation;probability density function},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{8877003,
author={Palacio, Ana León and López, Óscar Pastor},
booktitle={2019 13th International Conference on Research Challenges in Information Science (RCIS)},
title={Infoxication in the Genomic Data Era and Implications in the Development of Information Systems},
year={2019},
volume={},
number={},
pages={1-9},
abstract={We live in an age where data acquisition is no longer a problem and the real challenge is how to determine which information is the right one to take important and sometimes difficult decisions. Infoxication (also known as Infobesity or Information Overload) is a term used to describe the difficulty of adapting to new situations and effectively making decisions when there is too much information to manage. With the advent of the Big Data, infoxication is affecting critical domains such as Health Sciences, where tough decisions for patient's health is being taken every day based on heterogeneous, unconnected and sometimes conflicting information. In order to understand the magnitude of the challenge, based on the information publicly available about the genetic causes of the disease and using data quality assessment techniques, we performed an exhaustive analysis of the DNA variations that have been associated to the risk of suffering migraine headache. The same analysis has been repeated 8 months after, and the results have allowed us to exemplify i) how fragile is the information in this domain, ii) the difficulty of finding repositories of contrasted and reliable data, and iii) the need to have information systems that, far from integrating and storing huge volumes of data, are able to support the decision-making process by providing mechanisms agile and flexible enough to be able to adapt to the changing user needs.},
keywords={Bioinformatics;Genomics;Diseases;DNA;Task analysis;Databases;Infoxication;Genomics;Information Systems;SILE method},
doi={10.1109/RCIS.2019.8877003},
ISSN={2151-1357},
month={May},}
@INPROCEEDINGS{8982563,
author={Yoo, Yeisol and Yoo, Jin Soung},
booktitle={2019 IEEE International Conferences on Ubiquitous Computing & Communications (IUCC) and Data Science and Computational Intelligence (DSCI) and Smart Computing, Networking and Services (SmartCNS)},
title={RFID Data Warehousing and OLAP with Hive},
year={2019},
volume={},
number={},
pages={476-483},
abstract={Radio Frequency Identification (RFID) technology is used in many applications for monitoring object movement. The use of RFID in supply chain management systems enables to track the movement of products from suppliers to warehouses, store backrooms, and eventually points of sale. The vast amount of data resulting from the proliferation of RFID readers and tags poses challenges for data management and analytics. RFID data warehousing can enhance data quality and consistency, and give great potential benefits for Online Analytical Processing (OLAP) applications. Traditional data warehouses are built primarily on relational database management systems. However, the size of RFID data being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hive is an open-source data warehousing solution built on top of Hadoop which is a popular Big Data computing framework. This paper presents alternative RFID data warehouse designs which can handle a large amount of RFID data and support a variety of OLAP queries. The proposed approaches are implemented on Hive and evaluated for query performance in cloud computing environment.},
keywords={RFID data warehousing;OLAP;Hive;Cloud computing},
doi={10.1109/IUCC/DSCI/SmartCNS.2019.00105},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7951874,
author={Fan Xiaojiang and Zheng Liwei and Liu Jianbin},
booktitle={2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Measurement for social network data currency and trustworthiness},
year={2017},
volume={},
number={},
pages={1-5},
abstract={Along with the explosive growth of the information in Social Network Service, the research of the quality of data has become a new hot point in related research field. High quality social data can more effectively support data mining, knowledge discovery, and can provide reliable and efficient data for users. Based on the measure problems of data quality, this paper discussed the measurement of two important dimensions of data quality: currency and trustworthiness. Computing models for currency measurement of data with or without time stamp are given. And based on the currency values, a trustworthiness measurement method is also given.},
keywords={Facsimile;social network service;data quality;currency;trustworthiness},
doi={10.1109/ICCCBDA.2017.7951874},
ISSN={},
month={April},}
@ARTICLE{8528409,
author={Xiong, Jinbo and Chen, Xiuhua and Tian, Youliang and Ma, Rong and Chen, Lei and Yao, Zhiqiang},
journal={IEEE Access},
title={MAIM: A Novel Incentive Mechanism Based on Multi-Attribute User Selection in Mobile Crowdsensing},
year={2018},
volume={6},
number={},
pages={65384-65396},
abstract={In the user selection phase of mobile crowdsensing, most existing incentive mechanisms focus on either single-attribute selection or random selection, which possibly lead to serious consequences such as low user enthusiasm, decreased task completion rate, and increased cost of platform consumption. To tackle these issues, in this paper, we propose a novel incentive mechanism MAIM, which is based on multi-attribute user selection and participation intention analysis function in mobile crowdsensing. In this mechanism, the sensing platform employs the analytic hierarchy process to determine the weights of three attributes: participation threshold, cost, and reputation. The weight calculation results of each sensing user with respect to each attribute are then integrated to obtain the sorted weight of each user, with which the sensing platform will then obtain the optimal user set. From the users' perspective, they can autonomously decide whether to accept task processing requests, as enabled by the participation intention analysis function, thereby voiding the absolute authority and control of the sensing platform over users and achieving a two-way selection between the sensing platform and the sensing users. Furthermore, the sensing platform establishes a score-based reputation reward to inspire active performers and utilizes a punishment mechanism to overawe malicious vandals, which substantially helps activize enthusiasm of user participation and improve sensing data quality. Simulation results indicate that the proposed MAIM has significantly improved the sensing task completion ratio and the budget surplus ratio compared with the existing incentive mechanisms in mobile crowdsensing.},
keywords={Sensors;Task analysis;Analytic hierarchy process;Training;Technological innovation;Data integrity;Simulation;Mobile crowdsensing;incentive mechanism;analytic hierarchy process;multi-attribute user selection;participation intention analysis},
doi={10.1109/ACCESS.2018.2878761},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9194651,
author={Müftüoğlu, Zümrüt and Kizrak, M. Ayyüce and Yildlnm, Tülay},
booktitle={2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)},
title={Differential Privacy Practice on Diagnosis of COVID-19 Radiology Imaging Using EfficientNet},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Medical sciences are an important application area of artificial intelligence. Healthcare requires meticulousness in the whole process from collecting data to processing. It should also be handled in terms of data quality, data size, and data privacy. Various data are used within the scope of the COVID-19 outbreak struggle. Medical and location data collected from mobile phones and wearable devices are used to prevent the spread of the epidemic. In addition to this, artificial intelligence approaches are presented by using medical images in order to identify COVID-19 infected people. However, studies should be carried out by taking care not to endanger the security of the data, people, and countries needed for these useful applications. Therefore, differential privacy (DP) application, which was an interesting research subject, has been included in this study. CXR images have been collected from COVID-19 infected 139 and a total of 373 public data sources were used for a diagnostic concept. It has been trained with EfficientNet- B0, a recent and robust deep learning model, and proposal the possibility of infected with an accuracy of 94.7%. Other evaluation parameters were also discussed in detail. Despite the data constraint, this performance showed that it can be improved by augmenting the dataset. The most important aspect of the study was the proposal of differential privacy practice for such applications to be reliable in real-life use cases. With this view, experiments were repeated with DP applied images and the results obtained were presented. Here, Private Aggregation of Teacher Ensembles (PATE) approach was used to ensure privacy assurance.},
keywords={Machine learning;Privacy;Computed tomography;Medical diagnostic imaging;COVID-19;deep learning;EfficientNet;X-Ray;radiology imaging;PATE;differential privacy},
doi={10.1109/INISTA49547.2020.9194651},
ISSN={},
month={Aug},}
@ARTICLE{9143114,
author={Liu, Yangxiaoyue and Yang, Yaping and Jing, Wenlong},
journal={IEEE Access},
title={Potential Applicability of SMAP in ECV Soil Moisture Gap-Filling: A Case Study in Europe},
year={2020},
volume={8},
number={},
pages={133114-133127},
abstract={The Essential Climate Variable (ECV) soil moisture (SM) datasets, originated from the European Space Agency, have revealed great potential for application in hydrology and agriculture. Hence, it is essential to continuously enhance the data quality and spatial completeness to satisfy the increasing scientific research requirements. In this study, we explore the potential possibility of Soil Moisture Active Passive (SMAP) datasets in filling the gaps of ECV SM. The comprehensive assessment results show that: (1) The data missing percent of gap-filled ECV decreases 20% on average, which can be one step closer to generate a seamlessly covered global land surface SM product with favorable quality. (2) Compared to the original ECV, the gap-filled ECV products express similar good response to the in-situ measurements, suggesting that the SMAP SM products could be taken to efficiently fill the gaps and consistently maintain favorable accuracy at the same time. (3) Compared to the in-situ measurements, the original ECV SM products demonstrate extremely high probability density peak percentages. Fortunately, this eminent high value could be effectively rectified through gap-filling progress using SMAP. Overall, this study conducts objective and detailed evaluation on the performance of applying SMAP to fill the gaps of ECV, and is expected to act as a valuable reference in ECV SM gap-filling method.},
keywords={Microwave radiometry;Meteorology;Satellite broadcasting;Sensors;Soil moisture;Synthetic aperture radar;Soil measurements;Gap-filling;satellite retrieved soil moisture;the essential climate variable soil moisture;the soil moisture active passive soil moisture},
doi={10.1109/ACCESS.2020.3009977},
ISSN={2169-3536},
month={},}
@ARTICLE{9374986,
author={Lu, Yujiao and Cui, Ziang and Guo, Rong and Xu, Lijun and Gao, Shuo},
journal={IEEE Sensors Journal},
title={A Machine-Learning-Based Touch Orientation Detection Method for Piezoelectric Touch Sensing in Noisy Environment},
year={2021},
volume={21},
number={23},
pages={26373-26381},
abstract={Touch orientation detection is important for piezoelectric touch panels to stabilize force-voltage responsivities. Current touch orientation estimation techniques utilize machine learning algorithms for orientation classification. However, environmental noise could weaken the data quality which will result in a lowered detection accuracy. To address this issue, in this article, we present a noise robustness technique, in which different levels of noise data are injected into the training data. The performance of “dirty” data trained model exhibits a good performance (average mean absolute error (MAE) of 7.8 degrees) among signal-to-noise ratio (SNR) from 3 dB to 40 dB, indicating that an improved user experience can be obtained.},
keywords={Force;Sensors;Signal to noise ratio;Capacitance;Electrodes;Machine learning;Electromagnetic interference;Piezoelectric touch panel;touch orientation;force–voltage responsivity;white Gaussian noise;regression model},
doi={10.1109/JSEN.2021.3065525},
ISSN={1558-1748},
month={Dec},}
@INPROCEEDINGS{7997260,
author={Li, Peng and Luo, Hong and Wu, Tin-Yu and Obaidat, Mohammad S.},
booktitle={2017 IEEE International Conference on Communications (ICC)},
title={QoS prediction method for data supply chain based on context},
year={2017},
volume={},
number={},
pages={1-7},
abstract={Due to the execution paradigm may be different at different invocation time, users obtain different QoS when interacting with the same Data Supply Chain (DSC). However, existing QoS prediction methods seldom took this observation into consideration, which shall decrease the prediction accuracy. In this paper, we propose a context-based QoS prediction method for data supply chain. First, a QoS mathematical model is developed for considering the mass data transmission across elementary sub-chains. Then, two execution paradigms of data supply chain are discussed. Besides, we explored several special context factors of data supply chain (such as invocation time, data source update period and execution paradigm) which influence QoS. By processing such context information, we can obtain the part of data supply chain which is need to execute when the user query occurs and leverage them to predict QoS. Experimental results indicate that our approach improves the prediction accuracy and efficiency of QoS when compared to previous methods.},
keywords={Quality of service;Supply chains;Web services;Mathematical model;Prediction methods;Time factors;Corporate acquisitions;Data supply chain;QoS model;processing context;QoS prediction},
doi={10.1109/ICC.2017.7997260},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{7877045,
author={Yaseen, Muhammad Usman and Anjum, Ashiq and Antonopoulos, Nick},
booktitle={2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)},
title={Spatial Frequency Based Video Stream Analysis for Object Classification and Recognition in Clouds},
year={2016},
volume={},
number={},
pages={18-26},
abstract={The recent rise in multimedia technology has made it easier to perform a number of tasks. One of these tasks is monitoring where cheap cameras are producing large amount of video data. This video data is then processed for object classification to extract useful information. However, the videodata obtained by these cheap cameras is often of low qualityand results in blur video content. Moreover, various illuminationeffects caused by lightning conditions also degradethe video quality. These effects present severe challenges forobject classification. We present a cloud-based blur and illumination invariant approach for object classification fromimages and video data. The bi-dimensional empirical modedecomposition (BEMD) has been adopted to decompose avideo frame into intrinsic mode functions (IMFs). TheseIMFs further undergo to first order Reisz transform to generatemonogenic video frames. The analysis of each IMF hasbeen carried out by observing its local properties (amplitude, phase and orientation) generated from each monogenic videoframe. We propose a stack based hierarchy of local patternfeatures generated from the amplitudes of each IMF whichresults in blur and illumination invariant object classification. The extensive experimentation on video streams aswell as publically available image datasets reveals that oursystem achieves high accuracy from 0.97 to 0.91 for increasingGaussian blur ranging from 0.5 to 5 and outperformsstate of the art techniques under uncontrolled conditions. The system also proved to be scalable with high throughputwhen tested on a number of video streams using cloud infrastructure.},
keywords={Streaming media;Cameras;Cloud computing;Lighting;Image color analysis;Empirical mode decomposition;Feature extraction;Empirical Mode Decomposition;Local Ternary Patterns;Riesz Transform;Amplitude Spectrum;Cloud Computing;Big Data Analytics;Object Classification},
doi={},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6352418,
author={Fomferra, Norman and Böttcher, Martin and Zühlke, Marco and Brockmann, Carsten and Kwiatkowska, Ewa},
booktitle={2012 IEEE International Geoscience and Remote Sensing Symposium},
title={Calvalus: Full-mission EO cal/val, processing and exploitation services},
year={2012},
volume={},
number={},
pages={5278-5281},
abstract={ESA's Earth Observation (EO) missions provide a unique dataset of observational data of our environment. Calibration, algorithm development and validation of the derived products are indispensable tasks for an efficient exploitation of EO data and form the basis for reliable scientific conclusions. In spite of its importance, the cal/val and algorithm development work is often hindered by insufficient means to access data, time consuming work used to identify suitable in-situ data matching the EO data, incompatible software and limited possibilities for a rapid prototyping and testing of ideas. In view of the amount of data produced by the future ESAs series of Sentinel satellites, a very efficient technological backbone is required to maintain the ability of ensuring data quality and algorithm performance. Brockmann Consult has developed such a backbone based on leading edge technologies within an ESA R&D study. Calvalus is a new processing system that utilises the map-reduce programming model with a distributed file system.},
keywords={File systems;Algorithm design and analysis;Calibration;Reliability;Servers;Programming;Data processing;Algorithm prototyping;data processing;big data handling;map-reduce;calibration/validation},
doi={10.1109/IGARSS.2012.6352418},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{9197774,
author={Ahlawat, Deepak and Kaur, Amandeep and Gupta, Deepali},
booktitle={2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)},
title={Enhancement of the Accuracy and QoS in Clustering of Data},
year={2020},
volume={},
number={},
pages={849-853},
abstract={Clustering is an important data mining and tool for examining data. The paper compares the two techniques of clustering, in the first technique only Cosine similarity is used and in the second technique Improved Rank Similarity (Cosine Similarity + Gaussian Similarity) is used. The results are compared with various parameters constituting the Accuracy in NetBeans and QoS parameters using AODV routing protocol. The simulation is done on MATLAB, a network is created and communication from source node to target node is noted.},
keywords={Throughput;Clustering algorithms;Big Data;Quality of service;Data mining;Tools;Matlab;Clustering;Cosine Similarity;Gaussian Similarity;Hybrid Similarity;AODV},
doi={10.1109/ICRITO48877.2020.9197774},
ISSN={},
month={June},}
@ARTICLE{9693431,
author={Kardash, Adam and Morin, Suzanne},
journal={IEEE Security & Privacy},
title={The Practices and Challenges of Generating Nonidentifiable Data},
year={2022},
volume={20},
number={1},
pages={113-118},
abstract={This article summarizes the key findings of a Canadian Anonymization Network study of several large data custodians who utilize deidentification and similar privacy-enhancing processes prior to engaging in analytics, secondary uses, and disclosure of personal information.},
keywords={Data privacy;Big data;Computer security;Data quality},
doi={10.1109/MSEC.2021.3126185},
ISSN={1558-4046},
month={Jan},}
@INPROCEEDINGS{7406326,
author={Ahmadov, Ahmad and Thiele, Maik and Eberius, Julian and Lehner, Wolfgang and Wrembel, Robert},
booktitle={2015 IEEE/ACM 2nd International Symposium on Big Data Computing (BDC)},
title={Towards a Hybrid Imputation Approach Using Web Tables},
year={2015},
volume={},
number={},
pages={21-30},
abstract={Data completeness is one of the most important data quality dimensions and an essential premise in data analytics. With new emerging Big Data trends such as the data lake concept, which provides a low cost data preparation repository instead of moving curated data into a data warehouse, the problem of data completeness is additionally reinforced. While traditionally the process of filling in missing values is addressed by the data imputation community using statistical techniques, we complement these approaches by using external data sources from the data lake or even the Web to lookup missing values. In this paper we propose a novel hybrid data imputation strategy that, takes into account the characteristics of an incomplete dataset and based on that chooses the best imputation approach, i.e. either a statistical approach such as regression analysis or a Web-based lookup or a combination of both. We formalize and implement both imputation approaches, including a Web table retrieval and matching system and evaluate them extensively using a corpus with 125M Web tables. We show that applying statistical techniques in conjunction with external data sources will lead to a imputation system which is robust, accurate, and has high coverage at the same time.},
keywords={Indexes;Lakes;Companies;Big data;Data mining;Industries;Data analysis;Web mining;Data preprocessing;Machine learning},
doi={10.1109/BDC.2015.38},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8463091,
author={Bantug, Derek and Franklin, Paul and Boone, Ted},
booktitle={2018 Annual Reliability and Maintainability Symposium (RAMS)},
title={Product Reliability and Databases: Lessons Learned},
year={2018},
volume={},
number={},
pages={1-5},
abstract={This experience paper describes some lessons learned using “big data.” Managers want to make data driven decisions, and many companies spend substantial effort and resources to develop collection methods, record facts, and store records in large data warehouses. Additional resources take this collection of data and produce reports, which are then used to support decision making. We work with customer premises equipment, and our databases track nearly 50 million serial numbers. As reliability engineers, we use this data as the basis of analysis to assess field performance of the equipment the databases track. In 2016, we began to use a standard tool to serve as a definitive repository and an engine to do preliminary postprocessing. This database uses data dimensions, where each dimension is an array. It is convenient to think of the lefthand column as a set of labels and the cells to the right as measures (either collected data or computed metrics) for each label. The advantage of creating dimensions is that-rather than working with individual data items and the relationships between them-a dimension preprocesses data into a set that has relationships with other sets. This models the “real world” more closely. A cube is just a set of one or more dimensions. Using a cube allows complex questions to be asked and answered in ways that relational databases do not. We have been using this data structure to support analysis of customer premises equipment, typically set top boxes, modems, and similar equipment that is leased by the provider to customers at their residences and businesses. Tools that support cubes offer several advantages. It is possible to do analyses in a cube that are difficult in a relational database that does not support the “logical ecology” that a cube does By moving up and down the data hierarchy, it is possible to see relationships on the screen, and outputs can be saved to other more powerful post processing tools for more detailed analysis Cubes support faster and less error prone analysis This paper describes these points and illustrate them with a simple example. Our objective is to illustrate the concepts rather than work through a detailed problem. Our work to date suggests that it is critical to manage data quality in a broad sense so that the resulting reports and analysis are trustworthy. We have had a generally positive experience with this technology and found that it benefits the business by allowing processes to be modeled. This refines our understanding of the meaning of the various process metrics, and in some cases, we have been able to recommend changes to policy.},
keywords={Databases;Tools;Measurement;Maintenance engineering;Reliability;Engines;Arrays;Databases;data cube;big data;lessons learned},
doi={10.1109/RAM.2018.8463091},
ISSN={2577-0993},
month={Jan},}
@ARTICLE{9431193,
author={Yustiawan, Yoga and Ramadhan, Hani and Kwon, Joonho},
journal={IEEE Access},
title={A Stacked Denoising Autoencoder and Long Short-Term Memory Approach With Rule-Based Refinement to Extract Valid Semantic Trajectories},
year={2021},
volume={9},
number={},
pages={73152-73168},
abstract={Indoor location-based services have been widely investigated to take advantage of semantic trajectories for providing user oriented services in indoor environments. Although indoor semantic trajectories can provide seamless understanding to users regarding the provided location-based services, studies on the application of deep learning approaches for robust and valid semantic indoor localization are lacking. In this study, we combined a stacked denoising autoencoder and long short term memory technique with a rule-based refinement method applying a rule-based hidden Markov model (HMM) to perform robust and valid semantic trajectory extraction. In particular, our rule-based HMM approach incorporates a direct set of rules into HMM to resolve invalid movements of the extracted semantic trajectories and is extensible to various deep learning techniques. We compared the performance of our proposed approach with that of other cutting-edge deep learning approaches on two different real-world data sets. The experimental results demonstrate the feasibility of our proposed approach to produce more robust and valid semantic trajectories.},
keywords={Semantics;Trajectory;Location awareness;Hidden Markov models;Deep learning;Noise measurement;Indoor environment;Deep learning;indoor localization;the Internet of Things;rule-based refinement;semantic trajectories},
doi={10.1109/ACCESS.2021.3080288},
ISSN={2169-3536},
month={},}
@ARTICLE{8649758,
author={Li, Zhi and Guo, Hanyang and Wang, Wai Ming and Guan, Yijiang and Barenji, Ali Vatankhah and Huang, George Q. and McFall, Kevin S. and Chen, Xin},
journal={IEEE Transactions on Industrial Informatics},
title={A Blockchain and AutoML Approach for Open and Automated Customer Service},
year={2019},
volume={15},
number={6},
pages={3642-3651},
abstract={Customer service is transforming from traditional manual service toward automated service, which utilizes different computational informatics to achieve a higher efficient and quality services. Automated customer service requires big data and expertise in data analysis as prerequisites. However, many companies, especially small and medium enterprises, do not have sufficient data and experience due to their limited scale and resources. They need to rely on third parties, and this reliance results in the lack of development of core customer service competency. In order to overcome these challenges, an open and automated customer service platform based on Internet of things (IoT), blockchain, and automated machine learning (AutoML) is proposed. The data are gathered with the use of IoT devices during the customer service. An open but secured environment to achieve data trading is ensured by using blockchain. AutoML is adopted to automate the data analysis processes for reducing the reliance of costly experts. The proposed platform is analyzed through use case evaluation. A prototype system has also been developed and evaluated. The simulation results show that our platform is scalable and efficient.},
keywords={Customer services;Blockchain;Companies;Machine learning;Personnel;Data models;Smart contracts;Automated customer service;automated machine learning (AutoML);blockchain;open customer service},
doi={10.1109/TII.2019.2900987},
ISSN={1941-0050},
month={June},}
@ARTICLE{9261414,
author={Duan, Gui-Jiang and Yan, Xin},
journal={IEEE Access},
title={A Real-Time Quality Control System Based on Manufacturing Process Data},
year={2020},
volume={8},
number={},
pages={208506-208517},
abstract={Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.},
keywords={Manufacturing processes;Production;Quality control;Product design;Real-time systems;Manufacturing;Quality assessment;Quality management;production control;prediction methods},
doi={10.1109/ACCESS.2020.3038394},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7515750,
author={Abdellaoui, Sabrina and Bellatreche, Ladjel and Nader, Fahima},
booktitle={2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)},
title={A Quality-Driven Approach for Building Heterogeneous Distributed Databases: The Case of Data Warehouses},
year={2016},
volume={},
number={},
pages={631-638},
abstract={Data Warehouse (DW) is a collection of data, consolidated from several heterogeneous sources, used to perform data analysis and support decision making in an organization. Extract-Transform-Load (ETL) phase plays a crucial role in designing DW. To overcome the complexity of the ETL phase, different studies have recently proposed the use of ontologies. Ontology-based ETL approaches have been used to reduce heterogeneity between data sources and ensure automation of the ETL process. Existing studies in semantic ETL have largely focused on fulfilling functional requirements. However, the ETL process quality dimension has not been sufficiently considered by these studies. As the amount of data has exploded with the advent of big data era, dealing with quality challenges in the early stages of designing the process become more important than ever. To address this issue, we propose to keep data quality requirements at the center of the ETL phase design. We present in this paper an approach, defining the ETL process at the ontological level. We define a set of quality indicators and quantitative measures that can anticipate data quality problems and identify causes of deficiencies. Our approach checks the quality of data before loading them into the target data warehouse to avoid the propagation of corrupted data. Finally, our proposal is validated through a case study, using Oracle Semantic DataBase sources (SDBs), where each source references the Lehigh University BenchMark ontology (LUBM).},
keywords={Ontologies;Semantics;Measurement;Unified modeling language;Standards;Proposals;Data warehouse;ETL design;Ontologies;Data quality;Semantic Database Sources},
doi={10.1109/CCGrid.2016.79},
ISSN={},
month={May},}
@INPROCEEDINGS{9060201,
author={Chang, Huijuan and Yu, Zhiyong and Yu, Zhiwen and Guo, Bin},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={Selecting Sensing Location Leveraging Spatial and Cross-Domain Correlations},
year={2019},
volume={},
number={},
pages={661-666},
abstract={In environmental monitoring applications, selecting appropriate locations to sense is important relating to data quality and Sensing cost. This paper addresses the challenge by collecting data from a subset of locations, then leveraging the spatial and cross-domain correlations to deduce data of other locations, thus can obtain acceptable data quality with lower sensing cost. Referring to active learning, the proposed framework is constructed by two types modules (i.e., estimators and selectors) and a cyclic process of estimating and selecting. Estimators based on kriging interpolation and regression tree are implemented, and their corresponding selectors are designed. We evaluate the effectiveness of the framework by taking air quality sensing as an example. Results show that to reach data quality of about 25% MAPE, the framework only needs 15% locations, while random selector needs 25% locations.},
keywords={Sensors;Correlation;Air quality;Data models;Estimation;Data integrity;Task analysis;Active learning;location selection;kriging interpolation;regression tree},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00149},
ISSN={},
month={Aug},}
@INBOOK{9822310,
author={Zgolli, Asma and Collet, Christine and Madera, Cédrine},
booktitle={Data Lakes},
title={Metadata in Data Lake Ecosystems},
year={2020},
volume={},
number={},
pages={57-96},
abstract={The notion of metadata has been used in information management long before the emergence of computer science, in fields related to documentation or cataloguing. The National Information Security Organization classification of metadata encompasses the following four types: descriptive metadata, structural metadata, administrative metadata, and markup languages. A metadata schema consists of a labeling, marking or encoding system, used for storing information about the way data is organized and structured. Business metadata define the information content that the data provide in a business context. Business metadata are an important aspect in any successful information governance program. Navigational integration metadata describe the data linkage and data movement within the environments. Operational metadata describes the data integration applications and supporting job runs. Primary sources of operational metadata include data integration job logs and data quality checks. In a data lake ecosystem, as well as other information systems, metadata are varied and pervasive.},
keywords={Metadata;Business;Big Data applications;Organizations;Interoperability;Knowledge based systems;Standards organizations},
doi={10.1002/9781119720430.ch4},
ISSN={},
publisher={Wiley},
isbn={9781119720423},
url={https://ieeexplore.ieee.org/document/9822310},}
@ARTICLE{7900340,
author={Luo, Xin and Zhou, MengChu and Li, Shuai and Xia, YunNi and You, Zhu-Hong and Zhu, QingSheng and Leung, Hareton},
journal={IEEE Transactions on Cybernetics},
title={Incorporation of Efficient Second-Order Solvers Into Latent Factor Models for Accurate Prediction of Missing QoS Data},
year={2018},
volume={48},
number={4},
pages={1216-1228},
abstract={Generating highly accurate predictions for missing quality-of-service (QoS) data is an important issue. Latent factor (LF)-based QoS-predictors have proven to be effective in dealing with it. However, they are based on first-order solvers that cannot well address their target problem that is inherently bilinear and nonconvex, thereby leaving a significant opportunity for accuracy improvement. This paper proposes to incorporate an efficient second-order solver into them to raise their accuracy. To do so, we adopt the principle of Hessian-free optimization and successfully avoid the direct manipulation of a Hessian matrix, by employing the efficiently obtainable product between its Gauss-Newton approximation and an arbitrary vector. Thus, the second-order information is innovatively integrated into them. Experimental results on two industrial QoS datasets indicate that compared with the state-of-the-art predictors, the newly proposed one achieves significantly higher prediction accuracy at the expense of affordable computational burden. Hence, it is especially suitable for industrial applications requiring high prediction accuracy of unknown QoS data.},
keywords={Quality of service;Predictive models;Optimization;Computational modeling;Mathematical model;Data models;Web services;Big data;latent factor model;missing data prediction;quality-of-service (QoS);second-order solver;service computing sparse matrices;Web service},
doi={10.1109/TCYB.2017.2685521},
ISSN={2168-2275},
month={April},}
@INPROCEEDINGS{8471712,
author={Darari, Fariz and Nutt, Werner and Razniewski, Simon},
booktitle={2018 International Workshop on Big Data and Information Security (IWBIS)},
title={Comparing Index Structures for Completeness Reasoning},
year={2018},
volume={},
number={},
pages={49-56},
abstract={Data quality is a major issue in the devel- opment of knowledge graphs. Data completeness is a key factor in data quality pertaining to how broad and deep is information contained in knowledge graphs. As for large- scale knowledge graphs (e.g., DBpedia, Wikidata), it is conceivable that given the vast amount of information contained in there, they may be complete for a wide range of topics, such as children of Joko Widodo, cantons of Switzerland, and presidents of Indonesia. Previous research has shown how one can augment knowledge graphs with statements about their completeness, stating which parts of data are complete. Such meta-information can be leveraged to check query completeness, that is, whether the answer returned by a query is complete. Yet, it is still unclear how such a check can be done in practice, especially when many completeness statements are involved. We devise implementation techniques to make completeness reasoning in the presence of large sets of completeness statements feasible, and experimentally evaluate their effectiveness in realistic settings based on the characteristics of real-world knowledge graphs.},
keywords={Cognition;Resource description framework;Metadata;Data integrity;Tools;Iris;Complexity theory},
doi={10.1109/IWBIS.2018.8471712},
ISSN={},
month={May},}
@INPROCEEDINGS{6832210,
author={Zheng, Liwei},
booktitle={2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing},
title={AMD Based Service Agent Collaboration and Specification},
year={2013},
volume={},
number={},
pages={2277-2284},
abstract={With the emergence of Big Data in Internet, composing existing web services for satisfying new requirements, such as data quality enhancing, effective data choosing, knowledge discovering etc, has gained daily expanding attentions and interests. Many efforts have been pursued for supporting the essential activities in service composition. However, the existing techniques only focus on passive services which are waiting there for being discovered and invoked. We argue that it might be more attractive when Web services become active entities (Service Agent) distributed in Internet which can recognize the newly emergent requirements and compete with others for realize (part of) the requirements. Retreating or refinement of Big data will hardly be accomplished by one or two data handling center, Service Agent collaboration would be a competitive method for the big data handling problem. Mostly more than one service agents have to collaborate to satisfy requirements in current internet environment especially with social networks. That could be called as the requirement driven agent collaboration. Research on such collaboration might be useful for the previous problem. We have given a preliminary model for the requirement driven agent collaboration based on a function ontology and the automated mechanism design in the earlier work. This paper extended the Function Ontology, and enhanced the AMD model. That makes the interactions in MAS generated by agent collaboration can be described. A negotiation frame for the evaluation and choice of collaboration solutions is also given in this paper. It helps the requester evaluate the possible MAS systems, and helps the service agents make decisions to choose a good enough solution by negotiation. According to the dependencies provided in Function Ontology, a specification is given to describe the execution process of the chosen MAS. And also a method is given to translate the specification to BPEL which is more standard, acceptable, and easier to understood.},
keywords={Collaboration;Ontologies;Dynamic scheduling;Vectors;Web services;Multi-Agent;Collaboration;Mechanism design},
doi={10.1109/HPCC.and.EUC.2013.327},
ISSN={},
month={Nov},}
@ARTICLE{8660441,
author={Guo, Wenzhong and Zhu, Weiping and Yu, Zhiyong and Wang, Jiangtao and Guo, Bin},
journal={IEEE Access},
title={A Survey of Task Allocation: Contrastive Perspectives From Wireless Sensor Networks and Mobile Crowdsensing},
year={2019},
volume={7},
number={},
pages={78406-78420},
abstract={Wireless sensor networks (WSNs) and mobile crowdsensing (MCS) are two important paradigms in urban dynamic sensing. In both sensing paradigms, task allocation is a significant problem, which may affect the completion quality of sensing tasks. In this paper, we give a survey of task allocation in WSNs and MCS from the contrastive perspectives in terms of data quality and sensing cost, which help to better understand related objectives and strategies. We first analyze the different characteristics of two sensing paradigms, which may lead to difference in task allocation issues or strategies. Then, we present some common issues in task allocation with objectives in data quality and sensing cost. Furthermore, we provide reviews of unique task allocation issues in MCS according to its new characteristics. Finally, we identify some potential opportunities for the future research.},
keywords={Sensors;Task analysis;Wireless sensor networks;Resource management;Data integrity;Wireless communication;Mobile handsets;Mobile crowdsensing (MCS);task allocation;wireless sensor networks (WSNs)},
doi={10.1109/ACCESS.2019.2896226},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8890371,
author={Liang, Zilu and Chapa Martell, Mario Alberto},
booktitle={2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
title={Achieving Accurate Ubiquitous Sleep Sensing with Consumer Wearable Activity Wristbands Using Multi-class Imbalanced Classification},
year={2019},
volume={},
number={},
pages={768-775},
abstract={Consumer activity wristbands such as Fitbit provide an affordable method for ubiquitous sleep sensing in daily settings. These devices are also increasingly used in scientific studies as measurement tools of sleep outcomes. Nevertheless, the accuracy of Fitbit has raised wide concern. In this paper, we explore the feasibility of applying machine learning to improve the quality of Fitbit sleep data. The problem of interest was formulated into a multiclass imbalanced classification problem. We examined the performance of different combinations of seven machine learning algorithms and three resampling techniques. The preliminary results showed that the accuracy in detecting wakefulness and light sleep was improved by up to 43% and 44% respectively compared to the proprietary algorithm of Fitbit. Our future work will focus on improving the overall accuracy of the classification models in detecting all sleep stages.},
keywords={Sleep;Training;Machine learning algorithms;Classification algorithms;Standards;Vegetation;Heart rate;wearable;data quality;sleep;machine learning;Fitbit},
doi={10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00143},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9060352,
author={Xiao, Yunlong and Gu, Yang and Wang, Jiwei and Wu, Tong},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={A Collaborative Multi-modality Selection Method Based on Data Utility Assessment},
year={2019},
volume={},
number={},
pages={454-459},
abstract={Multimodal fusion is more and more widely used in the field of machine learning, but it faces a prominent problem in practical application: data utility is not stable. The data of different modalities may be missing and noisy randomly, which will interfere the machine learning model of multi-modal fusion. Most of the existing multi-modal fusion methods neglect data utility problems or only adopt simple data denoising methods to improve data utility. To solve the problem of unstable data utility, we propose a data selection method based on the evaluation of data utility. By training a special machine learning model, the optimal modal combination is predicted according to the quality evaluation of multi-modal data samples to accomplish the dynamic selection of data modalities. The experimental results show that the proposed method can effectively improve the accuracy of multi-modal recognition under low data utility.},
keywords={Data integrity;Machine learning;Data models;Training;Task analysis;Mathematical model;Gesture recognition;data selection;multimodal;data utility;data quality assessment},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00120},
ISSN={},
month={Aug},}
@ARTICLE{9475591,
author={Li, Xiaolu and Zhou, Yier and Hua, Baocheng},
journal={IEEE Transactions on Instrumentation and Measurement},
title={Study of a Multi-Beam LiDAR Perception Assessment Model for Real-Time Autonomous Driving},
year={2021},
volume={70},
number={},
pages={1-15},
abstract={Light detection and ranging (LiDAR) provides a 3-D understanding of environment and plays an important role in autonomous driving. To study the influence of 3-D data quality on the environment perception and provide a theoretical basis for optimizing system design, a multi-beam LiDAR perception assessment model has been established to reveal the relationship between data quality and multi-parameters, including system and motion parameters. A novel ground segmentation algorithm was proposed with a combination of the grid elevation and the neighbor relationship, which was used to validate how the data quality influences the results of environment perception. By the way of down-sampling based on the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset, the experimental results showed that the proposed ground segmentation with combination of grid-elevation and neighbor-relationship (GSCGN) method was superior than other general ground segmentation methods in terms of accuracy and efficiency. It should be noted that the mean vertical angular resolution (MVAR), laser repetition frequency, and beam numbers were the dominant influencing parameters on the point density and the accuracy of ground segmentation. Based on the experimental results, the lower limits of system parameters were determined as 16-beam and 4-kHz repetition frequency, with the acceptable recall of 92.2% for ground and 93.5% for object, the accuracy of 92.9% and the runtime of 0.036 s, which can not only provide a reliable environment perception effect, but also reducing the computational burden to satisfy the real-time autonomous driving. This study offers a meaningful investigation to guide LiDAR system design with balancing the contradiction between the optimized system design and the high-degree environment perception.},
keywords={Laser radar;Three-dimensional displays;Solid modeling;System analysis and design;Laser modes;Laser beams;Autonomous vehicles;Evaluation indicator;lower limit;multi-beam light detection and ranging (LiDAR);perception assessment model;system design},
doi={10.1109/TIM.2021.3094230},
ISSN={1557-9662},
month={},}
@INPROCEEDINGS{7152629,
author={Yan, Cairong and Song, Yalong and Wang, Jian and Guo, Wenjing},
booktitle={2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
title={Eliminating the Redundancy in MapReduce-Based Entity Resolution},
year={2015},
volume={},
number={},
pages={1233-1236},
abstract={Entity resolution is the basic operation of data quality management, and the key step to find the value of data. The parallel data processing framework based on MapReduce can deal with the challenge brought by big data. However, there exist two important issues, avoiding redundant pairs led by the multi-pass blocking method and optimizing candidate pairs based on the transitive relations of similarity. In this paper, we propose a multi-signature based parallel entity resolution method, called multi-sig-er, which supports unstructured data and structured data. Two redundancy elimination strategies are adopted to prune the candidate pairs and reduce the number of similarity computation without affecting the resolution accuracy. Experimental results on real-world datasets show that our method tends to handle large datasets and it is more suitable for complex similarity computation than simple object matching.},
keywords={Redundancy;Computational modeling;Accuracy;Big data;Parallel processing;Time complexity;Conferences;entity resolution;MapReduce;blocking;redundancy elimination},
doi={10.1109/CCGrid.2015.24},
ISSN={},
month={May},}
@INPROCEEDINGS{6691642,
author={Chen, Chien-Chih and Chang, Yu-Jung and Chung, Wei-Chun and Lee, Der-Tsai and Ho, Jan-Ming},
booktitle={2013 IEEE International Conference on Big Data},
title={CloudRS: An error correction algorithm of high-throughput sequencing data based on scalable framework},
year={2013},
volume={},
number={},
pages={717-722},
abstract={Next-generation sequencing (NGS) technologies produce huge amounts of data. These sequencing data unavoidably are accompanied by the occurrence of sequencing errors which constitutes one of the major problems of further analyses. Error correction is indeed one of the critical steps to the success of NGS applications such as de novo genome assembly and DNA resequencing as illustrated in literature. However, it requires computing time and memory space heavily. To design an algorithm to improve data quality by efficiently utilizing on-demand computing resources in the cloud is a challenge for biologists and computer scientists. In this study, we present an error-correction algorithm, called the CloudRS algorithm, for correcting errors in NGS data. The CloudRS algorithm aims at emulating the notion of error correction algorithm of ALLPATHS-LG on the Hadoop/ MapReduce framework. It is conservative in correcting sequencing errors to avoid introducing false decisions, e.g., when dealing with reads from repetitive regions. We also illustrate several probabilistic measures we introduce into CloudRS to make the algorithm more efficient without sacrificing its effectiveness. Running time of using up to 80 instances each with 8 computing units shows satisfactory speedup. Experiments of comparing with other error correction programs show that CloudRS algorithm performs lower false positive rate for most evaluation benchmarks and higher sensitivity on genome S. cerevisiae. We demonstrate that CloudRS algorithm provides significant improvements in the quality of the resulting contigs on benchmarks of NGS de novo assembly.},
keywords={Error correction;Algorithm design and analysis;Sequential analysis;Assembly;Bioinformatics;Genomics;Benchmark testing;error correction;mapreduce;genome assembly;next-generation sequencing},
doi={10.1109/BigData.2013.6691642},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9060131,
author={Wang, Liang and Yang, Congying and Yu, Zhiwen and Liu, Yimeng and Wang, Zhu and Guo, Bin},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={CrackSense: A CrowdSourcing Based Urban Road Crack Detection System},
year={2019},
volume={},
number={},
pages={944-951},
abstract={As a common road surface distress, cracks pose a serious threat to road infrastructure and traffic safety in cities today. Consequently, road crack detection is considered as an essential step for effective road maintenance and road structure sustainability. However, due to the high cost incurred by dedicated devices and professional operators, it is impossible for existing systems to achieve universal spatiotemporal coverage across citywide road networks. To fill this gap, in this paper, we present the CrackSense, a mobile crowdsourcing based system to detect urban road crack and estimate its damage degree. Specifically, for the heterogeneous crack data, we put forward a crowdsourcing data quality evaluation and selection mechanism. And then, by utilizing the multi-source sensing data aggregation, we propose tow algorithms, namely RCTR and RCDE, to recognize road crack types, i.e., horizontal crack, vertical crack, and net crack, and estimate the crack damage degree, respectively. We implement the system and develop a smartphone APP for mobile users. By conducting intensive experiments and field study, the results demonstrate the accuracy and effectiveness of our proposed approaches.},
keywords={Roads;Crowdsourcing;Sensors;Estimation;Data models;Data integrity;Three-dimensional displays;Mobile crowdsourcing;road crack detection;image processing;sensors},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00188},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8712843,
author={Feng, Zikun and Yang, Haojie and Li, Xinyi and Li, Yan and Liu, Zhao and Liu, Ryan Wen},
booktitle={2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)},
title={Real-Time Vessel Trajectory Data-Based Collison Risk Assessment in Crowded Inland Waterways},
year={2019},
volume={},
number={},
pages={128-134},
abstract={With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and enterprise economy. Therefore, it is of vital significance to study the risk of ship collision in practical applications. This paper proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are computed via the Monte Carlo probabilistic algorithm. For the sake of better understanding, the kernel density estimation method is adopted to visually generate the ship collision risk in maps. Experimental results have illustrated the effectiveness of the proposed method in crowded inland waterways.},
keywords={Marine vehicles;Artificial intelligence;Interpolation;Trajectory;Navigation;Rivers;Accidents;Ship domain;trajectory data;ship collision risk;automatic identification system;Monte Carlo method},
doi={10.1109/ICBDA.2019.8712843},
ISSN={},
month={March},}
@INPROCEEDINGS{7588816,
author={Wang, Fei and Wang, Hongbo},
booktitle={2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)},
title={Record Linkage Using the Combination of Twice Iterative SVM Training and Controllable Manual Review},
year={2016},
volume={},
number={},
pages={31-38},
abstract={Record linkage is widely used in many fields, which is a crucial step to increase data quality before data analyzing and data mining. The task of record linkage is to identify records that correspond to the same entities from multi-sources data. In this paper, we describe detailed process of record linkage through an application of internet video, with the purpose of guiding the practice. A method of combination of twice Iterative SVM (Support Vector Machine) training and controllable manual review has been presented. The experiment based on abundant actual data achieves over 98% in F-score.},
keywords={Couplings;Support vector machines;Training;Indexing;Manuals;Motion pictures;record linkage;internet video;support vector machine;manual review},
doi={10.1109/DASC-PICom-DataCom-CyberSciTec.2016.21},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7011533,
author={Song, Guanli and Wang, Yinghui and Zhang, Runshun and Liu, Baoyan and Zhou, Xuezhong and Song, Guanbo and Xie, Liang and Huang, Xinghuan},
booktitle={2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)},
title={Methods and technologies of traditional Chinese medicine clinical information datamation in real world},
year={2014},
volume={},
number={},
pages={1-5},
abstract={Under the guidance of clinical research paradigm of traditional Chinese medicine (TCM) in real world, the research group developed the clinical research information sharing system, in which structured electronic medical record system of traditional Chinese medicine is the technology platform of datamation of clinical diagnosis and treatment information. The clinical diagnosis and treatment information can be activated and used effectively only after datamation and truly become the treasures of knowledge of TCM. This paper discusses the implementation process and technologies and methods of TCM clinical information datamation, and take admission records as an example to demonstrate the contents and realization way of datamation, and a brief introduction of the effect of implementation and application of datamation. By making full use of technologies and methods of datamation, strengthening data quality control in the datamation process, greatly improving the quality of TCM clinical research data, to lay a good foundation for establishment of knowledge base through further statistical analysis or data mining of TCM clinical data.},
keywords={Medical diagnostic imaging;Clinical diagnosis;Electronic medical records;History;Maintenance engineering;Discharges (electric);Hospitals;traditional Chinese Medicine (TCM);clinical research paradigm;clinical research information sharing system;datamation;structured electronic medical record},
doi={10.1109/CIBD.2014.7011533},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8713242,
author={Li, Xinyi and Feng, Zikun and Li, Yan and Liu, Zhao and Liu, Ryan Wen},
booktitle={2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)},
title={Spatio-Temporal Vessel Trajectory Smoothing Using Empirical Mode Decomposition and Wavelet Transform},
year={2019},
volume={},
number={},
pages={106-111},
abstract={The Automatic Identification System (AIS) has attracted increasing attention in recent years for its superior properties in ocean engineering and maritime management. The spatio-temporal vessel trajectory data is highly related to the received AIS data. However, the AIS raw data often suffer from undesirable noise during signal acquisition and analog-to-digital conversion. To improve AIS-based vessel trajectory data quality, we propose to develop a vessel trajectory smoothing method by combining empirical mode decomposition (EMD) with wavelet transform. In particular, EMD is introduced to decompose the original vessel trajectory data into several sub-trajectories. The EMD decomposition is able to assist in enhancing the robustness of trajectory smoothing. Wavelet transform is directly adopted to smooth the decomposed sub-trajectories. The final high-quality trajectory is obtained by combining the smoothed sub-trajectories in this work. The proposed method has the capacity of removing the unwanted noise while preserving the important trajectory features. Numerous experiments have illustrated the superior smoothing performance of the proposed combined method.},
keywords={Trajectory;Wavelet transforms;Noise reduction;Artificial intelligence;Marine vehicles;Navigation;Empirical mode decomposition;wavelet transform;data denoising;automatic identification system;trajectory data},
doi={10.1109/ICBDA.2019.8713242},
ISSN={},
month={March},}
@INPROCEEDINGS{9781127,
author={Chen, Mingkang and Sun, Jingtao and Aida, Kento and Figueiredo, Renato J. and Ku, Yun-Jung and Subratie, Kensworth},
booktitle={2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)},
title={Intelligent Live Video Streaming for Object Detection},
year={2021},
volume={},
number={},
pages={1427-1434},
abstract={These days, sensors and cameras are being deployed on an increasingly large scale. Furthermore, the rapid development of machine learning models for computer vision now presents novel opportunities for the use of artificial intelligence (AI) and Internet of Things (IoT) combinations in various application scenarios. However, challenges remain in supporting low-latency video streaming from distributed mobile IoT devices under dynamic network environments, and overcoming video data quality degradation that results from weather “noise”, which reduces the accuracy of AI-based data analyses such as object detection. In this paper, we propose a live video stream processing system for supporting intelligent services that integrates the following features. First, to cope with dynamic networks and achieve low latency, our approach employs a peer-to-peer (P2P)-based virtual network at the edge and a multi-tiered architecture composed of IoT cameras, edge, and cloud servers. Second, we construct a flexible messaging system for video analysis built upon SINETStream, which is a messaging system that adopts a topic-based pub/sub model. Third, we implement a framework that can remove weather-related (rain, snow, and fog) noise by applying weather classification and adaptive noise removal models that improve the accuracy of video analysis from data collected outdoors. The latency, throughput, and image quality benchmark experiments conducted to validate the feasibility of our proposed system showed that the process resulted in image quality improvements of approximately 30% (on average).},
keywords={Image quality;Adaptation models;Image edge detection;Object detection;Streaming media;Throughput;Internet of Things;IoT;Edge Computing;P2P;Video Stream;Object Detection},
doi={10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00214},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9644693,
author={Wu, Junhang and Hu, Ruimin and Li, Dengshi and Xiao, Yilin and Ren, Lingfei and Hu, Wenyi},
booktitle={2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)},
title={Multi-network Embedding for Missing Point-of-Interest Identification},
year={2021},
volume={},
number={},
pages={1386-1393},
abstract={The large volume of data flowing throughout location-based social networks (LBSNs) provides an opportunity for human mobility behavior understanding and prediction. However, data quality issues (e.g., historical check-in POI missing, data sparsity) limit the effectiveness of existing LBSN-oriented studies, e.g., Point-of-Interest (POI) recommendation or prediction. Contrary to previous efforts in next POI recommendation or prediction, we focus on identifying the missing POI which the user has visited at a past specific time and proposed a multi-network Embedding (MNE) method. Specifically, the model jointly captures temporal cyclic effect, user preference and sequence transition influence in a unified way by embedding five relational information graphs into a shared dimensional space from both POI- and category-instance levels. The proposed model also incorporates region-level spatial proximity to explore geographical influence, and derives the ranking score list of candidates for missing POI identification. We conduct extensive experiments to evaluate the performance of our model on two real large-scale datasets, and the experimental results show its superiority over other competitors. Significantly, it also proves that the proposed model can be naturally transferred to general next POI recommendation and prediction tasks with competitive performances.},
keywords={Social networking (online);Data integrity;Predictive models;Task analysis;Tuning;Context modeling;location-based social networks (LBSNs);missing POI identification;multi-network embedding},
doi={10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00189},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9276830,
author={Zhang, Huaxin and Liu, Yu and Wang, Zituo and Li, Tiansong and Cao, Keyin},
booktitle={2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)},
title={Research on Film Data Preprocessing and Visualization},
year={2020},
volume={1},
number={},
pages={946-952},
abstract={Data is the core of information, and good data quality is a prerequisite for many data analysis. Data cleaning is to increase the fault tolerance rate by correcting the error value of detected data. This paper aims to solve the problem of data set processing and visualization in the recommendation algorithm, so as to better apply in the field of recommendation algorithm. The recommendation algorithm and data sets Movielens and IMDB are analyzed theoretically. First, data set A was processed from data reading and movie score calculation; Again, the IMDB is processed in four steps to make it more suitable for the recommendation algorithm field; Finally, the plot function is used to visualize the key information. experiment shows: The data set sorted out by the above methods can effectively improve the quality and availability of data and provide relevant basis for better application in the algorithm.},
keywords={Motion pictures;Data visualization;Visualization;Prediction algorithms;Market research;Arrays;Electronic commerce;recommended algorithm;Dataset;data processing;data visualization},
doi={10.1109/ICIBA50161.2020.9276830},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9361005,
author={Lu, Xin and Wang, Yu and Yuan, Jiao and Wang, Xun and Fu, Kun and Yang, Ke},
booktitle={2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)},
title={A Parallel Adaptive DBSCAN Algorithm Based on k-Dimensional Tree Partition},
year={2020},
volume={},
number={},
pages={249-256},
abstract={The existing parallel DBSCAN (density based spatial clustering of applications with noise) algorithm needs to determine the parameter settings manually, and the datasets will be repeatedly accessed in the process of data partitioning and data merging, which reduces the efficiency of the algorithm excuting. Therefore, this paper proposes a parallel adaptive DBSCAN algorithm based on k-dimensional tree partition. It divides the dataset into several balanced data partitions by using k-dimensional tree, and carries out parallel computing in spark distributed computing framework, thus increasing the concurrent processing ability of the algorithm program and improving the I/O access speed. In addition, the improved adaptive DBSCAN parameter method is applied to each data partition for clustering analysis to obtain local clusters, which solves the random problem of manual setting parameters in the clustering process, and ensures the data quality of clustering mining. At the same time of creating local clusters, this algorithm also puts the mapping relationship between data points and adjacent points into the HashMap data structure of the master node, and uses it to merge local clusters into whole clusters, which can reduce the time cost of data merging. The experimental results show that the proposed algorithm can save about 18% running time compared with RDD-DBSCAN algorithm without reducing the clustering quality. With the increase of the number of cluster nodes, the running efficiency of the algorithm can be further improved, so it is suitable for processing massive data clustering analysis.},
keywords={Machine learning algorithms;Merging;Clustering algorithms;Data structures;Partitioning algorithms;Sparks;Data mining;clustering analysis;data partition;k-dimensional tree;adaptive computing;Spark framework},
doi={10.1109/MLBDBI51377.2020.00053},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9402999,
author={Lu, Tielin and Fan, Zitian and Lei, Yue and Shang, Yujia and Wang, Chunxi},
booktitle={2021 IEEE 6th International Conference on Big Data Analytics (ICBDA)},
title={The Edge Computing Cloud Architecture Based on 5G Network for Industrial Vision Detection},
year={2021},
volume={},
number={},
pages={328-332},
abstract={The emergence of a large number of real-time data putforward higher requirements on network transmission technology. The new edge computing cloud technology based on 5G network has become an important research direction of vision detection. However, for the industrial users, they still confuse the architecture of the non-public 5G network (NPN) and misunderstand the data quality of service (QOS). In order to overcome the unstable network structure of 5G for vision detection in industry in a limited bandwidth, achieve high-quality transmission of detection image, and obtain intelligent optimal results, has become an urgent problem to be solved. This paper proposes the network configuration and mode, also design a intelligent edge computing cloud based on 5G scheme. In the ends, an vision detection architecture case has been developed on the 5G communication structure and verified visual detection application scene design its feasibility purpose in the wireless network.},
keywords={Industries;Cloud computing;Visualization;5G mobile communication;Wireless networks;Data integrity;Computer architecture;5G network;vision detection;communication structure;edge computing;information systems},
doi={10.1109/ICBDA51983.2021.9402999},
ISSN={},
month={March},}
@INPROCEEDINGS{9730364,
author={Wu, Hao and Liu, Qi and Liu, Xiaodong and Zhang, Yonghong and Xu, Xiaolong and Bilal, Muhammad},
booktitle={2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
title={A FCN Approach to Blockage Correction in Radars},
year={2021},
volume={},
number={},
pages={482-487},
abstract={Doppler weather radar is the most widely used convection detector with the highest resolution in the ground. Echo reflectance data from the weather radar is the key reference for the meteorological department to carry out severe convective weather forecast and early warning, quantitative precipitation estimation(QPE) and quantitative precipitation forecast(QPF). However, in the process of radar detection, it is inevitable to be affected by obstacles, ground object echo interference, radar echo attenuation and other phenomena, resulting in poor data quality of detection results. Therefore, it is very important to correct the missing or disturbed data. On the other hand, with the rapid development of artificial intelligence technology in recent years, more and more meteorological researchers begin to introduce deep learning and other machine learning methods into the research of meteorological field such as weather radar data processing. In this paper, a deep convolutional encoder-decoder network is proposed to correct the beam blocking of weather radar. In this study, the correction of radar beam blockage is regarded as an image inpainting problem. It's the first trying to use deep learning to realize the correction of radar beam blockage. Experiment shows that the method proposed in this paper is significantly better than the traditional method in accuracy, error rate, false alarm rate and other aspects. The method can directly identify and correct the blocking area, and the operation procedure is simple compared traditional methods.},
keywords={Deep learning;Reflectivity;Meteorological radar;Semantics;Neural networks;Radar detection;Weather forecasting;Deep learning;convolutional neural networks;encoder-decoder network;weather radar;image inpainting;blockage correction},
doi={10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00086},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9302543,
author={Zada, Muhammad Sadiq Hassan and Yuan, Bo and Anjum, Ashiq and Azad, Muhammad Ajmal and Khan, Wajahat Ali and Reiff-Marganiec, Stephan},
booktitle={2020 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)},
title={Large-scale Data Integration Using Graph Probabilistic Dependencies (GPDs)},
year={2020},
volume={},
number={},
pages={27-36},
abstract={The diversity and proliferation of Knowledge bases have made data integration one of the key challenges in the data science domain. The imperfect representations of entities, particularly in graphs, add additional challenges in data integration. Graph dependencies (GDs) were investigated in existing studies for the integration and maintenance of data quality on graphs. However, the majority of graphs contain plenty of duplicates with high diversity. Consequently, the existence of dependencies over these graphs becomes highly uncertain. In this paper, we proposed graph probabilistic dependencies (GPDs) to address the issue of uncertainty over these large-scale graphs with a novel class of dependencies for graphs. GPDs can provide a probabilistic explanation for dealing with uncertainty while discovering dependencies over graphs. Furthermore, a case study is provided to verify the correctness of the data integration process based on GPDs. Preliminary results demonstrated the effectiveness of GPDs in terms of reducing redundancies and inconsistencies over the benchmark datasets.},
keywords={Probabilistic logic;Uncertainty;Data integration;Data integrity;Redundancy;Scalability;Erbium;data integration;information retrieval;NoSQL databases;graph probabilistic dependencies;data science},
doi={10.1109/BDCAT50828.2020.00028},
ISSN={},
month={Dec},}
