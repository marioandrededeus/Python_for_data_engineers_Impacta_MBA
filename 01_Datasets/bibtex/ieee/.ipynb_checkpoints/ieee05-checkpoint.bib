@INPROCEEDINGS{8547038,
author={Brahim, Mohamed Ben and Menouar, Hamid},
booktitle={2017 6th IEEE International Conference on Advanced Logistics and Transport (ICALT)},
title={Optimizing V2X Data Collection and Storage for a Better Cost and Quality Trade-off},
year={2017},
volume={},
number={},
pages={7-12},
abstract={Future vehicles will be equipped with advanced communication capabilities and a multitude of sensing devices. Vehicle-to-vehicle and to Infrastructure (V2X) is one of these future technologies. V2X-technology-enabled vehicles are expected to become a great source of big data. This data, if gathered in the right time and processed in the right way, can enable an interesting number of existing and new applications. This can be a challenging task, taken into account the considerable size of the data that will be gathered. One of the challenges is to find a good balance between the number of data to filter out and the quality of the end data. This contribution tackles this specific challenge, by studying data storage cost reduction and evaluating its impact on the data quality. The proposed solution compares three approaches of treating the collected data at the road-side unit after taking out unnecessary information details. This solution has been tested and validated through simulations that show promising results.},
keywords={Sensors;Containers;Roads;Data integrity;Vehicle dynamics;Data mining;Memory;V2X wireless communication;vehicular edge computing;data storage;data quality;dimension reduction;data sampling},
doi={10.1109/ICAdLT.2017.8547038},
ISSN={},
month={July},}
@INPROCEEDINGS{9888158,
author={Xiong, Jianying},
booktitle={2022 International Conference on Computation, Big-Data and Engineering (ICCBE)},
title={Recognition of Illegal Websites Based on Similarity of Sensitive Features of Mixed Elements},
year={2022},
volume={},
number={},
pages={9-12},
abstract={In order to identify illegal websites efficiently, a recognition method based on feature similarity of multiple web page mixed elements is proposed. This method constructs the website feature vector based on the web page text content, access path, and website association platform attribute. The method calculates the similarity between the website to be judged and the historical website in the case base. Then, the average similarity of top k cases is selected as the basis for judging the possibility of illegal websites. The experiment shows that the judgment based on similarity can achieve better results in the context of fewer historical cases and better case data quality. Therefore, the method is suitable for scenes where it is difficult to obtain typical illegal website cases.},
keywords={Sensitivity;Data integrity;Web pages;Feature extraction;Character recognition;llegal websites;website identification;sensitive feature Similarity;Mixed Elements of websites},
doi={10.1109/ICCBE56101.2022.9888158},
ISSN={},
month={May},}
@INPROCEEDINGS{9899089,
author={Yuan, Jianan and Huo, Chao and Zhang, Ganghong and Gao, Jian},
booktitle={CIBDA 2022; 3rd International Conference on Computer Information and Big Data Applications},
title={Research on data fusion method based on Federated learning},
year={2022},
volume={},
number={},
pages={1-5},
abstract={With the development of large data technology, the large variety and quantity of data put forward higher requirements on how to better use the data. The distribution of large data is characterized by "data island", "wide area dispersion", data dispersion, etc. Data types, data relationships and data quality are increasingly different, and contain a large number of unlabeled data, data sparse areas and domain knowledge. In order to solve the problem of multi-source heterogeneous data fusion, this paper innovatively introduces the Federal Learning algorithm, builds a Federal Learning Classifier, aggregates multiple fusion models of the same kind of data, and achieves the iterative optimization of data fusion without uploading data. The simulation results show that this algorithm model has certain advantages in data fusion stability and model iteration convergence speed.},
keywords={},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{7982314,
author={Makoondlall, Y.K. and Khaddaj, S. and Makoond, B. and Kethan, K.},
booktitle={2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)},
title={ZDLC : Layered Lineage Report across Technologies},
year={2016},
volume={},
number={},
pages={638-641},
abstract={As technology moves from being an enabler to become the lifeblood of businesses in the digital era, so does the data used to support the implementation of these technological shifts. In order for the data to be an asset and not a liability, it is primordial to ensure that the data captured and maintained over time is accurate, traceable, reliable and current. However, very often, in order to complete a business function the data from one system may be exported to another system or to a third party tool, which also changes the data. In order to keep track of the data and maximize the use and analysis of data, the Zero Deviation Life Cycle (ZDLC) framework proposes a series of tools to which can trace the data lineage, across several database technologies.},
keywords={Databases;Tools;Software;Big Data;Data mining;Companies;Zero Deviation Life Cycle (ZDLC);ZDLC;layered lineage;lineage report;data lineage;data quality},
doi={10.1109/CSE-EUC-DCABES.2016.252},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8574573,
author={Lv, Yirong and Sun, Bin and Luo, Qingyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
booktitle={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
title={CounterMiner: Mining Big Performance Data from Hardware Counters},
year={2018},
volume={},
number={},
pages={613-626},
abstract={Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running in a "24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance. In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.},
keywords={Hardware;Time series analysis;Program processors;Cloud computing;Microarchitecture;Data mining;Benchmark testing;Performance;big data;computer architecture;performance counters;data mining},
doi={10.1109/MICRO.2018.00056},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6927666,
author={Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu},
booktitle={2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)},
title={Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data},
year={2014},
volume={2},
number={},
pages={495-502},
abstract={In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.},
keywords={Privacy;Data privacy;Algorithm design and analysis;Educational institutions;Equations;Computer science;Data analysis;privacy preserving data mining;data publishing;algorithm},
doi={10.1109/WI-IAT.2014.139},
ISSN={},
month={Aug},}
@ARTICLE{9352014,
author={Jiang, Haoyu and Chen, Kai and Ge, Quanbo and Wang, Yun and Xu, Jinqiang and Li, Chunxi},
journal={IEEE Internet of Things Journal},
title={Fault Diagnosis of Power IoT System Based on Improved Q-KPCA-RF Using Message Data},
year={2021},
volume={8},
number={11},
pages={9450-9459},
abstract={As the power system develops from informatization to intelligence. Research on data services based on the Internet of Things (IoT) focuses more on application functions, but the research on the data quality of the IoT itself is insufficient. Long-term continuous operation of the big data IoT system has the risk of performance degradation or even partial fault, which leads to a decrease in the availability of collected data for intelligent analysis. In this article, based on the power IoT message data, the characteristics are established through a variety of improved detection methods, and then the abnormal data type is obtained through Q learning and fusion of the random forest (RF) identification features. Finally, the topology of the specific power user IoT system is combined with kernel principal component analysis (KPCA) + improved RF algorithm getting the abnormal location of the IoT. The results show that the research method has a significantly higher positioning accuracy (from 61% to 97%) than the traditional RF method, and the combination method has more advantages in parameter adjustment and classification accuracy than directly using a multilayer perceptron (MLP).},
keywords={Monitoring;Internet of Things;Random forests;Logic gates;Feature extraction;Clustering algorithms;Neural networks;Communication message;power Internet of Things (IoT) system;Q learning;random forest (RF)},
doi={10.1109/JIOT.2021.3058563},
ISSN={2327-4662},
month={June},}
@INPROCEEDINGS{9510797,
author={Yan, Xiaowen and Zhou, Yu and Huang, Fuxing and Wang, Xiaofen and Yuan, Peisen},
booktitle={2021 IEEE 4th International Electrical and Energy Conference (CIEEC)},
title={Privacy protection method of power metering data in clustering based on differential privacy},
year={2021},
volume={},
number={},
pages={1-6},
abstract={Power companies can use the power grid big data platform to cluster analysis of power metering data, which can improve the personalized service quality of power grid companies for different users and discover the power stealing behavior of users to protect the interests of power grid companies. However, in the cluster analysis of power measurement data, the privacy information of power users may also be disclosed. To defend the privacy information of power users, the article applies differential privacy technology to cluster analysis of power metering data to avoid power usersâ€™ privacy leakage. First, the article presents the attack model that exists in the cluster analysis of power metering data. Then, the article add Laplacian noise to the power metering data to defend against attacks in the cluster analysis of attackers. Next, to enhance the data availability of noise-added power measurement data in cluster analysis, the article limits noise distance based on the results of the cluster analysis. Experiments show that method proposed in article can guarantee the privacy information of power data during the cluster analysis of power metering data, and ensure the data quality of the power metering data after privacy protection.},
keywords={Differential privacy;Analytical models;Power measurement;Laplace equations;Data integrity;Conferences;Companies;power metering data;cluster analysis;differential privacy;Laplacian noise},
doi={10.1109/CIEEC50170.2021.9510797},
ISSN={},
month={May},}
@ARTICLE{9046025,
author={Pan, Yongsheng and Liu, Mingxia and Lian, Chunfeng and Xia, Yong and Shen, Dinggang},
journal={IEEE Transactions on Medical Imaging},
title={Spatially-Constrained Fisher Representation for Brain Disease Identification With Incomplete Multi-Modal Neuroimages},
year={2020},
volume={39},
number={9},
pages={2965-2975},
abstract={Multi-modal neuroimages, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), can provide complementary structural and functional information of the brain, thus facilitating automated brain disease identification. Incomplete data problem is unavoidable in multi-modal neuroimage studies due to patient dropouts and/or poor data quality. Conventional methods usually discard data-missing subjects, thus significantly reducing the number of training samples. Even though several deep learning methods have been proposed, they usually rely on pre-defined regions-of-interest in neuroimages, requiring disease-specific expert knowledge. To this end, we propose a spatially-constrained Fisher representation framework for brain disease diagnosis with incomplete multi-modal neuroimages. We first impute missing PET images based on their corresponding MRI scans using a hybrid generative adversarial network. With the complete (after imputation) MRI and PET data, we then develop a spatially-constrained Fisher representation network to extract statistical descriptors of neuroimages for disease diagnosis, assuming that these descriptors follow a Gaussian mixture model with a strong spatial constraint (i.e., images from different subjects have similar anatomical structures). Experimental results on three databases suggest that our method can synthesize reasonable neuroimages and achieve promising results in brain disease identification, compared with several state-of-the-art methods.},
keywords={Magnetic resonance imaging;Feature extraction;Diseases;Positron emission tomography;Medical diagnosis;Brain modeling;Deep learning;Multi-modal neuroimage;incomplete data;generative adversarial network;fisher vector;brain disease diagnosis;MRI;PET},
doi={10.1109/TMI.2020.2983085},
ISSN={1558-254X},
month={Sep.},}
@INPROCEEDINGS{7378670,
author={Yan Zhou and Haitian Xie},
booktitle={2015 23rd International Conference on Geoinformatics},
title={The integration technology of sensor network based on web crawler},
year={2015},
volume={},
number={},
pages={1-7},
abstract={Along with the development of the sensor system and sensor network, the wide applications of sensor networks have arisen at the historic moment. In reality, all kinds of sensors monitor every aspect of our life, which provides various services and brings the challenge: how to effectively integrate those distributed sensor resources and then can be used to find more advanced information or implement the sharing of resources are the big problems to be solved. Based on the framework of Sensor Web Enablement(SWE) which was proposed by Open GIS Consortium (OGC)and combined with the function of web crawler, we study and find Sensor Observation Service (SOS) service which is the core components of the SWE then we design a system based on the web crawler technology and the Istituto Scienze della Terra Sensor Observation Service (Istsos) architecture. The design of sensor network technology integration architecture includes three parts. The layer of data access which is the lowest layer encapsulates the access to the database or other source of resources. The layer of business logic it provides the core operation of component which was named Request Operator, this layer is used for processing various requests from the lowest layer in order to return the classes of listening. The layer of web and the client is connected, which can provide some thin client of SOS. The published server includes the ability of new services creation, addition of new sensors and relative metadata, visualization, and manipulation of stored observations, registration of new measures and setting of system properties like observable properties and data quality codes. In order to get sensor data, web crawler technology is used in our research, which can make us get sensor data from the target website, and the standardized sensor data is gotten by filtering the original data and then the data is uploaded to the database of Istsos with the standardized format. At last, the implementation of SOS architecture has been configured. The test's results show that the integrated architecture of services can effectively obtain the required sensor data and display them graphically.},
keywords={Service-oriented architecture;web crawler;sensor network;Sensor Observation Service (SOS);Tomact;Istsos},
doi={10.1109/GEOINFORMATICS.2015.7378670},
ISSN={2161-024X},
month={June},}
@ARTICLE{8988265,
author={Zhang, Yuhui and Li, Ming and Yang, Dejun and Tang, Jian and Xue, Guoliang and Xu, Jia},
journal={IEEE Internet of Things Journal},
title={Tradeoff Between Location Quality and Privacy in Crowdsensing: An Optimization Perspective},
year={2020},
volume={7},
number={4},
pages={3535-3544},
abstract={Crowdsensing enables a wide range of data collection, where the data are usually tagged with private locations. Protecting users' location privacy has been a central issue. The study of various location perturbation techniques, e.g., k-anonymity, for location privacy has received widespread attention. Despite the huge promise and considerable attention, provable good algorithms considering the tradeoff between location privacy and location information quality from the optimization perspective in crowdsensing are lacking in the literature. In this article, we study two related optimization problems from two different perspectives. The first problem is to minimize the location quality degradation caused by the protection of users' location privacy. We present an efficient optimal algorithm OLoQ for this problem. The second problem is to maximize the number of protected users, subject to a location quality degradation constraint. To satisfy the different requirements of the platform, we consider two cases for this problem: 1) overlapping and 2) nonoverlapping perturbations. For the former case, we give an efficient optimal algorithm OPUMO. For the latter case, we first prove its NP-hardness. We then design a (1-E)-approximation algorithm NPUMN and a fast and effective heuristic algorithm HPUMN. Extensive simulations demonstrate that OLoQ, OPUMO, and HPUMN significantly outperform an existing algorithm.},
keywords={Privacy;Crowdsensing;Degradation;Optimization;Perturbation methods;Sensors;Approximation algorithms;Crowdsensing;location data quality;location privacy;k-anonymity},
doi={10.1109/JIOT.2020.2972555},
ISSN={2327-4662},
month={April},}
@INPROCEEDINGS{9188195,
author={Demchenko, Yuri and Cushing, Reggie and Los, Wouter and Grosso, Paola and de Laat, Cees and Gommans, Leon},
booktitle={2019 International Conference on High Performance Computing & Simulation (HPCS)},
title={Open Data Market Architecture and Functional Components},
year={2019},
volume={},
number={},
pages={1017-1021},
abstract={This paper discusses the principles of organisation and infrastructure components of Open Data Markets (ODM) that would facilitate secure and trusted data exchange between data market participants, and other cooperating organisations. The paper provides a definition of the data properties as economic goods and identifies the generic characteristics of ODM as a Service. This is followed by a detailed description of the generic data market infrastructure that can be provisioned on demand for a group of cooperating parties. The proposed data market infrastructure and its operation are employing blockchain technologies for securing data provenance and providing a basis for data monetisation. Suggestions for trust management and data quality assurance are discussed.},
keywords={Economics;Cloud computing;Data models;Contracts;Big Data;Computer architecture;Open Data Market;Data Marketplace;Trusted Data Market;Industrial Data Space;Data Economics;STREAM Data Properties},
doi={10.1109/HPCS48598.2019.9188195},
ISSN={},
month={July},}
@INPROCEEDINGS{8441160,
author={Soe, Thin Thin and Min, Myat Myat},
booktitle={2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)},
title={Speeding up Incomplete Data Analysis using Matrix-Represented Approximations},
year={2018},
volume={},
number={},
pages={206-211},
abstract={The veracity related with data quality such as incomplete, imprecise and inconsistent data creates a major challenge to data mining and data analysis. Rough set theory provides a special tool for handling the imprecise and incomplete data in information systems. However, the existing rough set based incomplete data analysis methods may not be able to handle large amount of data within the acceptable time. This paper focuses on speeding up the incomplete data analysis. The computation of the lower and upper approximations is a vital step for improving the performance of rough set based data analysis process. In this paper, the lower and upper approximations are characterized as matrix-represented approximations. The resulting approximations are exploited as inputs for data analysis method LERS (Learning from Examples based on Rough Set) used with LEM2 (Learning from Examples Module, Version2) rule induction algorithm. Then, this paper provides a set of experiments on missing datasets with different missing percent. The experimental results on incomplete or missing datasets from UCI Machine Learning Repository show that the proposed system effectively reduces the computational time in comparison with the existing system.},
keywords={Rough sets;Data analysis;Approximation algorithms;Big Data;Data integrity;Tools;rough set;incomplete data;missing values;approximations;matrix},
doi={10.1109/SNPD.2018.8441160},
ISSN={},
month={June},}
@INPROCEEDINGS{9783826,
author={Bai, Junfeng and Li, Xiang and Wang, Weibin and Qu, Haini},
booktitle={2022 7th Asia Conference on Power and Electrical Engineering (ACPEE)},
title={A new perspective of power system operation and management -- Research on power/energy data asset value evaluation model for power grid enterprises},
year={2022},
volume={},
number={},
pages={1462-1466},
abstract={Based on the energy characteristics, this paper constructs a data asset value evaluation model of power grid enterprises, in order to explores the operation and management of power system from a new perspective. Firstly, the influencing factors of data asset value of power grid enterprises are analyzed from the dimensions of data scale, data quality, data management, data application and data risk. Secondly, the evaluation index system of data asset value of power grid enterprises is analyzed based on the above perspectives, and the weighting evaluation is carried out by Delphi method and analytic hierarchy process. The model constructed in this paper provides a theoretical reference for the evaluation practice of data asset value of power grid enterprises in the future.},
keywords={Electrical engineering;Analytical models;Data integrity;Asia;Analytic hierarchy process;Big Data applications;Power grids;data asset value;electricity characteristics;energy characteristics;power grid enterprise;evaluation model},
doi={10.1109/ACPEE53904.2022.9783826},
ISSN={},
month={April},}
@INPROCEEDINGS{9064032,
author={Gao, Jian and Zhen, Yan and Bai, Huifeng and Huo, Chao and Wang, Dongshan and Zhang, Ganghong},
booktitle={2019 IEEE 5th International Conference on Computer and Communications (ICCC)},
title={Research and Analysis Validation of Data Fusion Technology Based on Edge Computing},
year={2019},
volume={},
number={},
pages={97-101},
abstract={Based on the smart grid as the research background, this paper responded to the massive multi-source data processing requirements of the smart grid, and combined with distributed computing to provide the edge of the solution, aiming at the existing data of electric power equipment state monitoring data in noise and redundant data problems. A distributed kalman filter algorithm based on edge of computing was put forward. In this algorithm, event decision strategy was added to the data processing and transmission process of edge computing terminal to control the communication times between nodes and terminals in an event-driven way. Meanwhile, redundant data and data interfered by noise were reduced through the processing of the algorithm, so as to ensure the data quality and improve the fusion efficiency. Finally, the effectiveness of the method was verified by the analysis of compression efficiency and data fusion time.},
keywords={Data integration;Power grids;Kalman filters;Monitoring;Big Data;Data models;Distributed databases;smart grid;multi-source data;edge computing;filtering algorithm},
doi={10.1109/ICCC47050.2019.9064032},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7543784,
author={Moysen, Jessica and Giupponi, Lorenza and Mangues-Bafalluy, Josep},
booktitle={2016 IEEE Symposium on Computers and Communication (ISCC)},
title={On the potential of ensemble regression techniques for future mobile network planning},
year={2016},
volume={},
number={},
pages={477-483},
abstract={Planning of current and future mobile networks is becoming increasingly complex due to the heterogeneity of deployments, which feature not only macrocells, but also an underlying layer of small cells whose deployment is not fully under the control of the operator. In this paper, we focus on selecting the most appropriate Quality of Service (QoS) prediction techniques for assisting network operators in planning future dense deployments. We propose to use machine learning as a tool to extract the relevant information from the huge amount of data generated in current 4G and future 5G networks during normal operation, which is then used to appropriately plan networks. In particular, we focus on radio measurements to develop correlative statistical models with the purpose of improving QoS-based network planning. In this direction, we combine multiple learners by building ensemble methods and use them to do regression in a reduced space rather than in the original one. We then compare the QoS prediction accuracy of various approaches that take as input the 3GPP Minimization of Drive Tests (MDT) measurements collected throughout a heterogeneous network and analyse their trade-offs. We also explain how the collected data is processed and used to predict QoS expressed in terms of Physical Resource Block (PRB)/ Megabit (MB) transmitted. This metric was selected because of the interest it may have for operators in planning, since it relates lower layer resources with their impact in terms of QoS up in the protocol stack, hence closer to the end-user.},
keywords={Quality of service;Planning;Training;Computers;Measurement;Principal component analysis;3GPP;Machine Learning;Big Data;Quality of Service;Prediction;Network planning;Minimization of Drive Tests},
doi={10.1109/ISCC.2016.7543784},
ISSN={},
month={June},}
@INPROCEEDINGS{9183812,
author={Tshikomba, Salome C. and Estrice, Milton and Ojo, Evans and Davidson, Innocent E},
booktitle={2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)},
title={Curbing Electricity Theft Using Wireless Technique with Communication Constraints},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Utility services are experiencing a common problem of power losses, which impose a significant impact on their annual budget. Practically, power losses consist of technical losses and non-technical losses. Technical losses are due to operations and aging of infrastructure, while nontechnical losses (NTL) are due to non-metered energy. The focus is on managing non-technical losses using an automation wireless method. The wireless ZigBee technique is proposed and further investigated for communication failure over long distances while solving the problem of stealing electricity. Advance-metering infrastructure (AMI) technique and smart meters are feasible for system integration; that is why they are chosen to be part of this study. The success of the study depends on quality data of the Utility, meaning the more accurate the data, the easier the analysis of outliers. The operation and planning of revenue protection contain a large amount of data that needs to be worked on, so data mining assists in that regard. Then the load profiling method assists in illustrating the variation in demand/electricalload over a specific time. This is a preliminary investigation using a wireless communication technique as a viable solution in curbing electricity theft. The uniqueness of the proposed ZigBee system is that it recognizes the everyday act of stealing electricity through tempering with the meter box and tapping of the supply.},
keywords={ZigBee;Meters;Wireless communication;Automation;Smart meters;Monitoring;Sensors;Losses;smart meters;AMI;wireless technique;ZigBee technology;NTL},
doi={10.1109/icABCD49160.2020.9183812},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8530818,
author={Francis, Akindipe Olusegun and Emmanuel, Bugingo and Zhang, Defu and Zheng, Wei and Qin, Yingsheng and Zhang, Dongzhan},
booktitle={2018 Sixth International Conference on Advanced Cloud and Big Data (CBD)},
title={Exploration of Secured Workflow Scheduling Models in Cloud Environment: A Survey},
year={2018},
volume={},
number={},
pages={71-76},
abstract={Cloud computing (CC) is a useful tool for executing complex applications. As a result of this, it has become so popular and used in diverse domains such as science, engineering, medicine. etc. CC structure is composed of a number of virtual machines(VMs) provisioned on demand and charged on a "Pay-as-you-go" basis, it is deployed in different form of access levels. Complex applications needed to be executed on clouds are represented as workflows. Workflow scheduling (WS) is one of the most important concepts in cloud computing. WS model contributes to minimizing cost, makespan and energy as well as maximize the quality of service(QoS) of applications in clouds. Despite the security constraints set by each provider, CC has become so critical due to the considerations of applications with sensitive intermediate data, this thereby requires a security level known as Secured workflow Scheduling(SWS). This security is on the level of executing workflows. It indicates that applications with sensitive interdependent data have to be protected during their execution across different cloud VMs. The addition of security in workflow execution generates time overhead, making it complex to meet up with the QoS required by the users. Some research works have proposed algorithms for providing the QoS requirements and security at the same time. In this work, we survey some existing works, by defining the factors needed in securing workflows during execution, clarifying the domains for security, sources of security threats and their solutions as well as cloud computing services that needs security and lastly classify the proposed algorithm depending cloud computing components.},
keywords={Cloud computing;Security;Task analysis;Processor scheduling;Quality of service;Scheduling;Computational modeling;cloud computing;workflow scheduling;security;survey},
doi={10.1109/CBD.2018.00022},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7229729,
author={Gowsalya, M and Krushitha, K and Valliyammai, C},
booktitle={2014 Sixth International Conference on Advanced Computing (ICoAC)},
title={Predicting the risk of readmission of diabetic patients using MapReduce},
year={2014},
volume={},
number={},
pages={297-301},
abstract={From the banking to retail, many sectors have already embraced big data regardless of whether the information comes from public or private sources. In the clinical sphere, the amount of patient data has grown exponentially because of computer based information systems. E-Health monitoring applications have some particularities concerning the importance on data quality. This paper presents a novel solution using Hadoop Mapreduce to analyze large datasets and extract useful insights from the dataset which helps doctors to effectively allocate resources. The successful healthcare delivery and planning strongly rely on data (e.g. sensed data, diagnosis, administration information); the higher quality of the data, the better will be the patient assistance. The applications are also particularly exposed to a contextual environment (i.e., patient's mobility, communication technologies, performance, information heterogeneity, etc.) that has an important impact on information management and application achievement. The main objective of our system is to predict the risk of diabetic patients for readmission in the next 30 days by measuring the probability using MapReduce. This risk score helps the physicians in recommending appropriate care for the patients.},
keywords={Atmospheric measurements;Particle measurements;Diabetes;Sociology;Statistics;Artificial neural networks;Big data;Healthcare;Predictive analytics;Diabetes},
doi={10.1109/ICoAC.2014.7229729},
ISSN={2377-6927},
month={Dec},}
@INPROCEEDINGS{9615868,
author={Li, Ling and Li, Weibang and Zhu, Lidong and Li, Chengjie and Zhang, Zhen},
booktitle={2021 International Symposium on Networks, Computers and Communications (ISNCC)},
title={Automatic Data Repairs with Statistical Relational Learning},
year={2021},
volume={},
number={},
pages={1-6},
abstract={Dirty data is ubiquitous in real-world, and data cleaning is a long-standing problem. The importance of data cleaning is growing in the era of big data. In this paper we propose a novel data repairing approach by leveraging statistical relational learning (SRL). We learn Bayesian networks of attributes from the dirty data, then transform the dependency relationships among attributes into first-order logic formulas. We calculate the weight of each formula based on the mutual information of the attributes involved in the formula and obtain Markov logic network (often abbreviated as MLN) by assigning weight to each first-order logic formula. Then we transform Markov logic networks into inference rules and conduct these inference rules on DeepDive. The inference results are utilized to repair dirty data at last. Experiments on real-world datasets demonstrate that our approach has higher accuracy in terms of different situations and is universal for different kinds of datasets.},
keywords={Data integrity;Transforms;Markov processes;Maintenance engineering;Probabilistic logic;Cleaning;Inference algorithms;Data repairing;data cleaning;data quality;statistical relational learning;DeepDive;factor graph},
doi={10.1109/ISNCC52172.2021.9615868},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9332846,
author={Fei, Chen},
booktitle={2020 IEEE International Conference on Industrial Application of Artificial Intelligence (IAAI)},
title={Research on Lidar Data Error Correction Method Based on Bayesian Network},
year={2020},
volume={},
number={},
pages={486-491},
abstract={Data quality analysis is the first and key step of remote sensing mechanism/application research (especially quantitative remote sensing), which directly affects the accuracy of remote sensing inversion and the effect of remote sensing applications. Lidar (light detection and ranging, referred to as lidar) is a new active remote sensing technology. The research on hardware system development and data post-processing algorithm needs to be further strengthened, especially for the quality analysis of lidar data. After initializing the Bayesian network and checking the error, a correction mathematical model is established. Experiments have proved that after correction, the angle error of the radar is significantly improved, which verifies the feasibility and reliability of the precision orbit star calibration method.},
keywords={Laser radar;Orbits (stellar);Bayes methods;Error correction;Calibration;Reliability;Remote sensing;Bayesian network;lidar;data error;correction methoIntroduction},
doi={10.1109/IAAI51705.2020.9332846},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6406445,
author={Loh, Ji Meng and Dasu, Tamraparni},
booktitle={2012 IEEE 12th International Conference on Data Mining Workshops},
title={Effect of Data Repair on Mining Network Streams},
year={2012},
volume={},
number={},
pages={226-233},
abstract={Data quality issues have special implications in network data. Data glitches are propagated rapidly along pathways dictated by the hierarchy and topology of the network. In this paper, we use temporal data from a vast data network to study data glitches and their effect on network monitoring tasks such as anomaly detection. We demonstrate the consequences of cleaning the data, and develop targeted and customized cleaning strategies by exploiting the network hierarchy.},
keywords={Maintenance engineering;Data mining;Cleaning;Measurement;Time series analysis;Context;Information management;Data glitches;Big Data;missing values;outliers;network analysis;Earth Mover Distance},
doi={10.1109/ICDMW.2012.125},
ISSN={2375-9259},
month={Dec},}
@INPROCEEDINGS{9806000,
author={Dokic, Tatjana and Baembitov, Rashid and Hai, Ameen Abdel and Cheng, Zheyuan and Hu, Yi and Kezunovic, Mladen and Obradovic, Zoran},
booktitle={2022 International Conference on Smart Grid Synchronized Measurements and Analytics (SGSMA)},
title={Machine Learning Using a Simple Feature for Detecting Multiple Types of Events From PMU Data},
year={2022},
volume={},
number={},
pages={1-6},
abstract={This paper describes simple and efficient machine learning (ML) methods for efficiently detecting multiple types of power system events captured by PMUs scarcely placed in a large power grid. It uses a single feature from each PMU based on a rectangle area enclosing the event in a given data window. This single feature is sufficient to enable commonly used ML models to detect different types of events quickly and accurately. The feature is used by five ML models on four different data-window sizes. The results indicated a tradeoff between the execution speed and detection accuracy in variety of data-window size choices. The proposed method is insensitive to most data quality issues typical for data from field PMUs, and thus it does not require major data cleansing efforts prior to feature extraction.},
keywords={Measurement units;Measurement uncertainty;Area measurement;Feature extraction;Size measurement;Phasor measurement units;Time measurement;Big data;Event detection;Machine learning;Phasor measurement units;Power system faults;Time series analysis},
doi={10.1109/SGSMA51733.2022.9806000},
ISSN={},
month={May},}
@INPROCEEDINGS{7336420,
author={Zhao, Liang and Chen, Zhikui and Yang, Zhennan and Hu, Yueming},
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems},
title={A Hybrid Method for Incomplete Data Imputation},
year={2015},
volume={},
number={},
pages={1725-1730},
abstract={With the explosive increase of data volume, the research of data quality and data usability draws extensive attention. In this work, we focus on one aspect of data usability -- incomplete data imputation, and present a novel missing value imputation method using stacked auto-encoder and incremental clustering (SAICI). Specifically, SAICI's functionality rests on four pillars: (i) a distinctive value assigned to impute missing values initially, (ii) the stacked auto-encoder(SAE) applied to locate principal features, (iii) a new incremental clustering utilized to partition incomplete data set, and (iv) the top nearest neighbors' weighted values designed to refill the missing values. Most importantly, stages (ii)~(iv) iterate until convergence condition is satisfied. Experimental results demonstrate that the proposed scheme not only imputes the missing data values effectively, but also has better time performance. Moreover, this work is suitable for distributed data processing framework, which can be applied to the imputation of incomplete big data.},
keywords={Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Accuracy;Feature extraction;Integrated circuits;Time complexity;missing values;data imputation;stacked auto-encoder;incremental clustering},
doi={10.1109/HPCC-CSS-ICESS.2015.103},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8653702,
author={Suresh, T. and Murugan, A.},
booktitle={2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on},
title={Strategy for Data Center Optimization : Improve Data Center capability to meet business opportunities},
year={2018},
volume={},
number={},
pages={184-189},
abstract={Considering current evolving technology and the way data are growing, IT consulting and outsourcing industry expected to be strategic partner for technology innovation in addition to support on-going business with reduced operational cost. Data Center is backbone for digital economy, big data, cloud, artificial intelligence, IoT or wearable technology. Data growth and on-demand data access changed the focus of data center as storage and disaster recovery to access data instantly from cloud without compromising security controls and data quality. These technology transformations create demand for latency. Every organization like Facebook, Equinix, Amazon, and Google are having their own data centers and expanding their business on cloud services. Data Center plays major critical on success of digital business. It is important to find possible options to optimize infrastructure and improve efficiency and productivity of Data Center. At the same time, we need to make sure that environment is up and running without compromising quality and security of data. This paper gives few solutions to get more from Data Center, reduce operational cost and optimize infrastructure utilization.},
keywords={Data centers;Cloud computing;Optimization;Organizations;Servers;Maintenance engineering;Data Center;energy saving;green computing;server;network devices;cloud storage},
doi={10.1109/I-SMAC.2018.8653702},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6860977,
author={Fresco Zamora, Jane Louie and Sawada, Naoya and Sahara, Takemi and Kashihara, Shigeru and Taenaka, Yuzo and Yamaguchi, Suguru},
booktitle={2014 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) Proceedings},
title={Surface weather observation via distributed devices},
year={2014},
volume={},
number={},
pages={1405-1410},
abstract={With rain-related hazards, it is difficult to forecast and prepare for it due to the decline in the availability and reliability of global daily weather reports. Therefore, we need to make use of available unconventional commercial weather instruments to provide supplementary information to existing systems. A more accurate and reliable weather forecast can then be made due to the prompt availability of information from ubiquitous devices. As smartphones become a widely used device, we propose a conceptual design of a multi-device ground weather observation network using smartphones and other sensors. In this paper, we first investigate the differences between smartphone-based sensors and other sensors to determine issues to address for big data on global weather information. In our experiments, we found that the data quality differs among devices in a very small area of 100 m grid.},
keywords={Temperature measurement;Temperature sensors;Rain;Smart phones;Humidity;Distributed devices;Measurement;Localized Rain},
doi={10.1109/I2MTC.2014.6860977},
ISSN={1091-5281},
month={May},}
@INPROCEEDINGS{9774453,
author={Phan, Quoc-Thang and Wu, Yuan-Kang and Phan, Quoc-Dung and Lo, Hsin-Yen},
booktitle={2022 8th International Conference on Applied System Innovation (ICASI)},
title={A Study on Missing Data Imputation Methods for Improving Hourly Solar Dataset},
year={2022},
volume={},
number={},
pages={21-24},
abstract={In the era of big data, large period of missing data is a common problem which affect the data quality and final forecasting results if not handled properly. Therefore, filling missing data in datasets is importance since the most of real-time datasets have a huge number of missing values. This paper first gives a comprehensive overview of various imputation methods for filling missing data. Then proposes a technique based on a popular Multivariate Imputation by Chained Equation (MICE) to fill numeric data in PV dataset. Finally analyses the impact of this technique and compares the performance with other imputation algorithms. For practice, this study uses historical measurement PV generation from the North PV site of Taiwan, and Numerical Weather Prediction (NWP) data consists of solar irradiance, temperature, sea level pressure, humidity, rainfall, wind speed. The NWP dataset is provided by Taiwan Central Weather Bureau (CWB) which is called Deterministic Weather Research and Forecasting (WRFD). Experimental results showed that the proposed imputation algorithm can improve short-term PV generation forecasting accuracy based on RMSE.},
keywords={Temperature measurement;Temperature distribution;Wind speed;Sea measurements;Weather forecasting;Prediction algorithms;Mathematical models;Solar Power Forecasting;Preprocessing;Missing data;Multivariate Imputation By Chained Equations},
doi={10.1109/ICASI55125.2022.9774453},
ISSN={2768-4156},
month={April},}
@INPROCEEDINGS{8428903,
author={Dai, Minghui and Su, Zhou and Wang, Yuntao and Xu, Qichao},
booktitle={2018 International Conference on Selected Topics in Mobile and Wireless Networking (MoWNeT)},
title={Contract Theory Based Incentive Scheme for Mobile Crowd Sensing Networks},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Mobile crowd sensing networks (MCSNs) have emerged as a promising paradigm to provide various sensing services. With the increasing number of mobile users, how to develop an effective scheme to provide the high-quality and secure sensing data becomes a new challenge. In this paper, we propose a contract theory based scheme to provide sensing service in MCSNs. At first, with the analysis of the interaction experience between the crowd sensing platform and mobile user, a trust scheme is introduced to guarantee the quality of sensing data by considering the direct trust and indirect trust. Next, according to the transaction between crowd sensing platform and mobile user, an optimal contract based on incentive scheme is designed to stimulate mobile users to participate in crowd sensing network, where the contract item can not only maximize the platform utility, but also satisfy individual rationality and incentive compatibility. Finally, the numerical results show that the proposal outperforms the conventional schemes.},
keywords={Sensors;Contracts;Task analysis;Conferences;Big Data;Edge computing;Smart cities;Mobile crowd sensing;trust scheme;optimal contract},
doi={10.1109/MoWNet.2018.8428903},
ISSN={},
month={June},}
@ARTICLE{9797864,
author={Shi, Zhuan and Zhang, Lan and Yao, Zhenyu and Lyu, Lingjuan and Chen, Cen and Wang, Li and Wang, Junhao and Li, Xiang-Yang},
journal={IEEE Transactions on Big Data},
title={FedFAIM: A Model Performance-based Fair Incentive Mechanism for Federated Learning},
year={2022},
volume={},
number={},
pages={1-13},
abstract={Federated Learning (FL) has emerged as a privacy-preserving distributed machine learning paradigm. To motivate data owners to contribute towards FL, research on FL incentive mechanisms is gaining great interest. Existing monetary incentive mechanisms generally share the same FL model with all participants regardless of their contributions. Such an assumption can be unfair towards participants who contributed more and promote undesirable free-riding, especially when the final model is of great utility value to participants. In this paper, we propose a Fairness-Aware Incentive Mechanism for federated learning (FedFAIM) to address such problem. It satisfies two types of fairness notion: 1) aggregation fairness, which determines aggregation results according to data quality; 2) reward fairness, which assigns each participant a unique model with performance reflecting his contribution. Aggregation fairness is achieved through efficient gradient aggregation which examines local gradient quality and aggregates them based on data quality. Reward fairness is achieved through an efficient Shapley value-based contribution assessment method and a novel reward allocation method based on reputation and distribution of local and global gradients. We further prove reward fairness is theoretically guaranteed. Extensive experiments show that FedFAIM provides stronger incentives than similar non-monetary FL incentive mechanisms while achieving a high level of fairness.},
keywords={Computational modeling;Resource management;Servers;Training;Collaborative work;Particle measurements;Atmospheric measurements;Federated Learning;Incentive Mechanism;Fairness},
doi={10.1109/TBDATA.2022.3183614},
ISSN={2332-7790},
month={},}
@INPROCEEDINGS{9752191,
author={Zhao, Lixia and Jin, Wei},
booktitle={2022 International Conference on Electronics and Renewable Systems (ICEARS)},
title={Analysis on the Design and Implementation of the Metadata Management Model in the Cloud Computing Business Intelligence Platform},
year={2022},
volume={},
number={},
pages={1656-1659},
abstract={Through the method of metadata management development, it can give full play to its advantages and make up for its disadvantages. In order to fully grasp the composition, conversion, analysis and processing process of data in the platform, from metadata sources, metadata scope, metadata classification, metadata users, metadata integration project development methods, metadata models and metadata standards, metadata management the implementation of the system and other aspects expounded the metadata management strategy in the business intelligence system. Effective metadata management has increased the usability of the platform by 5.6% and the data quality of the platform by 7.8%.},
keywords={Analytical models;Renewable energy sources;Codes;Databases;Computational modeling;Data integrity;Metadata;Metadata Management Model;Cloud Computing;Physical Business Intelligence Platform;Big Data},
doi={10.1109/ICEARS53579.2022.9752191},
ISSN={},
month={March},}
@INPROCEEDINGS{7777890,
author={Finkelstein, Joseph and Jeong, In Cheol},
booktitle={2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)},
title={Using CART for advanced prediction of asthma attacks based on telemonitoring data},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Advanced prediction of asthma exacerbations may significantly improve patient quality of life and reduce costs of urgent care delivery. Majority of current algorithms predict who is likely to experience asthma exacerbation rather than when it is about to occur. We used data from asthma home-based telemonitoring for advanced prediction of asthma exacerbation. The goal of this project was to develop an algorithm that predicts asthma exacerbation one day in advance based on previous 7-day window. CART was used for predictive modeling. Resulting algorithm had specificity 0.971, sensitivity of 0.647, and accuracy of 0.809. We concluded that machine learning has great potential for advanced prediction of chronic disease exacerbations based on home telemonitoring data.},
keywords={Medical treatment;Diseases;Prediction algorithms;Predictive models;Pediatrics;Monitoring;Big data analytics;artificial intelligence;asthma;telemonitoring;exacerbation prediction},
doi={10.1109/UEMCON.2016.7777890},
ISSN={},
month={Oct},}
@ARTICLE{7523405,
author={Wang, Jianmin and Song, Shaoxu and Zhu, Xiaochen and Lin, Xuemin and Sun, Jiaguang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Efficient Recovery of Missing Events},
year={2016},
volume={28},
number={11},
pages={2943-2957},
abstract={For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering the missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all of the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem appears to be NP-hard. Nevertheless, advanced indexing, pruning techniques are developed to further improve the recovery efficiency. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to five orders of magnitudes improvement in time performance.},
keywords={Business;Petri nets;Engineering drawings;Indexes;Routing;Sun;Data mining;Data repairing;event data processing;petri net},
doi={10.1109/TKDE.2016.2594785},
ISSN={1558-2191},
month={Nov},}
@ARTICLE{8892516,
author={Liang, Tingting and Chen, Yishan and Gao, Wei and Chen, Ming and Zheng, Meilian and Wu, Jian},
journal={IEEE Access},
title={Exploiting User Tagging for Web Service Co-Clustering},
year={2019},
volume={7},
number={},
pages={168981-168993},
abstract={We propose a novel Web services clustering framework by considering the word distribution of WSDL documents and tags. Typically, tags are annotated to Web services by users for organization. In this paper, four strategies are proposed to integrate the tagging data and WSDL documents in the process of service clustering. Tagging data is inherently uncontrolled, ambiguous, and overly personalized. Two tag recommendation approaches are proposed to improve the tagging data quality and service clustering performance. Comprehensive experiments demonstrate the effectiveness of the proposed framework using a real-world dataset.},
keywords={Web services;Tagging;Search engines;Clustering algorithms;Task analysis;Feature extraction;Web service;WSDL documents clustering;co-clustering;tag recommendation},
doi={10.1109/ACCESS.2019.2950355},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7815071,
author={Tas, Yucel and Baeth, Mohamed Jehad and Aktas, Mehmet S.},
booktitle={2016 12th International Conference on Semantics, Knowledge and Grids (SKG)},
title={An Approach to Standalone Provenance Systems for Big Social Provenance Data},
year={2016},
volume={},
number={},
pages={9-16},
abstract={Provenance about data derivations in social networks is usually called social data provenance, which helps in the assessment of data quality, resource tracking, and understanding the dissemination of information in social networks. The collection and processing of social data provenance leads to some challenges such as scalability, data quality, and privacy awareness. This study introduces a test suite to evaluate the current state-of-the-art standalone and centralized provenance systems. We conduct performance (responsiveness) and scalability experiments and investigate whether the standalone provenance systems are capable of handling large-size social provenance data. We also propose a software architecture for a decentralized and scalable provenance management system for big social provenance data.},
keywords={Social network services;Scalability;Data privacy;Distributed databases;Web services;provenance systems;social provenance data;big provenance data;provenance storage systems;decentralized provenance systems},
doi={10.1109/SKG.2016.010},
ISSN={},
month={Aug},}
@ARTICLE{9464670,
author={Zhang, Yang and Guo, Hanqi and Shang, Lanyu and Wang, Dong and Peterka, Tom},
journal={IEEE Transactions on Big Data},
title={A Multi-branch Decoder Network Approach toAdaptive Temporal Data Selection andReconstruction for Big Scientific Simulation Data},
year={2021},
volume={},
number={},
pages={1-1},
abstract={A key challenge in scientific simulation is that the simulation outputs often require intensive I/O and storage space to store the results for effective post hoc analysis. This paper focuses on a quality-aware adaptive temporal data selection and reconstruction problem where the goal is to adaptively select simulation data samples at certain key timesteps in situ and reconstruct the discarded samples with quality assurance during post hoc analysis. This problem is motivated by the limitation of current solutions that a significant amount of simulation data samples are either discarded or aggregated during the sampling process, leading to inaccuratemodeling of the simulated phenomena. Two unique challenges exist: 1) the sampling decisions have to be made in situ and adapted tothe dynamics of the complex scientific simulation data; 2) the reconstruction error must be strictly bounded to meet the application requirement. To address the above challenges, we developDeepSample, an error-controlled convolutional neural network framework, that jointly integrates a set of coherent multi-branch deep decoders to effectively reconstruct the simulation data with rigorous quality assurance. The results on two real-world scientific simulation applications show that DeepSample significantly outperforms other state-of-the-art methods on both sampling efficiency and reconstructed simulation data quality.},
keywords={Data models;Adaptation models;Computational modeling;Analytical models;Image reconstruction;Data integrity;Adaptive systems;Big Scientific Simulation Data;Adaptive Temporal Data Selection and Reconstruction;Multi-branch Decoder Network},
doi={10.1109/TBDATA.2021.3092174},
ISSN={2332-7790},
month={},}
@INPROCEEDINGS{9060228,
author={Zhang, Xuejun and Chen, Qian and Peng, Xiaohui and Jiang, Xinlong},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={Differential Privacy-Based Indoor Localization Privacy Protection in Edge Computing},
year={2019},
volume={},
number={},
pages={491-496},
abstract={With the popularity of smart devices and the widespread use of the Wi-Fi-based indoor localization, edge computing is becoming the mainstream paradigm of processing massive sensing data to acquire indoor localization service. However, these data which were conveyed to train the localization model unintentionally contain some sensitive information of users/devices, and were released without any protection may cause serious privacy leakage. To solve this issue, we propose a lightweight differential privacy-preserving mechanism for the edge computing environment. We extend Îµ-differential privacy theory to a mature machine learning localization technology to achieve privacy protection while training the localization model. Experimental results on multiple real-world datasets show that, compared with the original localization technology without privacy-preserving, our proposed scheme can achieve high accuracy of indoor localization while providing differential privacy guarantee. Through regulating the value of Îµ, the data quality loss of our method can be controlled up to 8.9% and the time consumption can be almost negligible. Therefore, our scheme can be efficiently applied in the edge networks and provides some guidance on indoor localization privacy protection in the edge computing.},
keywords={Privacy;Training;Fingerprint recognition;Edge computing;Cloud computing;Indoor localization, Differential privacy, Privacy preserving, Edge computing.},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00125},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9744889,
author={Yang, Bikai and Bing, Han and Zhao, Haiyang and Gu, Tianshu and Cai, Tianxiao},
booktitle={2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)},
title={Multi dimensional disease intelligent detection device for underwater pier column structure through machine learning},
year={2022},
volume={},
number={},
pages={544-549},
abstract={The total number of railway bridges in China has exceeded twenty thousand. As an important part of the railway line, the existing railway bridge bears great live load, excessive concentrated force, obvious dynamic effect and high requirements for foundation stability. However, because the underwater structure of Railway Pier is constantly affected by many factors such as water scouring and ship collision during its service, it leads to a variety of diseases such as concrete peeling and exposed reinforcement. These diseases are often hidden in the water, which poses a serious threat to the structural safety of the bridge and the safety of people's lives and property. However, the traditional detection method of underwater structure of Railway Pier is affected by water flow, water quality, water depth and other environment, which has the pain points of high detection risk, low efficiency, poor detection data quality and missing disease location, so it is difficult to effectively detect underwater structure diseases; In this context, a multi-dimensional disease detection device is designed and developed for Wuhan Yangtze River Bridge (both highway and railway). The device includes a fixed module on water and an underwater detection module, which can realize safe and efficient detection under the condition of rapids and deep water, solve the problems that it is difficult for personnel and existing equipment to reach the structure to be tested, and the detection effect obtained by existing detection means is not ideal The detection information is not comprehensive, which is difficult to be used for key problems such as follow-up structural technical state evaluation.},
keywords={Underwater structures;Bridges;Road transportation;Employee welfare;Structural panels;Water quality;Rail transportation;Railway bridges;Pier underwater structure;Non destructive testing;Disease location;Machine learning},
doi={10.1109/EEBDA53927.2022.9744889},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9760371,
author={Han, Shengqiang and Qu, Jianhua and Song, Jinyi and Liu, Zijing},
booktitle={2022 7th International Conference on Big Data Analytics (ICBDA)},
title={Second-hand Car Price Prediction Based on a Mixed-Weighted Regression Model},
year={2022},
volume={},
number={},
pages={90-95},
abstract={With the development of motor vehicles, the circulation demand of motor vehicles in the form of "second-hand cars" in circulation links is increasing. As a special "e-commerce commodity", second-hand cars are more complicated than ordinary e-commerce commodities. As a result, it is difficult to estimate the price of second-hand cars, which is not only influenced by the basic configuration of the car, but also by the car conditions. At present, the state has not issued a standard to judge the value of second-hand car. To solve this problem, in this paper, first making feature engineering, which includes data preprocessing and feature screening. Data preprocessing includes data cleaning and data transformation, data cleaning includes removing outliers and filling missing values, and data transformation is used to unify data format to improve data quality. The feature screening includes correlation analysis and feature extraction based on LightMBG, and the screened features provide the basis for model building, training and prediction. Then, five regression models are constructed by using the feature attributes obtained by the feature engineering for training, and evaluated. Then, Random Forest and XGBoost are weighted and mixed to got a novel regression model, and the effect of the model is better than that of the five regression models. Finally, the novel regression model is used to predict the price of second-hand cars.},
keywords={Training;Analytical models;Data preprocessing;Linear regression;Predictive models;Feature extraction;Cleaning;second-hand car price prediction;weighted regression model;LightGBM;XGBoost;random forest},
doi={10.1109/ICBDA55095.2022.9760371},
ISSN={},
month={March},}
@INPROCEEDINGS{7425923,
author={Hailong Liu and Zhanhuai Li and Cheqing Jin and Qun Chen},
booktitle={2016 International Conference on Big Data and Smart Computing (BigComp)},
title={Web-based techniques for automatically detecting and correcting information errors in a database},
year={2016},
volume={},
number={},
pages={261-264},
abstract={It is critical to detect and correct information errors effectively to achieve higher data quality in many applications. Most existing techniques only use the intrinsic information to detect and correct a database, provided that data is adequate and well-structured. These techniques will not work properly if there is no sufficient data available. Integrating the information from external sources, like the World Wide Web (WWW), can help us overcome the shortcomings of existing techniques to a great extent. In this paper, we introduce our on-going work that is capable of detecting and correcting data errors in a database by integrating external information from the WWW. The goal of our research is to pursuit another effective way to enhance information quality.},
keywords={World Wide Web;Databases;Knowledge based systems;Computational fluid dynamics;Data mining;Web sites;Strain},
doi={10.1109/BIGCOMP.2016.7425923},
ISSN={2375-9356},
month={Jan},}
@ARTICLE{9806308,
author={Zeng, Tengchan and Semiariy, Omid and Chen, Mingzhe and Saad, Walid and Bennis, Mehdi},
journal={IEEE Transactions on Wireless Communications},
title={Federated Learning on the Road Autonomous Controller Design for Connected and Autonomous Vehicles},
year={2022},
volume={},
number={},
pages={1-1},
abstract={The deployment of future intelligent transportation systems is contingent upon seamless and reliable operation of connected and autonomous vehicles (CAVs). One key challenge in developing CAVs is the design of an autonomous controller that can accurately execute near real-time control decisions, such as a quick acceleration when merging to a highway and frequent speed changes in a stop-and-go traffic. However, the use of conventional feedback controllers or traditional learning-based controllers, solely trained by each CAVâ€™s local data, cannot guarantee a robust controller performance over a wide range of road conditions and traffic dynamics. In this paper, a new federated learning (FL) framework enabled by large-scale wireless connectivity is proposed for designing the autonomous controller of CAVs. In this framework, the learning models used by the controllers are collaboratively trained among a group of CAVs. To capture the varying CAV participation in the FL training process and the diverse local data quality among CAVs, a novel dynamic federated proximal (DFP) algorithm is proposed that accounts for the mobility of CAVs, the wireless fading channels, as well as the unbalanced and non-independent and identically distributed data across CAVs. A rigorous convergence analysis is performed for the proposed algorithm to identify how fast the CAVs converge to using the optimal autonomous controller. In particular, the impacts of varying CAV participation in the FL process and diverse CAV data quality on the convergence of the proposed DFP algorithm are explicitly analyzed. Leveraging this analysis, an incentive mechanism based on contract theory is designed to improve the FL convergence speed. Simulation results using real vehicular data traces show that the proposed DFP-based controller can accurately track the target CAV speed over time and under different traffic scenarios. Moreover, the results show that the proposed DFP algorithm has a much faster convergence compared to popular FL algorithms such as federated averaging (FedAvg) and federated proximal (FedProx). The results also validate the feasibility of the contract-theoretic incentive mechanism and show that the proposed mechanism can improve the convergence speed of the DFP algorithm by 40% compared to the baselines.},
keywords={Heuristic algorithms;Data integrity;Convergence;Roads;Wireless communication;Vehicle dynamics;Servers},
doi={10.1109/TWC.2022.3183996},
ISSN={1558-2248},
month={},}
@INPROCEEDINGS{8904039,
author={Wallis, Kevin and Schillinger, Fabian and Reich, Christoph and Schindelhauer, Christian},
booktitle={2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4)},
title={Safeguarding Data Integrity by Cluster-Based Data Validation Network},
year={2019},
volume={},
number={},
pages={78-86},
abstract={Ensuring data quality is central to the digital transformation in industry. Business processes such as predictive maintenance or condition monitoring can be implemented or improved based on the available data. In order to guarantee high data quality, a single data validation system are usually used to validate the production data for further use. However, using a single system allows an attacker only to perform one successful attack to corrupt the whole system. We present a new approach in which a data validation system using multiple different validators minimizes the probability of success for the attacker. The validators are arranged in clusters based on their properties. For a validation process, a challenge is given that specifies which validators should perform the current validation. Validation results from other validators are dropped. This ensures that even for more than half of the validators being corrupted anomalies can be detected during the validation process.},
keywords={Logic gates;Data integrity;Task analysis;Maintenance engineering;Blockchain;Industrial Internet of Things;Internet of Things;Data Validation;Cluster-Based Data Validation;Big Data},
doi={10.1109/WorldS4.2019.8904039},
ISSN={},
month={July},}
@INPROCEEDINGS{8419368,
author={Shi, Jingyi and Zheng, Mingna and Yao, Lixia and Ge, Yaorong},
booktitle={2018 IEEE International Conference on Healthcare Informatics (ICHI)},
title={A Publication-Based Popularity Index (PPI) for Healthcare Dataset Ranking},
year={2018},
volume={},
number={},
pages={247-254},
abstract={Data are critical in this age of big data and machine learning. Due to their inherent complexity, health-related data are unique in that the datasets are usually acquired for specific purposes and with special designs. As more and more healthcare datasets become available, of which many are public, choosing a quality dataset that is suitable for specific research inquiries is becoming a challenging question for health informatics researchers, especially the learners of this field. On the other hand, from the data provider's perspective, it is important to identify features of datasets that make some datasets more valuable than others so as to improve the design and acquisition of future datasets. To address these questions, we need to develop formal mechanisms to measure the goodness of datasets according to certain criteria. In this study, we propose one way of measuring the value of healthcare datasets that is based on how often the datasets are used and reported by researchers, which we call the Publication-based Popularity Index (PPI). In this article, we describe the design of the PPI and discuss its properties. We demonstrate the utility of the PPI by ranking 14 representative healthcare datasets. We believe that the PPI can enable an overall ranking of all healthcare datasets and thus provide an important dimension to sort search results for dataset integration systems as well as a starting point for identifying and examining the design of the most valuable healthcare datasets so that features of these datasets can inform future designs.},
keywords={Medical services;Market research;Indexes;Data integrity;Informatics;Histograms;Software;popularity index;healthcare dataset;data quality;quantified measurement;regression},
doi={10.1109/ICHI.2018.00035},
ISSN={2575-2634},
month={June},}
@INPROCEEDINGS{7396216,
author={Wiktorski, Tomasz and Hacker, Thomas and Hansen, Raymond A. and Rodgers, Gregory},
booktitle={2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)},
title={Experience with Problem-Based Learning in a Hybrid Classroom},
year={2015},
volume={},
number={},
pages={575-581},
abstract={Constructive alignment has been shown to elicit higher levels of learning among students. Problem-based Learning is one of the forms of constructive alignment often used in medicine and engineering education. We have applied a PBL-based approach to a graduate (master) course in Data Intensive Systems taught simultaneously through a video link at two universities in Europe and USA. Application of standardized measuring methodology shows inconclusive impact of PBL approach on students' learning. We present survey results and analyze major factors that could have lead to inconclusive result, including: low data quality, general applicability of constructivism in computer science, and issues with hybrid classroom and alternative laboratory environments. Finally, we discuss reversed classroom method as a potential solution to the issues encountered.},
keywords={Cloud computing;Computer science;Big data;Computers;Education;Virtual machining;problem-based learning;constructive alignment;data intensive systems;data science},
doi={10.1109/CloudCom.2015.70},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9351123,
author={Ouyang, Jianna and Liang, Shuo and Chen, Shaonan and Li, Shan and Zhou, Yangjun and Liwen, QIN},
booktitle={2020 IEEE Sustainable Power and Energy Conference (iSPEC)},
title={Design and Realization of Data Application Architecture Oriented to the Requirements of Distribution Network},
year={2020},
volume={},
number={},
pages={2354-2359},
abstract={In recent years, the rapid growth of all kinds of data and information in power grid has brought great challenges to the safe and stable operation and data analysis of the system. This paper constructs the data application architecture oriented to the requirements of distribution network based on the data requirements of reliability and economy evaluation, operation state evaluation and weak link identification, asset operation efficiency evaluation and lean management. It can realize the functions of data automatic classification storage, data processing, data quality monitoring and evaluation, multi-source heterogeneous data fusion and hierarchical classification database construction, etc. It supports the lean management of production business in distribution network comprehensively.},
keywords={Data integration;Systems architecture;Distribution networks;Production;Big Data applications;Reliability engineering;Business;distribution network;application requirements;data application architecture;design and realization},
doi={10.1109/iSPEC50848.2020.9351123},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6354737,
author={CurÃ©, Olivier and Kerdjoudj, Fadhela and Faye, David and Le Duc, Chan and Lamolle, Myriam},
booktitle={2012 Third International Conference on Emerging Intelligent Data and Web Technologies},
title={On the Potential Integration of an Ontology-Based Data Access Approach in NoSQL Stores},
year={2012},
volume={},
number={},
pages={166-173},
abstract={No SQL stores are emerging as an efficient alternative to relational database management systems in the context of big data. Many actors in this domain consider that to gain a wider adoption, several extensions have to be integrated. Some of them focus on the ways of proposing more schema, supporting adapted declarative query languages and providing integrity constraints in order to control data consistency and enhance data quality. We consider that these issues can be dealt with in the context of Ontology Based Data Access (OBDA). OBDA is a new data management paradigm that exploits the semantic knowledge represented in ontologies when querying data stored in a database. We provide a proof of concept of OBDA's ability to tackle these three issues in a social application related to the medical domain.},
keywords={Ontologies;Diseases;Indexes;Drugs;Semantics;Context;Ontology Based Data Access (OBDA);NoSQL;Document store;SPARQL;Social Application},
doi={10.1109/EIDWT.2012.27},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8847069,
author={Zhao, Peng and Quan, Dekui and Yu, Wei and Yang, Xinyu and Fu, Xinwen},
booktitle={2019 28th International Conference on Computer Communication and Networks (ICCCN)},
title={Towards Deep Learning-Based Detection Scheme with Raw ECG Signal for Wearable Telehealth Systems},
year={2019},
volume={},
number={},
pages={1-9},
abstract={The electrocardiogram (ECG) signal, as one of the most important vital signs, can provide indications of many heart-related diseases. Nonetheless, in the case of telehealth context, the automated analysis and accurate detection of ECG signals remain unsolved issues, because the poor data quality collected by the wearable devices and unprofessional users further increases the complexity of hand-crafted feature extraction, ultimately affecting the efficiency of feature extraction and the detection accuracy. To address this issue and improve the detection accuracy, in this paper we present a novel detection scheme with the raw ECG signal in wearable telehealth system. Our system benefits from the concept of big data, sensing and pervasive computing and the emerging deep learning technology. In particular, a Deep Heartbeat Classification (DHC) scheme is proposed to analyze the ECG signal for arrhythmia detection. Distinct from existing solutions, the detection model in DHC can be trained directly on the raw ECG signal without hand-crafted feature extraction. A cloud-based prototypical system is also designed and implemented with the functions of data acquisition, wireless transmission, back-end data management, and ECG detection. The experimental results demonstrate that our prototypical system is feasible and effective in real-world practice, and extensive experimentation based on the MIT-BIH database demonstrates that the proposed DHC scheme outperforms baseline schemes.},
keywords={Electrocardiography;Feature extraction;Biomedical monitoring;Sensors;Cloud computing;Computational modeling;Data analysis},
doi={10.1109/ICCCN.2019.8847069},
ISSN={2637-9430},
month={July},}
@INPROCEEDINGS{7336422,
author={Zhang, Jinkui and Zhu, Yongxin and Shi, Weiwei and Sheng, Gehao and Chen, Yufeng},
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems},
title={An Improved Machine Learning Scheme for Data-Driven Fault Diagnosis of Power Grid Equipment},
year={2015},
volume={},
number={},
pages={1737-1742},
abstract={In recent power grid systems, data-driven approach has been taken to grid condition evaluation and classification after successful adoption of big data techniques in internet applications. However, the raw training data from single monitoring system, e.g. dissolved gas analysis (DGA), are rarely sufficient for training in the form of valid instances and the data quality can rarely meet the requirement of precise data analytics since raw data set usually contains samples with noisy data. This paper proposes a machine learning scheme (PCA_IR) to improve the accuracy of fault diagnose, which combines dimension-increment procedure based on association analysis, dimension-reduction procedure based on principal component analysis and back propagation neural network (BPNN). First, the dimension of training data is increased by adding selected data which originates from different source such as production management system (PMS) to the original data obtained by DGA. The added data would also inevitably result in more noise. Thus, we then take advantage of the PCA method to reduce the noise in the training data as well as retaining significant information for classification. Finally, the new training data yielded after PCA procedure is inputted into BPNN for classification. We test the PCA_IR scheme on fault diagnosis of power transformers in power grid system. The experimental results show that the classifiers based on our scheme achieve higher accuracy than traditional ones. Therefore, the scheme PCA_IR would be successfully deployed for fault diagnosis in power grid system.},
keywords={Fault diagnosis;Principal component analysis;Accuracy;Power grids;Correlation;Power transformers;Correlation coefficient;transformer fault diagnosis;PCC;PCA;BPNN},
doi={10.1109/HPCC-CSS-ICESS.2015.236},
ISSN={},
month={Aug},}
@ARTICLE{9761878,
author={Yu, Han and Hanafy, Sherif M. and Liu, Lulu},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={A Weighted Closure-Phase Statics Correction Method: Synthetic and Field Data Examples},
year={2022},
volume={60},
number={},
pages={1-13},
abstract={Recorded seismograms are usually distorted by statics owing to complex geological conditions, such as lateral variations in sediment thickness or complex topographies. These distorted and discontinuous signals usually exist in either arrival times or amplitudes of waves, and they are most likely to be smeared as velocity perturbations along their associated raypaths. Therefore, statics may blur images of the target bodies or, even worse, introduce unexpected and false anomalies into subsurface structures. To partly resolve this problem, we develop a weighted statics correction method to estimate unwanted temporal shifts of traces using the closure-phase technique, which is utilized in astronomical imaging. In the proposed method, the source and receiver statics are regarded as independent quantities contributing to the waveform shifts based on their acquisition geometries. Numerical tests on both the synthetic and field cases show noticeable, although gradual, improvements in data quality compared to the conventional plusâ€“minus (PM) method. In general, this method provides a straightforward strategy to reedit the travel times in seismic profiles without inverting for a near-surface velocity model. Moreover, it can be extended to any interferometrical methods in seismic data processing that satisfies the closure-phase conditions.},
keywords={Receivers;Mathematical models;Surface treatment;Sea surface;Indexes;Earth;Computational modeling;Closure phase;first arrivals;interferometry;statics},
doi={10.1109/TGRS.2022.3169519},
ISSN={1558-0644},
month={},}
@ARTICLE{9729745,
author={Kazemi, Arefeh and Mozafari, Jamshid and Nematbakhsh, Mohammad Ali},
journal={IEEE Access},
title={PersianQuAD: The Native Question Answering Dataset for the Persian Language},
year={2022},
volume={10},
number={},
pages={26045-26057},
abstract={Developing Question Answering systems (QA) is one of the main goals in Artificial Intelligence. With the advent of Deep Learning (DL) techniques, QA systems have witnessed significant advances. Although DL performs very well on QA, it requires a considerable amount of annotated data for training. Many annotated datasets have been built for the QA task; most of them are exclusively in English. In order to address the need for a high-quality QA dataset in the Persian language, we present PersianQuAD, the native QA dataset for the Persian language. We create PersianQuAD in four steps: 1) Wikipedia article selection, 2) question-answer collection, 3) three-candidates test set preparation, and 4) Data Quality Monitoring. PersianQuAD consists of approximately 20,000 questions and answers made by native annotators on a set of Persian Wikipedia articles. The answer to each question is a segment of the corresponding article text. To better understand PersianQuAD and ensure its representativeness, we analyze PersianQuAD and show it contains questions of varying types and difficulties. We also present three versions of a deep learning-based QA system trained with PersianQuAD. Our best system achieves an F1 score of 82.97% which is comparable to that of QA systems on English SQuAD, made by the Stanford University. This shows that PersianQuAD performs well for training deep-learning-based QA systems. Human performance on PersianQuAD is significantly better (96.49%), demonstrating that PersianQuAD is challenging enough and there is still plenty of room for future improvement. PersianQuAD and all QA models implemented in this paper are freely available.},
keywords={Internet;Online services;Encyclopedias;Training;Task analysis;Machine translation;Buildings;Dataset;deep learning;natural language processing;Persian;question answering;machine reading comprehension},
doi={10.1109/ACCESS.2022.3157289},
ISSN={2169-3536},
month={},}
@BOOK{8187386,
author={Ilyas, Ihab F. and Chu, Xu},
booktitle={Trends in Cleaning Relational Data: Consistency and Deduplication},
year={2015},
volume={},
number={},
pages={},
abstract={Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions. According to a report by InsightSquared in 2012, poor data across businesses and the government cost the United States economy 3.1 trillion dollars a year. To detect data errors, data quality rules or integrity constraints (ICs) have been proposed as a declarative way to describe legal or correct data instances. Any subset of data that does not conform to the defined rules is considered erroneous, which is also referred to as a violation. Various kinds of data repairing techniques with different objectives have been introduced where algorithms are used to detect subsets of the data that violate the declared integrity constraints, and even to suggest updates to the database such that the new database instance conforms with these constraints. While some of these algorithms aim to minimally change the database, others involve human experts or knowledge bases to verify the repairs suggested by the automatic repeating algorithms. Trends in Cleaning Relational Data: Consistency and Deduplication discusses the main facets and directions in designing error detection and repairing techniques. It proposes a taxonomy of current anomaly detection techniques, including error types, the automation of the detection process, and error propagation. It also sets out a taxonomy of current data repairing techniques, including the repair target, the automation of the repair process, and the update model. It concludes by highlighting current trends in "big data" cleaning.},
keywords={Data Cleaning and Information Extraction;Data Integration and Exchange},
doi={10.1561/1900000045},
ISSN={},
publisher={now},
isbn={9781680830231},
url={https://ieeexplore.ieee.org/document/8187386},}
@INPROCEEDINGS{6863306,
author={McMorran, A. W. and Rudd, S. E. and Shand, C. M. and Simmins, J. J. and McCollough, N. and Stewart, E. M.},
booktitle={2014 IEEE PES T&D Conference and Exposition},
title={Data integration challenges for standards-compliant mobile applications},
year={2014},
volume={},
number={},
pages={1-5},
abstract={Modern mobile devices are capable of running sophisticated, network-enabled applications exploiting a variety of sensors on a single low-cost piece of hardware. The electrical industry can benefit from these new platforms to automate existing processes and provide engineers and field crew with access to large amounts of complex data in real-time, anywhere in the world. The development of a standards-based application decouples the mobile client application from a single vendor or existing enterprise system, but requires a complex data integration architecture to support the use and exploitation of large amounts of data spread across multiple existing systems. The integration with a mobile application introduces new challenges when dealing with remote devices where data network communications cannot be relied on, especially under storm conditions, and the devices themselves are at risk of being lost or stolen. Addressing these challenges offers the potential to improve data quality, enable access to accurate, up-to-date information in the field and ultimately save a utility time and money.},
keywords={Computer integrated manufacturing;Mobile communication;IEC standards;Logic gates;Servers;Data models;Asset management;Application virtualization;Virtual reality;Visualization;Standards;Data handling;Data visualization;CIM;Data integration;Big Data},
doi={10.1109/TDC.2014.6863306},
ISSN={2160-8563},
month={April},}
@INPROCEEDINGS{8400228,
author={PeÅ‚ech-Pilichowski, T.},
booktitle={2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)},
title={On adaptive prediction of nonstationary and inconsistent large time series data},
year={2018},
volume={},
number={},
pages={1260-1265},
abstract={The use of time series prediction results in benefits for an organization. Forecasting efficiency relies on applied prediction formula and quality of data received from technical devices and manually inputted. They are often of low quality, with inconsistencies. However, high data quality is crucial for efficient forecasting/prediction purposes (also event detection from time series and pattern recognition), in particular during large data sets processing (often heterogeneous, including data obtained from IoT devices). Such processing should cover inconsistency analysis, interpolation of missing/lacking data, as well as the use of data pre-transformations. The paper presents problems of inconsistent, nonstationary data prediction on the example of stock level daily forecasting. Selected methods of time series interpolation are outlined. Results of implementation of algorithms for short-term time series prediction are illustrated and discussed. Prediction quality measured based on errors values calculated both in total and in a moving window is discussed. A concept of an adaptive algorithm based on a change in the prognostic formula depending on short-term characteristics of time series is outlined.},
keywords={Time series analysis;Interpolation;Predictive models;Forecasting;Prediction algorithms;Extrapolation;Adaptation models;time series analysis;prediction;forecasting;interpolation;adaptive prediction algorithms;Big Data;IoT},
doi={10.23919/MIPRO.2018.8400228},
ISSN={},
month={May},}
@INPROCEEDINGS{9677226,
author={Qasim, Amer and El Refae, Ghaleb A. and Issa, Hussein and Eletter, Shorouq},
booktitle={2021 22nd International Arab Conference on Information Technology (ACIT)},
title={The Impact of Drone Technology on The Accounting Profession: The Case of Revenue Recognition in Long-Term Construction Contracts},
year={2021},
volume={},
number={},
pages={1-4},
abstract={The accounting profession has gone through radical changes due to recent technological advancements in AI, blockchain technologies, big data, etc. More recently, the accounting literature discussed the possibility of using the drone innovative technology in conducting inventory observation as well as internal and external auditing. This study is a visionary paper which investigates the applicability of a remotely auditing process using drone technology in real-estate accounting. The drone will be used to conduct site inspection to assess and monitor the construction progress through applying the percentage of completion method to recognize revenues from long-term contracts. This innovative technology has the ability to collect better data quality, saving cost and saving time with improved site safety which will help improve the auditor tasks.},
keywords={Costs;Data integrity;Inspection;Safety;Blockchains;Task analysis;Information technology;Drones;Artificial Intelligence;Real-Estate Accounting;ERP systems;Revenue Recognition},
doi={10.1109/ACIT53391.2021.9677226},
ISSN={},
month={Dec},}
@INBOOK{9821411,
author={Ahlemeyer-Stubbe, Andrea and Coleman, Shirley},
booktitle={Monetizing Data: How to Uplift Your Business},
title={About Data and Data Science},
year={2018},
volume={},
number={},
pages={9-28},
abstract={This chapter deals with aspects of data that are relevant to the practitioner wishing to apply data analytics to monetise data. It reviews the types of data that are available and how they are accessed. The chapter considers the fastâ€growing big data from internet exchanges and the attendant quality and storage issues, and which employees are best placed to maximise the value added from the data. It also then considers the slower buildâ€up of transactional data from small traders and experiments on consumer behaviour. The chapter defines scales of measurement and terms commonly used to distinguish different types of data, the meaning and necessity of data quality, amounts of data and its storage, the skills needed for different data functions, and data readiness and how to assess where a company is on the cycle of data improvement.},
keywords={Companies;Business;Process control;Graphics;Soft sensors;Social networking (online);Reliability},
doi={10.1002/9781119125167.ch2},
ISSN={},
publisher={Wiley},
isbn={9781119125143},
url={https://ieeexplore.ieee.org/document/9821411},}
@ARTICLE{8704713,
author={Ahmad, Arshad and Feng, Chong and Li, Kan and Asim, Syed Mohammad and Sun, Tingting},
journal={IEEE Access},
title={Toward Empirically Investigating Non-Functional Requirements of iOS Developers on Stack Overflow},
year={2019},
volume={7},
number={},
pages={61145-61169},
abstract={Context: Mobile application developers are getting more concerned due to the importance of quality requirements or non-functional requirements (NFR) in software quality. Developers around the globe are actively asking a question(s) and sharing solutions to the problems related to software development on Stack Overflow (SO). The knowledge shared by developers on SO contains useful information related to software development such as feature requests (functional/non-functional), code snippets, reporting bugs or sentiments. Extracting the NFRs shared by iOS developers on programming Q&A website SO has become a challenge and a less researched area. Objective: To identify and understand the real problems, needs, trends, and the critical NFRs or quality requirements discussed on Stack Overflow related to iOS mobile application development. Method: We extracted and used only the iOS posts data of SO. We applied the well-known statistical topical model Latent Dirichlet Allocation (LDA) to identify the main topics in iOS posts on SO. Then, we labeled the extracted topics with quality requirements or NFRs by using the wordlists to assess the trend, evolution, hot and unresolved NFRs in all iOS discussions. Results: Our findings revealed that the highly frequent topics the iOS developers discussed are related to usability, reliability, and functionality followed by efficiency. Interestingly, the most problematic areas unresolved are also usability, reliability, and functionality though followed by portability. Besides, the evolution trend of each of the six different quality requirements or NFRs over time is depicted through comprehensive visualization. Conclusion: Our first empirical investigation on approximately 1.5 million iOS posts and comments of SO gives insight on comprehending the NFRs in iOS application development through the lens of real-world practitioners.},
keywords={Software;Market research;Data mining;Application programming interfaces;Mobile communication;Mobile applications;Technological innovation;Non-functional requirements (NFRs);quality requirements;iOS;Latent Dirichlet allocation (LDA);Stack Overflow},
doi={10.1109/ACCESS.2019.2914429},
ISSN={2169-3536},
month={},}
@ARTICLE{9267091,
author={Yuan, Yanhua O and BozdaÄŸ, Ebru and Ciardelli, Caio and Gao, Fuchun and Simons, F J},
journal={Geophysical Journal International},
title={The exponentiated phase measurement, and objective-function hybridization for adjoint waveform tomography},
year={2019},
volume={221},
number={1},
pages={1145-1164},
abstract={Seismic tomography has arrived at the threshold of the era of big data. However, how to extract information optimally from every available time-series remains a challenge; one that is directly related to the objective function chosen as a distance metric between observed and synthetic data. Time-domain cross-correlation and frequency-dependent multitaper traveltime measurements are generally tied to window selection algorithms in order to balance the amplitude differences between seismic phases. Even then, such measurements naturally favour the dominant signals within the chosen windows. Hence, it is difficult to select all usable portions of seismograms with any sort of optimality. As a consequence, information ends up being lost, in particular from scattered waves. In contrast, measurements based on instantaneous phase allow extracting information uniformly over the seismic records without requiring their segmentation. And yet, measuring instantaneous phase, like any other phase measurement, is impeded by phase wrapping. In this paper, we address this limitation by using a complex-valued phase representation that we call â€˜exponentiated phaseâ€™. We demonstrate that the exponentiated phase is a good substitute for instantaneous-phase measurements. To assimilate as much information as possible from every seismogram while tackling the non-linearity of inversion problems, we discuss a flexible hybrid approach to combine various objective functions in adjoint seismic tomography. We focus on those based on the exponentiated phase, to take into account relatively small-magnitude scattered waves; on multitaper measurements of selected surface waves; and on cross-correlation measurements on specific windows to select distinct body-wave arrivals. Guided by synthetic experiments, we discuss how exponentiated-phase, multitaper and cross-correlation measurements, and their hybridization, affect tomographic results. Despite their use of multiple measurements, the computational cost to evaluate gradient kernels for the objective functions is scarcely affected, allowing for issues with data quality and measurement challenges to be simultaneously addressed efficiently.},
keywords={Inverse theory;Time-series analysis;Seismic tomography},
doi={10.1093/gji/ggaa063},
ISSN={1365-246X},
month={Dec},}
@INPROCEEDINGS{8278765,
author={Ruester, Christian and Haussel, Fabian and Huehn, Thomas and El Sayed, Nadim},
booktitle={International ETG Congress 2017},
title={VEREDELE-FACDS Field Trial: Wide Area Power Quality Assessment With IOT Sensors and Cloud-Based Analytics},
year={2017},
volume={},
number={},
pages={1-5},
abstract={With the increasing share of renewable generation in low voltage distribution, the edge of the power grid is slowly replacing classical, large-scale power stations, taking up roles and responsibilities that were unimaginable only a few years ago. However, while feed-in on the 400V level is now commonplace, grid state and power quality monitoring still lag far behind because of obvious cost and complexity reasons. In fact, only very few parts of Germanys approx. 1,1 million km long 400V distribution grid are actively monitored today and even basic grid quality parameters such as the voltage level at the end of the line are typically unknown. Reducing the cost per measurement point and the complexity of data analysis is thus of paramount importance for enabling wide-area monitoring of the LV power grid. The authors explore the feasibility of this goal by examining the performance of a nonconventional measurement system. It consists of a network of Internet-of-Things (IOT)-based power quality sensors, connected to a cloud-based big data analysis platform. Specifically, measurement nodes comprise of voltage sensors attached to consumer-grade smartphones and WIFI access points. Sensor data is automatically uploaded to the cloud system with the MQTT protocol. First results from a field trial in a rural area in Germany indicate good data quality and show excellent promise for detailed assessments of the edge of the power grid.},
keywords={},
doi={},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7577612,
author={Xue, Chunlu and Guo, Lin and Hu, Hualang and Pei, Zhiyuan},
booktitle={2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics)},
title={Management and spatial evolution of rural land circulation: Taking Zhengjia Town as an example},
year={2016},
volume={},
number={},
pages={1-6},
abstract={The scale operation in various forms of rural land circulation in China is the way for the development of modern agriculture, and is also the basic direction of agricultural reform. The right to rural land contractual management registration makes the position of cadastral land and its interrelated information, as contractor and area, clear by Geographic Information Systems (GIS). That supplied a data base for land circulation spatial management. In this paper, we discussed the correlation between the management and the spatial position of transferred rural land, in order to support the policy and decision for agriculture. Taking Zhengjia Town as an example, which located in the west of Dongchangfu District, Liaocheng, Shandong, we made a survey and got the information about the land circulation, the operators who engaged in the transferred land, and the farmers who had transferred their farmlands from 2011 to 2015. Based on the outcome data of right to rural land contractual management registration, the circulation parcels and the cadastral parcels were linked by their only parcel code, and then formed the land circulation spatial information. Using ArcGIS 10.1 for spatial analysis and correlation analysis, we analyzed the correlation between the management and the spatial evolution of the transferred land on some indicators including acquirement intention, operation style, land use, circulation variability, distribution of circulation management right, and compared the income of the operators and the farmers. According to the survey, the scope of the transferred land was gradually increased. The area was 16.09ha in 2011 and 147.27ha in 2015, 19.22% of the contracted land. The results show that the main form of land circulation was to rent, and scale operation in northern and fragmented operation in southern. The transferred lands which were acquired by family contract have the largest area. However, in the south, the farmers seemed like to transfer their lands which were acquired by bidding. The operators aged in 40-49 managed the most transferred land, accounting for 77.00% of the total circulation area. Followed by 39 years old under, and the over 50 years old was the least. Before transferred, all farmland was used to grow grain. But 93% farmlands were changed the crops after land circulation. As the investment of time, capital, farmland and implied labor increased, there was a certain increase in income of the operators and the farmers, however, and a few operators had no profit because of the business model. There was no spatial correlation between the farmlands position and the farmers' income at the fragmented operation region. The results can provide an idea for spatial information management of land circulation based on GIS, and defend the farmers' contractual rights when the boundaries are destroyed. With the spatial informatization of farmland and the improvement of data quality and quantity, big data will further predict land circulation spatial arrangement and business model.},
keywords={Correlation;Agriculture;Vegetation;Geographic information systems;Economics;Information management;GIS;land circulation;right registration;spatial evolution},
doi={10.1109/Agro-Geoinformatics.2016.7577612},
ISSN={},
month={July},}
@ARTICLE{9382202,
author={},
journal={IEEE Std 3652.1-2020},
title={IEEE Guide for Architectural Framework and Application of Federated Machine Learning},
year={2021},
volume={},
number={},
pages={1-69},
abstract={Federated machine learning defines a machine learning framework that allows a collective model to be constructed from data that is distributed across repositories owned by different organizations or devices. A blueprint for data usage and model building across organizations and devices while meeting applicable privacy, security and regulatory requirements is provided in this guide. It defines the architectural framework and application guidelines for federated machine learning, including description and definition of federated machine learning; the categories federated machine learning and the application scenarios to which each category applies; performance evaluation of federated machine learning; and associated regulatory requirements.},
keywords={IEEE Standards;Machine learning;Privacy;Modeling;Economics;Collaborative work;Metasearch;computation efficiency;economic viability;federated machine learning (FML);IEEE 3652.1â„¢;incentive mechanism;machine learning;model performance;privacy;privacy regulations;security},
doi={10.1109/IEEESTD.2021.9382202},
ISSN={},
month={March},}
@INPROCEEDINGS{7944912,
author={},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Big Data Quality Assurance Workshop Chairs and Committee},
year={2017},
volume={},
number={},
pages={xvii-xvii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/BigDataService.2017.57},
ISSN={},
month={April},}
@INPROCEEDINGS{7133983,
author={Nicklas, Daniela},
booktitle={2015 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)},
title={Keynote: Context, big data, and digital prejudices},
year={2015},
volume={},
number={},
pages={1-1},
abstract={In pervasive computing research and literature, context has mostly been seen as an information source for applications that adapt their behavior according to the current situation of their user or their (often physical) environment. This adaptation could be the change of the user interface, the performance of actions (like sending messages or triggering actuators), or the change of used resources (like network bandwidth or processing power). To determine relevant situations, many heterogeneous data sources could be used, ranging from sensor data over mined patterns in files to explicit user input. Since most sensors are not perfect, context quality has to be considered. And since many context-aware applications are mobile, the set of data sources may change during runtime. According to the widely used definition by Anind Dey, context can be â€œany information that can be used to characterize the situation of an entityâ€. In the past years, we have seen a significant increase in the so-called â€œbig dataâ€ domain, in research, technology, and industrial usage. The desire to analyze, gain knowledge and use more and more data it in new ways is rising in a way that resemble a gold rush. Data is the new oil. Beside applications like predictive maintenance of machines or optimization of industrial processes, a main target for big data analyses are humans - in their roles as travelers, current or potential clients, or application users. We could say that big data is â€œany information that can be used to characterize the situation of a userâ€, and relate these approaches to what have been done in context modelling and reasoning. This gets even clearer when these analyses leave the virtual world (e.g., client behavior in web shops) and enter the real world (e.g., client behavior in retail). In addition to the ambiguities of the analysis itself that only leads to predictions with a limited probability, sensor data quality becomes an issue: the sensor data might be inaccurate, outdated or conflicting with other observations or physical laws; in addition, sensor data processing algorithms like object classification or tracking might lead to ambiguous results. In this talk, we will shortly review these two domains and derive what could be learned for context-aware applications. A special focus will be given on quality of context on all semantic levels, and how the improper consideration of quality issues can lead to dangerous digital prejudices.},
keywords={context;big data},
doi={10.1109/PERCOMW.2015.7133983},
ISSN={},
month={March},}
@ARTICLE{8913513,
author={Garg, S. and Guizani, M. and Guo, S. and Verikoukis, C.},
journal={IEEE Transactions on Industrial Informatics},
title={Guest Editorial Special Section on AI-Driven Developments in 5G-Envisioned Industrial Automation: Big Data Perspective},
year={2020},
volume={16},
number={2},
pages={1291-1295},
abstract={The papers in this special section examine artificial intelligence (AI)-driven developments in 5G mobile communications for industrial automation applications from a Big Data perspective. With the recent advances in information and communication technologies, industrial automation is expanding at a rapid pace. This transition is characterized by â€œIndustry 4.0â€, the fourth revolution in the field of manufacturing. Industry 4.0, also called as â€œIndustrial Internet of Things (IIoT)â€ or â€œSmart Factoriesâ€, is a reflection of new industrial revolution that is not only interconnected, but also communicate, analyze, and use the information to create a more holistic and better connected ecosystem for the industries.},
keywords={Special issues and sections;Artificial intelligence;Big Data;Quality of service;Automation;5G mobile communication},
doi={10.1109/TII.2019.2955963},
ISSN={1941-0050},
month={Feb},}
@INPROCEEDINGS{7407050,
author={Xiaofang Zhou},
booktitle={2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI)},
title={Keynote speech V deriving values from spatial trajectories},
year={2015},
volume={},
number={},
pages={29-29},
abstract={Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. How do we manage them? What values can a business derive from them, and how? Due to their very large volumes, the nature of streaming itself, highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data is one of the crucial areas for big data analytics. In this talk, we will introduce this increasingly important research area in the context of new applications, new problems and new opportunities. We will discuss recent advances in trajectory data management and trajectory mining, from their foundations to high performance processing with modern computing infrastructures.},
keywords={},
doi={10.1109/TAAI.2015.7407050},
ISSN={2376-6824},
month={Nov},}
@INPROCEEDINGS{7207185,
author={},
booktitle={2015 IEEE International Congress on Big Data},
title={[Title page i]},
year={2015},
volume={},
number={},
pages={i-i},
abstract={The following topics are dealt with: data mining; social network; data privacy; learning; query processing; big data processing; big data quality; big data platform; big data semantics; health care; network management; distributed processing; social media; and image processing.},
keywords={},
doi={10.1109/BigDataCongress.2015.1},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7944901,
author={},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)},
title={[Front cover]},
year={2017},
volume={},
number={},
pages={c1-c1},
abstract={The following topics are dealt with: intelligent data mining; frameworks for Big Data processing; Big Data processing and mining; Big Data analysis; smart city Big Data; data analytics and visualization; Big Data applications; Big Data framework, technology and solutions; security services for smart cities; Big Data for security; Big Data quality assurance and validation; and quality assurance and validation for Big Data-based applications.},
keywords={},
doi={10.1109/BigDataService.2017.59},
ISSN={},
month={April},}
@INPROCEEDINGS{7858181,
author={},
booktitle={2016 International Conference on Information Technology Systems and Innovation (ICITSI)},
title={[Title page]},
year={2016},
volume={},
number={},
pages={1-1},
abstract={The following topics are dealt with: SDLC SPASI v. 4.0. business process; information extraction; statistics indicator tables; rule generalizations; ontology; conventional learning system; ICT-based learning; job training system; time-series data; RAID; software-based accelerator; virtualization environment; enterprise architecture government organization; TOGAF ADM; SONA; e- library; modified quantitative models for performance measurement system method; business process improvement; district government innovation service case study; government organization; m-government implementation evaluation; trusted Big Data; official statistics study case; data profiling; data quality improvement; secure internet access; copyright protection; color images; transform domain; luminance component; information network architecture; local government; software as a service; expert system; meningitis disease; certainty factor method; digital asset management system; broadcasting organizations; e-portofolio definition; system security requirement identification; electronic payment system; Internet-based long distance education; operational model data governance; requirement engineering; open government information network development; process capability assessment; information security management; information security governance; national cyber physical systems; e-learning readiness; remote control system; serial communications mobile; microcontroller; knowledge sharing; indonesia higher educational institutions; cultural heritage metadata; geo linked open data; NUSANTARA: knowledge management system; adaptive personalized learning system; interactive learning media; RPP ICT; government human capital management; knowledge management tools utilization; knowledge management readiness; analytic hierarchy process; government institutions; usability testing; scrum methodology; assistant information system; automatic arowana raiser; pSPEA2; strength Pareto evolutionary algorithm 2; early diagnosis expert system deficiency; digital forensic; user acceptance; human resource information system; automated plasmodium detection; malaria diagnosis; thin blood smear image; 3D virtual game; MOODLE; SLOODLE; open simulator case study; color-based segmentation; feature detection; ball post; goal post; mobile soccer robot game field; smart farming; real time q-log-based feature normalization; distant speech recognition; Monte Carlo localization; robot operating system; finite element method; 3D DC resistivity modeling; multi GPU; breast cancer lesions; adaptive thresholding; morphological operation; gamification framework; online training; collaborative working system; classification breast cancer ultrasound images; posterior features; three-wheeled omnidirectional robot controller; public services satisfaction; sentiment analysis; color blind test quantification; RGB primary color cluster; ERP modules requirement; micro, small and medium enterprise fashion industry; small culinary enterprises; business system requirement; small craft companies ; power analysis attack; DES and IT value model.},
keywords={},
doi={10.1109/ICITSI.2016.7858181},
ISSN={},
month={Oct},}
