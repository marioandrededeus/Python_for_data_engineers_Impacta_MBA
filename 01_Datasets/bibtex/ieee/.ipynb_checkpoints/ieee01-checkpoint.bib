@INPROCEEDINGS{8029366,
author={Taleb, Ikbal and Serhani, Mohamed Adel},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Big Data Pre-Processing: Closing the Data Quality Enforcement Loop},
year={2017},
volume={},
number={},
pages={498-501},
abstract={In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.},
keywords={Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing},
doi={10.1109/BigDataCongress.2017.73},
ISSN={},
month={June},}
@INPROCEEDINGS{8332632,
author={Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun},
booktitle={2017 14th Web Information Systems and Applications Conference (WISA)},
title={A Big Data Framework for Electric Power Data Quality Assessment},
year={2017},
volume={},
number={},
pages={289-292},
abstract={Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.},
keywords={Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework},
doi={10.1109/WISA.2017.29},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8686099,
author={Abdallah, Mohammad},
booktitle={2019 International Conference on Big Data and Computational Intelligence (ICBDCI)},
title={Big Data Quality Challenges},
year={2019},
volume={},
number={},
pages={1-3},
abstract={Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.},
keywords={Big Data;Quality Measurement;Quality Model;Quality Assurance},
doi={10.1109/ICBDCI.2019.8686099},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9245455,
author={Loetpipatwanich, Sakda and Vichitthamaros, Preecha},
booktitle={2020 1st International Conference on Big Data Analytics and Practices (IBDAP)},
title={Sakdas: A Python Package for Data Profiling and Data Quality Auditing},
year={2020},
volume={},
number={},
pages={1-4},
abstract={Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.},
keywords={Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline},
doi={10.1109/IBDAP50342.2020.9245455},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7816918,
author={Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik},
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)},
title={Big Data Quality: A Quality Dimensions Evaluation},
year={2016},
volume={},
number={},
pages={759-765},
abstract={Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.},
keywords={Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122},
ISSN={},
month={July},}
@INPROCEEDINGS{7364064,
author={Becker, David and King, Trish Dunn and McMullen, Bill},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Big data, big data quality problem},
year={2015},
volume={},
number={},
pages={2644-2653},
abstract={A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the "truth about Big Data" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.},
keywords={Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale},
doi={10.1109/BigData.2015.7364064},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7840595,
author={Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Antecedents of big data quality: An empirical examination in financial service organizations},
year={2016},
volume={},
number={},
pages={116-121},
abstract={Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.},
keywords={Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance},
doi={10.1109/BigData.2016.7840595},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8605945,
author={Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2018 International Conference on Innovations in Information Technology (IIT)},
title={Big Data Quality Assessment Model for Unstructured Data},
year={2018},
volume={},
number={},
pages={69-74},
abstract={Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.},
keywords={Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data},
doi={10.1109/INNOVATIONS.2018.8605945},
ISSN={2325-5498},
month={Nov},}
@INPROCEEDINGS{9260067,
author={Wong, Ka Yee. and Wong, Raymond K.},
booktitle={2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)},
title={Big Data Quality Prediction on Banking Applications: Extended Abstract},
year={2020},
volume={},
number={},
pages={791-792},
abstract={Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.},
keywords={Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning},
doi={10.1109/DSAA49011.2020.00119},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7364065,
author={Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Data quality issues in big data},
year={2015},
volume={},
number={},
pages={2654-2660},
abstract={Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.},
keywords={Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality},
doi={10.1109/BigData.2015.7364065},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9006294,
author={Arruda, Darlan and Madhavji, Nazim H.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications},
year={2019},
volume={},
number={},
pages={5977-5979},
abstract={The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.},
keywords={Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool},
doi={10.1109/BigData47090.2019.9006294},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8862267,
author={Juneja, Ashish and Das, Nripendra Narayan},
booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)},
title={Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application},
year={2019},
volume={},
number={},
pages={559-563},
abstract={Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.},
keywords={Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing},
doi={10.1109/COMITCon.2019.8862267},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7374131,
author={Juddoo, Suraj},
booktitle={2015 International Conference on Computing, Communication and Security (ICCCS)},
title={Overview of data quality challenges in the context of Big Data},
year={2015},
volume={},
number={},
pages={1-9},
abstract={Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.},
keywords={Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics},
doi={10.1109/CCCS.2015.7374131},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7473058,
author={Gao, Jerry and Xie, Chunli and Tao, Chuanqi},
booktitle={2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)},
title={Big Data Validation and Quality Assurance -- Issuses, Challenges, and Needs},
year={2016},
volume={},
number={},
pages={433-441},
abstract={With the fast advance of big data technology and analytics solutions, big data computing and service is becoming a very hot research and application subject in academic research, industry community, and government services. Nevertheless, there are increasing data quality problems resulting in erroneous data costs in enterprises and businesses. Current research seldom discusses how to effectively validate big data to ensure data quality. This paper provides informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, the paper presents a comparison among big data validation tools and several major players in industry are discussed. Furthermore, the primary issues, challenges, and needs are discussed.},
keywords={Big data;Quality assurance;Organizations;Q-factor;Standards organizations;Quality assurance;big data quality assurance;big data validation;data validation},
doi={10.1109/SOSE.2016.63},
ISSN={},
month={March},}
@INPROCEEDINGS{8397554,
author={Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin},
booktitle={2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={Data quality in big data processing: Issues, solutions and open problems},
year={2017},
volume={},
number={},
pages={1-7},
abstract={With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.},
keywords={Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system},
doi={10.1109/UIC-ATC.2017.8397554},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8465129,
author={Juddoo, Suraj and George, Carlisle},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)},
title={Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.},
keywords={Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis},
doi={10.1109/ICABCD.2018.8465129},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6755349,
author={Freitas, Patrícia Alves de and Reis, Everson Andrade dos and Michel, Wanderson Senra and Gronovicz, Mauro Edson and Rodrigues, Márcio Alexandre de Macedo},
booktitle={2013 IEEE 16th International Conference on Computational Science and Engineering},
title={Information Governance, Big Data and Data Quality},
year={2013},
volume={},
number={},
pages={1142-1143},
abstract={The value of information as a competitive differential has been taken into consideration in companies all over the world for some time already. In recent years, there has been heated debate about some terms originated from new concepts related to information, such as big data, due to the promise that such topic might revolutionise world trade. Hence, data and information governance and quality have been increasingly discussed in the business world.},
keywords={Companies;Information management;Data handling;Data storage systems;Computer architecture;Information Governance;Big Data;Data Quality},
doi={10.1109/CSE.2013.168},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8386521,
author={Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai},
booktitle={2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory},
year={2018},
volume={},
number={},
pages={248-252},
abstract={Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.},
keywords={Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment},
doi={10.1109/ICCCBDA.2018.8386521},
ISSN={},
month={April},}
@INPROCEEDINGS{9323615,
author={Han, Weiguo and Jochum, Matthew},
booktitle={IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium},
title={A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System},
year={2020},
volume={},
number={},
pages={3101-3103},
abstract={In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.},
keywords={Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest},
doi={10.1109/IGARSS39084.2020.9323615},
ISSN={2153-7003},
month={Sep.},}
@INPROCEEDINGS{7498367,
author={Sadiq, Shazia and Papotti, Paolo},
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)},
title={Big data quality - whose problem is it?},
year={2016},
volume={},
number={},
pages={1446-1447},
abstract={The increased reliance on data driven enterprise has seen an unprecedented investment in big data initiatives. Organizations averaged US$8M in investments in big data-related initiatives and programs in 2014, with 70% of large enterprises and 56% of small and medium enterprises (SMEs) having already deployed, or planning to deploy, big-data projects [1]. As companies intensify their efforts to get value from big data, the growth in the amount of data being managed continues at an exponential rate, leaving organizations with a massive footprint of unexplored, unfamiliar datasets. On February 8th, 2015, a group of global thought leaders from the database research community outlined the grand challenges in getting value from big data [2]. The key message was the need to develop the capacity to `understand how the quality of data affects the quality of the insight we derive from it'.},
keywords={Big data;Cleaning;Computer science;Databases;Investment},
doi={10.1109/ICDE.2016.7498367},
ISSN={},
month={May},}
@INPROCEEDINGS{8457745,
author={Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2018 IEEE International Congress on Big Data (BigData Congress)},
title={Big Data Quality: A Survey},
year={2018},
volume={},
number={},
pages={166-173},
abstract={With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.},
keywords={Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data},
doi={10.1109/BigDataCongress.2018.00029},
ISSN={},
month={July},}
@INPROCEEDINGS{9529590,
author={Bhardwaj, Dave and Ormandjieva, Olga},
booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)},
title={Toward a Novel Measurement Framework for Big Data (MEGA)},
year={2021},
volume={},
number={},
pages={1579-1586},
abstract={Big Data is quickly becoming a chief part of the decision-making process in both industry and academia. As more and more institutions begin relying on Big Data to make strategic decisions, the quality of the underlying data comes into question. The quality of Big Data isn’t always transparent and large-scale systems may even lack its visibility, which adversely affects the credibility of the Big Data systems. Continuous monitoring and measurement of data quality is therefore paramount in assessing whether the information can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses the need for Big Data quality measurement modeling and automation by proposing a novel conceptual quality measurement framework for Big Data (MEGA) with the purpose of assessing the underlying quality characteristics of Big Data (also known as the V’s of Big Data) at each step of the Big Data Pipelines. The theoretical quality measurement models for four of the Big Data V’s (Volume, Variety, Velocity, Veracity) are currently automated; the remaining 6 V’s (Vincularity, Validity, Value, Volatility, Valence and Vitality) will be tackled in our future work. The approach is illustrated on a case study.},
keywords={Solid modeling;Data integrity;Volume measurement;Pipelines;Big Data;Data models;Software;Big Data quality characteristics;The V’s;Big Data Quality Measurement Framework;Big Data Pipelines},
doi={10.1109/COMPSAC51774.2021.00235},
ISSN={0730-3157},
month={July},}
@INPROCEEDINGS{7944952,
author={Xie, Chunli and Gao, Jerry and Tao, Chuanqi},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Big Data Validation Case Study},
year={2017},
volume={},
number={},
pages={281-286},
abstract={With the advent of big data, data is being generated, collected, transformed, processed and analyzed at an unprecedented scale. Since data is created at a fast velocity and with a large variety, the quality of big data is far from perfect. Recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis. Data validation is an important process to recognize and improve data quality. In this paper, a case study that is relevant to big data quality is designed to study original big data quality, data quality dimension, data validation process and tools.},
keywords={Big Data;Tools;Databases;Electronic mail;Reliability;Temperature measurement;Null value;big data quality;big data validation;data checklist;big data tool;case study},
doi={10.1109/BigDataService.2017.44},
ISSN={},
month={April},}
@ARTICLE{8935096,
author={Li, Mingda and Wang, Hongzhi and Li, Jianzhong},
journal={Big Data Mining and Analytics},
title={Mining conditional functional dependency rules on big data},
year={2020},
volume={3},
number={1},
pages={68-84},
abstract={Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.},
keywords={Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality},
doi={10.26599/BDMA.2019.9020019},
ISSN={2096-0654},
month={March},}
@INPROCEEDINGS{8078796,
author={HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Some key problems of data management in army data engineering based on big data},
year={2017},
volume={},
number={},
pages={149-152},
abstract={This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.},
keywords={Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality},
doi={10.1109/ICBDA.2017.8078796},
ISSN={},
month={March},}
@INPROCEEDINGS{8258267,
author={Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={My (fair) big data},
year={2017},
volume={},
number={},
pages={2974-2979},
abstract={Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.},
keywords={Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality},
doi={10.1109/BigData.2017.8258267},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8596389,
author={Ezzine, Imane and Benhlima, Laila},
booktitle={2018 IEEE 5th International Congress on Information Science and Technology (CiSt)},
title={A Study of Handling Missing Data Methods for Big Data},
year={2018},
volume={},
number={},
pages={498-501},
abstract={Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.},
keywords={Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning},
doi={10.1109/CIST.2018.8596389},
ISSN={2327-1884},
month={Oct},}
@INPROCEEDINGS{7384166,
author={Chenran, Xiong and Youde, Wu},
booktitle={2015 International Conference on Intelligent Transportation, Big Data and Smart City},
title={The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology},
year={2015},
volume={},
number={},
pages={869-872},
abstract={This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.},
keywords={Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment},
doi={10.1109/ICITBS.2015.220},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8029373,
author={Zhou, Lixiao and Huang, Maohai},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Challenges of Software Testing for Astronomical Big Data},
year={2017},
volume={},
number={},
pages={529-532},
abstract={Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.},
keywords={Big Data;Software testing;Software;Astronomy;Data analysis;Distributed databases;software testing; big data; astronomical software},
doi={10.1109/BigDataCongress.2017.91},
ISSN={},
month={June},}
@INPROCEEDINGS{9678209,
author={Yalaoui, Mehdi and Boukhedouma, Saida},
booktitle={2021 International Conference on Information Systems and Advanced Technologies (ICISAT)},
title={A survey on data quality: principles, taxonomies and comparison of approaches},
year={2021},
volume={},
number={},
pages={1-9},
abstract={Nowadays, data generation keeps increasing exponentially due to the emergence of the Internet of Things (IoT) and Big data technologies. The manipulation of such Big amount of data becomes more and more difficult because of its size and its variety. For better governance of organizations (decision making, data analysis, earnings increase …), data quality and data governance at present of Big data are two major pillars for the design of any system handling data within the organization. This explains the number of researches conducted as it constitutes a research subject with several gaps and opportunities. Many works were conducted to define and standardize Data Quality (DQ) and its dimensions, others were directed to design and propose data quality assessment and improvement models or frameworks. This work aims to recall the data quality principles starting by the needed background knowledge, then identify and compare the relevant taxonomies existing in the literature, next surveys and compares the available Data quality assessment and improvement approaches. After that, we propose a metamodel highlighting the main concepts of DQ assessment and we describe a generic process for DQ assessment and improvement. Finally, we evoke the main challenges in the field of DQ before and after the emergence of Big Data.},
keywords={Data integrity;Taxonomy;Standards organizations;Decision making;Organizations;Big Data;Data models;Data Quality;Big Data;Quality Dimensions;Quality Metrics;Metamodel;Assessment process;Improvement},
doi={10.1109/ICISAT54145.2021.9678209},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7207219,
author={Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel},
booktitle={2015 IEEE International Congress on Big Data},
title={Big Data Pre-processing: A Quality Framework},
year={2015},
volume={},
number={},
pages={191-198},
abstract={With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.},
keywords={Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing},
doi={10.1109/BigDataCongress.2015.35},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7584971,
author={Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana},
booktitle={2016 IEEE International Congress on Big Data (BigData Congress)},
title={An Hybrid Approach to Quality Evaluation across Big Data Value Chain},
year={2016},
volume={},
number={},
pages={418-425},
abstract={While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.},
keywords={Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment},
doi={10.1109/BigDataCongress.2016.65},
ISSN={},
month={June},}
@INPROCEEDINGS{8942297,
author={Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz},
booktitle={2019 Third International Conference on Intelligent Computing in Data Sciences (ICDS)},
title={Towards a multi-agents model for errors detection and correction in big data flows},
year={2019},
volume={},
number={},
pages={1-5},
abstract={The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.},
keywords={Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors},
doi={10.1109/ICDS47004.2019.8942297},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7364060,
author={Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Evaluation of data quality of multisite electronic health record data for secondary analysis},
year={2015},
volume={},
number={},
pages={2612-2620},
abstract={Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.},
keywords={Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics},
doi={10.1109/BigData.2015.7364060},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9314391,
author={Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif},
booktitle={2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)},
title={Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.},
keywords={Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration},
doi={10.1109/ICECOCS50124.2020.9314391},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9073586,
author={Khaleel, Majida Yaseen and Hamad, Murtadha M.},
booktitle={2019 12th International Conference on Developments in eSystems Engineering (DeSE)},
title={Data Quality Management for Big Data Applications},
year={2019},
volume={},
number={},
pages={357-362},
abstract={Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.},
keywords={Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.},
doi={10.1109/DeSE.2019.00072},
ISSN={2161-1351},
month={Oct},}
@INPROCEEDINGS{9006187,
author={Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={DQA: Scalable, Automated and Interactive Data Quality Advisor},
year={2019},
volume={},
number={},
pages={2913-2922},
abstract={Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.},
keywords={Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science},
doi={10.1109/BigData47090.2019.9006187},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9006358,
author={Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams},
year={2019},
volume={},
number={},
pages={3260-3266},
abstract={Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.},
keywords={Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science},
doi={10.1109/BigData47090.2019.9006358},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8102206,
author={Alqarni, Mohammed A.},
booktitle={2017 14th International Conference on Smart Cities: Improving Quality of Life Using ICT & IoT (HONET-ICT)},
title={Benefits of SDN for Big data applications},
year={2017},
volume={},
number={},
pages={74-77},
abstract={Big data applications depend on underlying networks that make the transfer of information possible. These networks may be real (conventional) or virtual (in case of services hosted in data centers). Either way, the responsibility of smooth execution of the application, despite increasing traffic volume, lies with the service provider. The service providers face many challenges with respect to providing a high quality of service. It is therefore in the best interest of the service providers that efficiency of the applications is increased. SDN has the potential to improve big data application performance. In this paper we have a look at the recent advancements in technology that helps improve big data applications using SDN and discuss our observations.},
keywords={Big Data applications;Optimization;Servers;Protocols;Multimedia communication;SDN;Network Virtualization;Software Defined Networks;Big data},
doi={10.1109/HONET.2017.8102206},
ISSN={1949-4106},
month={Oct},}
@ARTICLE{8667300,
author={Lee, Doyoung},
journal={IEEE Access},
title={Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea},
year={2019},
volume={7},
number={},
pages={36294-36299},
abstract={In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.},
keywords={Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty},
doi={10.1109/ACCESS.2019.2904286},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8695373,
author={Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita},
booktitle={2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
title={A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control},
year={2019},
volume={},
number={},
pages={463-466},
abstract={Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.},
keywords={Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control},
doi={10.1109/MIPR.2019.00093},
ISSN={},
month={March},}
@INPROCEEDINGS{7840769,
author={Ganapathi, Archana and Chen, Yanpei},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Data quality: Experiences and lessons from operationalizing big data},
year={2016},
volume={},
number={},
pages={1595-1602},
abstract={Data quality issues pose a significant barrier to operationalizing big data. They pertain to the meaning of the data, the consistency of that meaning, the human interpretation of results, and the contexts in which the results are used. Data quality issues arise after organizations have moved past clear-cut technical solutions to early bottlenecks in using data. Left unaddressed, such issues can and have led to high profile missteps, and raise doubts about the data-driven world view altogether. In this paper, we share real-world case studies of tackling data quality challenges across industry verticals. We present initial ideas on how to systematically address data quality issues via technology. The success of operationalizing big data will depend on the quality of data involved, and whether such data causes uncertainty and disruptions, or delivers genuine knowledge and value.},
keywords={Cleaning;Big data;Measurement;Business;Software;Instruments;Industries},
doi={10.1109/BigData.2016.7840769},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9134140,
author={Gan, Wenting},
booktitle={2020 International Conference on E-Commerce and Internet Technology (ECIT)},
title={Design of Network Precision Marketing Based on Big Data Analysis Technology},
year={2020},
volume={},
number={},
pages={77-81},
abstract={In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.},
keywords={Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design},
doi={10.1109/ECIT50008.2020.00026},
ISSN={},
month={April},}
@INPROCEEDINGS{8258221,
author={Hee, Kim},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Is data quality enough for a clinical decision?: Apply machine learning and avoid bias},
year={2017},
volume={},
number={},
pages={2612-2619},
abstract={This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.},
keywords={Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias},
doi={10.1109/BigData.2017.8258221},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7474375,
author={Ciancarini, Paolo and Poggi, Francesco and Russo, Daniel},
booktitle={2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Big Data Quality: A Roadmap for Open Data},
year={2016},
volume={},
number={},
pages={210-215},
abstract={Open Data (OD) is one of the most discussed issue of Big Data which raised the joint interest of public institutions, citizens and private companies since 2009. However, the massive amount of freely available data has not yet brought the expected effects: as of today, there is no application that has fully exploited the potential provided by large and distributed information sources in a non-trivial way, nor any service has substantially changed for the better the lives of people. The era of a new generation applications based on OD is far to come. In this context, we observe that OD quality is one of the major threats to achieving the goals of the OD movement. The starting point of this case study is the quality of the OD released by the five Constitutional offices of Italy. Our exploratory case study aims to assess the quality of such releases and the real implementations of OD. The outcome suggests the need of a drastic improvement in OD quality. Finally we highlight some key quality principles for OD, and propose a roadmap for further research.},
keywords={Big data;Metadata;Government;ISO Standards;Distributed databases;Open Data Quality;Information Modeling;E-Government;Big Data Knowledge Extraction},
doi={10.1109/BigDataService.2016.37},
ISSN={},
month={March},}
@INBOOK{9821748,
author={Aytas, Yusuf},
booktitle={Designing Big Data Platforms: How to Use, Deploy, and Maintain Big Data Systems},
title={An Introduction: What's a Modern Big Data Platform},
year={2021},
volume={},
number={},
pages={1-9},
abstract={This chapter discusses the different aspects of designing Big Data platforms, in order to define what makes a big platform and to set expectations for these platforms. The solutions for Big Data processing vary based on the company strategy. A modern Big Data platform has several requirements, and to meet them correctly, expectations with regard to data should be set. Securing data has become a crucial aspect of a modern Big Data platform. The data quality depends on factors such as accuracy, consistency, reliability, and visibility. One of the hard problems of Big Data is backups as the vast amount of storage needed is overwhelming for backups. The Big Data platform should provide an extract, transform, and load (ETL) solution/s that manages the experience end to end. ETL developers should be able to develop, test, stage, and deploy their changes. Big Data platforms are quite complex as they are built based on distributed systems.},
keywords={Big Data;Task analysis;Cloud computing;Business;Security;Monitoring;Data mining},
doi={10.1002/9781119690962.ch1},
ISSN={},
publisher={Wiley},
isbn={9781119690948},
url={https://ieeexplore.ieee.org/document/9821748},}
@INPROCEEDINGS{7840935,
author={Haug, Frank S.},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Bad big data science},
year={2016},
volume={},
number={},
pages={2863-2871},
abstract={As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.},
keywords={Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata},
doi={10.1109/BigData.2016.7840935},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8258380,
author={Fu, Qian and Easton, John M.},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Understanding data quality: Ensuring data quality by design in the rail industry},
year={2017},
volume={},
number={},
pages={3792-3799},
abstract={The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.},
keywords={Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema},
doi={10.1109/BigData.2017.8258380},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8621924,
author={Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example},
year={2018},
volume={},
number={},
pages={5328-5329},
abstract={Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.},
keywords={Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot},
doi={10.1109/BigData.2018.8621924},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9491629,
author={Hattawi, Waleed and Shaban, Sameeh and Shawabkah, Aon Al and Alzu’bi, Shadi},
booktitle={2021 International Conference on Information Technology (ICIT)},
title={Recent Quality Models in BigData Applications},
year={2021},
volume={},
number={},
pages={811-815},
abstract={In this time the big data became an important part of all areas, it can be used in multiple industrials such as banking, education, government, networking, energy, health care, etc. So, because of that the huge amount of data became have problems or unnecessary data, and so that comes from the difficulty of measure the quality of these data. In this research we show the quality characteristic that can be help to increase the efficiency of quality measurement process of BDA by comparing it with other quality model of BDA and applying it on the 7V's of big data.},
keywords={Analytical models;Area measurement;Government;Energy measurement;Medical services;Big Data;Data models;Big Data;Quality;Quality Models;Data Quality;Big Data Application;The 7v's Of Big Data},
doi={10.1109/ICIT52682.2021.9491629},
ISSN={},
month={July},}
@INPROCEEDINGS{8416208,
author={Blanquer, Ignacio and Meira, Wagner},
booktitle={2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)},
title={EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform},
year={2018},
volume={},
number={},
pages={47-48},
abstract={This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.},
keywords={Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics},
doi={10.1109/DSN-W.2018.00023},
ISSN={2325-6664},
month={June},}
@INPROCEEDINGS{9006446,
author={Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection},
year={2019},
volume={},
number={},
pages={200-205},
abstract={Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.},
keywords={Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning},
doi={10.1109/BigData47090.2019.9006446},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7321730,
author={Ahnn, Jong Hoon},
booktitle={2014 IEEE/ACM International Symposium on Big Data Computing},
title={A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung},
year={2014},
volume={},
number={},
pages={64-73},
abstract={We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.},
keywords={Adaptation models;Speech;Acoustics;Computational modeling;Engines;Big data;Speech recognition;Big Data;Cloud Computing;Hadoop;Speech Recognition;Scalability;Personalization},
doi={10.1109/BDC.2014.11},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9727023,
author={Xing, Xiaobo},
booktitle={2021 International Symposium on Advances in Informatics, Electronics and Education (ISAIEE)},
title={Financial Big Data Reconciliation Method},
year={2021},
volume={},
number={},
pages={260-263},
abstract={For data errors in distributed financial system caused by multi-system interaction, asynchronous processing and system bug, this paper proposes offline and quasi real-time data reconciliation methods based on the combination of Alibaba big data processing platform and accounting theory. In offline data reconciliation, Full data reconciliation and hour level incremental data reconciliation are introduced. And in quasi real-time data reconciliation, single system and distributed multi-system reconciliation models are introduced. These data reconciliation methods are then verified against 7 million pieces of daily data of the distributed loan system in a financial company. Results show that these methods can complete the financial big data processing, discover the data quality problems timely, and minimize the financial system capital loss.},
keywords={Costs;Data integrity;Education;Distributed databases;Big Data;Maintenance engineering;Real-time systems;financial big data;full data reconciliation;incremental data reconciliation;quasi real time data reconciliation},
doi={10.1109/ISAIEE55071.2021.00071},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8622229,
author={Austin, Claire C.},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={A Path to Big Data Readiness},
year={2018},
volume={},
number={},
pages={4844-4853},
abstract={"Big Data readiness" begins at the source where data are first created and extends along a path through an organization to the outside world. This paper focuses on practical solutions to common problems experienced when integrating diverse datasets from disparate sources. Following the Introduction, Section 2 situates Big Data in the larger context of open government, open science, science integrity, and Standards, internationally and in Canada. Section 3 analyses the Big Data problem space, while Section 4 proposes a Big Data solution space. Section 5 proposes eight data checklist modules and suggests implementation strategies to effectively meet a variety of organizational needs. Section 6 summarizes conclusions and describes future work.},
keywords={Big Data;Government;Standards organizations;Europe;Tools;Big Data;data quality;data checklist;data repository;open science;open government;data science},
doi={10.1109/BigData.2018.8622229},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7809598,
author={Kläs, Michael and Putz, Wolfgang and Lutz, Tobias},
booktitle={2016 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM-MENSURA)},
title={Quality Evaluation for Big Data: A Scalable Assessment Approach and First Evaluation Results},
year={2016},
volume={},
number={},
pages={115-124},
abstract={High-quality data is a prerequisite for most types of analysis provided by software systems. However, since data quality does not come for free, it has to be assessed and managed continuously. The increasing quantity, diversity, and velocity that characterize big data today make these tasks even more challenging. We identified challenges that are specific for big data quality assessments with particular emphasis on their usage in smart ecosystems and make a proposal for a scalable cross-organizational approach that addresses these challenges. We developed an initial prototype to investigate scalability in a multi-node test environment using big data technologies. Based on the observed horizontal scalability behavior, there is an indication that the proposed approach also allows dealing with increasing volumes of heterogeneous data.},
keywords={Big data;Quality assessment;Metadata;Instruments;Ecosystems;Companies;big data quality assessment;quality measurement;velocity;volume;variety;SQA4BD;QUAMOCO;smart ecosystems;SPARK;HADOOP},
doi={10.1109/IWSM-Mensura.2016.026},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9263667,
author={Xiangwei, Kong},
booktitle={2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
title={Evaluation of Flight Test Data Quality Based on Rough Set Theory},
year={2020},
volume={},
number={},
pages={1053-1057},
abstract={With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.},
keywords={Data integrity;Rough sets;Feature extraction;Image color analysis;Shape;Packet loss;Data mining;Flight test;data quality;rough set;quality evaluation},
doi={10.1109/CISP-BMEI51763.2020.9263667},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8034978,
author={Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua and Gudivada, Venkat},
booktitle={2017 IEEE International Conference on Services Computing (SCC)},
title={Building a Deep Learning Classifier for Enhancing a Biomedical Big Data Service},
year={2017},
volume={},
number={},
pages={140-147},
abstract={Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.},
keywords={Diffraction;Big Data;Machine learning;Morphology;Support vector machines;Three-dimensional displays;Noise measurement;machine learning;deep learning;big data;data quality;cross-validation},
doi={10.1109/SCC.2017.25},
ISSN={2474-2473},
month={June},}
@INPROCEEDINGS{7363865,
author={Rettig, Laura and Khayati, Mourad and Cudré-Mauroux, Philippe and Piórkowski, Michal},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Online anomaly detection over Big Data streams},
year={2015},
volume={},
number={},
pages={1113-1122},
abstract={Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.},
keywords={Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks},
doi={10.1109/BigData.2015.7363865},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7584934,
author={Yin, Jianwei and Tang, Yan and Lo, Wei and Wu, Zhaohui},
booktitle={2016 IEEE International Congress on Big Data (BigData Congress)},
title={From Big Data to Great Services},
year={2016},
volume={},
number={},
pages={165-172},
abstract={Big Data is increasingly adopted by a wide range of service industries to improve the quality and value of their services, e.g., inventory that matches well the supply and demand, and pricing that reflects well the market needs. Customers benefit from higher quality of service enabled by Big Data. Service providers get higher profits from more precise control of costs and accurate knowledge of customer needs. In this paper, we define the next generation high quality services as Great Services, characterized by 4P Quality-of-Service (QoS) dimensions: Panorama, Penetration, Prediction and Personalization, which go much further than current services. The transformation of Big Data into Great Services would be difficult and expensive without methodical techniques and software tools. We call the intermediate step Deep Knowledge, which is generated by Big Data (with the 4V challenges - Volume, Velocity, Variety, and Veracity) and used in the creation of Great Services. Deep Knowledge is distinguished from traditional Big Data by 4C properties (Complexity, Cross-domain, Customization, and Convergence). In order to achieve the 4P QoS dimensions of Great Services, we need Deep Knowledge with 4C properties. In this paper, we describe an informal characterization of Great Services with 4P QoS dimensions with examples, and outline the techniques and tools that facilitate the transformation of Big Data into Deep Knowledge with 4C properties, and then the use of Deep Knowledge in Great Services.},
keywords={Big data;Quality of service;Public transportation;Vehicles;Industries;Next generation networking;Complexity theory;Great Services;4P QoS},
doi={10.1109/BigDataCongress.2016.28},
ISSN={},
month={June},}
@INPROCEEDINGS{9671672,
author={Poon, Lex and Farshidi, Siamak and Li, Na and Zhao, Zhiming},
booktitle={2021 IEEE International Conference on Big Data (Big Data)},
title={Unsupervised Anomaly Detection in Data Quality Control},
year={2021},
volume={},
number={},
pages={2327-2336},
abstract={Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.},
keywords={Data integrity;Decision making;Process control;Quality control;Organizations;Big Data;Data models;data quality control;data quality assessment;unsupervised learning;anomaly detection;automated data quality control},
doi={10.1109/BigData52589.2021.9671672},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9590700,
author={Du, Jinming},
booktitle={2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE)},
title={Research on the Construction of Educational Data Quality Model Based on Multiple Constraints Model},
year={2021},
volume={},
number={},
pages={363-367},
abstract={With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.},
keywords={Training;Analytical models;Systematics;Databases;Data integrity;Biological system modeling;Education;Educational data;Data quality;Statistics},
doi={10.1109/ICISCAE52414.2021.9590700},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9620761,
author={Ku, Tai-Yeon and Park, Wan-Ki and Choi, Hoon},
booktitle={2021 International Conference on Information and Communication Technology Convergence (ICTC)},
title={Mechanism of a big-data platform for residential heat energy consumption},
year={2021},
volume={},
number={},
pages={1450-1452},
abstract={Although the solar energy industry is becoming widespread, it is necessary to manage the charging and generating scheduling of solar power generation according to the ever-changing climate environment. In order to do this, a judgment criterion that can give timely charge / discharge instructions is needed and it needs to be actively performed. In this paper, we define a big-data platform for residential heat energy consumption. As a technology to secure thermal energy data of apartment houses, collect thermal energy data by dividing it into supply/equipment/usage. In order to secure standardized thermal energy data from the calorimeter installed. Equipped with data classification and processing, LP storage and management, data quality measurement and analysis functions. Develop a data adapter, from several multiunit dwellings with different calorimeter types. We will collect thermal energy data with an integrated big data system.},
keywords={Energy consumption;Temperature distribution;Water storage;Data integrity;Water heating;Solar energy;Big Data;energy management;energy big data;energy information collection},
doi={10.1109/ICTC52510.2021.9620761},
ISSN={2162-1233},
month={Oct},}
@INPROCEEDINGS{8258543,
author={Liu, Lixin and Chen, Jun},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={The influences of deep-sea vision data quality on observational analysis},
year={2017},
volume={},
number={},
pages={4789-4791},
abstract={Deep-sea study by optical observation method is an interdisciplinary subject and faces plenty of difficulties. To find out the influences of vision data quality, characteristic of vision data for deep-sea observation is analyzed, and a deep-sea landing experiment has been implemented. Data quality analyzing based on real deep-sea vision data that collected in the in-situ observation platform is actualized. It is expected that the research on influence mechanism of deep-sea vision quality is beneficial to the detection of region of interest, the judging of animal existence, the classification of species, and the trajectories labeling. Further analyzing on unsupervised deep-sea vision data quality control is necessary.},
keywords={Scattering;Optical sensors;Optical imaging;Fish;Backscatter;deep-sea observation;vision data quality;automatic judging},
doi={10.1109/BigData.2017.8258543},
ISSN={},
month={Dec},}
@ARTICLE{8361574,
author={Meng, Qianyu and Wang, Kun and He, Xiaoming and Guo, Minyi},
journal={Big Data Mining and Analytics},
title={QoE-driven big data management in pervasive edge computing environment},
year={2018},
volume={1},
number={3},
pages={222-233},
abstract={In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.},
keywords={Big Data;Quality of experience;Edge computing;Quality of service;Computational modeling;Training;Streaming media;Quality-of-Experience (QoE); high-dimensional big data management; deep learning; pervasive edge},
doi={10.26599/BDMA.2018.9020020},
ISSN={2096-0654},
month={Sep.},}
@INPROCEEDINGS{8740576,
author={Zan, Songting and Zhang, Xu},
booktitle={2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)},
title={Medical Data Quality Assessment Model Based on Credibility Analysis},
year={2018},
volume={},
number={},
pages={940-944},
abstract={The current main problem of medical data is low accuracy. Although existing data quality evaluation and audit can meet the needs, network security problem will be more serious in the future. As a result, the data quality evaluation model must include data credibility. This paper proposes the medical big data quality evaluation model based on credibility analysis and Analytic Hierarchy Process(AHP). Firstly, analyze the data credibility. After eliminating the unreliable data, calculate the data quality dimensions respectively. Then obtain the data quality evaluation result by integrating all dimensions with AHP. Through simulation, the data quality evaluation model can effectively identify the data that affects the credibility. The data quality evaluation results are improved after removing the untrusted data, besides the amount of data is reduced, which is applicable to the big data scenario.},
keywords={Data models;Data integrity;Big Data;Mathematical model;Analytical models;Analytic hierarchy process;Surgery;data quality evaluation;medical;credibility analysis;analytic hierarchy process},
doi={10.1109/ITOEC.2018.8740576},
ISSN={},
month={Dec},}
@ARTICLE{9852477,
author={Hart, Philip and He, Lijun and Wang, Tianyi and Kumar, Vijay S. and Aggour, Kareem and Subramanian, Arun and Yan, Weizhong},
journal={IEEE Open Access Journal of Power and Energy},
title={Application of Big Data Analytics and Machine Learning to Large-Scale Synchrophasor Datasets: Evaluation of Dataset ‘Machine Learning-Readiness’},
year={2022},
volume={9},
number={},
pages={386-397},
abstract={This manuscript presents a data quality analysis and holistic ‘machine learning-readiness’ evaluation of a representative set of large-scale, real-world phasor measurement unit (PMU) datasets provided under the United States Department of Energy-funded FOA 1861 research program. A major focus of this study is to understand the present-day suitability of large-scale, real-world synchrophasor datasets for application of commercially-available, off-the-shelf big data and supervised or semi-supervised machine learning (ML) tools and catalogue any major obstacles to their application. To this end, dataset quality is methodically examined through an interconnect-wide quantifications of basic bad data occurrences, a summary of several harder-to-detect data quality issues that can jeopardize successful application of machine learning, and an evaluation of the adequacy of event log labeling for supervised training of models used for online event classification. A global ‘six-point’ statistical analyses of several key dataset variables is demonstrated as a means by which to identify additional hard-to-detect data quality issues, also providing an example successful application of big data technology to extract insights regarding reasonable operational bounds of the US power system. Obstacles for application of commercial ML technologies are summarized, with a particular focus on supervised and semi-supervised ML. Lessons-learned are provided regarding challenges associated with present-day event labeling practices, large spatial scope of the dataset, and dataset anonymization. Finally, insight into efficacy of employed mitigation strategies are discussed, and recommendations for future work are made.},
keywords={Phasor measurement units;Data integrity;Big Data;Power systems;Machine learning;Training;Statistical analysis;Phasor measurement units;synchrophasor datasets;statistical analysis;data quality analysis;label quality;nonymized datasets;supervised machine learning;big data;wide area monitoring system},
doi={10.1109/OAJPE.2022.3197553},
ISSN={2687-7910},
month={},}
@INPROCEEDINGS{8075416,
author={Xie, Huan and Tong, Xiaohua and Meng, Wen and Wang, Fang and Xu, Xiong},
booktitle={2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)},
title={Multiple stratified sampling strategy for assessing the big remote sensing products},
year={2015},
volume={},
number={},
pages={1-4},
abstract={The number and volume of remote sensing data and its derived products, which are regarded as typical “big data”, have grown exponentially. How to assess the quality of these big remote sensing products become a challenge. As an importance technique, spatial sampling is regarded to be necessary for the quality assessment of remote sensing derived products. This paper proposes an approach of multiple stratified spatial sampling for assessing the remote sensing products, with the aim of resolving the issue of the quality inspection of big remote sensing products. The proposed method improves the sampling accuracy without increasing the sampling size, and the whole procedure is repeatable and easily adopted for the quality inspection of remote sensing derived products.},
keywords={Remote sensing;Inspection;Sampling methods;Sociology;Big Data;Quality assessment;multiple stratified;spatial sampling;quality assessment;remote sensing products;big data},
doi={10.1109/WHISPERS.2015.8075416},
ISSN={2158-6276},
month={June},}
@INPROCEEDINGS{8257913,
author={Benbernou, Salima and Ouziri, Mourad},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Enhancing data quality by cleaning inconsistent big RDF data},
year={2017},
volume={},
number={},
pages={74-79},
abstract={We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.},
keywords={Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems},
doi={10.1109/BigData.2017.8257913},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8785536,
author={Auer, Florian and Felderer, Michael},
booktitle={2019 IEEE/ACM 4th International Workshop on Metamorphic Testing (MET)},
title={Addressing Data Quality Problems with Metamorphic Data Relations},
year={2019},
volume={},
number={},
pages={76-83},
abstract={In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.},
keywords={Big Data;Data integrity;Testing;Encyclopedias;Internet;Electronic publishing;metamorphic testing, data quality, big data, quality assessment, metamorphic data relations},
doi={10.1109/MET.2019.00019},
ISSN={},
month={May},}
@INPROCEEDINGS{9616552,
author={Li, Congli and Yang, Bin and Chen, Xuezhen and Zhang, Enjie and Huang, Hongjun and Li, Da},
booktitle={2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)},
title={Research on smart grid big data’s curve mean clustering algorithm for edge-cloud collaborative application},
year={2021},
volume={},
number={},
pages={395-398},
abstract={As the demand for smart grid construction increases, advanced power applications based on edge-cloud collaboration continue to increase. Among them, there are many data-driven artificial intelligence calculations and analyses, all of which are calculated and analyzed based on electric power big data. However, for the massive electric power big data, it is impossible to obtain more internally related information only by observing the data from the surface. To a certain extent, it directly affects the upper-level advanced applications. To solve this problem, this paper studies and proposes a curve-mean clustering algorithm for load big data, which is the most widely used load data in smart grid. By analyzing the advanced measurement infrastructure, the matrix low-rank property of load big data and the calculation of singular value, the curve mean clustering of load big data is realized, and the optimal determination method of cluster number is expounded. Experiments are conducted based on actual resident user load data and compared with the classic mean shift clustering algorithm. By calculating the average distance within the cluster, the average distance between clusters and the DI index, it is verified that the proposed method clustering is more accurate and the selection of cluster number is optimal. The research plays a very good role in basic analysis for improving the big data analysis capability and data quality of smart grid.},
keywords={Wireless communication;Low voltage;Data integrity;Urban areas;Clustering algorithms;Collaboration;Big Data;Smart grid big data;Data quality;Curve mean clustering Algorithm},
doi={10.1109/ICWCSG53609.2021.00085},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8315370,
author={Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal},
booktitle={2017 IEEE 7th International Symposium on Cloud and Service Computing (SC2)},
title={Quality Profile-Based Cloud Service Selection for Fulfilling Big Data Processing Requirements},
year={2017},
volume={},
number={},
pages={149-156},
abstract={Big data has emerged as promising technology to handle huge and special data. Processing Big data involves selecting the appropriate services and resources thanks to the variety of services offered by different Cloud providers. Such selection is difficult, especially if a set of Big data requirements should be met. In this paper, we propose a dynamic cloud service selection scheme that assess Big data requirements, dynamically map these to the most available cloud services, and then recommend the best match services that fulfill different Big data processing requests. Our selection is conducted in two stages: 1) relies on a Big data task profile that efficiently capture Big data task's requirements and map them to QoS parameters, and then classify cloud providers that best satisfy these requirements, 2) uses the list of selected providers from stage 1 to further select the appropriate Cloud services to fulfill the overall Big Data task requirements. We extend the Analytic Hierarchy Process (AHP) based ranking mechanism to cope with the problem of multi-criteria selection. We conduct a set of experiments using simulated cloud setup to evaluate our selection scheme as well as the extended AHP against other selection techniques. The results show that our selection approach outperforms the others and select efficiently the appropriate cloud services that guarantee Big data task's QoS requirements.},
keywords={Big Data;Quality of service;Task analysis;Cloud computing;Data models;Analytic hierarchy process;Mathematical model;Big Data Task profile;Cloud service selection;QoS},
doi={10.1109/SC2.2017.30},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8276745,
author={Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib},
booktitle={2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)},
title={Towards a Data Quality Framework for Heterogeneous Data},
year={2017},
volume={},
number={},
pages={155-162},
abstract={Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.},
keywords={Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment},
doi={10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28},
ISSN={},
month={June},}
@INPROCEEDINGS{7823519,
author={Li, Tao and He, Yihai and Zhu, Chunling},
booktitle={2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)},
title={Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry},
year={2016},
volume={},
number={},
pages={181-186},
abstract={The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.},
keywords={Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM},
doi={10.1109/ICIICII.2016.0052},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9297009,
author={Juddoo, Suraj and George, Carlisle},
booktitle={2020 3rd International Conference on Emerging Trends in Electrical, Electronic and Communications Engineering (ELECOM)},
title={A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry},
year={2020},
volume={},
number={},
pages={58-66},
abstract={Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.},
keywords={Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning},
doi={10.1109/ELECOM49001.2020.9297009},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8532518,
author={Burkhardt, Andrew and Berryman, Sheila and Brio, Ashley and Ferkau, Susan and Hubner, Gloria and Lynch, Kevin and Mittman, Susan and Sonderer, Kathy},
booktitle={2018 IEEE AUTOTESTCON},
title={Measuring Manufacturing Test Data Analysis Quality},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.},
keywords={Data integrity;Manufacturing;Measurement;Decision making;Production facilities;Data models;manufacturing test;data quality;test data quality;cost of data quality},
doi={10.1109/AUTEST.2018.8532518},
ISSN={1558-4550},
month={Sep.},}
@INPROCEEDINGS{6972054,
author={Ludwig, Heiko},
booktitle={2014 IEEE 18th International Enterprise Distributed Object Computing Conference},
title={Managing Big Data Effectively - A Cloud Provider and a Cloud Consumer Perspective},
year={2014},
volume={},
number={},
pages={91-91},
abstract={Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.},
keywords={Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments},
doi={10.1109/EDOC.2014.21},
ISSN={1541-7719},
month={Sep.},}
@INPROCEEDINGS{7751629,
author={Li, Mingxin and Wei, Heng and Liao, Hongxi},
booktitle={2016 16th International Symposium on Communications and Information Technologies (ISCIT)},
title={Mobile terminal quality of experience analysis based on big data},
year={2016},
volume={},
number={},
pages={241-245},
abstract={In this paper, we proposes a method to analyze and evaluate the quality of experience (QoE) in mobile terminals using “big data”. The feature parameters of key quality indicator (KQI) are obtained from operators and quantized through the use of a scoring system. Then the scores of customer experience indicator (CEI) and QoE are calculated based on our proposed analytical model. In combination with the data of market operation, the terminal QoE evaluation scores contribute to offer effective suggestions on the promotion of mobile terminal.},
keywords={Indexes;Mobile communication;Mobile computing;Big data;Internet;Delays;big data;terminal;QoE;variable coefficient method},
doi={10.1109/ISCIT.2016.7751629},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8367700,
author={Yu, Bin and Zhang, Chen and Tang, ZhouHua and Sun, JiangYan},
booktitle={2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)},
title={Verification method of data quality in science and technology cloud in Shaanxi province},
year={2018},
volume={},
number={},
pages={319-323},
abstract={This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center “Science and Technology Cloud” project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in “Science and Technology Cloud” project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.},
keywords={Data integrity;Redundancy;Databases;Dynamic programming;Education;Remuneration;Organizations;Science and Technology Cloud;data quality;data redundancy;missing value processing},
doi={10.1109/ICBDA.2018.8367700},
ISSN={},
month={March},}
@INPROCEEDINGS{8275101,
author={Setiadi, Yazid and Uluwiyah, Ana},
booktitle={2017 International Workshop on Big Data and Information Security (IWBIS)},
title={Improving data quality through big data: Case study on big data-mobile positioning data in Indonesia tourism statistics},
year={2017},
volume={},
number={},
pages={43-48},
abstract={Big Data is a new concept that has become widely popularised in recent years. The revolutionized meaning of information communication technologies and Internet technologies refers to mobile communications which enable individuals to move and generate, transmit and receive different kinds of information. There are many communication options where users can search, interact and share information with other users such as website, social media, online communities blogs, and email called as Digital transformation. In Digital transformation era, over 95 % of travellers today use digital resources. Digital traveler can be as data source for official statistics. One of methods to capture the number of tourist can use Mobile Positioning Data (MPD). This method is considered able to improve the quality of survey data. This article will discuss further how to improve the quality of survey data through Big Data with case studies of MPD users in Indonesian Tourism Statistics.},
keywords={1/f noise;Mobile communication;Big Data;Accuracy;Mobile Positioning Data;Tourism Statistics},
doi={10.1109/IWBIS.2017.8275101},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8465132,
author={Segooa, Mmatshuene Anna and Kalema, Billy Mathias},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)},
title={Improve Decision Making Towards Universities Performance Through Big Data Analytics},
year={2018},
volume={},
number={},
pages={1-5},
abstract={The technology Big Bang has seen organizations universities inclusive generating big volumes of data in various formats and at high speed than they used to do. Such data is referred to as Big Data. This voluminous data can be of great significance to organizations if better insights are drawn for management to improve decision making. However, to draw valued insight from Big Data, advanced forms of analytics need to be employed and such techniques are commonly known as Big Data analytics (BDA), This paper sough to report on analysis of factors influencing the leverage of BDA to improve performance in universities.},
keywords={Big Data;Organizations;Decision making;Standards organizations;Learning management systems;Market research;Big Data;Big Data Analytics;universities decision making;data quality},
doi={10.1109/ICABCD.2018.8465132},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7321725,
author={Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay},
booktitle={2014 IEEE/ACM International Symposium on Big Data Computing},
title={A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning},
year={2014},
volume={},
number={},
pages={16-25},
abstract={In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.},
keywords={Big data;Engines;Bayes methods;Partitioning algorithms;Accuracy;Algorithm design and analysis;Distributed databases;Big Data;Bayesian network;Distributed computing;Ensemble learning;Scientific workflow;Kepler;Hadoop},
doi={10.1109/BDC.2014.10},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8603594,
author={Farooqi, M. Mashab and Ali Khattak, Hasan and Imran, Muhammad},
booktitle={2018 14th International Conference on Emerging Technologies (ICET)},
title={Data Quality Techniques in the Internet of Things: Random Forest Regression},
year={2018},
volume={},
number={},
pages={1-4},
abstract={Internet of Things (IoTs) is one of the most promising fields in computer science. It consists of physical devices, automobiles, home appliances, embedded hardware, sensors and actuators which empowers these objects to interface and share information with other devices over the network. The data gathered from these devices is used to make intelligent decisions. If the data quality is poor, decisions are likely to be flawed. A little work has been carried out regarding data quality in the Internet of Things, but there is no scheme which is experimentally proved. In this paper we will identify data quality challenges in the Internet of Things domain and propose a model which ensure data quality standards provided by ISO 8000. We evaluated our model on the weather dataset and used the random forest prediction method to calculate the accuracy of our data. Results show that when compared with the baseline model the proposed system improves accuracy by 38.88%.},
keywords={Data integrity;Data models;Internet of Things;Cleaning;Meteorology;Sensors;Predictive models;data quality;internet of things;big data;machine learning},
doi={10.1109/ICET.2018.8603594},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7347061,
author={Wenlu Yang and Da Silva, Alzennyr and Picard, Marie-Luce},
booktitle={2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)},
title={Computing data quality indicators on Big Data streams using a CEP},
year={2015},
volume={},
number={},
pages={1-5},
abstract={Big Data is often referred to as the 3Vs: Volume, Velocity and Variety. A 4th V (validity) was introduced to address the quality dimension. Poor data quality can be costly, lead to breaks in processes and invalidate the company's efforts on regulatory compliance. In order to process data streams in real time, a new technology called CEP (complex event processing) was developed. In France, the current deployment of smart meters will generate massive electricity consumption data. In this work, we developed a diagnostic approach to compute generic quality indicators of smart meter data streams on the fly. This solution is based on Tibco StreamBase CEP. Visualization tools were also developed in order to give a better understanding of the inter-relation between quality issues and geographical/temporal dimensions. According to the application purpose, two visualization methods can be loaded: (1) StreamBase LiveView is used to visualize quality indicators in real time; and (2) a Web application provides a posteriori and geographical analysis of the quality indicators which are plotted on a map within a color scale (lighter colors indicate good quality and darker colors indicate poor quality). In future works, new quality indicators could be added to the solution which can be applied in an operational context in order to monitor data quality from smart meters.},
keywords={Smart meters;Data visualization;Smart grids;Real-time systems;Image color analysis;Indexes;Data quality;Big Data;data stream;CEP;smart grids},
doi={10.1109/IWCIM.2015.7347061},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7507493,
author={Rathore, Purva and Shukla, Deepak},
booktitle={2015 International Conference on Communication Networks (ICCN)},
title={Analysis and performance improvement of K-means clustering in big data environment},
year={2015},
volume={},
number={},
pages={43-46},
abstract={The big data environment is used to support the huge amount of data processing. In this environment tons (i.e. Giga bytes, Tera bytes) of data is processed. Therefore the various online applications where the huge data request are generated are treated using the big data i.e. facebook, google. In this presented work the big data environment is studied and investigated how the data is consumed using the big data and how the supporting tools are working with the Hadoop storage. Furthermore, for keen understanding and investigation, a cluster analysis technique more specifically the K-mean clustering algorithm is implemented through the Hadoop and MapReduce. The clustering is a part of big data analytics where the unlabelled data is processed and utilized to make groups of the data. In addition of that it is observed the traditional k-mean algorithm is not much suitably works with the Hadoop and MapReduce thus small amount of modification is performed on the data processing technique. In addition of that during cluster analysis various issues are found in traditional k-means i.e. fluctuating accuracy, outliers and empty cluster. Therefore a new clustering algorithm with modification on traditional approach of k-means clustering is proposed and implemented. That approach first enhances the data quality by removing the outlier points in datasets and then the bi-part method is used to perform the clustering. The proposed clustering technique implemented using the JAVA, Hadoop and MapReduce finally the performance of the proposed clustering approach is evaluated and compared with the traditional k-means clustering algorithm. The obtained performance shows the effective results and enhanced accuracy of cluster formation with the removal of the de-efficiency. Thus the proposed work is adoptable for the big data environment with improving the performance of clustering.},
keywords={Image recognition;Data visualization;Breast;Image segmentation;data mining;clustering;big data;performance improvement;implementation},
doi={10.1109/ICCN.2015.9},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7818902,
author={Lakshen, Guma Abdulkhader and Vraneš, Sanja and Janev, Valentina},
booktitle={2016 24th Telecommunications Forum (TELFOR)},
title={Big data and quality: A literature review},
year={2016},
volume={},
number={},
pages={1-4},
abstract={Big Data refers to data volumes in the range of Exabyte (1018) and beyond. Such volumes exceed the capacity of current on-line storage and processing systems. With characteristics like volume, velocity and variety big data throws challenges to the traditional IT establishments. Computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of Big Data. There are many issues with Big Data that warrant quality assessment methods. The issues are pertaining to storage and transport, management, and processing. This paper throws light into the present state of quality issues related to Big Data. It provides valuable insights that can be used to leverage Big Data science activities.},
keywords={Big data;Benchmark testing;Quality assessment;Databases;Software;Sparks;Engines;Big Data;Quality assessment;stream processing;survey;Big Data frameworks},
doi={10.1109/TELFOR.2016.7818902},
ISSN={},
month={Nov},}
@ARTICLE{9481251,
author={Yu, Wenjin and Liu, Yuehua and Dillon, Tharam and Rahayu, Wenny and Mostafa, Fahed},
journal={IEEE Internet of Things Journal},
title={An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques},
year={2022},
volume={9},
number={3},
pages={2443-2454},
abstract={With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.},
keywords={Big Data;Internet of Things;Intelligent sensors;Data analysis;Cloud computing;Smart manufacturing;Sensor phenomena and characterization;Big data;health state monitoring;Internet of Things (IoT);noisy data cleaning;real-time systems;sensor selection},
doi={10.1109/JIOT.2021.3096637},
ISSN={2327-4662},
month={Feb},}
@INPROCEEDINGS{9079066,
author={Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and Alhroob, Aysh},
booktitle={2020 11th International Conference on Information and Communication Systems (ICICS)},
title={An Approach to Improve Data Quality from Big Data Aspect by Sensitive Cost and Time},
year={2020},
volume={},
number={},
pages={022-026},
abstract={Big data is term of dataset with characteristic volume, value and veracity that lead to challenges unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the appropriate resources of organization in many phases by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope depends on high trust which is getting high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally choose from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation arranging between them start by project scope as strongest one then cost, product and finally time is weakest between them, in the final when select best quality use two sides generally from quality degree and be middle-quality interval and especially from relative distance with the strongest factor.},
keywords={Costs;Correlation;Statistical analysis;Data integrity;Volume measurement;Project management;Big Data;Big Data;Project Management;Sensitive Rule;Quality},
doi={10.1109/ICICS49469.2020.239526},
ISSN={2573-3346},
month={April},}
@ARTICLE{8756123,
author={Zhang, Xi and Zhu, Qixuan},
journal={IEEE Journal on Selected Areas in Communications},
title={Information-Centric Virtualization for Software-Defined Statistical QoS Provisioning Over 5G Multimedia Big Data Wireless Networks},
year={2019},
volume={37},
number={8},
pages={1721-1738},
abstract={The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.},
keywords={Big Data;Quality of service;Wireless networks;5G mobile communication;Resource management;Wireless sensor networks;5G multimedia big data wireless networks;ICN;NFV;SDN;optimal transmit power;statistical delay-bounded QoS;effective capacity;relay selection},
doi={10.1109/JSAC.2019.2927088},
ISSN={1558-0008},
month={Aug},}
@INPROCEEDINGS{9352886,
author={Qi, Cui and Mingyue, Sun and Na, Mi and Honggang, Wang and Yanhong, Jian and Jing, Zhu},
booktitle={2020 IEEE 3rd International Conference on Electronics and Communication Engineering (ICECE)},
title={Regional Electricity Sales Forecasting Research Based on Big Data Application Service Platform},
year={2020},
volume={},
number={},
pages={229-233},
abstract={Regional monthly electricity sales forecast is an important basis for regional power grid planning and construction, evaluation of regional economic development and operation, and protection of residents' lives. It is also an important work of regional power regulation and management, decision-making of power generation and purchase, improvement of power supply equipment utilization rate and deepening of power system reform. Based on the current situation of power supply enterprise information development, distribution network business status and characteristics, this paper analyzes the factors affecting electricity sales. According to the characteristics of annual changes in electricity sales and data quality factors, the recurrent neural network model is selected based on the big data application service platform. The long short term memory neural network model performs multi-step multivariate prediction on time series, and uses the attention mechanism to combine two independent models for prediction. Experiments conducted on the historical electricity sales data set of a power supply company show that compared with traditional machine learning methods, this method has advantages in accuracy and efficiency.},
keywords={Recurrent neural networks;Power supplies;Time series analysis;Predictive models;Big Data applications;Prediction algorithms;Data models;Recurrent neural network;Long Short-Term Memory neural network;regional monthly electricity sales;electricity sales forecast;big data application service platform},
doi={10.1109/ICECE51594.2020.9352886},
ISSN={},
month={Dec},}
@ARTICLE{7974765,
author={Khalfi, Besma and de Runz, Cyril and Faiz, Sami and Akdag, Herman},
journal={IEEE Transactions on Big Data},
title={A New Methodology for Storing Consistent Fuzzy Geospatial Data in Big Data Environment},
year={2021},
volume={7},
number={2},
pages={468-482},
abstract={In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities' consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.},
keywords={Fuzzy sets;Geospatial analysis;Big Data;Data models;NoSQL databases;Spatial databases;Fuzzy set theory;Spatial databases;data storage representations;schema and subschema;fuzzy set;imprecision;consistency;big data;NoSQL systems;JSON},
doi={10.1109/TBDATA.2017.2725904},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{9209633,
author={Byabazaire, John and O’Hare, Gregory and Delaney, Declan},
booktitle={2020 29th International Conference on Computer Communications and Networks (ICCCN)},
title={Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments},
year={2020},
volume={},
number={},
pages={1-9},
abstract={Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.},
keywords={Data integrity;Data models;Big Data;Biological system modeling;Measurement;Standards;Internet of Things;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning},
doi={10.1109/ICCCN49398.2020.9209633},
ISSN={2637-9430},
month={Aug},}
@INPROCEEDINGS{8444581,
author={Zhang, Mingming and Wo, Tianyu and Xie, Tao},
booktitle={2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)},
title={A Platform Solution of Data-Quality Improvement for Internet-of-Vehicle Services},
year={2018},
volume={},
number={},
pages={1-7},
abstract={Interconnection and intelligence have become the latest trends of the new generation of vehicle and transportation technologies. Applications built upon platforms of cloud-centered vehicle networking, i.e., Internet-of-Vehicles (IoVs), have been increasingly developed and deployed to provide data-centric services (e.g., driving assistance). Because these services are often safety critical, assuring service dependability has become an important requirement. In this paper, we propose DQI, a platform-level solution of Data-Quality Improvement designed to assure service dependability for Internet-of-Vehicle services. As an example, DQI is deployed in CarStream, an industrial system of big data processing designed for chauffeured car services. Via CarStream, over 30,000 vehicles are organized in a virtual vehicle network by sharing vehicle-status data in a near real-time manner. Such data often have low-quality issues and compromise the dependability of data-centric services. DQI includes techniques of data-quality improvement, including detecting outliers, extracting frequent patterns, and interpolating sequences. DQI enhances the dependability of data-centric services in IoVs by addressing the common data-quality requirements at the platform level. Upper-level services can benefit from DQI for data-quality improvement and reduce the complexity of service logic. We evaluate DQI by using a three-year dataset of vehicles and real applications deployed in CarStream. The result shows that compared with existing approaches, DQI can effectively restore missing data and correct anomalies with more than 30.0% improvement in precision. By studying multiple real applications, we also show that this data-quality improvement can indeed enhance the dependability of IoV services.},
keywords={Interpolation;Task analysis;Inspection;Cloud computing;Roads;Data integrity;Conferences;Dependability;Interpolation;Sequence Matching;Data Quality;Big Data;Internet-of-Vehicles},
doi={10.1109/PERCOM.2018.8444581},
ISSN={2474-249X},
month={March},}
@INPROCEEDINGS{9378181,
author={Srivastava, Divesh},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Towards High-Quality Big Data: Lessons from FIT},
year={2020},
volume={},
number={},
pages={4-4},
abstract={Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Data are being generated, collected, and analyzed today at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. As the use of big data has grown, so too have concerns that poor-quality data, prevalent in large data sets, can have serious adverse consequences on data-driven decision making. Responsible data science thus requires a recognition of the importance of veracity, the fourth "V" of big data. In this talk, we first present a vision of high-quality big data and highlight the substantial challenges that the first three V’s, volume, velocity, and variety, bring to dealing with veracity in big data. We then present the FIT Family of adaptive, data-driven statistical tools that we have designed, developed, and deployed at AT&T for continuous data quality monitoring of a large and diverse collection of continuously evolving data. These tools monitor data movement to discover missing, partial, duplicated, and delayed data; identify changes in the content of spatiotemporal streams; and pinpoint anomaly hotspots based on persistence, pervasiveness, and priority. We conclude with lessons from FIT relevant to big data quality that are cause for optimism.},
keywords={Big Data;Monitoring;Decision making;Data science;Conferences;Spatiotemporal phenomena;Magnetic heads},
doi={10.1109/BigData50022.2020.9378181},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8029349,
author={Kim, Hee Young and Cho, June-Suh},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Data Governance Framework for Big Data Implementation with a Case of Korea},
year={2017},
volume={},
number={},
pages={384-391},
abstract={Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA. While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data. To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization. In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services. We propose the Big Data Governance Framework in this paper. The Big Data governance framework presents criteria different from existing criteria at the data quality level. It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services. In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems. This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea. Big Data services in the public sector are an inevitable choice to improve the quality of people's life. Big Data governance and its framework are the essential components for the realization of Big Data service.},
keywords={Big Data;Government;Data privacy;Reliability;Social network services;Big data;Data governance;Data governance framework;Case analysis},
doi={10.1109/BigDataCongress.2017.56},
ISSN={},
month={June},}
@INPROCEEDINGS{7273312,
author={Wang, May D.},
booktitle={2015 IEEE 39th Annual Computer Software and Applications Conference},
title={Biomedical Big Data Analytics for Patient-Centric and Outcome-Driven Precision Health},
year={2015},
volume={3},
number={},
pages={1-2},
abstract={Rapid advancements in biotechnologies such as -omic (genomics, proteomics, metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies, molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine and health wellness. Multiple nations around the world have been seeking novel effective ways to make sense of "big data" for evidence-based, outcome-driven, and affordable 5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare. My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular, whole body, individual, and population) biomedical data analytics for discovery, development, and delivery, including translational bioinformatics in biomarker discovery for personalized care; imaging informatics in histopathology for clinical diagnosis decision support; bionanoinformatics for minimally-invasive image-guided surgery; critical care informatics in ICU for real-time evidence-based decision making; and chronic care informatics for patient-centric health. In this talk, first, I will highlight major challenges in biomedical and health informatics pipeline consisting of data quality control, information feature extraction, advanced knowledge modeling, decision making, and proper action taking through feedback. Second, I will present informatics methodological research in (i) data integrity and integration; (ii) case-based reasoning for individualized care; and (iii) streaming data analytics for real-time decision support using a few mobile health case studies (e.g. Sickle Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there is big shortage of data scientists and engineers who are capable of handling Big Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e. patients, physicians, payers, and hospitals) on how to tackle these grant challenges. I will discuss efforts such as patient-centric educational intervention, community-based crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition, Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta, Atlanta Clinical and Translational Science Institute, and industrial partners such as Microsoft Research and HP.},
keywords={Biomedical imaging;Informatics;Bioinformatics;Big data;Cancer;Decision making},
doi={10.1109/COMPSAC.2015.343},
ISSN={0730-3157},
month={July},}
@INPROCEEDINGS{8713218,
author={Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin},
booktitle={2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)},
title={An Automated Big Data Accuracy Assessment Tool},
year={2019},
volume={},
number={},
pages={193-197},
abstract={Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.},
keywords={Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest neighbors;logistic regression;decision trees},
doi={10.1109/ICBDA.2019.8713218},
ISSN={},
month={March},}
@INPROCEEDINGS{7159309,
author={Subhashini, R. and Akila, G},
booktitle={2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]},
title={Valence arousal similarity based recommendation services},
year={2015},
volume={},
number={},
pages={1-4},
abstract={Web Services play a vital role in e-commerce and e-business applications. A WS (Web Service) application is interoperable and can work on any platform i.e.; platform independent, large scale distributed systems can be established easily. A Recommender System is a precious tool for providing appropriate recommendations to all users in a Hotel Reservation Website. User based, Top k and profile based approaches are used in collaborative filtering algorithm which does not provide personalized results to the users and inefficiency and scalability problem also occurs due to the increase in the size of large datasets. To address the above mentioned challenges, a Valence-Arousal Similarity based Recommendation Services, called VAS based RS, is proposed. Our proposed mechanism aims to presents a personalized service recommendation list and recommending the most suitable service to the end users. Moreover, it classifies the positive and negative preferences of the users from their reviews to improve the prediction accuracy. For improve its efficiency and scalability in big data environment, VAS based RS is implemented using collaborative filtering algorithm on MapReduce parallel processing paradigm in Hadoop, a widely-adopted distributed computing platform.},
keywords={Web services;Collaboration;Big data;Quality of service;Recommender systems;Scalability;Web Service;Big Data;Recommender System;MapReduce;Hadoop},
doi={10.1109/ICCPCT.2015.7159309},
ISSN={},
month={March},}
@INPROCEEDINGS{8748630,
author={Saha, Ajitesh Kumar and Kumar, Ashwini and Tyagi, Vishu and Das, Sanjoy},
booktitle={2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)},
title={Big Data and Internet of Things: A Survey},
year={2018},
volume={},
number={},
pages={150-156},
abstract={In this digital era, here the sum of data is generate with store has prolonged inside a less period of time. The data in this era generated high speed leads to many challenges. The size is primarily and periodically, just the measurement that bounces in the big data position. In this survey, we have attempt to give a broad explanation of big data that captures its other unique and important features. We have discussed 4V's model. Also, the latest technologies uses in big data, like Hadoop, HDFS, MapReduce and different type of methods used by big data. Finally various benefits of using big data analysis and include some feature of cloud computing.},
keywords={Big Data;Reliability;Cloud computing;Social networking (online);Sensors;Videos;Data integrity;Big Data;Big Data Analytics;Big Data Quality;Data Reliability;IOT;Hadoop;HDFS;Cloud Computing},
doi={10.1109/ICACCCN.2018.8748630},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8121916,
author={Sattart, Farook and McQuay, Colter and Driessen, Peter F.},
booktitle={2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)},
title={Marine mammal sound anomaly and quality detection using multitaper spectrogram and hydrophone big data},
year={2017},
volume={},
number={},
pages={1-6},
abstract={This paper proposes a novel method for anomaly and quality detection of marine mammal sounds using multitaper spectrogram and hydrophone big data. The proposed method is aimed to automatically detect anomaly, such as high-frequency vessel noise, Doppler noise, in sperm whale (SPW) sound as well as the quality of the sound. A new signature function derived from a multi-taper spectrogram is able to detect the anomaly in the data and a new anomaly distortion measure can detect the sound quality into good/bad. The proposed method, is tested with 1905 minutes of data spanning a single year, and using a human operator's annotations. The experimental results reveal that the proposed multitaper spectrogram based approach is efficient in detecting anomaly as well as sperm whale sound quality for hydrophone big data and high detection accuracy (>85%) is achieved for raw input hydrophone data.},
keywords={Spectrogram;Whales;Sonar equipment;Big Data;Acoustic distortion;Anomaly detection;Anomaly detection;Hydrophone Big data;Quality detection;Multitaper spectrogram;Marine mammal sound;Sperm whale},
doi={10.1109/PACRIM.2017.8121916},
ISSN={},
month={Aug},}
