@INPROCEEDINGS{8836982,
author={Islam Sarker, Md Nazirul and Wu, Min and Chanthamith, Bouasone and Yusufzada, Shaheen and Li, Dan and Zhang, Jie},
booktitle={2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD)},
title={Big Data Driven Smart Agriculture: Pathway for Sustainable Development},
year={2019},
volume={},
number={},
pages={60-65},
abstract={Increasing agricultural production is top most solution in the face of rapid population growth through digitalization of agriculture by using most developed technology like big data. There is a long debate on the application of big data in agriculture. This study is an attempt to explore the suitability of the big data technologies for increasing production and improving quality in agriculture. The study uses an extensive review of current research works and studies in agriculture for exploring the best and compatible practices which can help farmers at field level for increasing production and improving quality. This study reveals a number of available big data technologies and practices in agriculture for solving the current problems and challenges at field level. A conceptual model is developed for proper implementation of available big data technologies at farmer's field level. The study highlights data generation procedure, availability of technology, availability of hardware, software, data collection techniques, method of analysis and suitability of application of big data technologies for smart agriculture. The article explores that there are still some challenges exists in this field as a new domain in agriculture like privacy of data, data quality, availability, initial investment, infrastructure and related expertise. The study suggests that government initiatives, public-private partnership, openness of data, financial investment and regional basis research work are necessary for implementing the big data technologies in agriculture at large scale.},
keywords={Agriculture;Big Data;Production;Sensors;Systematics;Sociology;Statistics;big data;smart agriculture;data driven;precision agriculture;smart farming},
doi={10.1109/ICAIBD.2019.8836982},
ISSN={},
month={May},}
@INPROCEEDINGS{7435456,
author={Yang, Sha and Yu, Wei and Hu, Yahui and Wang, Kai and Wang, Jun and Li, Shijun},
booktitle={2015 Third International Conference on Advanced Cloud and Big Data},
title={An Automatic Discovery Framework of Cross-Source Data Inconsistency for Web Big Data},
year={2015},
volume={},
number={},
pages={73-79},
abstract={The vigorous growth of big data has triggered both opportunities and challenges in business and industry. However, Web big data distributed in diverse sources with multiple data structures frequently conflict with each other, i.e. inconsistency in cross-source Web big data. In this paper, we propose a state-of-the-art architecture of auto-discovering inconsistency with Web big data. Our contributions include: (1) we classify the inconsistency features to formalize inconsistency data and establish an algebraic operation system, (2) we propose three algorithms to auto-discover inconsistency, including constraint-based, SDA-based and HPDM-based method and (3) we conduct experiments on real-world dataset to compare aforesaid schemes with Oracle-based inconsistency detection framework. The empirical results show that our methods outperform traditional framework both on accuracy and efficiency under Web big data.},
keywords={Big data;Data models;Computers;Data mining;Industries;Algorithm design and analysis;Distributed databases;Web Big Data;Data Consistency;Web Data Management;Data Quality Assessment;Data Analysis},
doi={10.1109/CBD.2015.22},
ISSN={},
month={Oct},}
@ARTICLE{8415743,
author={Yao, Le and Ge, Zhiqiang},
journal={IEEE Transactions on Industrial Electronics},
title={Scalable Semisupervised GMM for Big Data Quality Prediction in Multimode Processes},
year={2019},
volume={66},
number={5},
pages={3681-3692},
abstract={In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.},
keywords={Data models;Big Data;Predictive models;Inference algorithms;Prediction algorithms;Semisupervised learning;Computational modeling;Big data;Gaussian mixture model (GMM);multimode process modeling;quality prediction;semisupervised modeling;stochastic variational inference (SVI)},
doi={10.1109/TIE.2018.2856200},
ISSN={1557-9948},
month={May},}
@INPROCEEDINGS{7724441,
author={Aggarwal, Ankur},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={Identification of quality parameters associated with 3V's of Big Data},
year={2016},
volume={},
number={},
pages={1135-1140},
abstract={Big Data approach uses an empirical process that does not lie on the understanding of underlying mechanisms, but lies on the observation of facts. Achieving high quality in Big Data is a critical issue for both the database researchers and practitioners. More explicit consideration must be given to data quality since data increasingly outlives the application for which it was initially designed. In this paper, identification of quality parameters is done which are compatible to the 3V's of big data which will further provide enhancement in achieving quality data to be stored in the repository. Good utilization of Big Data strengthens the performance and competitiveness of the firms by enabling better and faster results to its customer needs. In this paper GQM (Goal Question Metric) methodology is proposed to measure quality using metrics. It describes how to include data quality metrics to project, progress and maintain levels of quality in an organization. It helps to make a decision whether or not our current data satisfies our quality prospects.},
keywords={Big data;Organizations;Conferences;Social network services;Electronic mail;Databases;Big Data;Quality;Volume;Variety;Velocity},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{9391025,
author={Wang, Fengling and Wang, Han and Xue, Liang},
booktitle={2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
title={Research on Data Security in Big Data Cloud Computing Environment},
year={2021},
volume={5},
number={},
pages={1446-1450},
abstract={In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.},
keywords={Cloud computing;Data security;Data integrity;Big Data;Maintenance engineering;Virtualization;Information technology;big data;cloud computing;data security;big data cloud computing;security policy},
doi={10.1109/IAEAC50856.2021.9391025},
ISSN={2689-6621},
month={March},}
@INPROCEEDINGS{9172875,
author={Zhang, Guobao},
booktitle={2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)},
title={A data traceability method to improve data quality in a big data environment},
year={2020},
volume={},
number={},
pages={290-294},
abstract={In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.},
keywords={Data Governance;Data Credibility;Data Traceability},
doi={10.1109/DSC50466.2020.00051},
ISSN={},
month={July},}
@ARTICLE{6949519,
author={O'Leary, Daniel E.},
journal={IEEE Intelligent Systems},
title={Embedding AI and Crowdsourcing in the Big Data Lake},
year={2014},
volume={29},
number={5},
pages={70-73},
abstract={Daniel E. O'Leary examines the notion of the Big Data Lake and contrasts it with decision support-based data warehouses. In addition, some of the risks of the emerging Lake concept that ultimately require data governance are analyzed. O'Leary investigates using different AI and crowdsourcing (human intelligence) applications in that lake in order to integrate disparate data sources, facilitate master data management and analyze data quality. Although data governance often is not seen as a technology issue, it is seen as a critical component of making the Big Data Lake "work".},
keywords={Crowdsourcing;Artificial intelligence;Big data;Data warehouses;Decision support systems;Databases;Business;Big Data Lake;data warehouses;artificial intelligence;crowdsourcing;data governance;master data management;intelligent systems},
doi={10.1109/MIS.2014.82},
ISSN={1941-1294},
month={Sep.},}
@INPROCEEDINGS{9397990,
author={Labeeb, Kashshaf and Chowdhury, Kuraish Bin Quader and Riha, Rabea Basri and Abedin, Mohammad Zoynul and Yesmin, Sarmila and Khan, Mohammad Nasfikur Rahman},
booktitle={2020 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE)},
title={Pre-Processing Data In Weather Monitoring Application By Using Big Data Quality Framework},
year={2020},
volume={},
number={},
pages={284-287},
abstract={In this research, we are working with Big Data for obtaining, preparing and analyzing data-based information to make use of the data retrieved which will benefit any organization. It is a progressing part of all divisions of industry and business. All organizations in any field, for example, oil, money, fabricating hardware and so forth produce big data, which can show incredibly helpful designs to business directors to make and develop their organizations, when the information is gathered and analyzed accurately. It permits us to gather, store, and decipher immense measures of big data to produce useful outcomes. Data quality is affected by the information that is gathered to be analyzed as that data will make sure whether in the long run a specific method of conducting the ongoing process is useful or not. Consequently, the consistency of big data very important. Here, we propose that the various types of raw information should be analyzed to expand its precision in the pre-handling stage, as those pieces of information are not utilized later in the process. During investments, we break down and model the big data to decrease overhead expenses to create and add to a solid understanding of results to improve information consistency.},
keywords={Solid modeling;Oils;Organizations;Big Data;Solids;Monitoring;Meteorology;heterogeneous data;noise filter;convergence;climate change},
doi={10.1109/WIECON-ECE52138.2020.9397990},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7051902,
author={Zillner, Sonja and Oberkampf, Heiner and Bretschneider, Claudia and Zaveri, Amrapali and Faix, Werner and Neururer, Sabrina},
booktitle={Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)},
title={Towards a technology roadmap for big data applications in the healthcare domain},
year={2014},
volume={},
number={},
pages={291-296},
abstract={Big Data technologies can be used to improve the quality and efficiency of healthcare delivery. The highest impact of Big Data applications is expected when data from various healthcare areas, such as clinical, administrative, financial, or outcome data, can be integrated. However, as of today, the seamless access to the various healthcare data pools is only possible in a very constrained and limited manner. For enabling the seamless access several technical requirements, such as data digitalization, semantic annotation, data sharing, data privacy and security as well as data quality need to be addressed. In this paper, we introduce a detailed analysis of these technical requirements and show how the results of our analysis lead towards a technical roadmap for Big Data in the healthcare domain.},
keywords={Medical services;Big data;Semantics;Data privacy;Standards;Biomedical imaging;Security;Big Data;technical requirements;data digitalization;semantic annotation;data integration;data privacy and security;data quality},
doi={10.1109/IRI.2014.7051902},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9820258,
author={He, Min},
booktitle={2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD)},
title={An optimized Graph Embedding based Knowledge Graph Cleaning Algorithm},
year={2022},
volume={},
number={},
pages={539-542},
abstract={Data quality of knowledge graph is a one of the most important guareentees for many knowledge-based applications. We investigate the konwledge graph cleaning problem. We propose a knowledge graph error detection framework and design an optimized embedding based clean algorithm. The framework maps the knowledge graph into an numerical space and keeps the relationship between different nodes. With this framework, both miss data error and errous relationship can be cleaned. Extensive experimental study over different data sets validate the effectiveness of the method.},
keywords={Data integrity;Knowledge based systems;Big Data;Cleaning;Classification algorithms;Noise measurement;knowledge graph;graph embedding;big data;data cleaning;data quality},
doi={10.1109/ICAIBD55127.2022.9820258},
ISSN={},
month={May},}
@INPROCEEDINGS{7029280,
author={Earley, Seth},
booktitle={2014 IT Professional Conference},
title={Presentation 1. Information governance in the age of big data},
year={2014},
volume={},
number={},
pages={1-3},
abstract={Organizations have understood the value of their structured data — mostly financial transactions — since the first mainframes were developed in the 40's. Data quality issues have always been a challenge and the increasing numbers of applications consuming and producing structured transactional data has grown exponentially. Unstructured information has been given less significance and strategic importance and therefore fewer resources and less attention on the part of leadership. All of that has changed and is changing faster than anyone imagined. Unstructured content is what humans produce. They create the documents: strategies, proposals, support documents, marketing content, white papers, engineering specifications, etc. that form the intelligence and core knowledge capital of the enterprise. Many organizations have left business units to fend for themselves and “go figure it out” with little guidance or support. These has led to terabytes of content that people cannot find their way through and that leaves the organization open to risks, liabilities and costs of e discovery. According to the Minnesota Journal of Law, Science & Technology, a gig of data costs $30,000 in e discovery costs. The cost of storage is ten cents. The problem is that the enterprise does not understand the hidden costs of not making data accessible and usable — lost time, lost IP, inefficiency, poor customer service that can lead to lost customers, slower growth, etc. As newer collaboration technologies are deployed, they expose the bad habits and sins of the past. Deploying a new search engine shows that the content is not curated. Standing up a new content management application like SharePoint reveals the haphazard shanty town of an information architecture with inconsistencies in models, terminology and applications. Today's landscape of marketing and customer experience technologies is complex and interconnected and requires those upstream knowledge processes that produce unstructured content as the fuel. Customer experience entails everything that happens before you purchase (marketing, education and outreach), when you purchase (e commerce with product content and data), and after you purchase (self-service systems and knowledge bases that support the call center). This is the customer lifecycle and at each step in the process systems and tools need to be harmonized as they gather information about the users using attribute models that are consistent and that serve the business and the customer. They take content and data as input and then output more data. One applications exhaust is another applications fuel. Organizations are also purchasing data streams to enrich their internal information sources. Social media is an enormous virtually untapped reservoir of data about customers and what they think about organizations. This can be mined for sentiment and to gauge marketing effectiveness. Increasingly, much of this is being placed into the hands of the marketing organization. In fact, a study by Gartner Group said that by 2017 the CMO will spend more money on IT than the CIO. What all of this means is that information, content and data governance need to be considered as part of a whole and not as separate initiatives. Elements of good information governance include: • Deployment and Operationalization • Alignment with User Needs • Business Value • Buy-In and Change Management • Sponsorship and Accountability Leads to the following outcomes: • Manages conflicts in business priorities (between initiatives, business units, drivers, etc) • Allows for ongoing input from various stakeholders and constituencies in order to evolve capabilities with the needs of the business • Prioritizes efforts and allocation of resources • Assigns roles and responsibilities with accountability to critical functions • Takes into consideration various levels of maturity in the organization — no one size fits all • Ensures that investments in systems, processes and tools are providing sufficient return to the business • Balances centralized standards with decentralized decision making • Aligns incentives to use a system with business goals This session will review governance concepts, discuss how they apply to various types of data and content and provide a framework for developing governance processes and structures.},
keywords={Organizations;Standards organizations;Information architecture;Fuels;Information management;Big data},
doi={10.1109/ITPRO.2014.7029280},
ISSN={},
month={May},}
@INPROCEEDINGS{8787092,
author={Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai},
booktitle={2017 5th Intl Conf on Applied Computing and Information Technology/4th Intl Conf on Computational Science/Intelligence and Applied Informatics/2nd Intl Conf on Big Data, Cloud Computing, Data Science (ACIT-CSII-BCD)},
title={A Survey on Big Data Pre-processing},
year={2017},
volume={},
number={},
pages={241-247},
abstract={In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.},
keywords={Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality},
doi={10.1109/ACIT-CSII-BCD.2017.49},
ISSN={},
month={July},}
@ARTICLE{8658160,
author={Singh, Nitin and Lai, Kee-Hung and Vejvar, Markus and Cheng, T. C. E.},
journal={IEEE Engineering Management Review},
title={Big Data Technology: Challenges, Prospects, and Realities},
year={2019},
volume={47},
number={1},
pages={58-66},
abstract={We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.},
keywords={Big Data;Data integrity;Data analysis;Software;Investment;Companies;Big data;business analytics;Hadoop;people;talent gap;implementation},
doi={10.1109/EMR.2019.2900208},
ISSN={1937-4178},
month={Firstquarter},}
@INPROCEEDINGS{8289792,
author={El-Ghafar, Randa M. Abd and Gheith, Mervat H. and El-Bastawissy, Ali H. and Nasr, Eman S.},
booktitle={2017 13th International Computer Engineering Conference (ICENCO)},
title={Record linkage approaches in big data: A state of art study},
year={2017},
volume={},
number={},
pages={224-230},
abstract={Record Linkage aims to find records in a dataset that represent the same real-world entity across many different data sources. It is a crucial task for data quality. With the evolution of Big Data, new difficulties appeared to deal mainly with the 5Vs of Big Data properties; i.e. Volume, Variety, Velocity, Value, and Veracity. Therefore Record Linkage in Big Data is more challenging. This paper investigates ways to apply Record Linkage algorithms that handle the Volume property of Big Data. Our investigation revealed four major issues. First, the techniques used to resolve the Volume property of Big Data mainly depend on partitioning the data into a number of blocks. The processing of those blocks is parallelly distributed among many executers. Second, MapReduce is the most famous programming model that is designed for parallel processing of Big Data. Third, a blocking key is usually used for partitioning the big dataset into smaller blocks; it is often created by the concatenation of the prefixes of chosen attributes. Partitioning using a blocking key may lead to unbalancing blocks, which is known as data skew, where data is not evenly distributed among blocks. An uneven distribution of data degrades the performance of the overall execution of the MapReduce model. Fourth, to the best of our knowledge, a small number of studies has been done so far to balance the load between data blocks in a MapReduce framework. Hence more work should be dedicated to balancing the load between the distributed blocks.},
keywords={Couplings;Big Data;Programming;Data integration;Task analysis;Data models;Databases;Big Data;Big Data Integration;blocking;entity matching;entity resolution;Hadoop;machine learning;MapReduce;Record Linkage},
doi={10.1109/ICENCO.2017.8289792},
ISSN={2475-2320},
month={Dec},}
@INPROCEEDINGS{7840906,
author={Angryk, Rafal A. and Galarus, Douglas E.},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={The SMART approach to comprehensive quality assessment of site-based spatial-temporal data},
year={2016},
volume={},
number={},
pages={2636-2645},
abstract={There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.},
keywords={Sensors;Quality control;Metadata;Meteorology;Interpolation;Quality assessment;Time series analysis;data quality;data stream processing;spatial-temporal data;quality control;interpolation},
doi={10.1109/BigData.2016.7840906},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8241335,
author={Mejía-Lavalle, Manuel and Meusel, Winfrid and Tavira, Jonathan Villanueva and Cruz, Mirian Calderón},
booktitle={2017 International Conference on Mechatronics, Electronics and Automotive Engineering (ICMEAE)},
title={Effective Data Quality Diagnostic Schema for Big Data},
year={2017},
volume={},
number={},
pages={163-168},
abstract={Big Data environment is a computing area with a great growth. Today it is common that we hear about databases with huge volumes of information and also we hear about Data Mining and Business Intelligence projects related with these huge databases. However, in general, little attention has been given to the quality of the data. Here we propose and present innovative metrics and schema designed to perform a basic task related to the Data Quality issue, this is, the diagnostic. The preliminary results that we obtained when we apply our approaches to Big Data encourage us to continue this work.},
keywords={Databases;Big Data;Measurement;Cleaning;Data mining},
doi={10.1109/ICMEAE.2017.29},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9760331,
author={Wang, Shiyang},
booktitle={2022 7th International Conference on Big Data Analytics (ICBDA)},
title={The Prediction Method of KPIs by Using LS-TSVR},
year={2022},
volume={},
number={},
pages={177-180},
abstract={Closely monitoring service performance and making predictions of Key Performance Indicators (KPIs) are critical for Internet-based services. However, fast yet accurate prediction of these seasonal KPIs with various patterns and data quality has been a great challenge. This paper tackles this challenge through a novel approach based on auto-regressive Least Square Twin Support Vector Regression (LS-TSVR). As an improved version of SVR, LS-TSVR can handle big data without any external optimization, and meanwhile, the prediction accuracy is better than that of SVR. For seasonal KPI data in a production dataset, our methods satisfy or approximate a mean average error (MAE) of around 0.013, which is significantly lower than the baseline method.},
keywords={Support vector machines;Data integrity;Key performance indicator;Time series analysis;Production;Prediction methods;Big Data;time series prediction;support vector regression;least square approximation},
doi={10.1109/ICBDA55095.2022.9760331},
ISSN={},
month={March},}
@INPROCEEDINGS{8258218,
author={Colborne, Adrienne and Smit, Michael},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Identifying and mitigating risks to the quality of open data in the post-truth era},
year={2017},
volume={},
number={},
pages={2588-2594},
abstract={Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.},
keywords={Big Data;Meteorology;Portals;Voting;open data;post-truth;fake news;risk identification;risk mitigation;data quality assurance},
doi={10.1109/BigData.2017.8258218},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8406658,
author={Palacio, Ana León and López, Óscar Pastor},
booktitle={2018 12th International Conference on Research Challenges in Information Science (RCIS)},
title={From big data to smart data: A genomic information systems perspective},
year={2018},
volume={},
number={},
pages={1-11},
abstract={During the last two decades, data generated by Next Generation Sequencing Technologies have revolutionized our understanding of human biology and improved the study on how changes (variations) in the DNA are involved in the risk of suffering a certain disease. A huge amount of genomic data is publicly available and frequently used by the research community in order to extract meaningful and reliable gene-disease relationships. However, management of this exponential growth of data has become a challenge for biologists; under such a big data problem perspective, they are forced to delve into a lake of complex data spread in over thousand heterogeneous repositories, represented in multiple formats and with different levels of quality; but when data are used to solve a concrete problem only a small part of that “data lake” is really significant; this is what we call the “smart” data perspective. Using conceptual models and the principles of data quality management, adapted to the genomic domain, we propose a systematic approach to move from a big data to a smart data perspective. The aim of this approach is to populate an Information System with genomic data which must be accessible, informative and actionable enough to extract valuable knowledge.},
keywords={Genomics;Bioinformatics;Big Data;Data integrity;Diseases;Data models;Conceptual Modelling;Data Quality;Big Data;Smart Data;Genomics},
doi={10.1109/RCIS.2018.8406658},
ISSN={2151-1357},
month={May},}
@ARTICLE{9082104,
author={Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash and Lloret, Jaime},
journal={IEEE Transactions on Vehicular Technology},
title={TRADING: Traffic Aware Data Offloading for Big Data Enabled Intelligent Transportation System},
year={2020},
volume={69},
number={7},
pages={6869-6879},
abstract={Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.},
keywords={Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big data;gateway;intelligent transportation systems;VANET;vehicle-to-internet},
doi={10.1109/TVT.2020.2991372},
ISSN={1939-9359},
month={July},}
@INPROCEEDINGS{8901326,
author={Zeyong, Wang and Yutian, Hong and Zhongzheng, Tong},
booktitle={2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Risk Assessment Model and Experimental Analysis of Electric Power Production Based on Big Data},
year={2019},
volume={},
number={},
pages={88-91},
abstract={This paper studies the characteristics of big data of power, and aims at the data quality problems faced by power system. It puts forward an assessment method of power system data quality. Based on the characteristics of large power data, a series of indicators influencing the data are analyzed and hierarchically divided to determine the measurement standard of power production data during the process of risk management, namely, the risk index system. Then, the risk assessment model of power data is established by referring to the assessment model in other fields or the rules of deduction and induction in data mining. It can be used to evaluate the quality of power system data, and find a framework and solution suitable for large data quality assessment. Finally, the model is implemented on Hadoop platform, which proves that it takes into account the completeness of the index system, the objectivity of the assessment method and the rapidity of the calculation method.},
keywords={risk assessment;electric power;fuzzy comprehensive evaluation;Hadoop;index},
doi={10.1109/ICSGEA.2019.00028},
ISSN={},
month={Aug},}
@ARTICLE{7937830,
author={Benbernou, Salima and Huang, Xin and Ouziri, Mourad},
journal={IEEE Transactions on Big Data},
title={Semantic-Based and Entity-Resolution Fusion to Enhance Quality of Big RDF Data},
year={2021},
volume={7},
number={2},
pages={436-450},
abstract={Within an organisation, the quality in big data is a cornerstone to operational, transactional processes and to the reliability of business analytics for decision making. In fact, as organizations are harnessing multi-sources data to rise the benefits of their business, the quality of data becomes important and crucial. This paper presents a new approach to query big data sources using Resource Description Framework (RDF) representation to ensure data quality by harvesting more relevant and complete query results. Our approach handles two important types of heterogeneity over multiple data sources: semantic heterogeneity and URI-based entity identification. It proposes (1) a semantic entity resolution method based on inference mechanism using rules to manage the misunderstanding of data, in real world entities (2) Data Quality enhancement using MapReduce-based query rewriting approach includes the entity resolution results to infer and adds implicit data into query results (3) a parallel combination of MapReduce jobs of saturation and query rewriting inferences to handle transitive and cyclic rules for a richer rules' expression language (4) experiments to assess the efficiency of the proposed approach over real big RDF data originating from insurance and synthetic data sets.},
keywords={Big Data;Resource description framework;Semantics;Joining processes;Erbium;Organizations;Data quality;big data fusion;inferences;entity resolution;query rewriting},
doi={10.1109/TBDATA.2017.2710346},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{7371861,
author={Laranjeiro, Nuno and Soydemir, Seyma Nur and Bernardino, Jorge},
booktitle={2015 IEEE 21st Pacific Rim International Symposium on Dependable Computing (PRDC)},
title={A Survey on Data Quality: Classifying Poor Data},
year={2015},
volume={},
number={},
pages={179-188},
abstract={Data is part of our everyday life and an essential asset in numerous businesses and organizations. The quality of the data, i.e., the degree to which the data characteristics fulfill requirements, can have a tremendous impact on the businesses themselves, the companies, or even in human lives. In fact, research and industry reports show that huge amounts of capital are spent to improve the quality of the data being used in many systems, sometimes even only to understand the quality of the information in use. Considering the variety of dimensions, characteristics, business views, or simply the specificities of the systems being evaluated, understanding how to measure data quality can be an extremely difficult task. In this paper we survey the state of the art in classification of poor data, including the definition of dimensions and specific data problems, we identify frequently used dimensions and map data quality problems to the identified dimensions. The huge variety of terms and definitions found suggests that further standardization efforts are required. Also, data quality research on Big Data appears to be in its initial steps, leaving open space for further research.},
keywords={Standards organizations;Industries;Training;Companies;Decision making;Poor data quality;dirty data;poor data classification;data quality problems},
doi={10.1109/PRDC.2015.41},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8890376,
author={Chen, Chengling and Su, Zhou and Li, Weiwei and Wang, Yuntao},
booktitle={2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
title={Big Data Driven Computing Offloading Scheme with Driverless Vehicles Assistance},
year={2019},
volume={},
number={},
pages={412-416},
abstract={In the era of big data, edge computing is emerged as a promising paradigm to alleviate the pressure on the backbone network and facilitate vehicular services on the road. As edge nodes deployed for vehicular applications, roadside units (RSUs) need to undertake a large number of local computing tasks. However, due to the uncertainty of the vehicular network topology, static RSU deployments are subject to short-term overload and cannot handle various delay-sensitive computing tasks concurrently. To address the problem, we propose a big data driven computing offloading scheme to dispatch idle driverless vehicles to enhance the capacities of RSUs dynamically. First, we present a trust assessment model to evaluate the credibility of driverless vehicles. Then, a multi-attribute reverse auction is applied to maximize the utilities of RSUs and driverless vehicles. In addition, a secure forwarding method is developed to protect the privacy of computing tasks.},
keywords={Task analysis;Computational modeling;Automobiles;Big Data;Quality of service;Privacy;Bandwidth;Vehicular ad-hoc networks, reverse auction, computing offloading, driverless vehicles},
doi={10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00084},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9102068,
author={Wang, Jiye and Li, Yang and Guo, Jian and Cao, Junwei and Hua, Haochen and Xing, Chunxiao and Qi, Caijuan and Pi, Zhixian},
booktitle={2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA)},
title={Data Quality Analysis Framework and Evaluation Methods for Power System Operation with High Proportion of Renewable Energy Penetration},
year={2020},
volume={},
number={},
pages={687-692},
abstract={Global climate crisis in 21st century pushed countries to move towards energy transformation in generation and consumption. To achieve green and low-carbon energy transformation goals, it is necessary that a large number of renewable energy resources such as wind and solar to be consumed. Renewable energy with intermittent fluctuations in time dimension and agglomerations in spatial dimension increases the complexity of green energy consumption friendly. Therefore, comprehensive data and advanced predictive analysis methods are required to guarantee safety of operation and transactions for renewable energy plants and stations. We can even say that quality of renewable energy data determines the accuracy of prediction and analysis. Firstly, this article analyzes the operation and transaction characteristics of distributed renewable energy plants, and data quality analysis framework for distributed renewable energy operations and transactions was built on the new energy cloud platform. Data information were classified into model parameter and status instance, which are related to dispatching and energy power transaction businesses such as equipment model management, operation monitoring and security analysis, measurement statistics etc. The importance between them is determined according to pairwise comparison. Finally, analytic hierarchy process (AHP) theory was applied to calculate weights for data integrity, accuracy, consistency and timeliness, data quality assessment process and calculation methods were designed, and load series data was used to verify its correctness.},
keywords={Data integrity;Renewable energy sources;Data models;Analytical models;Production;Monitoring;Clouds;renewable energy cloud;data quality analysis;AHP;evaluation metrics},
doi={10.1109/ICIEA49774.2020.9102068},
ISSN={},
month={April},}
@INPROCEEDINGS{9298378,
author={Desai, Vinod and H A, Dinesha},
booktitle={2020 IEEE International Conference for Innovation in Technology (INOCON)},
title={A Hybrid Approach to Data Pre-processing Methods},
year={2020},
volume={},
number={},
pages={1-4},
abstract={This is an era of big data, as data is growing exponentially and resources are running out of infrastructure, so it is required to accommodate all the data that gets generated. We collect data in enormous amounts to derive meaningful conclusions, perform effective data analytics and improve decision making. As we don't have enough infrastructures to support data storage for huge volumes, it is needed to clean the data in compulsion. It is a mandatory to carry out a step before doing anything with the data. We call it pre-processing of data and this is carried out in various steps. Pre-processing includes data cleaning, data integration, data filtering, and data transformation and so on. As such preprocessing is not limited to the number of steps or a number of methods or definitive methods. We must innovatively preprocess the data before it is being consumed for data analytics. It has become a responsibility for every data analyst or big data researcher to handpick data for his or her analytics. Considering all these techniques in mind we are proposing a hybrid technique to leverage various algorithms available to pre-process our data along with minor modifications such as at the run time, choosing an algorithm or technique wisely based on the data that we have.},
keywords={Big Data;Data integrity;Error analysis;Data mining;Data analysis;Transforms;Time series analysis;Big Data;Data Pre-processing;Data Quality checks},
doi={10.1109/INOCON50539.2020.9298378},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8258222,
author={Lazar, Alina and Jin, Ling and Spurlock, C. Anna and Wu, Kesheng and Sim, Alex},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Data quality challenges with missing values and mixed types in joint sequence analysis},
year={2017},
volume={},
number={},
pages={2620-2627},
abstract={The goal of this paper is to investigate the impact of missing values in categorical time series sequences on common data analysis tasks. Being able to more effectively identify patterns in socio-demographic longitudinal data is an important component in a number of social science settings. However, performing fundamental analytical operations, such as clustering for grouping these data based on similarity patterns, is challenging due to the categorical and multi-dimensional nature of the data, and their corruption by missing and inconsistent values. To study these data quality issues, we employ longitudinal sequence data representations, a similarity measure designed for categorical and longitudinal data, together with state-of-the art clustering methodologies reliant on hierarchical algorithms. The key to quantifying the similarity and difference among data records is a distance metric. Given the categorical nature of our data, we employ an “edit” type distance using Optimal Matching (OM). Because each data record has multiple variables of different types, we investigate the impact of mixing these variables in a single similarity measure. Between variables with binary values and those with multiple nominal values, we find that the ability to overcome missing data problems is harder in the nominal domain versus the binary domain. Additionally, artificial clusters introduced by the alignment of leading missing values can be resolved by tuning the missing value substitution cost parameter.},
keywords={Trajectory;Sequences;Time series analysis;Education;Employment;joint sequence analysis;optimal matching;missing values;time series clustering;data quality},
doi={10.1109/BigData.2017.8258222},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7432708,
author={Zhang, Lu and Chen, Yanxia and Zhu, Jie and Pan, Mingyu and Sun, Zhou and Wang, Weixian},
booktitle={2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies (DRPT)},
title={Data quality analysis and improved strategy research on operations management system for electric vehicles},
year={2015},
volume={},
number={},
pages={2715-2720},
abstract={It is very important for Operations Management System (OMS) and big data analysis application to improve the data quality of Electric Vehicle (EV) charging service. This paper focuses on the charging transaction record data from the Beijing EV charging OMS, and analyzes error types and distributed locations of the abnormal data. Based on the mathematical logic among various kinds of operation data, the data selection rules and processing method are proposed, and the system improved scheme is given. Through the design and application of data selection and processing module, the abnormal data can be timely detected and corrected. It is also beneficial for the system operation and maintenance to improve the acquisition data quality. The comparative analysis results verify the feasibility and effectivity of the proposed scheme. This research is a necessary guarantee for the big data technology application.},
keywords={Big data;Distributed databases;Decision support systems;Power industry;Electric vehicles;Systems operation;Maintenance engineering;electric vehicle;operations management system;big data;data quality;data selection and processing},
doi={10.1109/DRPT.2015.7432708},
ISSN={},
month={Nov},}
@ARTICLE{8950481,
author={Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang},
journal={Chinese Journal of Electrical Engineering},
title={A missing power data filling method based on improved random forest algorithm},
year={2019},
volume={5},
number={4},
pages={33-39},
abstract={Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.},
keywords={Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality},
doi={10.23919/CJEE.2019.000025},
ISSN={2096-1529},
month={Dec},}
@INPROCEEDINGS{9732098,
author={Wrembel, Robert},
booktitle={2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)},
title={Still Open Problems in Data Warehouse and Data Lake Research: extended abstract},
year={2021},
volume={},
number={},
pages={01-03},
abstract={During recent years, we observe a widespread of new data sources, especially all types of social media and IoT devices, which produce huge data volumes, whose content ranges from fully structured to totally unstructured. All these types of data are commonly referred to as big data. They are typically described by the three most important characteristics, called 3V [1], namely: an extremely large volume, a variety of data models and structures (data representations), as well as a high velocity at which data are generated. We argue that out of these three Vs, the most challenging is variety [2]. Such data need to be integrated and transformed into a common representation, which is suitable for analysis, in a similar manner as traditional (mainly table-like) data.},
keywords={Social networking (online);Soft sensors;Transforms;Data warehouses;Big Data applications;Data models;Security;data integration;data warehouse;data lake;big data;extract transform load;data processing workflow;data processing pipeline;data quality;ETL optimization;data source evolution;metadata},
doi={10.1109/SNAMS53716.2021.9732098},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8882761,
author={Ullah, Faheem and Ali Babar, M.},
booktitle={2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)},
title={QuickAdapt: Scalable Adaptation for Big Data Cyber Security Analytics},
year={2019},
volume={},
number={},
pages={81-86},
abstract={Big Data Cyber Security Analytics (BDCA) leverages big data technologies for collecting, storing, and analyzing a large volume of security events data to detect cyber-attacks. Accuracy and response time, being the most important quality concerns for BDCA, are impacted by changes in security events data. Whilst it is promising to adapt a BDCA system's architecture to the changes in security events data for optimizing accuracy and response time, it is important to consider large search space of architectural configurations. Searching a large space of configurations for potential adaptation incurs an overwhelming adaptation time, which may cancel the benefits of adaptation. We present an adaptation approach, QuickAdapt, to enable quick adaptation of a BDCA system. QuickAdapt uses descriptive statistics (e.g., mean and variance) of security events data and fuzzy rules to (re) compose a system with a set of components to ensure optimal accuracy and response time. We have evaluated QuickAdapt for a distributed BDCA system using four datasets. Our evaluation shows that on average QuickAdapt reduces adaptation time by 105× with a competitive adaptation accuracy of 70% as compared to an existing solution.},
keywords={Big Data;Quality of service;Time factors;Feature extraction;Computer crime;Computer architecture;big data, cyber security, adaptation, accuracy},
doi={10.1109/ICECCS.2019.00016},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7490949,
author={Yuan Gao and Hong Ao and Kang Wang and Weigui Zhou and Yi Li},
booktitle={2015 4th International Conference on Computer Science and Network Technology (ICCSNT)},
title={The diagnosis of wired network malfunctions based on big data and traffic prediction: An overview},
year={2015},
volume={01},
number={},
pages={1204-1208},
abstract={The increasing demand on higher transmission speed and shorter delay in wired networks becomes critical in recent communication networks. However, the capacity of transmission link is limited by the method of transmission. In this paper, aiming at the situation of large scale networks, an overview of the network optimization based on big data and traffic prediction is given in our proposed work. In wired networks, how to make full use of the transmission bandwidth and provide more reliable QoS is in great demand. Based on the network topology in our facility, we make a summary of current diagnosis method of the network and then propose the future possible way to solve the network malfunction based on big data through network log and complex monitors, then we make an overview of the diagnosis method based on traffic prediction, which could effectively make full use of bandwidth and avoid collision of the network.},
keywords={Big data;Quality of service;Market research;Monitoring;Satellites;Delays;Prediction methods;Network Diagnosis;Big Data;Traffic Prediction;Large Scale Network;Complex Network},
doi={10.1109/ICCSNT.2015.7490949},
ISSN={},
month={Dec},}
@ARTICLE{9496639,
author={Khan, Abudul Wahid and Khan, Maseeh Ullah and Khan, Javed Ali and Ahmad, Arshad and Khan, Khalil and Zamir, Muhammad and Kim, Wonjoon and Ijaz, Muhammad Fazal},
journal={IEEE Access},
title={Analyzing and Evaluating Critical Challenges and Practices for Software Vendor Organizations to Secure Big Data on Cloud Computing: An AHP-Based Systematic Approach},
year={2021},
volume={9},
number={},
pages={107309-107332},
abstract={Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors’ organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.},
keywords={Cloud computing;Security;Big Data;Software;Organizations;Social networking (online);STEM;Security challenges;big data;cloud computing;SLR;vendor;SPSS},
doi={10.1109/ACCESS.2021.3100287},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9671890,
author={Geronazzo, Angela and Ziegler, Markus},
booktitle={2021 IEEE International Conference on Big Data (Big Data)},
title={QMLEx: Data Driven Digital Transformation in Marketing Analytics},
year={2021},
volume={},
number={},
pages={5900-5902},
abstract={This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to eliminate the need for the expert input. The methodology is applied to our internal process devoted to creating groups of products with similar features, one of the most relevant use case in marketing analytics.},
keywords={Itemsets;Soft sensors;Digital transformation;Data integrity;Conferences;Big Data;Feature extraction;Digital transformation;entity linking;topic extraction;word embedding;pattern search;frequent itemset mining;data quality},
doi={10.1109/BigData52589.2021.9671890},
ISSN={},
month={Dec},}
@ARTICLE{9320548,
author={Luo, Xin and Chen, Minzhi and Wu, Hao and Liu, Zhigang and Yuan, Huaqiang and Zhou, MengChu},
journal={IEEE Transactions on Automation Science and Engineering},
title={Adjusting Learning Depth in Nonnegative Latent Factorization of Tensors for Accurately Modeling Temporal Patterns in Dynamic QoS Data},
year={2021},
volume={18},
number={4},
pages={2142-2155},
abstract={A nonnegative latent factorization of tensors (NLFT) model precisely represents the temporal patterns hidden in multichannel data emerging from various applications. It often adopts a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm is not adjustable, resulting in frequent training fluctuation or poor model convergence caused by overshooting. To address this issue, this study carefully investigates the connections between the performance of an NLFT model and its learning depth via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Empirical studies on two industrial data sets demonstrate that compared with the state-of-the-art NLFT models, a DNL model achieves significant accuracy gain when performing missing data estimation on a high-dimensional and incomplete tensor with high efficiency. Note to Practitioners—Multichannel data are often encountered in various big-data-related applications. It is vital for a data analyzer to correctly capture the temporal patterns hidden in them for efficient knowledge acquisition and representation. This article focuses on analyzing temporal QoS data, which is a representative kind of multichannel data. To correctly extract their temporal patterns, an analyzer should correctly describe their nonnegativity. Such a purpose can be achieved by building a nonnegative latent factorization of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth is not adjustable, making an NLFT model frequently suffer from severe fluctuations in its training error or even fail to converge. To address this issue, this study carefully investigates the learning rules for an NLFT model’s decision parameters using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly and exponentially, thereby making the learning depth adjustable. Based on it, this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Compared with the existing NLFT models, a DNL model better represents multichannel data. It meets industrial needs well and can be used to achieve high performance in data analysis tasks like temporal-aware missing data estimation},
keywords={Tensors;Big Data;Quality of service;Computational efficiency;Machine learning;Web services;Algorithm;big data;dynamics;high-dimensional and incomplete (HDI) data;machine learning;missing data estimation;multichannel data;nonnegative latent factorization of tensors (NLFT);temporal pattern;quality of service (QoS);web service},
doi={10.1109/TASE.2020.3040400},
ISSN={1558-3783},
month={Oct},}
@INPROCEEDINGS{7510775,
author={Liu, Yunshu and Chen, Xuanyu and Chen, Cailian and Guan, Xingping},
booktitle={2016 IEEE International Conference on Communications (ICC)},
title={Traffic big data analysis supporting vehicular network access recommendation},
year={2016},
volume={},
number={},
pages={1-6},
abstract={With the explosive growth of Internet of Vehicles (IoV), it is undoubted that vehicular demands for real-time Internet access would get a surge in the near future. Therefore, it is foreseeable that the cars within the IoV will generate enormous data. On the one hand, the huge volume of data mean we could get much information (e.g., vehicle's condition and real-time traffic distribution) through the big data analysis. On the other hand, the huge volume of data will overload the cellular network since the cellular infrastructure still represents the dominant access methods for ubiquitous connections. The vehicular ad hoc network (VANET) offloading is a promising solution to alleviate the conflict between the limited capacity of cellular network and big data collection. In a vehicular heterogeneous network formed by cellular network and VANET, an efficient network selection is crucial to ensure vehicles' quality of service. To address this issue, we develop an intelligent network recommendation system supported by traffic big data analysis. Firstly, the traffic model for network recommendation is built through big data analysis. Secondly, vehicles are recommended to access an appropriate network by employing the analytic framework which takes traffic status, user preferences, service applications and network conditions into account. Furthermore an Android application is developed, which enables individual vehicle to access network automatically based on the access recommender. Finally, extensive simulation results show that our proposal can effectively select the optimum network for vehicles, and network resource is fully utilized at the same time.},
keywords={Vehicles;Vehicular ad hoc networks;Roads;Big data;Quality of service;Internet;Real-time systems},
doi={10.1109/ICC.2016.7510775},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{9263243,
author={Yousfi, Aola and El Yazidi, Moulay Hafid and Zellou, Ahmed},
booktitle={2020 International Conference on Advanced Computer Science and Information Systems (ICACSIS)},
title={HASSO: A Highly-Automated Source Selection and Ordering System Based on Data Quality Factors},
year={2020},
volume={},
number={},
pages={155-164},
abstract={Big data integration gives access to a large number of data sources through a unified user interface. Answers include high-quality data, medium-quality data, and low-quality data. Selecting a subset of high accurate and consistent data sources and ordering them appropriately is critical to obtain as many high-quality answers as possible right after querying few data sources. However, the process of selecting and ordering data sources can be quite complicated and can present several challenges. The main challenge faced during that process is identifying the most adequate data quality factors to consider. In this paper, we present HASSO, a Highly-Automated Source Selection and Ordering System based on data quality factors. To produce consistent and high accurate answers, HASSO identifies, for each data source, its domain, data consistency and data accuracy using the schema matches. To maximize the total number of complete and non-redundant answers returned right after querying a small number of data sources, HASSO orders data sources in terms of their data overlap and in a decreasing order of their overall coverage. Experimental results in real-world domains show that HASSO produces high-quality answers at high speed.},
keywords={Computer science;Data integrity;User interfaces;Big Data;Information systems;Source Selection;Source Ordering;Data Quality;Big Data Integration;Semantic Similarity},
doi={10.1109/ICACSIS51025.2020.9263243},
ISSN={2330-4588},
month={Oct},}
@INPROCEEDINGS{9378296,
author={Shrivastava, Shrey and Patel, Dhaval and Zhou, Nianjun and Iyengar, Arun and Bhamidipaty, Anuradha},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={DQLearn : A Toolkit for Structured Data Quality Learning},
year={2020},
volume={},
number={},
pages={1644-1653},
abstract={Data Quality (DQ) has been one of the key focuses as Data Analytics and Artificial Intelligence (AI) fields continue to grow. Yet, data quality analysis has mostly been a disjointed, ad-hoc, and cumbersome process in the overall data analysis workflow. There have been ongoing attempts to formalize this process, but the solutions that have come out are not universally applicable. Most of the proposed solutions try to address the problem of data quality from a limited perspective and suc-cessfully address only a subset of all challenges. These solutions fail to translate to other domains due to a lack of structure. In this paper, we present DQLearn, a toolkit for structured data quality learning. We start by presenting the core principle on which we build our library and introduce the four components that provide a solid base to address the needs of the data quality problem. Then, we showcase our automation structure - "Workflows", and the two optimization techniques equipped with it, that help the users to structure their learning problem very easily. Next, we discuss four important scenarios of the DQ Workflows in the overall life-cycle. Finally, we demonstrate the utility of the proposed toolkit with public datasets and show benchmark results from optimization experiments.},
keywords={Data analysis;Automation;Data integrity;Optimization methods;Big Data;Solids;Task analysis},
doi={10.1109/BigData50022.2020.9378296},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9357200,
author={Ezzine, Imane and Benhlima, Laila},
booktitle={2020 6th IEEE Congress on Information Science and Technology (CiSt)},
title={Technology against COVID-19 A Blockchain-based framework for Data Quality},
year={2020},
volume={},
number={},
pages={84-89},
abstract={The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.},
keywords={COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big Data;Data Quality;data governance},
doi={10.1109/CiSt49399.2021.9357200},
ISSN={2327-1884},
month={June},}
@INPROCEEDINGS{6816764,
author={Saha, Barna and Srivastava, Divesh},
booktitle={2014 IEEE 30th International Conference on Data Engineering},
title={Data quality: The other face of Big Data},
year={2014},
volume={},
number={},
pages={1294-1297},
abstract={In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the “data to speak for itself” in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.},
keywords={Information management;Data handling;Data storage systems;Databases;Maintenance engineering;Quality management;Cleaning},
doi={10.1109/ICDE.2014.6816764},
ISSN={2375-026X},
month={March},}
@INPROCEEDINGS{7363987,
author={Priebe, Torsten and Markus, Stefan},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Business information modeling: A methodology for data-intensive projects, data science and big data governance},
year={2015},
volume={},
number={},
pages={2056-2065},
abstract={This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.},
keywords={Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog},
doi={10.1109/BigData.2015.7363987},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8771733,
author={Radhakrishnan, Asha and Das, Sarasij},
booktitle={2018 20th National Power Systems Conference (NPSC)},
title={Quality Assessment of Smart Grid Data},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Enormous amount of data gets generated in the Smart Grids (SGs) due to the large number of measuring devices, higher measurement rates and various types of sensors. Smart grid data contains important and critical information about the grid. Data driven applications are being developed for better planning, monitoring and operation of SGs. The outcome of data analytics heavily depends on the quality of SG data. However, not much work has been reported on the quality assessment of SG data. This paper addresses the objective assessment of SG data quality. Various dimensions of SG data quality are identified in this paper. Mathematical formulations are proposed to quantify the SG data quality. Proposed data quality metrics have been applied on the SCADA and PMU measurements collected from the Southern Regional Grid of India to demonstrate their effectiveness.},
keywords={Data integrity;Smart grids;Phasor measurement units;Big Data;Quality assessment;Big data;data quality;power system;smart grid},
doi={10.1109/NPSC.2018.8771733},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8997699,
author={Pan, Lingling and Liu, Jun and Li, Feng},
booktitle={2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
title={Multi-dimensional Index Construction of Electric Power Multi-source Measurement Data considering Spatio-temporal Correlation},
year={2019},
volume={1},
number={},
pages={2386-2390},
abstract={The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.},
keywords={Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional index},
doi={10.1109/IAEAC47372.2019.8997699},
ISSN={2381-0947},
month={Dec},}
@INPROCEEDINGS{9232648,
author={Hossen, Md Ismail and Goh, Michael and Hossen, Abid and Rahman, Md. Armanur},
booktitle={2020 11th IEEE Control and System Graduate Research Colloquium (ICSGRC)},
title={A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools and Trends Towards Cleaning Dirty Data},
year={2020},
volume={},
number={},
pages={209-213},
abstract={The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.},
keywords={Data integrity;Tools;Companies;Cleaning;Task analysis;Machine learning;Regulation;E-business;Big data;data quality;dirty data;machine learning},
doi={10.1109/ICSGRC49013.2020.9232648},
ISSN={},
month={Aug},}
@ARTICLE{8293821,
author={Li, Xiaoyong and Yuan, Jie and Ma, Huadong and Yao, Wenbin},
journal={IEEE Transactions on Information Forensics and Security},
title={Fast and Parallel Trust Computing Scheme Based on Big Data Analysis for Collaboration Cloud Service},
year={2018},
volume={13},
number={8},
pages={1917-1931},
abstract={Providing high trustworthy service is the most fundamental task for any cloud computing platform. Users are willing to deliver their computing tasks and the most sensitive data to cloud data centers, which is based on the trust relationship established between users and cloud service providers. However, with the development of collaboration cloud computing, how to provider fast response for a large number of users' service requests becomes a challenging problem. In order to quickly provide highly trustworthy services, the service platform must efficiently and quickly reply tens of millions of service requests, and automatically match-make tens of thousands of service resources. In this context, lightweight and fast (high-speed, low-overhead) trust computing schemes become the fundamental demand for implementing a trustworthy and collaborative cloud service. In this paper, we propose an innovative and parallel trust computing scheme based on big data analysis for the trustworthy cloud service environment. First, a distributed and modular perceiving architecture for large-scale virtual machines' service behavior is proposed relying on distributed monitoring agents. Then, an adaptive, lightweight, and parallel trust computing scheme is proposed for big monitored data. To the best of our knowledge, this paper is the first to use a blocked and parallel computing mechanism, the speed of trust calculation is greatly accelerated, which makes this trust computing scheme very suitable for a large-scale cloud computing environment. Performance analysis and experimental results verify feasibility and effectiveness of the proposed scheme.},
keywords={Cloud computing;Collaboration;Monitoring;Security;Big Data;Quality of service;Computer architecture;Cloud computing;service behavior monitoring;trust computing;big data analysis},
doi={10.1109/TIFS.2018.2806925},
ISSN={1556-6021},
month={Aug},}
@ARTICLE{7452294,
author={Cherubini, Giovanni and Jelitto, Jens and Venkatesan, Vinodh},
journal={Computer},
title={Cognitive Storage for Big Data},
year={2016},
volume={49},
number={4},
pages={43-51},
abstract={Storage system efficiency can be significantly improved by determining the value of data. A key concept is cognitive storage, or optimizing storage systems by better comprehending the relevance of data to user needs and preferences. The Web extra at https://youtu.be/P-ZxlTLwzTI is a video of authors Giovanni Cherubini and Vinodh Venkatesan of IBM Research--Zurich discussing the concepts, applications, and benefits of cognitive storage for big data.},
keywords={Energy efficiency;Performance evaluation;Storage area networks;Big data;Quality of service;Context modeling;Data storage aystems;mass storage;big data;storage management;value of information;repositories;autonomous systems;content analysis;indexing},
doi={10.1109/MC.2016.117},
ISSN={1558-0814},
month={Apr},}
@INPROCEEDINGS{7009043,
author={Pankowska, Malgorzata},
booktitle={International Conference on Information Society (i-Society 2014)},
title={Service science facing Big Data},
year={2014},
volume={},
number={},
pages={207-212},
abstract={The Big Data is a modification of the traditional view of information organization, particularly view of the data warehouses and databases. Nowadays, business organizations must address a mix of structured, unstructured and streaming data that supports queries and reports. Business recognized the wealth of untapped information in open social media data. Therefore, the goal of this paper is to present the procedural approach on how to cope with massive data sets' management. The proposal included in this paper covers service science application.},
keywords={Big data;Quality of service;Monitoring;Decision making;Organizations;Data analysis;Big Data;information governance;service science;Service Level Management;service quality},
doi={10.1109/i-Society.2014.7009043},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8991080,
author={Guo, Peng and Yang, Guosheng and Wang, Wenhuan and Yan, Zhoutian and Zhang, Lie and Zhang, Hanfang},
booktitle={2019 International Conference on Electronic Engineering and Informatics (EEI)},
title={Relay Protection Data Integrity Check Method Based on Big Data Association Algorithm},
year={2019},
volume={},
number={},
pages={506-508},
abstract={Relay protection big data creates good conditions for the improvement of professional applications, and data integrity is an important aspect that reflects data quality. The association of relay protection big data is intense. This paper applies Apriori algorithm to mine data relevance and generate association rules. Based on this, the integrity of relay protection data is checked, and the incomplete data is predicted. Taking the relay protection defect data as an example, the paper explores the correlation among 251 items of the six dimensions of protection relay defect data such as type of protection, the severity of the defect, whether the protection is out of operation, the defect location, the cause of the defect, and the equipment manufacturer, completing the processing of incomplete data with great application results.},
keywords={Apriori algorithm;relay protection;defect data;integrity check},
doi={10.1109/EEI48997.2019.00115},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9251206,
author={Jiang, Yushan and Liu, Yongxin and Liu, Dahai and Song, Houbing},
booktitle={2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
title={Applying Machine Learning to Aviation Big Data for Flight Delay Prediction},
year={2020},
volume={},
number={},
pages={665-672},
abstract={Flight delay has been a serious and widespread problem that needs to be solved. One promising solution is the flight delay prediction. Although big data analytics and machine learning have been applied successfully in many domains, their applications in aviation are limited. This paper presents a comprehensive study of flight delay spanning data pre-processing, data visualization and data mining, in which we develop several machine learning models to predict flight arrival delays. Two data sets were used, namely Airline On-Time Performance (AOTP) Data and Quality Controlled Local Climatological Data (QCLCD). This paper aims to recognize useful patterns of the flight delay from aviation data and perform accurate delay prediction. The best result for flight delay prediction (five classes) using machine learning models is 89.07% (Multilayer Perceptron). A Convolution neural network model is also built which is enlightened by the idea of pattern recognition and success of neural network method, showing a slightly better result with 89.32% prediction accuracy.},
keywords={Atmospheric modeling;Neural networks;Machine learning;Predictive models;Big Data;Delays;Quantum cascade lasers;Flight Delay;Machine Learning;Aviation Data Analytics},
doi={10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00114},
ISSN={},
month={Aug},}
@ARTICLE{9407288,
author={Hampson, Gary and Hargreaves, Neil and Jakubowicz, Helmut and Williams, Gareth and Hatton, Les},
journal={IEEE Software},
title={Open Collaboration, Data Quality, and COVID-19},
year={2021},
volume={38},
number={3},
pages={137-141},
abstract={The flavor of this "Impact" department is somewhat different. In a pandemic, everybody has to come together. In April 2020, a call went out in the United Kingdom for groups to informally form and collaborate to study this brutal pathogen in whatever way they could. The five authors of this article, old friends from the geophysical industry with decades of experience in numerical modeling and big data, formed such a group.},
keywords={Industries;Pathogens;Pandemics;Data integrity;Collaboration;Data models;Numerical models},
doi={10.1109/MS.2021.3056642},
ISSN={1937-4194},
month={May},}
@INPROCEEDINGS{9006422,
author={Grueneberg, K. and Calo, S. and Dewan, P. and Verma, D. and O’Gorman, Tristan},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={A Policy-based Approach for Measuring Data Quality},
year={2019},
volume={},
number={},
pages={4025-4031},
abstract={With the growing importance of data in all aspects of the functioning of an enterprise, having good quality of data is crucial in support of business processes. However, there do not exist good metrics to measure the quality of data that is available within an enterprise. While there are several data quality standards, their complexity and their required customization makes them difficult to use in real-world industrial scenarios. In this paper, we discuss the challenges encountered in measuring data quality within asset management systems. We propose a policy-based approach for measuring data quality, and show how such an approach can be customized and interpreted easily by practitioners in the field.},
keywords={Data integrity;Standards;Measurement;Asset management;Data models;Complexity theory;Data Quality;Asset Management Systems;Policy based Data Management},
doi={10.1109/BigData47090.2019.9006422},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6969470,
author={Buyya, Rajkumar and Murray, Derek},
booktitle={2014 IEEE International Parallel & Distributed Processing Symposium Workshops},
title={HPGC Keynotes},
year={2014},
volume={},
number={},
pages={850-851},
abstract={These keynote speeches discuss the following: Market-oriented cloud computing and Big Data applications; and Low-latency distributed analytics in Naiad.},
keywords={Cloud computing;Computational modeling;Big data;Quality of service;Distributed processing;Conferences;Educational institutions},
doi={10.1109/IPDPSW.2014.234},
ISSN={},
month={May},}
@INPROCEEDINGS{7545062,
author={Tu, Shouzhong and Huang, Minlie},
booktitle={2016 IEEE Second International Conference on Multimedia Big Data (BigMM)},
title={Scalable Functional Dependencies Discovery from Big Data},
year={2016},
volume={},
number={},
pages={426-431},
abstract={Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases. The discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. There has been growing interest in the problem of functional dependencies discovery in the last ten years. However, existing functional dependencies discovery algorithms are mainly applied to centralized small data. It is far more challenging to discover functional dependencies from big data. In this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. We prune candidate FDs at each node by local fragmented data and batch verify candidate FDs in parallel. Load balance is taken into account when discovering functional dependencies. Experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.},
keywords={Distributed databases;Big data;Lattices;Algorithm design and analysis;Partitioning algorithms;Knowledge discovery;Functional dependencies;Discovering functional dependencies;Knowledge discovery;Big data},
doi={10.1109/BigMM.2016.63},
ISSN={},
month={April},}
@INPROCEEDINGS{7877056,
author={Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Marín-Tordera, Eva},
booktitle={2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)},
title={Towards a Comprehensive Data LifeCycle Model for Big Data Environments},
year={2016},
volume={},
number={},
pages={100-106},
abstract={A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.},
keywords={Data models;Adaptation models;Biological system modeling;Big Data;Organizations;Data acquisition;Computational modeling;Big Data;Data LifeCycle;Data Management;Data Organization;Data Complexity;Vs Challenges},
doi={},
ISSN={},
month={Dec},}
@ARTICLE{8667006,
author={Xu, Xuefang and Lei, Yaguo and Li, Zeda},
journal={IEEE Transactions on Industrial Electronics},
title={An Incorrect Data Detection Method for Big Data Cleaning of Machinery Condition Monitoring},
year={2020},
volume={67},
number={3},
pages={2326-2336},
abstract={The presence of incorrect data leads to the decrease of condition-monitoring big data quality. As a result, unreliable or misleading results are probably obtained by analyzing these poor-quality data. In this paper, to improve the data quality, an incorrect data detection method based on an improved local outlier factor (LOF) is proposed for data cleaning. First, a sliding window technique is used to divide data into different segments. These segments are considered as different objects and their attributes consist of time-domain statistical features extracted from each segment, such as mean, maximum and peak-to-peak value. Second, a kernel-based LOF (KLOF) is calculated using these attributes to evaluate the degree of each segment being incorrect data. Third, according to these KLOF values and a threshold value, incorrect data are detected. Finally, a simulation of vibration data generated by a defective rolling element bearing and three real cases concerning a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to verify the effectiveness of the proposed method, respectively. The results demonstrate that the proposed method is able to detect both missing segments and abnormal segments, which are two typical incorrect data, effectively, and thus is helpful for big data cleaning of machinery condition monitoring.},
keywords={Big Data;Machinery;Feature extraction;Condition monitoring;Data integrity;Fault diagnosis;Cleaning;Condition-monitoring big data;data cleaning;data quality;incorrect data;local outlier factor (LOF)},
doi={10.1109/TIE.2019.2903774},
ISSN={1557-9948},
month={March},}
@INPROCEEDINGS{9140534,
author={Molinari, Andrea and Nollo, Giandomenico},
booktitle={2020 IEEE 20th Mediterranean Electrotechnical Conference ( MELECON)},
title={The quality concerns in health care Big Data},
year={2020},
volume={},
number={},
pages={302-305},
abstract={Health information technology is showing an impressive growing interest towards Big Data. Big Data Analytics is expected to bring important achievements for building sophisticated models, methods and tools that are expected to improve healthcare services and citizen health and wellbeing. In spite of these expectations data quality and analytics methods are not getting the attention they deserve. In this short paper, we aimed to highlight the issues of data quality in the context of Big Data Healthcare Analytics. The common sources of errors, the consequence of these errors, and potential solutions that should be considered to mitigate errors and pitfalls are discussed in the healthcare context.},
keywords={Big Data;Medical services;Data integrity;Biomedical monitoring;Business;Big Data;Analytics;healthcare quality;entity reconciliation},
doi={10.1109/MELECON48756.2020.9140534},
ISSN={2158-8481},
month={June},}
@INPROCEEDINGS{7818437,
author={Jain, Shashwat and Khandelwal, Manish and Katkar, Ashutosh and Nygate, Joseph},
booktitle={2016 12th International Conference on Network and Service Management (CNSM)},
title={Applying big data technologies to manage QoS in an SDN},
year={2016},
volume={},
number={},
pages={302-306},
abstract={Managing QoS in a telecommunications network is a complex process. Effective network design and sizing in conjunction with load balancing, access control and traffic prioritization need to be orchestrated to optimize CAPEX investment, maximize network utilization and ensure that performance metrics and SLAs are met. This work shows how big data analytics were used to improve the management of QoS in an SDN by performing multi-dimensional analysis of Key Performance Indicators (KPIs) and applying machine learning algorithms to discover new correlations, perform root cause analysis and predict traffic congestion.},
keywords={Delays;Correlation;Quality of service;Jitter;Big data;Ports (Computers)},
doi={10.1109/CNSM.2016.7818437},
ISSN={2165-963X},
month={Oct},}
@INPROCEEDINGS{8784484,
author={Zhang, Xupeng and Liang, Du},
booktitle={2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)},
title={Construction of Elevator Inspection Quality Evaluation System Based on Big Data},
year={2019},
volume={},
number={},
pages={238-242},
abstract={Elevator inspection information has typical big data characteristics. This paper points out that the elevator inspection data introduces the method of elevator inspection big data analysis. Taking elevator inspection as an example, it lists several kinds of big data analysis methods for inspection data, including the risk points describing the basic information of the elevator, the scanning inspection process and the inspection quality. Based on frequency analysis of active factors, outlier test, quality assessment, correlation analysis. Using big data technology, it can make statistical analysis on the data obtained by elevator inspection, make the inspection situation more intuitive, help the management organization to understand the overall elevator quality and elevator inspection, and build an elevator inspection quality evaluation system to make the work more transparent and management more precise. Find more accurate questions, deeper supervision, and more scientific government decisions.},
keywords={Elevators;Inspection;Big Data;Safety;Market research;Testing;Monitoring;Elevator;inspection;big data;quality evaluation},
doi={10.1109/ICEIEC.2019.8784484},
ISSN={2377-844X},
month={July},}
@INPROCEEDINGS{7011535,
author={Liu, Wanting and Peng, Yonghong and Tobin, Desmond J},
booktitle={2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)},
title={Integrated analytics of microarray big data reveals robust gene signature},
year={2014},
volume={},
number={},
pages={1-7},
abstract={The advance of high throughput biotechnology enables the generation of large amount of biomedical data. The microarray is increasingly a popular approach for the detection of genome-wide gene expression. Microarray data have thus increased significantly in public accessible database repositories, which provide valuable big data for scientific research. To deal with the challenge of microarray big data collected in different research labs using different experimental set-ups and on different bio-samples, this paper presents a primary study to evaluate the impact of two important factors (the origin of bio-samples and the quality of microarray data) on the integrated analytics of multiple microarray data. The aim is to enable the extraction of reliable and robust gene biomarkers from microarray big data. Our work showed that in order to enhance biomarker discovery from microarray big data (i) it is necessary to treat the microarray data differently in terms of their quality, (ii) it is recommended to stratifying (i.e., sub-group) the data according to the origin of bio-samples in the analytics.},
keywords={Accuracy;Diseases;Big data;Robustness;Educational institutions;Bioinformatics;Malignant tumors;Microarray;integrated analytics;biomarkers},
doi={10.1109/CIBD.2014.7011535},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8539254,
author={Hossen, J. and Jesmeen H, M. Z. and Sayeed, Shohel},
booktitle={2018 7th International Conference on Computer and Communication Engineering (ICCCE)},
title={Modifying Cleaning Method in Big Data Analytics Process using Random Forest Classifier},
year={2018},
volume={},
number={},
pages={208-213},
abstract={Accurate data is a key success factor influencing the performance of data analytics results, especially for the detection and prediction purpose. Nowadays, Big Data analytics (BDA) is used to analyze the sheer volume of data available in an organization. These data quality must be maintained in order to obtain correct alert and valuable insights from the rapidly changing data of high volume, velocity, variety, veracity, and value. This paper aim is to modify existing framework of big data analytics by improving an important step in pre-processing (i.e. Data Cleaning). Initially, feature selection based on Random Forest is used to extract effective features. Then, two classifier algorithms (i.e. Random Forest classifier and Linear SVM classifier) are applied to train using the dataset to classify data quality and to develop an intelligent model. In evaluation, our experimental results show a consistent accuracy of Random Forest and Linear Regression around 90%. Using this approach, we expect to provide a set of cleaned data for further processing. Besides, analysts can benefit from this system in data analytical process in cleaning stage and conclude that the data is cleaned. Finally, a comparison is presented between available functions which are used to handle missing values with the developed system.},
keywords={Forestry;Support vector machines;Data models;Cleaning;Feature extraction;Training;Big Data;Big data;Data Analytics;missing data;data cleaning;Machine learning;Random Forest;Gini Index},
doi={10.1109/ICCCE.2018.8539254},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8117153,
author={ur Rehman, Shafiq and Hark, Andre and Gruhn, Volker},
booktitle={2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)},
title={A framework to handle big data for cyber-physical systems},
year={2017},
volume={},
number={},
pages={72-78},
abstract={The use of big data for cyber-physical systems (CPS) is gaining more importance due to the ever-increasing amount of collectable data. Due to the decreasing cost of sensors and the growth of embedded systems, which are increasingly used in the industries as well as in the private sectors, new methods are needed to evaluate and process the collected data. Therefore, in this paper we proposed a framework to handle big data for cyber-physical systems. The framework considered the possible solutions that would be standardization, cloud computing, online and data stream learning, a methodology to process data and multi-agent systems for CPS. Furthermore, we examine the security challenges and big data issues of cyber-physical systems.},
keywords={Big Data;Cyber-physical systems;Safety;Real-time systems;Sensors;Big data;cyber-physical system (CPS);security;real-time;standardization;infrastructure;data quality},
doi={10.1109/IEMCON.2017.8117153},
ISSN={},
month={Oct},}
@ARTICLE{8809689,
author={Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre, Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and Kozievitch, Nádia P. and Fonseca, Keiko Verônica Ono and Antunes, Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira, Wagner and Aloisio, Giovanni},
journal={IEEE Access},
title={An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management},
year={2019},
volume={7},
number={},
pages={117652-117677},
abstract={Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.},
keywords={Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed environment;public transport management;smart city},
doi={10.1109/ACCESS.2019.2936941},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9480634,
author={Elsahlamy, Ebtsam and Eshra, Abeer and Eshra, Nadia and El-Fishawy, Nawal},
booktitle={2021 International Conference on Electronic Engineering (ICEEM)},
title={Empowering GIS with Big Data: A review of recent advances},
year={2021},
volume={},
number={},
pages={1-7},
abstract={In the past few decades, the use of geographic information systems (GIS) was efficient with servers that could handle the amount of data used. However, as geographical big data grows in size and complexity, storing, managing, processing, analyzing, visualizing, and confirming data quality becomes more difficult. Academia, industry, government, and other institutions are increasingly interested in this information. It’s known as Big Data. Since that kind of data recently became massive, there was a need to develop methods to deal with big data and analyze it to keep pace with development. In this paper, we review the previous studies that involve both Big Data and GIS in different applications. Moreover, we focus on the field of agriculture, which is considered one of the most important sources of the economy. Produced results in this research area help decision-makers to make sound executive steps to reach better production.},
keywords={Government;Data visualization;Production;Big Data;Agriculture;Mobile handsets;Servers;GIS;Big-Data;Geospatial;Agriculture;map-reduce},
doi={10.1109/ICEEM52022.2021.9480634},
ISSN={},
month={July},}
@INPROCEEDINGS{6906820,
author={Papageorgiou, Apostolos and Zahn, Manuel and Kovacs, Ernö},
booktitle={2014 IEEE International Congress on Big Data},
title={Auto-configuration System and Algorithms for Big Data-Enabled Internet-of-Things Platforms},
year={2014},
volume={},
number={},
pages={490-497},
abstract={Internet of Things (IoT) platforms that handle Big Data might perform poorly or not according to the goals of their operator (in terms of costs, database utilization, data quality, energy-efficiency, throughput) if they are not configured properly. The latter configuration refers mainly to system parameters of the data-collecting gateways, e.g., polling intervals, capture intervals, encryption schemes, used protocols etc. However, re-configuring the platform appropriately upon changes of the system context or the operator targets is currently not taking place. This happens because of the complexity or unawareness of the synergies between system configurations and various aspects of the Big Data-handling IoT platform, but also because of the human resources that an efficient re-configuration would require. This paper presents an auto-configuration solution based on interpretable configuration suggestions, focusing on the algorithms for computing the mentioned suggested configurations. Five such algorithms are contributed, while a thorough evaluation reveals which of these algorithms should be used in different operation scenarios in order to achieve high fulfillment of the operator's targets.},
keywords={Logic gates;Big data;Measurement;Heuristic algorithms;Complexity theory;Standards;Optimization;M2M;IoT;configuration;gateway;autonomic;self-management},
doi={10.1109/BigData.Congress.2014.78},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{8422106,
author={Meng, Qianyu and Wang, Kun and Liu, Bo and Miyazaki, Toshiaki and He, Xiaoming},
booktitle={2018 IEEE International Conference on Communications (ICC)},
title={QoE-Based Big Data Analysis with Deep Learning in Pervasive Edge Environment},
year={2018},
volume={},
number={},
pages={1-6},
abstract={In the age of big data, the services in pervasive edge environment are expected to offer end-users better Quality of Experience (QoE) than that in a normal edge environment. Nevertheless, various types of edge devices with storage, delivery, and sensing are coming into our environment and produce the high-dimensional big data accompanied by a volume of pervasive big data increasingly with a lot of redundancy. Therefore, the satisfaction of QoE becomes the primary challenge in high dimensional big data on the basis of pervasive edge environment. In this paper, we first propose a QoE model to evaluate the quality of service in pervasive edge environment. The value of QoE does not only include the accurate data, but also the transmission rate. Then, on the basis of the accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on Deep Learning, which is suitable for pervasive edge environment with high-dimensional big data analysis. Simulation results reveal that our proposals could achieve high QoE performance.},
keywords={Big Data;Quality of experience;Training;Tensile stress;Machine learning;Data models;Quality of service},
doi={10.1109/ICC.2018.8422106},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{9149151,
author={Guo, Weisi},
booktitle={ICC 2020 - 2020 IEEE International Conference on Communications (ICC)},
title={Partially Explainable Big Data Driven Deep Reinforcement Learning for Green 5G UAV},
year={2020},
volume={},
number={},
pages={1-7},
abstract={UAV enabled terrestrial wireless networks enables targeted user-centric service provisioning to en-richen both deep urban coverage and target various rural challenge areas. However, UAVs have to balance the energy consumption of flight with the benefits of wireless capacity delivery via a high dimensional optimisation problem. Classic reinforcement learning (RL) cannot meet this challenge and here, we propose to use deep reinforcement learning (DRL) to optimise both aggregate and minimum service provisioning. In order to achieve a trusted autonomy, the DRL agents have to be able to explain its actions for transparent human-machine interrogation. We design a Double Dueling Deep Q-learning Neural Network (DDDQN) with Prioritised Experience Replay (PER) and fixed Q-targets to achieve stable performance and avoid over-fitting, offering performance gains over naive DQN algorithms. We then use a big data driven case study and found that UAVs battery size determines the nature of its autonomous mission, ranging from an efficient exploiter of one hotspot (100% reward gain) to a stochastic explorer of many hotspots (60-150% reward gain). Using a variety of telecom and social media data, we infer driving Quality-of-Experience (QoE) and Quality-of-Service (QoS) metrics that are in contention with UAV power and communication constraints. Our greener UAVs (30-40% energy saved) address both quantitative QoS and qualitative QoE issues. Partial interpretability in the reinforcement learning is achieved using data features extracted in the hidden layers, offering an initial step for explainable AI (XAI) connecting machine intelligence with human expertise.},
keywords={5G mobile communication;Batteries;Wireless communication;Big Data;Machine learning;Optimization;big data;machine learning;deep reinforcement learning;radio resource management;UAV;energy efficiency;XAI},
doi={10.1109/ICC40277.2020.9149151},
ISSN={1938-1883},
month={June},}
@INPROCEEDINGS{7996546,
author={Gu, Liqiu and Wang, Kun and Liu, Xiulong and Guo, Song and Liu, Bo},
booktitle={2017 IEEE International Conference on Communications (ICC)},
title={A reliable task assignment strategy for spatial crowdsourcing in big data environment},
year={2017},
volume={},
number={},
pages={1-6},
abstract={With the ubiquitous deployment of the mobile devices with increasingly better communication and computation capabilities, an emerging model called spatial crowdsourcing is proposed to solve the problem of unstructured big data by publishing location-based tasks to participating workers. However, massive spatial data generated by spatial crowdsourcing entails a critical challenge that the system has to guarantee quality control of crowdsourcing. This paper first studies a practical problem of task assignment, namely reliability aware spatial crowdsourcing (RA-SC), which takes the constrained tasks and numerous dynamic workers into consideration. Specifically, the worker confidence is introduced to reflect the completion reliability of the assigned task. Our RA-SC problem is to perform task assignments such that the reliability under budget constraints is maximized. Then, we reveal the typical property of the proposed problem, and design an effective strategy to achieve a high reliability of the task assignment. Besides the theoretical analysis, extensive experimental results also demonstrate that the proposed strategy is stable and effective for spatial crowdsourcing.},
keywords={Crowdsourcing;Big Data;Reliability theory;Sensors;Computational modeling;Measurement;Big data;crowdsourcing;task assignment},
doi={10.1109/ICC.2017.7996546},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{8421858,
author={Jiang, Wei and Ning, Xiuli and Xu, Yingcheng},
booktitle={2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)},
title={Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods},
year={2018},
volume={},
number={},
pages={95-102},
abstract={Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.},
keywords={Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution},
doi={10.1109/CSCloud/EdgeCom.2018.00025},
ISSN={},
month={June},}
@ARTICLE{8641478,
author={Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong},
journal={IEEE Access},
title={One-Pass Inconsistency Detection Algorithms for Big Data},
year={2019},
volume={7},
number={},
pages={22377-22394},
abstract={Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.},
keywords={Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency detection;big data;one-pass algorithm;data quality;denial constraint},
doi={10.1109/ACCESS.2019.2898707},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8258223,
author={Müller, Daniel and Te, Yiea-Funk and Jain, Pratiksha},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Improving data quality through high precision gender categorization},
year={2017},
volume={},
number={},
pages={2628-2636},
abstract={First name to gender mappings have been widely recognized as a critical tool to complete, study and validate data records in a range of different areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 6 million people, provided by a car insurance. We then study how naming conventions have changed over time and how they differ by nationality. Second, we build a probabilistic first name to gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in a two label and three label setting and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies' outcomes and find that our mapping produces high precision results. We validate that the additional information of nationality and year of birth improve the recall scores of name to gender mappings. Therefore, it constitutes an efficient process to improve data quality of organizations' records, whenever the attribute gender is missing or unreliable.},
keywords={Organizations;Patents;Databases;Systematics;Pragmatics;data quality improvement;hot deck imputation;record completion;gender name mapping;patenting},
doi={10.1109/BigData.2017.8258223},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8035055,
author={Albertoni, Riccardo and De Martino, Monica and Quarati, Alfonso},
booktitle={2017 International Conference on High Performance Computing & Simulation (HPCS)},
title={Linked Thesauri Quality Assessment and Documentation for Big Data Discovery},
year={2017},
volume={},
number={},
pages={37-44},
abstract={Thesauri are knowledge systems which may ease Big Data access, fostering their integration and re-use. Currently several Linked Data thesauri covering multi-disciplines are available. They provide a semantic foundation to effectively support cross-organization and cross-disciplinary management and usage of Big Data. Thesauri effectiveness is affected by their quality. Diverse quality measures are available taking into account different facets. However, an overall measure is needed to compare several thesauri and to identify those more qualified for a proper reuse. In this paper, we propose a Multi Criteria Decision Making based methodology for the documentation of the quality assessment of linked thesauri as a whole. We present a proof of concept of the Analytic Hierarchy Process adoption to the set of Linked Data thesauri for the Environment deployed in LusTRE. We discuss the step-by-step practice to document the overall quality measurements, generated by the quality assessment, with the W3C promoted Data Quality Vocabulary.},
keywords={Thesauri;Metadata;Vocabulary;Measurement;Quality assessment;Big Data;quality;linked data;thesauri;AHP;metadata;DQV},
doi={10.1109/HPCS.2017.16},
ISSN={},
month={July},}
@INPROCEEDINGS{7363818,
author={Bonner, Stephen and McGough, Andrew Stephen and Kureshi, Ibad and Brennan, John and Theodoropoulos, Georgios and Moss, Laura and Corsar, David and Antoniou, Grigoris},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Data quality assessment and anomaly detection via map/reduce and linked data: A case study in the medical domain},
year={2015},
volume={},
number={},
pages={737-746},
abstract={Recent technological advances in modern healthcare have lead to the ability to collect a vast wealth of patient monitoring data. This data can be utilised for patient diagnosis but it also holds the potential for use within medical research. However, these datasets often contain errors which limit their value to medical research, with one study finding error rates ranging from 2.3%-26.9% in a selection of medical databases. Previous methods for automatically assessing data quality normally rely on threshold rules, which are often unable to correctly identify errors, as further complex domain knowledge is required. To combat this, a semantic web based framework has previously been developed to assess the quality of medical data. However, early work, based solely on traditional semantic web technologies, revealed they are either unable or inefficient at scaling to the vast volumes of medical data. In this paper we present a new method for storing and querying medical RDF datasets using Hadoop Map / Reduce. This approach exploits the inherent parallelism found within RDF datasets and queries, allowing us to scale with both dataset and system size. Unlike previous solutions, this framework uses highly optimised (SPARQL) joining strategies, intelligent data caching and the use of a super-query to enable the completion of eight distinct SPARQL lookups, comprising over eighty distinct joins, in only two Map / Reduce iterations. Results are presented comparing both the Jena and a previous Hadoop implementation demonstrating the superior performance of the new methodology. The new method is shown to be five times faster than Jena and twice as fast as the previous approach.},
keywords={Resource description framework;Medical diagnostic imaging;Medical services;Big data;Sensors;Biomedical monitoring;RDF;Medical Data;Map / Reduce;Joins},
doi={10.1109/BigData.2015.7363818},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7364058,
author={Feng, Tao and Zhuang, Zhenyun and Pan, Yi and Ramachandra, Haricharan},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={A memory capacity model for high performing data-filtering applications in Samza framework},
year={2015},
volume={},
number={},
pages={2600-2605},
abstract={Data quality is essential in big data paradigm as poor data can have serious consequences when dealing with large volumes of data. While it is trivial to spot poor data for small-scale and offline use cases, it is challenging to detect and fix data inconsistency in large-scale and online (real-time or near-real time) big data context. An example of such scenario is spotting and fixing poor data using Apache Samza, a stream processing framework that has been increasingly adopted to process near-real-time data at LinkedIn. To optimize the deployment of Samza processing and reduce business cost, in this work we propose a memory capacity model for Apache Samza to allow denser deployments of high performing data-filtering applications built on Samza. The model can be used to provision just-enough memory resource to applications by tightening the bounds on the memory allocations. We apply our memory capacity model on Linkedln's real use cases in production, which significantly increases the deployment density and saves business costs. We will share key learning in this paper.},
keywords={Containers;LinkedIn;Data models;Big data;Java;Measurement;Real-time systems;Apache Samza;capacity model;data filtering;performance},
doi={10.1109/BigData.2015.7364058},
ISSN={},
month={Oct},}
@ARTICLE{8962328,
author={Konanahalli, Ashwini and Marinelli, Marina and Oyedele, Lukumon},
journal={IEEE Transactions on Engineering Management},
title={Drivers and Challenges Associated With the Implementation of Big Data Within U.K. Facilities Management Sector: An Exploratory Factor Analysis Approach},
year={2022},
volume={69},
number={4},
pages={916-929},
abstract={The recent advances in Internet of Things (IoT), computational analytics, processing power, and assimilation of Big Data (BD) are playing an important role in revolutionizing maintenance and operations regimes within the wider facilities management (FM) sector. The BD offers the potential for the FM to obtain valuable insights from a large amount of heterogeneous data collected through various sources and IoT allows for the integration of sensors. The aim of this article is to extend the exploratory studies conducted on Big Data analytics (BDA) implementation and empirically test and categorize the associated drivers and challenges. Using exploratory factor analysis (EFA), the researchers aim to bridge the current knowledge gap and highlight the principal factors affecting the BDA implementation. Questionnaires detailing 26 variables are sent to the FM organization in the U.K. who are in the process or have already implemented BDA initiatives within their FM operations. Fifty-two valid responses are analyzed by conducting EFA. The findings suggest that driven by market competition and ambitious sustainability goals, the industry is moving to holistically integrate analytics into its decision making. However, data quality, technological barriers, inadequate preparedness, data management, and governance issues and skill gaps are posing to be significant barriers to the fulfillment of expected opportunities. The findings of this study have important implications for FM businesses that are evaluating the potential of the BDA and IoT applications for their operations. Most importantly, it addresses the role of the BD maturity in FM organizations and its implications for perception of drivers.},
keywords={Frequency modulation;Organizations;Big Data;Maintenance engineering;Data mining;Internet of Things;Analytics;Big Data (BD);facilities management (FM);technology implementation},
doi={10.1109/TEM.2019.2959914},
ISSN={1558-0040},
month={Aug},}
@INPROCEEDINGS{9659660,
author={Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and AyshAlhroob},
booktitle={2021 International Conference on Engineering and Emerging Technologies (ICEET)},
title={An Improve The Quality Of Data Considering Big Data Aspect Based On Sensitive Of Cost Time},
year={2021},
volume={},
number={},
pages={1-6},
abstract={Big data is term of dataset with characteristic volume, value and veracity that lead to confrontation unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the suitable resources of organization in many stages by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope consist on high trust which is bring high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally select from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation organizing between them start by project scope as strongest one then cost, product and Last one time is weakest between them, in the final when select best quality use two sides mostly from quality degree and be center of quality interval and in particular from closest distance with the strongest factor.},
keywords={Costs;Correlation;Data integrity;Volume measurement;Project management;Big Data;Time measurement;Big Data;Project Management;Sensitive Rule;Quality},
doi={10.1109/ICEET53442.2021.9659660},
ISSN={2409-2983},
month={Oct},}
@INPROCEEDINGS{7976069,
author={Vieira, Vanessa and Pedrosa, Isabel and Soares, Bruno Horta},
booktitle={2017 12th Iberian Conference on Information Systems and Technologies (CISTI)},
title={Big data & analytics: An approach using audit experts' interviews},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Big Data is one of the great trends in the short and medium term in organizations. There is a growing concern in provid solutions to address this trend, to methodical analysis of data and to do better decisions. The amount of data becomes less relevant when there is efficiency in Analytics. Internal auditors need to be in compliance with technology evolution. This research main objective is to understand how are internal auditors perceiving Big Data & Analytics' and which are the opportunities and difficulties pointed to address that challenge. To achieve this main goal, semi-structured interviews were conducted focused on internal auditors group. Those interviews intend to analyze and classify respondents' contributions in order to provide more insights for the present research. As a result, the main opportunities listed were greater information security and greater efficiency in data processing. Pointed obstacles were data quality, security and users' training.},
keywords={Big Data;Interviews;Market research;Organizations;Surges;Software;Big Data;organizations;information;Audit;technology},
doi={10.23919/CISTI.2017.7976069},
ISSN={},
month={June},}
@INPROCEEDINGS{7364061,
author={Abboura, Asma and Sahrl, Soror and Ouziri, Mourad and Benbernou, Salima},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={CrowdMD: Crowdsourcing-based approach for deduplication},
year={2015},
volume={},
number={},
pages={2621-2627},
abstract={Matching dependencies (MDs) were recently introduced as quality rules for data cleaning and entity resolution. They are rules that specify what values should be considered duplicates, and have to be matched. Defining such quality rules on a database instance, is a very expensive and a time consuming process, and requires huge efforts to analyse the whole database. In this demo paper, we present CrowdMD, a hybrid machine-crowd system for generating MDs. It first asks the crowd to determine whether a given pair, from training sample pairs, match or not. Then, it uses data mining techniques to generate attributes constituting an MD. Using a Restaurant database, we will show how the crowders can help to generate MDs by labelling the training sample through the CrowdMD user interface and how MDs can be mined from this training set.},
keywords={Big data;Databases;Labeling;Hybrid power systems;Crowdsourcing;Training;Cleaning;matching rules;deduplication;entity resolution;big data quality},
doi={10.1109/BigData.2015.7364061},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7471171,
author={Amato, Flora and Moscato, Francesco},
booktitle={2016 30th International Conference on Advanced Information Networking and Applications Workshops (WAINA)},
title={Automatic Cloud Services Composition for Big Data Management},
year={2016},
volume={},
number={},
pages={46-51},
abstract={Every-Day lives are becoming increasingly instrumented by electronic devices and any kind of computer-based (distributed) service. As a result, organizations need to analyse an enormous amounts of data in order to increase their incomings or to improve their services. Anyway, setting-up a private infrastructure to execute analytics over Big Data is still expensive. The exploitation of Cloud infrastructure in Big Data management is appealing because of costs reductions and potentiality of storage, network and computing resources. The Cloud can consistently reduce the cost of analysis of data from different sources, opening analytics to big storages in a multi-cloud environment. Anyway, creating and executing this kind of service is very complex since different resources have to be provisioned and coordinated depending on users' needs. Orchestration is a solution to this problem, but it requires proper languages and methodologies for automatic composition and execution. In this work we propose a methodology for composition of services used for analyses of different Big Data sources: in particular an Orchestration language is reported able to describe composite services and resources in a multi-cloud environment.},
keywords={Cloud computing;Computer architecture;Semantics;Big data;Quality of service;Meteorology;NIST;Cloud Computing;Orchestration;Formal Semantics;Availability},
doi={10.1109/WAINA.2016.169},
ISSN={},
month={March},}
@INPROCEEDINGS{8029838,
author={Wang, Haiyan and Zhang, Han},
booktitle={2017 IEEE International Conference on Web Services (ICWS)},
title={User Requirements Based Service Identification for Big Data},
year={2017},
volume={},
number={},
pages={800-807},
abstract={Service identification meets with new challenges with overwhelming rise of categories and numbers of services in big data scenarios. Most of the current service identification approaches have paid little attention to the granularity of indicator for service identification, neither do they provide with any trustworthy monitoring mechanism during the process of service identification. To address the problems above, we propose a user requirements based service identification approach for big data (URBSI-BD). In the proposed URBSI-BD, we firstly cluster massive services with BIRCH clustering algorithm to obtain a number of service sets. We then employ PSO algorithm with MapReduce mechanism to achieve a fine-grained evaluation of indicator for service identification. Based on the integration, candidate services which can better meet with user requirements will be selected. Finally, we use Beth trust model on the quality of experience of users and set up a monitoring mechanism to better obtain required services. Simulation results and analysis demonstrate that the proposed approach has better performance in service identification compared with other current approaches in big data scenarios.},
keywords={Clustering algorithms;Big Data;Quality of service;Monitoring;Reliability;Optimization;Service Identification;User Requirements;PSO Algorithm;Quality of Experience (QoE)},
doi={10.1109/ICWS.2017.11},
ISSN={},
month={June},}
@INPROCEEDINGS{7207308,
author={Lawson, Victor and Ramaswamy, Lakshmish},
booktitle={2015 IEEE International Congress on Big Data},
title={Data Quality and Energy Management Tradeoffs in Sensor Service Clouds},
year={2015},
volume={},
number={},
pages={749-752},
abstract={Cloud-based sensor data collection services are becoming an essential part of the Internet of Things (IoT). As the consumer demand grows for these services, the data quality (DQ) of the stream becomes an increasingly vital issue. Of particular interest is the inherent tradeoff between the DQ and the energy consumption of the sensor. Unfortunately, there has been very little research on the management of this tradeoff that allows data consumers to receive high quality data while simultaneously conserving energy. Our work seeks to explore this tradeoff in detail by combining DQ services for the data stream consumer with customizable energy efficient "EE" throttling algorithms for the data feed producers. These energy management services provide cost reduction rewards for consumers who would otherwise make poor DQ/EE decisions. Our primary contributions include cloud-based services for monitoring the tradeoff, an architecture that adjusts to DQ needs and a producer/consumer data stream best matching cloud service. We envision that our services architecture will reward energy efficiency decisions and profoundly affect consumer choices.},
keywords={Feeds;Big data;Computer architecture;Clouds;Wireless sensor networks;Software;Conferences;data quality;cloud service;energy management;sensor network},
doi={10.1109/BigDataCongress.2015.124},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{9378148,
author={O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD},
year={2020},
volume={},
number={},
pages={1914-1923},
abstract={The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).},
keywords={Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality},
doi={10.1109/BigData50022.2020.9378148},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7951876,
author={Liwei Zheng},
booktitle={2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={SNSQ ontology: A domain ontology for SNSs data quality},
year={2017},
volume={},
number={},
pages={11-18},
abstract={the advent of online social networks has been one of the most exciting events in this decade. Many popular online social networks such as Twitter, Wechat, Weibo, LinkedIn, and Facebook have become increasingly popular. The consequences of the poor quality of data in a social network are often experienced in everyday life. This paper gives a domain ontology model, SNSQ Ontology, for data quality in the area of social networks. It could be a knowledge base for the quality assessment of the rich and linkage data in the social network. High-quality data would be relevant in the data searching, analyzing and mining. Based on the SNSQ Ontology the strategy for data quality assessment and repair is given. And the co-influence among the four quality dimensions, completeness, consistency, currency, and accuracy, are discussed to guarantee an effective assessment process.},
keywords={Ontologies;Social network services;Artificial neural networks;Synchronization;Maintenance engineering;ontology;social network;data quality assessment},
doi={10.1109/ICCCBDA.2017.7951876},
ISSN={},
month={April},}
@INPROCEEDINGS{9319350,
author={Iyengar, Arun and Patel, Dhaval and Shrivastava, Shrey and Zhou, Nianjun and Bhamidipaty, Anuradha},
booktitle={2020 IEEE Second International Conference on Cognitive Machine Intelligence (CogMI)},
title={Real-Time Data Quality Analysis},
year={2020},
volume={},
number={},
pages={101-108},
abstract={Data quality is critically important for big data and machine learning applications. Data quality systems can analyze data sets for quality and detection of potential errors. They can also provide remediation to fix problems encountered in analyzing data sets. This paper discusses key features that of data quality analysis systems. We also present new algorithms for efficiently maintaining updated data quality metrics on changing data sets. Our algorithms consider anomalies in data regions in determining how much different regions of data contribute to overall data metrics. We also make intelligent choices of which data metrics to update and how frequently to do so in order to limit the overhead for data quality metric updates.},
keywords={Data integrity;Measurement;Prediction algorithms;Anomaly detection;Machine learning algorithms;Task analysis;Interpolation;data quality;data analytics;real time data analytics},
doi={10.1109/CogMI50398.2020.00022},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8647366,
author={Hussain, Bilal and Du, Qinghe and Ren, Pinyi},
booktitle={2018 IEEE Global Communications Conference (GLOBECOM)},
title={Deep Learning-Based Big Data-Assisted Anomaly Detection in Cellular Networks},
year={2018},
volume={},
number={},
pages={1-6},
abstract={5G is envisioned to have an artificial intelligence (AI)-empowerment to efficiently plan, manage and optimize the extremely complex network by leveraging colossal amount of data (big data) generated at different levels of the network architecture. Cell outages and congestion pose serious threat to the network management. Sleeping cell is a special case of cell outage in which the cell provides inferior services to its users. This peculiar behavior of the cell is particularly challenging to detect as it disguises itself from the network monitoring entity. Inadequate accuracy and high false alarms are two major constraints of state-of-the-art approaches for the anomaly-sleeping cell and surge in user traffic activity that may lead to congestion-detection in cellular networks. This implies squandering of scarce resources which ultimately results in increased operational expenditure (OPEX) while disrupting network's quality of service (QoS) and user's quality of experience (QoE). Inspired from the prominent success of deep learning (DL) technology in machine learning domain, this is the first study that applies DL for the detection of abovementioned anomalies. We utilized, and did a comprehensive study of, L-layer deep feedforward neural network fueled by real call detail record (CDR) dataset (big data) and achieved 94.6% accuracy with 1.7% false positive rate (FPR), that are remarkable improvements and overcome the limitations of the previous studies. The preliminary results elucidate the feasibility and preeminence of our proposed anomaly detection framework.},
keywords={Computer architecture;Microprocessors;Big Data;Quality of service;Anomaly detection;Cellular networks;Quality of experience},
doi={10.1109/GLOCOM.2018.8647366},
ISSN={2576-6813},
month={Dec},}
@INPROCEEDINGS{7979931,
author={Wen, Hongsheng and Chen, Zhiqiang and Gu, Jianping and Zhu, Qiangqiang},
booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
title={Big Data Analysis on Radiographic Image Quality},
year={2016},
volume={},
number={},
pages={341-346},
abstract={Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.},
keywords={Detectors;Image edge detection;Radiography;Standards;Image quality;X-ray imaging;Indexes;image quality;in-service;radiographic product;routine data;quality control},
doi={10.1109/CCBD.2016.073},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8622378,
author={Kaplunovich, Alex and Yesha, Yelena},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Consolidating billions of Taxi rides with AWS EMR and Spark in the Cloud : Tuning, Analytics and Best Practices},
year={2018},
volume={},
number={},
pages={4501-4507},
abstract={Saving nature using Big Data Analytics is a very noble goal. Using New York taxi rides data, we decided to learn how many rides could be consolidated. It was a journey we would like to share. First, we had to choose the platform for calculation between Amazon Athena, Serverless Microservices, SQL or NoSql databases, Hadoop and Spark. Then, we had to find an optimal solution for the platform using assorted tuning and optimization techniques. Although the problem seems to be straight forward, it turned out that the solution is quite challenging because of the input size, data quality, calculation complexities and numerous EMR/Spark tuning options. We have been using New York taxi data from 2009 to 2017 to quantify the rides that can be joined together. The taxi rides were consolidated based on pickup location, pickup time and drop-off location. We have been calculating the percentage of taxi rides that can be joined. The benchmark originally set was rides within five minutes with a pickup and drop-off locations within half a kilometer. Then we started experimenting with different times and locations. We have been using parquet format, parallel Scala collections, compression, filtering, new column introduction, tuning parameters, I/O overhead tuning, bucketing, timeouts and partitioning. Over 1.2 billion rides were processed using Amazon EMR with Spark. We have been optimizing calculation time and processing price. Spark has hundreds of parameters, EMR has over fifty instances to choose from. It was challenging to process our data within reasonable time. We were able to find the optimal Spark queries (plans), tested different types of joins and compared their performances. Also, we were able to compare I/O and in-memory operations during partitioning and large files manipulation (the input file sizes were hundreds of Gigabytes). The results were amazing - we could consolidate around thirty five percent of total rides, saving tons of gas and improving environment and traffic in New York City.},
keywords={Sparks;Public transportation;Servers;Tuning;Big Data;Structured Query Language;Tools;Analytics;Spark;EMR;Cloud;BigData;Best Practices;Parquet;AWS;Optimization;Tuning},
doi={10.1109/BigData.2018.8622378},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7536334,
author={Lawson, Victor J. and Ramaswamy, Lakshmish},
booktitle={2016 International Conference on Distributed Computing in Sensor Systems (DCOSS)},
title={TAU-FIVE: A Multi-tiered Architecture for Data Quality and Energy-Sustainability in Sensor Networks},
year={2016},
volume={},
number={},
pages={169-176},
abstract={Current research on wireless sensor networks "WSNs" in the Internet of Things "IoT" has focused on performance, scalability and energy efficiency. Innovations in these areas have many challenges due to the increasing volume of smart device data streams in the internet of Everything "IoE". Data feeds from future IoE systems such as the internet of vehicles, smart homes and smart-cities will need real time consolidation. This merger of technologies will require innovative big data algorithms and architectures that authenticate the data streams. A primary concern is in dynamically quantifying the data quality "DQ" of the streams while constructing real-time metrics to assess the energy efficiency "EE" of these IoE devices. In order to define the relationship between sensor stream DQ and EE, we propose our multi-tiered cloud-service architecture TAU-FIVE. The technical contributions of our framework includes data quality and energy efficiency models based on 7 DQ attributes and multiple reprogrammable smart sensors that dynamically modify and regulate the DQ and EE of a WSN. Our research maintains that WSN's can balance sustainability with quality of service by creating real-time metrics that merge energy usage with data stream integrity. This equilibrium will impact energy awareness in the IoT as the multitude of batch device data streams are integrated with the variety of social and professional networks and evolve into the IoE.},
keywords={Measurement;Feeds;Computer architecture;Wireless sensor networks;Energy efficiency;Clouds;Data models;Data quality;cloud computing;energy model;applications to sensing;green networks},
doi={10.1109/DCOSS.2016.42},
ISSN={2325-2944},
month={May},}
@INPROCEEDINGS{9378483,
author={Hossain, Md Monir and Sebestyen, Mark and Mayank, Dhruv and Ardakanian, Omid and Khazaei, Hamzeh},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Large-scale Data-driven Segmentation of Banking Customers},
year={2020},
volume={},
number={},
pages={4392-4401},
abstract={This paper presents a novel big data analytics framework for creating explainable personas for retail and business banking customers. These personas are essential to better tailor financial products and improve customer retention. This framework is comprised of several components including anomaly detection, binning and aggregation of contextual data, clustering of transaction time series, and mining association rules that map contextual data to cluster identifiers. Leveraging rich transaction and contextual data available from nearly 60,000 retail and 90,000 business customers of a financial institution, we empirically evaluate this framework and describe how the identified association rules can be used to explain and refine existing customer classes, and identify new customer classes and various data quality issues. We also analyze the performance of the proposed framework and show that it can easily scale to millions of banking customers.},
keywords={Data integrity;Conferences;Time series analysis;Banking;Big Data;Anomaly detection;Business;customer segmentation;clustering;association rules mining;anomaly detection},
doi={10.1109/BigData50022.2020.9378483},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9672007,
author={Zhou, Xiantian and Ordonez, Carlos},
booktitle={2021 IEEE International Conference on Big Data (Big Data)},
title={Programming Languages in Data Science: a Comparison from a Database Angle},
year={2021},
volume={},
number={},
pages={3147-3154},
abstract={In a typical Data Science project, the analyst uses many programming languages to explore and analyze big data coming from diverse data sources. A major challenge is managing and pre-processing so much data, with potentially inconsistent content, significant redundancy, in diverse formats, with varying data quality. Database systems research has tackled such problems for a long time, but mostly on relational databases. With such motivation in mind, this paper compares strengths and weaknesses of popular languages used nowadays from a database pespective: Python, R and SQL. We discuss the entire analytic pipeline, going from data integration, cleaning and pre-processing to model application and tuning. From a database systems perspective, we present a comprehensive survey of storage mechanisms, data processing algorithms, external algorithms, run-time memory management, consistency, optimizations and parallel processing. From a programming languages angle, we consider elegance, expressiveness, abstraction, composability, interactive behavior and automatic code optimization. We present a short experimental evaluation comparing the performance of the three languages on typical data exploration and pre-processing tasks. Our conclusion: there is no winner.},
keywords={Soft sensors;Redundancy;Relational databases;Big Data;Data science;Database systems;Task analysis},
doi={10.1109/BigData52589.2021.9672007},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9284399,
author={Mystakidis, Aristeidis and Tjortjis, Christos},
booktitle={2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA},
title={Big Data Mining for Smart Cities: Predicting Traffic Congestion using Classification},
year={2020},
volume={},
number={},
pages={1-8},
abstract={This paper provides an analysis and proposes a methodology for predicting traffic congestion. Several machine learning algorithms and approaches are compared to select the most appropriate one. The methodology was implemented using Data Mining and Big Data techniques along with Python, SQL, and GIS technologies and was tested on data originating from one of the most problematic, regarding traffic congestion, streets in Thessaloniki, the 2nd most populated city in Greece. Evaluation and results have shown that data quality and size were the most critical factors towards algorithmic accuracy. Result comparison showed that Decision Trees were more accurate than Logistic Regression.},
keywords={Machine learning algorithms;Smart cities;Big Data;Prediction algorithms;Data mining;Traffic congestion;Regression tree analysis;Data Mining;Big Data;Machine learning;Smart Cities;Prediction;Classification;Traffic Congestion},
doi={10.1109/IISA50023.2020.9284399},
ISSN={},
month={July},}
@INPROCEEDINGS{9599192,
author={Zhang, Zhenwei and Wu, Wenyan and Wu, Dongjie},
booktitle={2021 2nd International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)},
title={A Multi-Mode Learning Behavior Real-time Data Acquisition Method Based on Data Quality},
year={2021},
volume={},
number={},
pages={64-69},
abstract={With the rapid development of new technologies such as artificial intelligence, big data, and the Internet of Things, many researchers have probed into the study of learning analysis, trying to solve the problems of teaching by analyzing the learning behavior data from learning process. And in many learning behavior research, the sensor network usually consists of a host of mutually independent data sources, which can be used to monitor measured objects from multiple dimensions thereby obtaining the multi-source multi-modal sensory data. However, there still exist false negative readings, false positive readings and environmental interference, etc. Therefore, we propose a multi-source multimode sensory data acquisition method based on Date Quality(DQ). We first define the data quality in terms of four aspects-accuracy, integrity, consistency and instantaneity. Then, by the modeling there aspects respectively, we propose metrics to estimate the comprehensive data quality method of multi-source multi-mode sensory data. Finally, a data acquisition method is presented based on data quality, which selects a part of data sources for data transmission according to the given precision. This method aims at reducing the consumption of the sensory network on the premise of the data quality guarantee. An extensive experimental evaluation demonstrates the efficiency and effectiveness of the algorithm.},
keywords={Measurement;Data integrity;Data acquisition;Learning (artificial intelligence);Interference;Data models;Real-time systems;multi-mode Data;Learning behavior;data quality;data acquisition},
doi={10.1109/ISCEIC53685.2021.00021},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9179628,
author={Tsoumakos, Dimitrios and Giannakopoulos, Ioannis},
booktitle={2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Content-Based Analytics: Moving beyond Data Size},
year={2020},
volume={},
number={},
pages={33-40},
abstract={Efforts on Big Data technologies have been highly directed towards the amount of data a task can access or crunch. Yet, for content-driven decision making, it is not (only) about the size, but about the "right" data: The number of available datasets (a different type of volume) can reach astronomical sizes, making a thorough evaluation of each input prohibitively expensive. The problem is exacerbated as data sources regularly exhibit varying levels of uncertainty and velocity/churn. To date, there exists no efficient method to quantify the impact of numerous available datasets over different analytics tasks and workflows. This visionary work puts the spotlight on data content rather than size. It proposes a novel modeling, planning and processing research bundle that assesses data quality in terms of analytics performance. The main expected outcome is to provide efficient, continuous and intelligent management and execution of content-driven data analytics. Intelligent dataset selection can achieve massive gains on both accuracy and time required to reach a desired level of performance. This work introduces the notion of utilizing dataset similarity to infer operator behavior and, consequently, be able to build scalable, operator-agnostic performance models for Big Data tasks over different domains. We present an overview of the promising results from our initial work with numerical and graph data and respective operators. We then describe a reference architecture with specific areas of research that need to be tackled in order to provide a data-centric analytics ecosystem.},
keywords={Data models;Analytical models;Task analysis;Predictive models;Uncertainty;Biological system modeling;Numerical models;modeling;data quality;big data;Machine Learning;scheduling},
doi={10.1109/BigDataService49289.2020.00013},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9545944,
author={Ying, KangHui and Hu, WenYu and Chen, Jin Bo and Li, Guo Nong},
booktitle={2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)},
title={Research on instance-level data cleaning technology},
year={2021},
volume={},
number={},
pages={238-242},
abstract={Effectivedata analysis and data mining are based on data availability and data quality. Data cleaning is a commonly used technique to improve data quality. Instance-level data cleaning is an important part of data cleaning. The focus is on the comparison and analysis of the detection and cleaning methods of attributes and recorded values in the instance-level data cleaning technology, and the experimental analysis of the repeated record cleaning methods. This paper introduces the application field of data cleaning technology represented by the electrical engineering field combined with the application situation, and provides valuable selection suggestions for the characteristics of different data sets and the applicable instancelevel data cleaning technology. Summarizing and analyzing the existing detection and cleaning technology methods, it is concluded that instance-level data cleaning has a lot of research and development space in long text, unstructured data and specific fields. Finally, the challenges and development directions of the instance-level data cleaning technology are prospected.},
keywords={Electrical engineering;Data integrity;Big Data;Cleaning;Data mining;Artificial intelligence;Research and development;instance-level data cleaning;duplicate records cleaning;attribute cleaning},
doi={10.1109/CAIBDA53561.2021.00057},
ISSN={},
month={May},}
@INPROCEEDINGS{7502278,
author={Shanmugam, Srinivasan and Seshadri, Gokul},
booktitle={2016 IEEE 2nd International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS)},
title={Aspects of Data Cataloguing for Enterprise Data Platforms},
year={2016},
volume={},
number={},
pages={134-139},
abstract={As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.},
keywords={Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service},
doi={10.1109/BigDataSecurity-HPSC-IDS.2016.52},
ISSN={},
month={April},}
@INPROCEEDINGS{9107851,
author={Dehui, Fu and Feng, Wang and Shuai, Yuan and Guangzhen, Wang and Mingxin, Shao},
booktitle={2019 6th International Conference on Information Science and Control Engineering (ICISCE)},
title={Fuzzy Comprehensive Evaluation Method for On-line Monitoring Data Quality of Substation Equipment},
year={2019},
volume={},
number={},
pages={753-757},
abstract={This paper analyses the existing problems in on-line monitoring and data quality evaluation of substation equipment, and proposes a multi-dimensional fuzzy comprehensive evaluation method for on-line monitoring data quality of substation equipment. The evaluation index set of online monitoring data quality of substation equipment with 5 dimensions and 11 secondary indexes is established. The weight is determined by combining subjective and objective methods. The fuzzy transformation is completed based on membership function and a multi-dimensional fuzzy comprehensive evaluation model is established. Finally, the evaluation grade of online monitoring data of substation equipment is obtained. Finally, compared with other methods, the validity and accuracy of this method are verified.},
keywords={big data;data quality;subordination;fuzzy comprehensive evaluation;On-line monitoring},
doi={10.1109/ICISCE48695.2019.00154},
ISSN={},
month={Dec},}
@ARTICLE{8198798,
author={He, Ying and Yu, F. Richard and Zhao, Nan and Leung, Victor C. M. and Yin, Hongxi},
journal={IEEE Communications Magazine},
title={Software-Defined Networks with Mobile Edge Computing and Caching for Smart Cities: A Big Data Deep Reinforcement Learning Approach},
year={2017},
volume={55},
number={12},
pages={31-37},
abstract={Recent advances in networking, caching, and computing have significant impacts on the developments of smart cities. Nevertheless, these important enabling technologies have traditionally been studied separately in the existing works on smart cities. In this article, we propose an integrated framework that can enable dynamic orchestration of networking, caching, and computing resources to improve the performance of applications for smart cities. Then we present a novel big data deep reinforcement learning approach. Simulation results with different system parameters are presented to show the effectiveness of the proposed scheme.},
keywords={Smart cities;Streaming media;Cloud computing;Big Data;Quality of service;Mobile communication;Urban areas},
doi={10.1109/MCOM.2017.1700246},
ISSN={1558-1896},
month={Dec},}
@INPROCEEDINGS{9497187,
author={Adnan, Kiran and Akbar, Rehan and Wang, Khor Siak},
booktitle={2021 International Conference on Computer & Information Sciences (ICCOINS)},
title={Towards Improved Data Analytics Through Usability Enhancement of Unstructured Big Data},
year={2021},
volume={},
number={},
pages={1-6},
abstract={A high volume of unstructured data is being generated from diverse and heterogeneous sources. The unstructured data analytics process is used to extract valuable insights from these unstructured data sources but unlocking useful and usable information is critical for analytics. Despite advancements in technologies, data preparation requires an inordinate amount of time in unstructured data manipulation into a usable form. Although several data manipulation and preparation techniques have been proposed for unstructured big data, relatively limited research has addressed the usability issues of unstructured data. This study identifies the usability issues of unstructured big data for the analytical process to bridge the identified gap. The usability enhancement model has been proposed for unstructured big data to facilitate the subjective and objective efficacy of unstructured big data for data preparation and manipulation activities. Moreover, concept mapping is an essential element to improve the usability of unstructured big data incorporated in the proposed model with usability rules. These rules reduce the usability gap between data availability and its usefulness for an intended purpose. The proposed research model will help to improve the efficiency of unstructured big data analytics.},
keywords={Bridges;Analytical models;Data analysis;Data integrity;Decision making;Data visualization;Big Data;Unstructured data;data usability;unstructured data analytics;pragmatic data quality},
doi={10.1109/ICCOINS49721.2021.9497187},
ISSN={},
month={July},}
@INPROCEEDINGS{9353794,
author={Xia, Hong and Zhang, YongKang and Wang, Han and Chen, YanPing and Wang, ZhongMin},
booktitle={2020 International Conference on Networking and Network Applications (NaNA)},
title={Crowdsourcing Answer Integration Algorithm For Big Data Environment},
year={2020},
volume={},
number={},
pages={335-341},
abstract={Crowdsourcing is an emerging distributed computing model that is widely used. Aiming at the uneven quality of crowdsourcing answers due to different workers' capabilities and attitudes, it is necessary to effectively study the hotspot issue of crowdsourcing answer integration. A crowdsourced answer integration algorithm based on “filter-evaluate-vote” is proposed. This algorithm is implemented using MapReduce parallel programming model in the Hadoop platform, and experiments are performed on multiple data sets. The results show that the proposed algorithm can be effective. It improves the accuracy of crowdsourced answers, and has high computing performance and horizontal scalability, which is suitable for answer integration in a big data environment.},
keywords={Crowdsourcing;Computational modeling;Scalability;Big Data;Quality assessment;Time factors;Task analysis;Crowdsourcing;quality assessment;answer integration;MapReduce},
doi={10.1109/NaNA51271.2020.00064},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9148148,
author={Bai, Zhongxian and Zhuo, Rongqing},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
title={Quality Management of Crowd Sensing Data Based on Machine Learning},
year={2020},
volume={},
number={},
pages={185-188},
abstract={Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.},
keywords={Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method},
doi={10.1109/CIBDA50819.2020.00049},
ISSN={},
month={April},}
@INPROCEEDINGS{6597123,
author={Ramaswamy, Lakshmish and Lawson, Victor and Gogineni, Siva Venkat},
booktitle={2013 IEEE International Congress on Big Data},
title={Towards a Quality-centric Big Data Architecture for Federated Sensor Services},
year={2013},
volume={},
number={},
pages={86-93},
abstract={As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.},
keywords={Feeds;Clouds;Wireless sensor networks;Computer architecture;Fluid flow measurement;Markup languages;Data models;Internet of Things;Federated Sensor Clouds;Data Quality;Sensor Virtualization},
doi={10.1109/BigData.Congress.2013.21},
ISSN={2379-7703},
month={June},}
