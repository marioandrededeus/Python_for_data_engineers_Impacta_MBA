@incollection{DIMITRAKOPOULOS202119,
title = {Chapter 2 - AI and software enablers for highly automated and autonomous vehicles},
editor = {George Dimitrakopoulos and Aggelos Tsakanikas and Elias Panagiotopoulos},
booktitle = {Autonomous Vehicles},
publisher = {Elsevier},
pages = {19-31},
year = {2021},
isbn = {978-0-323-90137-6},
doi = {https://doi.org/10.1016/B978-0-323-90137-6.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390137600014X},
author = {George Dimitrakopoulos and Aggelos Tsakanikas and Elias Panagiotopoulos},
keywords = {Algorithms, Artificial Intelligence, Cognitive, Decision-making, Deep learning, Machine learning},
abstract = {The goal of this chapter is to present the latest advances and challenges on management algorithms and software for enabling highly automated and autonomous driving, enhanced by Artificial Intelligence (AI) tools and methods. Indicative examples include cognitive computing, knowledge-based systems, and noncausal reasoning algorithms, which are used for active and passive safety of autonomous vehicles, as well as emergency management systems for highly automated/autonomous vehicles.}
}
@article{LI2021181,
title = {High-Throughput physiology-based stress response phenotyping: Advantages, applications and prospective in horticultural plants},
journal = {Horticultural Plant Journal},
volume = {7},
number = {3},
pages = {181-187},
year = {2021},
note = {Abiotic Stresses in Horticultural Plants},
issn = {2468-0141},
doi = {https://doi.org/10.1016/j.hpj.2020.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2468014120300996},
author = {Yanwei Li and Xinyi Wu and Wenzhao Xu and Yudong Sun and Ying Wang and Guojing Li and Pei Xu},
keywords = {Phenomics, Physiolomics, Isohydric/anisohydric, Abiotic stress},
abstract = {Phenomics is a new branch of science that provides high-throughput quantification of plant and animal traits at systems level. The last decade has witnessed great successes in high-throughput phenotyping of numerous morphological traits, yet major challenges still exist in precise phenotyping of physiological traits such as transpiration and photosynthesis. Due to the highly dynamic nature of physiological traits in responses to the environment, appropriate selection criteria and efficient screening systems at the physiological level for abiotic stress tolerance have been largely absent in plants. In this review, the current status of phenomics techniques was briefly summarized in horticultural plants. Specifically, the emerging field of high-throughput physiology-based phenotyping, which is referred to as “physiolomics”, for drought stress responses was highlighted. In addition to analyzing the advantages of physiology-based phenotyping over morphology-based approaches, recent examples that applied high-throughput physiological phenotyping to model and non-model horticultural plants were revisited and discussed. Based on the collective findings, we propose that high-throughput, non-destructive, and automatic physiological assays can and should be used as routine methods for phenotyping stress response traits in horticultural plants.}
}
@article{CHOWDHURY2021e00597,
title = {Recent machine learning guided material research - A review},
journal = {Computational Condensed Matter},
volume = {29},
pages = {e00597},
year = {2021},
issn = {2352-2143},
doi = {https://doi.org/10.1016/j.cocom.2021.e00597},
url = {https://www.sciencedirect.com/science/article/pii/S2352214321000605},
author = {Mohammad Asaduzzaman Chowdhury and Nayem Hossain and Md Bengir {Ahmed Shuvho} and Mohammad Fotouhi and Md Sakibul Islam and Md Ramjan Ali and Mohammod Abul Kashem},
keywords = {ML, Material science, Design, Characterization, Advancements, Challenges},
abstract = {Sustainable development of modern society demands discovering new materials with superior properties in different applications such as aerospace, wind, civil, automotive, etc. Characterizing and predicting material properties using traditional methods are time consuming and expensive. Therefore, advanced methods have been developed to meet the need for quick and reliable design and characterizing of materials properties. ML methods have made it possible to optimize and automate design performance and discover new materials. This review paper gives an overview of the implementation of ML in i) discovery of new materials, and ii) characterization of materials ML. Various ML models for materials manufacturing as well as how ML is applied to model materials are discussed. Recent advances, ML applications, as well as upcoming challenges and perspectives are discussed.}
}
@article{HERRGARDH2021102694,
title = {Hybrid modelling for stroke care: Review and suggestions of new approaches for risk assessment and simulation of scenarios},
journal = {NeuroImage: Clinical},
volume = {31},
pages = {102694},
year = {2021},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2021.102694},
url = {https://www.sciencedirect.com/science/article/pii/S2213158221001388},
author = {Tilda Herrgårdh and Vince I. Madai and John D. Kelleher and Rasmus Magnusson and Mika Gustafsson and Lili Milani and Peter Gennemark and Gunnar Cedersund},
keywords = {Stroke, Mechanistic modelling, Machine learning, Bioinformatics, Precision medicine},
abstract = {Stroke is an example of a complex and multi-factorial disease involving multiple organs, timescales, and disease mechanisms. To deal with this complexity, and to realize Precision Medicine of stroke, mathematical models are needed. Such approaches include: 1) machine learning, 2) bioinformatic network models, and 3) mechanistic models. Since these three approaches have complementary strengths and weaknesses, a hybrid modelling approach combining them would be the most beneficial. However, no concrete approach ready to be implemented for a specific disease has been presented to date. In this paper, we both review the strengths and weaknesses of the three approaches, and propose a roadmap for hybrid modelling in the case of stroke care. We focus on two main tasks needed for the clinical setting: a) For stroke risk calculation, we propose a new two-step approach, where non-linear mixed effects models and bioinformatic network models yield biomarkers which are used as input to a machine learning model and b) For simulation of care scenarios, we propose a new four-step approach, which revolves around iterations between simulations of the mechanistic models and imputations of non-modelled or non-measured variables. We illustrate and discuss the different approaches in the context of Precision Medicine for stroke.}
}
@incollection{OPRISIU2021208,
title = {In Silico ADME Modeling},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {208-222},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11532-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383115326},
author = {Ioana Oprisiu and Susanne Winiwarter},
keywords = {Absorption, AI, Distribution, DMPK, Drug discovery, Excretion, In silico ADME, IVIVe, Machine learning, Metabolism, QSAR, QSPR},
abstract = {Modeling of absorption, distribution, metabolism and excretion properties is well established in drug discovery. Here we present principles, considerations when and how to build models and how to apply them in real life in the industrial setting. The availability and quality of experimental data is important. However, for in silico models to be utilized availability is a major factor. Additionally, model quality measures need to be presented in an easily understandable manner for chemists and DMPK scientists.}
}
@article{EYKE2021120,
title = {Toward Machine Learning-Enhanced High-Throughput Experimentation},
journal = {Trends in Chemistry},
volume = {3},
number = {2},
pages = {120-132},
year = {2021},
note = {Special Issue: Machine Learning for Molecules and Materials},
issn = {2589-5974},
doi = {https://doi.org/10.1016/j.trechm.2020.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2589597420303117},
author = {Natalie S. Eyke and Brent A. Koscher and Klavs F. Jensen},
keywords = {high-throughput experimentation, machine learning, active learning},
abstract = {Recent literature suggests that the fields of machine learning (ML) and high-throughput experimentation (HTE) have separately received considerable attention from chemists and engineers, leading to the development of powerful reactivity models and platforms capable of rapidly performing thousands of reactions. The merger of ML with HTE presents a wealth of opportunities for the exploration of chemical space, but the integration of the two has yet to be fully realized. We highlight examples of recent developments in ML and HTE that collectively suggest the utility of their integration. Our analysis highlights the complementarity of the two fields, while exposing a number of obstacles that can and should be overcome to take full advantage of this merger and thereby accelerate chemical research.}
}
@article{CHEN2021111375,
title = {A novel short-term load forecasting framework based on time-series clustering and early classification algorithm},
journal = {Energy and Buildings},
volume = {251},
pages = {111375},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111375},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821006599},
author = {Zhe Chen and Yongbao Chen and Tong Xiao and Huilong Wang and Pengwei Hou},
keywords = {Short-term load forecasting, Light gradient boosting machine (LightGBM), Time series clustering, Early classification, Feature engineering},
abstract = {With the development of data-driven models, extracting information from historical data for better energy forecasting is critically important for energy planning and optimization in buildings. Feature engineering is a key factor in improving the performance of forecasting models. Adding load pattern labels for different daily energy consumption patterns resulting from different time schedules and weather conditions can help improve forecasting accuracy. Traditionally, pattern labeling focuses mainly on finding a day similar to the forecasting day based on calendar or other information, such as weather conditions. The most intuitive approach for dividing historical time-series load into patterns is clustering; however, the pattern cannot be determined before the load is known. To address this problem, this study proposes a novel short-term load forecasting framework integrating an early classification algorithm that uses a stochastic algorithm to predetermine the load pattern of a forecasting day. In addition, a hybrid multistep method combining the strengths of single-step forecasting and recursive multistep forecasting is integrated into the framework. The proposed framework was validated through a case study using actual metered data. The results demonstrate that the early classification and proposed labeling strategy produce satisfactory forecasting accuracy and significantly improve the forecasting performance of the LightGBM model.}
}
@article{BANKER2021741,
title = {International Committee for Monitoring Assisted Reproductive Technologies (ICMART): world report on assisted reproductive technologies, 2013},
journal = {Fertility and Sterility},
volume = {116},
number = {3},
pages = {741-756},
year = {2021},
issn = {0015-0282},
doi = {https://doi.org/10.1016/j.fertnstert.2021.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0015028221002442},
author = {Manish Banker and Silke Dyer and Georgina M. Chambers and Osamu Ishihara and Markus Kupka and Jacques {de Mouzon} and Fernando Zegers-Hochschild and G. David Adamson},
keywords = {Assisted reproductive technology, IVF/ICSI outcome, frozen embryo transfer, ICMART, cumulative live birth rate, registry},
abstract = {Objective
To report the utilization, effectiveness, and safety of practices in assisted reproductive technology (ART) globally in 2013 and assess global trends over time.
Design
Retrospective, cross-sectional survey on the utilization, effectiveness, and safety of ART procedures performed globally during 2013.
Setting
Seventy-five countries and 2,639 ART clinics.
Patient(s)
Women and men undergoing ART procedures.
Intervention(s)
All ART.
Main Outcome Measure(s)
The ART cycles and outcomes on country-by-country, regional, and global levels. Aggregate country data were processed and analyzed based on methods developed by the International Committee for Monitoring Assisted Reproductive Technology (ICMART).
Result(s)
A total of 1,858,500 ART cycles were conducted for the treatment year 2013 across 2,639 clinics in 75 participating countries with a global participation rate of 73.6%. Reported and estimated data suggest 1,160,474 embryo transfers (ETs) were performed resulting in >344,317 babies. From 2012 to 2013, the number of reported aspiration and frozen ET cycles increased by 3% and 16.4%, respectively. The proportion of women aged >40 years undergoing nondonor ART increased from 25.2% in 2012 to 26.3% in 2013. As a percentage of nondonor aspiration cycles, intracytoplasmic sperm injection (ICSI) was similar to results for 2012. The in vitro fertilization (IVF)/ICSI combined delivery rates per fresh aspiration and frozen ET cycles were 24.2% and 22.8%, respectively. In fresh nondonor cycles, single ET increased from 33.7% in 2012 to 36.5% in 2013, whereas the average number of transferred embryos was 1.81—again with wide country variation. The rate of twin deliveries after fresh nondonor transfers was 17.9%; the triplet rate was 0.7%. In frozen ET cycles performed in 2013, single ET was used in 57.6%, with an average of 1.49 embryos transferred and twin and triplet rates of 10.8% and 0.4%, respectively. The cumulative delivery rate per aspiration was 30.4%, similar to that in 2012. Perinatal mortality rate per 1,000 births was 22.2% after fresh IVF/ICSI and 16.8% after frozen ET. The data presented depended on the quality and completeness of the data submitted by individual countries. This report covers approximately two-thirds of world ART activity. Continued efforts to improve the quality and consistency of reporting ART data by registries are still needed.
Conclusion(s)
Reported ART cycles, effectiveness, and safety increased between 2012 and 2013 with adoption of a better method for estimating unreported cycles.
Comité Internacional para la monitorización de las Tecnologías de Reproducción Asistida (ICMART): informe mundial.
Objetivo
Informar sobre la utilización, la eficacia y la seguridad de las prácticas de la tecnología de reproducción asistida (TRA) a nivel mundial en 2013 y evaluar las tendencias mundiales a lo largo del tiempo.
Diseño
Encuesta retrospectiva y transversal sobre la utilización, la eficacia y la seguridad de los procedimientos de TRA realizados a nivel mundial durante el año 2013.
Entorno
Setenta y cinco países y 2639 clínicas de TRA.
Pacientes
Mujeres y hombres sometidos a procedimientos de TRA.
Intervención(es)
Todas las TRA.
Medida(s) principal(es) del resultado
Los ciclos de TRA y los resultados a nivel de país, regional y mundial. Los datos agregados de los países se procesaron y analizaron según los métodos desarrollados por el Comité Internacional para la Vigilancia de las Tecnologías de Reproducción Asistida (ICMART). Tecnología de Reproducción Asistida (ICMART).
Resultados
En el año de tratamiento 2013 se realizaron un total de 1.858.500 ciclos de TRA en 2.639 clínicas de 75 países participantes con una tasa de participación global del 73,6%. Los datos informados y estimados sugieren que se realizaron 1.160.474 transferencias de embriones (TE) que dieron lugar a >344.317 bebés. De 2012 a 2013, el número de ciclos de aspiración y de TE congelados notificados aumentó un 3% y un 16,4%, respectivamente. La proporción de mujeres de >40 años que se sometieron a TRA autóloga aumentó del 25,2% en 2012 al 26,3% en 2013. Como porcentaje de ciclos de aspiración autóloga, la inyección intracitoplasmática de espermatozoides (ICSI) fue similar a los resultados de 2012. Las tasas de parto combinadas de fecundación in vitro (FIV)/ICSI por ciclos de aspiración en fresco y TE congelada fueron del 24,2% y el 22,8%, respectivamente. En los ciclos autólogos en fresco,la TE única aumentó del 33,7% en 2012 al 36,5% en 2013, mientras que el número medio de embriones transferidos fue de 1,81 -de nuevo, con una amplia variación entre países. La tasa de partos gemelares tras transferencias autólogas en fresco fue del 17,9%; la tasa de trillizos fue del 0,7%. En los ciclos de TE de congelados realizados en 2013, se utilizó la TE simple en el 57,6%, con una media de 1,49 embriones transferidos y unas tasas de gemelos y trillizos del 10,8% y 0,4%, respectivamente. La tasa acumulada de partos por aspiración fue del 30,4%, similar a la de 2012. La tasa de mortalidad perinatal por cada 1.000 nacimientos fue del 22,2% tras la FIV/ICSI en fresco y del 16,8% tras la TE de congelados. Los datos presentados dependían de la calidad y la exhaustividad de los datos presentados por cada país. Este informe abarca aproximadamente dos tercios de la actividad mundial de TRA. Los esfuerzos continuos para mejorar la calidad y la coherencia de los datos presentados por los registros sobre la TRA deben seguir siendo objeto de esfuerzos continuos.
Conclusión(es)
Los ciclos de TRA notificados, la eficacia y la seguridad aumentaron entre 2012 y 2013 con la adopción de un mejor método para estimar los ciclos no reportados.}
}
@article{YANG2021243,
title = {The code of targeted poverty alleviation in China: A geography perspective},
journal = {Geography and Sustainability},
volume = {2},
number = {4},
pages = {243-253},
year = {2021},
issn = {2666-6839},
doi = {https://doi.org/10.1016/j.geosus.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666683921000420},
author = {Yuanyuan Yang and Yansui Liu},
keywords = {Targeted poverty alleviation, Sustainable development, China, Geography, Experience},
abstract = {Geography is suitable for the study of sustainability from a transdisciplinary perspective, which takes the human-land relationship as the core research. As a key obstacle to rural sustainability, poverty is an external manifestation of the coupling maladjustment of elements in human-land territorial systems. As the world's largest developing country, China eradicated extreme poverty in 2020 and made significant contributions to global poverty reduction. Especially over the last eight years, China has implemented a targeted poverty alleviation (TPA) strategy and has continuously promoted theoretical, organizational and institutional innovations for poverty reduction. From the perspective of geography, this paper extracts the experiences of China's TPA strategy, represented by the "5W2H" mode. The research concludes that: (1) Precise identification, as the foundation of TPA, aims to introduce a registration system to obtain records of all poor households and then answer the "5W" (what, where, why, who, when) issues of the geography of poverty. (2) Precise assistance is the key of TPA, which aims to solve the issue of "how to offer help and support". The barriers to escaping poverty can be accomplished through policies and measures that focus on the diverse causes of poverty and considering different situations. (3) Accurate assessments are an essential means of TPA, relevant to solve "how to measure the end of poverty alleviation", and third-party evaluations play an important role in improving the accuracy of poverty alleviation. (4) The TPA mechanism lies in reconstructing the human-land-industry structures in the impoverished areal system. It is urgent to introduce China's successful experience and typical modes of TPA for global human-earth system coordination and sustainable development and contribute to building a community of human destiny.}
}
@incollection{CASASROMA2021111,
title = {Chapter 6 - A literature review on artificial intelligence and ethics in online learning},
editor = {Santi Caballé and Stavros N. Demetriadis and Eduardo Gómez-Sánchez and Pantelis M. Papadopoulos and Armin Weinberger},
booktitle = {Intelligent Systems and Learning Data Analytics in Online Education},
publisher = {Academic Press},
pages = {111-131},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-823410-5},
doi = {https://doi.org/10.1016/B978-0-12-823410-5.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234105000061},
author = {Joan Casas-Roma and Jordi Conesa},
keywords = {artificial intelligence in education, artificial intelligence, data science, learning analytics, ethics, artificial morality, online learning},
abstract = {In recent years, artificial intelligence (AI) has been used in online learning to improve teaching and learning, with the aim of providing a more efficient, purposeful, adaptive, ubiquitous, and fair learning experiences. However, and as it has been seen in other contexts, the integration of AI can have unforeseen consequences with detrimental effects which can result in unfair and discriminatory decisions. Therefore it is worth thinking about potential risks that learning environments integrating AI systems might pose. This work explores the intersections between AI, online learning, and ethics in order to understand the ethical concerns surrounding this crossroads. We review the main ethical challenges identified in the literature and distill a set of guidelines to support the ethical design and integration of AI systems in online learning environments. This should help ensure that online learning is how is meant to be: accessible, inclusive, fair, and beneficial to society.}
}
@article{MALINI20216105,
title = {Investigation of factors affecting student performance evaluation using education materials data mining technique},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {6105-6110},
year = {2021},
note = {SI: TIME-2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321036026},
author = {J. Malini and Y. Kalpana},
keywords = {Educational datamining, Dataset, Students performance, Attributes, Features, Machine learning, Materials and methods, Analysis},
abstract = {Every year students success rate was analysed by the Educational Institutions to develop their Academic standard. To identify the success rate many kinds of techniques are used such as statistics, physical examination and currently ongoing data mining techniques. Data mining Techniques was widely used in many fields, it is also used in the Educational environment known as Educational Data Mining (EDM). Educational data mining generate prototype in solving the research problems in students data and used to locate the unseen patterns in the students detailed dateset. This paper uses the EDM to characterize the distinct factors affecting the students performance by making predictions with efficient algorithms. Educational professionals have to identify the causes for the student failure in academic performance and the students not succeed in completing their education which becomes a social problem these days. The machine learning techniques help the researchers to analyse the student’s learning habits and their performance in academic. This paper would discuss different kinds of algorithms to analyse the economic background of the students which mainly affects the students performance. The dataset was utilized from the UCI Repository of secondary school students performance and analysed using the Weka tool for the data mining process.}
}
@article{QADEER2021112736,
title = {Developing machine learning models for relative humidity prediction in air-based energy systems and environmental management applications},
journal = {Journal of Environmental Management},
volume = {292},
pages = {112736},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.112736},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721007982},
author = {Kinza Qadeer and Ashfaq Ahmad and Muhammad Abdul Qyyum and Abdul-Sattar Nizami and Moonyong Lee},
keywords = {Air quality parameters, Random forest, Machine learning-based estimation, Aspen hysys, Support vector machine, Environmental management operations},
abstract = {The prediction of relative humidity is a challenging task because of its nonlinear nature. The machine learning-based prediction strategies have attained significant attention in tackling a broad class of challenging nonlinear and complex problems. The random forest algorithm is a well-proven machine learning algorithm due to its ease of training and implementation, as it requires minimal preprocessing. The random forest algorithm has hitherto not been employed for estimating air quality parameters, such as relative humidity. In this study, the random forest approach is implemented to estimate the relative humidity as a function of dry- and wet-bulb temperatures. A well-known commercial process simulator called Aspen HYSYS® V10 is linked with MATLAB® version 2019a to establish a data mining environment. The robustness of the prediction model is evaluated against varying wet-bulb depressions. There is high absolute deviation that indicates a lower prediction performance of the model against the higher wet-bulb depression i.e., ~20.0 °C. The random forest model can predict relative humidity with a 1.1% mean absolute deviation compared to the values obtained through Aspen HYSYS. The performance of the RF estimation model is also compared with a well-known support vector regression model. The random forest model demonstrates 74.4% better performance than the support vector machine model for the problem of interest, i.e., relative humidity estimation. This study will significantly help the practitioners in efficient designing of air-dependent energy systems as well as in better environmental management through rigorous prediction of relative humidity.}
}
@article{PICCIONE2021308,
title = {Realistic interplays between data science and chemical engineering in the first quarter of the 21st century, part 2: Dos and don’ts},
journal = {Chemical Engineering Research and Design},
volume = {169},
pages = {308-318},
year = {2021},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2021.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0263876221001301},
author = {Patrick M. Piccione},
keywords = {Data science, Chemical engineering, Digital transformation, Industry 4.0, Smart manufacturing},
abstract = {Under various names, such as, data science, Industry 4.0, or smart manufacturing, digital technologies are transforming our world. Although value statements and promises are published in a steady stream, uptake in the chemical and process industries has been moderate. Successful transformations are not confined to tasks, the “whats”. They also require great care in how they are carried out. This overview, aimed at all participants in the digital transformation of the chemical industry, presents “dos and don’ts” method recommendations for three successive steps: strategy development to define goals, (organisational) mobilisation for implementation, and project delivery. Successful strategy development requires assembling an empowered and skilled team; truly understanding the data science and digital transformation topics; accepting emergence and iteration; and focusing on real needs. Mobilising an organisation is essential so that it can translate strategy to tactics and value. Within organisations, one must therefore: enable project identification; set up a supportive organisational structure and skilful people within it. Looking outside, participation in partnerships is essential to access external resources. Delivery of valuable projects is the end goal. A diverse portfolio is needed, as well as effective collaborations between subject matter experts and data scientists. Technically, the use of software best practice is beneficial, and care must be taken of the data themselves. In the longer term, data science opportunities will extend beyond merely improving traditional analytics to make them faster, better, and more user-friendly. The early identification of beneficial future trends requires encouraging those individuals who have an interest in disruptive currents, and the perceptiveness to sense their areas of application.}
}
@article{GE2021107394,
title = {Improved adaptive gray wolf genetic algorithm for photovoltaic intelligent edge terminal optimal configuration},
journal = {Computers and Electrical Engineering},
volume = {95},
pages = {107394},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107394},
url = {https://www.sciencedirect.com/science/article/pii/S004579062100361X},
author = {Leijiao Ge and Jiaheng Liu and Bo Wang and Yue Zhou and Jun Yan and Ming Wang},
keywords = {Distributed photovoltaic, Photovoltaic intelligent edge terminal, Optimal configuration, Improved adaptive genetic algorithm},
abstract = {Photovoltaic (PV) intelligent edge terminals (IETs) integrate data acquisition, processing, storage and upload functions for intelligent operations of PV power stations. However, the cost of installing a PV IET at one PV station is relatively high. In order to achieve the goal of multiple distributed PV stations sharing one PV IET on the premise of ensuring reliability, the paper proposes a method for the optimal configuration of PV IETs. First of all, considering the economy and reliability of optimizing configuration of PV IET, a two-layer optimization model is established. After that, to solve the nonlinearity of the proposed model, an improved adaptive genetic algorithm and gray wolf optimization (IAGA-GWO) is proposed. Finally, through two application cases of PV IETs, it is proved that the optimized configuration method in this paper can reduce the cost under the premise of ensuring the reliability.}
}
@article{MENG2021102692,
title = {Modification of Newell’s car-following model incorporating multidimensional stochastic parameters for emission estimation},
journal = {Transportation Research Part D: Transport and Environment},
volume = {91},
pages = {102692},
year = {2021},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2020.102692},
url = {https://www.sciencedirect.com/science/article/pii/S1361920920308762},
author = {Dongli Meng and Guohua Song and Yizheng Wu and Zhiqiang Zhai and Lei Yu and Jianbo Zhang},
keywords = {Newell’s car-following model, Stochasticity, Emission estimation, Driving behavior heterogeneity, Vehicle trajectory data},
abstract = {Existing studies have indicated that the vehicle trajectories derived from Newell's car-following model (NCM) fail to capture driving behavior heterogeneity, resulting in considerable emission estimation errors. This study investigated the situation-dependent heterogeneity of car-following behavior, based on field vehicle trajectories in Beijing, and proposed a multidimensional stochastic Newell car-following model (MSNCM) incorporating three stochastic parameters: random response time, speed-dependent critical jam spacing, and speed difference- and spacing-dependent acceleration. The comparison between the field data and numerical simulations of the NCM and MSNCM shown that the MSNCM performed well in generating realistic vehicle trajectories for emission estimation. The relative errors of the emission factors derived from the field and the MSNCM simulated trajectories were 0.26%, 0.91%, 1.37%, and 0.25% for CO2, CO, HC, and NOx, respectively, which represented reductions of approximately 15%-46% compared with the traditional NCM.}
}
@article{XU2021e266,
title = {Wireless skin sensors for physiological monitoring of infants in low-income and middle-income countries},
journal = {The Lancet Digital Health},
volume = {3},
number = {4},
pages = {e266-e273},
year = {2021},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(21)00001-7},
url = {https://www.sciencedirect.com/science/article/pii/S2589750021000017},
author = {Shuai Xu and Alina Y Rwei and Bellington Vwalika and Maureen P Chisembele and Jeffrey S A Stringer and Amy Sarah Ginsburg and John A Rogers},
abstract = {Summary
Globally, neonatal mortality remains unacceptability high. Physiological monitoring is foundational to the care of these vulnerable patients to assess neonatal cardiopulmonary status, guide medical intervention, and determine readiness for safe discharge. However, most existing physiological monitoring systems require multiple electrodes and sensors, which are linked to wires tethered to wall-mounted display units, to adhere to the skin. For neonates, these systems can cause skin injury, prevent kangaroo mother care, and complicate basic clinical care. Novel, wireless, and biointegrated sensors provide opportunities to enhance monitoring capabilities, reduce iatrogenic injuries, and promote family-centric care. Early validation data have shown performance equivalent to (and sometimes exceeding) standard-of-care monitoring systems in premature neonates cared for in high-income countries. The reusable nature of these sensors and compatibility with low-cost mobile phones have the future potential to enable substantially lower monitoring costs compared with existing systems. Deployment at scale, in low-income countries, holds the promise of substantial improvements in neonatal outcomes.}
}
@article{YAHAYA2021105851,
title = {Ensemble-based model selection for imbalanced data to investigate the contributing factors to multiple fatality road crashes in Ghana},
journal = {Accident Analysis & Prevention},
volume = {151},
pages = {105851},
year = {2021},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2020.105851},
url = {https://www.sciencedirect.com/science/article/pii/S0001457520316717},
author = {Mahama Yahaya and Runhua Guo and Xinguo Jiang and Kamal Bashir and Caroline Matara and Shiwei Xu},
keywords = {Multiple fatal injury crash, Classification, Model selection, Class imbalance, Oversampling, Ensemble classifiers},
abstract = {The study aims to identify relevant variables to improve the prediction performance of the crash injury severity (CIS) classification model. Unfortunately, the CIS database is invariably characterized by the class imbalance. For instance, the samples of multiple fatal injury (MFI) severity class are typically rare as opposed to other classes. The imbalance phenomenon may introduce a prediction bias in favour of the majority class and affect the quality of the learning algorithm. The paper proposes an ensemble-based variable ranking scheme that incorporates the data resampling. At the data pre-processing level, majority weighted minority oversampling (MWMOTE) is employed to treat the imbalanced training data. Ensemble of classifiers induced from the balanced data is used to evaluate and rank the individual variables according to their importance to the injury severity prediction. The relevant variables selected are then applied to the balanced data to form a training set for the CIS classification modelling. An empirical comparison is conducted through considering the variable ranking by: 1) the learning of single inductive algorithm with imbalanced data where the relevant variables are applied to the imbalanced data to form the training data; 2) the learning of single inductive algorithm with MWMOTE data and the relevant variables identified are applied to the balanced data to form the training data; and 3) the learning of ensembles with imbalanced data where the relevant variables identified are applied to the imbalanced data to form the training data. Bayesian Networks (BNs) classifiers are then developed for each ranking method, where nested subsets of the top ranked variables are adopted. The model predictions are captured in four performance indicators in the comparative study. Based on three-year (2014–2016) crash data in Ghana, the empirical results show that the proposed method is effective to identify the most prolific predictors of the CIS level. Finally, based on the inference results of BNs developed on the best subset, the study offers the most probable explanations to the occurrence of MFI crashes in Ghana.}
}
@article{HUANG2021103115,
title = {Privacy protection among three antithetic-parties for context-aware services},
journal = {Journal of Network and Computer Applications},
volume = {191},
pages = {103115},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103115},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001351},
author = {Yan Huang and Wei Li and Jinbao Wang and Zhipeng Cai and Anu G. Bourgeois},
keywords = {Privacy protection, Game theory, Context-aware services},
abstract = {The popularity of context-aware services is improving the quality of life, while raising serious privacy issues. In order for users to receive quality service, they are at risk of leaking private information by adversaries that are possibly eavesdropping on the data and/or by the untrusted service platform selling off its data to adversaries. Game theory has been utilized as a powerful tool to achieve privacy preservation by strategically balancing the trade-off between profit (service) and cost (data leakage) for the user. However, most of the existing schemes cannot fully exploit the power of game theory, as they fail to depict the mutual relationship between any two (of the three) parties involved: user, platform, and adversary. Existing schemes are also not always able to provide specific guidance for a user to reduce the impact of the joint threats from the platform and adversary. In this paper, we design a privacy-preserving game to quantify the three parties’ concerns and capture interactions between any two of them. We also identify the best strategy for each party at a fine-grained level, i.e. specific settings, not simply binary choices. We validate the performance of our proposed game model through both a theoretical analysis and experiments.}
}
@article{MARTIN2021100255,
title = {Multi-Temperate Logical Data Warehouse Design for Large-Scale Healthcare Data},
journal = {Big Data Research},
volume = {25},
pages = {100255},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100255},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000721},
author = {Bryan Martin and Karen C. Davis},
keywords = {Data warehouse design, OLAP workloads, Healthcare data management, Data partitioning algorithms, Logical data warehouses, Columnar databases},
abstract = {Modern hardware architectures and advances in database technology are driving increased adoption of logical data warehouses (LDWs) that complement traditional physical data warehousing (PDW) approaches. In contrast to PDW design methodologies that emphasize physical consolidation of all data of interest on a single (perhaps distributed) computing platform, along with early-binding approaches that pre-materialize transformations and changes to the source data, LDW techniques allow for the integration and transformation of data at run-time and typically physically move or modify much less data in advance. In an environment with premium hardware such as multi-temperate storage, the successful design of LDWs depends on replication of high value data to their physical core to maximize spatial locality. Identifying and collocating high value data is a non-trivial task that has not been adequately explored in the context of LDWs in multi-temperate storage systems. In this paper, we gather queries to construct an OLAP workload for use in supporting and evaluating LDW design algorithms for a large healthcare organization. We introduce new algorithms to address the preprocessing of the workload, identification of data clusters to support OLAP queries, and assignment of clusters to appropriate (hot, warm, and cold) storage tiers, allowing the LDW to deliver results more efficiently by covering a higher percentage of its query workload using the fastest storage devices. Any use case involving copying data from sources to tiered storage targets for analytic querying could benefit from the techniques and solutions presented here.}
}
@article{FINN20211021,
title = {Is it time to put rest to rest?},
journal = {Trends in Cognitive Sciences},
volume = {25},
number = {12},
pages = {1021-1032},
year = {2021},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661321002345},
author = {Emily S. Finn},
keywords = {resting state, task-based, functional connectivity, naturalistic tasks, brain–behavior prediction},
abstract = {The so-called resting state, in which participants lie quietly with no particular inputs or outputs, represented a paradigm shift from conventional task-based studies in human neuroimaging. Our foray into rest was fruitful from both a scientific and methodological perspective, but at this point, how much more can we learn from rest on its own? While rest still dominates in many subfields, data from tasks have empirically demonstrated benefits, as well as the potential to provide insights about the mind in addition to the brain. I argue that we can accelerate progress in human neuroscience by de-emphasizing rest in favor of more grounded experiments, including promising integrated designs that respect the prominence of self-generated activity while offering enhanced control and interpretability.}
}
@article{MARABELLI2021101683,
title = {The lifecycle of algorithmic decision-making systems: Organizational choices and ethical challenges},
journal = {The Journal of Strategic Information Systems},
volume = {30},
number = {3},
pages = {101683},
year = {2021},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2021.101683},
url = {https://www.sciencedirect.com/science/article/pii/S0963868721000305},
author = {Marco Marabelli and Sue Newell and Valerie Handunge},
keywords = {Algorithmic decision-making systems (ADMS), Strategic choices, Ethical implications, IS strategizing, Automatic systems},
abstract = {In this viewpoint article we discuss algorithmic decision-making systems (ADMS), which we view as organizational sociotechnical systems with their use in practice having consequences within and beyond organizational boundaries. We build a framework that revolves around the ADMS lifecycle and propose that each phase challenges organizations with “choices” related to technical and processual characteristics – ways to design, implement and use these systems in practice. We argue that it is important that organizations make these strategic choices with awareness and responsibly, as ADMS’ consequences affect a broad array of stakeholders (the workforce, suppliers, customers and society at-large) and involve ethical considerations. With this article we make two main contributions. First, we identify key choices associated with the design, implementation and use in practice of ADMS in organizations, that build on past literature and are tied to timely industry-related examples. Second, we provide IS scholars with a broad research agenda that will promote the generation of new knowledge and original theorizing within the domain of the strategic applications of ADMS in organizations.}
}
@article{WANG2021112665,
title = {A method for land surface temperature retrieval based on model-data-knowledge-driven and deep learning},
journal = {Remote Sensing of Environment},
volume = {265},
pages = {112665},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112665},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721003850},
author = {Han Wang and Kebiao Mao and Zijin Yuan and Jiancheng Shi and Mengmeng Cao and Zhihao Qin and Sibo Duan and Bohui Tang},
keywords = {Land surface temperature (LST), Model-data-knowledge-driven, Deep learning, Geophysical logical reasoning, Expert knowledge},
abstract = {Most algorithms for land surface temperature (LST) retrieval depend on acquiring prior knowledge. To overcome this drawback, we propose a novel LST retrieval method based on model-data-knowledge-driven and deep learning, called the MDK-DL method. Based on the expert knowledge and radiation transfer model, we deduce LST retrieval mechanism and determine the best combination of the thermal infrared (TIR) bands of the sensor. Then, we use the radiation transfer model simulation and reliable satellite-ground data to establish a training and test database, and finally use the deep learning neural network for optimal computation. Three typical high-, medium- and low-spatial-resolution TIR remote sensing datasets (from Gaofen, the Moderate Resolution Imaging Spectroradiometer (MODIS), and Fengyun) are used for theoretical simulation and application analysis. The simulation shows that the minimum mean absolute error (MAE) is less than 0.1 K (standard deviation: 0.04 K; correlation coefficient: 1.000) at a small viewing direction (<7.5°) and less than 0.8 K at a large viewing direction (<65°). The in situ validation shows that the minimum MAE obtained by the optimal band combination is approximately 1 K (root mean square error (RMSE) = 1.12 K; coefficient of determination (R2) = 0.902). The retrieval accuracy is improved by increasing the number of TIR bands in the atmospheric window, and adding accurate atmospheric water vapor information produces better results. In general, four TIR bands in the atmospheric window bands are sufficient to retrieve the LST with high accuracy. Likewise, three TIR bands plus atmospheric water vapor information are sufficient for the retrieval requirements. All analyses indicate that our method is feasible and reliably accurate and can also be used to help design the instrument band to retrieve the LST with high precision.}
}
@incollection{YADAV2021123,
title = {Chapter 8 - A research review on semantic interoperability issues in electronic health record systems in medical healthcare},
editor = {Sanjay Kumar Singh and Ravi Shankar Singh and Anil Kumar Pandey and Sandeep S. Udmale and Ankit Chaudhary},
booktitle = {IoT-Based Data Analytics for the Healthcare Industry},
publisher = {Academic Press},
pages = {123-138},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-821472-5},
doi = {https://doi.org/10.1016/B978-0-12-821472-5.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128214725000090},
author = {Rimmy Yadav and Saniksha Murria and Anil Sharma},
keywords = {Medical healthcare, Electronic health record, Interoperability, Semantic interoperability},
abstract = {With constantly diminishing costs and prolonged effectiveness of wireless communication and transmission techniques and, importantly, the Internet of Things (IoT) emerging as a powerful technology, some aspects of our lives have changed and broadened. The healthcare sector in particular is a developing and highly demanding application sector. IoT contributions to the medical healthcare domain include remote health and monitoring services, care for the elderly, recognition as well as tactical management of chronic illnesses, and offering of adaptive and self-regulated medical facilities. In medical healthcare, electronic health record (EHR) systems provide efficient management of clinical records in today’s clinical healthcare organizations. However, medical records are generating huge amounts of data, with every medical record having its own standard pattern, schema, and level of abstraction and interoperability. To interact with EHRs, medical stakeholders must use standard and well-structured methods and ontology-based languages to analyze and mine the useful information from huge data records. Much research has been done on interoperability issues, particularly syntactic interoperability and technical interoperability. After reviewing the research articles and chapters from respected medical databases such as IEEE Xplore, Elsevier, and Science Direct, the authors noted that semantic interoperability is, in the EHR framework, one of the critical issues. To achieve full semantic interoperability, researchers and scholars have developed and structured numerous methodologies, tools, and techniques. This research review thus includes methodologies, frameworks, tools, and models, along with their advantages and limitations, developed by researchers to cope with semantic interoperability issues in medical healthcare. Furthermore, in this chapter, the authors have focused on searching the papers related to the semantic web with a model-driven architecture (MDA) approach for semantic interoperability. Applications of the MDA approach and advanced features of the semantic web may be able to resolve the issue of semantic interoperability.}
}
@article{GRAINGER2021354,
title = {A Perspective on the Analytical Challenges Encountered in High-Throughput Experimentation},
journal = {Organic Process Research & Development},
volume = {25},
number = {3},
pages = {354-364},
year = {2021},
issn = {1083-6160},
doi = {https://doi.org/10.1021/acs.oprd.0c00463},
url = {https://www.sciencedirect.com/science/article/pii/S1083616021016492},
author = {Rachel Grainger and Stuart Whibley},
keywords = {analytical, HTE, mass spectrometry, nanoscale, optimization},
abstract = {ABSTRACT
High-throughput experimentation (HTE) is a well-established technique used in the pharmaceutical industry to accelerate compound synthesis and route optimization through automated chemical processes. A key part of any HTE workflow is the analytical component, through which the reaction outcome can be determined. The development of new analytical techniques capable of high-throughput data generation from nanoscale chemical reactions has been required to streamline the HTE process and address challenges generated through the recent move to miniaturize synthesis. In this Perspective, we review the currently available state-of-the-art analytical methods, discuss the challenges encountered in high-throughput analysis—with a particular focus on the analysis of nanoscale reactions, and provide a future outlook on potential developments in the field.}
}
@article{ZHOU20211274,
title = {Intelligent Ironmaking Optimization Service on a Cloud Computing Platform by Digital Twin},
journal = {Engineering},
volume = {7},
number = {9},
pages = {1274-1281},
year = {2021},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S209580992100299X},
author = {Heng Zhou and Chunjie Yang and Youxian Sun},
keywords = {Cloud factory, Blast furnace, Multi-objective optimization, Distributed computation},
abstract = {The shortage of computation methods and storage devices has largely limited the development of multi-objective optimization in industrial processes. To improve the operational levels of the process industries, we propose a multi-objective optimization framework based on cloud services and a cloud distribution system. Real-time data from manufacturing procedures are first temporarily stored in a local database, and then transferred to the relational database in the cloud. Next, a distribution system with elastic compute power is set up for the optimization framework. Finally, a multi-objective optimization model based on deep learning and an evolutionary algorithm is proposed to optimize several conflicting goals of the blast furnace ironmaking process. With the application of this optimization service in a cloud factory, iron production was found to increase by 83.91 t∙d−1, the coke ratio decreased 13.50 kg∙t−1, and the silicon content decreased by an average of 0.047%.}
}
@article{MORRIS2021199,
title = {Impact of the COVID-19 pandemic on the detection and management of colorectal cancer in England: a population-based study},
journal = {The Lancet Gastroenterology & Hepatology},
volume = {6},
number = {3},
pages = {199-208},
year = {2021},
issn = {2468-1253},
doi = {https://doi.org/10.1016/S2468-1253(21)00005-4},
url = {https://www.sciencedirect.com/science/article/pii/S2468125321000054},
author = {Eva J A Morris and Raphael Goldacre and Enti Spata and Marion Mafham and Paul J Finan and Jon Shelton and Mike Richards and Katie Spencer and Jonathan Emberson and Sam Hollings and Paula Curnow and Dominic Gair and David Sebag-Montefiore and Chris Cunningham and Matthew D Rutter and Brian D Nicholson and Jem Rashbass and Martin Landray and Rory Collins and Barbara Casadei and Colin Baigent},
abstract = {Summary
Background
There are concerns that the COVID-19 pandemic has had a negative effect on cancer care but there is little direct evidence to quantify any effect. This study aims to investigate the impact of the COVID-19 pandemic on the detection and management of colorectal cancer in England.
Methods
Data were extracted from four population-based datasets spanning NHS England (the National Cancer Cancer Waiting Time Monitoring, Monthly Diagnostic, Secondary Uses Service Admitted Patient Care and the National Radiotherapy datasets) for all referrals, colonoscopies, surgical procedures, and courses of rectal radiotherapy from Jan 1, 2019, to Oct 31, 2020, related to colorectal cancer in England. Differences in patterns of care were investigated between 2019 and 2020. Percentage reductions in monthly numbers and proportions were calculated.
Findings
As compared to the monthly average in 2019, in April, 2020, there was a 63% (95% CI 53–71) reduction (from 36 274 to 13 440) in the monthly number of 2-week referrals for suspected cancer and a 92% (95% CI 89–95) reduction in the number of colonoscopies (from 46 441 to 3484). Numbers had just recovered by October, 2020. This resulted in a 22% (95% CI 8–34) relative reduction in the number of cases referred for treatment (from a monthly average of 2781 in 2019 to 2158 referrals in April, 2020). By October, 2020, the monthly rate had returned to 2019 levels but did not exceed it, suggesting that, from April to October, 2020, over 3500 fewer people had been diagnosed and treated for colorectal cancer in England than would have been expected. There was also a 31% (95% CI 19–42) relative reduction in the numbers receiving surgery in April, 2020, and a lower proportion of laparoscopic and a greater proportion of stoma-forming procedures, relative to the monthly average in 2019. By October, 2020, laparoscopic surgery and stoma rates were similar to 2019 levels. For rectal cancer, there was a 44% (95% CI 17–76) relative increase in the use of neoadjuvant radiotherapy in April, 2020, relative to the monthly average in 2019, due to greater use of short-course regimens. Although in June, 2020, there was a drop in the use of short-course regimens, rates remained above 2019 levels until October, 2020.
Interpretation
The COVID-19 pandemic has led to a sustained reduction in the number of people referred, diagnosed, and treated for colorectal cancer. By October, 2020, achievement of care pathway targets had returned to 2019 levels, albeit with smaller volumes of patients and with modifications to usual practice. As pressure grows in the NHS due to the second wave of COVID-19, urgent action is needed to address the growing burden of undetected and untreated colorectal cancer in England.
Funding
Cancer Research UK, the Medical Research Council, Public Health England, Health Data Research UK, NHS Digital, and the National Institute for Health Research Oxford Biomedical Research Centre.}
}
@article{HE2021103749,
title = {Human resource management structure of communication enterprise based on microprocessor system and embedded network},
journal = {Microprocessors and Microsystems},
volume = {81},
pages = {103749},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103749},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120308942},
author = {Ying He and Ming Li},
keywords = {Information systems, Human resource management, HRISHR professionals},
abstract = {In the past few years, human resource management (HRM) has undergone significant changes. The focus from administrative tasks to become strategic partners into the organization's overall strategy, mainly in the field of development of information technology, has given strong support. The technology support using Microprocessor System and Embedded Network to help handle the HRM knowledge process. Long-term use of information systems today have a significant impact on how to manage HRM. The Human Resources (HR) processes and practices within the organization; in other words, the collection of information, storage, use, and sharing method have changed completely. The Microprocessor System using to store the condition base data and then communicate with internet support. Part of the HRM process becomes more efficient; due to these improved service levels, it can now be more participation. Human resources of this new business strategy in business strategy have a significant impact on the human resources function and its experts. This chapter reviews the existing literature on this topic and considers the advantages and benefits of HRM information systems. It also outlines some of the technical applications in the functional areas of HRM in the organization.}
}
@incollection{GRANTMULLER2021135,
title = {Technology Enabled Data for Sustainable Transport Policy},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {135-141},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10627-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102671710627X},
author = {Susan M. Grant-Muller and Mahmoud Abdelrazek and Hannah Budnitz and Caitlin D. Cottrill and Fiona Crawford and Charisma F. Choudhury and Teddy Cunningham and Gillian Harrison and Frances C. Hodgson and Jinhyun Hong and Adam Martin and Oliver O’Brien and Claire Papaix and Panagiotis Tsoleridis},
keywords = {Big data, Ethics, Influencing technologies, New and emerging data forms, Policy, Transport modeling},
abstract = {The explosive growth of New and Emerging Data Forms (NEDF) has enabled profound new insights into human behavior, especially related to mobility. NEDF are facilitated by technologies such as smartphones, sensor networks and distributed computing architectures, which are all becoming increasingly advanced and widespread. NEDF, which may offer large sample sizes of high resolution data, offer great potential for informing sustainable transport policy, as well as the development of crosssectoral policies, covering public-health, environment, land-use, and social equity. However, many challenges in exploiting NEDF exist including accessing data in public/private ownership; understanding the representativeness, measuring/accommodating biases/missing data; and the integration of traditional data with new forms to maximize overall utility. Questions remain on whether NEDF can be used to actively influence travel choice/behavior, the new skills and additional resources needed by stakeholders to realize data potential and the ethical challenges for all engaging with the data.}
}
@article{OBERDORF2021103481,
title = {Analytics-enabled escalation management: System development and business value assessment},
journal = {Computers in Industry},
volume = {131},
pages = {103481},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103481},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000889},
author = {Felix Oberdorf and Nikolai Stein and Christoph M. Flath},
keywords = {Escalation management, Industry 4.0, Machine learning, Business analytics},
abstract = {Industry 4.0 initiatives can help traditional manufacturing industry cope with increasing global competition. Such solutions facilitate transparency, automation as well as business process transformation. This paper elaborates on a collaboration with a medium-sized manufacturing company. We highlight the design, evaluation and roll-out of an escalation management system with integrated data-driven decision support. We do so by applying an action design research process. Thereby, our study focuses on the system design concerning the creation of business value. The system leverages state-of-the-art machine learning algorithms for disruption type classification and escalation handling duration prediction. These predictions can be embedded in an integrated planning procedure leveraging diverse organizational data sources (e.g., personnel availability, production plans) to instantiate a prescriptive analytics solution. Combined with informative analytics insights, this allows the proposed system to generate significant business value by reducing escalation durations. In the long run, the transformational business value enabled by the system is likely to exceed the automational business value. This highlights the special importance of tight integration of industrial analytics applications within business processes.}
}
@article{OGRADY2021328,
title = {Service design for climate-smart agriculture},
journal = {Information Processing in Agriculture},
volume = {8},
number = {2},
pages = {328-340},
year = {2021},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2020.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214317320301906},
author = {Michael O'Grady and David Langton and Francesca Salinari and Peter Daly and Gregory O'Hare},
keywords = {Smart agriculture, Climate services, Agrometeorology, Precision agriculture},
abstract = {Holistic information systems for climate-smart agriculture demands the seamless integration of various categories of climate, meteorological and weather data. Any actor in the agricultural value chain may harness weather forecasts at the short and medium-range, local weather history, and prevailing climatic conditions, to inform decision-making. Weather is fundamental to many day-to-day operations, especially at farm-level, influencing decision-making at various spatial and temporal scales. Many operational decisions ideally require hyper-localized service provision. In practice, integrating weather information into decision-support services demands a comprehensive understanding of various categories of weather-related data, their genesis, as well as the specific standards and data formats used by the meteorological community. This paper considers the weather as a crucial context for the delivery of farm-level operational services in smart agriculture, highlighting critical issues for reflection by system designers during the service design and implementation phases.}
}
@article{ALVAREZCOELLO2021658,
title = {Towards a Data-Centric Architecture in the Automotive Industry},
journal = {Procedia Computer Science},
volume = {181},
pages = {658-663},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.215},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002581},
author = {Daniel Alvarez-Coello and Daniel Wilms and Adnan Bekan and Jorge {Marx Gómez}},
keywords = {Connected vehicles, data-centric architecture, standardized data, semantic AI, modern data architecture},
abstract = {Vehicle software architectures have been evolving over the last twenty years to support data-driven functionalities. Several enterprises from different domains are currently focusing on improving their data architectures by re-defining the underlying data models to enable core support for analytics and artificial intelligence. Moreover, a common desire to add clear data provenance and explicit context impulses the field of semantics and knowledge graphs. Nevertheless, in the automotive industry, the scenario of connected vehicles implies extra complexity. Vehicle data has an enormous variety, making it essential to develop and adopt standards. This paper presents aspects of ongoing research at the BMW Research Department regarding a conceptual design for vehicle software architectures in the automotive industry. We discuss the principles of a modern data architecture with particular emphasis on the data-centric mindset. We also explore the current challenges and possible working points as the foundation to move from siloed data towards a so-called AI factory.}
}
@article{LI2021106183,
title = {A review of artificial neural network based chemometrics applied in laser-induced breakdown spectroscopy analysis},
journal = {Spectrochimica Acta Part B: Atomic Spectroscopy},
volume = {180},
pages = {106183},
year = {2021},
issn = {0584-8547},
doi = {https://doi.org/10.1016/j.sab.2021.106183},
url = {https://www.sciencedirect.com/science/article/pii/S0584854721001403},
author = {Lu-Ning Li and Xiang-Feng Liu and Fan Yang and Wei-Ming Xu and Jian-Yu Wang and Rong Shu},
keywords = {Laser-induced breakdown spectroscopy, Artificial neural network, Machine learning, Chemometrics, Classification},
abstract = {In the past decades various categories of chemometrics for laser-induced breakdown spectroscopy (LIBS) analysis have been developed, among which an important category is that based on artificial neural network (ANN). The most common ANN scheme employed in LIBS researches so far is back-propagation neural network (BPNN), while there are also several other kinds of neural networks appreciated by the LIBS community, including radial basis function neural network (RBFNN), convolutional neural network (CNN), self-organizing map (SOM), etc. In this paper, we introduce the principles of some representative ANN methods, and offer criticism on their features along with comparison between them. Then we afford an overview of ANN-based chemometrics applied in LIBS analysis, involving material identification/classification, component concentration quantification, and some unconventional applications as well. Furthermore, a comprehensive discussion on ANN-LIBS methodologies is provided from four aspects. First, a few general progressing trends are displayed. Next we expound some specific implementation techniques, including variable selection, network construction, data set utilization, network training, model evaluation, and chemometrics selection. In addition, the limitations of ANN approaches are remarked, mainly concerning overfitting and interpretability. Finally a prospect of future development of ANN-LIBS chemometrics is presented. Throughout the discussion quite a few good practices have been highlighted. This review is expected to shed light on the further upgrade of ANN-based LIBS chemometrics in the future.}
}
@incollection{KUBASSOVA20211,
title = {Chapter 1 - History, current status, and future directions of artificial intelligence},
editor = {Michael Mahler},
booktitle = {Precision Medicine and Artificial Intelligence},
publisher = {Academic Press},
pages = {1-38},
year = {2021},
isbn = {978-0-12-820239-5},
doi = {https://doi.org/10.1016/B978-0-12-820239-5.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202395000024},
author = {Olga Kubassova and Faiq Shaikh and Carlos Melus and Michael Mahler},
keywords = {Artificial intelligence, Neural networks, Rheumatoid arthritis, Fatty liver disease, Electrocardiography, Blockchain technology},
abstract = {Artificial intelligence (AI) as a technology concept is making a major impact on a wide range of industries and sectors. This is largely attributed to technical advancements in machine and especially deep learning methodologies fueled by improved computational capabilities which have led to sophisticated approaches in applying AI to various scenarios. These AI applications aim to improve productivity, decrease cost, and comprehend the ever-increasing volumes of data available to ultimately provide actionable insights. Included in this paradigm shift is medicine, where AI is beginning to enable clinical assistance, decision support, improved management and accelerated scientific discovery and development.}
}
@article{HASHMI2021101316,
title = {Transdisciplinary systems approach to realization of digital transformation},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101316},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101316},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000690},
author = {Muhammad A. Hashmi and John P.T. Mo and Ronald C. Beckett},
keywords = {Transdisciplinary engineering, System of systems, Cyber physical systems, Data analytics, Work 4.0, Data integrity},
abstract = {With advancement in technology and emergence of fast networks, operation of businesses and global companies increasingly depend on the Internet and digital transformation of their infrastructures. Adaptation to Industry 4.0 paradigm gives rise to societal, technological and communication issues due to challenges in product, services, social and inter-disciplinary interactions. A more fundamental approach that can mitigate complexity of the new business is necessary. This paper adopts a system of systems model embedded into a transdisciplinary system design to describe a typical X4.0 system where X can be any industry sector migrating to Industry 4.0 paradigm. Two industry sectors: Education 4.0 and Retail 4.0 are studied under the amalgamated transdisciplinary system of systems model. Results show that the four artifacts in X4.0 can form the foundation of new sectorial 4.0 development with focus on specific elements in Cyber Physical Systems and Work4.0 artifacts. The transdisciplinary system approach has the advantage of a self-improving model that drives realization of digital transformation in evolutionary cycles.}
}
@article{LU2021103816,
title = {Exploring smart construction objects as blockchain oracles in construction supply chain management},
journal = {Automation in Construction},
volume = {129},
pages = {103816},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103816},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002673},
author = {Weisheng Lu and Xiao Li and Fan Xue and Rui Zhao and Liupengfei Wu and Anthony G.O. Yeh},
keywords = {Blockchain, Oracles, Smart contract, Supply chain management, Smart construction objects, Prefabricated construction},
abstract = {Blockchain technology has attracted the interest of the global construction industry for its potential to enhance the transparency, traceability, and immutability of construction data and enables collaboration and trust throughout the supply chain. However, such potential cannot be achieved without blockchain “oracles” needed to bridge the on-chain (i.e., blockchain system) and off-chain (i.e., real-life physical project) worlds. This study presents an innovative solution that exploits smart construction objects (SCOs). It develops a SCOs-enabled blockchain oracles (SCOs-BOs) framework. To instantiate this framework, the system architecture of a blockchain-enabled construction supply chain management (BCSCM) system is developed and validated using a case study, whereby four primary smart contracts are examined in the context of off-site logistics and on-site assembly services. The validation results show that accurate data is retrieved against malicious data in each request, and the corresponding reputation scores are successfully recorded. The innovativeness of the research lies in two aspects. In addition to mobilizing SCOs as blockchain oracles to bridge the on-chain and off-chain worlds, it develops a decentralized SCO network to avoid the single point of failure (SPoF) problem widely existing in blockchain systems. This study contributes to existing research and practice to harness the power of blockchain in construction.}
}
@article{DU2021113581,
title = {Neighbor-aware review helpfulness prediction},
journal = {Decision Support Systems},
volume = {148},
pages = {113581},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113581},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621000919},
author = {Jiahua Du and Jia Rong and Hua Wang and Yanchun Zhang},
keywords = {Review helpfulness, Sequential bias, Review neighbors, Context clues, Deep learning},
abstract = {Helpfulness prediction techniques have been widely incorporated into online decision support systems to identify high-quality reviews. Most current studies on helpfulness prediction assume that a review's helpfulness only relies on information from itself. In practice, however, consumers hardly process reviews independently because reviews are displayed in sequence; a review is more likely to be affected by its adjacent neighbors in the sequence, which is largely understudied. In this paper, we proposed the first end-to-end neural architecture to capture the missing interaction between reviews and their neighbors. Our model allows for a total of 12 (three selection × four aggregation) schemes that contextualize a review into the context clues learned from its neighbors. We evaluated our model on six domains of real-world online reviews against a series of state-of-the-art baselines. Experimental results confirm the influence of sequential neighbors on reviews and show that our model significantly outperforms the baselines by 1% to 5%. We further revealed how reviews are influenced by their neighbors during helpfulness perception via extensive analysis. The results and findings of our work provide theoretical contributions to the field of review helpfulness prediction and offer insights into practical decision support system design.}
}
@article{EACHEMPATI2021120903,
title = {Validating the impact of accounting disclosures on stock market: A deep neural network approach},
journal = {Technological Forecasting and Social Change},
volume = {170},
pages = {120903},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120903},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521003358},
author = {Prajwal Eachempati and Praveen Ranjan Srivastava and Ajay Kumar and Kim Hua Tan and Shivam Gupta},
keywords = {Disclosures, Data intelligence, Analytics, Finance, Machine learning, Deep learning, Stock market, Forecasts, Private decision-making},
abstract = {Firms disclose information either voluntarily or due to the regulator's mandatory requirements, and such disclosures form good sources to know the prospects of a firm. Information in the disclosures and analysts' opinions influence investor-trading behavior, and consequently, affects the asset prices. As sentiments factored in disclosures are a source of market action, this study aims to capture the sentiments from disclosure information to assess asset prices' impact. The paper adopts a deep neural network-based prediction model for conducting sentiment analysis on heterogeneous datasets. We construct a sentiment simulation model of voluntary disclosures to know whether the managers can use the market sentiment as a strategic input to boost market performance by suitably drafting the tone and content of disclosures without compromising their quality and veracity. The Deep Neural Networks with LSTM algorithm is found to outperform the Deep Neural Networks with RNN and other baseline machine learning classifiers in terms of predictive accuracy of the NSE NIFTY50. The variable importance computed also validates that market news, combined with historical indicators, predicts the stock market trend closer to the actual trend.}
}
@article{YAN2021128665,
title = {A stack-based set inversion model for smart water, carbon and ecological assessment in urban agglomerations},
journal = {Journal of Cleaner Production},
volume = {319},
pages = {128665},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128665},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621028651},
author = {Pengdong Yan and Hongwei Lu and Yizhong Chen and Ziheng Li and Hao Li},
keywords = {Water, Carbon and ecological footprints, Smart evaluation and prediction, Ensemble inversion model, Urban agglomeration, Yangtze river},
abstract = {Footprint evaluation is an important tool for assessing the appropriation of ecological assets, GHG emissions, freshwater consumption parameters, etc., within a specified region. However, traditional evaluation of footprints for mega cities or urban agglomerations requires overmuch different types of high-quality data. There is a great need of seeking a smart model/approach with declined data requirements for evaluation of footprints where part of data can hardly be accessed. Here we propose a new ensemble inversion model (EIM) based on integrated multitask machine learning (MML) and multi-modeling stacking (MMS) algorithms for smart evaluation and prediction of water, carbon and ecological footprints. The accuracy and generalization capability of the model are illustrated through three largest urban agglomerations in the middle reaches of the Yangtze River (MRYR). The testing results show that the EIM achieves similar prediction performance compared to traditional footprints calculation methods (R2 = 0.91, RMSE = 0.18, MAE = 0.11), yet greatly reduces the amount of required data by approximately 80%. Moreover, the accuracy of the EIM is improved by more than 20%, compared with other models using a single inversion algorithm. The modeling results also show that 1) water, carbon and ecological footprints are significantly positively correlated, and 2) an annual increase of 4.8% can be found in terms of the urban environmental pressure index (UEPI), and its projection is even less optimistic for the future.}
}
@article{ARNARDOTTIR2021447,
title = {The Future of Sleep Measurements: A Review and Perspective},
journal = {Sleep Medicine Clinics},
volume = {16},
number = {3},
pages = {447-464},
year = {2021},
note = {Sleep Medicine: Current Challenges and its Future},
issn = {1556-407X},
doi = {https://doi.org/10.1016/j.jsmc.2021.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1556407X21000333},
author = {Erna Sif Arnardottir and Anna Sigridur Islind and María Óskarsdóttir},
keywords = {Sleep measurement, Subjective data, Objective data, Sleep diary, Codesign, Machine learning, Data management platform, Data science}
}
@article{PRIKSHAT2021100860,
title = {AI-augmented HRM: Antecedents, assimilation and multilevel consequences},
journal = {Human Resource Management Review},
pages = {100860},
year = {2021},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2021.100860},
url = {https://www.sciencedirect.com/science/article/pii/S1053482221000395},
author = {Verma Prikshat and Ashish Malik and Pawan Budhwar},
keywords = {Technology-driven HRM, AI-adoption in HRM, AI-augmented HRM, Processual factors},
abstract = {The current literature on the use of disruptive innovative technologies, such as artificial intelligence (AI) for human resource management (HRM) function, lacks a theoretical basis for understanding. Further, the adoption and implementation of AI-augmented HRM, which holds promise for delivering several operational, relational and transformational benefits, is at best patchy and incomplete. Integrating the technology, organisation and people (TOP) framework with core elements of the theory of innovation assimilation and its impact on a range of AI-Augmented HRM outcomes, or what we refer to as (HRM(AI)), this paper develops a coherent and integrated theoretical framework of HRM(AI) assimilation. Such a framework is timely as several post-adoption challenges, such as the dark side of processual factors in innovation assimilation and system-level factors, which, if unattended, can lead to the opacity of AI applications, thereby affecting the success of any HRM(AI). Our model proposes several testable future research propositions for advancing scholarship in this area. We conclude with implications for theory and practice.}
}
@article{PAUL2021100296,
title = {Mobile phone technologies for disaster risk reduction},
journal = {Climate Risk Management},
volume = {32},
pages = {100296},
year = {2021},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100296},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321000255},
author = {Jonathan D. Paul and Emma Bee and Mirianna Budimir},
keywords = {Citizen science, Disaster risk management (DRM), Disaster risk reduction (DRR), Mobile phone technologies, Natural hazards, User-centered design (UCD)},
abstract = {The explosion of increasingly sophisticated mobile phone technologies can usefully be harnessed by disaster risk reduction (DRR) as a means of enhancing inclusivity and local relevance of knowledge production and resilience building. However, much new technology is designed on an ad hoc basis without considering user needs – especially mobile applications (apps), which often terminate at the proof-of-concept stage. Here, we examine best practice by marshalling learnings from 45 workers representing 20 organisations working globally across the disaster risk management (DRM) lifecycle, including physical and social science, NGOs, technological developers, and (inter)governmental regulatory bodies. We present a series of generalisable and scalable guidelines that are novel in being independent of any specific natural hazard or development setting, designed to maximise the positive societal impact of exploiting mobile technologies. Specifically, the local context, dynamics, and needs must be carefully interrogated a priori, while any product should ideally be co-developed with local stakeholders through a user-centered design approach.}
}
@article{SAMAL2021100800,
title = {Multi-directional temporal convolutional artificial neural network for PM2.5 forecasting with missing values: A deep learning approach},
journal = {Urban Climate},
volume = {36},
pages = {100800},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100800},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521000304},
author = {K. Krishna Rani Samal and Korra Sathya Babu and Santos Kumar Das},
keywords = {Deep learning, Time series forecasting, Temporal Convolutional Network, Artificial Neural Network, PM2.5},
abstract = {Data imputation and forecasting are the major research areas in environmental data engineering. Solving those critical issues has an immense impact on air pollution management, consequently improving social, economic growth, and public health. Missing data is a common issue for all the domains, especially for environmental data analysis. Most of the research study tries to solve all these problems of time series data using different models. This research study presents a novel deep learning-based hybrid model architecture to solve these issues in a single training process. We come up with Multi-directional Temporal Convolutional Artificial Neural Network (MTCAN) model to impute and forecast PM2.5 pollutant concentration in a single training process. The main idea of the multi-directional properties of MTCAN is to interpolate the PM2.5 pollutant feature matrix to impute its value. Ultimately, it maintains the temporal correlation within the features' measurement and meteorological and pollutant variables to impute PM2.5 missing values. The MTCAN model performs feature learning and sequential modeling simultaneously with a wide range of past observations for long-term forecasting, minimizing memory size requirement and training cost. Experimental results indicate that the proposed model is superior to baseline pollution forecasting models, which prove its effectiveness in air quality modeling.}
}
@article{PARRALOPEZ2021105537,
title = {Digital transformation of the agrifood system: Quantifying the conditioning factors to inform policy planning in the olive sector},
journal = {Land Use Policy},
volume = {108},
pages = {105537},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2021.105537},
url = {https://www.sciencedirect.com/science/article/pii/S026483772100260X},
author = {Carlos Parra-López and Liliana Reina-Usuga and Carmen Carmona-Torres and Samir Sayadi and Laurens Klerkx},
keywords = {Digitalisation, Olive, SWOT, PESTLE, AHP, TOWS},
abstract = {Despite the growing importance of the digital transformation (DT) of the agrifood sector on the political agenda, traditional policies are not enough to provide proactive responses to rapid technological changes and new approaches for policy planning are necessary especially at regional level. This manuscript proposes and illustrates the implementation of a new methodological framework for DT policy planning in the case of Andalusia, the olive world leader region, but applicable to other regions and sectors, with two objectives: 1) to quantitatively determine the importance of the conditioning factors of DT in the olive sector in the short/medium term, by developing an AHP/SWOT/PESTLE model, and 2) to design public policies to strengthen the DT, taking advantage of the potentialities and alleviating the deficiencies, by carrying out a quantitative TOWS analysis. The knowledge of diverse groups of experts, i.e. stakeholders in the sector, has been used in all analyses due to the lack of reliable data and the complex nature of the issues analysed. The results show that the opportunities and strengths are more prominent than weaknesses and threats for DT. Environmental issues stand out as an opportunity to boost DT. There is also a growing interest in developing an interoperability strategy which is an opportunity to overcome the low technological integration of the value chain. DT can also enable a more transparent value chain and improved traceability. Some negative factors are the lack of evidence on the economic viability of investment in digital technologies, shortage of labour and young farmers, and potential unintended and unanticipated effects of DT. Important policies strategies to foster DT are: improving environmental efficiency though DT; promoting youth employment in the sector; enhancing coordination among innovation actors; developing a common interoperability strategy; and fostering technological integration in the sector.}
}
@article{PRASTYO2021108,
title = {A Combination of Query Expansion Ranking and GA-SVM for Improving Indonesian Sentiment Classification Performance},
journal = {Procedia Computer Science},
volume = {189},
pages = {108-115},
year = {2021},
note = {AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921011698},
author = {Pulung Hendro Prastyo and Igi Ardiyanto and Risanuri Hidayat},
keywords = {Query Expansion Ranking, Genetic Algorithm, Feature Selection, Machine Learning, Sentiment Classification},
abstract = {The sentiment classification method is a research field that is proliferating in Indonesia since it is fast in extracting public opinion and provides essential and valuable information for stakeholders. Of the best-performing sentiment classification approaches, machine learning is one of them that has excellent performance. However, the method has several problems, such as noisy features and high dimensionality of features that significantly affect the sentiment classification performance. Therefore, to overcome the problems, this paper presents a novel feature selection using a combination of Query Expansion Ranking (QER) and Genetic Algorithm-Support Vector Machine (GA-SVM) for improving sentiment classification performance. Based on the experimental results, the proposed method could significantly improve sentiment classification performance, outperform all state-of-the-art algorithms, and decrease computational time. The method achieved the best performance in average precision, recall, and f-measure with the value of 96.78%, 96.76%, and 96.75%, respectively.}
}
@article{WIERINGA2021915,
title = {Data analytics in a privacy-concerned world},
journal = {Journal of Business Research},
volume = {122},
pages = {915-925},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319303078},
author = {Jaap Wieringa and P.K. Kannan and Xiao Ma and Thomas Reutterer and Hans Risselada and Bernd Skiera},
abstract = {Data is considered the new oil of the economy, but privacy concerns limit their use, leading to a widespread sense that data analytics and privacy are contradictory. Yet such a view is too narrow, because firms can implement a wide range of methods that satisfy different degrees of privacy and still enable them to leverage varied data analytics methods. Therefore, the current study specifies different functions related to data analytics and privacy (i.e., data collection, storage, verification, analytics, and dissemination of insights), compares how these functions might be performed at different levels (consumer, intermediary, and firm), outlines how well different analytics methods address consumer privacy, and draws several conclusions, along with future research directions.}
}
@article{NEMOTO2021160,
title = {A Pitfall of Learning from User-generated Data: In-Depth Analysis of Subjective Class Problem},
journal = {Procedia Computer Science},
volume = {185},
pages = {160-169},
year = {2021},
note = {Big Data, IoT, and AI for a Smarter Future},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101098X},
author = {Kei Nemoto and Shweta Jain},
keywords = {Text classification, Supervised learning, Subjective class, Class noise},
abstract = {Research in the supervised learning algorithms field implicitly assumes that training data is labeled by domain experts or at least semi-professional labelers accessible through crowdsourcing services like Amazon Mechanical Turk. With the advent of the Internet, data has become abundant and a large number of machine learning based systems are being trained with user-generated data, where categorical data is used as labels. However, little work has been done in the area of supervised learning with user-defined labels where users are not necessarily experts and might be unable to provide correct labels to some data or the labels might contain significant human bias. In this article, we propose two types of classes in user-defined labels: subjective class and objective class - showing that the objective classes are as reliable as if they were provided by domain experts, whereas the subjective classes are subject to error and bias. We name this a subjective class problem and propose a Normalized Feature Indicative Score that can be effective in detecting subjective classes in a dataset without querying oracle. This score provides early detection of subjective classes in the data, saving time for data mining practitioners working with data that might contain errors and biases.}
}
@incollection{2021419,
title = {Index},
editor = {Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas},
booktitle = {Machine Learning in Cardiovascular Medicine},
publisher = {Academic Press},
pages = {419-424},
year = {2021},
isbn = {978-0-12-820273-9},
doi = {https://doi.org/10.1016/B978-0-12-820273-9.20001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202739200018}
}
@article{SAMAL2021100943,
title = {Multi-output TCN autoencoder for long-term pollution forecasting for multiple sites},
journal = {Urban Climate},
volume = {39},
pages = {100943},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100943},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521001735},
author = {K. Krishna Rani Samal and Ankit Kumar Panda and Korra Sathya Babu and Santos Kumar Das},
keywords = {Temporal convolutional network, Spatio-temporal prediction, Autoencoder, Deep learning, Pollution},
abstract = {Air pollution is one of the major environmental issues attracting massive attention from researchers and policymakers. Both the developed and developing countries are undergoing a high concentration of pollution levels. Fine particulate matter PM2.5 (particles having a diameter less than 2.5μm) and PM10 (particles having a diameter less than 10μm) can easily penetrate the lungs and respiratory system and causes adverse health issues like heart attacks, cardiovascular diseases, lung function reduction. Real-time pollutant information is of great importance to providing prompt and complete information on air quality. Air pollution forecasting is another significant step of air pollution management, which can help policymakers and citizens make proper decisions to prevent air pollution-related diseases. This research study explores a novel pollutant forecasting model named as Multi-output temporal convolutional network autoencoder (MO-TCNA). The model accumulates each step's predicted values to perform multi-step ahead long-term forecasting for multiple pollutants and multiple sites in a single training model. The MO-TCNA network serves both the PM2.5 and PM10 pollutants forecasting for various locations instead of performing single output and site-specific pollutant forecasting. Consequently, the experimental results show that the MO-TCNA network is time-saving and has better performance than the traditional site-specific forecasting models.}
}
@article{ZHENG2021103831,
title = {Research on the strategy of mobile short video in product sales based on 5G network and embedded system},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103831},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.103831},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121000119},
author = {Lu Zheng and Shikun Liu},
keywords = {Recurrent Neural Network (RNN), Mobile short video, Product sales data upload bandwidth size, Embedded systems, 5G technology},
abstract = {Mobile short video-based product sales sharing sites like YouTube and Tudor have many established user content for creating and distributing shares. The increasing number of mobile devices for product sales leads to the upcoming new 5G technology roadmap for embedded systems and 5G network connectivity. As these are the main sources of 5G information and online activities for consumers, mobile phone short films are rapidly being replaced by embedded systems. As the demand for more embedded system devices and applications continues to grow, supported bandwidth is also essential to meet this growing connection demand. The existing system does not allocate the product sales data upload bandwidth size. The system proposed here focuses on user upload bandwidth allocation, one of the basic resources of a short video sharing system with product details. Its allocation upload bandwidth Recurrent Neural Network (RNN) algorithm is proposed in a centralized or decentralized way and evaluated for balancing widely used strategies (equal allocation) and a mobile short video. Embedded systems are responsible for running professional product sales and control applications consistently and predictably. Development while using the microprocessor is also important. It increases the need to process product sales to handle the bandwidth, latency requirements, product sales data and data generated from multiple connected devices. It's a big challenge for the industry to data and data capabilities.}
}
@article{THAPA2021104130,
title = {Precision health data: Requirements, challenges and existing techniques for data security and privacy},
journal = {Computers in Biology and Medicine},
volume = {129},
pages = {104130},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2020.104130},
url = {https://www.sciencedirect.com/science/article/pii/S0010482520304613},
author = {Chandra Thapa and Seyit Camtepe},
keywords = {Precision health, Legal requirements, Ethical guidelines, Security, Privacy, Artificial intelligence},
abstract = {Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Besides, the public, who is the data source, always expects the security, privacy, and trust of their data. Otherwise, they can avoid contributing their data to the precision health system. Consequently, as the public is the targeted beneficiary of the system, the effectiveness of precision health diminishes. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.}
}
@article{HAO202155,
title = {Optimized data manipulation methods for intensive hesitant fuzzy set with applications to decision making},
journal = {Information Sciences},
volume = {580},
pages = {55-68},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.08.063},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521008719},
author = {Zhinan Hao and Zeshui Xu and Hua Zhao and Zhan Su},
keywords = {Hesitant fuzzy set, Fuzzy data optimization, Data aggregation method, Fuzzy decision making},
abstract = {The emergence of large amounts of hesitant fuzzy data brings more opportunities and challenges for optimal decision-making results. The granularity of the hesitant fuzzy set has been significantly improved, but the potential for more inconsistent data also increases. Due to high computational complexity and low efficiency, the existing data aggregation methods cannot handle the intensive hesitant fuzzy data. To solve this problem, we first propose the optimization strategy for the intensive hesitant fuzzy data. Then the data redundancy elimination method and the data aggregation method are proposed. The extensions of hesitant fuzzy set and normal-type hesitant fuzzy set are proposed for intensive hesitant fuzzy data manipulations. The aggregation methods and the data structure estimation methods of the extensions are also discussed. Finally, a practical application to the wine quality determination is provided, and some comparative analyses are conducted to demonstrate the effectiveness of the proposed methods.}
}
@article{ALI2021111073,
title = {Review of urban building energy modeling (UBEM) approaches, methods and tools using qualitative and quantitative analysis},
journal = {Energy and Buildings},
volume = {246},
pages = {111073},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111073},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821003571},
author = {Usman Ali and Mohammad Haris Shamsi and Cathal Hoare and Eleni Mangina and James O’Donnell},
keywords = {Urban building energy modeling, Top-down, Bottom-up, Data-driven, Energy modeling, UBEM, Energy efficiency, SWOT},
abstract = {The world has witnessed a significant population shift to urban areas over the past few decades. Urban areas account for about two-thirds of the world’s total primary energy consumption, of which the urban building sector constitutes a significant proportion approximately 40%. Stakeholders such as urban planners and policy makers face substantial challenges when targeting sustainable energy and climate goals related to the buildings’ sector, i.e. to reduce energy use and associated emissions. Urban energy modeling is one possible solution that leverages limited resources to estimate building energy use and support appropriate policy formation. Over the past few years, there have been only a few review studies on urban building energy modeling approaches. These studies lack an in-depth discussion of the challenges and future research opportunities related to data-driven, reduced-order, and simulation-based modeling methods. This paper proposes Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis of approaches, methods and tools used for urban building energy modeling. Furthermore, this paper proposes a generalized framework based on existing literature for different urban energy modeling methods. The aim of this study is to assist urban planners and energy policymakers when choosing appropriate methods to develop and implement in-depth sustainable building energy planning and analysis projects based on limited available resources.}
}
@article{RAMYA2021114141,
title = {Establishment of bioinformatics pipeline for deciphering the biological complexities of fragmented sperm transcriptome},
journal = {Analytical Biochemistry},
volume = {620},
pages = {114141},
year = {2021},
issn = {0003-2697},
doi = {https://doi.org/10.1016/j.ab.2021.114141},
url = {https://www.sciencedirect.com/science/article/pii/S0003269721000427},
author = {Laxman Ramya and Divakar Swathi and Santhanahalli Siddalingappa Archana and Maharajan Lavanya and Sivashanmugam Parthipan and Sellappan Selvaraju},
keywords = {Bioinformatics pipeline, Fragmented transcripts, Transcriptomics, Differential gene expression, Bovine spermatozoa},
abstract = {Despite the development of several tools for the analysis of the transcriptome data, non-availability of a standard pipeline for analyzing the low quality and fragmented mRNA samples pose a major challenge to the computational molecular biologist for effective interpretation of the data. Hence the present study aimed to establish a bioinformatics pipeline for analyzing the biologically fragmented sperm RNA. Sperm transcriptome data (2 x 75 PE sequencing) generated from bulls (n = 8) of high-fertile (n = 4) and low-fertile (n = 4) classified based on the fertility rate (41.52 ± 1.07 vs 36.04 ± 1.04%) were analyzed with different bioinformatics tools for alignment, quantitation, and differential gene expression studies. TopHat2 was effectual compared to HISAT2 and STAR for sperm mRNA due to the higher exonic (6% vs 2%) mapping percentage and quantitating the low expressed genes. TopHat2 also had significantly strong correlation with STAR (0.871, p = 0.05) and HISAT2 (0.933, p = 0.01). TopHat2 and Cufflinks combo quantitated the number of genes higher than the other combinations. Among the tools (Cuffdiff, DESeq, DESeq2, edgeR, and limma) used for the differential gene expression analysis, edgeR and limma identified the largest number of significantly differentially expressed genes (p < 0.05) with biological relevance. The concordance analysis concurred that edgeR had an edge over the other tools. It also identified a higher number (9.5%) of fertility-related genes to be differentially expressed between the two groups. The present study established that TopHat2, Cufflinks, and edgeR as a suitable pipeline for the analysis of fragmented mRNA from bovine spermatozoa.}
}
@article{SCHNEBELIN2021599,
title = {How digitalisation interacts with ecologisation? Perspectives from actors of the French Agricultural Innovation System},
journal = {Journal of Rural Studies},
volume = {86},
pages = {599-610},
year = {2021},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2021.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0743016721002205},
author = {Éléonore Schnebelin and Pierre Labarthe and Jean-Marc Touzard},
keywords = {Digitalisation, Agriculture, Digital technology, Agricultural innovation system, Organic farming, Institutional economics, Ecological transition},
abstract = {Two major agricultural transformations are currently being promoted worldwide: digitalisation and ecologisation, that include different practices such as organic farming and sustainable intensification. In literature and in societal debates, these two transformations are sometimes described as antagonistic and sometimes as convergent but are rarely studied together. Using an innovation system approach, this paper discusses how diverse ecologisation pathways grasp digitalisation in the French agricultural sector; and do not discriminate against organic farming. Based on interviews with key representatives of conventional agriculture, organic agriculture and organisations that promote or develop digital agriculture, we explore how these actors perceive and participate in digital development in agriculture. We show that although all the actors are interested and involved in digital development, behind this apparent convergence, organic and conventional actors perceive neither the same benefits nor the same risks and consequently do not implement the same innovation processes. We conclude that digitalisation has different meanings depending on the actors’ paradigm, but that digital actors fail to perceive these differences. This difference in perception should be taken into account if digital development is to benefit all kinds of agriculture and not discriminate against organic farming and more widely, against agroecology.}
}
@article{ZUO2021101032,
title = {Reference-free video-to-real distance approximation-based urban social distancing analytics amid COVID-19 pandemic},
journal = {Journal of Transport & Health},
volume = {21},
pages = {101032},
year = {2021},
issn = {2214-1405},
doi = {https://doi.org/10.1016/j.jth.2021.101032},
url = {https://www.sciencedirect.com/science/article/pii/S2214140521000268},
author = {Fan Zuo and Jingqin Gao and Abdullah Kurkcu and Hong Yang and Kaan Ozbay and Qingyu Ma},
keywords = {Social distancing, COVID-19, Close contact, Pedestrian, Deep learning, Computer vision},
abstract = {Introduction
The rapidly evolving COVID-19 pandemic has dramatically reshaped urban travel patterns. In this research, we explore the relationship between “social distancing,” a concept that has gained worldwide familiarity, and urban mobility during the pandemic. Understanding social distancing behavior will allow urban planners and engineers to better understand the new norm of urban mobility amid the pandemic, and what patterns might hold for individual mobility post-pandemic or in the event of a future pandemic.
Methods
There are still few efforts to obtain precise information on social distancing patterns of pedestrians in urban environments. This is largely attributed to numerous burdens in safely deploying any effective field data collection approaches during the crisis. This paper aims to fill that gap by developing a data-driven analytical framework that leverages existing public video data sources and advanced computer vision techniques to monitor the evolution of social distancing patterns in urban areas. Specifically, the proposed framework develops a deep-learning approach with a pre-trained convolutional neural network to mine the massive amount of public video data captured in urban areas. Real-time traffic camera data collected in New York City (NYC) was used as a case study to demonstrate the feasibility and validity of using the proposed approach to analyze pedestrian social distancing patterns.
Results
The results show that microscopic pedestrian social distancing patterns can be quantified by using a generalized real-distance approximation method. The estimated distance between individuals can be compared to social distancing guidelines to evaluate policy compliance and effectiveness during a pandemic. Quantifying social distancing adherence will provide decision-makers with a better understanding of prevailing social contact challenges. It also provides insights into the development of response strategies and plans for phased reopening for similar future scenarios.}
}
@incollection{2021237,
title = {Index},
editor = {Stephanie Kay Ashenden},
booktitle = {The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry},
publisher = {Academic Press},
pages = {237-245},
year = {2021},
isbn = {978-0-12-820045-2},
doi = {https://doi.org/10.1016/B978-0-12-820045-2.09997-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200452099979}
}
@article{SINGH202153,
title = {Challenges and Opportunities in Machine-Augmented Plant Stress Phenotyping},
journal = {Trends in Plant Science},
volume = {26},
number = {1},
pages = {53-69},
year = {2021},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2020.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1360138520302405},
author = {Arti Singh and Sarah Jones and Baskar Ganapathysubramanian and Soumik Sarkar and Daren Mueller and Kulbir Sandhu and Koushik Nagasubramanian},
keywords = {image-based phenotyping, machine learning, deep learning, biotic stress, abiotic stress, standard area diagram},
abstract = {Plant stress phenotyping is essential to select stress-resistant varieties and develop better stress-management strategies. Standardization of visual assessments and deployment of imaging techniques have improved the accuracy and reliability of stress assessment in comparison with unaided visual measurement. The growing capabilities of machine learning (ML) methods in conjunction with image-based phenotyping can extract new insights from curated, annotated, and high-dimensional datasets across varied crops and stresses. We propose an overarching strategy for utilizing ML techniques that methodically enables the application of plant stress phenotyping at multiple scales across different types of stresses, program goals, and environments.}
}
@article{HAO2021e01512,
title = {Different response of alpine meadow and alpine steppe to climatic and anthropogenic disturbance on the Qinghai-Tibetan Plateau},
journal = {Global Ecology and Conservation},
volume = {27},
pages = {e01512},
year = {2021},
issn = {2351-9894},
doi = {https://doi.org/10.1016/j.gecco.2021.e01512},
url = {https://www.sciencedirect.com/science/article/pii/S2351989421000627},
author = {Aihua Hao and Hanchen Duan and Xufeng Wang and Guohui Zhao and Quangang You and Fei Peng and Heqiang Du and Feiyao Liu and Chengyang Li and Chimin Lai and Xian Xue},
keywords = {NDVI, Vegetation variation, Climate change, Anthropogenic disturbance, Qinghai-Tibetan Plateau},
abstract = {Climate change and anthropogenic disturbance are two main drivers for vegetation dynamics on the Qinghai-Tibetan Plateau (QTP). Alpine meadow and alpine steppe are the primary rangeland ecosystem types on the QTP. However, the vegetation trends of the two land cover types and the underlying mechanisms behind their variation remain under debate. In this study, we used Global Inventory Modeling and Mapping Studies (GIMMS) 3g Normalized Difference Vegetation Index (NDVI) (i.e., GIMMS NDVI3g) by coupling the Breaks for Additive Season and Trend (BFAST) model and the Boosted Regression Tree (BRT) model to analyze alpine meadow and alpine steppe vegetation trends on the QTP between 1982 and 2015. We also assessed vegetation variation response to climatic and anthropogenic indicators in conjunction with climatic and human footprint datasets. Results show that growing season NDVI (GSNDVI) values increased overall for both alpine meadow (0.0001 year−1, p = 0.33) and alpine steppe (0.0002 year−1, p < 0.05) throughout 1982–2015. Significant greening trends in both alpine meadow (0.0007 year−1; p < 0.05) and alpine steppe (0.0005 year−1; p < 0.05) ecosystems were obtained before 1998 and 2001, respectively. However, browning trends ascertained by GSNDVI (−0.0006 year−1; p = 0.12) in alpine meadows were observed throughout 1998–2015, while greening trends ascertained by GSNDVI (0.0002 year−1; p = 0.12) in alpine steppes were observed throughout during 2001–2015. Opposing trends in precipitation, solar radiation, and the Standardized Precipitation Evapotranspiration Index (SPEI) occurred before and after breakpoints in both ecosystems. For the alpine meadow ecosystem, adverse precipitation trends caused browning before 1998 followed by greening after 1998 in the Three-River-Source National Park (TNP). Conversely, opposing changes in precipitation, solar radiation, and SPEI resulted in greening before 1998 followed by browning after 1998 in southern Tibet and the southeastern QTP. Alpine meadow vegetation trends were generally dominated by solar radiation before 1998 and jointly by precipitation and solar radiation after 1998. Prior to 2001 variation in alpine steppe greenness was controlled by precipitation, while after 2001 solar radiation dominated. Along with an increase in human footprint pressure (HFP) gradients, greenness trends gradually increased before 1998 but reversed after 1998 in the alpine meadow ecosystem. Additionally, greenness trends gradually decreased before 2001 but remained unchanged after 2001 for the alpine steppe ecosystem. These results highlight the different effects that climate change and anthropogenic disturbances have had on alpine meadow and alpine steppe ecosystems on the QTP over different time frames.}
}
@article{ALBAHRI2021102873,
title = {IoT-based telemedicine for disease prevention and health promotion: State-of-the-Art},
journal = {Journal of Network and Computer Applications},
volume = {173},
pages = {102873},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102873},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520303374},
author = {A.S. Albahri and Jwan K. Alwan and Zahraa K. Taha and Sura F. Ismail and Rula A. Hamid and A.A. Zaidan and O.S. Albahri and B.B. Zaidan and A.H. Alamoodi and M.A. Alsalem},
keywords = {Telemedicine, Remote monitoring, Healthcare services, Diseases, Internet of things, Network},
abstract = {Numerous studies have focused on making telemedicine smart through the Internet of Things (IoT) technology. These works span a wide range of research areas to enhance telemedicine architecture such as network communications, artificial intelligence methods and techniques, IoT wearable sensors and hardware devices, smartphones and cloud computing. Accordingly, several telemedicine applications covering various human diseases have presented their works from a specific perspective and resulted in confusion regarding the IoT characteristics. Although such applications are useful and necessary for improving telemedicine contexts related to monitoring, detection and diagnostics, deriving an overall picture of how IoT characteristics are currently integrated with the telemedicine architecture is difficult. Accordingly, this study complements the academic literature with a systematic review covering all main aspects of advances in IoT-based telemedicine architecture. This study also provides a state-of-the-art telemedicine classification taxonomy under IoT and reviews works in different fields in relation to that classification. To this end, this study checked the ScienceDirect, Institute of Electrical and Electronics Engineers (IEEE) Xplore, and Web of Science databases. A total of 2121 papers were collected from 2014 to July 2020. The retrieved articles were filtered according to the defined inclusion criteria. A final set of 141 articles were selected and classified into two categories, each followed by subcategories and sections. The first category includes an IoT-based telemedicine network that accounts for 24.11% (n = 34/141). The second category includes IoT-based telemedicine healthcare services and applications that account for 75.89% (n = 107/141). This multi-field systematic review has exposed new research opportunities, motivations, recommendations and challenges that need attention for the synergistic integration of interdisciplinary works. This extensive study also lists a set of open issues and provides innovative key solutions along with a systematic review. The classification of diseases under IoT-based telemedicine is divided into 14 groups. Furthermore, the crossover in our taxonomy is demonstrated. The lifecycle of the context of IoT-based telemedicine healthcare applications is mapped for the first time, including the procedure sequencing and definition for each context. We believe that this study is a useful guide for researchers and practitioners in providing direction and valuable information for future research. This study can also address the ambiguity in the trends in IoT-based telemedicine.}
}
@article{JIANG2021106431,
title = {A comprehensive study of macro factors related to traffic fatality rates by XGBoost-based model and GIS techniques},
journal = {Accident Analysis & Prevention},
volume = {163},
pages = {106431},
year = {2021},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106431},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521004620},
author = {Feifeng Jiang and Jun Ma},
keywords = {Traffic fatality rates, Macro factors, National scale, XGBoost, GIS, Feature importance},
abstract = {With the fast development of economics, road safety is becoming a serious problem. Exploring macro factors is effective to improve road safety. However, the existing studies have some limitations: (1) The existing studies only considered one aspect of macro factors and constructed models based on a few data samples. (2) The methods commonly used cannot address the non-linear relationship or calculate the feature importance. The findings obtained from such models may be limited and biased. To address the limitations, this study proposes a BO-CV-XGBoost framework to explore the macro factors related to traffic fatality rate classes based on a high-dimensional dataset that fully considers the impact of multi-factor interaction with adequate data samples. The proposed framework is applied to a dataset in the US. 453 county-level macro factors are collected from various data sources, covering ten macro aspects, including topography, transportation, etc. The optimized BO-CV-XGBoost model obtains the best classification performance with an AUC of 0.8977 and an accuracy of 85.02%. Compared with other methods, the proposed model has superiority on fatality rate classification. Ten macro factors are identified, including ‘Current-dollar GDP’, ‘highway miles per person’, etc. The ten factors contain four aspects of information, including economics, transportation, education, and medical condition. Geographic information system (GIS) techniques are further used for spatial analysis of the identified macro factors. Therefore, targeted and effective measures are accordingly proposed to prevent traffic fatalities and improve road safety}
}
@article{LO2021101297,
title = {A review of digital twin in product design and development},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101297},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101297},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000513},
author = {C.K. Lo and C.H. Chen and Ray Y. Zhong},
keywords = {Digital twin, Product design, New product development, Product lifecycle, Review},
abstract = {In the era of digitalization, there are many emerging technologies, such as the Internet of Things (IoT), Digital Twin (DT), Cloud Computing and Artificial Intelligence (AI), which are quickly developped and used in product design and development. Among those technologies, DT is one promising technology which has been widely used in different industries, especially manufacturing, to monitor the performance, optimize the progresses, simulate the results and predict the potential errors. DT also plays various roles within the whole product lifecycle from design, manufacturing, delivery, use and end-of-life. With the growing demands of individualized products and implementation of Industry 4.0, DT can provide an effective solution for future product design, development and innovation. This paper aims to figure out the current states of DT research focusing on product design and development through summarizing typical industrial cases. Challenges and potential applications of DT in product design and development are also discussed to inspire future studies.}
}
@article{REICHARDT2021968,
title = {Procedure model for the development and launch of intelligent assistance systems},
journal = {Procedia Computer Science},
volume = {180},
pages = {968-977},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.348},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921004026},
author = {Paul Reichardt and Sebastian Lang and Tobias Reggelin},
keywords = {Production planning, control, Machine learning, Planning assistance system, Practical implementation, Generic procedure model, Digital twin},
abstract = {The paper analyses the current state of knowledge on approaches for the practical implementation of machine learning based assistance systems for production planning and control. A concept of a procedure model for application-oriented projects in the field of industrial series production is proposed. It focusses on order sequencing and machine allocation in a real time production environment. As part of an application-oriented research project, a use case is referenced. In this paper, a first conceptual approach is presented, using the example of an industrial production of printed circuit boards. In the following steps, practical suitability is checked on the basis of the practical reference, conclusions are drawn and the methodology will be developed further. The aim is a generally valid procedure model for industrial series production.}
}
@article{FU2021688,
title = {Automated Detection of Periprosthetic Joint Infections and Data Elements Using Natural Language Processing},
journal = {The Journal of Arthroplasty},
volume = {36},
number = {2},
pages = {688-692},
year = {2021},
issn = {0883-5403},
doi = {https://doi.org/10.1016/j.arth.2020.07.076},
url = {https://www.sciencedirect.com/science/article/pii/S088354032030869X},
author = {Sunyang Fu and Cody C. Wyles and Douglas R. Osmon and Martha L. Carvour and Elham Sagheb and Taghi Ramazanian and Walter K. Kremers and David G. Lewallen and Daniel J. Berry and Sunghwan Sohn and Hilal Maradit Kremers},
keywords = {total joint arthroplasty, periprosthetic joint infection, natural language processing, electronic health records, artificial intelligence},
abstract = {Background
Periprosthetic joint infection (PJI) data elements are contained in both structured and unstructured documents in electronic health records and require manual data collection. The goal of this study is to develop a natural language processing (NLP) algorithm to replicate manual chart review for PJI data elements.
Methods
PJI was identified among all total joint arthroplasty (TJA) procedures performed at a single academic institution between 2000 and 2017. Data elements that comprise the Musculoskeletal Infection Society (MSIS) criteria were manually extracted and used as the gold standard for validation. A training sample of 1208 TJA surgeries (170 PJI cases) was randomly selected to develop the prototype NLP algorithms and an additional 1179 surgeries (150 PJI cases) were randomly selected as the test sample. The algorithms were applied to all consultation notes, operative notes, pathology reports, and microbiology reports to predict the correct status of PJI based on MSIS criteria.
Results
The algorithm, which identified patients with PJI based on MSIS criteria, achieved an f1-score (harmonic mean of precision and recall) of 0.911. Algorithm performance in extracting the presence of sinus tract, purulence, pathologic documentation of inflammation, and growth of cultured organisms from the involved TJA achieved f1-scores that ranged from 0.771 to 0.982, sensitivity that ranged from 0.730 to 1.000, and specificity that ranged from 0.947 to 1.000.
Conclusion
NLP-enabled algorithms have the potential to automate data collection for PJI diagnostic elements, which could directly improve patient care and augment cohort surveillance and research efforts. Further validation is needed in other hospital settings.
Level of Evidence
Level III, Diagnostic.}
}
@article{EBITU2021105326,
title = {Citizen science for sustainable agriculture – A systematic literature review},
journal = {Land Use Policy},
volume = {103},
pages = {105326},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2021.105326},
url = {https://www.sciencedirect.com/science/article/pii/S0264837721000491},
author = {Larmbert Ebitu and Helen Avery and Khaldoon A. Mourad and Joshua Enyetu},
keywords = {Sustainable agriculture, Citizen science, Farmers, Methodologies},
abstract = {Farmers as volunteers in research could potentially provide a rich resource for exploring sustainable agricultural research questions. To discern emerging patterns in citizen science-based studies on topics with relevance for sustainable agriculture and reveal salient challenges and opportunities for conducting such studies, we conducted a literature review of 27 articles from the period 2004–2019 of 250 publications screened from Google Scholar. These articles were thematically grouped under the topics: Soil health, climate adaptation, pest/pathogen monitoring, invasive species, inputs and outputs and pollination. Participants’ characteristics, motivations, study design and project outcomes in the reviewed articles were summarized and discussed. Both observational and experimental studies were represented in the articles, while emerging trends point towards field experimentation and ‘Large-N′ trials by lay farmers. Crowdsourcing lends itself to projects where the main role of the public is local visual observations and reporting, such as in pest/pathogen monitoring. Challenges included methodological issues such as validation procedures, but above all motivation, recruitment, and retention of volunteers. Despite the importance of participatory approaches for deeper citizen involvement for sustainability transitions and for the quality of knowledge outcomes, the role of citizens was overall restricted to data collection. Several of the methodologies proposed would be difficult to implement in low-income countries, and relatively few studies pertained to agricultural concerns of the global South. To lend value to farmers' time, we recommend projects relevant to livelihoods, health issues or local farming problems, accompanied by well-structured data feedback protocols, routing study results back to farmers.}
}
@article{WILHELM2021278,
title = {Overview on hybrid approaches to fault detection and diagnosis: Combining data-driven, physics-based and knowledge-based models},
journal = {Procedia CIRP},
volume = {99},
pages = {278-283},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.041},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003152},
author = {Yannick Wilhelm and Peter Reimann and Wolfgang Gauchel and Bernhard Mitschang},
keywords = {Fault detection, Fault diagnosis, Hybrid methods, Diagnostics, maintenance, Knowledge-driven methods, Machine learning},
abstract = {In this paper, we review hybrid approaches for fault detection and fault diagnosis (FDD) that combine data-driven analysis with physics-based and knowledge-based models to overcome a lack of data and to increase the FDD accuracy. We categorize these hybrid approaches according to the steps of an extended common workflow for FDD. This gives practitioners indications of which kind of hybrid FDD approach they can use in their application.}
}
@article{CISNEROSCABRERA2021103391,
title = {An approach and decision support tool for forming Industry 4.0 supply chain collaborations},
journal = {Computers in Industry},
volume = {125},
pages = {103391},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103391},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306254},
author = {Sonia Cisneros-Cabrera and Grigory Pishchulov and Pedro Sampaio and Nikolay Mehandjiev and Zixu Liu and Sophia Kununka},
keywords = {Digitalization, Supply chain collaboration, Industry 4.0, Decision support systems, Interoperability, Ontology},
abstract = {Industry 4.0 technologies, process digitalisation and automation can be applied to support the formation of supply chain collaborations in manufacturing. Underpinned by information and communication technologies, collaborations of independent companies can dynamically pool production capacities and capabilities to jointly react to new business opportunities. These collaborations may involve a wide range of enterprises with different sizes and scope that individually would not be able to tender for such new business opportunities. To form these collaborative teams, assistive processes and technologies can underpin the effort towards exploring the tender requirements, unbundling the tender into smaller tasks and finding a suitable supplier for each task. In this paper, we present an approach and a tool to support decision making concerning forming supply chain collaborations in Industry 4.0. The approach proposed is unique in integrating industry domain ontologies, assistive human-computer interaction tools and multi-criteria decision support techniques to form team compositions speeding-up the collaboration process whilst maximising the chances of forming a viable team to fulfil the tender requirements. We also show evaluation results involving stakeholders from the supply chain function pointing to the effectiveness of the proposed solution, available online as a demo11http://130.88.97.225:4200 (username: TDMS@uniman.eu; password: uniman)..}
}
@article{XIANG2021114989,
title = {High-end equipment data desensitization method based on improved Stackelberg GAN},
journal = {Expert Systems with Applications},
volume = {180},
pages = {114989},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114989},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421004309},
author = {Nan Xiang and Xiongtao Zhang and Yajie Dou and Xiangqian Xu and Kewei Yang and Yuejin Tan},
keywords = {High-end equipment, Data desensitization, Generative adversarial networks},
abstract = {High-end equipment refers to a type of technical equipment with high technical content, large capital investment, and long development cycle. Therefore, high-end equipment data has extraordinary significance and its desensitization is an urgent problem in data analysis. Traditional data desensitization principles are processing original data such as substitution and adding noise. These methods may not only damage data correlation information, but also result in data disclosure and high computing cost. Given the aforementioned reasons, the study proposes a high-end equipment data desensitization method based on improved Stackelberg Generative Adversarial Networks (GAN). When compared with the normal GAN, the structure proposed in the study includes more generators and discriminators. By inputting the original data, the trained GAN can output indistinguishable data from the original data which helps data mining and also ensures the privacy of data. We experimented on two datasets: optimal improvement was determined by Gaussian dataset experiments, i.e. Stackelberg GAN with eight discriminators. The second experiment results on real-world datasets proved that the 8-discriminator Stackelberg GAN better fits the original data and significantly aids data desensitization.}
}
@article{ZENG2021111661,
title = {Biological characteristics of energy conversion in carbon fixation by microalgae},
journal = {Renewable and Sustainable Energy Reviews},
volume = {152},
pages = {111661},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111661},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121009369},
author = {Jing Zeng and Zhenjun Wang and Guobin Chen},
keywords = {Microalgae, Carbon fixation, Photosynthetic reaction, Carbon pump, Energy conversion},
abstract = {CO2-fixation by microalgae can be regarded as a biological process of energy conversion with CO2 and H2O in microalgae cells in the sunlight. The study of the biological intrinsic characteristics of energy conversion is helpful to reveal the high-efficiency carbon fixation mechanism of microalgae. Firstly, the CO2 emission control technology and the external influencing factors are summarized in this paper, which have laid the foundation for researching the internal biological intrinsic characteristics of carbon fixation by microalgae. Based on photosynthetic reactions, in-situ reaction experiments, hydrodynamic simulations and metabolic networks have been integrated to analyze the biological intrinsic characteristics of carbon fixation by microalgae. The collation of representative studies on theory and quantitative calculation methods reveals that free energy dissipation seriously affects the carbon fixation efficiency of microalgae; Secondly, thermodynamics and metabolic networks are discussed. The role of thermodynamics in addressing the constraints is explored mainly from the perspective of energy conversion mechanisms, free energy dissipation mechanisms, framework and methods. Metabolic networks are studied using sampling methods based on thermodynamic systems and metabolic engineering based on a systems perspective; Thirdly, the key supporting technologies and biological intrinsic characteristics are reviewed from an interdisciplinary perspective, and the researches on metabolic networks based on thermodynamic constraints are given; Finally, challenges are summarized to provide a basis and direction for future research.}
}
@article{ZHANG2021109380,
title = {Collision-avoidance navigation systems for Maritime Autonomous Surface Ships: A state of the art survey},
journal = {Ocean Engineering},
volume = {235},
pages = {109380},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109380},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821007940},
author = {Xinyu Zhang and Chengbo Wang and Lingling Jiang and Lanxuan An and Rui Yang},
keywords = {Collision avoidance, Autonomous navigation systems, Cognitive navigation, e-navigation, Maritime autonomous surface ships},
abstract = {The rapid development of artificial intelligence significantly promotes collision-avoidance navigation of maritime autonomous surface ships (MASS), which in turn provides prominent services in maritime environments and enlarges the opportunity for coordinated and interconnected operations. Clearly, full autonomy of the collision-avoidance navigation for the MASS in complex environments still faces huge challenges and highly requires persistent innovations. First, we survey relevant guidance of the International Maritime Organization (IMO) and industry code of each country on MASS. Then, major advances in MASS industry R&D, and collision-avoidance navigation technologies, are thoroughly overviewed, from academic to industrial sides. Moreover, compositions of collision-avoidance navigation, brain-inspired cognitive navigation, and e-navigation technologies are analyzed to clarify the mechanism and principles efficiently systematically in typical maritime environments, whereby trends in maritime collision-avoidance navigation systems are highlighted. Finally, considering a general study of existing collision avoidance and action planning technologies, it is pointed out that collision-free navigation would significantly benefit the integration of MASS autonomy in various maritime scenarios.}
}
@article{ZHANG2021109535,
title = {Data mining approach for automatic ship-route design for coastal seas using AIS trajectory clustering analysis},
journal = {Ocean Engineering},
volume = {236},
pages = {109535},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109535},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821009288},
author = {Daheng Zhang and Yingjun Zhang and Chuang Zhang},
keywords = {RU, FA-DBSCAN, Ship-route design, Data mining, AIS data},
abstract = {In this paper, we propose an automatic route design method based on simple recurrent unit (SRU) and automatic identification system (AIS) data. Laplacian eigen maps and Gaussian kernel functions are used to compress the AIS data and extract the turning points of all ships. Fuzzy adaptive density-based spatial clustering of applications with noise (FA-DBSCAN) technique is used to cluster the turning points obtained at the preprocessing stage to obtain the turning region. Optimal turn region matching is used to connect the turning regions of similar routes, and the SRU neural network algorithm is used to learn the relationship between different types, sizes, and drafts of ships in each turning region; extract the feature-turning points; and obtain the recommended coastal routes, speed, and course of each type of ship. In the experimental stage, a large variety of AIS data from two sea areas are used to compare and analyze the designed route and real-ship data through LSTM and SRU experiments. The results show that the SRU algorithm improves the training speed and accuracy in comparison to LSTM, while the generated automatic route meets the requirements of navigation practice.}
}
@article{SUBRAMANIYAN2021734,
title = {Artificial intelligence for throughput bottleneck analysis – State-of-the-art and future directions},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {734-751},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001588},
author = {Mukund Subramaniyan and Anders Skoogh and Jon Bokrantz and Muhammad Azam Sheikh and Matthias Thürer and Qing Chang},
keywords = {Throughput bottlenecks, Artificial intelligence, Production system, Data-driven, Manufacturing},
abstract = {Identifying, and eventually eliminating throughput bottlenecks, is a key means to increase throughput and productivity in production systems. In the real world, however, eliminating throughput bottlenecks is a challenge. This is due to the landscape of complex factory dynamics, with several hundred machines operating at any given time. Academic researchers have tried to develop tools to help identify and eliminate throughput bottlenecks. Historically, research efforts have focused on developing analytical and discrete event simulation modelling approaches to identify throughput bottlenecks in production systems. However, with the rise of industrial digitalisation and artificial intelligence (AI), academic researchers explored different ways in which AI might be used to eliminate throughput bottlenecks, based on the vast amounts of digital shop floor data. By conducting a systematic literature review, this paper aims to present state-of-the-art research efforts into the use of AI for throughput bottleneck analysis. To make the work of the academic AI solutions more accessible to practitioners, the research efforts are classified into four categories: (1) identify, (2) diagnose, (3) predict and (4) prescribe. This was inspired by real-world throughput bottleneck management practice. The categories, identify and diagnose focus on analysing historical throughput bottlenecks, whereas predict and prescribe focus on analysing future throughput bottlenecks. This paper also provides future research topics and practical recommendations which may help to further push the boundaries of the theoretical and practical use of AI in throughput bottleneck analysis.}
}
@article{SHARMA2021102316,
title = {Fifty years of information management research: A conceptual structure analysis using structural topic modeling},
journal = {International Journal of Information Management},
volume = {58},
pages = {102316},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102316},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000098},
author = {Anuj Sharma and Nripendra P. Rana and Robin Nunkoo},
keywords = {Information management, Structural topic models, Topic modeling, Generative models, Text analytics},
abstract = {Information management is the management of organizational processes, technologies, and people which collectively create, acquire, integrate, organize, process, store, disseminate, access, and dispose of the information. Information management is a vast, multi-disciplinary domain that syndicates various subdomains and perfectly intermingles with other domains. This study aims to provide a comprehensive overview of the information management domain from 1970 to 2019. Drawing upon the methodology from statistical text analysis research, this study summarizes the evolution of knowledge in this domain by examining the publication trends as per authors, institutions, countries, etc. Further, this study proposes a probabilistic generative model based on structural topic modeling to understand and extract the latent themes from the research articles related to information management. Furthermore, this study graphically visualizes the variations in the topic prevalences over the period of 1970 to 2019. The results highlight that the most common themes are data management, knowledge management, environmental management, project management, service management, and mobile and web management. The findings also identify themes such as knowledge management, environmental management, project management, and social communication as academic hotspots for future research.}
}
@article{KOLAR2021105392,
title = {On interdisciplinarity in the humanities: A comment on Fanta et al. (2020) on the bias in dating obtained from historical sources},
journal = {Journal of Archaeological Science},
volume = {132},
pages = {105392},
year = {2021},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2021.105392},
url = {https://www.sciencedirect.com/science/article/pii/S0305440321000625},
author = {Jan Kolář and Péter Szabó},
keywords = {Settlement history, Written records, Middle ages, Time lag, Archaeological method, Bohemia, Central Europe},
abstract = {Medieval settlement history in Europe is a common topic in several scientific disciplines. Recently, Fanta et al. (2020) examined colonization processes in Bohemia through the comparison of archaeological evidence and historical records. They concluded that the first mentions of settlements in historical documents are not reliable sources for settlement dating and should always be verified and preferably superseded by archaeological data, which are, in contrast, mostly unproblematic. We argue that this conclusion is controversial from several aspects. Firstly, it neglects the disciplinary constraints of archaeological evidence for medieval settlement development, as regards quality and chronology. Secondly, there are several legitimate perspectives from which to analyse the data. Our reanalysis of the original dataset showed that – in partial contrast to the conclusions of Fanta et al. (2020) – when viewed from the point of view of historical evidence, the time lag between the historical and archaeological dating increased with time and that the historical dating of most of the settlements between the 10th and 13th centuries was supported by archaeological evidence. Lastly, we demonstrated how research combining different disciplines (archaeology, history, palaeoecology, geography) and types evidence can reveal the manifold processes of human settlement dynamics. In our view each type of evidence has advantages as well as drawbacks, therefore strictly prioritising one at the expense of others hardly furthers the understanding of complex social phenomena.}
}
@article{LI2021129113,
title = {Spatializing environmental footprint by integrating geographic information system into life cycle assessment: A review and practice recommendations},
journal = {Journal of Cleaner Production},
volume = {323},
pages = {129113},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129113},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621033023},
author = {Junjie Li and Yajun Tian and Yueling Zhang and Kechang Xie},
keywords = {Life cycle assessment, Geographic information system, Environmental footprint, GIS-LCA},
abstract = {Life cycle assessment (LCA) is a methodological tool that estimates the environmental footprint from a cradle-to-grave perspective. With the increased need for the geographically explicit assessment, the geographic information system (GIS) is integrating into LCA as a frontier methodology to spatialize the environmental footprint. This paper reviews a total of 105 publications about GIS-LCA, including 50 methodological studies that are analyzed following the four phases of LCA and 55 applied studies that are classified into different domains. The review shows that although GIS-LCA methodology has certain explorations and practices and a large number of cases are carried out in the energy industry, agricultural sector, urban facility, and waste management, the current knowledge system faces several challenges in spatializing environmental footprint. In this case, a universal methodology framework of GIS-LCA and specific schemes are proposed to address the following issues: (1) how to set up a geographically referenced system in the goal and scope definition phase; (2) how to spatialize life-cycle data and integrate and compute foreground and background data in the inventory analysis phase; (3) how to develop spatialized characterization factors with different requirements on resolution and data availability in the impact assessment phase; and (4) how to uniform the contribution analysis of different zones, unit processes, and elementary flows to visualize spatialized environmental footprint in the interpretation phase. The framework we developed provides preliminary practices and recommendations for spatializing environmental footprint, which lays a foundation to support future work.}
}
@article{DOORNENBAL2021101515,
title = {Opening the black box: Uncovering the leader trait paradigm through machine learning},
journal = {The Leadership Quarterly},
pages = {101515},
year = {2021},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2021.101515},
url = {https://www.sciencedirect.com/science/article/pii/S1048984321000205},
author = {Brian M. Doornenbal and Brian R. Spisak and Paul A. {van der Laken}},
keywords = {Leader trait paradigm, Machine learning, Complexity, Interpretability, Personality},
abstract = {Understanding the traits that define a leader is a perennial quest. An ongoing debate surrounds the complexity required to unravel the leader trait paradigm. With the advancement of machine learning, scholars are now better equipped to model leadership as an outcome of complex patterns in traits. However, interpreting those models is often harder. In this paper, we guide researchers in the application of machine learning techniques to uncover complex relationships. Specifically, we demonstrate how applying machine learning can help to assess the complexity of a relationship and show techniques that help interpret the outcomes of “black box” machine learning algorithms. While demonstrating techniques to uncover complex relationships, we are using the Big Five Inventory and need for cognition to predict leadership role occupancy. Among our sample (n = 3385), we find that the leader trait paradigm can benefit from modeling complexity beyond linear effects and generate several interpretable results.}
}
@article{MIAO2021108327,
title = {Federated deep reinforcement learning based secure data sharing for Internet of Things},
journal = {Computer Networks},
volume = {197},
pages = {108327},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108327},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003285},
author = {Qinyang Miao and Hui Lin and Xiaoding Wang and Mohammad Mehedi Hassan},
keywords = {Secure data sharing, Federated learning, Deep reinforcement learning, IoT},
abstract = {The increasing number of Internet of Things (IoT) devices motivate the data sharing that improves the quality of IoT services. However, data providers usually suffer from the privacy leakage caused by direct data sharing. To solve this problem, in this paper, we propose a Federated Learning based Secure data Sharing mechanism for IoT, named FL2S. Specifically, to accomplish efficient and secure data sharing, a hierarchical asynchronous federated learning (FL) framework is developed based on the sensitive task decomposition. In addition, to improve data sharing quality, the deep reinforcement learning (DRL) technology is utilized to select participants of sufficient computational capabilities and high quality datasets. By integrating task decomposition and participant selection, reliable data sharing is realized by sharing local data models instead of the source data with data privacy preserved. Experiment results show that the proposed FL2S achieves high accuracy in secure data sharing for various IoT applications.}
}
@incollection{BRAHEM2021269,
title = {12 - Data perspective on environmental mobile crowd sensing},
editor = {Siddhartha Bhattacharyya and Naba Kumar Mondal and Jan Platos and Václav Snášel and Pavel Krömer},
booktitle = {Intelligent Environmental Data Monitoring for Pollution Management},
publisher = {Academic Press},
pages = {269-288},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-819671-7},
doi = {https://doi.org/10.1016/B978-0-12-819671-7.00012-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196717000129},
author = {Mariem Brahem and Hafsa E.L. Hafyani and Souheir Mehanna and Karine Zeitouni and Laurent Yeh and Yehia Taher and Zoubida Kedad and Ahmad Ktaish and Mohamed Chachoua and Cyril Ray},
keywords = {Mobile crowd sensing, Environmental sensing, Data management, Data mining, Exposure analysis, Big data framework, Scalability},
abstract = {The advent of the new generation of low-cost lightweight and connected sensors made a paradigm shift in environmental studies. In particular, nomadic sensors allow for a very precise personalized measurement, by continuously quantifying the individual exposure to air pollution components. Moreover, a broad dissemination among volunteers of these devices, or their deployment on vehicle fleets, is becoming a credible solution. Another major interest of such sensor deployment is to densify the air quality monitoring network, indoor, as in the outdoor, which is today restricted to sparse nodes. However, this high spatiotemporal resolution raises several issues related to their analysis. After an overview of the projects relying on this technology, this chapter points out the remaining challenges to be addressed. Part of these challenges constitutes the research program of the ongoing project Polluscope in France.}
}
@article{WANG20211,
title = {TCM network pharmacology: A new trend towards combining computational, experimental and clinical approaches},
journal = {Chinese Journal of Natural Medicines},
volume = {19},
number = {1},
pages = {1-11},
year = {2021},
issn = {1875-5364},
doi = {https://doi.org/10.1016/S1875-5364(21)60001-8},
url = {https://www.sciencedirect.com/science/article/pii/S1875536421600018},
author = {Xin WANG and Zi-Yi WANG and Jia-Hui ZHENG and Shao LI},
keywords = {Network pharmacology, Traditional Chinese medicine, Network target, Computation, Experiment, Clinical approach},
abstract = {Traditional Chinese medicine (TCM) is a precious treasure of the Chinese nation and has unique advantages in the prevention and treatment of diseases. The holistic view of TCM coincides with the new generation of medical research paradigm characterized by network and system. TCM gave birth to a new method featuring holistic and systematic “network target”, a core theory and method of network pharmacology. TCM is also an important research object of network pharmacology. TCM network pharmacology, which aims to understand the network-based biological basis of complex diseases, TCM syndromes and herb treatments, plays a critical role in the origin and development process of network pharmacology. This review introduces new progresses of TCM network pharmacology in recent years, including predicting herb targets, understanding biological foundation of diseases and syndromes, network regulation mechanisms of herbal formulae, and identifying disease and syndrome biomarkers based on biological network. These studies show a trend of combining computational, experimental and clinical approaches, which is a promising direction of TCM network pharmacology research in the future. Considering that TCM network pharmacology is still a young research field, it is necessary to further standardize the research process and evaluation indicators to promote its healthy development.}
}
@incollection{VANI202199,
title = {Chapter 6 - Impetus to machine learning in cardiac disease diagnosis},
editor = {Kalpana Chauhan and Rajeev Kumar Chauhan},
booktitle = {Image Processing for Automated Diagnosis of Cardiac Diseases},
publisher = {Academic Press},
pages = {99-116},
year = {2021},
isbn = {978-0-323-85064-3},
doi = {https://doi.org/10.1016/B978-0-323-85064-3.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323850643000091},
author = {T. Vani},
keywords = {Machine learning, disease diagnosis, healthcare, cardiac disease, disease detection, medical imaging, automated diagnosis, cardiac diagnosis},
abstract = {Machine learning is a branch of computer science, and it is a subset of artificial intelligence. It comprises many algorithms based on statistical methods to build automated systems for solving a particular problem. Due to its versatility, it is popular in many fields in real life, including scientific researches, healthcare field, industries, pharmaceutical field for drug discovery, social anomalies such as epidemic, and pandemic diseases spread. This chapter aims to identify the impact of machine learning techniques in the diagnosis of cardiac diseases. This chapter starts with the justification of the need for machine learning technology in the healthcare field. The basics of machine learning technology and its various algorithms are explained in the next section. The applications of these algorithms, which includes the diagnosis of various diseases such as diabetics, coronary artery disease (CAD), coronary heart disease (CHD), liver ailments, cancer detection and prevention, radiology, pathology, clinical trials, robotic surgery, drug discovery, and personalized treatments, are described from the contemporary researches. The challenges it faces in the healthcare field are also listed. In the end, the constraints of machine learning techniques in the healthcare field are explained with the suggestions to make accurate and efficient diagnoses in the future.}
}
@article{SINGH2021101322,
title = {Making Energy-transition headway: A Data driven assessment of German energy startups},
journal = {Sustainable Energy Technologies and Assessments},
volume = {47},
pages = {101322},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101322},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821003325},
author = {Mahendra Singh and Jiao Jiao and Marian Klobasa and Rainer Frietsch},
keywords = {Energy startups, Emerging technologies, Energy-transition, Funding, Innovation, Digitalisation, Data analysis, Business model, X-as-a-service, Digital platform},
abstract = {This paper explores the linkage between ongoing clean energy-transition, technology and business model emergence in the German energy sector. The speed of energy-transition is often led by innovative startups. Startups with innovative products, services, or value propositions are a key indicator, supporting successful energy-transition. Though, commercial databases cover comprehensive details to understand startup’s financial activity and stakeholder relation, but without considering their innovation and business activity. Measuring the actual activities of energy startups is pivotal to capture the impact of energy-transition. To put this into perspective, a hybrid approach of data collection combining structured and unstructured data has been proposed in the following work. A list of 240 innovative startups belonging to different categories and technology focus are examined. Furthermore, data-driven analysis is performed over the data collected from multiple sources. Renewable technologies are yet the most preferred technology focus among German entrepreneurs and stakeholders. 24.6% startups are identified in this category followed by 17.5% in energy management and 16.2% in energy storage. The evidence from this study suggests a clear shift in technology and the value proposition of successful innovative startups in Germany. Digitalisation of the energy sector is fostering the development of multi-sided digital platform driven business models. The result suggested that 8.0% of startups have implemented purely platform based services while 15.7% are experimenting with platform business models along with traditional business to business (B2B) and business to customer (B2C) business models. Findings could guide policymakers and federal agencies to provide a vision for future technology and business model adaptation in the German energy sector.}
}
@article{VANDERVOORT2021121160,
title = {Data science as knowledge creation a framework for synergies between data analysts and domain professionals},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121160},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121160},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100593X},
author = {Haiko {van der Voort} and Sabine {van Bulderen} and Scott Cunningham and Marijn Janssen},
keywords = {Data science, Knowledge, Predictive model, Value creation, Risk-based inspection, Professionalism},
abstract = {The road from data generation to data use is commonly approached as a data-driven, functional process in which domain expertise is integrated as an afterthought. In this contribution we complement this functional view with an institutional view, that takes data analysis and domain professionalism as complementary (yet fallible) knowledge sources. We developed a framework that identifies and amplifies synergies between data analysts and domain professionals instead of taking one of them (i.e. data analytics) at the centre of the analytical process. The framework combines the often-cited CRISP-DM framework with a knowledge creation framework. The resulting framework is used in a data science project at a Dutch inspectorate that seeks to use data for risk-based inspection. The findings show first support of our framework. They also show that whereas more complex models have a higher predictive power, simpler models are sometimes preferred as they have the potential to create more synergies between inspectors and data analyst. Another issue driven by the integrated framework is about who of the involved actors should own the predictive model: data analysts or inspectors.}
}
@article{MATHIAS2021614,
title = {Exploring Distance Based Approaches for Reducing Sensor Data in Defect Related Prognosis},
journal = {Procedia Computer Science},
volume = {184},
pages = {614-621},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.076},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007092},
author = {Selvine G. Mathias and Daniel Grossmann and Tapanta Bhanja},
keywords = {sensors, data, reduced distance, machine learning, accuracy scores},
abstract = {Vibration data consists of batches of time series which if accumulated over a period of time is a huge collection of numeric data. Reducing such data for use in deep learning models for computational effciency is a challenge. Combinatorial and discrete approaches, on the other hand, is not an extensively explored area when it comes to datasets. This paper aims to identify feature reduction techniques based on discrete approaches such as euclidean distance using dot products on vibration data samples from accelerometers fitted on bearings. In this limited experimentation, the procured dataset by this approach is considerably smaller in size as compared to the actual complete data, and with comparable results in prediction models, it can be used as a smaller representation of a sensor timeline. The results based on different models show that such reductions can be considered in building IoT applications in industries based on sensors.}
}
@article{AUGUSTE2021102053,
title = {Heterogeneity in head and neck cancer incidence among black populations from Africa, the Caribbean and the USA: Analysis of cancer registry data by the AC3},
journal = {Cancer Epidemiology},
volume = {75},
pages = {102053},
year = {2021},
issn = {1877-7821},
doi = {https://doi.org/10.1016/j.canep.2021.102053},
url = {https://www.sciencedirect.com/science/article/pii/S1877782121001703},
author = {Aviane Auguste and Samuel Gathere and Paulo S. Pinheiro and Clement Adebamowo and Adeola Akintola and Kellie Alleyne-Mike and Simon G. Anderson and Kimlin Ashing and Fred Kwame Awittor and Baffour Awuah and Bernard Bhakkan and Jacqueline Deloumeaux and Maira du Plessis and Ima-Obong A. Ekanem and Uwemedimbuk Ekanem and Emmanuel Ezeome and Nkese Felix and Andrew K. Gachii and Stanie Gaete and Tracey Gibson and Robert Hage and Sharon Harrison and Festus Igbinoba and Kufre Iseh and Evans Kiptanui and Ann Korir and Heather-Dawn Lawson-Myers and Adana Llanos and Daniele Luce and Dawn McNaughton and Michael Odutola and Abidemi Omonisi and Theresa Otu and Jessica Peruvien and Nasiru Raheem and Veronica Roach and Natasha Sobers and Nguundja Uamburu and Camille Ragin},
keywords = {Head and neck cancer, Incidence, Blacks, Tobacco smoking, Alcohol drinking, HPV, Caribbean, Africa, USA, Population-based cancer registry},
abstract = {Background
Africa and the Caribbean are projected to have greater increases in Head and neck cancer (HNC) burden in comparison to North America and Europe. The knowledge needed to reinforce prevention in these populations is limited. We compared for the first time, incidence rates of HNC in black populations from African, the Caribbean and USA.
Methods
Annual age-standardized incidence rates (IR) and 95% confidence intervals (95%CI) per 100,000 were calculated for 2013–2015 using population-based cancer registry data for 14,911 HNC cases from the Caribbean (Barbados, Guadeloupe, Trinidad & Tobago, N = 443), Africa (Kenya, Nigeria, N = 772) and the United States (SEER, Florida, N = 13,696). We compared rates by sub-sites and sex among countries using data from registries with high quality and completeness.
Results
In 2013–2015, compared to other countries, HNC incidence was highest among SEER states (IR: 18.2, 95%CI = 17.6–18.8) among men, and highest in Kenya (IR: 7.5, 95%CI = 6.3–8.7) among women. Nasopharyngeal cancer IR was higher in Kenya for men (IR: 3.1, 95%CI = 2.5–3.7) and women (IR: 1.5, 95%CI = 1.0–1.9). Female oral cavity cancer was also notably higher in Kenya (IR = 3.9, 95%CI = 3.0–4.9). Blacks from SEER states had higher incidence of laryngeal cancer (IR: 5.5, 95%CI = 5.2–5.8) compared to other countries and even Florida blacks (IR: 4.4, 95%CI = 3.9–5.0).
Conclusion
We found heterogeneity in IRs for HNC among these diverse black populations; notably, Kenya which had distinctively higher incidence of nasopharyngeal and female oral cavity cancer. Targeted etiological investigations are warranted considering the low consumption of tobacco and alcohol among Kenyan women. Overall, our findings suggest that behavioral and environmental factors are more important determinants of HNC than race.}
}
@incollection{QUIGLEY202179,
title = {Chapter 5 - The design of blended learning experiences for clean data to allow proper observation of student participation},
editor = {Fun Man Fung and Christoph Zimmermann},
booktitle = {Technology-Enabled Blended Learning Experiences for Chemistry Education and Outreach},
publisher = {Elsevier},
pages = {79-94},
year = {2021},
isbn = {978-0-12-822879-1},
doi = {https://doi.org/10.1016/B978-0-12-822879-1.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228791000044},
author = {Cormac Quigley and Elaine Leavy and Etain Kiely and Garrett Jordan},
keywords = {Learning analytics, Clean data, Multidisciplinary team, Motivations, VLE, Moodle},
abstract = {This chapter shares the results and insights from a collaborative project to use learning analytics to capture and transform learning in the first year of undergraduate science programs. The multidisciplinary team is composed of academics and technical staff with a shared goal and numerous motivations. The shared goal was to use analytics to describe and optimize learning. This is an ongoing project first instigated in 2016, which has evolved from using descriptive analytics to create personalized feedback forms, to creating dashboards and is working toward using historical data to train models to monitor and predict engagement and disengagement (identify at-risk students). Data are collected through a blended learning model that has enabled students to take greater ownership of their learning and staff to enhance curriculum and learning strategies.}
}
@incollection{2021xxxiii,
title = {Author biographies},
editor = {David Baker and Lucy Ellis},
booktitle = {Libraries, Digital Information, and COVID},
publisher = {Chandos Publishing},
pages = {xxxiii-xliii},
year = {2021},
series = {Chandos Digital Information Review},
isbn = {978-0-323-88493-8},
doi = {https://doi.org/10.1016/B978-0-323-88493-8.00030-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884938000306}
}
@article{SHI2021116041,
title = {Mapping lead concentrations in urban topsoil using proximal and remote sensing data and hybrid statistical approaches},
journal = {Environmental Pollution},
volume = {272},
pages = {116041},
year = {2021},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2020.116041},
url = {https://www.sciencedirect.com/science/article/pii/S0269749120367300},
author = {Tiezhu Shi and Chao Yang and Huizeng Liu and Chao Wu and Zhihua Wang and He Li and Huifang Zhang and Long Guo and Guofeng Wu and Fenzhen Su},
keywords = {Jenny’s state factor model, Visible and near-infrared reflectance spectroscopy, Landsat image, Geographically weighted regression, Regression kriging},
abstract = {Due to rapid urbanization in China, lead (Pb) continues to accumulate in urban topsoil, resulting in soil degradation and increased public exposure. Mapping Pb concentrations in urban topsoil is therefore vital for the evaluation and control of this exposure risk. This study developed spatial models to map Pb concentrations in urban topsoil using proximal and remote sensing data. Proximal sensing reflectance spectra (350–2500 nm) of soils were pre-processed and used to calculate the principal components as landscape factors to represent the soil properties. Other landscape factors, including vegetation and land-use factors, were extracted from time-sequential Landsat images. Two hybrid statistical approaches, regression kriging (RK) and geographically weighted regression (GWR), were adopted to establish prediction models using the landscape factors. The results indicated that the use of landscape factors derived from combined remote and proximal sensing data improved the prediction of Pb concentrations compared with useing these data individually. GWR obtained better results than RK for predicting soil Pb concentration. Thus, joint proximal and remote sensing provides timely, easily accessible, and suitable data for extracting landscape factors.}
}
@article{CECULA2021e06626,
title = {Applications of artificial intelligence to improve patient flow on mental health inpatient units - Narrative literature review},
journal = {Heliyon},
volume = {7},
number = {4},
pages = {e06626},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e06626},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021007295},
author = {Paulina Cecula and Jiakun Yu and Fatema Mustansir Dawoodbhoy and Jack Delaney and Joseph Tan and Iain Peacock and Benita Cox},
keywords = {Mental health, Patient flow, Artificial intelligence, National health service, Inpatient units},
abstract = {Background
Despite a growing body of research into both Artificial intelligence and mental health inpatient flow issues, few studies adequately combine the two. This review summarises findings in the fields of AI in psychiatry and patient flow from the past 5 years, finds links and identifies gaps for future research.
Methods
The OVID database was used to access Embase and Medline. Top journals such as JAMA, Nature and The Lancet were screened for other relevant studies. Selection bias was limited by strict inclusion and exclusion criteria.
Research
3,675 papers were identified in March 2020, of which a limited number focused on AI for mental health unit patient flow. After initial screening, 323 were selected and 83 were subsequently analysed. The literature review revealed a wide range of applications with three main themes: diagnosis (33%), prognosis (39%) and treatment (28%). The main themes that emerged from AI in patient flow studies were: readmissions (41%), resource allocation (44%) and limitations (91%). The review extrapolates those solutions and suggests how they could potentially improve patient flow on mental health units, along with challenges and limitations they could face.
Conclusion
Research widely addresses potential uses of AI in mental health, with some focused on its applicability in psychiatric inpatients units, however research rarely discusses improvements in patient flow. Studies investigated various uses of AI to improve patient flow across specialities. This review highlights a gap in research and the unique research opportunity it presents.}
}
@article{LIU20211,
title = {Visualization and visual analysis of vessel trajectory data: A survey},
journal = {Visual Informatics},
volume = {5},
number = {4},
pages = {1-10},
year = {2021},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X21000401},
author = {Haiyan Liu and Xiaohui Chen and Yidi Wang and Bing Zhang and Yunpeng Chen and Ying Zhao and Fangfang Zhou},
keywords = {Maritime traffic, Vessel trajectory data, Automatic identification system, Visualization and visual analysis},
abstract = {Maritime transports play a critical role in international trade and commerce. Massive vessels sailing around the world continuously generate vessel trajectory data that contain rich spatial–temporal patterns of vessel navigations. Analyzing and understanding these patterns are valuable for maritime traffic surveillance and management. As essential techniques in complex data analysis and understanding, visualization and visual analysis have been widely used in vessel trajectory data analysis. This paper presents a literature review on the visualization and visual analysis of vessel trajectory data. First, we introduce commonly used vessel trajectory data sets and summarize main operations in vessel trajectory data preprocessing. Then, we provide a taxonomy of visualization and visual analysis of vessel trajectory data based on existing approaches and introduce representative works in details. Finally, we expound on the prospects of the remaining challenges and directions for future research.}
}
@incollection{ROMEO20211,
title = {Chapter 1 - Baseline data for spill assessments: ambient conditions, socioeconomic data, sensitivity maps},
editor = {Oleg Makarynskyy},
booktitle = {Marine Hydrocarbon Spill Assessments},
publisher = {Elsevier},
pages = {1-25},
year = {2021},
isbn = {978-0-12-819354-9},
doi = {https://doi.org/10.1016/B978-0-12-819354-9.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128193549000077},
author = {Lucy Romeo and Patrick Wingo and Michael Sabbatino and Jennifer Bauer},
keywords = {Baseline, data, spill preparedness, spill response, ambient, socioeconomic, sensitivity mapping, machine learning},
abstract = {Effective oil spill preparedness and response relies heavily on the availability of baseline data. Baselines comprise measurements and information collected prior to natural or anthropogenic disasters and can be applied to predict the transport and fate of pollutants, plan for socioeconomic stressors, and overall mitigate impacts. Spatial and temporal in nature, these datasets represent the current state of a specific area. Baselines representing offshore areas comprise ambient conditions, socioeconomic statuses, and environmental sensitivities. This chapter will highlight the value of baselines and identify means to collect and build representative databases for marine and coastal ecosystems to aid in spill assessments.}
}
@article{DEVILLIERS2021598,
title = {A (new) role for business – Promoting the United Nations’ Sustainable Development Goals through the internet-of-things and blockchain technology},
journal = {Journal of Business Research},
volume = {131},
pages = {598-609},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320308262},
author = {Charl {de Villiers} and Sanjaya Kuruppu and Dinithi Dissanayake},
keywords = {Internet-of-things, Blockchain, Sustainable development goals, Innovation},
abstract = {We outline the business opportunity for the provision of measurement technology, linked to the internet, i.e. the internet-of-things (IoT), which feeds information into blockchains, providing reliable and trusted data and an incentive for others to contribute towards progress on the United Nations’ Sustainable Development Goals (SDGs). Both existing businesses and start-ups could exploit these new opportunities, which could inspire the participation of employees, volunteers, donors, and other participants. We provide a conceptual framework for the different ways business can play a role in facilitating measurement of SDGs, and trust in these measurements, by harnessing technology.}
}
@incollection{KARADUZOVICHADZIABDIC2021327,
title = {Chapter 15 - Artificial intelligence in clinical decision-making for diagnosis of cardiovascular disease using epigenetics mechanisms},
editor = {Yvan Devaux and Emma Louise Robinson},
booktitle = {Epigenetics in Cardiovascular Disease},
publisher = {Academic Press},
pages = {327-345},
year = {2021},
volume = {24},
series = {Translational Epigenetics},
isbn = {978-0-12-822258-4},
doi = {https://doi.org/10.1016/B978-0-12-822258-4.00020-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222584000201},
author = {Kanita Karađuzović-Hadžiabdić and Antje Peters},
keywords = {Artificial intelligence, Machine learning, Computational biology, Data mining, Cardiovascular disease, Epigenetics},
abstract = {This chapter provides an overview of machine learning, a mainstream discipline of artificial intelligence. Machine learning is discussed in the context of medical research in general and epigenetics research in cardiology and cardiovascular research in particular. The chapter begins with an overview of machine learning concepts. Main stages of the machine learning workflow including the description of the most popular machine learning techniques used in cardiovascular medicine are presented. In order to reflect the importance of machine learning in biomedical research, selected machine learning applications for disease prediction and diagnosis are reviewed.}
}
@article{WACHTER2021105567,
title = {Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI},
journal = {Computer Law & Security Review},
volume = {41},
pages = {105567},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105567},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000406},
author = {Sandra Wachter and Brent Mittelstadt and Chris Russell},
keywords = {European union, Non-discrimination, Fairness, Discrimination, Bias, Algorithm, Law, Demographic parity, Machine learning, Artificial intelligence},
abstract = {In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in artificial intelligence (AI) and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automated fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as “contextual equality.” This Article makes three contributions. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU's current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a case-by-case basis. We show that automating fairness or non-discrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems. Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) which can act as a signal to victims that discrimination has occurred. Equivalent signalling mechanisms and agency do not exist in algorithmic systems. Compared to traditional forms of discrimination, automated discrimination is more abstract and unintuitive, subtle, intangible, and difficult to detect. The increasing use of algorithms disrupts traditional legal remedies and procedures for detection, investigation, prevention, and correction of discrimination which have predominantly relied upon intuition. Consistent assessment procedures that define a common standard for statistical evidence to detect and assess prima facie automated discrimination are urgently needed to support judges, regulators, system controllers and developers, and claimants. Finally, we examine how existing work on fairness in machine learning lines up with procedures for assessing cases under EU non-discrimination law. A ‘gold standard’ for assessment of prima facie discrimination has been advanced by the European Court of Justice but not yet translated into standard assessment procedures for automated discrimination. We propose ‘conditional demographic disparity’ (CDD) as a standard baseline statistical measurement that aligns with the Court's ‘gold standard’. Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of automated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.}
}
@article{WU2021169,
title = {Hiding sensitive information in eHealth datasets},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {169-180},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330594},
author = {Jimmy Ming-Tai Wu and Gautam Srivastava and Alireza Jolfaei and Philippe Fournier-Viger and Jerry Chun-Wei Lin},
keywords = {Privacy, Preserving, Data mining, eHealth, Dynamic threshold, Sensitive, Evolutionary computation},
abstract = {Privacy in the realm of data mining known as PPDM has become a hot topic in both academic research and industry due to the fact it can discover implicit rules as well as hide sensitive information for data sanitization. Many different algorithms and heuristics have been investigated to hide sensitive information using the act of transaction deletion based on evolutionary computation techniques, but to date, these algorithms only consider a uniform threshold value for sanitization progress. This technique is not applicable in real-world situations, especially for eHealth based medical datasets. For example, a patient can still be identified if he/she has more confidential information (i.e., symptoms) that cause privacy threats and security leakage in medical applications. In this work, we investigate a unique novel methodology to set varied threshold values that lead to varied lengths of sensitive patterns within a Genetic Algorithm (GA)-based framework. As the pattern length increases, a tighter threshold manifests to provide better protection of sensitive information that can avoid individual patients to be identified in eHealth datasets. Two GA-based models are developed for data sanitization using record deletion techniques. The experimental results are conducted and compared with the traditional Evolutionary Computation (EC)-based PPDM approaches and the results showed that the designed methods offer greater protection than previous methods in terms of side effects. Therefore, the designed models are effective to hide sensitive information in medical situations that can be used in real-world scenarios.}
}
@article{CRAMER2021586,
title = {Towards a flexible process-independent meta-model for production data},
journal = {Procedia CIRP},
volume = {99},
pages = {586-591},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.112},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004157},
author = {Simon Cramer and Max Hoffmann and Peter Schlegel and Marco Kemmerling and Robert H. Schmitt},
keywords = {Process data, Meta-model, Predictive quality, Smart production, Production data, Digital shadow, Data analytics},
abstract = {Data integration is a considerable challenge when investigating information sources from a multi-step manufacturing process. The interpretation of the process data profoundly depends on the incurred meta-data. However, during most data aggregating processes along the production chain, accompanying meta-information of vital importance is lost. To address this shortcoming, we propose a flexible and process-independent meta-model for efficient data integration for multi-step manufacturing processes. The product-oriented model unites process- and meta-data to reflect their mutual relationships within the manufacturing process. The context provided by the meta-information enables automatic data analysis for Predictive Quality applications in a cross-company setting.}
}
@incollection{SADRANI2021426,
title = {Sensors and Data Driven Approaches in Transport},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {426-431},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10790-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717107900},
author = {Mohammad Sadrani and Constantinos Antoniou},
keywords = {Driver behavior monitoring, Machine learning methods, Origin–destination matrices estimation, Safety analysis, Sensors, Sensor fusion approaches, Smartphone-based sensors, Traffic data, Traffic network surveillance, Transportation mode inference, Travel time estimation},
abstract = {Sensors play an important role in collecting real-time information for transportation systems. Nowadays, several different sensor technologies, ranging from traditional ones to mobile sensors in smartphones, are being used to collect a massive data volume on the real-time location and dynamics of users. For example, most of the modern smartphones are equipped with multiple motion sensors, such as accelerometer and magnetometer sensors, which can provide an unprecedented opportunity for the monitoring of the motion status of mobile phone users. On the other hand, there are a wide variety of data mining and prediction techniques, which can support transportation researchers in analyzing raw travel data collected from sensor technologies. This article provides a review of various applications of sensor technologies in transport networks, including travel time estimation, origin–destination matrices estimation, safety analysis, driver behavior monitoring, and transportation mode inference.}
}
@article{COSOLI2021109966,
title = {Measurement of multimodal physiological signals for stimulation detection by wearable devices},
journal = {Measurement},
volume = {184},
pages = {109966},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.109966},
url = {https://www.sciencedirect.com/science/article/pii/S026322412100899X},
author = {Gloria Cosoli and Angelica Poli and Lorenzo Scalise and Susanna Spinsante},
keywords = {Acoustic stimulation detection, Wearable devices, Measurement systems, Multimodal physiological signals, Features selection, Machine learning},
abstract = {The presence of stimuli and the consequent reactions undoubtedly reflect in experience-related changes of physiological parameters, which can be monitored by wearable devices. Generally, reactions related to the sympathetic nervous system activity are assessed through heart rate variability analysis. However, the exploitation of multimodal physiological signals provides a broader fingerprint. This study aims to identify the elicitation of acoustic stimulation through a wearable device; physiological signals, including electrodermal activity and skin temperature, were measured on a test population wearing a wrist-worn medical device. Eight machine learning algorithms were evaluated in a binary classification (presence/absence of stimuli), using 22 meaningful metrics from the collected data. The experimental results showed that Linear Regression (LR) algorithm, followed by Support Vector Machine (SVM), performed satisfactorily across all the evaluation metrics, achieving 75.00% and 72.62% of accuracy rate, respectively. Finally, the trained LR and SVM algorithms have been validated on a publicly available dataset (WESAD).}
}
@article{SUN2021103751,
title = {A critical review of distributed fiber optic sensing for real-time monitoring geologic CO2 sequestration},
journal = {Journal of Natural Gas Science and Engineering},
volume = {88},
pages = {103751},
year = {2021},
issn = {1875-5100},
doi = {https://doi.org/10.1016/j.jngse.2020.103751},
url = {https://www.sciencedirect.com/science/article/pii/S1875510020306053},
author = {Yankun Sun and Jinquan Liu and Ziqiu Xue and Qi Li and Chengkai Fan and Xu Zhang},
keywords = {Distributed fiber-optic sensing, Geologic CO sequestration, Brillouin- Rayleigh backscattering, Strain response, Temperature profile, Microseismicity detection},
abstract = {Geologic CO2 sequestration (GCS) has been identified as the most viable option for effectively reducing greenhouse gases emissions to mitigate global warming and worldwide climate change. However, CO2 injection into subsurface can induce reservoir expansion and fault reactivation, which ultimately result in near-surface infrastructure damage and personnel insecurity. Distributed fiber optic sensing (DFOS) technologies function one single fiber as an array of sensors to in-situ monitor multi-parameters, such as geomechanical deformation (i.e., strain), temperature, acoustics and pressure along the entire fiber or cable length. Due to its superiority over conventional geophone and detector, DFOS tool possesses great potential to sense geofluid injection-induced small disturbances in deep subsurface. Here we begin by highlighting recent research efforts in available monitoring tools employed in GCS sites. Given the increasing attentions of optical sensing, we present a first-hand review of DFOS categories, sensing principles, and advantages for GCS related investigations from both laboratory and field scales. We discuss in detail three typical DFOS-deployed GCS projects and explore the implicit findings to guide subsequent GCS field applications. Finally, we summarize the major challenges and going forward in developing, utilizing, and extending DFOS systems to widely apply for the future large-scale all-optical GCS monitoring sites.}
}
@incollection{ILMUDEEN2021363,
title = {Chapter 16 - Design and development of IoT-based decision support system for dengue analysis and prediction: case study on Sri Lankan context},
editor = {Valentina E. Balas and Souvik Pal},
booktitle = {Healthcare Paradigms in the Internet of Things Ecosystem},
publisher = {Academic Press},
pages = {363-380},
year = {2021},
isbn = {978-0-12-819664-9},
doi = {https://doi.org/10.1016/B978-0-12-819664-9.00016-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196649000168},
author = {Aboobucker Ilmudeen},
keywords = {Decision support system, Dengue, Disease analysis and prediction, Fuzzy Rule Neural Classification, Internet of Things},
abstract = {Dengue fever is an epidemic viral disease that is spread by various types of dengue viruses of the genus Aedes, primarily Aedes aegypti. Dengue epidemics are common in humid and subhumid areas of the world, mostly in cities and suburban regions. The old methods were delay in diagnosing and restricting the growth of dengue eruption. This chapter proposes a fresh approach in Fuzzy Rule–based Neural Classification with Internet of Things (IoT), cloud computing, and fog computing to analyze and predict dengue outbreak. The proposed fog-driven IoT architecture in which each component is seamlessly connected with each other to execute activities such as disease management, preventative care, clinical monitoring, early warning systems, e-medicine, and drug and food recommender system. This IoT-based decision support system aims to stop, control, and enable forecasting of eruptions of dengue, facilitating medical officers the information and insights to handle the outbreak, well in advance.}
}
@article{RICONDO2021762,
title = {A digital twin framework for the simulation and optimization of production systems},
journal = {Procedia CIRP},
volume = {104},
pages = {762-767},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.128},
url = {https://www.sciencedirect.com/science/article/pii/S221282712101026X},
author = {Itziar Ricondo and Alain Porto and Miriam Ugarte},
keywords = {digital twin, discrete-event simulation, monitoring, optimization, servitization},
abstract = {Industry 4.0 has raised the expectations on productivity, automation, and resource efficiency of manufacturing systems. This paper proposes a digital twin framework for the simulation and optimization of production lines and cells that can be used in the design and operation stages. The framework is supported by an architecture that connects manufacturing and machine tool data (digital shadow), the discrete event simulation model and the optimization engine, allowing for a variety of functionalities to plan and manage the production system. A use case is provided to demonstrate this framework, implemented in an automated line for the manufacturing of railway axles.}
}
@article{AHAMMAD2021102292,
title = {QoS Performance Enhancement Policy through Combining Fog and SDN},
journal = {Simulation Modelling Practice and Theory},
volume = {109},
pages = {102292},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2021.102292},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X21000216},
author = {Ishtiaq Ahammad and Md. Ashikur Rahman Khan and Zayed Us Salehin},
keywords = {Internet of Things, Software-Defined Networking, Fog Computing, Quality of Service, Modeling and Simulation, iFogSim Simulator},
abstract = {The goal of Internet of Things (IoT) is to bring any object online, thereby creating a massive volume of data that can overwhelm the existing computing and networking technologies. Therefore, centered cloud isn't ideal for rapidly expanding IoT environmental requirements. Fog computing (FC) moves some portion of the computing load (related to real-time services) from the cloud into edge fog devices. FC is expected to become the subsequent major computing transition and this one has ability to overcome existing cloud limitations. However the key obstacles facing FC are: wide distribution, isolated coupling, quality-of-service (QoS) regulation, adaptability to conditions, and particularly the standardization and normalization is still in phase of development. Software defined networking (SDN) will help fog to solve these obstacles. SDN means unified network control plane (which is separated from data plane), allowing the introduction for advanced traffic control and the orchestration mechanisms of networks and resources. On the grounds of SDN concept, and then combining it with FC, the network type can be modified to resolve all those cloud drawbacks and improve IoT system's QoS. Within this paper, architecture is developed through the combination of independently researched areas of SDN and FC to enhance the QoS in an IoT system. An algorithm (which is dependent on partition the SDN virtually) is presented to support the architecture whose purpose is to select the optimal access point and optimal place to process the data. The main objective of this algorithm is to provide improved QoS by partitioning the corresponding fog devices through the SDN controller. A use case dependent on the presented architecture and algorithm is then provided and assessed this use case's QoS parameter values (network usage, cost, latency and power consumption) using the iFogSim simulator. In contrast to cloud-only deployment, the result indicates a major enhancement of the mentioned QoS parameter values in the deployment of fog with SDN. In addition, once compared to a relative former identical use case; the findings of this paper show improved results for power consumption, network usage and latency. In fact, when compared to a former identical use case, the outcome of this paper shows around 3 times less latency and 2 times less network usage. Finally the ground (IoMT, Industry 4.0, Green IoT, and 5G) that is influenced by this QoS improvement is broadly illustrated in this paper.}
}