@article{WANG2021122727,
title = {An integrated approach to uncover quality markers of stir-baking Semen Cuscuta with salt solution preventing recurrent spontaneous abortion based on chemical and metabolomic profiling},
journal = {Journal of Chromatography B},
volume = {1177},
pages = {122727},
year = {2021},
issn = {1570-0232},
doi = {https://doi.org/10.1016/j.jchromb.2021.122727},
url = {https://www.sciencedirect.com/science/article/pii/S1570023221002075},
author = {Xiaoli Wang and Haiyan Gao and Song Tan and Chao Xu and Fengqing Xu and Tongsheng Wang and Jijun Chu and Yanquan Han and Deling Wu and Chuanshan Jin},
keywords = {, Stir-baking with salt solution, Metabolomics, UHPLC-Q-TOF-MS},
abstract = {The previous research of clinical big data mining showed that stir-baking Semen Cuscuta with salt solution (YP) ranked the first in the usage rate of treating abortion caused by kidney deficiency. At the same time, pharmacodynamic studies also showed that YP has better effect on improving recurrent spontaneous abortion (RSA) compared to raw products of Semen Cuscuta (SP). However, there were few studies on the biomarkers of YP improving RSA. In this study, the chemical and metabonomic profiling were used to screen the quality markers of YP on improving RSA. Firstly, a metabolomics study was carried out to select representative biomarkers of RSA. The ultra-high performance liquid chromatography coupled with electrospray ionization-quadrupole-time of flight-mass spectrometry (UPLC-ESI-Q-TOF-MS) technique was used to investigate the components of exogenous and endogenous in serum of rats after administrated with YP and SP. As a result, 14 differential compounds were identified between the serum of rats administrated SP and YP. Compared to SP, there was an upward trend in YP of the compounds including kaempferol-3-glucuronide, iso-kaempferol-3-glucuronide, (1S) −11-hydroxyhexadecanoic acid and 3-phenylpropionic acid. Meanwhile, there was a reducing trend in YP of the compounds including kaempferol 3-arabinofuranoside, apigenin-3-O-glucoside, hyperoside, caffeic acid-β-D glucoside, dicaffeoylquinic acid, linoleic acid, 3,4-dicaffeoylquinic acid, caffeic acid, palmitic acid and methyl myristate. 12 biomarkers for RSA indication were identified. SP and YP have a certain effect on the endogenous biomarker. The regulation effect of YP was higher than that of SP. The main metabolic pathways included phenylalanine, tyrosine and tryptophan biosynthesis, glycerophospholipid metabolism, fatty acid biosynthesis, sphingolipid metabolism, biosynthesis of unsaturated fatty acids. This study demonstrated a promising way to elucidate the active chemical and endogenous material basis of TCM.}
}
@article{KRISTOFFERSEN2021108205,
title = {The effects of business analytics capability on circular economy implementation, resource orchestration capability, and firm performance},
journal = {International Journal of Production Economics},
volume = {239},
pages = {108205},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108205},
url = {https://www.sciencedirect.com/science/article/pii/S092552732100181X},
author = {Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li},
keywords = {Digital circular economy, Big data analytics, Circular economy capability, Sustainability, Policy implication, Digitalization},
abstract = {Today, most organizations are undergoing a digital transformation. At the same time, the gravity of environmental issues has put sustainability and the circular economy at the top of corporate agendas. To this end, information systems, in particular business analytics, are being highlighted as essential enablers of an accelerated circular economy transition. However, effectively managing this joint transformation is a challenge. Firms struggle to identify which organizational resources they should target and how those should be leveraged towards a firm-wide business analytics capability for circular economy. To address these questions, this study draws on recent literature dealing with smart circular economy and business analytics capabilities along with the resource-based and resource orchestration view to (1) create an instrument to measure firms’ business analytics capability for circular economy, and (2) examine the relationship among a circular economy-specific business analytics capability, circular economy implementation, resource orchestration capability, and firm performance. The proposed research model was tested using partial least squares structural equation modeling of survey data from 125 top-level managers at companies across Europe. The results show that firms with a strong business analytics capability have an increased resource orchestration capability and a greater ability to excel in the circular economy, resulting in improved organizational performance in building a more sustainable competitive advantage in an increasingly competitive business landscape. The effect of business analytics capability on firm performance is not direct but fully mediated through resource orchestration capability and circular economy implementation. The results empirically validate the proposed research model and offer pathways to future information systems research streams to support the operationalization of circular strategies. The study provides the first empirical evidence of a business analytics capability for circular economy and its effect on firm performance.}
}
@article{TANG2021102095,
title = {Social media-based disaster research: Development, trends, and obstacles},
journal = {International Journal of Disaster Risk Reduction},
volume = {55},
pages = {102095},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102095},
url = {https://www.sciencedirect.com/science/article/pii/S2212420921000613},
author = {Jiting Tang and Saini Yang and Weiping Wang},
keywords = {Social media, Disaster research, Obstacles, Data applicability},
abstract = {Social media, as a new data source, is a promising field in disaster research. Despite doubts about its validity, a growing number of research institutes and commercial companies are exploring the potential of social media in disaster risk management. To understand the development and trends in this domain, a bibliometric analysis was performed using 1573 related published articles in Web of Science between 1991 and 2019. We found that (1) the number of annual publications and new research institutes in this field grew rapidly but seems to have become saturated in recent years. (2) The main research force is independent universities with limited cooperation, and a knowledge network has not yet been formed in this arena. (3) Research hotspots evolve in the path of “conceptualization - refinement - application”. Due to the three features of social media data, namely, timeliness, subjectivity, and disequilibrium, obstacles still exist in applicable disaster types and population representativeness. We anticipate new scientific advances emerging to overcome some technical difficulties. Knowledge sharing, advanced computer science, and multiorganizational cooperation will benefit this arena. These findings indicate potential directions for the development of and innovation in social media-based disaster research.}
}
@article{ZIA2021100033,
title = {B-DRIVE: A blockchain based distributed IoT network for smart urban transportation},
journal = {Blockchain: Research and Applications},
volume = {2},
number = {4},
pages = {100033},
year = {2021},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2021.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2096720921000282},
author = {Mohammed Zia},
keywords = {Blockchain, Open data, Urban traffic, Internet of Things, Vehicle navigation},
abstract = {In this paper, I present B-DRIVE—a blockchain-based distributed IoT (Internet of Things) network for smart urban transportation. The network is designed to connect a large fleet of IoT devices, installed on various vehicles and roadside infrastructures, to distributed data storage centers, called as Full-Nodes, to log and disseminate sensor generated data. It connects devices from around the city to multiple Full-Nodes to log timestamped data into the blockchain. These sensors vary from GPS (Global Positioning System), air quality meter, gyrometer to speed cameras in order to facilitate efficient urban mobility. The three identified hardware layers that comprise the network are the IoT layer, Storage layer, and User layer. They consist of Moving/Static-Nodes, Full-Nodes, and Smart devices, respectively. The Moving/Static-Nodes are primarily made up of moving vehicles and road-side infrastructures, respectively, thus acting as various data sources. Whereas, Full-Nodes and Smart devices are institutions and mobile phones, acting as data handler/disseminator and navigator/data visualizer, respectively. The data, or data blocks, received by Full-Nodes get appended into Full and Running-Blockchain, meant for specific purposes. The network is designed to be free from any block mining activity. It provides open access to anonymous sensor data to end-users, especially scientists, policy-makers and entrepreneurs, to develop innovative urban transportation solutions. It is believed that a system like B-DRIVE, along with existing VANETs (Vehicular Ad-hoc NETworks), is capable of answering some of the current urban transportation issues around traffic congestion, navigation, and vehicle parking. Other applications of blockchain data could vary from user activity mapping to VGI (volunteered geographic information) data quality assessment. Two identified limitations of the presented architecture are the low processing power of current IoT devices and the lack of urban IoT infrastructure.}
}
@article{YU2021103538,
title = {Posture-related data collection methods for construction workers: A review},
journal = {Automation in Construction},
volume = {124},
pages = {103538},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103538},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520311183},
author = {Yantao Yu and Waleed Umer and Xincong Yang and Maxwell Fordjour Antwi-Afari},
keywords = {Behavior-based safety (BBS), Computer vision, Construction worker, Deep learning, Motion sensor, Occupational safety and health (OSH), Pose estimation},
abstract = {Construction workers' posture-related data is closely connected with their safety, health, and productivity performance. The importance of posture-related data has drawn the attention of researchers in construction management and other fields. Accordingly, many data collection methods have been developed and applied to collect posture-related data. Despite the importance of workers' posture-related data, there lacks a review of previous data collection methods in the construction industry. This paper fills the research gap by reviewing previous methods to collect posture-related data for construction workers via 1) summarizing working principles and applications of posture-related data collection in construction management, which demonstrates the extensive use of motion sensors and Red-Green-Blue (RGB) cameras in posture-related data collection, 2) comparing the above methods based on data quality and feasibility on construction sites, which reveals the reason why motion sensors and RGB cameras have been prevalent in previous studies, 3) revealing research gaps of posture-related data collection tools and applications, and providing possible future research directions.}
}
@article{SHI2021102334,
title = {To introduce a store brand or not: Roles of market information in supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {150},
pages = {102334},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2021.102334},
url = {https://www.sciencedirect.com/science/article/pii/S136655452100106X},
author = {Chun-lai Shi and Wei Geng},
keywords = {Store brand, National brand, Market information, Information accuracy, Information sharing},
abstract = {Retailers in multiple market segments have opposing business practices regarding whether to introduce a store brand. Assuming that manufacturers and retailers have symmetric knowledge on market information, prior literature has shown that retailers have incentive to introduce a store brand in cases in which the store brand intensively competes with the national brand. Information asymmetry, however, is prevalent in supply chains. In the era of big data, the latest advances in technology intensify information asymmetry by allowing retailers to access market information with improved accuracy. To address such information asymmetry, some retailers share information with manufacturers, while others do not. The gap between prior studies and current industrial practices motivates us to explore the roles of market information in supply chains regarding store brand introduction. In this research, we consider a two-echelon supply chain in which the retailer has an advantage over the manufacturer in accessing market information. In the focal supply chain, four decision scenarios are present, each of which deviates from the others on the retailer’s decision on information sharing and store brand introduction. By evaluating and comparing supply chain performances in these decision scenarios, we demonstrate the influence of information accuracy on the incentive for the retailer to introduce a store brand and the mitigation effect of information sharing on such influence. In particular, we contribute to the literature by finding that, surprisingly, high information accuracy prevents store brand introduction even if brand competition is intensive. This research bridges the gap between prior research and industrial practices and is the first to consider information asymmetry and information sharing in the context of store brand introduction. Our findings contribute to theories of store brand introduction and provide information sharing for practitioners as a managerial tool to realize the full potential of store brands.}
}
@article{WU2021110,
title = {Transcriptional regulation and functional analysis of Nicotiana tabacum under salt and ABA stress},
journal = {Biochemical and Biophysical Research Communications},
volume = {570},
pages = {110-116},
year = {2021},
issn = {0006-291X},
doi = {https://doi.org/10.1016/j.bbrc.2021.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0006291X21010366},
author = {Hui Wu and Huayang Li and Wenhui Zhang and Heng Tang and Long Yang},
keywords = {Tobacco, Transcriptome, MAPK, Soil salinization mechanism},
abstract = {Soil salinization is an important factor that restricts crop quality and yield and causes an enormous toll to human beings. Salt stress and abscisic acid (ABA) stress will occur in the process of soil salinization. In this study, transcriptome sequencing of tobacco leaves under salt and ABA stress in order to further study the resistance mechanism of tobacco. Compared with controlled groups, 1654 and 3306 DEGs were obtained in salt and ABA stress, respectively. The genes function enrichment analysis showed that the up-regulated genes in salt stress were mainly concentrated in transcription factor WRKY family and PAR1 resistance gene family, while the up-regulated genes were mainly concentrated on bHLH transcription factor, Kunitz-type protease inhibitor, dehydrin (Xero1) gene and CAT (Catalase) family protein genes in ABA stress. Tobacco MAPK cascade triggered stress response through up-regulation of gene expression in signal transduction. The expression products of these up-regulated genes can improve the abiotic stress resistance of plants. These results have an important implication for further understanding the mechanism of salinity tolerance in plants.}
}
@incollection{MCGILVRAY202129,
title = {Chapter 3 - Key Concepts},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {29-72},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000098},
author = {Danette McGilvray},
keywords = {Framework for Information Quality (FIQ), information life cycle, data life cycle, lineage, data quality dimensions, business impact techniques, data categories, master data, data specifications, metadata, data standards, reference data, data models, business rules, data governance, data stewardship, Ten Steps Process, data quality improvement cycle},
abstract = {This chapter introduces fundamental ideas, the understanding of which, will aid data quality work and use of the Ten Steps Process. Information, like financial and human resources, must be properly managed throughout its life cycle to get the full use and benefit from it, so the information life cycle is discussed with the acronym POSMAD as an easy way to remember the six phases of the information life cycle: Plan, Obtain, Store and Share, Maintain, Apply, and Dispose. POSMAD plus several additional concepts are summarized in the Framework for Information Quality (FIQ), which provides an at-a-glance view of the components necessary to have high quality information. Other concepts central to understanding and managing data are discussed, such as data quality dimensions, business impact techniques, data categories, and data specifications (with a focus on metadata, data standards, reference data, data models, and business rules), data governance and stewardship. An overview of each of the steps in the Ten Steps Process is given, along with their relationship to the concepts.}
}
@article{YAN2021102485,
title = {Large-scale crop mapping from multi-source optical satellite imageries using machine learning with discrete grids},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {103},
pages = {102485},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102485},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421001926},
author = {Shuai Yan and Xiaochuang Yao and Dehai Zhu and Diyou Liu and Lin Zhang and Guojiang Yu and Bingbo Gao and Jianyu Yang and Wenju Yun},
keywords = {Crop Mapping, Large-Scale, Remote Sensing, Machine Learning, Discrete Grids},
abstract = {The spatial distribution of crops is an important agricultural parameter, which is used to derive important information about crop productivity and food security. However, crop mapping on a large scale is challenging due to the low spatio-temporal information of satellite data, sparse sampling, and poor computational efficiency for massive data. To alleviate these problems, this study proposes a method based on discrete grids with machine learning to integrate GaoFen-1 and Sentinel-2 imagery. First, the proposed method fuses multi-source satellite data with similar observation characteristics to improve the spatial and temporal coverage of satellites. Second, a data augmentation technique based on a discrete grid framework was proposed to solve the problem of sparse samples. Finally, a machine learning algorithm in a discrete grid was introduced to improve processing efficiency and ensure the crop classification precision of large-scale remote sensing images. An experiment in the Sanjiang Plain area (approximately 108900 km2) of Northeast China showed that the proposed scheme benefited from a high spatio-temporal multi-source dataset and achieved good performance. Compared with a single data source, the accuracy of crop mapping using multi-source optical remote sensing data is higher, attaining up to 86 and 88 % in 2017 and 2018, respectively. Furthermore, the advantages of machine learning in discrete grids over large-scale areas are validated by evaluating the accuracy of different classifiers, which indicates the suitability of discrete grids in data augmentation and large-scale crop mapping. Finally, discrete grid technology offers a possibility for crop mapping over large-scale areas, and improves the processing efficiency of remote sensing big data. The findings in this study can contribute to studies on large-scale crop classification and serve as a reference to them.}
}
@article{BOLHASANI2021100550,
title = {Deep learning applications for IoT in health care: A systematic review},
journal = {Informatics in Medicine Unlocked},
volume = {23},
pages = {100550},
year = {2021},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2021.100550},
url = {https://www.sciencedirect.com/science/article/pii/S235291482100040X},
author = {Hamidreza Bolhasani and Maryam Mohseni and Amir Masoud Rahmani},
keywords = {Deep learning, Internet of things, Healthcare, Medical imaging, Wearable device, Systematic literature review},
abstract = {In machine learning, deep learning is the most popular topic having a wide range of applications such as computer vision, natural language processing, speech recognition, visual object detection, disease prediction, drug discovery, bioinformatics, biomedicine, etc. Of these applications, health care and medical science-related applications are dramatically on the rise. The tremendous big data growth, the Internet of Things (IoT), connected devices, and high-performance computers utilizing GPUs and TPUs are the main reasons why deep learning is so popular. Based on their specific tasks, medical IoT, digital images, electronic health record (EHR) data, genomic data, and central medical databases are the primary data sources for deep learning systems. Several potential issues such as privacy, QoS optimization, and deployment indicate the pivotal part of deep learning. In this paper, deep learning for IoT applications in health care systems is reviewed based on the Systematic Literature Review (SLR). This paper investigates the related researches, selected from among 44 published research papers, conducted within a period of ten years – 2010 to 2020. Firstly, theoretical concepts and ideas of deep learning and technical taxonomy are proposed. Afterwards, major deep learning applications for IoT in health care and medical sciences are presented through analyzing the related works. Later, the main idea, advantages, disadvantages, and limitations of each study are discussed, preceding suggestions for further research.}
}
@incollection{SARIDE2021105,
title = {Chapter 4 - Application of data handling techniques to predict pavement performance},
editor = {Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {44},
pages = {105-127},
year = {2021},
booktitle = {Data Science: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169716120300420},
author = {Sireesh Saride and Pranav R.T. Peddinti and B. Munwar Basha},
keywords = {Data handling, Automation, User interface (UI), Regression, Fatigue, Rutting, Pavement},
abstract = {The present study discusses the design of pavements and the importance of big data handling in improving their performance. A comprehensive framework based on a simple natural language processing technique is presented to reduce the computational time and error in data handling for pavement applications. The application of the proposed method to automate a graphical user interface (UI) adopted in pavement design is demonstrated. The proposed method was found to reduce the run-time by about 83% as compared to the conventional procedures. The proposed framework is highly flexible and can be adapted to extract data from various file formats and automate UIs at ease. To present the potential of this framework, about 0.2 million data sets representing pavement geometry and material properties were generated using language processing algorithms. Further, robust non-linear regression equations for calculating pavement damage in terms of fatigue and rutting strains were developed by using automated data processing through the pavement design interface.}
}
@article{DONG2021101724,
title = {A review of social media-based public opinion analyses: Challenges and recommendations},
journal = {Technology in Society},
volume = {67},
pages = {101724},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101724},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21001998},
author = {Xuefan Dong and Ying Lian},
keywords = {Public opinion, Social media, PRISMA, Challenges, Recommendations},
abstract = {Compared with survey polls, social media can yield a better and more comprehensive understanding of public perceptions of special topics in a more scientific manner. However, despite this advantage, there seem to be limited investigations into the challenges in social media-based public opinion analysis. This study offers an understanding of the challenges in this field and some corresponding recommendations. Through a systematic literature review, we identify 54 papers to analyze and discuss issues related to data collection, data quality, and data mining. This paper summarizes a framework for social media-based public opinion analysis as well as the commonly employed data mining methodologies. We found that collecting public opinion data from Facebook and Weibo is difficult because of their restricted application programming interface and measures against Web Crawler. How to effectively and conveniently delete invalid data and how to design data mining methods for social media data, especially for those in Chinese, are still two main challenges in social media-based public opinion analysis. We claim that using multiple data sources, optimizing keyword settings, enhancing interdisciplinary cooperation, and paying more attention to the functional role of social media can benefit the development of social media-based public opinion analysis. This study also highlights the potential risks of releasing the personal information of the public in the use of social media data in research.}
}
@article{ADAMYAN20211774,
title = {Gene Expression Signature of Endometrial Samples from Women with and without Endometriosis},
journal = {Journal of Minimally Invasive Gynecology},
volume = {28},
number = {10},
pages = {1774-1785},
year = {2021},
issn = {1553-4650},
doi = {https://doi.org/10.1016/j.jmig.2021.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1553465021001709},
author = {Leila Adamyan and Yana Aznaurova and Assia Stepanian and Daniil Nikitin and Andrew Garazha and Maria Suntsova and Maxim Sorokin and Anton Buzdin},
keywords = {Endometriosis, Molecular diagnostics, RNA sequencing, Gene expression signature, Big data in clinical medicine},
abstract = {ABSTRACT
Study Objective
To develop a prototype of a complex gene expression biomarker for the diagnosis of endometriosis on the basis of differences between the molecular signatures of the endometrium from women with and without endometriosis.
Design
Prospective observational cohort study. Evidence obtained from a well-designed, controlled trial without randomization.
Setting
Department of reproductive medicine and surgery, A.I. Evdokimov Moscow State University of Medicine and Dentistry.
Patients
A total of 33 women (aged 32–38 years) were included in this study. Patients with and without endometriosis were divided into 2 separate groups. The group composed of patients with endometriosis included 19 living patients with endometriosis who underwent laparoscopic excision of endometriosis. The control group included 6 living patients who underwent laparoscopic excision of incompetent uterine scar after cesarean section, with both surgically and histologically confirmed absence of endometriosis and adenomyosis. An additional control/verification group included various previously RNA-sequencing–profiled tissue samples (endocervix, ovarian surface epithelium) of 8 randomly selected healthy female cadaveric donors aged 32 to 38 years. The exclusion criteria for all patients were hormone therapy and any intrauterine device use for more than 1 year preceding surgery, as well as absence of other diseases of the uterus, fallopian tubes, and ovaries.
Interventions
Laparoscopic excision of endometriotic foci and hysteroscopy with endometrial sampling were performed. The cadaveric tissue samples included endocervix and ovarian surface epithelium. Endometrial sampling was obtained from the women in the control group. RNA sequencing was performed using Illumina HiSeq 3000 equipment (Illumina, Inc., San Diego, CA) for single-end sequencing. Unique bioinformatics algorithms were developed and validated using experimental and public gene expression datasets.
Measurements and Main Results
We generated a characteristic signature of 5 genes downregulated in the endometrium and endometriotic tissue of the patients with endometriosis, selected after comparison with the endometrium of the women without endometriosis. This gene signature showed a capacity for nearly perfect separation of all 52 analyzed tissue samples of the patients with endometriosis (endometrial as well as endometriotic samples) from the 14 tissue samples of both living and cadaveric donors without endometriosis (area under the curve = 0.982, Matthews correlation coefficient = 0.832).
Conclusion
The gene signature of the endometrium identified in this study may potentially serve as a nonsurgical diagnostic method for endometriosis detection. Our data also suggest that the statistical method of 5-fold cross-validation of differential gene expression analysis can be used to generate robust gene signatures using real-world clinical data.}
}
@article{ALSHAFEEY20217601,
title = {Evaluating neural network and linear regression photovoltaic power forecasting models based on different input methods},
journal = {Energy Reports},
volume = {7},
pages = {7601-7614},
year = {2021},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.10.125},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721011446},
author = {Mutaz AlShafeey and Csaba Csáki},
keywords = {Solar energy, Photovoltaic technology, Prediction model, Multiple regression, Artificial neural network, Prediction accuracy},
abstract = {As Photovoltaic (PV) energy is impacted by various weather variables such as solar radiation and temperature, one of the key challenges facing solar energy forecasting is choosing the right inputs to achieve the most accurate prediction. Weather datasets, past power data sets, or both sets can be utilized to build different forecasting models. However, operators of grid-connected PV farms do not always have full sets of data available to them especially over an extended period of time as required by key techniques such as multiple regression (MR) or artificial neural network (ANN). Therefore, the research reported here considered these two main approaches of building prediction models and compared their performance when utilizing structural, time-series, and hybrid methods for data input. Three years of PV power generation data (of an actual farm) as well as historical weather data (of the same location) with several key variables were collected and utilized to build and test six prediction models. Models were built and designed to forecast the PV power for a 24-hour ahead horizon with 15 min resolutions. Results of comparative performance analysis show that different models have different prediction accuracy depending on the input method used to build the model: ANN models perform better than the MR regardless of the input method used. The hybrid input method results in better prediction accuracy for both MR and ANN techniques, while using the time-series method results in the least accurate forecasting models. Furthermore, sensitivity analysis shows that poor data quality does impact forecasting accuracy negatively especially for the structural approach.}
}
@article{SCANNAPIECO2021100393,
title = {How to be responsible in all the steps of a data science pipeline: The case of the Italian public sector},
journal = {Patterns},
volume = {2},
number = {12},
pages = {100393},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100393},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921002609},
author = {Monica Scannapieco and Antonino Virgillito},
abstract = {The paper highlights how each step of a data science pipeline can be performed in a “responsible” way, taking into account privacy, ethics, and quality issues. Several examples from the Italian public sector contribute to clarifying how data collections and data analyses can be carried out under a responsible view.}
}
@incollection{RAO2021319,
title = {16 - Data duplication using Amazon Web Services cloud storage},
editor = {Tin Thein Thwel and G.R. Sinha},
booktitle = {Data Deduplication Approaches},
publisher = {Academic Press},
pages = {319-334},
year = {2021},
isbn = {978-0-12-823395-5},
doi = {https://doi.org/10.1016/B978-0-12-823395-5.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128233955000069},
author = {M. Varaprasad Rao},
keywords = {Data duplication, cloud computing, Amazon Web Services, data storage, fault tolerance},
abstract = {Since Big Data has become a worldwide phenomenon, both decision-makers and experts in data and analysis have been concerned about data scalability. They have been too reliant on the theory of better data in the digital age. Data duplication is an occurrence where a business has more than one copy of the same source of data. To a layman, data replication appears like a basic issue which could be avoided by any professional data scientist or experienced network administrator. Needless to say, the replication of data is also a very common issue. Cloud computing is a big part of the business sector today, where computing services are supplied to consumers via the Internet on demand. Data storage is one of the most common services provided by cloud computing. The key benefit of using cloud storage from the perspective of consumers is that they are only liable for the required quantity of data, which can be scaled and decreased on demand, to minimize their costs in buying and maintaining their data infrastructure. A decrease in data sizes will help providers minimize costs by using large storage facilities and minimize energy usage by rising data size in cloud computing. It has contributed to the implementation of data replication technology to increase cloud storage capacity. Because of the volatile existence of data in cloud storage and cloud overtime shifts, some data chunks can also be read in time but cannot be accessed in another time. Many databases can be accessed or modified simultaneously by several users and others need a high consistency level for accuracy. The author has suggested a dynamic cloud storage virtualization scheme to maximize storage capacity and preserve fault tolerance redundancy.}
}
@article{ROMEROSILVA2021102388,
title = {Learning from the past to shape the future: A comprehensive text mining analysis of OR/MS reviews},
journal = {Omega},
volume = {100},
pages = {102388},
year = {2021},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2020.102388},
url = {https://www.sciencedirect.com/science/article/pii/S0305048320307428},
author = {Rodrigo Romero-Silva and Sander {de Leeuw}},
keywords = {Operations research, Management science, Text mining, Bibliometric analysis, Emerging trends, Literature reviews},
abstract = {This paper provides an overview of the evolution and state-of-the-art of the Operations Research and Management Science (OR/MS) subject area from 1956 to 2019. Using text mining techniques on the content of the title, abstract, and author keywords of papers classified by the Web of Science as literature review studies in OR/MS, we found that there are 76 topical consolidated clusters in the field covering a wide range of reviewed topics. Since 2015, reviews on supply chain risk management and big data analytics have had the highest impact in the field, whereas topics such as Industry 4.0, socio-technical systems, social networks, green supply, sustainable supply chain, and resilience engineering have all received significant attention from researchers. Reviews on analytic hierarchy process were found to be the most impactful overall, showing the high relevance of multi-criteria decision making in the current research and practice contexts. Furthermore, a text mining analysis of the papers citing OR/MS literature reviews showed that optimization continues to be one of the most highly influential methodological contributions of OR/MS to other research areas and that topics such as circular economy, carbon emissions, and social commerce have yet to find some traction in OR/MS research, suggesting future research and multidisciplinary opportunities for the field. Results also show that the research area of Public Administration has been greatly influenced by OR/MS reviews as 16% of all the papers published in that field have cited at least one of the 1744 review papers included in this study. Finally, a summary table of published structured literature reviews per topic (benchmarks, classifications, taxonomies) is presented as a short bibliography of OR/MS review papers.}
}
@article{AFROZ2021106368,
title = {Risk-based centralized data monitoring of clinical trials at the time of COVID-19 pandemic},
journal = {Contemporary Clinical Trials},
volume = {104},
pages = {106368},
year = {2021},
issn = {1551-7144},
doi = {https://doi.org/10.1016/j.cct.2021.106368},
url = {https://www.sciencedirect.com/science/article/pii/S155171442100104X},
author = {Most Alina Afroz and Grant Schwarber and Mohammad Alfrad Nobel Bhuiyan},
keywords = {Centralize data monitoring, Risk based data monitoring},
abstract = {Objectives
COVID-19 pandemic caused several alarming challenges for clinical trials. On-site source data verification (SDV) in the multicenter clinical trial became difficult due to travel ban and social distancing. For multicenter clinical trials, centralized data monitoring is an efficient and cost-effective method of data monitoring. Centralized data monitoring reduces the risk of COVID-19 infections and provides additional capabilities compared to on-site monitoring. The key steps for on-site monitoring include identifying key risk factors and thresholds for the risk factors, developing a monitoring plan, following up the risk factors, and providing a management plan to mitigate the risk.
Methods
For analysis purposes, we simulated data similar to our clinical trial data. We classified the data monitoring process into two groups, such as the Supervised analysis process, to follow each patient remotely by creating a dashboard and an Unsupervised analysis process to identify data discrepancy, data error, or data fraud. We conducted several risk-based statistical analysis techniques to avoid on-site source data verification to reduce time and cost, followed up with each patient remotely to maintain social distancing, and created a centralized data monitoring dashboard to ensure patient safety and maintain the data quality.
Conclusion
Data monitoring in clinical trials is a mandatory process. A risk-based centralized data review process is cost-effective and helpful to ignore on-site data monitoring at the time of the pandemic. We summarized how different statistical methods could be implemented and explained in SAS to identify various data error or fabrication issues in multicenter clinical trials.}
}
@article{FORREST202160,
title = {PCORnet® 2020: current state, accomplishments, and future directions},
journal = {Journal of Clinical Epidemiology},
volume = {129},
pages = {60-67},
year = {2021},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2020.09.036},
url = {https://www.sciencedirect.com/science/article/pii/S0895435620311227},
author = {Christopher B. Forrest and Kathleen M. McTigue and Adrian F. Hernandez and Lauren W. Cohen and Henry Cruz and Kevin Haynes and Rainu Kaushal and Abel N. Kho and Keith A. Marsolo and Vinit P. Nair and Richard Platt and Jon E. Puro and Russell L. Rothman and Elizabeth A. Shenkman and Lemuel Russell Waitman and Neely A. Williams and Thomas W. Carton},
keywords = {Distributed data network, Clinical research network, Health plan research network, PCORnet, Pragmatic clinical trials, Electronic health records, Big data, Learning health system},
abstract = {Objective
To describe PCORnet, a clinical research network developed for patient-centered outcomes research on a national scale.
Study Design and Setting
Descriptive study of the current state and future directions for PCORnet. We conducted cross-sectional analyses of the health systems and patient populations of the 9 Clinical Research Networks and 2 Health Plan Research Networks that are part of PCORnet.
Results
Within the Clinical Research Networks, electronic health data are currently collected from 337 hospitals, 169,695 physicians, 3,564 primary care practices, 338 emergency departments, and 1,024 community clinics. Patients can be recruited for prospective studies from any of these clinical sites. The Clinical Research Networks have accumulated data from 80 million patients with at least one visit from 2009 to 2018. The PCORnet Health Plan Research Network population of individuals with a valid enrollment segment from 2009 to 2019 exceeds 60 million individuals, who on average have 2.63 years of follow-up.
Conclusion
PCORnet’s infrastructure comprises clinical data from a diverse cohort of patients and has the capacity to rapidly access these patient populations for pragmatic clinical trials, epidemiological research, and patient-centered research on rare diseases.}
}
@article{SHALA2021104603,
title = {Completion of electronic nursing documentation of inpatient admission assessment: Insights from Australian metropolitan hospitals},
journal = {International Journal of Medical Informatics},
volume = {156},
pages = {104603},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104603},
url = {https://www.sciencedirect.com/science/article/pii/S138650562100229X},
author = {Danielle Ritz Shala and Aaron Jones and Greg Fairbrother and Duong {Thuy Tran}},
keywords = {Electronic clinical documentation, Nursing admission, eMR data, Health informatics, Data science, Nursing informatics},
abstract = {Introduction
Electronic nursing documentation is an essential aspect of inpatient care and multidisciplinary communication. Analysing data in electronic medical record (eMR) systems can assist in understanding clinical workflows, improving care quality, and promoting efficiency in the healthcare system. This study aims to assess timeliness of completion of an electronic nursing admission assessment form and identify patient and facility factors associated with form completion in three metropolitan hospitals.
Materials and Methods
Records of 37,512 adult inpatient admissions (November 2018-November 2019) were extracted from the hospitals’ eMR system. A dichotomous variable descriptive of completion of the nursing assessment form (Yes/No) was created. Timeliness of form completion was calculated as the interval between date and time of admission and form completion. Univariate and multivariate multilevel logistic regression were used to identify factors associated with form completion.
Results
An admission assessment form was completed for 78.4% (n = 29,421) of inpatient admissions. Of those, 78% (n = 22,953) were completed within the first 24 h of admission, 13.3% (n = 3,910) between 24 and 72 h from admission, and 8.7% (n = 2,558) beyond 72 h from admission. Patient length of hospital stay, admission time, and admitting unit’s nursing hours per patient day were associated with form completion. Patient gender, age, and admitting unit type were not associated with form completion.
Discussion
Form completion rate was high, though more emphasis needs to be placed on the importance of timely completion to allow for adequate patient care planning. Staff education, qualitative understanding of delayed form completion, and streamlined guidelines on nursing admission and eMR use are recommended.}
}
@article{RANGELMARTINEZ2021414,
title = {Machine learning on sustainable energy: A review and outlook on renewable energy systems, catalysis, smart grid and energy storage},
journal = {Chemical Engineering Research and Design},
volume = {174},
pages = {414-441},
year = {2021},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2021.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0263876221003312},
author = {Daniel Rangel-Martinez and K.D.P. Nigam and Luis A. Ricardez-Sandoval},
keywords = {Machine learning, Artificial neural networks, Renewable energies, Catalysis, Power systems, Sustainability, Energy efficiency},
abstract = {This study presents a broad view of the current state of the art of ML applications in the manufacturing sectors that have a considerable impact on sustainability and the environment, namely renewable energies (solar, wind, hydropower, and biomass), smart grids, the industry of catalysis and power storage and distribution. Artificial neural networks are the most preferred techniques over other ML algorithms because of their generalization capabilities. Demands for ML techniques in the energy sectors will increase considerably in the coming years, since there is a growing demand of academic programmes related to artificial intelligence in science, math, and engineering. Data generation, management, and safety are expected to play a key role for the successful implementation of ML algorithms that can be shared by major stakeholders in the energy sector, thereby promoting the development of ambitious energy management projects. New algorithms for producing reliable data and the addition of other sources of information (e.g., novel sensors) will enhance flow of information between ML and systems. It is expected that unsupervised and reinforcement learning will take a central role in the energy sector, but this will depend on the expansion of other major fields in data science such as big data analytics. Massive implementations, specialized algorithms, and new technologies like 5G will promote the development of sustainable applications of ML in non-industrial applications for energy management.}
}
@article{NGUEILBAYE2021107167,
title = {Modulo 9 model-based learning for missing data imputation},
journal = {Applied Soft Computing},
volume = {103},
pages = {107167},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107167},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621000909},
author = {Alladoumbaye Ngueilbaye and Hongzhi Wang and Daouda Ahmat Mahamat and Sahalu B. Junaidu},
keywords = {Data quality, Machine learning algorithms, Missing data, Modulo 9},
abstract = {Missing Values Management is one of the challenges faced by Data Analysts. Therefore, the creation of effective data models will be the right decision for missing data imputation. However, learning, training, and Data Analysis must be implemented through machine learning algorithms. Missing Data is a problem with no feedback or variables. This problem (missing data) can result in serious Data Analysis, which may eventually lead to erroneous conclusions. This research paper first studies how missing data can affect Machine Learning Algorithms, and decision-making based on the Data Analysis’s output. Secondly, it proposes Modulo 9 as a novel method for handling missing data problems. The proposed novel method is assessed with wide-ranging experiments compared with robust Machine Learning techniques such as Support Vector Machine (SVM) Algorithm, Linear Regression (LR), K-Nearest Neighbors (KNN), Naïve Bayes (NB), Support Vector Classifier (SVC), Linear Support Vector Classifier (LSVC), Random Forest Classifier (RFC), Decision Tree Regressor (DTR), Deletion Method, Multi-Layer Perceptron (MLP), and the Mean Value. The results show that the novel method outperforms the eleven (11) existing methods.}
}
@article{SCHUHMACHER20212786,
title = {Systematic risk identification and assessment using a new risk map in pharmaceutical R&D},
journal = {Drug Discovery Today},
volume = {26},
number = {12},
pages = {2786-2793},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1359644621002877},
author = {Alexander Schuhmacher and Clara Brieke and Oliver Gassmann and Markus Hinder and Dominik Hartl},
keywords = {Pharmaceutical, Research and development (R&D), Artificial intelligence, Drug discovery, Drug development, Risk},
abstract = {Delivering transformative therapies to patients while maintaining growth in the pharmaceutical industry requires an efficient use of research and development (R&D) resources and technologies to develop high-impact new molecular entities (NMEs). However, increasing global R&D competition in the pharmaceutical industry, growing impact of generics and biosimilars, more stringent regulatory requirements, as well as cost-constrained reimbursement frameworks challenge current business models of leading pharmaceutical companies. Big data-based analytics and artificial intelligence (AI) approaches have disrupted various industries and are having an increasing impact in the biopharmaceutical industry, with the promise to improve and accelerate biopharmaceutical R&D processes. Here, we systematically analyze, identify, assess, and categorize key risks across the drug discovery and development value chain using a new risk map approach, providing a comprehensive risk–reward analysis for pharmaceutical R&D.}
}
@incollection{KIZRAK202191,
title = {6 - Limitations and challenges on the diagnosis of COVID-19 using radiology images and deep learning},
editor = {Utku Kose and Deepak Gupta and Victor Hugo C. {de Albuquerque} and Ashish Khanna},
booktitle = {Data Science for COVID-19},
publisher = {Academic Press},
pages = {91-115},
year = {2021},
isbn = {978-0-12-824536-1},
doi = {https://doi.org/10.1016/B978-0-12-824536-1.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128245361000071},
author = {Merve Ayyuce Kızrak and Zümrüt Müftüoğlu and Tülay Yıldırım},
keywords = {COVID-19, Data privacy, Deep learning, Differential privacy, EfficientNet, Explainable artificial intelligence, Radiology imaging, Small data},
abstract = {The world is facing a great threat nowadays. The COVID-19 virus outbreak that occurred in Wuhan in China in December 2019 continues to increase in the middle of 2020. Within the scope of this epidemic, different contents of data are published and products for improving the treatment process. One of the major symptoms of COVID-19 epidemic disease, which was revealed by the World Health Organization, is intense cough and breathing difficulties. Chest X-ray (CXR) and computing tomography (CT) images of patients infected with COVID-19 are also a type of data that allows data scientists to work with healthcare professionals during this struggle. Fast evaluation of these images by experts is important in the days when the epidemic has suffered. This chapter focuses on artificial intelligence (AI) for a successful and rapid diagnostic recommendation as part of these deadly epidemic prevention efforts that have emerged. As a study case, a dataset of 373 CXR images, 139 of which were COVID-19 infected, collected from open sources, was used for diagnosis with deep learning approaches of COVID-19. The use of EfficientNet, an up-to-date and robust deep learning model for education, offers the possibility to become infected with an accuracy of 94.7%. Nevertheless, some limitations must be considered when producing AI solutions by making use of medical data. Using these results, a perspective is provided on the limitations of deep learning models in the diagnosis of COVID-19 from radiology images for data quality, amount of data, data privacy, explainability, and robust solutions.}
}
@article{BAUMANN2021105096,
title = {A general conceptual framework for multi-dimensional spatio-temporal data sets},
journal = {Environmental Modelling & Software},
volume = {143},
pages = {105096},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105096},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221001390},
author = {Peter Baumann},
keywords = {Coverage, Spatio-temporal field, Datacube, Conceptual model, Standards, ISO, OGC},
abstract = {In the era of ubiquitous data collection and generation, demands are high to make these data accessible as widely as possible, with as little effort and as much power and flexibility as ever possible. On Earth data, this holds in particular for pixel data and point clouds, some of the main “Big Data” today. Coverages represent a unifying concept for space/time-varying data, especially for spatio-temporal gridded data, nowadays often called “datacubes". Coverage standards exist, however, their fundaments appear in places technically outdated, imprecise, and not suitable for the full spectrum of data. Due to this lack there is a danger of missing interoperability goals and impeding future-directed “Big Earth Data” services. We introduce the conceptual coverage model of the forthcoming ISO 19123-1 standard. It is generic, supporting all spatio-temporal dimensions in a unified manner, and is compatible with the existing coverage implementation standards of OGC and ISO. We demonstrate feasibility through concrete service examples.}
}
@article{MATHRANI2021100060,
title = {Perspectives on the challenges of generalizability, transparency and ethics in predictive learning analytics},
journal = {Computers and Education Open},
volume = {2},
pages = {100060},
year = {2021},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2021.100060},
url = {https://www.sciencedirect.com/science/article/pii/S2666557321000318},
author = {Anuradha Mathrani and Teo Susnjak and Gomathy Ramaswami and Andre Barczak},
keywords = {Learning analytics, Generalizability, Interpretability, Feature extraction, Transparency, Ethics protocol},
abstract = {Educational institutions need to formulate a well-established data-driven plan to get long-term value from their learning analytics (LA) strategy. By tracking learners’ digital traces and measuring learners’ performance, institutions can discern consequential learning trends via use of predictive models to enhance their instructional services. However, questions remain on how the proposed LA system is suitable, meaningful, and justifiable. In this concept paper, we examine generalizability and transparency of the internals of predictive models, alongside the ethical challenges in using learners’ data for building predictive capabilities. Model generalizability or transferability is hindered by inadequate feature representation, small and imbalanced datasets, concept drift, and contextually un-related domains. Additional challenges relate to trustworthiness and social acceptance of these models since algorithmic-driven models are difficult to interpret by themselves. Further, ethical dilemmas are faced in engaging with learners’ data while developing and deploying LA systems at an institutional level. We propose methodologies for apprehending these challenges by establishing efforts for managing transferability and transparency, and further assessing the ethical standing on justifiable use of the LA strategy. This study showcases underlying relationships that exist between constructs pertaining to learners’ data and the predictive model. We suggest the use of appropriate evaluation techniques and setting up research ethics protocols, since without proper controls in place, the model outcome would not be portable, transferable, trustworthy, or admissible as a responsible outcome. This concept paper has theoretical and practical implications for future inquiry in the burgeoning field of learning analytics.}
}
@article{SUN202128,
title = {Laboratory information management system for biosafety laboratory: Safety and efficiency},
journal = {Journal of Biosafety and Biosecurity},
volume = {3},
number = {1},
pages = {28-34},
year = {2021},
issn = {2588-9338},
doi = {https://doi.org/10.1016/j.jobb.2021.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2588933821000042},
author = {Dingzhong Sun and Linhuan Wu and Guomei Fan},
keywords = {Laboratory information management system, Biosafety, Biological research laboratory, Laboratory safety, Workflow management},
abstract = {Laboratory information management system (LIMS) has been widely used to facilitate laboratory activities. However, the current LIMSs do not contain functions to improve the safety of laboratory work, which is the major concern of biosafety laboratories (BSLs). With tons of biosafety information that need to be managed and an increasing number of biosafety-related research projects under way, it is worthy of expanding the current framework of LIMS and building a system that is more suitable for BSL usage. Such a system should carefully trade off between the safety and efficiency of regular lab activities, allowing the laboratory staff to conduct their research as free as possible while ensuring their and the environment’s safety. In order to achieve this goal, the information on the research contents, laboratory personnel, experimental materials and experimental equipment need to be well collected and fully utilized by a centralized system and its databases.}
}
@article{NYANCHOGA20216238,
title = {Exploring electronic health records to estimate the extent of catch-up immunisation and factors associated with under-immunisation among refugees and asylum seekers in south east Queensland},
journal = {Vaccine},
volume = {39},
number = {42},
pages = {6238-6244},
year = {2021},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2021.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X21012056},
author = {Mercy Moraa Nyanchoga and Patricia Lee and Gaery Barbery},
keywords = {Vaccination, Catch-up immunisation, Refugee, Migrant Health, Digital Health, Electronic Health Records},
abstract = {Background
Australia is one of the leading countries resettling people from refugee-like backgrounds. Catch-up immunisation is a key priority in this cohort. However, few studies have included asylum seekers and the adult age group in their study sample. In addition, Electronic Health Records (EHR) has recently been recognised as a vital tool in big data analysis with the capacity to contribute to informed strategic decision making. As such, the main aim of this study is to explore EHR routinely used in a specialised refugee clinic in South East Queensland to estimate the extent of catch-up immunisation and assess the factors associated with under-immunisation among refugees and asylum seekers.
Methods
A quantitative study involving a secondary data analysis on a pre-existing dataset was undertaken. Relevant data was extracted from the EHR in the clinic. SPSS was used to perform Statistical data analysis.
Results
The majority of clients originated from Papua New Guinea, followed by Iran and Afghanistan. When assessing the uptake of catch-up immunisations among refugees and asylum seekers, MMR (Measles-Mumps-Rubella), Polio and DTP (Diphtheria-Tetanus-Pertussis) had the highest uptake, while HPV (Human Papilloma Virus), Pneumococcal and Hib (Haemophilus influenza type b) immunisations had the lowest uptake. Binary logistic regression revealed that the younger patients, the refugees (compared to asylum seekers) and those with a longer residential duration in Australia are at a higher risk of being under-immunised.
Conclusion
This study indicates that the broader group of immigrants, and in particular refugees and asylum seekers, do not represent a homogenous group in terms of immunisation coverage, and that each cohort should be carefully considered during immunisation interventions and strategies. This will be particularly important during targeted health promotions and future immunisation programs in this cohort.}
}
@article{MAAS2021132,
title = {Cross-disciplinary approaches for better research: The case of birds and bats},
journal = {Basic and Applied Ecology},
volume = {56},
pages = {132-141},
year = {2021},
issn = {1439-1791},
doi = {https://doi.org/10.1016/j.baae.2021.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1439179121001067},
author = {Bea Maas and Carolina Ocampo-Ariza and Christopher J. Whelan},
keywords = {Agricultural biodiversity, Collaborative conservation, Ecosystem functions, Ecosystem services, Knowledge co-production, Sustainable agriculture, Transdisciplinary research},
abstract = {Across a wide range of disciplines, mounting evidence points to solutions for addressing the global biodiversity and climate crisis through sustainable land use development. Managing ecosystem services offers promising potential of combining environmental, economic, and social interests in this process. Achieving sustainability, however, requires collaboration across disciplines, or in short “cross-disciplinary” approaches. Multi-, inter- and transdisciplinary approaches are often used as synonyms, although they are defined by different levels of integrating results and perspectives. We highlight challenges and opportunities related to these cross-disciplinary approaches by using research on bird- and bat-mediated ecosystem services as a case - with a focus on sustainable agricultural development. Examples from transdisciplinary collaborations show how more integrative and inclusive approaches promote the implementation of basic and applied ecological research into land use practices. Realizing this opportunity requires strong partnerships between science, practice and policy, as well as integration of diverse skills and perspectives. If appropriately funded and guided, this effort is rewarded by improved data quality, more targeted concepts, as well as improvement implementation and impact of sustainability research and practice. We outline a stepwise approach for developing these processes and highlight case studies from bird and bat research to inspire cross-disciplinary approaches within and beyond ecology.}
}
@article{AHMED2021100569,
title = {A simple and robust wetland classification approach by using optical indices, unsupervised and supervised machine learning algorithms},
journal = {Remote Sensing Applications: Society and Environment},
volume = {23},
pages = {100569},
year = {2021},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2021.100569},
url = {https://www.sciencedirect.com/science/article/pii/S2352938521001051},
author = {Kazi Rifat Ahmed and Simu Akter and Andres Marandi and Christoph Schüth},
keywords = {Wetland classification, Optical index, Machine learning, K-means cluster, Support vector machine classification},
abstract = {Wetlands are important for their peat reservoir, dynamic land cover and natural resources, ecological and hydrological regimes, fossil fuels reservoir, and crucial carbon storage. The global wetlands are decreasing since 1800 due to climatic phenomena and human activities. Wetland mapping with satellite data is not new but an ongoing challenge due to its precision relies on data quality and data classification schemes. The accuracy of such mapping is emerging due to the gradual establishment of satellite data and subsequent data modeling technologies, i.e., big data modeling with machine learning (ML) algorithms. Our study introduced a simple, scalable, and robust wetland classification by applying unsupervised (K-means cluster – KMC) and supervised (Support vector machine classification – SVMc) ML algorithms. We used Landsat optical data to model normalized difference vegetation index (NDVI) and normalized difference water index (NDWI), as the primary inputs for KMC. Later KMC data were supervised by SVMc with training data from 20 field observations. The accuracy tests insights that both optical indices have considerably less error and all SVMc models have about 99% accuracy. The 1 to 1 validation insights that our wetland classification presented more detail wetland cover areas than reference data. The test case SVMc showed that Class 1 and 2 are optimally fitting, Class 3 and 4 are overfitting, and Class 5 is underfitting. Furthermore, the sensitivity analysis insight that all SVMc models are optimally fitting and SVMc is more sensitive to NDVI than NDWI. From SVMc models we can see that Selisoo bog in Estonia lost a considerable amount of wetland covers including water bodies, and more large forest covers are taking wetland areas, i.e., mixed forest and coniferous trees. Our methodological approach insights a simple and robust wetland classification based on advanced unsupervised and supervised ML algorithms despite some unavoidable limitation.}
}
@article{MINGHAO202145,
title = {Surface Rotation of LAMOST-Kepler Li-rich Giant Stars},
journal = {Chinese Astronomy and Astrophysics},
volume = {45},
number = {1},
pages = {45-57},
year = {2021},
issn = {0275-1062},
doi = {https://doi.org/10.1016/j.chinastron.2021.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0275106221000035},
author = {DU Ming-hao and BI Shao-lan and SHI Jian-rong and YAN Hong-liang},
keywords = {Stars: rotation, Stars: abundance, stars: chemically peculiar, method: data analysis},
abstract = {The years-long high-precision photometric data observed by Kepler satellite combining with huge amount of spectra observed by LAMOST provide a great opportunity to study the relations between surface rotation and lithium abundance of li-rich giants. In this study, we cross match the Kepler data with li-rich giants catalog from LAMOST, and obtain 619 common sources. Then we measure 36 rotation periods from the full set of 295 li-rich giants with good data quality which consists of two sub-samples. The rotation periods of 14 stars was extracted from 205 stars with evolutionary stages determined using asteroseismology, including 11 core helium-burning stars (HeBs), 2 red giant branch stars (RGBs), and 1 unclassified star. All the super lithium-rich giant stars (A(Li)>3.3   dex) are HeBs in our sample. The remaining 90 giants do not have evolutionary stages confirmed, and in this sub-sample, 22 giants have their rotation period measured. The rotation detection rate of the former sub-sample is 6.8%, which is significantly higher than the detection rate of a large giant sample in previous studies (2.08%). With the surface rotation period measured, we confirm the relation between stellar rotation and lithium enrichment of giants. Meanwhile, we find that the less Li-enriched stars have a relatively dispersed distribution of rotation periods, and the giants with a high Li enrichment is concentrated on the rapidly rotating area, which is consistent with earlier studies. The present work also shows a jump at A(Li)≈3.3   dex in the relation between rotation period and Li abundance that coincidently is the boundary between Li-rich giants and super Li-rich giants which may indicate the different mechanisms. The rotation periods of super Li-rich giants become shorter as the lithium enrichment increases. This correlation provides an evidence for the rotational induced extra-mixing mechanism responsible for Li enrichment of giants.}
}
@article{COWLING202143,
title = {Logistic regression and machine learning predicted patient mortality from large sets of diagnosis codes comparably},
journal = {Journal of Clinical Epidemiology},
volume = {133},
pages = {43-52},
year = {2021},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2020.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0895435620312221},
author = {Thomas E. Cowling and David A. Cromwell and Alexis Bellot and Linda D. Sharples and Jan {van der Meulen}},
keywords = {Machine learning, Regression analysis, Big data, Electronic health records, International Classification of Diseases, Comorbidity, Prognosis},
abstract = {Objective
The objective of the study was to compare the performance of logistic regression and boosted trees for predicting patient mortality from large sets of diagnosis codes in electronic healthcare records.
Study Design and Setting
We analyzed national hospital records and official death records for patients with myocardial infarction (n = 200,119), hip fracture (n = 169,646), or colorectal cancer surgery (n = 56,515) in England in 2015–2017. One-year mortality was predicted from patient age, sex, and socioeconomic status, and 202 to 257 International Classification of Diseases 10th Revision codes recorded in the preceding year or not (binary predictors). Performance measures included the c-statistic, scaled Brier score, and several measures of calibration.
Results
One-year mortality was 17.2% (34,520) after myocardial infarction, 27.2% (46,115) after hip fracture, and 9.3% (5,273) after colorectal surgery. Optimism-adjusted c-statistics for the logistic regression models were 0.884 (95% confidence interval [CI]: 0.882, 0.886), 0.798 (0.796, 0.800), and 0.811 (0.805, 0.817). The equivalent c-statistics for the boosted tree models were 0.891 (95% CI: 0.889, 0.892), 0.804 (0.802, 0.806), and 0.803 (0.797, 0.809). Model performance was also similar when measured using scaled Brier scores. All models were well calibrated overall.
Conclusion
In large datasets of electronic healthcare records, logistic regression and boosted tree models of numerous diagnosis codes predicted patient mortality comparably.}
}
@article{LI2021102231,
title = {Towards high-throughput microstructure simulation in compositionally complex alloys via machine learning},
journal = {Calphad},
volume = {72},
pages = {102231},
year = {2021},
issn = {0364-5916},
doi = {https://doi.org/10.1016/j.calphad.2020.102231},
url = {https://www.sciencedirect.com/science/article/pii/S0364591620304946},
author = {Yue Li and Bjørn Holmedal and Boyu Liu and Hongxiang Li and Linzhong Zhuang and Jishan Zhang and Qiang Du and Jianxin Xie},
keywords = {Materials informatics, Machine learning, High-throughput computing, Microstructure simulation, Tabulation},
abstract = {The coupling of computational thermodynamics and kinetics has been the central research theme in Integrated Computational Material Engineering (ICME). Two major bottlenecks in implementing this coupling and performing efficient ICME-guided high-throughput multi-component industrial alloys discovery or process parameters optimization, are slow responses in kinetic calculations to a given set of compositions and processing conditions and the quality of a large amount of calculated thermodynamic data. Here, we employ machine learning techniques to eliminate them, including (1) intelligent corrupt data detection and re-interpolation (i.e. data purge/cleaning) to a big tabulated thermodynamic dataset based on an unsupervised learning algorithm and (2) parameterization via artificial neural networks of the purged big thermodynamic dataset into a non-linear equation consisting of base functions and parameterization coefficients. The two techniques enable the efficient linkage of high-quality data with a previously developed microstructure model. This proposed approach not only improves the model performance by eliminating the interference of the corrupt data and stability due to the boundedness and continuity of the obtained non-linear equation but also dramatically reduces the running time and demand for computer physical memory simultaneously. The high computational robustness, efficiency, and accuracy, which are prerequisites for high-throughput computing, are verified by a series of case studies on multi-component aluminum, steel, and high-entropy alloys. The proposed data purge and parameterization methods are expected to apply to various microstructure simulation approaches or to bridging the multi-scale simulation where handling a large amount of input data is required. It is concluded that machine learning is a valuable tool in fueling the development of ICME and high throughput materials simulations.}
}
@article{TANG20211274,
title = {Comparison of machine learning methods for ground settlement prediction with different tunneling datasets},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {13},
number = {6},
pages = {1274-1289},
year = {2021},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2021.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1674775521001347},
author = {Libin Tang and SeonHong Na},
keywords = {Surface settlement, Tunnel construction, Machine learning (ML), Hyperparameter optimization, Cross-validation (CV)},
abstract = {This study integrates different machine learning (ML) methods and 5-fold cross-validation (CV) method to estimate the ground maximal surface settlement (MSS) induced by tunneling. We further investigate the applicability of artificial intelligent (AI) based prediction through a comparative study of two tunnelling datasets with different sizes and features. Four different ML approaches, including support vector machine (SVM), random forest (RF), back-propagation neural network (BPNN), and deep neural network (DNN), are utilized. Two techniques, i.e. particle swarm optimization (PSO) and grid search (GS) methods, are adopted for hyperparameter optimization. To assess the reliability and efficiency of the predictions, three performance evaluation indicators, including the mean absolute error (MAE), root mean square error (RMSE), and Pearson correlation coefficient (R), are calculated. Our results indicate that proposed models can accurately and efficiently predict the settlement, while the RF model outperforms the other three methods on both datasets. The difference in model performance on two datasets (Datasets A and B) reveals the importance of data quality and quantity. Sensitivity analysis indicates that Dataset A is more significantly affected by geological conditions, while geometric characteristics play a more dominant role on Dataset B.}
}
@article{SPIESKE2021107452,
title = {Improving supply chain resilience through industry 4.0: A systematic literature review under the impressions of the COVID-19 pandemic},
journal = {Computers & Industrial Engineering},
volume = {158},
pages = {107452},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107452},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221003569},
author = {Alexander Spieske and Hendrik Birkel},
keywords = {Industry 4.0, Supply chain risk management, Supply chain resilience, Supply chain disruption, Digital supply chain, Literature review},
abstract = {The COVID-19 pandemic is one of the most severe supply chain disruptions in history and has challenged practitioners and scholars to improve the resilience of supply chains. Recent technological progress, especially industry 4.0, indicates promising possibilities to mitigate supply chain risks such as the COVID-19 pandemic. However, the literature lacks a comprehensive analysis of the link between industry 4.0 and supply chain resilience. To close this research gap, we present evidence from a systematic literature review, including 62 papers from high-quality journals. Based on a categorization of industry 4.0 enabler technologies and supply chain resilience antecedents, we introduce a holistic framework depicting the relationship between both areas while exploring the current state-of-the-art. To verify industry 4.0’s resilience opportunities in a severe supply chain disruption, we apply our framework to a use case, the COVID-19-affected automotive industry. Overall, our results reveal that big data analytics is particularly suitable for improving supply chain resilience, while other industry 4.0 enabler technologies, including additive manufacturing and cyber-physical systems, still lack proof of effectiveness. Moreover, we demonstrate that visibility and velocity are the resilience antecedents that benefit most from industry 4.0 implementation. We also establish that industry 4.0 holistically supports pre-disruption resilience measures, enabling more effective proactive risk management. Both research and practice can benefit from this study. While scholars may analyze resilience potentials of under-explored enabler technologies, practitioners can use our findings to guide industry 4.0 investment decisions.}
}
@article{XIONG2021107518,
title = {Assessment of spatial–temporal changes of ecological environment quality based on RSEI and GEE: A case study in Erhai Lake Basin, Yunnan province, China},
journal = {Ecological Indicators},
volume = {125},
pages = {107518},
year = {2021},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2021.107518},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X21001837},
author = {Yuan Xiong and Weiheng Xu and Ning Lu and Shaodong Huang and Chao Wu and Leiguang Wang and Fei Dai and Weili Kou},
keywords = {Spatial-temporal changes, Erhai Lake Basin, Ecological environment quality, Spatial auto-correlation analysis, Google Earth Engine},
abstract = {The Erhai Lake Basin is an area with the active economic and social development of agriculture and tourism, facing increasingly prominent environmental problems with rapid urbanization. Assessing spatial–temporal changes in ecological environment quality objectively and quantitatively in a timely fashion is crucial for environmental protection and policymaking. First, we selected the high-quality Landsat imagery acquired at the same time phase in the years of 1999, 2004, 2009, 2014, and 2019 respectively. Second, the remote sensing-based ecological index (RSEI) was constructed by using Landsat 5 TM and Landsat 8 OLI/TIRS imagery based on Google Earth Engine (GEE) platform. Third, we assessed the spatial–temporal changes and spatial autocorrelation of ecological environment quality in our study area based on five RSEI maps. The mean values of RSEI in 1999, 2004, 2009, 2014, and 2019 were 0.513 0.457, 0.462, 0.506, and 0.509, respectively, which indicated that the overall ecological environment quality of the Erhai Lake Basin degraded from 1999 to 2009 and promoted from 2009 to 2019. The worst degradation occurred between 1999 and 2004, about 27.12% of the total area was degraded, and the greatest improvement occurred between 2009 and 2014, about 26.42% of the total area was improved. The Globalmoran's I value ranged from 0.662 to 0.783 in 1999–2019, which indicated that the spatial distribution of ecological environment quality was positively correlated. The cluster map of local indicator of spatial association of RSEI show that the high-high points were mainly located in the western and southern high-altitude area of the study area, and the low-low points were mainly distributed in lakeside area, where populations were dense and human activities were frequent. This study provides a promising approach to assess the spatial–temporal changes in ecological environment quality based on RSEI and GEE, which is critical to investigate the interactions between human activities and ecosystem services in basin systems.}
}
@article{NAZARI2021101419,
title = {Machine learning approaches for classification of colorectal cancer with and without feature selection method on microarray data},
journal = {Gene Reports},
volume = {25},
pages = {101419},
year = {2021},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2021.101419},
url = {https://www.sciencedirect.com/science/article/pii/S2452014421004039},
author = {Elham Nazari and Mehran Aghemiri and Amir Avan and Amin Mehrabian and Hamed Tabesh},
keywords = {Colon cancer, Machine learning, Ensemble, Deep learning, Diagnosis, Classification, AdaBoost classifier, XGBclassifier, LightGBM, Random forest classifier, Big data, Advanced method, Feature, Microarray data, Gene, Early cancer diagnosis},
abstract = {To differentiate cancer cells from healthy cells, cancer research is focused on machine learning techniques because the high-accuracy classification provides the basis for early diagnosis and will lead to the appropriate treatment methods selection and costs reduction. Particularly in colon cancer, which is the most life-threatening cancer after lung cancer, early diagnosis can be extremely effective. In this paper, the lightGBM based Relief attribute evaluation and DNN based Relief attribute evaluation algorithm were proposed to differentiate between non-cancer cells and cancer cells in colon cancer compared to 20 other machine learning techniques. The data that was used involved 111 patients' microarray data including 22,278 features. Python and Weka software was used to implement the methods. Accuracy, precision, MCC, recall, and AUC indices were used for evaluation. Among the methods of the decision tree, decision stump, LMT, Naïve Bayes, bagging, AdaBoost, QDA, DNN, etc. the proposed methods with 100% accuracy performed the classification work. The results of comparing the methods would prove that the proposed method can be an effective way to separate cells, particularly for this type of data, which has a high diversity. Accordingly, the development of state-of-the-art methods using existing models should focus on the current research field to be useful in research areas related to diseases, especially cancers, and to have an effective performance in the benchmark.}
}
@article{LAMY2021102074,
title = {A data science approach to drug safety: Semantic and visual mining of adverse drug events from clinical trials of pain treatments},
journal = {Artificial Intelligence in Medicine},
volume = {115},
pages = {102074},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102074},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721000671},
author = {Jean-Baptiste Lamy},
keywords = {Data mining, Ontology, Visual analytics, Glyph, Drug safety, Adverse drug events, Pain treatments, Painkillers},
abstract = {Clinical trials are the basis of Evidence-Based Medicine. Trial results are reviewed by experts and consensus panels for producing meta-analyses and clinical practice guidelines. However, reviewing these results is a long and tedious task, hence the meta-analyses and guidelines are not updated each time a new trial is published. Moreover, the independence of experts may be difficult to appraise. On the contrary, in many other domains, including medical risk analysis, the advent of data science, big data and visual analytics allowed moving from expert-based to fact-based knowledge. Since 12 years, many trial results are publicly available online in trial registries. Nevertheless, data science methods have not yet been applied widely to trial data. In this paper, we present a platform for analyzing the safety events reported during clinical trials and published in trial registries. This platform is based on an ontological model including 582 trials on pain treatments, and uses semantic web technologies for querying this dataset at various levels of granularity. It also relies on a 26-dimensional flower glyph for the visualization of the Adverse Drug Events (ADE) rates in 13 categories and 2 levels of seriousness. We illustrate the interest of this platform through several use cases and we were able to find back conclusions that were initially found during meta-analyses. The platform was presented to four experts in drug safety, and is publicly available online, with the ontology of pain treatment ADE.}
}
@article{KAPLAN20212255,
title = {Artificial Intelligence/Machine Learning in Respiratory Medicine and Potential Role in Asthma and COPD Diagnosis},
journal = {The Journal of Allergy and Clinical Immunology: In Practice},
volume = {9},
number = {6},
pages = {2255-2261},
year = {2021},
issn = {2213-2198},
doi = {https://doi.org/10.1016/j.jaip.2021.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S221321982100194X},
author = {Alan Kaplan and Hui Cao and J. Mark FitzGerald and Nick Iannotti and Eric Yang and Janwillem W.H. Kocks and Konstantinos Kostikas and David Price and Helen K. Reddel and Ioanna Tsiligianni and Claus F. Vogelmeier and Pascal Pfister and Paul Mastoridis},
keywords = {Asthma, Artificial intelligence, COPD, Diagnosis, Machine learning, Respiratory disease},
abstract = {Artificial intelligence (AI) and machine learning, a subset of AI, are increasingly used in medicine. AI excels at performing well-defined tasks, such as image recognition; for example, classifying skin biopsy lesions, determining diabetic retinopathy severity, and detecting brain tumors. This article provides an overview of the use of AI in medicine and particularly in respiratory medicine, where it is used to evaluate lung cancer images, diagnose fibrotic lung disease, and more recently is being developed to aid the interpretation of pulmonary function tests and the diagnosis of a range of obstructive and restrictive lung diseases. The development and validation of AI algorithms requires large volumes of well-structured data, and the algorithms must work with variable levels of data quality. It is important that clinicians understand how AI can function in the context of heterogeneous conditions such as asthma and chronic obstructive pulmonary disease where diagnostic criteria overlap, how AI use fits into everyday clinical practice, and how issues of patient safety should be addressed. AI has a clear role in providing support for doctors in the clinical workplace, but its relatively recent introduction means that confidence in its use still has to be fully established. Overall, AI is expected to play a key role in aiding clinicians in the diagnosis and management of respiratory diseases in the future, and it will be exciting to see the benefits that arise for patients and doctors from its use in everyday clinical practice.}
}
@article{HUO2021102114,
title = {Commercial hypervisor-based task sandboxing mechanisms are unsecured? But we can fix it!},
journal = {Journal of Systems Architecture},
volume = {116},
pages = {102114},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102114},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121000874},
author = {Dongdong Huo and Chen Cao and Peng Liu and Yazhe Wang and Mingxuan Li and Zhen Xu},
keywords = {Cyber–Physical–Social Systems, Internet of Things devices, RTOS task sandboxing, Intra-Mode Privilege Separation},
abstract = {Cyber–Physical–Social Systems are frequently prescribed for providing valuable information on personalized services. The foundation of these services is big data which must be trustily collected and efficiently processed. Though High Performance Computing and Communication technique makes great contributions to addressing the issue of data processing, its effectiveness still relies on the veracity of data generated from Internet of Things (IoT) devices. Nevertheless, IoT devices, as basic production facilities to ensure data’s security, are unable to deploy expensive security extensions. Consequently, it causes the implementation of the task sandboxing, the fundamental security mechanism in Real-Time Operating Systems (RTOSs), much simpler and more vulnerable. In this paper, we take ARM Mbed uVisor as an example system, utilizing hypervisor-based task sandboxing mechanisms, and presents three new findings: First, we discover vulnerabilities against Mbed task sandboxing, which can be exploited to compromise system-maintained data structure to manipulate any tasks’ data. Second, we present LIPS (Lightweight Intra-Mode Privilege Separation), building a special protection domain to isolate particular system-maintained data structures. Finally, thorough evaluation and experimental tests show the efficiency of LIPS to defeat these attacks, with small runtime overheads and good portability.}
}
@article{MA2021103858,
title = {Deep learning for geological hazards analysis: Data, models, applications, and opportunities},
journal = {Earth-Science Reviews},
volume = {223},
pages = {103858},
year = {2021},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2021.103858},
url = {https://www.sciencedirect.com/science/article/pii/S0012825221003597},
author = {Zhengjing Ma and Gang Mei},
keywords = {Geological hazards, Earth observation data, Deep learning, Landslide detection, Seismic phase picking},
abstract = {As natural disasters are induced by geodynamic activities or abnormal changes in the environment, geological hazards tend to wreak havoc on the environment and human society. Recently, the dramatic increase in the volume of various types of Earth observation ‘big data’ from multiple sources, and the rapid development of deep learning as a state-of-the-art data analysis tool, have enabled novel advances in geological hazard analysis, with the ultimate aim to mitigate the devastation associated with these hazards. Motivated by numerous applications, this paper presents an overview of the advances in the utilization of deep learning for geological hazard analysis. First, six commonly available Earth observation data sources are described, e.g., unmanned aerial vehicles, satellite platforms, and in-situ monitoring systems. Second, the deep learning background and six typical deep learning models are introduced, such as convolutional neural networks and recurrent neural networks. Third, focusing on six typical geological hazards, i.e., landslides, debris flows, rockfalls, avalanches, earthquakes, and volcanoes, the deep learning applications for geological hazard analysis are reviewed, and common application paradigms are summarized. Finally, the challenges and opportunities for the application of deep learning models for geological hazard analysis are highlighted, with the aim to inspire further related research.}
}
@article{ASCENCIOVASQUEZ2021357,
title = {Typical Daily Profiles, a novel approach for photovoltaics performance assessment: Case study on large-scale systems in Chile},
journal = {Solar Energy},
volume = {225},
pages = {357-374},
year = {2021},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2021.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X21005764},
author = {Julián Ascencio-Vásquez and Juan Carlos Osorio-Aravena and Kristijan Brecl and Emilio Muñoz-Cerón and Marko Topič},
keywords = {Photovoltaic, Performance, PV system, Typical Daily Profiles, KPIs},
abstract = {A growing photovoltaic industry shows exponential deployment worldwide, and it is expected to largely contribute to the energy transition. Because PV technologies will play a major role in achieving global sustainable development and climate goals, driving more energy-efficient scenarios will require efficient approaches to evaluate systems, especially when dealing with big data of a large region or portfolio. In this work, a novel approach for the PV performance assessment of photovoltaic systems, called “Typical Daily Profiles” (TDP), is presented. This approach is tested on the entire PV fleet operating in Chile from 2014 to 2019. The TDP approach can help to calculate key performance indicators, identify the mounting configuration of PV systems, and to detect major technical issues. A detailed validation carried on the Chilean PV fleet confirms the TDP approach's capabilities to provide accurate PV performance results, neglecting external factors such as failures, grid curtailment, or poor operation activities. Chile was chosen due to the large variety of climate zones in its unique geography, which helps to understand the PV performance under different environmental conditions. Besides, this study reveals the immense potential of PV technologies in Chile compared to mature PV markets worldwide. On an annual basis, their unit capacity factors can reach up to 38%, performance ratios above 90%, and the highest energy yield close to 3350 kWh/kWp. Proving to be an accurate tool, the Typical Daily Profiles approach can be easily applied to other PV portfolios in different regions, and a periodical execution could help to identify and understand long-term performance losses.}
}
@article{LI2021106558,
title = {Towards automated greenhouse: A state of the art review on greenhouse monitoring methods and technologies based on internet of things},
journal = {Computers and Electronics in Agriculture},
volume = {191},
pages = {106558},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106558},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921005755},
author = {Haixia Li and Yu Guo and Huajian Zhao and Yang Wang and David Chow},
keywords = {Agriculture greenhouse building, Environment, Energy saving, Internet of Things (IoT), Monitoring system},
abstract = {As a controllable environment, greenhouse has less resource consumption and emission than field crop production and reduced greenhouse gas emissions from agricultural production. Besides, the greenhouse with an intelligent monitoring system has better energy-saving and emission reduction effects. Simultaneously, the intelligent monitoring system can predict the extreme greenhouse environment in advance, reduce diseases and insect pests, reduce the use of pesticides and fertilizers, and provide high-quality food. Researchers are becoming more and more interested in greenhouse monitoring systems, and how to put them into production correctly and effectively is a major challenge. This paper aims to review the intelligent greenhouse monitoring system systematically, serve the data transmission and server processing subsystems by identifying, listing and further explaining the greenhouse environmental parameters and studying the overall design of the greenhouse monitoring system. According to the characteristics of each component of the system, the paper makes a comparative study and obtains its development trend, summarizes the current popular technology and the future development trend of the intelligent monitoring system, and provides support for the research of greenhouse monitoring system. It was found that multi-parameter monitoring is beneficial to achieve effective greenhouse control, and wireless technology has gradually replaced wired mode for data transmission in the environment both inside and outside the greenhouse. Notably, deep learning, big data, and other advanced technologies used in greenhouse monitoring are considered valuable developments, further refine unmanned greenhouse management, and further improve greenhouse construction's energy utilization.}
}
@article{XI2021147706,
title = {The responses of weathering carbon sink to eco-hydrological processes in global rocks},
journal = {Science of The Total Environment},
volume = {788},
pages = {147706},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.147706},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721027777},
author = {Huipeng Xi and Shijie Wang and Xiaoyong Bai and Hong Tang and Guangjie Luo and Huiwen Li and Luhua Wu and Chaojun Li and Huan Chen and Chen Ran and Xuling Luo},
keywords = {Rock chemical weathering, Eco-hydrology, Global change, Big data},
abstract = {Eco-hydrological processes affect the chemical weathering carbon sink (CS) of rocks. However, due to data quality limitations, the magnitude of the CS of rocks and their responses to eco-hydrological processes are not accurately understood. Therefore, based on Global Erosion Model for CO2 fluxes (GEM-CO2 model), hydrological site data, and multi-source remote sensing data, we produced a 0.05° × 0.05° resolution dataset of CS for 11 types of rocks from 2001 to 2018. The results show that the total amount of CS of global rocks is 0.32 ± 0.02 Pg C, with an average flux of 2.7 t C km−2 yr−1, accounting for 53% and 3% of the “missing” carbon sink and fossil fuel emissions, respectively. This is 23% higher than previous research results, which may be due to the increased resolution. Although about 60% of the CS of global rocks are in a stable state, there are obvious differences among rocks. For example, the CS of carbonate rocks exhibited a significant increase (0.30 Tg C/yr), while the CS of siliceous clastic sedimentary rocks exhibited a significant decrease (−0.06 Tg C/yr). Although temperature is an important factor affecting the CS, the proportion of soil moisture in arid and temperate climate zones is higher (accounting for 24%), which is 3.6 times that of temperature. Simulations based on representative concentration pathways scenarios indicate that the global CS of rocks may increase by about 28% from 2050 to 2100. In short, we produced a set of high-resolution datasets for the CS of global rocks, which makes up for the lack of datasets in previous studies and improves our understanding of the magnitude and spatial pattern of the CS and its responses to eco-hydrological processes.}
}
@article{ANOWAR2021100378,
title = {Conceptual and empirical comparison of dimensionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE)},
journal = {Computer Science Review},
volume = {40},
pages = {100378},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100378},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000186},
author = {Farzana Anowar and Samira Sadaoui and Bassant Selim},
keywords = {Dimension reduction, Optimal set of features, Data quality, High-dimensional datasets, Correlation metrics, Classification accuracy, Run-time},
abstract = {Feature Extraction Algorithms (FEAs) aim to address the curse of dimensionality that makes machine learning algorithms incompetent. Our study conceptually and empirically explores the most representative FEAs. First, we review the theoretical background of many FEAs from different categories (linear vs. nonlinear, supervised vs. unsupervised, random projection-based vs. manifold-based), present their algorithms, and conduct a conceptual comparison of these methods. Secondly, for three challenging binary and multi-class datasets, we determine the optimal sets of new features and assess the quality of the various transformed feature spaces in terms of statistical significance and power analysis, and the FEA efficacy in terms of classification accuracy and speed.}
}
@article{GU2021106614,
title = {A classification framework for multivariate compositional data with Dirichlet feature embedding},
journal = {Knowledge-Based Systems},
volume = {212},
pages = {106614},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106614},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120307437},
author = {Jie Gu and Bin Cui and Shan Lu},
keywords = {Multivariate compositional data, Classification, Feature embedding, Dirichlet distribution, Support vector machine},
abstract = {Compositional data which contain relative or structure information of a whole occur commonly in many disciplines and practical scenarios. Yet relatively few works are available for multivariate compositional data classification with different numbers of parts using machine learning. This is because compositional data is inherently constrained to unit sum, resulting in the existing methods cannot be directly applied. Particularly, the multivariate analysis methods for compositional data variables with unequal sizes of parts are not sufficiently investigated. Moreover, to design a good classification model is indeed a complicated work. Except for the learning algorithm, data quality is also an essential determinant, which is rarely been concerned. In this paper, we propose an effective framework for multivariate compositional data classification. Specifically, the Dirichlet feature embedding is proposed to implement on the original compositional data features with the goal of removing the constraint and obtaining high quality training data, as well as reducing the dimension. Support vector machine is then used to build the classification model. Results of simulation study and real-world dataset show our proposed method can achieve good performances.}
}
@article{YU20211469,
title = {Insight into chemical basis of traditional Chinese medicine based on the state-of-the-art techniques of liquid chromatography−mass spectrometry},
journal = {Acta Pharmaceutica Sinica B},
volume = {11},
number = {6},
pages = {1469-1492},
year = {2021},
issn = {2211-3835},
doi = {https://doi.org/10.1016/j.apsb.2021.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2211383521000654},
author = {Yang Yu and Changliang Yao and De-an Guo},
keywords = {Liquid chromatography−mass spectrometry, Qualitative analysis, Traditional Chinese medicine, Data acquisition, Data post-processing},
abstract = {Traditional Chinese medicine (TCM) has been an indispensable source of drugs for curing various human diseases. However, the inherent chemical diversity and complexity of TCM restricted the safety and efficacy of its usage. Over the past few decades, the combination of liquid chromatography with mass spectrometry has contributed greatly to the TCM qualitative analysis. And novel approaches have been continuously introduced to improve the analytical performance, including both the data acquisition methods to generate a large and informative dataset, and the data post-processing tools to extract the structure-related MS information. Furthermore, the fast-developing computer techniques and big data analytics have markedly enriched the data processing tools, bringing benefits of high efficiency and accuracy. To provide an up-to-date review of the latest techniques on the TCM qualitative analysis, multiple data-independent acquisition methods and data-dependent acquisition methods (precursor ion list, dynamic exclusion, mass tag, precursor ion scan, neutral loss scan, and multiple reaction monitoring) and post-processing techniques (mass defect filtering, diagnostic ion filtering, neutral loss filtering, mass spectral trees similarity filter, molecular networking, statistical analysis, database matching, etc.) were summarized and categorized. Applications of each technique and integrated analytical strategies were highlighted, discussion and future perspectives were proposed as well.}
}
@article{UPADHYA20211,
title = {Automation and data-driven design of polymer therapeutics},
journal = {Advanced Drug Delivery Reviews},
volume = {171},
pages = {1-28},
year = {2021},
issn = {0169-409X},
doi = {https://doi.org/10.1016/j.addr.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169409X20302180},
author = {Rahul Upadhya and Shashank Kosuri and Matthew Tamasi and Travis A. Meyer and Supriya Atta and Michael A. Webb and Adam J. Gormley},
keywords = {Polymer chemistry, High throughput screening, Automation, Machine learning, Artificial intelligence, Drug delivery, Gene delivery},
abstract = {Polymers are uniquely suited for drug delivery and biomaterial applications due to tunable structural parameters such as length, composition, architecture, and valency. To facilitate designs, researchers may explore combinatorial libraries in a high throughput fashion to correlate structure to function. However, traditional polymerization reactions including controlled living radical polymerization (CLRP) and ring-opening polymerization (ROP) require inert reaction conditions and extensive expertise to implement. With the advent of air-tolerance and automation, several polymerization techniques are now compatible with well plates and can be carried out at the benchtop, making high throughput synthesis and high throughput screening (HTS) possible. To avoid HTS pitfalls often described as “fishing expeditions,” it is crucial to employ intelligent and big data approaches to maximize experimental efficiency. This is where the disruptive technologies of machine learning (ML) and artificial intelligence (AI) will likely play a role. In fact, ML and AI are already impacting small molecule drug discovery and showing signs of emerging in drug delivery. In this review, we present state-of-the-art research in drug delivery, gene delivery, antimicrobial polymers, and bioactive polymers alongside data-driven developments in drug design and organic synthesis. From this insight, important lessons are revealed for the polymer therapeutics community including the value of a closed loop design-build-test-learn workflow. This is an exciting time as researchers will gain the ability to fully explore the polymer structural landscape and establish quantitative structure-property relationships (QSPRs) with biological significance.}
}
@incollection{GANZINGER2021532,
title = {Biomedical and Clinical Research Data Management},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {532-543},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11621-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383116216},
author = {Matthias Ganzinger and Enrico Glaab and Jules Kerssemakers and Sven Nahnsen and Ulrich Sax and Nadine Sarah Schaadt and Matthieu-P. Schapranow and Thorsten Tiede},
keywords = {Bioinformatics, Data integration, Data management, Data quality, Data sharing, Medical informatics},
abstract = {Systems medicine is an interdisciplinary approach in medicine that relies on computational models based on data from a variety of sources. Typically, such sources include clinical and biomedical data with heterogeneous data definitions that are sometimes not even structured in a useful way. Consequently, the systematic management of data is an important element for the successful implementation of systems medicine in both research and clinical application. In this article, we provide an overview over the following selected aspects of data management:•Integration of multiple data sources•IT infrastructures•Data protection regulations•Data history and data quality•Data sharing/FAIR principles•Use and access policies The presented best practices and experiences result from several systems medicine projects in which the authors have participated. They can be considered as recommendations for future projects in order to quickly set up data management infrastructures for systems medicine.}
}
@article{CLARKE2021100213,
title = {Appyters: Turning Jupyter Notebooks into data-driven web apps},
journal = {Patterns},
volume = {2},
number = {3},
pages = {100213},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000234},
author = {Daniel J.B. Clarke and Minji Jeon and Daniel J. Stein and Nicole Moiseyev and Eryk Kropiwnicki and Charles Dai and Zhuorui Xie and Megan L. Wojciechowicz and Skylar Litz and Jason Hom and John Erol Evangelista and Lucas Goldman and Serena Zhang and Christine Yoon and Tahmid Ahamed and Samantha Bhuiyan and Minxuan Cheng and Julie Karam and Kathleen M. Jagodnik and Ingrid Shu and Alexander Lachmann and Sam Ayling and Sherry L. Jenkins and Avi Ma'ayan},
keywords = {workflow, notebooks, big data, data analysis, data visualization, RNA-seq, scRNA-seq, machine learning, TCGA, gene set enrichment analysis},
abstract = {Summary
Jupyter Notebooks have transformed the communication of data analysis pipelines by facilitating a modular structure that brings together code, markdown text, and interactive visualizations. Here, we extended Jupyter Notebooks to broaden their accessibility with Appyters. Appyters turn Jupyter Notebooks into fully functional standalone web-based bioinformatics applications. Appyters present to users an entry form enabling them to upload their data and set various parameters for a multitude of data analysis workflows. Once the form is filled, the Appyter executes the corresponding notebook in the cloud, producing the output without requiring the user to interact directly with the code. Appyters were used to create many bioinformatics web-based reusable workflows, including applications to build customized machine learning pipelines, analyze omics data, and produce publishable figures. These Appyters are served in the Appyters Catalog at https://appyters.maayanlab.cloud. In summary, Appyters enable the rapid development of interactive web-based bioinformatics applications.}
}
@article{ZHANG2021105762,
title = {Quantifying physical and psychological perceptions of urban scenes using deep learning},
journal = {Land Use Policy},
volume = {111},
pages = {105762},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2021.105762},
url = {https://www.sciencedirect.com/science/article/pii/S0264837721004853},
author = {Yonglin Zhang and Shanlin Li and Rencai Dong and Hongbing Deng and Xiao Fu and Chenxing Wang and Tianshu Yu and Tianxia Jia and Jingzhu Zhao},
keywords = {Complex perceptions, Cityscapes, Image big data, Deep learning, Massive street-view datasets, Human-oriented},
abstract = {The complicated relationship between urban scenes and public perceptions has long been a concern in many disciplines. Previous studies have lacked human-oriented technical paths and high-throughput datasets to quantify physical and psychological perceptions in different land-use scenarios. This paper adopts a novel transfer learning approach to quantify the six types of landsense indices (LSIs) as psychological perception metrics and employs panoptic segmentation to parameterize the view index (VI) and the number of foreground instances (NFIs) as physical perception measures. Then, a quantitative analysis is conducted in Beijing’s six Ring Road areas, and the connections between people’s physical and psychological perceptions of heterogeneous land use are explored. The landsense maps can depict the distribution of LSIs and facilitate the understanding of complex perceptions distributed at a large scale. The regression model shows that natural landscapes (trees, grasses, and mountains) in the Beijing built-up area exhibit an overall positive performance. Moreover, for several block-level land uses, industrial scenery is related to overall negative psychological feelings. Parks and green spaces are positively related to psychological perceptions, because of the greater exposure opportunities to natural landscapes for residents. The framework in this research has potential in assisting urban planning and land-use management, and it enriches the datasets with extensive information, thereby improving the psychological perceptions of urban scenes from residents’ perspectives. The novel approaches in this paper take a step forward in quantifying and understanding the public perceptions of urban landscapes.}
}
@incollection{YALUG2021589,
title = {Chapter 19 - Prospect of data science and artificial intelligence for patient-specific neuroprostheses},
editor = {Burak Güçlü},
booktitle = {Somatosensory Feedback for Neuroprosthetics},
publisher = {Academic Press},
pages = {589-629},
year = {2021},
isbn = {978-0-12-822828-9},
doi = {https://doi.org/10.1016/B978-0-12-822828-9.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228289000058},
author = {Buse Buz Yalug and Dilek Betul Arslan and Esin Ozturk-Isik},
keywords = {Artificial intelligence, machine learning, deep learning, patient-specific neuroprosthesis, big data},
abstract = {Machine learning and its subfield deep learning have recently gained interest in scientific research community due to their ability to analyze and learn from big data. In this chapter, we discuss the capabilities, limitations, and current applications of unsupervised and supervised machine learning methods in addition to more recent deep learning techniques for the design and control of patient-specific neuroprostheses. Furthermore, we speculate on what they could promise for future applications.}
}
@article{SCHNEBLE2021100446,
title = {Driver’s views on driverless vehicles: Public perspectives on defining and using autonomous cars},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {11},
pages = {100446},
year = {2021},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2021.100446},
url = {https://www.sciencedirect.com/science/article/pii/S2590198221001524},
author = {Christophe O. Schneble and David M. Shaw},
keywords = {Smart cars, Definition, Autonomous vehicles, Decision-making, Big data},
abstract = {Objectives
To investigate how members of the public define autonomous vehicles, including perceived advantages, disadvantages and reliability.
Methods
A series of qualitative interviews were conducted with 16 members of the public in Switzerland who were recruited online and through snowballing. Interviews were transcribed and coded, before being subjected to thematic analysis.
Results
Three main themes emerged from the interviews. These included differing perceptions of the level of self-automation of smart cars, across a spectrum from partial automation to fully autonomous vehicles; a variety of views on the perceived pros and cons of smart cars, including fewer accidents and potential loss of freedom; and opinions concerning whether a car would be ‘better than human’, and related issues including whether such cars should or could be subject to human override. Almost all participants saw the introduction of truly smart cars as beneficial, although many said they would not be early adopters and would always want humans to have the ability to take control.
Significance
Our results reveal generally positive attitudes to smart cars, but also some poor understanding of the different levels of automation that such cars can have, and potentially unrealistic expectations regarding the capacity of humans to re-assume control in emergencies. These factors must be considered by legislators and others involved in the introduction of smart cars onto our roads.}
}
@article{MORLOCK2021542,
title = {Concept for Enabling Customer-oriented Data Analytics via Integration of Production Process Improvement Methods and Data Science Methods},
journal = {Procedia CIRP},
volume = {104},
pages = {542-546},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.091},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009896},
author = {Friedrich Morlock and Mario Boßlau},
keywords = {Industry 4.0, Data Analytics, Lean Management, Process Improvement},
abstract = {Production companies are facing the major challenge of digitization. New technical developments enable new optimization potentials in production that can be leveraged in a highly competitive market. Data analysis is a good example of this, allowing large amounts of data to be used to support production in optimization projects. For data analysis the CRISP-DM approach has become generally accepted in science and practice. This process model offers a good support for data science projects with useful data analysis methods and tools. In practice, however, it can often be observed that aspects such as customer-oriented project definition and the integration of process knowledge in data science projects are difficult to achieve. This paper will present a solution for that challenge. The combination of process optimization methods from the Lean Management and Six Sigma toolbox as well as project management methods for each phase of the CRISP-DM approach support data science projects to customize the project definition and integrate process knowledge.}
}
@article{TIAN2021100756,
title = {Estimating cancer survival and prevalence with the Medical-Insurance-System-based Cancer Surveillance System (MIS-CASS): An empirical study in China},
journal = {EClinicalMedicine},
volume = {33},
pages = {100756},
year = {2021},
issn = {2589-5370},
doi = {https://doi.org/10.1016/j.eclinm.2021.100756},
url = {https://www.sciencedirect.com/science/article/pii/S2589537021000365},
author = {Hongrui Tian and Yanjun Hu and Qingxiang Li and Liang Lei and Zhen Liu and Mengfei Liu and Chuanhai Guo and Fangfang Liu and Ying Liu and Yaqi Pan and Isabel dos-Santos-Silva and Zhonghu He and Yang Ke},
keywords = {Cancer surveillance, Survival, Prevalence, Health-related big data, China},
abstract = {Background
We aimed to establish a new approach for surveillance of cancer prevalence and survival in China, based on the Medical-Insurance-System-based Cancer Surveillance System (MIS-CASS).
Methods
We constructed a standard procedure for data collection, cleaning, processing, linkage, verification, analysis, and estimation of cancer prevalence and survival (including both actual observations and model estimates) by conjoint use of medical insurance claims data and all-cause death surveillance data. As a proof-of-principle study, we evaluated the performance of this surveillance approach by estimating the latest prevalence and survival for upper gastrointestinal cancers in Hua County, a high-risk region for oesophageal cancer in China.
Findings
In Hua County, the age-standardised relative 5-year survival was 39·2% (male: 36·8%; female: 43·6%) for oesophageal cancer and 33·3% (male: 29·6%; female: 43·4%) for stomach cancer. For oesophageal cancer, better survival was observed in patients of 45–64 years compared with national average estimates, and women of <75 years had better survival than men. The 5-year prevalence rate in Hua County was 99·8/100,000 (male: 105·9/100,000; female: 93·3/100,000) for oesophageal cancer and 41·5/100,000 (male: 57·4/100,000; female: 24·5/100,000) for stomach cancer. For both of these cancers, the prevalence burden peaked at 65–79 years. The model estimates for survival and prevalence were close to the observations in real investigation, with a relative difference of less than 4·5%.
Interpretation
This novel approach allows accurate estimation of cancer prevalence and survival with a short delay, which has great potential for regular use in general Chinese populations, especially those not covered by cancer registries.
Funding
The National Key R&D Program of China (2016YFC0901404), the National Science & Technology Fundamental Resources Investigation Program of China (2019FY101102), the National Natural Science Foundation of China (82073626), the Taikang Yicai Public Health and Epidemic Control Fund (TKYC-GW-2020), the Beijing-Tianjin-Hebei Basic Research Cooperation Project (J200016), and the Digestive Medical Coordinated Development Center of Beijing Hospitals Authority (XXZ0204).}
}
@incollection{PONNALURI2021129,
title = {Chapter 9 - Data in the context of connected and automated vehicles},
editor = {Raj Ponnaluri and Priyanka Alluri},
booktitle = {Connected and Automated Vehicles},
publisher = {Elsevier},
pages = {129-149},
year = {2021},
isbn = {978-0-12-820567-9},
doi = {https://doi.org/10.1016/B978-0-12-820567-9.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205679000060},
author = {Raj Ponnaluri and Priyanka Alluri},
keywords = {SAE J2735, CAV Data, Data architecture, Data governance, V2X data exchange platform},
abstract = {This chapter discusses several key components of CAV data and a V2X Data Exchange Platform. It begins by discussing SAE J2735 and the commonly used message data sets: Basic Safety Messages, Traveler Information Messages, Signal Request Messages, Signal Status Messages, Signal Phase and Timing Messages, and MAP data. Next, the following six data quality dimensions in the context of CAV are presented: standardization, hygiene, timeliness, completeness, validity, and accuracy and precision. The CAV data architecture, along with cloud services, data lake, and data edge, is then discussed. An overview of data security, governance, and change management is then provided. Finally, a blueprint for the V2X Data Exchange Platform is presented.}
}
@article{WANG2021110929,
title = {Practical issues in implementing machine-learning models for building energy efficiency: Moving beyond obstacles},
journal = {Renewable and Sustainable Energy Reviews},
volume = {143},
pages = {110929},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.110929},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121002227},
author = {Zeyu Wang and Jian Liu and Yuanxin Zhang and Hongping Yuan and Ruixue Zhang and Ravi S. Srinivasan},
keywords = {Machine learning, Building energy modeling, Energy efficiency, Building energy management, Practical issues, Technology diffusion},
abstract = {Implementing machine-learning models in real applications is crucial to achieving intelligent building control and high energy efficiency. Over the past few decades, numerous studies have attempted to explore the application of machine-learning models to building energy efficiency. However, these studies have focused on analyzing the technical feasibility and superiority of machine learning algorithms for fitting building energy-related data and have not considered methods of implementing machine learning technology in building energy efficiency applications. Therefore, this review aims to summarize the current practical issues involved in applying machine-learning models to building energy efficiency by systematically analyzing existing research findings and limitations. The paper first reviews the application status of machine learning-based building energy efficiency research by analyzing the model implementation process and summarizing the main uses of the technology in the overall building energy management life cycle. The paper then elaborates on the causes of, influences on, and potential solutions for practical issues found in the implementation and promotion of machine learning-based building energy efficiency measures. Finally, this paper discusses valuable future machine learning-based building energy efficiency research directions with regard to technology opportunity discovery, data governance, feature engineering, generalizability test, technology diffusion, and knowledge sharing. This paper will provide building researchers and practitioners with a better understanding of the current application statuses of and potential research directions for machine learning models in building energy efficiency.}
}
@article{SHEN2021101204,
title = {Brain structural associations with depression in a large early adolescent sample (the ABCD study®)},
journal = {eClinicalMedicine},
volume = {42},
pages = {101204},
year = {2021},
issn = {2589-5370},
doi = {https://doi.org/10.1016/j.eclinm.2021.101204},
url = {https://www.sciencedirect.com/science/article/pii/S2589537021004855},
author = {Xueyi Shen and Niamh MacSweeney and Stella W.Y. Chan and Miruna C. Barbu and Mark J. Adams and Stephen M. Lawrie and Liana Romaniuk and Andrew M. McIntosh and Heather C. Whalley},
keywords = {Big data, Adolescent depression, Adolescent Brain and Cognitive Development Study, Brain structure},
abstract = {Background
Depression is the leading cause of disability worldwide with > 50% of cases emerging before the age of 25 years. Large-scale neuroimaging studies in depression implicate robust structural brain differences in the disorder. However, most studies have been conducted in adults and therefore, the temporal origins of depression-related imaging features remain largely unknown. This has important implications for understanding aetiology and informing timings of potential intervention.
Methods
Here, we examine associations between brain structure (cortical metrics and white matter microstructural integrity) and depression ratings (from caregiver and child), in a large sample (N = 8634) of early adolescents (9 to 11 years old) from the US-based, Adolescent Brain and Cognitive Development (ABCD) Study®. Data was collected from 2016 to 2018.
Findings
We report significantly decreased global cortical and white matter metrics, and regionally in frontal, limbic and temporal areas in adolescent depression (Cohen's d = -0⋅018 to -0⋅041, β = -0·019 to -0⋅057). Further, we report consistently stronger imaging associations for caregiver-reported compared to child-reported depression ratings. Divergences between reports (caregiver vs child) were found to significantly relate to negative socio-environmental factors (e.g., family conflict, absolute β = 0⋅048 to 0⋅169).
Interpretation
Depression ratings in early adolescence were associated with similar imaging findings to those seen in adult depression samples, suggesting neuroanatomical abnormalities may be present early in the disease course, arguing for the importance of early intervention. Associations between socio-environmental factors and reporter discrepancy warrant further consideration, both in the wider context of the assessment of adolescent psychopathology, and in relation to their role in aetiology.
Funding
Wellcome Trust (References: 104036/Z/14/Z and 220857/Z/20/Z) and the Medical Research Council (MRC, Reference: MC_PC_17209).}
}
@article{SNAPHAAN2021101691,
title = {Utilizing geo-referenced imagery for systematic social observation of neighborhood disorder},
journal = {Computers, Environment and Urban Systems},
volume = {90},
pages = {101691},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101691},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521000983},
author = {Thom Snaphaan and Wim Hardyns},
keywords = {Artificial intelligence, Big data, Convolutional neural networks, Geo-referenced imagery, Neighborhood disorder, Systematic social observation},
abstract = {Research methods in social science take advantage from broader trends such as digitalization and increasing computational power, however, this is an evolving explorative search. The main purpose of this article is to describe the methodological innovations in the collection and processing of geo-referenced imagery for the observation of neighborhood disorder. In this narrative review, attention is paid to advances in both the data sources and the data processing methods used. Neighborhood disorder is traditionally measured by means of survey methods and (systematic) (social) observations, but these methods have specific shortcomings, such as respectively the subjective measurement that does not deliver a valid measure of actual prevalence of disorderly phenomena and the intensive use of resources in terms of time and money. This has repercussions for (the interpretation of) the results based on these data. Today, scholars have innovative data sources and cutting-edge data processing methods at their disposal that can meet (some of) these shortcomings, but which have not yet been fully explored. In this article, the evolutions in the use of geo-referenced imagery for the observation of neighborhood disorder from the last 25 years are described with a focus on the empirical opportunities, and the methodological challenges and prospects. We conclude by outlining the road ahead: promising avenues for future research to exploit the full potential of ‘big primary data’.}
}
@article{FARAHANI2021102936,
title = {The convergence of IoT and distributed ledger technologies (DLT): Opportunities, challenges, and solutions},
journal = {Journal of Network and Computer Applications},
volume = {177},
pages = {102936},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102936},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520303945},
author = {Bahar Farahani and Farshad Firouzi and Markus Luecking},
keywords = {Internet of Things (IoT), Distributed Ledger Technology (DLT), Blockchain, Directed Acyclic Graph (DAG), Healthcare},
abstract = {Digital revolution is characterized by the convergence of technologies — from cloud computing to edge/fog computing, Artificial Intelligence (AI), big data, Intelligent Internet of Things (IoT), and Distributed Ledger Technology (DLT) — that is blurring the lines between physical and digital worlds. In this context, the IoT tsunami, the ubiquitous adoption of intelligent connected devices, and the public embracement of DLT, of which blockchain is a popular example, are increasingly becoming an integral feature of many modern systems, particularly, IoT-based smart and connected healthcare. Although the IoT and DLT/blockchain are two very different technologies and distinct from each other, the fusion of blockchain and IoT technologies is an unprecedented paradigm shift that is expected to disrupt both current and future systems in various fields. Blockchain has exactly what is needed to fix the weaknesses and vulnerabilities of IoT. It solves the security fault line among intelligent IoT where most of the IoT devices are connected to each other through the public trustless environment. Moreover, its distributed peer-to-peer nature can address the shortcomings of client/server models in Cloud-IoT solutions. Although the convergence of IoT and blockchain (Blockchain-IoT) can potentially tackle major shortcomings of today's solutions, its adoption is still in infancy, suffering from various issues and thus there is a necessity to address significant challenges including scalability, consensus algorithms, data privacy, efficiency, availability, storage, interoperability, and standardization, among others. In addition, there is no consensus towards any reference model or best practices that specify how blockchain should be utilized in IoT. This paper aims to present a holistic reference architecture as well as fundamentals, recent advancements, promises, and challenges in order to foster the investigations on cutting-edge research and allowing one to contribute to advancing the convergence of blockchain and IoT.}
}
@article{TIMOTIJEVIC2021405,
title = {Designing a research infrastructure (RI) on food behaviour and health: Balancing user needs, business model, governance mechanisms and technology},
journal = {Trends in Food Science & Technology},
volume = {116},
pages = {405-414},
year = {2021},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2021.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S0924224421004611},
author = {L. Timotijevic and S. Astley and M.J. Bogaardt and T. Bucher and I. Carr and G. Copani and J. {de la Cueva} and T. Eftimov and P. Finglas and S. Hieke and C.E. Hodgkins and B. {Koroušić Seljak} and N. Klepacz and K. Pasch and M. Maringer and B.E. Mikkelsen and A. Normann and K.T. Ofei and K. Poppe and G. Pourabdollahian and M.M. Raats and M. Roe and C. Sadler and T. Selnes and H. {van der Veen} and P. {van’t Veer} and K. Zimmermann},
keywords = {Food nutrition, e-infrastructure, Data platform, Food consumption, Determinants, Food intake, AI, Big data, Omics, Data governance},
abstract = {Background
A better understanding of food-related behaviour and its determinants can be achieved through harmonisation and linking of the various data-sources and knowledge platforms.
Scope
We describe the key decision-making in the development of a prototype of the Determinants and Intake Platform (DI Platform), a data platform that aims to harmonise and link data on consumer food behaviour. It will be part of the Food Nutrition Health Research Infrastructure (FNH-RI) that will facilitate health, social and food sciences.
Approach
The decision-making was based on the evidence of user needs and data characteristics that guided the specification of the key building blocks of the DI Platform. Eight studies were carried out, including consumer online survey; interview studies of key DI Platform stakeholders; desk research and workshops.
Key findings
Consumers were most willing to share data with universities, then industry and government. Trust, risk perception and altruism predicted willingness to share. For most other stakeholders non-proprietary data was most likely to be shared. Lack of data standards, and incentives for sharing were the main barriers for sharing data among the key stakeholders. The value of various data types would hugely increase if linked with other sources. Finding the right balance between optimizing data sharing and minimizing ethical and legal risks was considered a key challenge.
Conclusions
The development of DI Platform is based on careful balancing of the user, technical, business, legal and ethical requirements, following the FAIR principles and the need for financial sustainability, technical flexibility, transparency and multi-layered organisational governance.}
}
@article{DOU2021100017,
title = {Estimates of daily ground-level NO2 concentrations in China based on Random Forest model integrated K-means},
journal = {Advances in Applied Energy},
volume = {2},
pages = {100017},
year = {2021},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2021.100017},
url = {https://www.sciencedirect.com/science/article/pii/S266679242100010X},
author = {Xinyu Dou and Cuijuan Liao and Hengqi Wang and Ying Huang and Ying Tu and Xiaomeng Huang and Yiran Peng and Biqing Zhu and Jianguang Tan and Zhu Deng and Nana Wu and Taochun Sun and Piyu Ke and Zhu Liu},
keywords = {Ground-level NO concentration, China, Random Forest model, Multi-source big data, Socio-economic parameters},
abstract = {Nitrogen dioxide (NO2) is one of the most important atmospheric pollutants and the precursors of acid rain, tropospheric ozone, and atmospheric aerosols. However, due to the poor quality of source data and the computing power of the models, current ground-level NO2 concentration data lack either high-resolution coverage or full nation-wide coverage. This study estimates the ground-level NO2 concentration in China with national coverage at relatively high spatiotemporal resolution (0.25°; daily intervals) over the newest past 6 years (2013–2018). We developed an advanced model, named Random Forest model integrated K-means (RF-K), for the estimates with multi-source parameters. Besides meteorological parameters, satellite retrievals parameters, and anthropogenic emission inventories parameters, we also innovatively introduce socioeconomic parameters to assess the impact of human activities. Our results show that: (1) the RF-K model developed by us shows better prediction performance than others. (2) the annual average NO2 concentration of China showed a weak declining trend (-0.013±0.217 μgm−3yr−1) from 2013 to 2018, indicating that pollutant controlling targets had been achieved in China overall. By mapping daily nationwide ground-level NO2 concentrations, this study provides high-quality timely, and detailed data for air quality management and epidemiological analyses for China. The RF-K model can be used easily for other pollutants (e.g. SO2 and O3) considering that their ground-level concentrations can be estimated depending on the similar emitting sources and influence factors, and our model's input data sources also cover information on other pollutants.}
}
@article{SOPAOGLU2021107743,
title = {Classification utility aware data stream anonymization},
journal = {Applied Soft Computing},
volume = {110},
pages = {107743},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107743},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621006645},
author = {Ugur Sopaoglu and Osman Abul},
keywords = {Data streams, Data anonymization, Data privacy, Classification},
abstract = {Data streams are continuous, infinite and ordered sequences of data. In comparison to static dataset anonymization, data stream anonymization confront with a number of constraints and difficulties due to the dynamic nature of data flow. The literature already addressed the k-anonymization of data streams which contain quasi-identifier attributes. However, today most data streams contain sensitive and classification target attributes as well. This work’s main motivation is to develop a k-anonymization method for data streams which additionally protects the sensitivity and enables effective classification models. The k-anonymization, as a result, is formulated as a weighted multi-objective optimization problem. There are three objectives with respective weights as user parameters. A clustering based k-anonymization algorithm is developed as the solution. An extensive experimental evaluation on three real datasets shows the effectiveness of our proposal in various configurations. Moreover, the experimental results also confirm that our proposal attains better classification accuracies in comparison to popular data stream anonymization techniques.}
}
@article{MELENDEZ2021100335,
title = {Modelling traffic during Lilac Wildfire evacuation using cellular data},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {9},
pages = {100335},
year = {2021},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2021.100335},
url = {https://www.sciencedirect.com/science/article/pii/S2590198221000427},
author = {Benjamin Melendez and Sahar {Ghanipoor Machiani} and Atsushi Nara},
keywords = {Wildfire, Evacuation, Traffic, Cellular data},
abstract = {Southern California is prone to wildfire events that spark major evacuations of communities in the Wildland-Urban Interface. Highly developed regions such as Southern California have a number of transportation data sources to draw from that can support emergency managers’ decision-making processes. Up to date traffic sensors such as those found on the majority of California’s highways can inform emergency managers on current traffic densities, flows and speeds. Yet, in many wildfire prone regions of the United States, this is not the case. Despite this data shortfall, many regions do have robust cellular networks that inherently produce substantial amounts of location data. The location data produced by cellphone users can be used to predict vehicular densities on evacuation routes. This study examines how cellular data can be used to predict vehicular densities on evacuation routes. A mathematical model was developed to aid in the prediction of vehicular densities on evacuation networks. Correction factors were produced to adjust for the overestimation of users on roadways by cellular networks. Extrapolation factors were also developed for estimation of the number of cellular users based on a single cellphone counts data point. The Lilac Wildfire data in Dec 2017, was used to test and validate the developed model. This methodology may prove useful to transportation planners and emergency managers in planning evacuations in areas not served by a network of traffic sensors.}
}
@article{KHAN2021107230,
title = {Missing value imputation through shorter interval selection driven by Fuzzy C-Means clustering},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107230},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107230},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002172},
author = {Hufsa Khan and Xizhao Wang and Han Liu},
keywords = {Incomplete data processing, Missing value handling, Missing value imputation, Fuzzy C-Means clustering},
abstract = {The presence of missing data is a common and pivotal issue, which generally leads to a serious decrease of data quality and thus indicates the necessity to effectively handle missing data. In this paper, we propose a missing value imputation approach driven by Fuzzy C-Mean clustering to improve the classification accuracy by referring only to the known feature values of some selected instances. In particular, the missing values for each instance are imputed by selecting a shorter interval based on the cluster membership value within the certain threshold limit of each feature, while using a short interval is considered to improve the imputation effectiveness and get more accurate estimation of the values in comparison with using a long interval. Our method is evaluated through comparing with state-of-the-art imputation methods on UCI datasets. The experimental results demonstrate that the proposed approach performs closely to or better than those state-of-the-art imputation methods.}
}
@article{WANGEN2021104602,
title = {Improving farm decisions: The application of data engineering techniques to manage data streams from contemporary dairy operations},
journal = {Livestock Science},
volume = {250},
pages = {104602},
year = {2021},
issn = {1871-1413},
doi = {https://doi.org/10.1016/j.livsci.2021.104602},
url = {https://www.sciencedirect.com/science/article/pii/S1871141321002109},
author = {Steven R. Wangen and Fan Zhang and Liliana Fadul-Pacheco and Tadeu Eder {da Silva} and Victor E. Cabrera},
keywords = {Dairy brain, Data integration, Agricultural data hub, Big data},
abstract = {Modern dairy farms generate vast amounts of data, with different sections of the operation having the ability to produce its own uniquely structured data stream depending on the specific hardware and software used. As a result of this heterogeneity, these streams are difficult to link to each other, thus it is rarely done. This creates an opportunity to add value to the data by integrating and homogenizing data from the different sources, with the end result of enriching analyses and helping to improve farm management decisions. Within a proposed project, a two component modular system is being developed. One component collects, cleans, and integrates data from on-farm systems into a centralized hub (AgDH). This system provides data to a framework designed to deploy and operationalize existing research-derived analytical tools and provide access to these tools and data via a user interface. The AgDH follows five steps to ingest different data streams available at the dairy farm: 1) transporting raw data into a centralized system; 2) decoding and storing data in a database; 3) cleaning data to ensure its validity; 4) homogenization of data by extracting the common features among the different software and farms; and 5) integration of data from the different systems. Each of these steps is crucial to make data available from different sources in a consistent manner, ease algorithmic development and its implementation, and facilitate the deployment of new tools that utilize the integrated data. In order to automate the process and make the data continuously available an open source workflow management platform was used. Both historical and current data can be made available to authenticated users via an application programming interface hosted through a web service. This framework needs to be designated to be flexible and able to adapt quickly to the changes and new technologies that are continuously being developed in the dairy industry. The integration and accessibility of data can facilitate a wide range of descriptive, predictive, and prescriptive analytics that can be developed and deployed directly on farms to increase animal performance, efficiency, health and welfare, profit margins, and decrease the environmental impact of dairy farming.}
}
@article{JARRETT2021100502,
title = {Exploring and interrogating astrophysical data in virtual reality},
journal = {Astronomy and Computing},
volume = {37},
pages = {100502},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2021.100502},
url = {https://www.sciencedirect.com/science/article/pii/S2213133721000561},
author = {T.H. Jarrett and A. Comrie and L. Marchetti and A. Sivitilli and S. Macfarlane and F. Vitello and U. Becciani and A.R. Taylor and J.M. {van der Hulst} and P. Serra and N. Katz and M.E. Cluver},
keywords = {Virtual reality, Data visualisation, Radio astrophysics, 3D catalogues, Volumetric rendering},
abstract = {Scientists across all disciplines increasingly rely on machine learning algorithms to analyse and sort datasets of ever increasing volume and complexity. Although trends and outliers are easily extracted, careful and close inspection will still be necessary to explore and disentangle detailed behaviour, as well as identify systematics and false positives. We must therefore incorporate new technologies to facilitate scientific analysis and exploration. Astrophysical data is inherently multi-parameter, with the spatial-kinematic dimensions at the core of observations and simulations. The arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as well as the availability of versatile development tools for video games, has enabled scientists to deploy such technology to effectively interrogate and interact with complex data. In this paper we present development and results from custom-built interactive VR tools, called the iDaVIE suite, that are informed and driven by research on galaxy evolution, cosmic large-scale structure, galaxy–galaxy interactions, and gas/kinematics of nearby galaxies in survey and targeted observations. In the new era of Big Data ushered in by major facilities such as the SKA and LSST that render past analysis and refinement methods highly constrained, we believe that a paradigm shift to new software, technology and methods that exploit the power of visual perception, will play an increasingly important role in bridging the gap between statistical metrics and new discovery. We have released a beta version of the iDaVIE software system that is free and open to the community.}
}
@article{HSU2021100004,
title = {How circular are plastics in the EU?: MFA of plastics in the EU and pathways to circularity},
journal = {Cleaner Environmental Systems},
volume = {2},
pages = {100004},
year = {2021},
issn = {2666-7894},
doi = {https://doi.org/10.1016/j.cesys.2020.100004},
url = {https://www.sciencedirect.com/science/article/pii/S2666789420300040},
author = {Wan-Ting Hsu and Teresa Domenech and Will McDowall},
keywords = {Plastic, Plastic waste, Circular economy, Material flow analysis, EU Plastics strategy, Resource management},
abstract = {Plastic is valued for its versatility, but concerns have been raised over the environmental impacts of mismanaged plastic waste. A better understanding of plastic flows can help to identify areas of inefficiency and potential leakage to natural systems. This research provides an overview of plastic flows in the EU and discusses options to increase plastic circularity. The study conducted a comprehensive stationary material flow analysis covering over 400 categories of plastic-containing products with detailed analysis of the final destination of waste. The results show the relevance of the EU plastic sector with production of over 66 MT of plastic polymers/fibres and an estimated consumption for plastic products of 73 MT in 2016. Plastic waste arisings amounted to over 37 MT and, though increasing plastic recycling rates have been reported, the analysis shows that a significant amount of plastic waste was not returned to production in the EU. The uncertainty analysis highlights important data quality issues that need to be addressed, particularly: data on the plastic fraction in plastic-containing products, and data on the final destination of plastic waste. Building on the analysis, the paper discusses a number of strategies for re-directing the plastic system to more circular pathways.}
}
@article{LI2021107646,
title = {Tackling mode collapse in multi-generator GANs with orthogonal vectors},
journal = {Pattern Recognition},
volume = {110},
pages = {107646},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107646},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320304490},
author = {Wei Li and Li Fan and Zhenyu Wang and Chao Ma and Xiaohui Cui},
keywords = {GANs, Mode collapse, Multiple generators, Orthogonal vectors, Minimax formula},
abstract = {Generative Adversarial Networks (GANs) have been widely used to generate realistic-looking instances. However, training robust GAN is a non-trivial task due to the problem of mode collapse. Although many GAN variants are proposed to overcome this problem, they have limitations. Those existing studies either generate identical instances or result in negative gradients during training. In this paper, we propose a new approach to training GAN to overcome mode collapse by employing a set of generators, an encoder and a discriminator. A new minimax formula is proposed to simultaneously train all components in a similar spirit to vanilla GAN. The orthogonal vector strategy is employed to guide multiple generators to learn different information in a complementary manner. In this way, we term our approach Multi-Generator Orthogonal GAN (MGO-GAN). Specifically, the synthetic data produced by those generators are fed into the encoder to obtain feature vectors. The orthogonal value is calculated between any two feature vectors, which loyally reflects the correlation between vectors. Such a correlation indicates how different information has been learnt by generators. The lower the orthogonal value is, the more different information the generators learn. We minimize the orthogonal value along with minimizing the generator loss through back-propagation in the training of GAN. The orthogonal value is integrated with the original generator loss to jointly update the corresponding generator’s parameters. We conduct extensive experiments utilizing MNIST, CIFAR10 and CelebA datasets to demonstrate the significant performance improvement of MGO-GAN in terms of generated data quality and diversity at different resolutions.}
}
@article{YU2021121001,
title = {Impact of demand information sharing on organic farming adoption: An evolutionary game approach},
journal = {Technological Forecasting and Social Change},
volume = {172},
pages = {121001},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121001},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521004339},
author = {Yanan Yu and Yong He and Xuan Zhao},
keywords = {Organic food, Organic farming adoption, Sustainable agriculture supply chain, Information sharing, Stackelberg game, Evolutionary game theory},
abstract = {Consumer demand information on healthy food can be readily collected and shared through big data technologies. However, the impact of demand information sharing on the evolution of organic farming is still less understood. Consequently, we use the evolutionary game approach to study the long-term effect of information sharing on the producers’ organic farming adoption based on the profit matrix. Considering that the retailer has some private demand information, the basic models as Stackelberg games in normal forms consisting of one retailer and one producer under information symmetry and information asymmetry are established to form the profit matrix. We also employ the method of cooperation on the forecast to realize information sharing and analyze the advantages and disadvantages of information sharing. Then, we analyze the evolutionarily stable strategy of several producers and retailers playing as a two-player game. The results indicate that (Conversion, Sharing) equilibrium can be easier to achieve with higher customers’ green preference and lower effort cost, whereas (Conversion, Nonsharing) equilibrium can be easier to achieve with more accurate forecast information. More importantly, a higher initial conversion state prompts the retailers to evolve to sharing more quickly and vice versa.}
}
@article{ULLAH2021101527,
title = {Barriers to the digitalisation and innovation of Australian Smart Real Estate: A managerial perspective on the technology non-adoption},
journal = {Environmental Technology & Innovation},
volume = {22},
pages = {101527},
year = {2021},
issn = {2352-1864},
doi = {https://doi.org/10.1016/j.eti.2021.101527},
url = {https://www.sciencedirect.com/science/article/pii/S2352186421001759},
author = {Fahim Ullah and Samad M.E. Sepasgozar and Muhammad Jamaluddin Thaheem and Fadi Al-Turjman},
keywords = {Technology adoption barriers, Innovation, Non-adoption, Smart real estate, Disruptive digital technologies (DDTs), Managerial perspective},
abstract = {The real estate sector brings a fortune to the global economy. But, presently, this sector is regressive and uses traditional methods and approaches. Therefore, it needs a technological transformation and innovation in line with the Industry 4.0 requirements to transform into smart real estate. However, it faces the barriers of disruptive digital technology (DDT) adoption and innovation that need effective management to enable such transformation. These barriers present managerial challenges that affect DDT adoption and innovation in smart real estate. The current study assesses these DDTs adoption and innovation barriers facing the Australian real estate sector from a managerial perspective. Based on a comprehensive review of 72 systematically retrieved and shortlisted articles, we identify 21 key barriers to digitalisation and innovation. The barriers are grouped into the technology-organisation-external environment (TOE) categories using a Fault tree. Data is collected from 102 real estate and property managers to rate and rank the identified barriers. The results show that most of the respondents are aware of the DDTs and reported AI (22.5% of respondents), big data (12.75%) and VR (12.75%) as the most critical technologies not adopted so far due to costs, organisation policies, awareness, reluctance, user demand, tech integration, government support and funding. Overall, the highest barrier (risk) scores are observed for high costs of software and hardware (T1), high complexity of the selected technology dissemination system (T2) and lack of government incentives, R&D support, policies, regulations and standards (E1). Among the TOE categories, as evident from the fault tree analysis, the highest percentage of failure to adopt the DDT is attributed to E1 in the environmental group. For the technological group, the highest failure reason is attributed to T2. And for the organisational group, the barrier with the highest failure chances for DDT adoption is the lack of organisational willingness to invest in digital marketing (O4). These barriers must be addressed to pave the way for DDT adoption and innovation in the Australian real estate sector and move towards smart real estate.}
}
@article{NI2021102450,
title = {Differentially Private Double Auction with Reliability-Aware in Mobile Crowd Sensing},
journal = {Ad Hoc Networks},
volume = {114},
pages = {102450},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102450},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521000238},
author = {Tianjiao Ni and Zhili Chen and Gang Xu and Shun Zhang and Hong Zhong},
keywords = {Mobile crowd sensing, Differential privacy, Double auction, Aggregation, Reliability},
abstract = {With the unprecedented proliferation of mobile devices, Mobile Crowd Sensing (MCS) emerges as a promising computing paradigm which utilizes sensor-embedded smart devices to collect sensory data. Recently, a number of privacy-preserving auction-based incentive mechanisms have been proposed. However, none of them guarantees the quality of sensing data in double-side auction scenarios. In this paper, we propose a Differentially Private Double Auction With Reliability-Aware in Mobile Crowd Sensing (DPDR). Specifically, we design the incentive mechanism by employing the exponential mechanism in double-side auction to select the clearing price tuple. Moreover, to collect precise sensory data, we heuristically choose more reliable workers as candidates for each clearing price tuple. We further improve the social welfare of the mechanism by designing the utility function with less sensitivity, or adopting a more practical pricing strategy. Through theoretical analysis, we demonstrate that our mechanisms can guarantee both differential privacy and economic properties, including individual rationality, budget balance, approximate truthfulness and approximate maximal social welfare. Extensive experimental results show that the improved mechanisms can achieve better performance than DPDR in term of social welfare, and all proposed mechanisms can produce high-quality data.}
}
@article{ESPADINHACRUZ2021107122,
title = {Lead management optimization using data mining: A case in the telecommunications sector},
journal = {Computers & Industrial Engineering},
volume = {154},
pages = {107122},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107122},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221000267},
author = {P. Espadinha-Cruz and A. Fernandes and A. Grilo},
keywords = {Lead Management, Data Mining, Predictive Models, Machine Learning, Telecommunications},
abstract = {The growing competitiveness of the market has put pressure on companies to improve their customer relationship management strategies. In an era where mass marketing techniques are inadequate, lead management is at the forefront to provide a customized approach to customer acquisition. For this, lead management depends on the correct selection of leads and decision making on what type of approach to take to satisfy the requirements of customers. However, currently, firms are faced with massive quantity of data regarding customers and prospects. Data mining is a solution to cope with this problem, providing a robust approach to massive quantity of data and its complexity. In literature, there is a lack of documented applications of these techniques in lead management. In this sense, we propose a methodology that aims to improve efficiency on the distinct maturity stages of leads management. Also, the methodology aids in support the decision-making regarding the segmentation of leads. This research suggests the application of data mining techniques in the optimization of leads management processes, from capture to conversion, with the objective of improving customer conversion effectiveness. A case study was conducted in a telecommunications company. It was possible to implement the proposed method to estimate the probability of conversion for each lead. With this, was possible to segment the offer to each type of lead.}
}
@article{MULLER202121,
title = {Digital shop floor management enhanced by natural language processing},
journal = {Procedia CIRP},
volume = {96},
pages = {21-26},
year = {2021},
note = {8th CIRP Global Web Conference – Flexible Mass Customisation (CIRPe 2020)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.046},
url = {https://www.sciencedirect.com/science/article/pii/S221282712100069X},
author = {Marvin Müller and Emanuel Alexandi and Joachim Metternich},
keywords = {digital shop floor management, natural language processing, document clustering},
abstract = {This paper aims to develop concepts how digital shop floor management (dSFM) can be further enhanced by natural language processing (NLP) to bring a higher value to the shop floor team and decision makers. Based on the literature review on these two fields several valuable application of NLP in dSFM are theorized: recommender engines to improve knowledge management, text clustering to identify frequent problems, voice assistants to ease the interaction with the data base, chat log extraction to fill the database with unstructured written text from chats and spellcheck as well as auto fill to improve data quality. To show the feasibility for NLP in dSFM in industry, a case study for the document clustering is presented: A digital ticket system for shop floor issues used for two years and containing 2,735 entries is analysed with the “Graph”-feature from Elasticsearch to find the most frequent terms and intersections in the described problems. The approach is accurate, quick and detailed and will be established in the company and performed monthly.}
}
@article{WANG2021106931,
title = {Industry 3.5 to empower smart production for poultry farming and an empirical study for broiler live weight prediction},
journal = {Computers & Industrial Engineering},
volume = {151},
pages = {106931},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106931},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220306136},
author = {Chun-Yao Wang and Ying-Jen Chen and Chen-Fu Chien},
keywords = {Industry 3.5, Smart production, Poultry farming, Data-driven approach, Broiler weight estimation, Weight prediction},
abstract = {Emerging countries and traditional industries may not be ready for direct migration of Industry 4.0. In particular, the broiler is a major source for meat, while poultry farming in emerging countries is mainly by small and medium-sized enterprises (SMEs) or family businesses. The live broilers need to maintain the desired specification for food processing while optimizing the feeding conversion rate for revenue management. Conventionally, broiler growth monitoring and prediction rely on farmers’ experience and a small amount of data by unrigorous sampling. Although the automatic weighing system (AWS) has gradually been employed to replace manual weighing to reduce cost and casualties, existing automatic weighing systems have limited capability for weight monitoring and weight prediction of broiler future growth. To fill the gaps, this study aims to employ Industry 3.5 as a hybrid and develop a data-driven framework for weight monitoring and prediction to support smart production for poultry farming for revenue optimization. The proposed framework contains two modules. The weight monitoring integrates Gaussian Mixture Model, bootstrapping resampling, and weighted mean technique to estimate the current weight of live broilers in the farm via big data including electronic signals collected from the farms via multiple sensors and devices. The weight prediction module employs mathematical growth functions as a basis and daily feedback for adjustment to provide real-time weight prediction to support related decisions for smart production. An empirical study was conducted in Taiwan. The results have shown the practical viability of this approach. The developed solution is implemented in the broiler industry.}
}
@article{ZHAO2021102691,
title = {Understanding the interaction between human activities and physical health under extreme heat environment in Phoenix, Arizona},
journal = {Health & Place},
pages = {102691},
year = {2021},
issn = {1353-8292},
doi = {https://doi.org/10.1016/j.healthplace.2021.102691},
url = {https://www.sciencedirect.com/science/article/pii/S1353829221001878},
author = {Qunshan Zhao and Ziqi Li and Dhrumil Shah and Heather Fischer and Patricia Solís and Elizabeth Wentz},
keywords = {, Citizen science, Portable sensing, Urban heat, Community resilience},
abstract = {Long-term community resilience, which privileges a long view look at chronic issues influencing communities, has begun to draw more attention from city planners, researchers and policymakers. In Phoenix, resilience to heat is both a necessity and a way of life. In this paper, we attempt to understand how residents living in Phoenix experience and behave in an extreme heat environment. To achieve this goal, we introduced a smartphone application (ActivityLog) to study spatio-temporal dynamics of human interaction with urban environments. Compared with traditional paper activity log results we have in this study, the smartphone-based activity log has higher data quality in terms of total number of logs, response rates, accuracy, and connection with GPS and temperature sensors. The research results show that low-income residents in Phoenix mostly stay home during the summer but experience a relatively high indoor temperature due to the lack/low efficiency of air-conditioning (AC) equipment or lack of funds to run AC frequently. Middle-class residents have a better living experience in Phoenix with better mobility with automobiles and good quality of AC. The research results help us better understand user behaviors for daily log activities and how human activities interact with the urban thermal environment, informing further planning policy development. The ActivityLog smartphone application is also presented as an open-source prototype to design a similar urban climate citizen science program in the future.}
}
@incollection{BEACH2021379,
title = {Chapter 14 - Edge algorithms for wearables: an overview of a truly multi-disciplinary problem},
editor = {Edward Sazonov},
booktitle = {Wearable Sensors (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {379-414},
year = {2021},
isbn = {978-0-12-819246-7},
doi = {https://doi.org/10.1016/B978-0-12-819246-7.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128192467000140},
author = {Christopher Beach and Ertan Balaban and Alexander J. Casson},
keywords = {Accurate performance testing, Algorithm design, Battery selection, Circuit design, Design trends, Embedded signal processing, Low-power consumption, Real-time signal processing, Sensor node optimization, Wearable devices},
abstract = {This chapter introduces and comprehensively overviews emerging edge algorithms for wearable devices – signal processing algorithms which are embedded into the wearable hardware itself. We begin by overviewing some of the potential benefits of including low-power real-time signal processing in wearable sensors, with benefits of both increasing the operational lifetime and increasing the device functionality. Results from a practical state-of-the-art sensor platform demonstrate and quantify the design trade-offs present and the potential system optimizations available. We then consider the theory behind wearable algorithms and highlight the key properties of power-lifetime trade-off, Big Data performance testing, and performance-power trade-off that differentiate these new algorithms from conventional signal processing approaches. We conclude by investigating different implementation techniques and the different approaches available to minimize power consumption and also to maintain design flexibility and speed.}
}
@article{YANG2021104887,
title = {A comparative analysis of eleven neural networks architectures for small datasets of lung images of COVID-19 patients toward improved clinical decisions},
journal = {Computers in Biology and Medicine},
volume = {139},
pages = {104887},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104887},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521006818},
author = {Yuan Yang and Lin Zhang and Mingyu Du and Jingyu Bo and Haolei Liu and Lei Ren and Xiaohe Li and M. Jamal Deen},
keywords = {Deep learning, Computed tomography, COVID-19, Image classification},
abstract = {The 2019 novel severe acute respiratory syndrome coronavirus 2-SARS-CoV2, commonly known as COVID-19, is a highly infectious disease that has endangered the health of many people around the world. COVID-19, which infects the lungs, is often diagnosed and managed using X-ray or computed tomography (CT) images. For such images, rapid and accurate classification and diagnosis can be performed using deep learning methods that are trained using existing neural network models. However, at present, there is no standardized method or uniform evaluation metric for image classification, which makes it difficult to compare the strengths and weaknesses of different neural network models. This paper used eleven well-known convolutional neural networks, including VGG-16, ResNet-18, ResNet-50, DenseNet-121, DenseNet-169, Inception-v3, Inception-v4, SqueezeNet, MobileNet, ShuffeNet, and EfficientNet-b0, to classify and distinguish COVID-19 and non-COVID-19 lung images. These eleven models were applied to different batch sizes and epoch cases, and their overall performance was compared and discussed. The results of this study can provide decision support in guiding research on processing and analyzing small medical datasets to understand which model choices can yield better outcomes in lung image classification, diagnosis, disease management and patient care.}
}
@article{WONG2021103732,
title = {Search and visualization of gene-drug-disease interactions for pharmacogenomics and precision medicine research using GeneDive},
journal = {Journal of Biomedical Informatics},
volume = {117},
pages = {103732},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103732},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421000617},
author = {Mike Wong and Paul Previde and Jack Cole and Brook Thomas and Nayana Laxmeshwar and Emily Mallory and Jake Lever and Dragutin Petkovic and Russ B. Altman and Anagha Kulkarni},
keywords = {Gene interactions, Retrieval and visualization, Gene sets, Gene-disease and gene-drug relationships, Biomedical information retrieval},
abstract = {Background
Understanding the relationships between genes, drugs, and disease states is at the core of pharmacogenomics. Two leading approaches for identifying these relationships in medical literature are: human expert led manual curation efforts, and modern data mining based automated approaches. The former generates small amounts of high-quality data, and the latter offers large volumes of mixed quality data. The algorithmically extracted relationships are often accompanied by supporting evidence, such as, confidence scores, source articles, and surrounding contexts (excerpts) from the articles, that can be used as data quality indicators. Tools that can leverage these quality indicators to help the user gain access to larger and high-quality data are needed.
Approach
We introduce GeneDive, a web application for pharmacogenomics researchers and precision medicine practitioners that makes gene, disease, and drug interactions data easily accessible and usable. GeneDive is designed to meet three key objectives: (1) provide functionality to manage information-overload problem and facilitate easy assimilation of supporting evidence, (2) support longitudinal and exploratory research investigations, and (3) offer integration of user-provided interactions data without requiring data sharing.
Results
GeneDive offers multiple search modalities, visualizations, and other features that guide the user efficiently to the information of their interest. To facilitate exploratory research, GeneDive makes the supporting evidence and context for each interaction readily available and allows the data quality threshold to be controlled by the user as per their risk tolerance level. The interactive search-visualization loop enables relationship discoveries between diseases, genes, and drugs that might not be explicitly described in literature but are emergent from the source medical corpus and deductive reasoning. The ability to utilize user’s data either in combination with the GeneDive native datasets or in isolation promotes richer data-driven exploration and discovery. These functionalities along with GeneDive’s applicability for precision medicine, bringing the knowledge contained in biomedical literature to bear on particular clinical situations and improving patient care, are illustrated through detailed use cases.
Conclusion
GeneDive is a comprehensive, broad-use biological interactions browser. The GeneDive application and information about its underlying system architecture are available at http://www.genedive.net. GeneDive Docker image is also available for download at this URL, allowing users to (1) import their own interaction data securely and privately; and (2) generate and test hypotheses across their own and other datasets.}
}
@article{THEEK202130,
title = {Automation of data analysis in molecular cancer imaging and its potential impact on future clinical practice},
journal = {Methods},
volume = {188},
pages = {30-36},
year = {2021},
note = {Artificial Intelligence Approaches for Imaging Biomarkers},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2020.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S1046202319303160},
author = {Benjamin Theek and Zuzanna Magnuska and Felix Gremse and Horst Hahn and Volkmar Schulz and Fabian Kiessling},
keywords = {Molecular imaging, Radiomics, Big data, Image analysis, Artificial intelligence, Cancer},
abstract = {Digitalization, especially the use of machine learning and computational intelligence, is considered to dramatically shape medical procedures in the near future. In the field of cancer diagnostics, radiomics, the extraction of multiple quantitative image features and their clustered analysis, is gaining increasing attention to obtain more detailed, reproducible, and meaningful information about the disease entity, its prognosis and the ideal therapeutic option. In this context, automation of diagnostic procedures can improve the entire pipeline, which comprises patient registration, planning and performing an imaging examination at the scanner, image reconstruction, image analysis, and feeding the diagnostic information from various sources into decision support systems. With a focus on cancer diagnostics, this review article reports and discusses how computer-assistance can be integrated into diagnostic procedures and which benefits and challenges arise from it. Besides a strong view on classical imaging modalities like x-ray, CT, MRI, ultrasound, PET, SPECT and hybrid imaging devices thereof, it is outlined how imaging data can be combined with data deriving from patient anamnesis, clinical chemistry, pathology, and different omics. In this context, the article also discusses IT infrastructures that are required to realize this integration in the clinical routine. Although there are still many challenges to comprehensively implement automated and integrated data analysis in molecular cancer imaging, the authors conclude that we are entering a new era of medical diagnostics and precision medicine.}
}
@article{RETICO2021140,
title = {Enhancing the impact of Artificial Intelligence in Medicine: A joint AIFM-INFN Italian initiative for a dedicated cloud-based computing infrastructure},
journal = {Physica Medica},
volume = {91},
pages = {140-150},
year = {2021},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1120179721003203},
author = {Alessandra Retico and Michele Avanzo and Tommaso Boccali and Daniele Bonacorsi and Francesca Botta and Giacomo Cuttone and Barbara Martelli and Davide Salomoni and Daniele Spiga and Annalisa Trianni and Michele Stasi and Mauro Iori and Cinzia Talamonti},
keywords = {Artificial intelligence, Decision support systems, Computing infrastructure, Distributed learning},
abstract = {Artificial Intelligence (AI) techniques have been implemented in the field of Medical Imaging for more than forty years. Medical Physicists, Clinicians and Computer Scientists have been collaborating since the beginning to realize software solutions to enhance the informative content of medical images, including AI-based support systems for image interpretation. Despite the recent massive progress in this field due to the current emphasis on Radiomics, Machine Learning and Deep Learning, there are still some barriers to overcome before these tools are fully integrated into the clinical workflows to finally enable a precision medicine approach to patients’ care. Nowadays, as Medical Imaging has entered the Big Data era, innovative solutions to efficiently deal with huge amounts of data and to exploit large and distributed computing resources are urgently needed. In the framework of a collaboration agreement between the Italian Association of Medical Physicists (AIFM) and the National Institute for Nuclear Physics (INFN), we propose a model of an intensive computing infrastructure, especially suited for training AI models, equipped with secure storage systems, compliant with data protection regulation, which will accelerate the development and extensive validation of AI-based solutions in the Medical Imaging field of research. This solution can be developed and made operational by Physicists and Computer Scientists working on complementary fields of research in Physics, such as High Energy Physics and Medical Physics, who have all the necessary skills to tailor the AI-technology to the needs of the Medical Imaging community and to shorten the pathway towards the clinical applicability of AI-based decision support systems.}
}
@article{AITIO20213204,
title = {Predicting battery end of life from solar off-grid system field data using machine learning},
journal = {Joule},
volume = {5},
number = {12},
pages = {3204-3220},
year = {2021},
issn = {2542-4351},
doi = {https://doi.org/10.1016/j.joule.2021.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2542435121005328},
author = {Antti Aitio and David A. Howey},
keywords = {battery, health, machine learning, rural electrification, Gaussian process, classification, Kalman filter},
abstract = {Summary
Hundreds of millions of people lack access to electricity. Decentralized solar-battery systems are key for addressing this while avoiding carbon emissions and air pollution but are hindered by relatively high costs and rural locations that inhibit timely preventive maintenance. Accurate diagnosis of battery health and prediction of end of life from operational data improves user experience and reduces costs. However, lack of controlled validation tests and variable data quality mean existing lab-based techniques fail to work. We apply a scalable probabilistic machine learning approach to diagnose health in 1,027 solar-connected lead-acid batteries, each running for 400–760 days, totaling 620 million data rows. We demonstrate 73% accurate prediction of end of life, 8 weeks in advance, rising to 82% at the point of failure. This work highlights the opportunity to estimate health from existing measurements using “big data” techniques, without additional equipment, extending lifetime and improving performance in real-world applications.}
}
@incollection{PRASANNA2021133,
title = {5 - A data science perspective of real-world COVID-19 databases},
editor = {Le Gruenwald and Sarika Jain and Sven Groppe},
booktitle = {Leveraging Artificial Intelligence in Global Epidemics},
publisher = {Academic Press},
pages = {133-163},
year = {2021},
isbn = {978-0-323-89777-8},
doi = {https://doi.org/10.1016/B978-0-323-89777-8.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897778000087},
author = {Shivika Prasanna and Praveen Rao},
keywords = {COVID-19, apache spark, data science, machine learning, data visualization},
abstract = {The COVID-19 pandemic has devastated the lives of millions of people worldwide and damaged the economy of many countries. While the negative impact of the pandemic on mankind is unimaginable, this pandemic has triggered new research and innovation in the use of artificial intelligence for developing solutions to better understand and mitigate the pandemic. Several valuable datasets have been made available by different organizations and research groups. In this chapter, we provide an overview of real-world COVID-19 data sources available for developing novel applications and solutions for the pandemic. We provide a comparison between them from a data science perspective. Next, we delve deep into the Cerner Real-World Data for COVID-19. We discuss the schema of the database, data quality issues, data wrangling using Apache Spark, and data analysis using popular machine learning techniques. Specifically, we provide examples of querying the database, training machine learning models, and visualization. We also discuss the technical challenges that we encountered and how we overcame them to complete multiple clinical studies on COVID-19.}
}
@incollection{MCGILVRAY2021269,
title = {Chapter 6 - Other Techniques and Tools},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {269-304},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000037},
author = {Danette McGilvray},
keywords = {Tracking issues, tracking action items, data capture, assessment plan, information life cycle, flowchart, IP Map, survey, analyze results, synthesize, documentation, metrics, Six Sigma, ISO, data quality tools, project management.},
abstract = {This chapter outlines techniques, with many templates and examples, that can be applied in several places throughout the Ten Steps Process. Techniques include various approaches to visualizing an information life cycle, conducting surveys, developing data capture and assessment plans, and implementing metrics. The technique for tracking issues and action items helps with project management and another shows how the Ten Steps Process can be used with Six Sigma and ISO Standards. Every step, assessment and activity should make use of the technique Analyze, Synthesize, Recommend, Document, and Act on Results. Even though the methodology does not require specific tools, typical tool functionality is described which can be used when considering tools to be used in a data quality project. The techniques should be adjusted and applied to fit what is needed for the particular circumstances and step where they are used.}
}
@article{MONEREOSANCHEZ2021118174,
title = {Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations - insights from the Maastricht study},
journal = {NeuroImage},
volume = {237},
pages = {118174},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118174},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921004511},
author = {Jennifer Monereo-Sánchez and Joost J.A. {de Jong} and Gerhard S. Drenthen and Magdalena Beran and Walter H. Backes and Coen D.A. Stehouwer and Miranda T. Schram and David E.J. Linden and Jacobus F.A. Jansen},
keywords = {Brain segmentation, Cortical parcellation, FreeSurfer, Quality control, Manual editing, Outlier exclusion},
abstract = {Quality control of brain segmentation is a fundamental step to ensure data quality. Manual quality control strategies are the current gold standard, although these may be unfeasible for large neuroimaging samples. Several options for automated quality control have been proposed, providing potential time efficient and reproducible alternatives. However, those have never been compared side to side, which prevents consensus in the appropriate quality control strategy to use. This study aimed to elucidate the changes manual editing of brain segmentations produce in morphological estimates, and to analyze and compare the effects of different quality control strategies on the reduction of the measurement error. Structural brain MRI from 259 participants of The Maastricht Study were used. Morphological estimates were automatically extracted using FreeSurfer 6.0. Segmentations with inaccuracies were manually edited, and morphological estimates were compared before and after editing. In parallel, 12 quality control strategies were applied to the full sample. Those included: two manual strategies, in which images were visually inspected and either excluded or manually edited; five automated strategies, where outliers were excluded based on the tools “MRIQC” and “Qoala-T”, and the metrics “morphological global measures”, “Euler numbers” and “Contrast-to-Noise ratio”; and five semi-automated strategies, where the outliers detected through the mentioned tools and metrics were not excluded, but visually inspected and manually edited. In order to quantify the effects of each quality control strategy, the proportion of unexplained variance relative to the total variance was extracted after the application of each strategy, and the resulting differences compared. Manually editing brain surfaces produced particularly large changes in subcortical brain volumes and moderate changes in cortical surface area, thickness and hippocampal volumes. The performance of the quality control strategies depended on the morphological measure of interest. Overall, manual quality control strategies yielded the largest reduction in relative unexplained variance. The best performing automated alternatives were those based on Euler numbers and MRIQC scores. The exclusion of outliers based on global morphological measures produced an increase of relative unexplained variance. Manual quality control strategies are the most reliable solution for quality control of brain segmentation and parcellation. However, measures must be taken to prevent the subjectivity associated with these strategies. The detection of inaccurate segmentations based on Euler numbers or MRIQC provides a time efficient and reproducible alternative. The exclusion of outliers based on global morphological estimates must be avoided.}
}
@article{GBADAMOSI2021103486,
title = {IoT for predictive assets monitoring and maintenance: An implementation strategy for the UK rail industry},
journal = {Automation in Construction},
volume = {122},
pages = {103486},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103486},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520310669},
author = {Abdul-Quayyum Gbadamosi and Lukumon O. Oyedele and Juan Manuel Davila Delgado and Habeeb Kusimo and Lukman Akanbi and Oladimeji Olawale and Naimah Muhammed-yakubu},
keywords = {Internet of things, Predictive maintenance, Remote inspection, Rail assets, Augmented reality},
abstract = {With about 100% increase in rail service usage over the last 20 years, it is pertinent that rail infrastructure continues to function at an optimal level to avoid service disruptions, cancellations or delays due to unforeseen asset breakdown. In an endeavour to propose a strategy for the implementation of Internet of Things (IoT) in rail asset maintenance, a qualitative methodology was adopted through a series of focus-group workshops to identify the priority areas and enabling digital technologies for IoT implementation. The methods of data collection included audio recording, note-taking, and concept mapping. The audio records were transcribed and used for thematic analysis, while the concept maps were integrated for conceptual modelling and analysis. This paper presents an implementation strategy for IoT for rail assets maintenance with focus on priority areas such as real-time condition monitoring using IoT sensors, predictive maintenance, remote inspection, and integrated asset data management platform.}
}
@article{HONG2021125814,
title = {Online accurate state of health estimation for battery systems on real-world electric vehicles with variable driving conditions considered},
journal = {Journal of Cleaner Production},
volume = {294},
pages = {125814},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.125814},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621000342},
author = {Jichao Hong and Zhenpo Wang and Wen Chen and Leyi Wang and Peng Lin and Changhui Qu},
keywords = {State of health, Battery systems, Variable-length-input, Long short-term memory, Driving behavior},
abstract = {The environmental sustainability stimulates the development of electric vehicles with great energy-saving and emission reduction effects. State of health of the battery system in an electric vehicle is crucial to the safety of vehicle operation, charging station, and the environment. The existing techniques implemented in well-controlled experimental environments fail to learn unpredictable drivers’ driving behaviors and complex road/weather conditions during actual vehicular operation. This paper investigates a novel deep-learning-enabled method to perform accurate state of health estimation for battery systems on real-world electric vehicles. Eight potential evaluation schemes depending on the stable charging stages are recapped and discussed. By fitting the correlation between battery degeneration factors and various vehicle operation parameters such as ambient temperature and mileage, an approximate battery degeneration model oriented for the real application scenarios is obtained. The variable-length-input long short-term memory network is used to learn the variable battery degeneration factors acquired from different driving stages of a yearlong dataset. The test results show that the proposed method has a better performance than other estimation methods. More significantly, based on the acquisition advantages of big-data platforms, it can be used to full-state and full-climate vehicle applications unrestricted by complex actual environments.}
}
@article{MATHEUS2021101550,
title = {Design principles for creating digital transparency in government},
journal = {Government Information Quarterly},
volume = {38},
number = {1},
pages = {101550},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101550},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20303294},
author = {Ricardo Matheus and Marijn Janssen and Tomasz Janowski},
keywords = {Transparency, Digital transparency, Transparency-by-design, Open data, Open government, Design principles, Window theory},
abstract = {Under pressure to fight corruption, hold public officials accountable, and build trust with citizens, many governments pursue the quest for greater transparency. They publish data about their internal operations, externalize decision-making processes, establish digital inquiry lines to public officials, and employ other forms of transparency using digital means. Despite the presence of many transparency-enhancing digital tools, putting such tools together to achieve the desired level of digital transparency, to design entire government systems for digital transparency, remains challenging. Design principles and other design guides are lacking in this area. This article aims to fill this gap. We identify a set of barriers to digital transparency in government, define 16 design principles to overcome such barriers, and evaluate these principles using three case studies from different countries. Some principles apply to projects, others to systems, yet others to entire organizations. To achieve digital transparency, before building and deploying digital solutions, government organizations should build technological and institutional foundations and use such foundations to organize themselves for transparency. The proposed design principles can help develop and apply such foundations.}
}
@article{LUO2021111224,
title = {An overview of data tools for representing and managing building information and performance data},
journal = {Renewable and Sustainable Energy Reviews},
volume = {147},
pages = {111224},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111224},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121005116},
author = {Na Luo and Marco Pritoni and Tianzhen Hong},
keywords = {Building information modeling, Ontology, Data schema, Metadata, Building performance data},
abstract = {Building information modeling (BIM) has been widely adopted for representing and exchanging building data across disciplines during building design and construction. However, BIM's use in the building operation phase is limited. With the increasing deployment of low-cost sensors and meters, as well as affordable digital storage and computing technologies, growing volumes of data have been collected from buildings, their energy services systems, and occupants. Such data are crucial to help decision makers understand what, how, and when energy is consumed in buildings—a critical step to improving building performance for energy efficiency, demand flexibility, and resilience. However, practical analyses and use of the collected data are very limited due to various reasons, including poor data quality, ad-hoc representation of data, and lack of data science skills. To unlock value from building data, there is a strong need for a toolchain to curate and represent building information and performance data in common standardized terminologies and schemas, to enable interoperability between tools and applications. This study selected and reviewed 24 data tools based on common use cases of data across the building life cycle, from design to construction, commissioning, operation, and retrofits. The selected data tools are grouped into three categories: (1) data dictionary or terminology, (2) data ontology and schemas, and (3) data platforms. The data are grouped into ten typologies covering most types of data collected in buildings. This study resulted in five main findings: (1) most data representation tools can represent their intended data typologies well, such as Green Button for smart meter data and Brick schema for metadata of sensors in buildings and HVAC systems, but none of the tools cover all ten types of data; (2) there is a need for data schemas to represent the basis of design data and metadata of occupant data; (3) standard terminologies such as those defined in BEDES are only adopted in a few data tools; (4) integrating data across various stages in the building life cycle remains a challenge; and (5) most data tools were developed and maintained by different parties for different purposes, their flexibility and interoperability can be improved to support broader use cases. Finally, recommendations for future research on building data tools are provided for the data and buildings community based on the FAIR principles to make data Findable, Accessible, Interoperable, and Reusable.}
}
@article{LEE2021107513,
title = {Multistage MR-CART: Multiresponse optimization in a multistage process using a classification and regression tree method},
journal = {Computers & Industrial Engineering},
volume = {159},
pages = {107513},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107513},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221004174},
author = {Dong-Hee Lee and So-Hee Kim and Kwang-Jae Kim},
keywords = {Multistage process, Multiresponse optimization, Desirability function, Big data, Data mining, Classification and regression tree},
abstract = {A multistage process consists of sequential consecutive stages. In this process, each stage has multiple responses and is affected by its preceding stage, while at the same time, affecting the following stage. This complex structure makes it difficult to optimize the multistage process. Recently, it became easy to obtain a large amount of operational data from the multistage process due to development of information technologies. The proposed method employs a data mining method called a classification and regression tree for analyzing the data and desirability functions for simultaneously optimizing the multiresponse. To consider the relationship between stages, a backward optimization procedure which treats the multiresponse of the preceding stage as the input variables is proposed. The proposed method is described using a steel manufacturing process example and is compared with existing multiresponse optimization methods. The case study shows that the proposed method works well and outperforms the existing methods.}
}
@article{MAZUR2021144506,
title = {GC-HRMS with Complementary Ionization Techniques for Target and Non-target Screening for Chemical Exposure: Expanding the Insights of the Air Pollution Markers in Moscow Snow},
journal = {Science of The Total Environment},
volume = {761},
pages = {144506},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.144506},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720380372},
author = {D.M. Mazur and E.A. Detenchuk and A.A. Sosnova and V.B. Artaev and A.T. Lebedev},
keywords = {GC-HRMS, GC-HR-TOFMS, Electron capture negative ionization, Air pollution, Priority pollutants, Persistent organic pollutants, Emerging contaminants, Moscow snow pollution, Big data analysis},
abstract = {Environmental exposure assessment is an important step in establishing a list of local priority pollutants and finding the sources of the threats for proposing appropriate protection measures. Exposome targeted and non-targeted analysis as well as suspect screening may be applied to reveal these pollutants. The non-targeted screening is a challenging task and requires the application of the most powerful analytical tools available, assuring wide analytical coverage, sensitivity, identification reliability, and quantitation. Moscow, Russia, is the largest and most rapidly growing European city. That rapid growth is causing changes in the environment which require periodic clarification of the real environmental situation regarding the presence of the classic pollutants and possible new contaminants. Gas chromatography – high resolution time-of-flight mass spectrometry (GC-HR-TOFMS) with electron ionization (EI), positive chemical ionization (PCI), and electron capture negative ionization (ECNI) ion sources were used for the analysis of Moscow snow samples collected in the early spring of 2018 in nine different locations. Collection of snow samples represents an efficient approach for the estimation of long-term air pollution, due to accumulation and preservation of environmental contaminants by snow during winter period. The high separation power of GC, complementary ionization methods, high mass accuracy, and wide mass range of TOFMS allowed for the identification of several hundred organic compounds belonging to the various classes of pollutants, exposure to which could represent a danger to the health of the population. Although quantitative analysis was not a primary aim of the study, targeted analysis revealed that some priority pollutants exceeded the established safe levels. Thus, dibutylphthalate concentration was over 10-fold higher than its safe level (0.001 mg/L), while benz[a]pyrene concentration exceeded Russian maximal permissible concentration value of 5 ng/L in three samples. The large amount of information generated during the combination of targeted and non-targeted analysis and screening samples for suspects makes it feasible to apply the big data analysis to observe the trends and tendencies in the pollution exposome across the city.}
}
@article{BAAS2021121153,
title = {A methodological approach for structural health monitoring of mass-timber buildings under construction},
journal = {Construction and Building Materials},
volume = {268},
pages = {121153},
year = {2021},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2020.121153},
url = {https://www.sciencedirect.com/science/article/pii/S0950061820331573},
author = {Esther J. Baas and Mariapaola Riggio and André R. Barbosa},
keywords = {Big data, Cross-laminated timber, Construction monitoring, Mass plywood panel, Mass-timber, Self-centering rocking wall, Structural health monitoring},
abstract = {Structural health monitoring (SHM) is a method used to evaluate the performance of new structural systems and critical infrastructure. With mass-timber building construction on the rise, SHM programs have emerged to document hygrothermal, static, and dynamic behavior of these structures. To most efficiently document behavior and provide recommendations to industry, it is key that the research community work collaboratively to create consistent data by using standardized approaches. This paper presents a methodological approach for monitoring mass-timber buildings during construction to address this need. The approach was validated over ten months with a mass-timber building under construction at Oregon State University.}
}
@article{WEI2021101173,
title = {Deep learning-assisted elastic isotropy identification for architected materials},
journal = {Extreme Mechanics Letters},
volume = {43},
pages = {101173},
year = {2021},
issn = {2352-4316},
doi = {https://doi.org/10.1016/j.eml.2021.101173},
url = {https://www.sciencedirect.com/science/article/pii/S2352431621000055},
author = {Anran Wei and Jie Xiong and Weidong Yang and Fenglin Guo},
keywords = {Deep learning, Convolutional neural network, Architected material, Elastic isotropy, Rapid mechanical characterization, Transfer learning},
abstract = {Architected materials consisting of periodic unit cells are desirable for many engineering applications. Characterizing the elastic isotropy is of great significance for the mechanical design of architected materials. However, prevailing experimental and numerical approaches are normally too costly and time-consuming to screen out isotropic architected materials in the large design space. Here, a deep learning-based approach is developed as a highly efficient and portable tool to identify the elastic isotropy of architected materials directly from images of their unit cells with arbitrary component distributions. The measure of elastic isotropy for heterogeneous architected materials is derived firstly in this paper to construct a database with associated images of unit cells. Then a convolutional neural network is fully trained with the database, performing well on the isotropy identification with about 90% accuracy and milliseconds processing time per sample. Meanwhile, it exhibits enough robustness to maintain its performance under the fluctuating material properties in test sets. Moreover, the transfer learning of the convolutional neural network is successfully implemented among architected materials with different numbers of material components, which further promotes the efficiency of the deep learning-based approach without scarifying its identification performance. This study gives new inspirations on the rapid mechanical characterization of architectured materials, which holds promising applications in the big-data driven topological design and nondestructive testing of architected materials.}
}
@article{MOKHTAR2021100103,
title = {Prediction of voltage distribution using deep learning and identified key smart meter locations},
journal = {Energy and AI},
volume = {6},
pages = {100103},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100103},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000550},
author = {Maizura Mokhtar and Valentin Robu and David Flynn and Ciaran Higgins and Jim Whyte and Caroline Loughran and Fiona Fulton},
keywords = {Voltage prediction, Smart meters, Deep neural learning, Distribution network operation, Big Data Analytics, Analytic methods in power networks, Privacy-preserving data analysis},
abstract = {The energy landscape for the Low-Voltage (LV) networks is undergoing rapid changes. These changes are driven by the increased penetration of distributed Low Carbon Technologies, both on the generation side (i.e. adoption of micro-renewables) and demand side (i.e. electric vehicle charging). The previously passive ‘fit-and-forget’ approach to LV network management is becoming increasing inefficient to ensure its effective operation. A more agile approach to operation and planning is needed, that includes pro-active prediction and mitigation of risks to local sub-networks (such as risk of voltage deviations out of legal limits). The mass rollout of smart meters (SMs) and advances in metering infrastructure holds the promise for smarter network management. However, many of the proposed methods require full observability, yet the expectation of being able to collect complete, error free data from every smart meter is unrealistic in operational reality. Furthermore, the smart meter (SM) roll-out has encountered significant issues, with the current voluntary nature of installation in the UK and in many other countries resulting in low-likelihood of full SM coverage for all LV networks. Even with a comprehensive SM roll-out privacy restrictions, constrain data availability from meters. To address these issues, this paper proposes the use of a Deep Learning Neural Network architecture to predict the voltage distribution with partial SM coverage on actual network operator LV circuits. The results show that SM measurements from key locations are sufficient for effective prediction of the voltage distribution, even without the use of the high granularity personal power demand data from individual customers.}
}
@incollection{KASHYAP202145,
title = {Chapter 3 - Machine learning for predictive analytics},
editor = {Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas},
booktitle = {Machine Learning in Cardiovascular Medicine},
publisher = {Academic Press},
pages = {45-69},
year = {2021},
isbn = {978-0-12-820273-9},
doi = {https://doi.org/10.1016/B978-0-12-820273-9.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202739000038},
author = {Sehj Kashyap and Kristin M. Corey and Aman Kansal and Mark Sendak},
keywords = {Artificial intelligence, Cardiovascular medicine, Clinical decision support, Machine learning, Predictive models},
abstract = {Advances in the field of machine learning are enabling the development of new predictive models in the field of cardiovascular medicine. In this chapter, we cover how predictive machine learning models are developed, their recent use-cases in cardiovascular medicine, and limitations for their expanded use in healthcare. Predictive modeling has long been used in medicine, but recently, machine learning methods offer the ability to learn patterns in large and heterogeneous health data for improved predictive modeling. In addition to describing the general methods by which machine learning models are developed, we share their recent applications in electrophysiology, interventional cardiology, heart failure, and preventive cardiovascular care. We conclude with a discussion about issues regarding data quality, generalizability, bias, and interpretability, and how these influence model development and clinical integration.}
}
@article{POLLO2021116111,
title = {Chemometrics, Comprehensive Two-Dimensional gas chromatography and “omics” sciences: Basic tools and recent applications},
journal = {TrAC Trends in Analytical Chemistry},
volume = {134},
pages = {116111},
year = {2021},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2020.116111},
url = {https://www.sciencedirect.com/science/article/pii/S016599362030340X},
author = {Breno Jorge Pollo and Carlos Alberto Teixeira and Joao Raul Belinato and Mayra Fontes Furlan and Isabela Cristina de Matos Cunha and Caroline Rocha Vaz and Gustavo Veronezi Volpato and Fabio Augusto},
keywords = {GC×GC, Multivariate analysis, Big data, Data mining, Metabolomics, Petroleomics, Foodomics},
abstract = {The advent of Comprehensive Two-dimensional Gas Chromatography (GC × GC) as a practical and accessible analytical tool had a considerable impact on analytical procedures associated to the so-called “omics” sciences. Specially when GC × GC is hyphenated to mass spectrometers or other multichannel detectors, in a single run it is possible to separate, detect and identify up to thousands of metabolites. However, the resulting data sets are exceedingly complex, and retrieving proper biochemical information from them demands powerful statistical tools to deal effectively with the massive amount of information generated by GC × GC. Nevertheless, the obtention of results valid on a chemical and biological standpoint depends on a deep understanding by the analyst of the fundamentals both of GC × GC and chemometrics. This review focuses on the basics of contemporary, fundamental chemometric tools applied to proccessing of GC × GC obtained from metabolomic, petroleomic and foodomic analyses. Here, we described the fundamentals of pattern recognition methods applied to GC × GC. Also, we explore how different detectors affect data structure and approaches for better data handling. Limitations regarding data structure and deviations from linearity are stressed for each algorithm, as well as their typical applications and expected output.}
}
@article{SHAW2021150,
title = {Supplementing transportation data sources with targeted marketing data: Applications, integration, and internal validation},
journal = {Transportation Research Part A: Policy and Practice},
volume = {149},
pages = {150-169},
year = {2021},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2021.04.021},
url = {https://www.sciencedirect.com/science/article/pii/S0965856421001191},
author = {F. Atiyya Shaw and Xinyi Wang and Patricia L. Mokhtarian and Kari E. Watkins},
keywords = {Consumer data, Targeted marketing data, Travel behavior, Household travel survey, Big data, Third-party data, Travel demand modeling},
abstract = {Unlike many third-party data sources, targeted marketing (TM) data constitute holistic datasets, with disaggregate variables – ranging from socioeconomic and demographic characteristics to attitudes, propensities, and behaviors – available for most individuals in the population. These qualities, along with ease of accessibility and relatively low acquisition costs, make TM data an attractive source for the supplementation of traditional transportation survey data, which are facing growing threats to quality. This paper develops a typology demonstrating ways in which TM data can aid in the design of transport studies, as well as in the augmentation of modeling efforts and policy scenarios, allowing for improved understanding and forecasting of travel-related attributes. However, challenges associated with integrating, validating, and understanding TM variables have resulted in only a few transportation studies that have used these data thus far. In this paper, we provide a transportation discipline-specific resource for TM data, informed by our integration of an extensive TM database with both the National Household Travel Survey (Georgia subset) and a statewide travel behavior survey conducted in Georgia on behalf of the Georgia Department of Transportation. Using the resultant datasets, we validate TM data by means of several approaches, and find that the TM dataset reports gender, age, tenure, race, marital status, and household size with match rates ranging from 70% to 90% relative to both transportation surveys. However, we also identify biases in favor of population segments that may have more longstanding financial/transactional records (e.g., males, homeowners, non-minorities, and older individuals), biases comparable but not identical to those of survey data. While this work suggests wide-ranging implications for the use of TM data in transportation, we caution that flexible and responsible approaches to using these data are critical for staying abreast of evolving privacy regulations that govern third-party data sources such as these.}
}
@article{MULLER20211890,
title = {Production specific language characteristics to improve NLP applications on the shop floor},
journal = {Procedia CIRP},
volume = {104},
pages = {1890-1895},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.319},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121012178},
author = {Marvin Müller and Joachim Metternich},
keywords = {digital shop floor management, shop floor language specifics, natural language processing},
abstract = {A variety of assistance functions have been developed based on the rising data availability on the shop floor and increasing capabilities of artificial intelligence applications. An often-mentioned risk is the low data quality, especially in manual text entries for e.g. deviations or defects. This paper aims to evaluate production specific language characteristics to adjust natural language processing applications. To achieve this goal three industry data sets are analyzed, and the findings are used to improve a recommendation engine for previously solved problems.}
}
@article{CHEN2021134,
title = {An integrated workflow for biomarker development using microRNAs in extracellular vesicles for cancer precision medicine},
journal = {Seminars in Cancer Biology},
volume = {74},
pages = {134-155},
year = {2021},
note = {Extracellular vesicles in cancer, from signaling mechanisms to therapeutic potentials},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2021.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X21000614},
author = {Yu Chen and Tan Wu and Zhongxu Zhu and Hao Huang and Liang Zhang and Ajay Goel and Mengsu Yang and Xin Wang},
keywords = {microRNAs, Extracellular vesicles, Biomarker, Network, Data mining, Cancer},
abstract = {EV-miRNAs are microRNA (miRNA) molecules encapsulated in extracellular vesicles (EVs), which play crucial roles in tumor pathogenesis, progression, and metastasis. Recent studies about EV-miRNAs have gained novel insights into cancer biology and have demonstrated a great potential to develop novel liquid biopsy assays for various applications. Notably, compared to conventional liquid biomarkers, EV-miRNAs are more advantageous in representing host-cell molecular architecture and exhibiting higher stability and specificity. Despite various available techniques for EV-miRNA separation, concentration, profiling, and data analysis, a standardized approach for EV-miRNA biomarker development is yet lacking. In this review, we performed a substantial literature review and distilled an integrated workflow encompassing important steps for EV-miRNA biomarker development, including sample collection and EV isolation, EV-miRNA extraction and quantification, high-throughput data preprocessing, biomarker prioritization and model construction, functional analysis, as well as validation. With the rapid growth of “big data”, we highlight the importance of efficient mining of high-throughput data for the discovery of EV-miRNA biomarkers and integrating multiple independent datasets for in silico and experimental validations to increase the robustness and reproducibility. Furthermore, as an efficient strategy in systems biology, network inference provides insights into the regulatory mechanisms and can be used to select functionally important EV-miRNAs to refine the biomarker candidates. Despite the encouraging development in the field, a number of challenges still hinder the clinical translation. We finally summarize several common challenges in various biomarker studies and discuss potential opportunities emerging in the related fields.}
}
@article{WANG2021504,
title = {Deep time series models for scarce data},
journal = {Neurocomputing},
volume = {456},
pages = {504-518},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.12.132},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221001922},
author = {Qiyao Wang and Ahmed Farahat and Chetan Gupta and Shuai Zheng},
keywords = {Time series analysis, Scarce data, Deep learning models, Functional data analysis},
abstract = {Time series data have grown at an explosive rate in numerous domains and have stimulated a surge of time series modeling research. A comprehensive comparison of different time series models, for a considered data analytics task, provides useful guidance on model selection for data analytics practitioners. Data scarcity is a universal issue that occurs in a vast range of data analytics problems, due to the high costs associated with collecting, generating, and labeling data as well as some data quality issues such as missing data. In this paper, we focus on the temporal classification/regression problem that attempts to build a mathematical mapping from multivariate time series inputs to a discrete class label or a real-valued response variable. For this specific problem, we identify two types of scarce data: scarce data with small samples and scarce data with sparsely and irregularly observed time series covariates. Observing that all existing works are incapable of utilizing the sparse time series inputs for proper modeling building, we propose a model called sparse functional multilayer perceptron (SFMLP) for handling the sparsity in the time series covariates. The effectiveness of the proposed SFMLP under each of the two types of data scarcity, in comparison with the conventional deep sequential learning models (e.g., Recurrent Neural Network, and Long Short-Term Memory), is investigated through mathematical arguments and numerical experiments.}
}