@incollection{FAULKNER2020197,
title = {11 - Operational Matters},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {197-210},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00026-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000267},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Data-centric business models, Business metadata models, Data-centric operational model, Data-centric emergency preparedness},
abstract = {Suppose a system that incorporates data as a key enabler of safety, or the assurance of data, has been developed. This product or system has gained acceptance. It needs to be introduced to its operational environment. The operational concept describes its intended use and specified maintenance regime. This chapter considers how the system can be attuned to a data-centric operational organisation. It investigates appropriate business strategies via a value-mapping approach. This then facilitates the development of an appropriate operational model. Data, shared across multiple locations, may give rise to failures which are both temporally and geographically dispersed. The operational model, therefore, also includes considerations of emergency preparedness for data-centric organisations.}
}
@article{SHA2020195,
title = {A survey of edge computing-based designs for IoT security},
journal = {Digital Communications and Networks},
volume = {6},
number = {2},
pages = {195-202},
year = {2020},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2019.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864818303018},
author = {Kewei Sha and T. Andrew Yang and Wei Wei and Sadegh Davari},
keywords = {Edge computing, Internet of Things (IoT), Security, Architecture, Secure protocols, Firewall, Intrusion detection, Authentication, Authorization, Privacy},
abstract = {Pervasive IoT applications enable us to perceive, analyze, control, and optimize the traditional physical systems. Recently, security breaches in many IoT applications have indicated that IoT applications may put the physical systems at risk. Severe resource constraints and insufficient security design are two major causes of many security problems in IoT applications. As an extension of the cloud, the emerging edge computing with rich resources provides us a new venue to design and deploy novel security solutions for IoT applications. Although there are some research efforts in this area, edge-based security designs for IoT applications are still in its infancy. This paper aims to present a comprehensive survey of existing IoT security solutions at the edge layer as well as to inspire more edge-based IoT security designs. We first present an edge-centric IoT architecture. Then, we extensively review the edge-based IoT security research efforts in the context of security architecture designs, firewalls, intrusion detection systems, authentication and authorization protocols, and privacy-preserving mechanisms. Finally, we propose our insight into future research directions and open research issues.}
}
@article{MONIRUZZAMAN2020106585,
title = {Blockchain for smart homes: Review of current trends and research challenges},
journal = {Computers & Electrical Engineering},
volume = {83},
pages = {106585},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106585},
url = {https://www.sciencedirect.com/science/article/pii/S0045790619316465},
author = {Md. Moniruzzaman and Seyednima Khezr and Abdulsalam Yassine and Rachid Benlamri},
keywords = {Blockchain technology, Smart home, Adaptive requirements, P2P Data marketplace, Automation, Homecare, Smart city, Interoperability},
abstract = {The increasing popularity of smart home applications has introduced unprecedented demand for improving the underlying information technology infrastructure to ensure the transparency, security, and privacy of user data. Blockchain is a promising technology capable of addressing such demand. This paper provides a comprehensive review of current advancements and highlights the major progress of employing blockchain in smart home systems. We first discuss blockchain techniques and the prerequisites of a smart home to adapt blockchain technology. Then, we present various applications of blockchain for smart homes and discuss the most commonly used methods in the literature. We also present two case studies to show how blockchain empowers smart home transactions by eliminating the middleman. This paper also reports on the major challenges pertaining to interoperability protocols, security and privacy, data collection and sharing, data analytics, and latency. Finally, the paper identifies potentially promising areas for future research.}
}
@article{MICHE2020570,
title = {Prospective prediction of suicide attempts in community adolescents and young adults, using regression methods and machine learning},
journal = {Journal of Affective Disorders},
volume = {265},
pages = {570-578},
year = {2020},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2019.11.093},
url = {https://www.sciencedirect.com/science/article/pii/S0165032719311413},
author = {Marcel Miché and Erich Studerus and Andrea Hans Meyer and Andrew Thomas Gloster and Katja Beesdo-Baum and Hans-Ulrich Wittchen and Roselind Lieb},
keywords = {Machine learning, Future suicide attempt, Prediction, Adolescents and young adults, Community sample, Prospective design},
abstract = {Background
The use of machine learning (ML) algorithms to study suicidality has recently been recommended. Our aim was to explore whether ML approaches have the potential to improve the prediction of suicide attempt (SA) risk. Using the epidemiological multiwave prospective-longitudinal Early Developmental Stages of Psychopathology (EDSP) data set, we compared four algorithms—logistic regression, lasso, ridge, and random forest—in predicting a future SA in a community sample of adolescents and young adults.
Methods
The EDSP Study prospectively assessed, over the course of 10 years, adolescents and young adults aged 14–24 years at baseline. Of 3021 subjects, 2797 were eligible for prospective analyses because they participated in at least one of the three follow-up assessments. Sixteen baseline predictors, all selected a priori from the literature, were used to predict follow-up SAs. Model performance was assessed using repeated nested 10-fold cross-validation. As the main measure of predictive performance we used the area under the curve (AUC).
Results
The mean AUCs of the four predictive models, logistic regression, lasso, ridge, and random forest, were 0.828, 0.826, 0.829, and 0.824, respectively.
Conclusions
Based on our comparison, each algorithm performed equally well in distinguishing between a future SA case and a non-SA case in community adolescents and young adults. When choosing an algorithm, different considerations, however, such as ease of implementation, might in some instances lead to one algorithm being prioritized over another. Further research and replication studies are required in this regard.}
}
@article{ZHANG2020e97,
title = {China Kidney Disease Network (CK-NET) 2016 Annual Data Report},
journal = {Kidney International Supplements},
volume = {10},
number = {2},
pages = {e97-e185},
year = {2020},
note = {China Kidney Disease Network (CK-NET) 2016 Annual Data Report},
issn = {2157-1716},
doi = {https://doi.org/10.1016/j.kisu.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2157171620300228},
author = {Luxia Zhang and Ming-Hui Zhao and Li Zuo and Yue Wang and Feng Yu and Hong Zhang and Haibo Wang and Rui Chen and Hong Chu and Xinwei Deng and Lanxia Gan and Bixia Gao and Yifang Jiang and Lili Liu and Jianyan Long and Ying Shi and Zaiming Su and Xiaoyu Sun and Wen Tang and Fang Wang and Huai-Yu Wang and Jinwei Wang and Song Wang and Chao Yang and Dongliang Zhang and Xinju Zhao and Liren Zheng and Zhiye Zhou}
}
@article{ROBLESVELASCO2020106754,
title = {Prediction of pipe failures in water supply networks using logistic regression and support vector classification},
journal = {Reliability Engineering & System Safety},
volume = {196},
pages = {106754},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106754},
url = {https://www.sciencedirect.com/science/article/pii/S095183201930417X},
author = {Alicia Robles-Velasco and Pablo Cortés and Jesús Muñuzuri and Luis Onieva},
keywords = {Water supply networks, Pipe failures, Support vector classification, Logistic regression, Predictive algorithms},
abstract = {Companies in charge of water supply networks are making a huge effort to optimally plan the annual replacements of pipes. This would save costs, enable a higher quality of service and a sustainable management of infrastructure. This study presents a methodology to predict pipe failures in water supply networks. Logistic regression and support vector classification are chosen as predictive systems. Both provide a failure probability associated with each sample which is increasingly required by companies that manage these infrastructures. Furthermore, several pre-processing techniques that seek to improve the accuracy of predictions are addressed. The proposed methodology is illustrated with the real case of a Spanish city. This is an extensive water supply network whose recorded data contains 4,393 pipe failures. The results obtained state that the number of unexpected failures might be significantly reduced. Around 30% of failures could have been prevented by replacing only 3% of the network's pipes per year, which is a realistic and feasible option. As a future line of research, the objective must be to develop a global tool that incorporates the failure probability and its consequence, generating the optimal pipe replacement plan.}
}
@article{BARREDOARRIETA202082,
title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
journal = {Information Fusion},
volume = {58},
pages = {82-115},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
author = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
keywords = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.}
}
@incollection{ITO2020479,
title = {Chapter Twenty-Three - Detection and quantification of microRNAs (miRNAs) and high-throughput miRNA profiling},
editor = {Trygve Tollefsbol},
booktitle = {Epigenetics Methods},
publisher = {Academic Press},
pages = {479-493},
year = {2020},
volume = {18},
series = {Translational Epigenetics},
issn = {25425358},
doi = {https://doi.org/10.1016/B978-0-12-819414-0.00023-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194140000239},
author = {Masaki Ito and Haruto Uchino},
keywords = {MicroRNA, Biofluid, Profiling, qPCR, Microarray, High-throughput sequencing},
abstract = {MicroRNAs (miRNAs) are short, non-coding RNAs that regulate gene expression through epigenetic mechanisms such as stimulating controlled messenger RNA degradation or inhibiting translation initiation. Since their discovery in 1993, thousands of miRNAs have been identified in various species. To date, more than 2,600 mature miRNAs from Homo sapiens have been deposited in the miRNA repository, miRbase 22.1. As one miRNA can silence hundreds of target genes, and vice versa, one gene can be regulated by multiple miRNAs, miRNAs are regarded as fine tuners of various physiological processes, including cell development, differentiation, homeostasis, and apoptosis. Furthermore, increasing evidence suggests that miRNAs have clinical potential as diagnostic biomarkers and therapeutic targets in several diseases. Accurate detection and quantification of miRNAs are crucial in miRNA functional research. To date, there is a wide choice of technologies for this purpose with the advantages and limitations of each technology. qPCR has the highest sensitivity and specificity and is the least costly, but is not suitable for high-throughput miRNA profiling. Microarrays offer comprehensive coverage of known miRNAs, but accuracy is limited. miRNA-seq is ideal for the discovery of novel miRNAs and to distinguish isoforms of miRNAs, but the cost is high and data analysis is complex. In this chapter, therefore, we will review these major technologies for the detection and quantification of miRNAs, especially in blood plasma/serum samples.}
}
@article{CORO2020109187,
title = {A global-scale ecological niche model to predict SARS-CoV-2 coronavirus infection rate},
journal = {Ecological Modelling},
volume = {431},
pages = {109187},
year = {2020},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2020.109187},
url = {https://www.sciencedirect.com/science/article/pii/S0304380020302581},
author = {Gianpaolo Coro},
keywords = {Ecological niche modelling, Coronavirus, SARS-CoV-2, COVID-19, Maximum entropy},
abstract = {COVID-19 pandemic is a global threat to human health and economy that requires urgent prevention and monitoring strategies. Several models are under study to control the disease spread and infection rate and to detect possible factors that might favour them, with a focus on understanding the correlation between the disease and specific geophysical parameters. However, the pandemic does not present evident environmental hindrances in the infected countries. Nevertheless, a lower rate of infections has been observed in some countries, which might be related to particular population and climatic conditions. In this paper, infection rate of COVID-19 is modelled globally at a 0.5∘ resolution, using a Maximum Entropy-based Ecological Niche Model that identifies geographical areas potentially subject to a high infection rate. The model identifies locations that could favour infection rate due to their particular geophysical (surface air temperature, precipitation, and elevation) and human-related characteristics (CO2 and population density). It was trained by facilitating data from Italian provinces that have reported a high infection rate and subsequently tested using datasets from World countries’ reports. Based on this model, a risk index was calculated to identify the potential World countries and regions that have a high risk of disease increment. The distribution outputs foresee a high infection rate in many locations where real-world disease outbreaks have occurred, e.g. the Hubei province in China, and reports a high risk of disease increment in most World countries which have reported significant outbreaks (e.g. Western U.S.A.). Overall, the results suggest that a complex combination of the selected parameters might be of integral importance to understand the propagation of COVID-19 among human populations, particularly in Europe. The model and the data were distributed through Open-science Web services to maximise opportunities for re-usability regarding new data and new diseases, and also to enhance the transparency of the approach and results.}
}
@article{EGGINK202076,
title = {Joining element design and product variety in manufacturing industries},
journal = {Procedia CIRP},
volume = {88},
pages = {76-81},
year = {2020},
note = {13th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 17-19 July 2019, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120303292},
author = {Derk H.D. Eggink and Marco W. Groll},
keywords = {Artificial intelligence, Machine learning, Automation, Design for assembly, Joining elements, Product variety, Design, Engineering, Standardization, Modular design, Process selection, Joining locations, Complexity},
abstract = {Product variety is a growing trend of offering highly configurable products at the cost of inducing complexity in manufacturing. Joining is a key manufacturing process and historically was a paper-based process with incomplete variety documentation. Nowadays, digital joining element design is a substitution of paper for 3D space. Nonetheless, it remains an ambiguous manual task with limited automation, resulting in time-consuming iterative error-prone development trajectories and costly reworks. This contribution addresses the state of the art in joining element design in both research and industry practice. It reviews product variety and its impact on joining processes. The paper identifies a need for integrating product variety into joining element design and it proposes a solution pathway using artificial intelligence methods.}
}
@incollection{WANG202067,
title = {Chapter 3 - Conceptual design driven digital twin configuration},
editor = {Fei Tao and Ang Liu and Tianliang Hu and A.Y.C. Nee},
booktitle = {Digital Twin Driven Smart Design},
publisher = {Academic Press},
pages = {67-107},
year = {2020},
isbn = {978-0-12-818918-4},
doi = {https://doi.org/10.1016/B978-0-12-818918-4.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189184000038},
author = {Yuchen Wang and Lin Liu and Ang Liu},
keywords = {Conceptual design application, digital twin shaping, complexity management, digital twin configuration, design constraints management},
abstract = {Since the first introduction of digital twin (DT), its configuration has been disassembled into five compositions: physical entity (PE), virtual entity (VE), twin data, service, and connection. The development of DT depends on the design features of its physical model and practical use case. Nonetheless, its general configuration can still be developed with conceptual design methodology to reduce factors such as complexity, uncertainties, inconvenience, and couplings. This chapter aims to configure DT using a conceptual design methodology. Following the sequence of customer needs (CNs)–functional requirements (FRs)–design parameters (DPs)–design constraints, PE, VE, and twin data center (TDC) are developed. The classification of FRs, DPs, and constraints will be applied to simplify the development. A degree of reduction is introduced as a constraint for the VE, and the diversity of analysis possible among the PE, VE, and TDC is clarified. Development of service is introduced then with the mapping from CNs and FRs from the served product. For the development of connection, existing connection types among compositions are first introduced. Due to attribute similarity, the four conceptual design factors of connection are discussed together for all connection types. To manage the integration of five compositions, working mode, working sequence, and output ratio are introduced. Finally, to illustrate the introduced theory, a case study of an autonomous vehicle with the hypothetical scenario of traffic congestion, vehicle maintenance, and engine monitoring is presented.}
}
@article{SCHOGGL2020105073,
title = {The narrative of sustainability and circular economy - A longitudinal review of two decades of research},
journal = {Resources, Conservation and Recycling},
volume = {163},
pages = {105073},
year = {2020},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2020.105073},
url = {https://www.sciencedirect.com/science/article/pii/S0921344920303906},
author = {Josef-Peter Schöggl and Lukas Stumpf and Rupert J. Baumgartner},
keywords = {Circular economy, Sustainable development, Thematic mapping, Multiple correspondence analysis, Topic modeling, Historiographic analysis},
abstract = {Circular economy (CE) has gained momentum in the political, economic and scientific fields. The growing popularity of the concept is accompanied by some definitional ambiguities and conceptual uncertainties. In particular, the relationship and contribution of CE to sustainable development (SD) and thus to a more sustainable society is currently under discussion. The purpose of this paper is to contribute to this discussion by providing new insights into the evolution and state of CE research over the past two decades, in general, and its sustainability connotation, in particular. For doing so, a mixed-methods approach was adopted that combines a longitudinal bibliographic network analysis, multiple correspondence analysis and k-means clustering, correlated topic modeling, historiographic citation analysis and a semantic content analysis. The results indicate that the CE literature body can be divided into management and technically-oriented studies that have either a beginning-of-life or an end-of-life focus. Recycling is the most referred to R-strategy, followed by remanufacturing, repair and reuse, which, however, occur one order of magnitude less frequently. CE research and SD were found to exhibit a subset relationship, as only a limited number of environmental aspects is directly addressed. Social aspects form a periphery. The qualitative analysis further portraits the conceptual evolution of the CE-SD relationship between 2000 and 2019 by following the citation network of the 30 most influential CE papers. The results contribute to positioning CE research within the general Sustainable Development debate and to identifying potential, sustainability-related shortcomings and blind spots.}
}
@article{CONNOR2020e03434,
title = {Interorganizational cooperation and supplier performance in high-technology supply chains},
journal = {Heliyon},
volume = {6},
number = {3},
pages = {e03434},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2020.e03434},
url = {https://www.sciencedirect.com/science/article/pii/S2405844020302796},
author = {Neale O’ Connor and Paul Benjamin Lowry and Horst Treiblmaier},
keywords = {Supply chain performance, Buyer-supplier relationships, Relationship marketing theory, Social exchange theory, Collaboration, Electronics industry, Global supply chain, Business, Globalization, Operations management, Technology adoption, Technology management},
abstract = {Never in history have global supply-chain relationships in high-tech electronics firms been more sophisticated, complicated, and almost always tied in some major aspect to China. This research examines how interorganizational (IO) cooperation impacts performance and what role relationship learning and information technology (IT) integration play in the value-creation process for Chinese suppliers in business-to-business (B2B) supply chains. We examine this issue using data collected from face-to-face interviews with supply chain managers and executives from 1,004 Chinese high-tech electronic component suppliers. The results strongly support the hypothesis that IO cooperation improves a supplier's performance regarding both its major customer and overall marketplace. Relationship learning and IT integration are important mediating variables that drive performance. The strongest effect in our study was the influence of IO cooperation on relationship learning. A unique aspect of this study is that it focuses on a large sample of a specific supplier type—high-tech Chinese suppliers. This, combined with the fact that the sampled companies were involved in manufacturing 13 different product groups, greatly increases the generalizability of the results.}
}
@article{BADACH2020106743,
title = {A framework for Air Quality Management Zones - Useful GIS-based tool for urban planning: Case studies in Antwerp and Gdańsk},
journal = {Building and Environment},
volume = {174},
pages = {106743},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106743},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320301013},
author = {Joanna Badach and Dimitri Voordeckers and Lucyna Nyka and Maarten {Van Acker}},
keywords = {Air quality management, Urban ventilation, Urban planning, Urban morphology, GIS-Based analysis},
abstract = {There is a growing recognition of the importance of proper urban design in the improvement of air flow and pollution dispersion and in reducing human exposure to air pollution. However, a limited number of studies have been published so far focusing on the development of standard procedures which could be applied by urban planners to effectively evaluate urban conditions with respect to air quality. To fill this gap, a new approach for the determination of urban Air Quality Management Zones (AQMZs) was proposed and presented based on two case studies: Antwerp, Belgium and Gdańsk, Poland. The main objectives of the study were to 1) formulate a theoretical framework for the management of urban ventilation potential and human exposure to air pollution and to 2) develop methods for its implementation by means of a geographic information system (GIS). As a result of the analysis, the typologies that may be associated with decreased ventilation potential and the areas that require close monitoring due to potential human exposure to air pollution were identified for both cities. It is advocated that delimiting these typologies – combined with investigating local climate, wind and topography conditions and air pollution characteristics – could constitute a preliminary step in the urban planning process aimed at air quality improvement. These methods can be further applied to other urban areas in order to indicate where detailed studies are required and to facilitate the development of planning guidelines. Moreover, the directions for further research and urban planning strategies were discussed.}
}
@incollection{FAULKNER2020247,
title = {15 - DCI Investigation Methodologies},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {247-270},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00031-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000310},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Data-centric Incident investigation techniques, Data-centric AcciMap, Data-centric CAST, Data-centric systems dynamics},
abstract = {Accident investigation methods have been the subject of significant research as the socio-technical nature of accident causes in modern complex safety-critical systems have been identified. Focus on resilience and robustness via safety as practised have also shaped new techniques. In this chapter, incident modelling and how to derive appropriate models are investigated. Techniques such as AcciMap, CAST and FRAM are explained in some detail. Furthermore, ways to employ or adapt these techniques for data-centric systems and organisations are considered. The nature of data-centric systems and organisations means that potentially new avenues of investigation exist. This chapter considers the potential for network theory, systems dynamics and sneak circuit analysis, along with their associated assessment approaches to be employed as part of the investigation.}
}
@article{YACOUB2020117349,
title = {Ultra-high field (10.5 T) resting state fMRI in the macaque},
journal = {NeuroImage},
volume = {223},
pages = {117349},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117349},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920308351},
author = {Essa Yacoub and Mark D. Grier and Edward J. Auerbach and Russell L. Lagore and Noam Harel and Gregor Adriany and Anna Zilverstand and Benjamin Y. Hayden and Sarah R. Heilbronner and Kamil Uğurbil and Jan Zimmermann},
keywords = {Functional connectivity, Rhesus macaque, Resting-state, Spontaneous activity, Functional MRI (fMRI)},
abstract = {Resting state functional connectivity refers to the temporal correlations between spontaneous hemodynamic signals obtained using functional magnetic resonance imaging. This technique has demonstrated that the structure and dynamics of identifiable networks are altered in psychiatric and neurological disease states. Thus, resting state network organizations can be used as a diagnostic, or prognostic recovery indicator. However, much about the physiological basis of this technique is unknown. Thus, providing a translational bridge to an optimal animal model, the macaque, in which invasive circuit manipulations are possible, is of utmost importance. Current approaches to resting state measurements in macaques face unique challenges associated with signal-to-noise, the need for contrast agents limiting translatability, and within-subject designs. These limitations can, in principle, be overcome through ultra-high magnetic fields. However, imaging at magnetic fields above 7T has yet to be adapted for fMRI in macaques. Here, we demonstrate that the combination of high channel count transmitter and receiver arrays, optimized pulse sequences, and careful anesthesia regimens, allows for detailed single-subject resting state analysis at high resolutions using a 10.5 Tesla scanner. In this study, we uncover thirty spatially detailed resting state components that are highly robust across individual macaques and closely resemble the quality and findings of connectomes from large human datasets. This detailed map of the rsfMRI ‘macaque connectome’ will be the basis for future neurobiological circuit manipulation work, providing valuable biological insights into human connectomics.}
}
@article{EICKER2020109954,
title = {On the design of an urban data and modeling platform and its application to urban district analyses},
journal = {Energy and Buildings},
volume = {217},
pages = {109954},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.109954},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819322005},
author = {Ursula Eicker and Verena Weiler and Jürgen Schumacher and Reiner Braun},
keywords = {Software architecture, Urban data platform, Web-based orchestration, Energy demand simulation, CO-emissions reduction},
abstract = {An integrated urban platform is the essential software infrastructure for smart, sustainable and resilient city planning, operation and maintenance. Today such platforms are mostly designed to handle and analyze large and heterogeneous urban data sets from very different domains. Modeling and optimization functionalities are usually not part of the software concepts. However, such functionalities are considered crucial by the authors to develop transformation scenarios and to optimize smart city operation. An urban platform needs to handle multiple scales in the time and spatial domain, ranging from long term population and land use change to hourly or sub-hourly matching of renewable energy supply and urban energy demand. The paper discusses software architecture concepts for data and modeling urban platforms, which allow to analyze and optimize the urban infrastructure with their energy, water and further resources such as food or goods consumption. Building, commerce and industry as well as the transport sector are in the focus of the efficiency and renewable supply analysis. The main driver is to derive zero carbon strategies for cities while including all major sectors of CO2 generation. So far, two software architecture concepts have been implemented and tested, both using a 3D CityGML geometry data model: A workflow management system for city scale building energy modeling using a monthly energy balance calculation method and a micro service orchestration for dynamic building simulation modeling. The GIS based data analysis methodology and building energy workflow modeling method are applied to a case study district in New York City to demonstrate the implementation status and derive CO2 mitigation strategies. The results showed how data gathered from different sources for the relevant sectors can be translated into CO2-emissions. For a district in Brooklyn connected to a large electric substation with electricity monitoring data available, every individual building´s heating and cooling demand was simulated and resulted in a total annual heating demand of 1.72 TWh/a and 0.11 TWh/a of cooling. It could be shown that the building sector cooling demand can by reduced by 63% by a change of set point temperature for cooling and reduced infiltration rate, while reducing the heat demand by 12%. In addition, carbon accounting of the food and wastewater sector was done using the same GIS based modeling framework. The analysis showed that the food related electricity consumption for refrigeration corresponds to about 5% of the overall electricity consumption and requires 2.6 GWh/a for transport. Using food waste in co-digestion of wastewater treatment plants could contribute another 7.4 GWh/a of combined heat and power. Such quantification of demand and energy sources could contribute to prioritize actions for CO2 mitigation strategies in urban areas.}
}
@article{LI2020101789,
title = {Dynamic temporal ADS-B data attack detection based on sHDP-HMM},
journal = {Computers & Security},
volume = {93},
pages = {101789},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101789},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820300742},
author = {Tengyao Li and Buhong Wang and Fute Shang and Jiwei Tian and Kunrui Cao},
keywords = {Attack detection, Hidden markov model, Hierarchical dirichlet process, Automatic dependent surveillance - Broadcast, Air traffic surveillance},
abstract = {For the next generation air traffic surveillance, ADS-B is becoming the primary method to obtain more accurate data with wide coverage, which establishes the foundation for automatic and intelligent air traffic management system. However, ADS-B is designed without sufficient security considerations, transmitting with plain text without integrity and authentication validations. Thus, ADS-B data is in face of various attack threats, which may cause disruptions on system availability and reliability. To eliminate effects of attack behaviours, attack detection is in demand to avoid attack data injecting into decision making flow. Based on hidden Markov model with sticky hierarchical Dirichlet process, the dynamic temporal detection method is proposed to detect multiple attack patterns. Taking advantage of multiple attribute data, the dimensions of data are reduced to one dimension to set up feature sequences. With sticky hierarchical Dirichlet process, the parameters are obtained for hidden Markov model dynamically. Utilizing hidden Markov model, the generative model is established to predict hidden states of ADS-B data sequence. By analysing the contextual deviation information on hidden state sequences, the attack behaviours are discriminated to determine the attack data. By experiments on real ADS-B data, the feasibility and accuracy of the proposed method are validated.}
}
@article{WANG2020104425,
title = {A hybrid model considering spatial heterogeneity for landslide susceptibility mapping in Zhejiang Province, China},
journal = {CATENA},
volume = {188},
pages = {104425},
year = {2020},
issn = {0341-8162},
doi = {https://doi.org/10.1016/j.catena.2019.104425},
url = {https://www.sciencedirect.com/science/article/pii/S0341816219305673},
author = {Yumiao Wang and Luwei Feng and Sijia Li and Fu Ren and Qingyun Du},
keywords = {Landslide susceptibility, Machine learning, GeoSOM, Stacking, Zhejiang Province},
abstract = {Landslides are a type of serious geologic disaster causing great damage to the human environment. Landslide susceptibility mapping is an effective means to reduce landslide risk. However, previous studies have not considered spatial heterogeneity. In this study, a hybrid model considering spatial heterogeneity is designed by integrating GeoSOM and Stacking ensemble methods and is applied to map the landslide susceptibility of Zhejiang Province, China. The GeoSOM method was used to cluster the study area into several homogeneous regions to solve the heterogeneity problem, and each region was assigned a cluster attribute as one of the landslide model inputs. The Stacking ensemble technique was utilized to design a high-performance landslide model by combining three traditional machine learning methods (support vector machine (SVM), artificial neural network (ANN), and gradient-boosting decision tree (GBDT)). We collected 1051 landslide samples and fourteen affecting factors after feature selection. For landslide modelling, the landslides were randomly split into two subsets: 70% samples for training and the rest for validation. Landslide models were assessed by the receiver operating characteristic (ROC) curve and statistical measures. The results indicated that the hybrid model was 0.11–0.135 higher than those of traditional machine learning methods in term of the area under the ROC curve (AUC). In general, this hybrid model can generate high-quality landslide susceptibility maps and help to develop policies that reduce the burden of landslides.}
}
@incollection{CHANG2020193,
title = {Chapter 7 - Clinician Cognition and Artificial Intelligence in Medicine},
editor = {Anthony C. Chang},
booktitle = {Intelligence-Based Medicine},
publisher = {Academic Press},
pages = {193-266},
year = {2020},
isbn = {978-0-12-823337-5},
doi = {https://doi.org/10.1016/B978-0-12-823337-5.00007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823337500007X},
author = {Anthony C. Chang},
keywords = {Cognition, perception, evidence-based medicine, intelligence-based medicine},
abstract = {There is a myriad of reasons for there to be a more intelligent paradigm in medicine with the adoption of artificial intelligence: augmenting clinician knowledge and expertise; decreasing clinical and administrative burden; facilitating care coordination; and mitigating clinician burnout. There is a concomitantly long list of challenges for artificial intelligence adoption that pertain to data and databases, technology, stakeholders, and other issues such as bias and ethics. Clinician cognition will be more important than ever before with the advent of artificial intelligence. The clinician’s brain has several elements: perception, cognition, and operation, and these are used in various proportions depending on the subspecialty. Aspects of clinical medicine such as complexity, uncertainty as well as biases and heuristics will be additional challenges for medicine in the future. Evidence-based medicine and its limitations are discussed in the context of a new paradigm of intelligence-based medicine. Current applications of artificial intelligence in medicine and health care such as medical imaging, decision support, precision medicine, and altered reality, and robotic technology are briefly discussed.}
}
@article{MA202032,
title = {Intelligent algorithm of geotechnical test data based on Internet of Things},
journal = {Computer Communications},
volume = {158},
pages = {32-38},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420301791},
author = {Yawei Ma and Guihong Guo},
keywords = {Internet of Things, Geotechnical tests, Intelligent algorithms, Data analysis},
abstract = {In the geotechnical engineering geological survey industry, geotechnical test data is the basic data for analyzing and evaluating geotechnical engineering geology, forming reports, graphics, and survey reports. It plays an important role in the calculation of the bearing capacity, deformation calculation and physical and mechanical characteristics of the foundation soil. The purpose of this article is to solve the problems of tedious, inefficient and error-prone data collection, processing and analysis of geotechnical test data in the geotechnical and geological surveying industry of geotechnical engineering. By using the BP neural algorithm and selecting the intelligent algorithm, the SVM is used to solve the sample problem. The algorithm establishes an intelligent algorithm for geotechnical test data based on the Internet of Things. Then take the geological characteristics of the Ganjiang River Basin as an example, analyze the geotechnical test data to verify the feasibility of the intelligent algorithm for data analysis. The research results show that the algorithm realizes the automatic collection and processing of geotechnical test data, reduces the tester’s workload and the influence of human factors on the test results, makes up for the shortcomings of traditional acquisition algorithm hardware fixation, and solves the problem of simultaneous multitasking. Difficult problems have promoted the development of innovative experiments.}
}
@incollection{CROSBY2020317,
title = {Chapter 11 - Zero to a trillion: Advancing Earth surface process studies with open access to high-resolution topography},
editor = {Paolo Tarolli and Simon M. Mudd},
series = {Developments in Earth Surface Processes},
publisher = {Elsevier},
volume = {23},
pages = {317-338},
year = {2020},
booktitle = {Remote Sensing of Geomorphology},
issn = {0928-2025},
doi = {https://doi.org/10.1016/B978-0-444-64177-9.00011-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444641779000114},
author = {Christopher J. Crosby and J. Ramón Arrowsmith and Viswanath Nandigam},
keywords = {Geomorphology, Topography, Surface processes, Cyberinfrastructure, Geoinformatics},
abstract = {High-resolution topography (HRT) is a powerful observational tool for studying the Earth's surface, vegetation, and urban landscapes, with broad scientific, engineering, and education-based applications. Submeter resolution imaging is possible when collected with laser and photogrammetric techniques using the ground, air, and space-based platforms. Open access to these data and a cyberinfrastructure platform that enables users to discover, manage, share, and process then increases the impact of investments in data collection and catalyzes scientific discovery. Furthermore, open and online access to data enables broad interdisciplinary use of HRT across academia and in communities such as education, public agencies, and the commercial sector. OpenTopography, supported by the US National Science Foundation, aims to democratize access to Earth science-oriented, HRT data and processing tools. We utilize cyberinfrastructure, including large-scale data management, high-performance computing, and service-oriented architectures to provide efficient web-based visualization and access to large, HRT datasets. OT colocates data with processing tools to enable users to quickly access custom data and derived products for their application, with the ultimate goal of making these powerful data easier to use. OT's rapidly growing data holdings currently include 283 lidar and photogrammetric, point cloud datasets (>1.2 trillion points) covering 236,364km2. As a testament to OT's success, more than 86,000 users have processed over 5 trillion lidar points. This use has resulted in more than 290 peer-reviewed publications across numerous academic domains including Earth science, geography, computer science, and ecology.}
}
@article{WANG2020106578,
title = {Mapping Chemical Earth Program: Progress and challenge},
journal = {Journal of Geochemical Exploration},
volume = {217},
pages = {106578},
year = {2020},
issn = {0375-6742},
doi = {https://doi.org/10.1016/j.gexplo.2020.106578},
url = {https://www.sciencedirect.com/science/article/pii/S037567421930559X},
author = {Xueqiu Wang and Bimin Zhang and Lanshi Nie and Wei Wang and Jian Zhou and Shanfa Xu and Qinhua Chi and Dongsheng Liu and Hanliang Liu and Zhixuan Han and Qingqing Liu and Mi Tian and Baoyun Zhang and Hui Wu and Ruihong Li and Qinghai Hu and Taotao Yan and Yanfang Gao},
keywords = {Mapping Chemical Earth, Global Geochemical Observatory Networks, Geochemical baselines, Progress and challenge},
abstract = {There is a critical need to establish a global geochemical observation network to provide data for monitoring the chemical changes of the Earths near-surface environment. The International Centre on Global-scale Geochemistry, under auspices of UNESCO and Government of China, has initiated an International Scientific Cooperation Project called Mapping Chemical Earth. The project focuses on the establishment of Global Geochemical Observatory Network for documenting baselines and changes of nearly all natural chemical elements in the Earths surface and creating a digital Chemical Earth platform allowing anyone to access vast amounts of geochemical data through the Internet. A total area of about 37 million km2, nearly accounting for 27% of the global land, has been covered by global-/continental-scale sampling. Comparing the data of China, the US, Europe and Australia, the percentage of sites with toxic metals exceeding the risk limits of soil pollution according to “Environmental Quality Standard for Soil of China (GB 15618-1995)” to the total sample sites is 30.9%, 17.1%, 23.5% and 10.9% in Europe, China, USA, and Australia respectively. Comparing the China datasets of 15 years interval sampling between 1994, 1995 and in 2008–2012, toxic metals of As, Cd, Cr, Cu, Hg, Ni, Pb and Zn, particularly Cd at top soils significantly increase from 1990s to 2010s. The proportion of top soil samples exceeding the China Standard risk limit of 0.2 mg/kg Cd increases from 12.2% to 24.9%. The facts show that chemical changes of toxic metals induced by human activities can be well observed using catchment sediment sampling.}
}
@article{CHEN2020113483,
title = {Evaluating the environmental protection strategy of a printed circuit board manufacturer using a Tw fuzzy importance performance analysis with Google Trends},
journal = {Expert Systems with Applications},
volume = {156},
pages = {113483},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113483},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420303079},
author = {Kuen-Suan Chen and Kuo-Ping Lin and Li-Ju Lin},
keywords = {Printed circuit board, Environmental protection, Fuzzy importance-performance analysis, Google Trends},
abstract = {Printed circuit boards (PCBs) are very important materials in consumer electronic products. The process of making PCBs usually uses chemicals, and large quantities of water are needed. PCB manufacturers are devoted to improving the process in order to conform to environmental protection rules and regulations. However, evaluating the performance of environmental protection strategies by quantitative methods is very difficult. Therefore, this study attempts to evaluate the environmental protection strategy of a PCB manufacturer using the novel weakest t-norm (Tw) fuzzy importance-performance analysis with Google Trends (TFIPA-Google). The TFIPA-Google methodology obtain advantages of Tw operations, IPA, and Google Trends, which can handle uncertainty based on a fuzzy matrix of IPA, reduce fuzzy accumulation using the Tw operators, and analyze social media viewpoints using Google Trends. This empirical example based on the TFIPA-Google method shows that the recovered waste material management system should be a priority for PCB manufacturers. Moreover, the TFIPA-Google method can provide more creditable information, based on Tw operations and volume of Google Trends for decision-makers, than a conventional importance-performance analysis (IPA) model.}
}
@article{FENG2020100833,
title = {TEM-based dislocation tomography: Challenges and opportunities},
journal = {Current Opinion in Solid State and Materials Science},
volume = {24},
number = {3},
pages = {100833},
year = {2020},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2020.100833},
url = {https://www.sciencedirect.com/science/article/pii/S1359028620300310},
author = {Zongqiang Feng and Rui Fu and Chengwei Lin and Guilin Wu and Tianlin Huang and Ling Zhang and Xiaoxu Huang},
keywords = {Dislocation, Electron tomography, Crystallography, Technical challenges, Dislocation boundary, Four-dimensional characterization},
abstract = {Dislocation tomography based on transmission electron microscopy (TEM) exhibits excellent capabilities in three dimensional (3D) visualization of various dislocation structures but still suffers from poor quantification and coupling between the geometry and crystallography of dislocations. In the present paper, we review the research on 3D quantitative characterization of dislocation structures using TEM-based dislocation tomography and stereo pair methods, and briefly introduce a novel TEM-based tomographic crystallography method which can simultaneously and quantitatively characterize the geometric and crystallographic features of dislocations. We summarize some technical problems and challenges in the workflow of TEM-based dislocation tomography, including image contrast optimization, irradiation damage, image processing, reconstruction algorithms as well as dislocation segmentation and identification. We further discuss the potential applications of TEM-based dislocation tomography and envisage several promising approaches for developing an advanced four-dimensional (4D) dislocation characterization technique.}
}
@article{GKIONIS2020158,
title = {A marine geoarchaeological investigation for the cultural anthesis and the sustainable growth of Methoni, Greece},
journal = {Journal of Cultural Heritage},
volume = {42},
pages = {158-170},
year = {2020},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2019.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1296207419300809},
author = {Panagiotis Gkionis and George Papatheodorou and Maria Geraga and Elias Fakiris and Dimitris Christodoulou and Konstantinia Tranaka},
keywords = {Maritime archaeology, Marine geophysics, Hydrography, Bathymetry, Site evolution, Underwater cultural heritage},
abstract = {The ‘Evolved GE.N.ESIS Project’ highlights the underwater cultural heritage resources off the coast of Methoni, Greece that could locally drive sustainable socioeconomic growth. An integrated marine geophysical survey, a hydrographic survey, and a GNSS survey were conducted off Methoni, recording six historic wreck sites, artefacts, the ruins of a submerged prehistoric settlement, and the town's ancient harbour/breakwater, as well as the geophysical properties of the underwater environment. The preliminary project results present bathymetric surfaces, backscatter intensity and magnetic maps, drawings, and seismic reflection profiles of the underwater antiquities and of the seabed, all fused in a 3D geographical platform. The results also shed light on the archaeological potential of the site, the nearshore physical processes, and their effect on the underwater archaeological resources. The project outcomes have shown that the establishment of an underwater archaeological park and diving sites at the cultural heritage sites will support cultural tourism development in the area and will have a positive impact on local socioeconomic development. The underwater archaeological park should comply with the basic principles of a site management plan – one that is established in the context of an integrated coastal management plan that identifies the maritime synergies or conflicts among human activities, archaeological resources, and the local environment, and utilises the 3D synthesis of marine knowledge from the project outcomes as a decision-making tool.}
}
@article{SCHUH2020222,
title = {Creation of digital production twins for the optimization of value creation in single and small batch production},
journal = {Procedia CIRP},
volume = {93},
pages = {222-227},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.125},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307629},
author = {Günther Schuh and Christoph Kelzenberg and Jan Wiese and Niklas Kessler},
keywords = {Single, small batch production, Industry 4.0, Digitization, Digital twin},
abstract = {Due to their specific industry characteristics, companies in single and small batch production encounter completely different challenges than companies in series production. The lack of repetitive effects and thus the resulting lack of transparency, complicate the optimization of manufacturing processes. However, the realization of digital production twins promises the exploitation of existing potentials. The digital production twin represents a virtual image of the production process and, in addition to increased transparency regarding the occurring processes, enables data-supported decision making through the usage of individualized digital applications. This paper focusses on the development of a methodology for the creation and implementation of digital production twins in single and small batch production companies.}
}
@article{LAWAREE2020101761,
title = {A scoping review of knowledge syntheses in the field of evaluation across four decades of practice},
journal = {Evaluation and Program Planning},
volume = {79},
pages = {101761},
year = {2020},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2019.101761},
url = {https://www.sciencedirect.com/science/article/pii/S0149718919302605},
author = {Justin Lawarée and Steve Jacob and Mathieu Ouimet},
keywords = {Scoping review, Evaluation, Quality appraisal, Evidence-based policy, Knowledge synthesis},
abstract = {This scoping review of 62 knowledge syntheses published in evaluation-focused journals between 1979 and May 2018 provides a portrait of synthesis practices and their evolution in the mainstream of the field of evaluation. Concerns surrounding the production of knowledge syntheses to answer policy questions are not new in the field of evaluation. However, during this last decade, knowledge synthesis methods have expanded as a means to go beyond the limits and constraints of singular evaluations. This scoping review reveals and discusses two key issues with regards to the expansion of knowledge synthesis practices within the field of evaluation: the diversity—and muddling— of methodological practices and synthesis designs, and the frequent omission of quality appraisals.}
}
@article{TANWAR2020102407,
title = {Blockchain-based electronic healthcare record system for healthcare 4.0 applications},
journal = {Journal of Information Security and Applications},
volume = {50},
pages = {102407},
year = {2020},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2019.102407},
url = {https://www.sciencedirect.com/science/article/pii/S2214212619306155},
author = {Sudeep Tanwar and Karan Parekh and Richard Evans},
keywords = {Blockchain, Healthcare systems, Security, Chaincode, Electronic healthcare records},
abstract = {Modern healthcare systems are characterized as being highly complex and costly. However, this can be reduced through improved health record management, utilization of insurance agencies, and blockchain technology. Blockchain was first introduced to provide distributed records of money-related exchanges that were not dependent on centralized authorities or financial institutions. Breakthroughs in blockchain technology have led to improved transactions involving medical records, insurance billing, and smart contracts, enabling permanent access to and security of data, as well as providing a distributed database of transactions. One significant advantage of using blockchain technology in the healthcare industry is that it can reform the interoperability of healthcare databases, providing increased access to patient medical records, device tracking, prescription databases, and hospital assets, including the complete life cycle of a device within the blockchain infrastructure. Access to patients’ medical histories is essential to correctly prescribe medication, with blockchain being able to dramatically enhance the healthcare services framework. In this paper, several solutions for improving current limitations in healthcare systems using blockchain technology are explored, including frameworks and tools to measure the performance of such systems, e.g., Hyperledger Fabric, Composer, Docker Container, Hyperledger Caliper, and the Wireshark capture engine. Further, this paper proposes an Access Control Policy Algorithm for improving data accessibility between healthcare providers, assisting in the simulation of environments to implement the Hyperledger-based eletronic healthcare record (EHR) sharing system that uses the concept of a chaincode. Performance metrics in blockchain networks, such as latency, throughput, Round Trip Time (RTT). have also been optimized for achieving enhanced results. Compared to traditional EHR systems, which use client-server architecture, the proposed system uses blockchain for improving efficiency and security.}
}
@article{ZIS2020107697,
title = {Ship weather routing: A taxonomy and survey},
journal = {Ocean Engineering},
volume = {213},
pages = {107697},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.107697},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820306879},
author = {Thalis P.V. Zis and Harilaos N. Psaraftis and Li Ding},
keywords = {Ship weather routing, Voyage optimization, Maritime transport, Speed optimization},
abstract = {Ship weather routing has seen considerably increasing attention in recent years in both academia and industry. Problems in this area consider finding the optimal path and sailing speed for a given voyage considering the environmental conditions of wind and waves. The objectives typically consider minimizing operating costs, fuel consumption, or risk of passage. This paper presents a survey of weather routing and voyage optimization research in maritime transportation, explaining the main methodological approaches, and the key disciplines that are dealing with this problem. The main methodologies used to solve the weather routing problem include the isochrone method, dynamic programming, calculus of variations, the use of pathfinding algorithms and heuristics, while in recent years artificial intelligence and machine learning applications have also risen. Most of these methodologies are well established, and have not changed significantly throughout the years, although applications with a combination of these methods have been used. A taxonomy is subsequently presented based on the discipline, application area, methodological approach, and other important parameters. Considering the steep increase in the number of research papers published in recent years, this paper also seeks to propose future research topics in the field. The paper highlights the need to standardize the reporting of savings through weather routing, to facilitate comparisons between methodologies, which could be achieved through the creation of benchmarking instances.}
}
@article{ROMIC2020135875,
title = {Modelling spatial and temporal variability of water quality from different monitoring stations using mixed effects model theory},
journal = {Science of The Total Environment},
volume = {704},
pages = {135875},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.135875},
url = {https://www.sciencedirect.com/science/article/pii/S004896971935870X},
author = {Davor Romić and Annamaria Castrignanò and Marija Romić and Gabriele Buttafuoco and Marina {Bubalo Kovačić} and Gabrijel Ondrašek and Monika Zovko},
keywords = {Water quality monitoring, Sea water intrusion, Nitrate, Karst aquifer, Monitoring program},
abstract = {Polder-type agricultural catchments within river deltas are specific land formations which management is highly demanding from several aspects. The close contact with the coastal sea may additionally affect the quality of adjacent marine environment. This study uses the case of the Lower Neretva Valley (LNV) to test the efficiency of applying Linear Mixed Effect (LME) theory in modelling spatial and temporal variations of surface and groundwater quality within a polder-type agricultural catchment. The methodology uses linear regressive techniques while taking into account spatial and temporal autocorrelation of residuals. The objective was to assess and model the spatial and temporal variability of the quality of surface- and ground-waters, in order to predict the impact of natural processes and human activities. A dataset of physicochemical properties of surface and groundwater quality of the LNV, recorded monthly in the period 2009–2017, was used to model the spatial and temporal variations of water salinity and nitrate concentrations. The network of water quality monitoring sites covers four polders on five thousand hectares of agricultural land, including the following types of water bodies: river streams, lateral canals, pumping stations, drainage canals and groundwater. The method of data analysis, based on LME theory with correlated spatial and temporal residuals, takes also into account the heteroscedasticity of the variance associated with each type of water quality monitoring station. The two Linear Mixed Effects models proposed for the prediction of electrical conductivity and nitrate concentration in the surface waters and groundwater, proved to be efficient at adequately reproducing the heterogeneity and complexity of the study area. However, the prediction of nitrate concentration in the water was not equally satisfactory of the one of electrical conductivity due to the large variation in nutrient concentrations. To improve spatial prediction, the density of monitoring network should be increased.}
}
@article{TANG2020514,
title = {π-Hub: Large-scale video learning, storage, and retrieval on heterogeneous hardware platforms},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {514-523},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1931427X},
author = {Jie Tang and Shaoshan Liu and Jie Cao and Dawei Sun and Bolin Ding and Jean-Luc Gaudiot and Weisong Shi},
keywords = {Robotic cloud, Video retrieval, Internet of Things, Heterogeneous platform},
abstract = {The burgeoning of Internet of Things (IoT) and camera-equipped mobile devices contributes a tremendous amount of video data generated at the edge of the network. At the same time, we have witnessed the fast deployment of many video-based application services, such as plate recognition for public safety, intelligent transportation, Industry 4.0 and so on. The success of these services, in turn, requires large-scale video data being learned, stored, and retrieved in a more efficient way. A generic software and hardware framework for large-scale IoT video analysis and service support is still missing. To address this challenge, we present π-Hub, PerceptIn’s robotic cloud solution which supports large-scale video data analysis, storage, and query by implementing the learn-store-retrieve paradigm. Interestingly, we found that among the learning, storage, and retrieval services each of them stresses one type of resources on heterogeneous computing servers, i.e., GPU, CPU, and Memory, respectively, therefore it is extremely cost-efficient to co-locate these services together to fully utilize the resources. In addition, several optimization techniques for data writing, reading, and data reduction are proposed and evaluated. The evaluation results show that these techniques improve the performance of the learning, storage and retrieval services significantly as well as notably reduce the cost of the system. We also verify π-Hub’s scalability by reliably running a 1000-machine deployment to support up to one million users. Finally, we conclude the paper by discussing several lessons learned from this study and future work.}
}
@article{ZHANG2020120665,
title = {Toward intelligent construction: Prediction of mechanical properties of manufactured-sand concrete using tree-based models},
journal = {Journal of Cleaner Production},
volume = {258},
pages = {120665},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.120665},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620307125},
author = {Junfei Zhang and Dong Li and Yuhang Wang},
keywords = {Concrete, Manufactured sand, Compressive strength, Tensile strength, GUI, Machine learning},
abstract = {Depletion of river sand due to large-scale concrete production has caused many environmental problems. To address this issue, river sand can be replaced with sand manufactured from waste deposits. To facilitate manufactured-sand concrete production, this study proposes three tree-based models: one individual model (regression tree (RT)), and two ensemble models (random forest (RF) and gradient boosted regression tree (GBRT)) to predict its mechanical properties, such as uniaxial compressive strength (UCS), and splitting tensile strength (STS). These tree-based models were trained and tested on a dataset collected from previous literature. In addition, to understand the importance of each input variable on the mechanical properties of manufactured-sand concrete, the variable importance is calculated using the RF algorithm. The results show that the highest correlation coefficients are achieved by GBRT in predicting UCS (0.9887) and STS (0.9666), which respectively increase by 3.0%–10.8% and 16.0%–21.6% in comparison with the models in previous literature. The mechanical properties UCS and STS are highly sensitive to the curing age with relative importance of 36.8% and 40.3%, respectively. To facilitate the application of the tree-based models in predicting mechanical properties of manufactured-sand concrete, a graphical user interface has been designed in this study.}
}
@article{SU2020107003,
title = {Scalable logo detection by self co-learning},
journal = {Pattern Recognition},
volume = {97},
pages = {107003},
year = {2020},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2019.107003},
url = {https://www.sciencedirect.com/science/article/pii/S0031320319303061},
author = {Hang Su and Shaogang Gong and Xiatian Zhu},
keywords = {Object detection, Logo recognition, Logo dataset, Web data mining, Self-Learning, Co-Learning},
abstract = {Existing logo detection methods usually consider a small number of logo classes, limited images per class and assume fine-gained object bounding box annotations. This limits their scalability to real-world dynamic applications. In this work, we tackle these challenges by exploring a web data learning principle without the need for exhaustive manual labelling. Specifically, we propose a novel incremental learning approach, called Scalable Logo Self-co-Learning (SL2), capable of automatically self-discovering informative training images from noisy web data for progressively improving model capability in a cross-model co-learning manner. Moreover, we introduce a very large (2,190,757 images of 194 logo classes) logo dataset “WebLogo-2M” by designing an automatic data collection and processing method. Extensive comparative evaluations demonstrate the superiority of SL2 over the state-of-the-art strongly and weakly supervised detection models and contemporary web data learning approaches.}
}
@article{HUANG2020102277,
title = {Dynamic calculation of ship exhaust emissions based on real-time AIS data},
journal = {Transportation Research Part D: Transport and Environment},
volume = {80},
pages = {102277},
year = {2020},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2020.102277},
url = {https://www.sciencedirect.com/science/article/pii/S1361920919311617},
author = {Liang Huang and Yuanqiao Wen and Yimeng Zhang and Chunhui Zhou and Fan Zhang and Tiantian Yang},
keywords = {Ship exhaust emission, Real-time calculation, Dynamic inventory, AIS data stream},
abstract = {The activity-based methodology is becoming an increasing way to calculate exhaust emissions from ships in a port. Existing studies make great effort to build and analyze ship emission inventory in a variety of ports by applying this method to historical ship trajectory data. This kind of static emission inventory however, cannot meet the needs of real-time ship emission monitoring. This article proposes a method of dynamic calculation of ship exhaust emissions based on real-time ship trajectory data. Firstly, real-time ship AIS messages are partitioned into continuous data blocks and go through a series of pre-processing operations, including trajectory extraction, association and interpolation. Ship activity parameters are then determined by database querying and regression analysis based on ship attributes. Subsequently, an improved activity-based methodology is employed to estimate exhaust emissions from ships in a distributed way. Based on the grid model, regional ship exhaust emissions can be statistically and dynamically calculated by the spatial allocation of all ship emissions. In a case study, a real-time monitoring platform for ship exhaust emissions in Shenzhen port is developed to demonstrate the effectiveness of the proposed method.}
}
@article{KARAMI2020100013,
title = {Smart transportation planning: Data, models, and algorithms},
journal = {Transportation Engineering},
volume = {2},
pages = {100013},
year = {2020},
issn = {2666-691X},
doi = {https://doi.org/10.1016/j.treng.2020.100013},
url = {https://www.sciencedirect.com/science/article/pii/S2666691X20300142},
author = {Zahra Karami and Rasha Kashef},
keywords = {Smart transportation, Machine learning, Time-series, Prediction Models},
abstract = {By developing cities and increasing population, smart transportation becomes an essential component of modern societies. Extensive research activities using machine learning techniques and several industrial needs have paved the way for the emerging field of smart transportation. This paper presents data, methods, and models that are essential for intelligent planning of transportation. In particular, the current data sources for gathering information to control or forecast traffic are described, connected Vehicles (CVs) that bring smart and green transportation to modern life is also discussed. Clustering Analysis as an effective unsupervised machine learning method in trip distribution and generation and traffic zone division is discussed in the paper. Various machine learning techniques and models that use time series prediction are introduced in this paper including ARIMA, Kalman filtering, Holt winters'Exponential smoothing, Random walk, KNN Algorithm, and Deep Learning. Finally, a discussion on the main advantages and drawbacks of these models, as well as the business adoption of the forecasting models are presented.}
}
@article{GOKHBERG2020102476,
title = {Advanced text-mining for trend analysis of Russia’s Extractive Industries},
journal = {Futures},
volume = {115},
pages = {102476},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.102476},
url = {https://www.sciencedirect.com/science/article/pii/S0016328719303386},
author = {Leonid Gokhberg and Ilya Kuzminov and Elena Khabirova and Thomas Thurner},
keywords = {Extractive industries, Mineral sector, New technologies, Technology foresight, Horizon scanning, Text-mining, Science, Technology and innovation, Russian Federation, Critical technologies},
abstract = {The world economy relies on access to industrial metals, oil and gas for maintaining its critical industrial infrastructure. Although demand is likely to remain high, the most accessible deposits have been depleted. Future capacity growth will be facilitated through further technological developments. Russia as a leading producer is paying great attention to strengthening its competitive edge in global markets. This paper reports on a large-scale technology foresight study of the Russian extractive sector (including oil and gas), which combined expert-based foresight activities with statistical analyses and text-mining techniques based on artificial intelligence and machine learning technologies. The presented methodology helped to link the technologies to dominant discussions (e.g. climate change vs rural development) and to flag key trends. Furthermore, quantitative estimates can be identified quickly. The study’s methodology should function as an example for similar studies to support policy planning and investment decisions based on text-mining techniques.}
}
@article{SUGITA2020204,
title = {Recent developments and advances in atopic dermatitis and food allergy},
journal = {Allergology International},
volume = {69},
number = {2},
pages = {204-214},
year = {2020},
issn = {1323-8930},
doi = {https://doi.org/10.1016/j.alit.2019.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1323893019301595},
author = {Kazunari Sugita and Cezmi A. Akdis},
keywords = {Atopic dermatitis, Barrier, Food allergy, Precision medicine, Mechanisms and pathophysiology},
abstract = {This review highlights recent advances in atopic dermatitis (AD) and food allergy (FA), particularly on molecular mechanisms and disease endotypes, recent developments in global strategies for the management of patients, pipeline for future treatments, primary and secondary prevention and psychosocial aspects. During the recent years, there has been major advances in personalized/precision medicine linked to better understanding of disease pathophysiology and precision treatment options of AD. A greater understanding of the molecular and cellular mechanisms of AD through substantial progress in epidemiology, genetics, skin immunology and psychological aspects resulted in advancements in the precision management of AD. However, the implementation of precision medicine in the management of AD still requires the validation of reliable biomarkers, which will provide more tailored management, starting from prevention strategies towards targeted therapies for more severe diseases. Cutaneous exposure to food via defective barriers is an important route of sensitization to food allergens. Studies on the role of the skin barrier genes demonstrated their association with the development of IgE-mediated FA, and suggest novel prevention and treatment strategies for type 2 diseases in general because of their link to barrier defects not only in AD and FA, but also in asthma, chronic rhinosinusitis, allergic rhinitis and inflammatory bowel disease. The development of more accurate diagnostic tools, biomarkers for early prediction, and innovative solutions require a better understanding of molecular mechanisms and the pathophysiology of FA. Based on these developments, this review provides an overview of novel developments and advances in AD and FA, which are reported particularly during the last two years.}
}
@article{20201134,
title = {Abstracts},
journal = {Ophthalmology},
volume = {127},
number = {9},
pages = {1134-1138},
year = {2020},
issn = {0161-6420},
doi = {https://doi.org/10.1016/j.ophtha.2020.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S0161642020306886}
}
@incollection{ALPHONSE2020295,
title = {Chapter 12 - Blockchain and Internet of Things: An Overview},
editor = {Saravanan Krishnan and Valentina E. Balas and E. Golden Julie and Y. Harold Robinson and S. Balaji and Raghvendra Kumar},
booktitle = {Handbook of Research on Blockchain Technology},
publisher = {Academic Press},
pages = {295-322},
year = {2020},
isbn = {978-0-12-819816-2},
doi = {https://doi.org/10.1016/B978-0-12-819816-2.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128198162000125},
author = {A. Sherly Alphonse and M.S. Starvin},
keywords = {blockchain, IoT, security, smart home, transaction, bitcoin},
abstract = {Blockchain is the technique that underpins the digital currencies like bitcoin, litecoin, and ethereum, and so on. The data that are stored within a block depends on the category of blockchain. A Bitcoin block has data about the sender, receiver, and total number of bitcoins to be sent. Bitcoin is a currency that is used digitally and globally as a system currency. It permits people to send and receive cash through the internet, even to a less-trusted person. Money can also be exchanged without knowing the real identity. The mathematics of cryptography form the fundamentals of bitcoin’s security. Cryptocurrency wallets also get hacked usually, but the hack is not the usual one, outstanding at the blockchain itself. Chain is a company that creates cryptographic ledgers like blockchains for security in financial services. The tools helped banks and credit-card companies to securely store and manage their assets. The convergence of Internet of Things (IoT) and blockchain is of significant use in the industrial world. This chapter gives an overview of IoT, its applications and challenges, blockchain technology, its pros and cons, the convergence of blockchain technology with IoT, the different architectures, evaluation techniques, the advanced scenarios, and use-cases.}
}
@article{POLENGHI2020245,
title = {Data taxonomy to manage information and data in Maintenance Management},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {245-250},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301865},
author = {A. Polenghi and I. Roda and M. Macchi and A. Pozzetti},
keywords = {information, data, taxonomy, maintenance, Asset Management, industry},
abstract = {Nowadays Maintenance Management (MM) is covering a primary role for competitiveness in manufacturing. The advent of Asset Management (AM), in which MM is a core function, enlarges the scope MM was used to. Besides, digitalization has brought a vast amount of information and data sources that MM may exploit to improve its processes and asset-related decision-making. This evolution of MM has brought a lot of opportunities but also various criticalities about information and data management. Data models are envisioned to provide significant support to this end. However, a common reference data taxonomy is needed for the correct development of data models. This work aims at exploring how the data taxonomy could help in addressing the current criticalities by synthesizing most information and data classes that support MM. The data taxonomy, along with other elements, like data models, effectively support companies in improving the management of their information and data. The usefulness of a data taxonomy is proved thanks to action research in a company within the automotive sector aiming at improving the MM process.}
}
@article{KULKARNI2020104798,
title = {Preventing shipping accidents: Past, present, and future of waterway risk management with Baltic Sea focus},
journal = {Safety Science},
volume = {129},
pages = {104798},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104798},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520301958},
author = {Ketki Kulkarni and Floris Goerlandt and Jie Li and Osiris Valdez Banda and Pentti Kujala},
keywords = {Maritime risk management, Maritime safety, Accident prevention, Bibliometric analysis, Baltic Sea Region},
abstract = {Various national maritime authorities and international organizations show strong interest to implement risk management processes to decision making for shipping accident prevention in waterway areas. There is a recurring need for approaches, models, and tools for identifying, analysing, and evaluating risks of shipping accidents, and for strategies for preventively managing these in (inter-)organizational settings. This article presents a comprehensive review of academic work in this research area, aiming to identify patterns, trends, and gaps, serving as a guide for future research and development, with a particular focus on the Baltic Sea Region. To understand the links between research in the Baltic Sea area and the global community, a bibliometric analysis is performed, focusing on identifying dominant narratives and social networks in the research community. Articles from the Baltic Sea area are subsequently analysed more in-depth, addressing issues like the nature of the academic work done, the risk management processes involved, and the underlying accident theories. From the results, patterns in the historical evolution of the research domain are detected, and insights about current trends gained, which are used to identify future avenues for research.}
}
@article{ZENG2020104,
title = {A hybrid deep forest approach for outlier detection and fault diagnosis of variable refrigerant flow system},
journal = {International Journal of Refrigeration},
volume = {120},
pages = {104-118},
year = {2020},
issn = {0140-7007},
doi = {https://doi.org/10.1016/j.ijrefrig.2020.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0140700720303455},
author = {Yuke Zeng and Huanxin Chen and Chengliang Xu and Yahao Cheng and Qijian Gong},
keywords = {Variable refrigerant flow system, Outlier detection, Fault diagnosis, Deep forest model, Système à débit de frigorigène variable, Méthode de détection des valeurs aberrantes, Diagnostic des défaillances, Modèle de forêt neuronale profonde},
abstract = {This paper presents a hybrid deep forest approach for outlier detection and fault diagnosis. Isolation forest algorithm is combined with Pearson's correlation coefficient for outlier detection. The physical significance of outliers detected by the proposed algorithm is explained by origin analysis, which is rarely mentioned in existing studies. In addition, a novel non-neural network deep learning model-cascade forest model is proposed to fault diagnosis of HVAC system for the first time to achieve high precision accuracy in low-dimensional features. The proposed approach is validated with the refrigerant charge fault of VRF system. The results show that the isolation forest algorithm can improve the performance of fault diagnosis model and the mainly outliers of VRF system are defrosting data. The IF-CF model has short operation time, and high accuracy in low-dimensional features. When the dimension drops to 6, the accuracy of the IF-CF model is 94.16%, which is 5.26%, 10.02%, 5.87% and 3.34% higher than the IF-MLP, IF-BPNN, IF-SVM and IF-LSTM models, respectively. Moreover, IF-CF model does not require complex hyper-parameter optimization strategy because its maximum accuracy difference in different hyper-parameters is 2.04%. This study is enlightening which may inspire the potential of outlier detection technology and deep learning in HVAC field.}
}
@incollection{2020345,
title = {Index},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {345-360},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.20001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072200017}
}
@article{TIAN2020104173,
title = {Establishment and evaluation of a multicenter collaborative prediction model construction framework supporting model generalization and continuous improvement: A pilot study},
journal = {International Journal of Medical Informatics},
volume = {141},
pages = {104173},
year = {2020},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104173},
url = {https://www.sciencedirect.com/science/article/pii/S1386505620301362},
author = {Yu Tian and Weiguo Chen and Tianshu Zhou and Jun Li and Kefeng Ding and Jingsong Li},
keywords = {Multicenter collaborative research, Model generalization, Transfer learning, Prognosis prediction},
abstract = {Background and Objective
In recent years, an increasing number of clinical prediction models have been developed to serve clinical care. Establishing a data-driven prediction model based on large-scale electronic health record (EHR) data can provide a more empirical basis for clinical decision making. However, research on model generalization and continuous improvement is insufficiently focused, which also hinders the application and evaluation of prediction models in real clinical environments. Therefore, this study proposes a multicenter collaborative prediction model construction framework to build a prediction model with greater generalizability and continuous improvement capabilities while preserving patient data security and privacy.
Materials and Methods
Based on a multicenter collaborative research network, such as the Observational Health Data Sciences and Informatics (OHDSI), a multicenter collaborative prediction model construction framework is proposed. Based on the idea of multi-source transfer learning, in each source hospital, a base classifier was trained according to the model research setting. Then, in the target hospital with missing calibration data, a prediction model was established through weighted integration of base classifiers from source hospitals based on the smoothness assumption. Moreover, a passive-aggressive online learning algorithm was used for continuous improvement of the prediction model, which can help to maintain a high predictive performance to provide reliable clinical decision-making abilities. To evaluate the proposed prediction model construction framework, a prototype system for colorectal cancer prognosis prediction was developed. To evaluate the performance of models, 70,906 patients were screened, including 70,090 from 5 US hospital-specific datasets and 816 from a Chinese hospital-specific dataset. The area under the receiver operating characteristic curve (AUC) and the estimated calibration index (ECI) were used to evaluate the discrimination and calibration of models.
Results
Regarding the colorectal cancer prognosis prediction in our prototype system, compared with the reference models, our model achieved a better performance in model calibration (ECI = 9.294 [9.146, 9.441]) and a similar ability in model discrimination (AUC = 0.783 [0.780, 0.786]). Furthermore, the online learning process provided in this study can continuously improve the performance of the prediction model when patient data with specified labels arrive (the AUC value increased from 0.709 to 0.715 and the ECI value decreased from 13.013 to 9.634 after 650 patient instances with specified labels from the Chinese hospital arrived), enabling the prediction model to maintain a good predictive performance during clinical application.
Conclusions
This study proposes and evaluates a multicenter collaborative prediction model construction framework that can support the construction of prediction models with better generalizability and continuous improvement capabilities without the need to aggregate multicenter patient-level data.}
}
@article{ZHENG2020122347,
title = {A temporally-calibrated method for crowdsourcing based mapping of intra-urban PM2.5 concentrations},
journal = {Journal of Cleaner Production},
volume = {269},
pages = {122347},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122347},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620323945},
author = {Zhong Zheng and Bin Zou and Yongqian Wang and Shenxin Li and Yanghua Gao and Shiqi Yang},
keywords = {Crowdsourcing observation, Data calibration, Fine particular matter, Spatial mapping, Temporal variation},
abstract = {As a primary air pollutant, fine particulate matter (PM2.5) is increasingly attracting attention. Crowdsourcing observations based methods are thought to be the best solutions for identifying the spatio-temporal distribution of PM2.5 in intra-urban areas. However, inconsistent timing in the collection of crowdsourced data has typically been ignored in previous studies. To address this issue, a temporally calibrated method (TCM) was introduced in this study. By interpolating TCM-estimated observations using the inverse distance weighted (IDW) method, variations of PM2.5 concentrations across the urban areas of Changsha City were captured. The results demonstrate that TCM can efficiently resolve the inconsistent timing defects of raw crowdsourcing observations (R2 was 0.73 and the RMSE was 7.65 μg/m3). Furthermore, PM2.5 distributions developed using TCM-based interpolations are of a finer spatial scale than those developed from raw observations at crowdsourcing locations. With a lack of funds to build sufficient stationary monitoring sites, developing crowdsourcing observation-based technology is the most promising solution for revealing intra-urban PM2.5 variations at a higher spatio-temporal- resolution.}
}
@article{CAO202082,
title = {Deep learning-based remote and social sensing data fusion for urban region function recognition},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {163},
pages = {82-97},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S092427162030054X},
author = {Rui Cao and Wei Tu and Cuixin Yang and Qing Li and Jun Liu and Jiasong Zhu and Qian Zhang and Qingquan Li and Guoping Qiu},
keywords = {Urban function recognition, Multi-modal data fusion, Remote sensing, Social sensing, Deep learning},
abstract = {Urban region function recognition is key to rational urban planning and management. Due to the complex socioeconomic nature of functional land use, recognizing urban region function in high-density cities using remote sensing images alone is difficult. The inclusion of social sensing has the potential to improve the function classification performance. However, effectively integrating the multi-source and multi-modal remote and social sensing data remains technically challenging. In this paper, we have proposed a novel end-to-end deep learning-based remote and social sensing data fusion model to address this issue. Two neural network based methods, one based on a 1-dimensional convolutional neural network (CNN) and the other based on a long short-term memory (LSTM) network, have been developed to automatically extract discriminative time-dependent social sensing signature features, which are fused with remote sensing image features extracted via a residual neural network. One of the major difficulties in exploiting social and remote sensing data is that the two data sources are asynchronous. We have developed a deep learning-based strategy to address this missing modality problem by enforcing cross-modal feature consistency (CMFC) and cross-modal triplet (CMT) constraints. We train the model in an end-to-end manner by simultaneously optimizing three costs, including the classification cost, the CMFC cost and the CMT cost. Extensive experiments have been conducted on publicly available datasets to demonstrate the effectiveness of the proposed method in fusing remote and social sensing data for urban region function recognition. The results show that the seemingly unrelated physically sensed image data and social activities sensed signatures can indeed complement each other to help enhance the accuracy of urban region function recognition.}
}
@article{PAN2020103107,
title = {BIM log mining: Learning and predicting design commands},
journal = {Automation in Construction},
volume = {112},
pages = {103107},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103107},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519312701},
author = {Yue Pan and Limao Zhang},
keywords = {BIM, Log data mining, Long short-term memory neural network, Design command prediction},
abstract = {This paper develops a framework to learn and predict design commands based upon building information modeling (BIM) event log data stored in Autodesk Revit journal files, which has the potential to improve the modeling efficiency. BIM design logs, which automatically keep detailed records on the modeling process, are the basis of data acquisition and data mining. Long Short-Term Memory Neural Network (LSTM NN), as a probabilistic deep learning model for learning sequential data with varying lengths from logs, is established to provide designers with predictions about the possible design command class in the next step. To demonstrate the feasibility of this method, a case study runs at large design logs over 4 GB from an international design firm for command class prediction. To begin with, useful data retrieved from logs is cleaned and saved in a 320 MB Comma Separated Values (CSV) file with totally 352,056 lines of commands over 289 projects. Subsequently, various design commands are categorized into 14 classes according to their effects and given numerical labels, which are then fed into LSTM NN for training and testing. As a result, the overall accuracy of this particular case study can reach 70.5% in the test set, which outperforms some classical machine learning methods, like k nearest neighbor, random forest and support vector machine. This research contributes to applying a probabilistic LSTM NN with optimal parameters to learn features from designers' subjective behaviors effectively and predict the next possible design command class intelligently towards automation of the design process. Moreover, the three most possible command classes will be offered as the recommendations under the assumption that the correct class tends to appear owning the top three highest probabilities, which can possibly enhance the reliability of predictions.}
}
@article{RIVERALOPEZ202086,
title = {A permutational-based Differential Evolution algorithm for feature subset selection},
journal = {Pattern Recognition Letters},
volume = {133},
pages = {86-93},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2020.02.021},
url = {https://www.sciencedirect.com/science/article/pii/S016786552030060X},
author = {Rafael Rivera-López and Efrén Mezura-Montes and Juana Canul-Reich and Marco Antonio Cruz-Chávez},
keywords = {Machine learning, Evolutionary algorithms, Wrapper scheme},
abstract = {This paper describes a permutational-based Differential Evolution algorithm implemented in a wrapper scheme to find a feature subset to be applied in the construction of a near-optimal classifier. In this approach, the relevance of a feature chosen to build a better classifier is represented through its relative position in an integer-valued vector, and by using a permutational-based mutation operator, it is possible to create new feasible candidate solutions only. Furthermore, to provide a controlled diversity rate in the population, a straightforward repair-based recombination operator is utilized to evolve a population of candidate solutions. Unlike the other approaches in the existing literature using integer-valued vectors and requiring a predefined subset size, in this approach, this size is determined by an additional element included in the encoding scheme, allowing to find an adequate feature subset size to each specific dataset. Experimental results show that this approach is an effective way to create more accurate classifiers as they are compared with those obtained by other similar approaches.}
}
@article{MEHRANI2020102101,
title = {Sampling Rate Prediction of Biosensors in Wireless Body Area Networks using Deep-Learning Methods},
journal = {Simulation Modelling Practice and Theory},
volume = {105},
pages = {102101},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2020.102101},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X20300393},
author = {Mohammad Mehrani and Iman Attarzadeh and Mehdi Hosseinzadeh},
keywords = {WBAN, Sampling Frequency, Prediction, Energy Optimality, Data Traffic, ANFIS, LSTM},
abstract = {In this paper, we propose a scheme which aims at determining and forecasting sampling rate of active biosensors in Wireless Body Area Networks (WBANs). In this regard, from the first round until a certain round, the sampling rate of biosensors would be determined. Accordingly, we introduce our modified Fisher test, develop Spline interpolation method, introduce three main parameters namely information of patient's activity, patient's risk and pivot biosensor's value. Then, by employing these parameters plus introduced statistical and mathematical based strategies, the sampling rate of the active biosensors in the next round would be determined at the end of each entire round. After reaching a pre-denoted round the sampling rate of biosensors would be predicted through forecasting methods. In this regard, we develop two machine learning based techniques namely Adaptive Neuro Fuzzy Inference System (ANFIS) and Long Short Term Memory (LSTM) and compare them with four famous similar techniques. In addition to using forecasted sampling frequencies of the biosensors for controlling their energy expenditure, these forecasted values would also be used to forecast patient's status in the future. This is the first work in this domain that uses current information of the patient to determine adaptive sampling frequency and then employs the time series of determined sampling frequencies to forecast the patient's status and biosensors energy expenditure in the future. For estimating our schemes, we simulated them in MATLAB R2018b software and compared the results with a number of similar schemes. Based on the simulation results, the proposed schemes are capable to reduce data traffic by 81%, decrease energy consumption of the network by 73% while having the capability of predicting sampling rate of biosensors with 97% accuracy.}
}
@article{SARKAR2020100409,
title = {Comparison of data storage and analysis throughput in the light of high energy physics experiment MACE},
journal = {Astronomy and Computing},
volume = {33},
pages = {100409},
year = {2020},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2020.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2213133720300639},
author = {D. Sarkar and Mahesh P. and Padmini S. and N. Chouhan and C. Borwankar and A.K. Bhattacharya and A.K. Tickoo and R.C. Rannot},
keywords = {Gamma ray astronomy, NoSQL, ROOT, LINQ, Type provider, Online analysis},
abstract = {High Energy Physics (HEP) Experiments produce large amounts of data. The data produced in these experiments are in the range of terabytes and petabytes. The explosion of data has posed a challenge in data capture, storage, data integrity, searching, querying, visualization and analysis. This has led to the development of domain-specific file formats like FITS, HDF5, analysis frameworks like ROOT, storage architectures like relational and NoSQL databases, and parallel and distributed data handling methodologies. In this paper, we investigate the read–write performance by comparing the HEP domain-specific framework ROOT and a NoSQL database Berkeley DB in the context of a gamma-ray Cerenkov experiment to meet the requirement of real-time data analysis. Major Atmospheric Cerenkov Experiment (MACE) is a 21 m gamma-ray telescope set up by BARC at HANLE, India. It will generate a few hundred gigabytes of data per observational night. Aiming at the real-time analysis of the data we have developed a dynamic reading mechanism by implementing a binary type provider for data retrieval from the Berkeley DB database. Data analysis queries were performed and compared both in ROOT files using ROOT query methods and in Berkeley DB using Language Integrated Queries (LINQ). Finally, a generic framework facilitating the online analysis of the data is proposed in this paper.}
}
@incollection{2020465,
title = {Index},
editor = {Evelyn J.S. Hovenga and Cherrie Lowe},
booktitle = {Measuring Capacity to Care Using Nursing Data},
publisher = {Academic Press},
pages = {465-476},
year = {2020},
isbn = {978-0-12-816977-3},
doi = {https://doi.org/10.1016/B978-0-12-816977-3.09992-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128169773099925}
}
@article{WYDRA2020101242,
title = {Measuring innovation in the bioeconomy – Conceptual discussion and empirical experiences},
journal = {Technology in Society},
volume = {61},
pages = {101242},
year = {2020},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2020.101242},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X19305020},
author = {Sven Wydra},
keywords = {Innovation indicators, Bioeconomy, Monitoring, Patents, Innovation},
abstract = {Innovations in the bioeconomy are expected to provide new solutions to major economic, societal and ecological challenges like resource depletion, food insecurity or climate change. However, information about innovation activities in the bioeconomy and its outcomes is scattered and more systematic measurement efforts are useful for policy making to assess its impact and whether objectives are met. This article provides an overview of information needs and data availability for innovation indicators. Furthermore, data for key input and throughput indicators are presented and discussed for the bioeconomy in Germany. The data indicates a rather strong role of Germany for publications and patents. However, the commercial success remains unclear, because of current limitations in information availability about the output and outcome of innovations efforts. Here, the most critical information gap in exist. In order to improve this situation additional data collection such as innovation survey for the bioeconomy would be needed.}
}
@article{KSIAZEK20201512,
title = {Development of novel ensemble model using stacking learning and evolutionary computation techniques for automated hepatocellular carcinoma detection},
journal = {Biocybernetics and Biomedical Engineering},
volume = {40},
number = {4},
pages = {1512-1524},
year = {2020},
issn = {0208-5216},
doi = {https://doi.org/10.1016/j.bbe.2020.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0208521620300991},
author = {Wojciech Książek and Mohamed Hammad and Paweł Pławiak and U. Rajendra Acharya and Ryszard Tadeusiewicz},
keywords = {HCC, Stacking learning, Ensemble method, Machine learning, Genetic algorithm},
abstract = {The most common type of liver cancer is hepatocellular carcinoma (HCC), which begins in hepatocytes. The HCC, like most types of cancer, does not show symptoms in the early stages and hence it is difficult to detect at this stage. The symptoms begin to appear in the advanced stages of the disease due to the unlimited growth of cancer cells. So, early detection can help to get timely treatment and reduce the mortality rate. In this paper, we proposes a novel machine learning model using seven classifiers such as K-nearest neighbor (KNN), random forest, Naïve Bayes, and other four classifiers combined to form stacking learning (ensemble) method with genetic optimization helping to select the features for each classifier to obtain highest HCC detection accuracy. In addition to preparing the data and make it suitable for further processing, we performed the normalization techniques. We have used KNN algorithm to fill in the missing values. We trained and evaluated our developed algorithm using 165 HCC patients collected from Coimbra’s Hospital and University Centre (CHUC) using stratified cross-validation techniques. There are total of 49 clinically significant features in this dataset, which are divided into two groups such as quantitative and qualitative groups. Our proposed algorithm has achieved the highest accuracy and F1-score of 0.9030 and 0.8857, respectively. The developed model is ready to be tested with huge database and can be employed in cancer screening laboratories to aid the clinicians to make an accurate diagnosis.}
}
@article{HAWKS2020106426,
title = {Narrative review of social media as a research tool for diet and weight loss},
journal = {Computers in Human Behavior},
volume = {111},
pages = {106426},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106426},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220301795},
author = {Jessica R. Hawks and Hala Madanat and Eric R. Walsh-Buhi and Sheri Hartman and Atsushi Nara and David Strong and Cheryl Anderson},
abstract = {This narrative review examined the following research questions: 1) What are the characteristics and outcomes of social media-based diet/weight loss studies to date? 2) What are the methodological characteristics of social media-based diet/weight loss studies? 3) What research strengths and limitations exist among social media-based diet/weight loss studies? We conducted a narrative review of studies related to diet, weight loss, and social media. Out of 37 included articles, most focused exclusively on Facebook (n = 13, 35%) or Twitter (n = 12, 32%). Of 20 studies (54%) analyzing social media content, most analyzed textual content (n = 13, 65%). About half of studies (n = 20, 54%) had no guiding theoretical framework, and about one-quarter used Social Cognitive Theory (SCT) (n = 10, 27%). Studies designs used were non-experimental (n = 15, 41%), experimental (n = 12, 32%), qualitative (n = 8, 22%), and mixed methods (n = 2, 5%). Intervention research thus far has consisted mostly of inadequately controlled and powered pilot studies. More rigorous randomized controlled trials should be conducted that build on data gathered from pilot research. Further research on how exposure to/interaction with diet/weight loss social media translates to individual behavior change will aid in addressing the US's obesity epidemic.}
}
@article{WANG2020107591,
title = {A novel method for joint optimization of the sailing route and speed considering multiple environmental factors for more energy efficient shipping},
journal = {Ocean Engineering},
volume = {216},
pages = {107591},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.107591},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820305965},
author = {Kai Wang and Jiayuan Li and Lianzhong Huang and Ranqi Ma and Xiaoli Jiang and Yupeng Yuan and Ngome A. Mwero and Rudy R. Negenborn and Peiting Sun and Xinping Yan},
keywords = {Speed optimization, Route optimization, Energy consumption, CO emission, Energy system},
abstract = {Energy saving and emission reduction have attracted a great deal of attention in the maritime industry. The optimization of a ship's energy efficiency can reduce energy consumption and CO2 emissions effectively. However, most of the available studies only focus on either the sailing speed or route optimization, and the interaction between speed and route under the influence of multiple environmental factors was not accounted properly. In this paper, a novel joint optimization method of the sailing route and speed, which considers the interaction between route and speed as well as multiple environmental factors, is proposed to fully exploit the energy efficiency's potential. Moreover, a joint optimization model of the sailing route and speed, which is based on an energy consumption model that considers multiple environmental factors, is established. Next, a solution algorithm for the joint optimization model is investigated in order to achieve joint decision-making with regard to the sailing route and speed. Finally, a case study is conducted that demonstrates the effectiveness of the proposed method. The results show that the proposed method can achieve the optimal sailing route and speed under complex environmental conditions, as well as a reduction in fuel consumption and CO2 emissions of about 4%.}
}
@article{LIU2020105790,
title = {Automatic identification of fossils and abiotic grains during carbonate microfacies analysis using deep convolutional neural networks},
journal = {Sedimentary Geology},
volume = {410},
pages = {105790},
year = {2020},
issn = {0037-0738},
doi = {https://doi.org/10.1016/j.sedgeo.2020.105790},
url = {https://www.sciencedirect.com/science/article/pii/S0037073820302050},
author = {Xiaokang Liu and Haijun Song},
keywords = {Microfossils, Minerals, Sedimentary structures, Machine learning, Transfer learning},
abstract = {Petrographic analysis based on microfacies identification in thin sections is widely used in sedimentary environment interpretation and paleoecological reconstruction. Fossil recognition from microfacies is an essential procedure for petrographers to complete this task. Distinguishing the morphological and microstructural diversity of skeletal fragments requires extensive prior knowledge of fossil morphotypes in microfacies and long training sessions under the microscope. This requirement engenders certain challenges for sedimentologists and paleontologists, especially novices. However, a machine classifier can help address this challenge. In this study, we collected a microfacies image dataset comprising both public data from 1133 references and our own materials (including a total of 30,815 images of 22 fossil and abiotic grain groups). We employed a high-performance workstation to implement four classic deep convolutional neural networks, which have proven to be highly efficient in computer vision. Our framework uses a transfer learning technique, which reuses the pre-trained parameters that are trained on a larger ImageNet dataset as initialization for the network to achieve high accuracy with low computing costs. We obtained up to 95% of the top one and 99% of the top three test accuracies in the Inception ResNet v2 architecture. The machine classifier exhibited 0.99 precision on minerals such as dolomite and pyrite. Although it had some difficulty on samples having similar morphologies, such as bivalve, brachiopod, and ostracod, it nevertheless obtained 0.88 precision. Our machine learning framework demonstrates high accuracy with reproducibility and bias avoidance that is comparable to those of human classifiers. Its application can thus eliminate much of the tedious, manually intensive efforts by human experts conducting routine identification.}
}
@article{GONCALVES2020100164,
title = {Operations research models and methods for safety stock determination: A review},
journal = {Operations Research Perspectives},
volume = {7},
pages = {100164},
year = {2020},
issn = {2214-7160},
doi = {https://doi.org/10.1016/j.orp.2020.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2214716020300543},
author = {João N.C. Gonçalves and M. {Sameiro Carvalho} and Paulo Cortez},
keywords = {Safety stocks, Operations research, Systematic literature review, Inventory management, Supply chain},
abstract = {In supply chain inventory management it is generally accepted that safety stocks are a suitable strategy to deal with demand and supply uncertainty aiming to prevent inventory stock-outs. Safety stocks have been the subject of intensive research, typically covering the problems of dimensioning, positioning, managing and placement. Here, we narrow the scope of the discussion to the safety stock dimensioning problem, consisting in determining the proper safety stock level for each product. This paper reports the results of a recent in-depth systematic literature review (SLR) of operations research (OR) models and methods for dimensioning safety stocks. To the best of our knowledge, this is the first systematic review of the application of OR-based approaches to investigate this problem. A set of 95 papers published from 1977 to 2019 has been reviewed to identify the type of model being employed, as well as the modeling techniques and main performance criteria used. At the end, we highlight current literature gaps and discuss potential research directions and trends that may help to guide researchers and practitioners interested in the development of new OR-based approaches for safety stock determination.}
}
@article{SHULLA2020121735,
title = {Channels of collaboration for citizen science and the sustainable development goals},
journal = {Journal of Cleaner Production},
volume = {264},
pages = {121735},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.121735},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620317820},
author = {Kalterina Shulla and Walter {Leal Filho} and Jan Henning Sommer and Amanda {Lange Salvia} and Christian Borgemeister},
keywords = {Citizen science, SDGs, 2030 agenda for sustainable development},
abstract = {Citizen Science, known as the participation of individuals and groups in scientific processes, is an increasingly growing discipline, which can contribute for the achievement of the Sustainable Development Goals. The UN Agenda 2030 for Sustainable Development is all-inclusive, where every contribution is valid. Participation, partnerships, education, sustainable living and global citizenship, all of which can build on Citizen Science activities, are crucial for the Sustainable Development Goals. In this context, this study aims at exploring several collaboration channels for Citizen Science-related activities and the Agenda 2030. Challenges and critical aspects are discussed based on the opinions of practitioners collected through a comprehensive online survey. Furthermore, recommendations for future involvement are given on a framework of interactions at different levels for Citizen Science and the Agenda 2030.}
}
@article{JIANG20201,
title = {Toward optimal participant decisions with voting-based incentive model for crowd sensing},
journal = {Information Sciences},
volume = {512},
pages = {1-17},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309156},
author = {Nan Jiang and Dong Xu and Jie Zhou and Hongyang Yan and Tao Wan and Jiaqi Zheng},
keywords = {Participant decision, Incentive model, Voting mechanism, Crowd sensing},
abstract = {With the rapid development of crowd sensing in sensing applications, excellent incentive mechanisms are playing an increasingly important role. However, most existing solutions do not fully consider the ability of participants to perform tasks, the degree to which they complete tasks, or the credibility of the task sensing results. In this paper, we aim to develop an incentive model based on voting mechanism for crowd sensing(abbreviated as CIBV), which includes three algorithms. The first is a participant decision algorithm (PDA) that adopts a reverse auction model and comprehensively considers candidate execution capability; the second is the budget balance and extra reward algorithm (BBER); the third is the evaluate algorithm (EA) to be applied at the end of sensing tasks. Compared with previous work, the experimental results show that in our proposed CIBV model, each task is performed by multiple participants, and each participant can perform multiple tasks, our model can greatly improve the participants’ execution ability value and provide the platform with the ability to control the process of selecting participants.}
}
@article{JIN202035,
title = {Towards autonomic data management for staging-based coupled scientific workflows},
journal = {Journal of Parallel and Distributed Computing},
volume = {146},
pages = {35-51},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520303312},
author = {Tong Jin and Fan Zhang and Qian Sun and Melissa Romanus and Hoang Bui and Manish Parashar},
keywords = {Autonomic computing, Data management, In-situ, Data staging, HPC workflow},
abstract = {Emerging scientific workflows running at extreme scale are composed of multiple applications that interact and exchange data at runtime. While staging-based approaches, e.g. in-situ/in-transit processing, are promising, dynamic behaviors (e.g. data volumes and distributions) in coupled applications and varying resource constraints at runtime make the efficient use of these techniques challenging. Addressing these challenges requires fundamental changes in the way that workflows are executed at runtime. Specifically, it is required to monitor the operating environment and running applications, and then adapt and tune the application behaviors and resource allocations at runtime while meeting the data management requirements and constraints. In this paper, we propose a policy-based autonomic data management (ADM) approach that can adaptively respond at runtime to dynamic data management requirements. We first formulate the schematic abstraction of this ADM approach including its conceptual model and system elements. Then, we explore the realization of ADM runtime and demonstrate how to achieve adaptations in a cross-layer manner with pre-defined autonomic policies. We also prototype our ADM approach and evaluate its performance on the Intrepid IBM-BlueGene and Titan Cray-XK7 systems using Chombo-based AMR applications and a visualization application. The experimental results demonstrate its effectiveness in meeting user defined objectives and accelerating overall scientific discovery.}
}
@article{BURRESS2020102076,
title = {Exploring data literacy via a librarian-faculty learning community: A case study},
journal = {The Journal of Academic Librarianship},
volume = {46},
number = {1},
pages = {102076},
year = {2020},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2019.102076},
url = {https://www.sciencedirect.com/science/article/pii/S0099133319304069},
author = {Theresa Burress and Emily Mann and Tina Neville},
keywords = {Librarian-faculty collaboration, Data literacy, Higher education, Centers for teaching and learning, Faculty learning communities},
abstract = {Faculty learning communities (FLCs) are year-long professional development opportunities available at many higher education institutions in the United States. While the literature reflects some librarian engagement with FLCs, it seems limited primarily to areas of traditional librarian expertise such as information literacy and outreach. This article describes a case study of a librarian-facilitated FLC focused on data literacy, which resulted in the development of a teaching toolkit, library-led data literacy instruction, and ongoing collaborations between librarians and faculty. The FLC structure proved to be a valuable framework that facilitated collaborative learning in topics relevant to both disciplinary faculty and librarians. In addition, the tangible work products produced by the FLC serve to advance the strategic, curricular goals of the university while giving the library an opportunity to showcase its value in the academic lifecycle.}
}
@incollection{CUMMINS2020121,
title = {Chapter 5 - Machine learning in digital health, recent trends, and ongoing challenges},
editor = {Debmalya Barh},
booktitle = {Artificial Intelligence in Precision Health},
publisher = {Academic Press},
pages = {121-148},
year = {2020},
isbn = {978-0-12-817133-2},
doi = {https://doi.org/10.1016/B978-0-12-817133-2.00005-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128171332000057},
author = {Nicholas Cummins and Zhao Ren and Adria Mallol-Ragolta and Björn Schuller},
keywords = {Artificial intelligence, Machine learning, Deep learning, Explainability},
abstract = {As a result of a growing and aging population, as well as an increase in associated costs, there is a continual stretching of health care services worldwide. This issue is motivating researchers all over the world to optimize medical resources by utilizing digital tools explicitly addressed to health care and well-being. One of the main fields of research in this regard is artificial intelligence (AI), the endowment of machines with human-like learning, reasoning, and decision-making abilities. Combined with high penetration of sensor-based technologies—such as smartphones and wearables—in modern society, advancements in AI mean we are entering a new age of health care. Soon, we will be able to monitor vital signs and lifestyle habits, in real-time, in such a way that will help clinicians to monitor patients’ evolution and progress in a nonintrusive and remote manner. This chapter intended to be an introductory, higher-level overview, of the core concepts relating to the branch of AI known as machine learning (ML). Readers are introduced to the ML train-test pipeline and given an overview of commonly used ML algorithms. The chapter finishes by discussing challenges that need to be overcome to help fully realize the potential of ML in everyday digital health settings.}
}
@article{WANG2020114561,
title = {A novel improved model for building energy consumption prediction based on model integration},
journal = {Applied Energy},
volume = {262},
pages = {114561},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.114561},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920300738},
author = {Ran Wang and Shilei Lu and Wei Feng},
keywords = {Building energy prediction, Model integration, Data mining, Robustness},
abstract = {Building energy consumption prediction plays an irreplaceable role in energy planning, management, and conservation. Constantly improving the performance of prediction models is the key to ensuring the efficient operation of energy systems. Moreover, accuracy is no longer the only factor in revealing model performance, it is more important to evaluate the model from multiple perspectives, considering the characteristics of engineering applications. Based on the idea of model integration, this paper proposes a novel improved integration model (stacking model) that can be used to forecast building energy consumption. The stacking model combines advantages of various base prediction algorithms and forms them into “meta-features” to ensure that the final model can observe datasets from different spatial and structural angles. Two cases are used to demonstrate practical engineering applications of the stacking model. A comparative analysis is performed to evaluate the prediction performance of the stacking model in contrast with existing well-known prediction models including Random Forest, Gradient Boosted Decision Tree, Extreme Gradient Boosting, Support Vector Machine, and K-Nearest Neighbor. The results indicate that the stacking method achieves better performance than other models, regarding accuracy (improvement of 9.5%–31.6% for Case A and 16.2%–49.4% for Case B), generalization (improvement of 6.7%–29.5% for Case A and 7.1%-34.6% for Case B), and robustness (improvement of 1.5%–34.1% for Case A and 1.8%–19.3% for Case B). The proposed model enriches the diversity of algorithm libraries of empirical models.}
}
@incollection{TON2020659,
title = {27 - Beach and nearshore monitoring techniques},
editor = {Derek W.T. Jackson and Andrew D. Short},
booktitle = {Sandy Beach Morphodynamics},
publisher = {Elsevier},
pages = {659-687},
year = {2020},
isbn = {978-0-08-102927-5},
doi = {https://doi.org/10.1016/B978-0-08-102927-5.00027-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081029275000278},
author = {Anne Ton and Mark Lee and Sander Vos and Matthijs Gawehn and Kees {den Heijer} and Stefan Aarninkhof},
keywords = {monitoring, equipment, instruments, in situ, remote sensing, data management},
abstract = {Monitoring of beach and nearshore environments is essential for obtaining better insights into the functioning of the coastal zone. It has driven the understanding of these environments and worked beneficially alongside modelling studies. Hydrodynamics, water quality, and sedimentological and morphological processes can be observed and quantified through field measurements. A successful monitoring programme has a well-considered design, reflecting the interests of all parties involved and balancing scientific requirements (such as measuring scales and resolutions in time and space) against available budgets and resources. The key to utilizing the monitoring result is a data management system that accommodates the FAIR principles – Findable, Accessible, Interoperable and Reusable – for data handling. For the future of coastal monitoring we foresee that recent technological developments will help define the way; particularly miniaturized sensors, data transmission advances, and remote sensing techniques. These developments, especially if embedded in high-profile, open-access coastal observatories, can pave the way towards now-casting of coastal systems.}
}
@incollection{MISHRA2020245,
title = {Chapter 9 - An insight of Internet of Things applications in pharmaceutical domain},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar},
booktitle = {Emergence of Pharmaceutical Industry Growth with Industrial IoT Approach},
publisher = {Academic Press},
pages = {245-273},
year = {2020},
isbn = {978-0-12-819593-2},
doi = {https://doi.org/10.1016/B978-0-12-819593-2.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128195932000091},
author = {Sushruta Mishra and Anuttam Dash and Brojo Kishore Mishra},
keywords = {Pharmaceutical domain, Internet of Things (IoT), RFID, medical nursing system, body area network},
abstract = {IoT (Internet of Things) refers to the interconnectivity of physical nodes (devices) embedded with actuators, sensors, network connectivity, electronics, etc., for the purpose of facilitating easy exchange of data in real-time scenario and thereby providing connectivity. Due to its applications in the practical world, IoT has the potential for completely revolutionizing the pharmaceutical industry by automating and enabling remote patient monitoring, drug discovery, its access, and much more. With the integration of IoT into the increasing digitalization in this data-centric world, the pharmaceutical area consists of an immense amount of opportunities for large-scale disruption and overhauls in the industrial sector. Today’s era demands easier and fast access to health-care services. In the same manner the companies working in the pharmaceutical sector are also supposed to ensure the secure and safe transfer of drugs, better planned shipment and delivery, and clinical consequences. In order to facilitate speedy operations, it is required to harvest data in a way that will be both effective and well-organized, supplemented by obligatory analytics. In this chapter, we have briefly described the IoT trends and methodology that are being used in the pharmaceutical sector. Various aspects revolving around the role of IoT in the pharmaceutical industry have been discussed here. A sample case study has also been highlighted in the subsequent section of the chapter. In this case study a smart system for medical nursing based on wireless sensor networks, near Field communication (NFC), and radio-frequency identification technology has been discussed. This system not only promotes nursing home conditions but also upgrades the drug supply accuracy.}
}
@incollection{CLOUTIER2020335,
title = {Health Services and Service Restructuring},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {335-345},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10390-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955103907},
author = {Denise S. Cloutier and Daniel Brendle-Moczuk},
keywords = {Access, Beveridge model, biomedical, Bismarck model, complementary and alternative medicine, equity, globalization, health systems, neoliberalism, restructuring, service delivery},
abstract = {Health and health systems are central concepts within the subdisciplines of medical and health geography. Today, research in these fields explores comparative patterns of health and disease within and across geographic regions, and the homes, neighborhoods, and community contexts in which lives are lived, care is provided, and health is shaped. At regional and local scales, goals and values influence how health systems are organized, structured, and funded to address issues of availability, accessibility, equity, equality, and service use. In the past several decades, globalization patterns and trends and political ideologies such as neoliberalism have had monumental and pervasive effects on health systems, spurring debates about whether health care is an individual or collective responsibility and how to promote greater efficiency and effectiveness in health care regarding an overall affordability crisis in certain types of care. All of this has led to various reform and restructuring strategies. Emerging areas of inquiry for geographers interested in health and health systems include opportunities for enhanced spatial analysis and quantitative modeling related to improving access to care; utilization of large and linked reservoirs of data for planning and resource allocation purposes; the uptake and implications of technology for improvements in care; medical tourism; explorations of various “therapeutic landscapes” on health; and an enduring interest in the body as a site of care.}
}
@article{ZHANG2020105584,
title = {High-resolution satellite imagery applications in crop phenotyping: An overview},
journal = {Computers and Electronics in Agriculture},
volume = {175},
pages = {105584},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105584},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920309194},
author = {Chongyuan Zhang and Afef Marzougui and Sindhuja Sankaran},
keywords = {Phenomics, Crop improvement, Plant breeding, Low-orbiting satellite, Remote sensing},
abstract = {Over the past ten years, plant phenotyping technologies that utilize sensing and data mining approaches to estimate crop traits in a high-throughput and objective manner, have been evaluated and applied in different crop improvement and breeding programs. Multiple platforms, from proximal to unmanned aerial systems based remote sensing, have been developed and applied to increase the throughput, efficiency, and objectivity during field phenotyping. In recent years, the development and availability of high-resolution satellite imagery from low-orbit satellites have offered yet another opportunity for phenotyping applications. This review demonstrates the applications of satellite imagery in agricultural production and crop phenotyping and suggests plant traits that can be evaluated using high-resolution satellite imagery. The review summarizes the merits (e.g. rapid/automated data capture from larger and multiple field sites) and challenges (e.g. cloud occlusion) of satellite-based phenotyping in crop breeding programs, and discusses future perspectives/opportunities of high-resolution satellite imagery as a phenotyping tool. High-resolution satellite imagery can serve as a phenotyping tool for the assessment of crop varieties, thus assisting plants breeders in the process of selecting high-yielding, stress (abiotic and biotic) tolerant variety that can contribute to addressing world food demand amidst climate change.}
}
@article{VANDERDUIN2020102637,
title = {Dutch doubts and desires. Exploring citizen opinions on future and technology},
journal = {Futures},
volume = {124},
pages = {102637},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2020.102637},
url = {https://www.sciencedirect.com/science/article/pii/S0016328720301269},
author = {Patrick {van der Duin} and Paul Lodder and Dhoya Snijders},
keywords = {Technology, Opinion, Survey, The Netherlands, Citizen},
abstract = {Because the future belongs to all of us, not just to experts or professionals, knowing what people think about the future is not only interesting, but also relevant to the functioning of modern democracies. This study about what Dutch people think about the future and about technology shows, among other things, that they are generally positive about technology and science, but more gloomy when it comes to the future of society. People find it difficult to think about the (distant) future and spend little time reflecting on it. It is precisely the current speed and complexity of global changes that ensures a revaluation of the historical context when the world was still manageable. Another outcome is that the respondents are relatively down-to-earth. Representations of the future that are vastly different from our current state of affairs can count on little support. Dutch realism is also reflected in a long list of technologies that the respondents would rather not see developed.}
}
@article{VISSER2020102068,
title = {What users of global risk indicators should know},
journal = {Global Environmental Change},
volume = {62},
pages = {102068},
year = {2020},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2020.102068},
url = {https://www.sciencedirect.com/science/article/pii/S0959378019315389},
author = {H. Visser and S. {de Bruin} and A. Martens and J. Knoop and W. Ligtvoet},
keywords = {Campbell's law, Composite indicators, Global data sets, Metadata, Uncertainty and sensitivity analysis},
abstract = {There is growing public awareness of global risks that are related to land degradation, poverty, food security, migration flows, natural disasters and levels of violence and conflict. In the past decades, a wealth of performance databases has become available, and these are used to quantify those risks and to influence governance globally. We name the monitoring of the 17 Sustainable Development Goals (SDGs), the establishing of priorities in humanitarian aid programs and the design of early warning forecasting systems. This article addresses a question that underlies the social and political application of risk indicators, namely: how reliable are such data that can be accessed or downloaded ‘in a few mouse clicks’? Reliability is an important issue for users of these data since poor data will lead to poor inferences. In addition, flawed data are usually related to poor and fragile countries, countries that need humanitarian aid and financial investments the most. In order to get a grip on this reliability issue, we explore the possible uncertainties attached to global risk-related indicators. In this article we (i) provide an overview of available data sources, (ii) briefly describe the way institutes aggregate risk indicators from an underlying set of basic indicators to form composites, and (iii) identify various sources of uncertainty related to global risk indicators and their composites. Furthermore, we give solutions for coping with uncertainties in the partial or complete absence of such information. We acknowledge that these solutions are insufficient to quantify all (cascading) uncertainties concerning global indicators, especially those related to ‘Campbell's law’. Therefore, we applied a ‘ringtest’ across data from leading institutes as for five open access risk indicators: governance, impacts of natural disasters, conflicts, vulnerability/coping capacity, and all security risks combined. We find that the coherence between indicators from different organisations but with identical definitions varies enormously. We find that indicators denoted as ‘impacts of natural disasters’ are almost uncorrelated across four organisations. However, indicators denoting ‘governance’ or ‘all security risks combined’ show remarkable high correlations.}
}
@incollection{GUERRERO202045,
title = {4 - Mind mapping in artificial intelligence for data democracy},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Data Democracy},
publisher = {Academic Press},
pages = {45-82},
year = {2020},
isbn = {978-0-12-818366-3},
doi = {https://doi.org/10.1016/B978-0-12-818366-3.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128183663000046},
author = {José M. Guerrero},
keywords = {Artificial intelligence, Data democracy, Information overload, Mind mapping},
abstract = {One of the main hurdles in the quest for data democracy is the information overload problem that end users have to suffer when trying to analyze all information available to be informed and make sensible decisions. In this chapter, I talk about the definition, causes, consequences, and possible solutions to information overload. One of the solutions lies in the use of artificial intelligence (AI). But this is a double-edged word because, in some cases, it helps to reduce information overload and, in other cases, it helps to increase the problem. I introduce the use of the mind mapping (MM) technique to help reduce information overload in all cases and specifically in cases where AI has a negative effect on this problem. For readers not privy to the MM technique, I provide some introductory references that can be useful to them. I do also provide a detailed example of the advantages of MM in the visualization of the results of the semantic analysis of a complex text using IBM Watson NLU and MM automation software developed by myself to create MindManager mind maps. This is one of those cases where AI produces a large amount of information that has to be organized in the best possible way. MM is, in my opinion, the most adequate way to organize that kind of information. The MM technique is also a very important tool to achieve data democracy through the empowerment of end users when gathering and analyzing complex information. Mind maps are single compressed files containing information, notes, graphs, and links that simplify the organization and treatment of complex information. Finally, an HTML5 version of the mind map used is made available to all readers so that they can appreciate the look and feel of the MM experience by simply using their Internet browser.}
}
@incollection{2020493,
title = {Index},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {493-500},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00048-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000486}
}
@article{MATSUMOTO2020107616,
title = {Research on horizontal system model for food factories: A case study of process cheese manufacturer},
journal = {International Journal of Production Economics},
volume = {226},
pages = {107616},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107616},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320300049},
author = {Takao Matsumoto and Yijun Chen and Akihiro Nakatsuka and Qunzhi Wang},
keywords = {Dairy product, Industry 4.0, Smart factory, Operation technology, Plant control system, Manufacturing execution system},
abstract = {The diary food factories in Japan are facing serious challenges of severe labor shortage and the increased diversity of demand. Food manufacturing companies are forced to improve factories to be more productive and flexible to deal with the expanding market scale in the future and also the product diversity. To improve the productivity and the flexibility, automation technologies have been implemented in manufacturing system with the popularization of Industrial 4.0 and Smart Factory. Based on the actual system construction practice of a dairy factory which is as a case study, this paper proposes a five-level horizontal model with automation technologies, aiming to realize high efficiency, rapid integration and relocation of the manufacturing system. This paper introduces the composition, the specifications and the functions of the horizontal model, and evaluates the function of each level. Finally, through the case study and numerical comparison on cost and labor hours, we verify the superiority of the proposed horizontal hierarchical system model for food factories.}
}
@article{HADZIC2020126,
title = {Methodology for fuzzy duplicate record identification based on the semantic-syntactic information of similarity},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {1},
pages = {126-136},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817304512},
author = {Djulaga Hadzic and Nermin Sarajlic},
keywords = {Duplicate records, Fuzzy, Semantic, Syntactic, Blocking, Windowing},
abstract = {There are different methodologies for identification of fuzzy duplicate records in the process of data cleaning for data warehouse and data mining. The methodologies for duplicate record identification can be classified into three groups: blocking methods, windowing methods, and semantic methods. The article specifically focuses on semantic methods and describes Semantic-Syntactic Method for fuzzy duplicate record identification. Based on the conducted testing, comparative analysis is presented of the results obtained through the Semantic-Syntactic Method and two other standard methods over a selected data set. In the end, the article presents conclusions with regard to the quality and efficiency of the Semantic-Syntactic Method, as well as suggestions for future research in this field.}
}
@article{MANIYATH2020103134,
title = {An efficient image encryption using deep neural network and chaotic map},
journal = {Microprocessors and Microsystems},
volume = {77},
pages = {103134},
year = {2020},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103134},
url = {https://www.sciencedirect.com/science/article/pii/S014193312030301X},
author = {Shima Ramesh Maniyath and Thanikaiselvan V},
keywords = {Chaotic map, ANN, Deep neural network},
abstract = {Inspite of progressive growth of cryptography, encrypting sensitive information of an image is still a computationally complex task. After reviewing existing literature, it is now known that security problems are yet not solved and there is an open scope of further research. In most recent times, it has been noticed that neural network has proven cost effective optimization mechanism in offering security towards images. However, such implementation are computationally expensive process and do not solve various diversified attacks on image. Hence, the prime purpose of proposed system is to introduce an analytical research methodology for presenting a sophisticated framework where deep neural network has been used for optimizing the performance of simple encryption approaches. The robustness of optimization principle is further added with chaotic map concept for enhanced security performance. The study outcome shows that proposed implementation offers much better security performance without any negative effect on image quality.}
}
@article{XIONG2020100008,
title = {Zooplankton biodiversity monitoring in polluted freshwater ecosystems: A technical review},
journal = {Environmental Science and Ecotechnology},
volume = {1},
pages = {100008},
year = {2020},
issn = {2666-4984},
doi = {https://doi.org/10.1016/j.ese.2019.100008},
url = {https://www.sciencedirect.com/science/article/pii/S2666498419300080},
author = {Wei Xiong and Xuena Huang and Yiyong Chen and Ruiying Fu and Xun Du and Xingyu Chen and Aibin Zhan},
keywords = {Biodiversity, Micro-eukaryotes, Metabarcoding, Freshwater ecosystem, Water pollution},
abstract = {Freshwater ecosystems harbor a vast diversity of micro-eukaryotes (rotifers, crustaceans and protists), and such diverse taxonomic groups play important roles in ecosystem functioning and services. Unfortunately, freshwater ecosystems and biodiversity therein are threatened by many environmental stressors, particularly those derived from intensive human activities such as chemical pollution. In the past several decades, significant efforts have been devoted to halting biodiversity loss to recover services and functioning of freshwater ecosystems. Biodiversity monitoring is the first and a crucial step towards diagnosing pollution impacts on ecosystems and making conservation plans. Yet, bio-monitoring of ubiquitous micro-eukaryotes is extremely challenging, owing to many technical issues associated with micro-zooplankton such as microscopic size, fuzzy morphological features, and extremely high biodiversity. Here, we review current methods used for monitoring zooplankton biodiversity to advance management of impaired freshwater ecosystems. We discuss the development of traditional morphology-based identification methods such as scanning electron microscope (SEM) and ZOOSCAN and FlowCAM automatic systems, and DNA-based strategies such as metabarcoding and real-time quantitative PCR. In addition, we summarize advantages and disadvantages of these methods when applied for monitoring impacted ecosystems, and we propose practical DNA-based monitoring workflows for studying biological consequences of environmental pollution in freshwater ecosystems. Finally, we propose possible solutions for existing technical issues to improve accuracy and efficiency of DNA-based biodiversity monitoring.}
}
@article{MORIS2020614,
title = {Benefits and Risks of Primary Treatments for High-risk Localized and Locally Advanced Prostate Cancer: An International Multidisciplinary Systematic Review},
journal = {European Urology},
volume = {77},
number = {5},
pages = {614-627},
year = {2020},
issn = {0302-2838},
doi = {https://doi.org/10.1016/j.eururo.2020.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0302283820300713},
author = {Lisa Moris and Marcus G. Cumberbatch and Thomas {Van den Broeck} and Giorgio Gandaglia and Nicola Fossati and Brian Kelly and Raj Pal and Erik Briers and Philip Cornford and Maria {De Santis} and Stefano Fanti and Silke Gillessen and Jeremy P. Grummet and Ann M. Henry and Thomas B.L. Lam and Michael Lardas and Matthew Liew and Malcolm D. Mason and Muhammad Imran Omar and Olivier Rouvière and Ivo G. Schoots and Derya Tilki and Roderick C.N. {van den Bergh} and Theodorus H. {van Der Kwast} and Henk G. {van Der Poel} and Peter-Paul M. Willemse and Cathy Y. Yuan and Badrinath Konety and Tanya Dorff and Suneil Jain and Nicolas Mottet and Thomas Wiegel},
keywords = {Prostate cancer, Localized, Locally advanced, Primary therapy, Radical prostatectomy, External beam radiotherapy, Brachytherapy, Modality treatment, Systemic treatment, Systematic review},
abstract = {Context
The optimal treatment for men with high-risk localized or locally advanced prostate cancer (PCa) remains unknown.
Objective
To perform a systematic review of the existing literature on the effectiveness of the different primary treatment modalities for high-risk localized and locally advanced PCa. The primary oncological outcome is the development of distant metastases at ≥5 yr of follow-up. Secondary oncological outcomes are PCa-specific mortality, overall mortality, biochemical recurrence, and need for salvage treatment with ≥5 yr of follow-up. Nononcological outcomes are quality of life (QoL), functional outcomes, and treatment-related side effects reported.
Evidence acquisition
Medline, Medline In-Process, Embase, and the Cochrane Central Register of Randomized Controlled Trials were searched. All comparative (randomized and nonrandomized) studies published between January 2000 and May 2019 with at least 50 participants in each arm were included. Studies reporting on high-risk localized PCa (International Society of Urologic Pathologists [ISUP] grade 4–5 [Gleason score {GS} 8–10] or prostate-specific antigen [PSA] >20 ng/ml or ≥ cT2c) and/or locally advanced PCa (any PSA, cT3–4 or cN+, any ISUP grade/GS) or where subanalyses were performed on either group were included. The following primary local treatments were mandated: radical prostatectomy (RP), external beam radiotherapy (EBRT) (≥64 Gy), brachytherapy (BT), or multimodality treatment combining any of the local treatments above (±any systemic treatment). Risk of bias (RoB) and confounding factors were assessed for each study. A narrative synthesis was performed.
Evidence synthesis
Overall, 90 studies met the inclusion criteria. RoB and confounding factors revealed high RoB for selection, performance, and detection bias, and low RoB for correction of initial PSA and biopsy GS. When comparing RP with EBRT, retrospective series suggested an advantage for RP, although with a low level of evidence. Both RT and RP should be seen as part of a multimodal treatment plan with possible addition of (postoperative) RT and/or androgen deprivation therapy (ADT), respectively. High levels of evidence exist for EBRT treatment, with several randomized clinical trials showing superior outcome for adding long-term ADT or BT to EBRT. No clear cutoff can be proposed for RT dose, but higher RT doses by means of dose escalation schemes result in an improved biochemical control. Twenty studies reported data on QoL, with RP resulting mainly in genitourinary toxicity and sexual dysfunction, and EBRT in bowel problems.
Conclusions
Based on the results of this systematic review, both RP as part of multimodal treatment and EBRT + long-term ADT can be recommended as primary treatment in high-risk and locally advanced PCa. For high-risk PCa, EBRT + BT can also be offered despite more grade 3 toxicity. Interestingly, for selected patients, for example, those with higher comorbidity, a shorter duration of ADT might be an option. For locally advanced PCa, EBRT + BT shows promising result but still needs further validation. In this setting, it is important that patients are aware that the offered therapy will most likely be in the context a multimodality treatment plan. In particular, if radiation is used, the combination of local with systemic treatment provides the best outcome, provided the patient is fit enough to receive both. Until the results of the SPCG15 trial are known, the optimal local treatment remains a matter of debate. Patients should at all times be fully informed about all available options, and the likelihood of a multimodal approach including the potential side effects of both local and systemic treatment.
Patient summary
We reviewed the literature to see whether the evidence from clinical studies would tell us the best way of curing men with aggressive prostate cancer that had not spread to other parts of the body such as lymph glands or bones. Based on the results of this systematic review, there is good evidence that both surgery and radiation therapy are good treatment options, in terms of prolonging life and preserving quality of life, provided they are combined with other treatments. In the case of surgery this means including radiotherapy (RT), and in the case of RT this means either hormonal therapy or combined RT and brachytherapy.}
}
@article{JENKOBIZJAN202083,
title = {Challenges in identifying large germline structural variants for clinical use by long read sequencing},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {83-92},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2019.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S2001037019303678},
author = {Barbara {Jenko Bizjan} and Theodora Katsila and Tine Tesovnik and Robert Šket and Maruša Debeljak and Minos Timotheos Matsoukas and Jernej Kovač},
keywords = {Structural variations, Human genetics, Long reads sequencing, Theranostics},
abstract = {Genomic structural variations, previously considered rare events, are widely recognized as a major source of inter-individual variability and hence, a major hurdle in optimum patient stratification and disease management. Herein, we focus on large complex germline structural variations and present challenges towards target treatment via the synergy of state-of-the-art approaches and information technology tools. A complex structural variation detection remains challenging, as there is no gold standard for identifying such genomic variations with long reads, especially when the chromosomal rearrangement in question is a few Mb in length. A clinical case with a large complex chromosomal rearrangement serves as a paradigm. We feel that functional validation and data interpretation are of outmost importance for information growth to be translated into knowledge growth and hence, new working practices are highlighted.}
}
@article{ZHAO2020102562,
title = {Identification of land-use characteristics using bicycle sharing data: A deep learning approach},
journal = {Journal of Transport Geography},
volume = {82},
pages = {102562},
year = {2020},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2019.102562},
url = {https://www.sciencedirect.com/science/article/pii/S0966692318309232},
author = {Jiahui Zhao and Wei Fan and Xuehao Zhai},
keywords = {Land-use characteristics, Ensemble clustering, GIS, Deep neural network},
abstract = {Extensive research has shown that urban land-use characteristics, including resident, work, consumption, transit, etc., are significantly interrelated with travel behaviors and travel demands. Many research efforts have been made to evaluate the impact of land use planning or policies on travel behavior, however, few studies are able to quantitatively measure the land-use characteristics based on the data of travel behaviors or travel demand. In this paper, a new hybrid model that combines time series feature extraction and deep neural network is proposed to identify regional land use characteristics and quantify land use intensity using ridership data of bicycle sharing. This method consists of four main parts: (i) A set of land-use characteristic labels are evaluated based on planning and Geographic Information System (GIS) data. (ii) An ensemble clustering method is used to determine the segmentation points of ridership time series. (iii) The statistical characteristics of the segmented time series are extracted and used as input to the neural network. (iv) A deep neural network is established and trained based on the processed ridership features and land-use labels. In terms of data collection, ridership data of the bicycle-sharing parking spots and land-use planning data are obtained from bicycle-sharing system and planning department in San Francisco Bay Area, California U.S.A., respectively. The test results show that this approach has high accuracy for identifying land-use characteristics based on several standard evaluation measures and that the identification distribution can be well explained. The extension results further prove that the model can be applied to effectively analyze the main land-use characteristics of the region although the identification results may become unstable after 3–4 months.}
}
@article{KEMGANGGHOMSI2020103657,
title = {Cameroon's crustal configuration from global gravity and topographic models and seismic data},
journal = {Journal of African Earth Sciences},
volume = {161},
pages = {103657},
year = {2020},
issn = {1464-343X},
doi = {https://doi.org/10.1016/j.jafrearsci.2019.103657},
url = {https://www.sciencedirect.com/science/article/pii/S1464343X19303127},
author = {Franck Eitel {Kemgang Ghomsi} and Nguiya Sévérin and Animesh Mandal and Françoise Enyegue A. Nyam and Robert Tenzer and Alain P. {Tokam Kamga} and Robert Nouayou},
keywords = {Cameroon, Crust, Gravity inversion, Moho, Seismic data},
abstract = {We use gravity information obtained from the XGM2016 global gravitational model together with topographic, bathymetric and seismic data to interpret the crustal structure beneath Cameroon and adjoined geological regions. For this purpose, we apply the regularized non-linear gravity inversion for a gravimetric determination of the Moho depth utilizing existing results of seismic data analysis as constraints. The estimated Moho model reflects regional tectonic configuration and geological structure of this region, mainly consisting of two major geological units, i.e. the Cameroon Volcanic Line and the Congo Craton. A validation of gravimetric result at sites of the Cameroon Broadband Seismic Experiment (CBSE) reveals overall similarities between gravimetric and seismic estimates. A comparison of our result is also conducted with previously published results. The cross-comparison of these results reveals a good agreement between them, particularly beneath the Cameroon Volcanic Line, the Adamawa Plateau and the Garoua Rift. Nevertheless, some relatively large inconsistencies roughly reaching ±10 km in estimated values of the Moho depth are identified in geological regions of the Congo Craton and the Yaoundé domain. The spatial correlation analysis between the Moho geometry and the topography indicates an isostatic state of particular geological units, suggesting their compensation stage. Our result closely agrees with the assumption that most of isostatically over compensated geological structures were formed during a compressional tectonic regime, except for the Garoua Rift that was likely formed during an extensional regime. We also computed the Bouguer gravity data at different constant elevations above sea level in order to supress a gravitational signature of shallower sources, while enhancing a gravitational signature from deeper crustal and lithospheric structures, focusing primarily on cores of major cratonic formations. The Bouguer gravity maps indicate that the Yaoundé domain likely represents the crustal manifestation of the suture zone between the Congo Craton and the Adamawa-Yadé domain, acting as a micro-continent.}
}
@article{HUANG2020121583,
title = {Evaluation of real-time vehicle energy consumption and related emissions in China: A case study of the Guangdong–Hong Kong–Macao greater Bay Area},
journal = {Journal of Cleaner Production},
volume = {263},
pages = {121583},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.121583},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620316309},
author = {Wenke Huang and Yuanyuan Guo and Xiaoxiao Xu},
keywords = {Guangdong–Hong Kong–Macao Greater Bay Area, Vehicle emission, Road traffic, Spatiotemporal, Directions API},
abstract = {The Guangdong–Hong Kong–Macao Greater Bay Area (GBA) was recently proposed by the Chinese government for socioeconomic–political–environmental development. As one of the most important parts for the development of the GBA, the transportation sector has become a nonnegligible consumer of energy and source of emission. Therefore, vehicle energy consumption and related emissions in the GBA must be evaluated. However, an effective framework for accurately estimating the real-time transportation performance, vehicle energy consumption and emission in large urban areas is still lacking. This study aims to develop a novel method for examining the regional integration and the spatial connection that affect vehicle emission via crowdsourced traffic data and an emission model. The novelty of this study is that it explores the acquisition of large-scale and real-time data as well as the calculation of corresponding energy consumption and environmental impact. Based on the analysis of transportation in GBA, it is found that the vehicle energy consumption and emission on the expressway are nearly half as that on the other urban roads. Moreover, a significant difference exists in vehicle energy consumption and emission between passenger cars and medium-duty trucks. Regarding transportation performance, vehicle energy consumption and emissions, they are closely related to departure time. It is also found that most of the adjacent cities tend to have high vehicle energy consumption and emission. The contribution of the Hong Kong–Zhuhai–Macao Bridge to transportation from Hong Kong, Macao and Zhuhai is different. Hong Kong benefits most from the bridge in terms of traffic energy efficiency. This study would be valuable to both researchers and practitioners. It helps researchers apply a large dataset in replicating real-world travel, energy consumption and emission pattern at a large scale. Policymakers and practitioners would benefit from developing effective strategies for the sustainable development of the GBA.}
}
@article{HUANG2020103036,
title = {A CRISPR-Cas12a-based specific enhancer for more sensitive detection of SARS-CoV-2 infection},
journal = {EBioMedicine},
volume = {61},
pages = {103036},
year = {2020},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2020.103036},
url = {https://www.sciencedirect.com/science/article/pii/S2352396420304126},
author = {Weiren Huang and Lei Yu and Donghua Wen and Dong Wei and Yangyang Sun and Huailong Zhao and Yu Ye and Wei Chen and Yongqiang Zhu and Lijun Wang and Li Wang and Wenjuan Wu and Qianqian Zhao and Yong Xu and Dayong Gu and Guohui Nie and Dongyi Zhu and Zhongliang Guo and Xiaoling Ma and Liman Niu and Yikun Huang and Yuchen Liu and Bo Peng and Renli Zhang and Xiuming Zhang and Dechang Li and Yang Liu and Guoliang Yang and Lanzheng Liu and Yunying Zhou and Yunshan Wang and Tieying Hou and Qiuping Gao and Wujiao Li and Shuo Chen and Xuejiao Hu and Mei Han and Huajun Zheng and Jianping Weng and Zhiming Cai and Xinxin Zhang and Fei Song and Guoping Zhao and Jin Wang},
keywords = {COVID-19, SARS-CoV-2, rRT-PCR, CRISPR diagnosis, Cas12a, SENA},
abstract = {Background
Real-time reverse transcription-PCR (rRT-PCR) has been the most effective and widely implemented diagnostic technology since the beginning of the COVID-19 pandemic. However, fuzzy rRT-PCR readouts with high Ct values are frequently encountered, resulting in uncertainty in diagnosis.
Methods
A Specific Enhancer for PCR-amplified Nucleic Acid (SENA) was developed based on the Cas12a trans-cleavage activity, which is specifically triggered by the rRT-PCR amplicons of the SARS-CoV-2 Orf1ab (O) and N fragments. SENA was first characterized to determine its sensitivity and specificity, using a systematic titration experiment with pure SARS-CoV-2 RNA standards, and was then verified in several hospitals, employing a couple of commercial rRT-PCR kits and testing various clinical specimens under different scenarios.
Findings
The ratio (10 min/5 min) of fluorescence change (FC) with mixed SENA reaction (mix-FCratio) was defined for quantitative analysis of target O and N genes, and the Limit of Detection (LoD) of mix-FCratio with 95% confidence interval was 1.2≤1.6≤2.1. Totally, 295 clinical specimens were analyzed, among which 21 uncertain rRT-PCR cases as well as 4 false negative and 2 false positive samples were characterized by SENA and further verified by next-generation sequencing (NGS). The cut-off values for mix-FCratio were determined as 1.145 for positive and 1.068 for negative.
Interpretation
SENA increases both the sensitivity and the specificity of rRT-PCR, solving the uncertainty problem in COVID-19 diagnosis and thus providing a simple and low-cost companion diagnosis for combating the pandemic.
Funding
Detailed funding information is available at the end of the manuscript.}
}
@article{KORTELAINEN20207,
title = {Advanced technologies for effective asset management - two cases in capital intensive branches},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {7-12},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301397},
author = {Helena Kortelainen and Jyri Hanski and Pasi Valkokari},
keywords = {Asset, maintenance management, Asset life cycle management, Maintenance related services, service improvement},
abstract = {Digital transformation influences also asset management processes and offers solutions for more effective practices. This study focuses on the use of advanced technologies in the electricity supply and forest industry cases. The study revealed a broad spectrum of technological solutions and services that could create value for asset management activities in the industry 4.0 context. The solutions were evaluated with the asset life cycle management framework that offers means for linking the technologies and solutions to the asset management and maintenance activities.}
}
@incollection{KEOGH202011,
title = {Chapter 2 - The future food chain: digitization as an enabler of Society 5.0},
editor = {Darin Detwiler},
booktitle = {Building the Future of Food Safety Technology},
publisher = {Academic Press},
pages = {11-38},
year = {2020},
isbn = {978-0-12-818956-6},
doi = {https://doi.org/10.1016/B978-0-12-818956-6.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189566000026},
author = {John G. Keogh and Laurette Dube and Abderahman Rejeb and Karen J. Hand and Nida Khan and Kevin Dean},
keywords = {3D printing, 5G communication networks, Artificial intelligence, Augmented reality, Blockchain technology, Challenges, Cloud computing, Data analytics, Drone technology, Food Convergence Innovation: accelerate industry 4.0 contribution to Society 5.0, Food supply chain, Internet of things, Mixed reality, Virtual reality, Reference framework, Robotics, Society 5.0, Technology, Supply chain operations},
abstract = {Food systems and food supply chains (FSCs) have undergone significant changes in their operations and structure over the last decade as globalization expands both food choice and availability. As FSCs lengthen, and food passes through extended trading relationships, transparency on food origins, methods of cultivation, harvest, processing, and labor conditions and sustainability are reduced, along with food trust. Moreover, while the rapid pace of technology innovation benefits FSCs, we are witnesses to the usage of social media platforms by citizen-consumers to amplify the rhetoric related to recurring incidents and crises in food quality, food safety, food fraud, food security, sustainability, and other ethical lapses. Furthermore, we are witnesses to new evidence on the global burden of foodborne diseases, including noncommunicable diseases that range from severe malnutrition to morbid obesity and from severe illnesses requiring hospitalization to mortality. The World Health Organization claims that 31 foodborne hazards cause six hundred million illnesses and four hundred and twenty thousand deaths annually. Overcoming these challenges requires a holistic reframing of our food systems and societal challenges. The emergence of the UN’s Sustainable Development Goals (SDGs) provides an overarching framework for collaboration and alignment. Japan has put forward a vision for a human-centric, technology-enabled future branded as “Society 5.0.” Increasingly, the redesign of FSCs necessitates a concerted, multistakeholder effort and the development of digitization strategies in order to cope with the evolution toward the vision of Society 5.0 and to achieve the UN SDGs.}
}
@article{DENOUDEN2020106464,
title = {The role of Experiential Avoidance in transdiagnostic compulsive behavior: A structural model analysis},
journal = {Addictive Behaviors},
volume = {108},
pages = {106464},
year = {2020},
issn = {0306-4603},
doi = {https://doi.org/10.1016/j.addbeh.2020.106464},
url = {https://www.sciencedirect.com/science/article/pii/S0306460320305943},
author = {Lauren {Den Ouden} and Jeggan Tiego and Rico S.C. Lee and Lucy Albertella and Lisa-Marie Greenwood and Leonardo Fontenelle and Murat Yücel and Rebecca Segrave},
keywords = {Addiction, Obsessive–compulsive disorder, Gambling, Compulsive buying, Binge-eating, Experiential Avoidance},
abstract = {Compulsivity is recognized as a transdiagnostic phenotype, underlying a variety of addictive and obsessive–compulsive behaviors. However, current understanding of how it should be operationalized and the processes contributing to its development and maintenance is limited. The present study investigated if there was a relationship between the affective process Experiential Avoidance (EA), an unwillingness to tolerate negative internal experiences, and the frequency and severity of transdiagnostic compulsive behaviors. A large sample of adults (N = 469) completed online questionnaires measuring EA, psychological distress and the severity of seven obsessive–compulsive and addiction-related behaviors. Using structural equation modelling, results indicated a one-factor model of compulsivity was superior to the two-factor model (addictive- vs OCD-related behaviors). The effect of EA on compulsivity was fully mediated by psychological distress, which in turn had a strong direct effect on compulsivity. This suggests distress is a key mechanism in explaining why people with high EA are more prone to compulsive behaviors. The final model explained 41% of the variance in compulsivity, underscoring the importance of these constructs as likely risk and maintenance factors for compulsive behavior. Implications for designing effective psychological interventions for compulsivity are discussed.}
}
@article{TAHSIEN2020102630,
title = {Machine learning based solutions for security of Internet of Things (IoT): A survey},
journal = {Journal of Network and Computer Applications},
volume = {161},
pages = {102630},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102630},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520301041},
author = {Syeda Manjia Tahsien and Hadis Karimipour and Petros Spachos},
keywords = {Architecture, Attack surfaces, Challenges, Internet of Things, IoT attacks, Machine learning, Security solution},
abstract = {Over the last decade, IoT platforms have been developed into a global giant that grabs every aspect of our daily lives by advancing human life with its unaccountable smart services. Because of easy accessibility and fast-growing demand for smart devices and network, IoT is now facing more security challenges than ever before. There are existing security measures that can be applied to protect IoT. However, traditional techniques are not as efficient with the advancement booms as well as different attack types and their severeness. Thus, a strong-dynamically enhanced and up to date security system is required for next-generation IoT system. A huge technological advancement has been noticed in Machine Learning (ML) which has opened many possible research windows to address ongoing and future challenges in IoT. In order to detect attacks and identify abnormal behaviors of smart devices and networks, ML is being utilized as a powerful technology to fulfill this purpose. In this survey paper, the architecture of IoT is discussed, following a comprehensive literature review on ML approaches the importance of security of IoT in terms of different types of possible attacks. Moreover, ML-based potential solutions for IoT security has been presented and future challenges are discussed.}
}
@article{STRICKLAND2020102587,
title = {Leveraging crowdsourcing methods to collect qualitative data in addiction science: Narratives of non-medical prescription opioid, heroin, and fentanyl use},
journal = {International Journal of Drug Policy},
volume = {75},
pages = {102587},
year = {2020},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2019.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0955395919302944},
author = {Justin C. Strickland and Grant A. Victor},
keywords = {Crowdsourcing, Heroin, Mechanical Turk, Opioid, Qualitative, Mixed method},
abstract = {Background
Online crowdsourcing methods have proved useful for studies of diverse designs in the behavioral and addiction sciences. The remote and online setting of crowdsourcing research may provide easier access to unique participant populations and improved comfort for these participants in sharing sensitive health or behavioral information. To date, few studies have evaluated the use of qualitative research methods on crowdsourcing platforms and even fewer have evaluated the quality of data gathered. The purpose of the present analysis was to document the feasibility and validity of using crowdsourcing techniques for collecting qualitative data among people who use drugs.
Methods
Participants (N = 60) with a history of non-medical prescription opioid use with transition to heroin or fentanyl use were recruited using Amazon Mechanical Turk (mTurk). A battery of qualitative questions was included indexing beliefs and behaviors surrounding opioid use, transition pathways to heroin and/or fentanyl use, and drug-related contacts with structural institutions (e.g., health care, criminal justice).
Results
Qualitative data recruitment was feasible as evidenced by the rapid sampling of a relatively large number of participants from diverse geographic regions. Computerized text analysis indicated high ratings of authenticity for the provided narratives. These authenticity percentiles were higher than the average of general normative writing samples as well as than those collected in experimental settings.
Conclusions
These findings support the feasibility and quality of qualitative data collected in online settings, broadly, and crowdsourced settings, specifically. Future work among people who use drugs may leverage crowdsourcing methods and the access to hard-to-sample populations to complement existing studies in the human laboratory and clinic as well as those using other digital technology methods.}
}
@article{BAKER2020257,
title = {Degrader Analysis for Diagnostic and Predictive Capabilities: A Demonstration of Progress in DoD CBM+ Initiatives},
journal = {Procedia Computer Science},
volume = {168},
pages = {257-264},
year = {2020},
note = {“Complex Adaptive Systems”Malvern, PennsylvaniaNovember 13-15, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.253},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303926},
author = {William Baker and Steven Nixon and Jeffrey Banks and Karl Reichard and Kaitlynn Castelle},
keywords = {Reliability Centered Maintenance, Condition Based Maintenance, High Pressure Air Compressor},
abstract = {This paper presents a modified reliability centered maintenance (RCM) methodology developed by The Applied Research Laboratory at The Pennsylvania State University (ARL Penn State) to meet challenges in decreasing life cycle sustainment costs for critical Naval assets. The focus of this paper is on the requirements for the development of the on-board Prognostics and Health Management (PHM) system with a discussion on the implementation progress for two systems: the high pressure air compressor (HPAC), and the advanced carbon dioxide removal unit (ACRU). Recent Department of Defense (DoD) guidance calls for implementing Condition Based Maintenance (CBM) as an alternative to traditional reactive and preventative maintenance strategies that rely on regular and active participation from subject matter experts to evaluate the health condition of critical systems. The RCM based degrader analysis utilizes data from multiple sources to provide a path for selecting systems and components most likely to benefit from the implementation of diagnostic and predictive capabilities for monitoring and managing failure modes by determining various options of possible CBM system designs that provide the highest potential ROI. Sensor data collected by the PHM system can be used with machine learning applications to develop failure mode predictive algorithms with greatest benefit in terms of performance, sustainment costs, and increasing platform operational availability. The approach supports traditional maintenance strategy development by assessing the financial benefit of the PHM technology implementation with promising potential for many industrial and military complex adaptive system applications.}
}
@article{AVDEEVA20203761,
title = {Distributed Environment of Decision Support Centers: An Interests Representation Model of Virtual Collaboration and Technological Basic},
journal = {Procedia Computer Science},
volume = {176},
pages = {3761-3770},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319025},
author = {Zinaida Avdeeva and Svetlana Kovriga},
keywords = {Type your keywords here, separated by semicolons},
abstract = {The possibilities of using the potential of cognitive maps in the practice of group interaction and solving managerial problems in a system of distributed decision support centers are considered. The principles of the formation of coordinated representations of participants in virtual cooperation using cognitive maps are proposed; an approach to the construction and justification of a cognitive map based on the integration of expert representations and information identified by methods of searching and analysing data in an open information space.}
}
@article{REISI2020102095,
title = {Transport sustainability indicators for an enhanced urban analytics data infrastructure},
journal = {Sustainable Cities and Society},
volume = {59},
pages = {102095},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102095},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720300822},
author = {Marzieh Reisi and Soheil Sabri and Muyiwa Agunbiade and Abbas Rajabifard and Yiqun Chen and Mohsen Kalantari and Azadeh Keshtiarast and Yan Li},
keywords = {Transport sustainability indicators, Data integration, Ontology-based analytics, Urban analytics},
abstract = {This paper examines capabilities of a new Spatial Data Infrastructure called Urban Analytics Data Infrastructure (UADI11Spatial Data Infrastructure called Urban Analytics Data Infrastructure), through deriving and evaluating transport sustainability indicators. The UADI was developed in Australia to support multi-disciplinary and cross-jurisdictional analytics to overcome the challenges related to model generalisation, data accessibility, data integration, data heterogeneity and city performance benchmarking. In this paper, the UADI were evaluated through 5 technical lenses of: data accessibility; integration; harmonisation; data reliability and model reliability. The paper shows that using open geospatial standards in UADI enabled transport sustainability indicators to be derived through accessing and integrating different spatial data layers and the process of mapping data to ontology facilitated data harmonisation. In addition, the input data, tool processing, and output of ontology-based transport sustainability indicators could be traced in UADI, which addresses the challenge of model and data reliability. The paper highlights the role of spatial data infrastructures in decision support systems for uncertainty analysis and promoting smart cities and resilient environment.}
}
@article{YIFAN202080,
title = {Visualization of cardiovascular development, physiology and disease at the single-cell level: Opportunities and future challenges},
journal = {Journal of Molecular and Cellular Cardiology},
volume = {142},
pages = {80-92},
year = {2020},
issn = {0022-2828},
doi = {https://doi.org/10.1016/j.yjmcc.2020.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022282820300675},
author = {Chen Yifan and Yang Fan and Pu Jun},
keywords = {Atherosclerosis, Vulnerable plaques, Macrophage polarization, Melatonin, Single-cell sequencing analysis, Multiomics analysis},
abstract = {Single-cell RNA sequencing (scRNA-seq), a method of transcriptome sequencing at the single-cell level, has recently emerged as a revolutionary technology in the field of biomedical research. Compared to conventional gene expression profiling in bulk, scRNA-seq resolves biological differences among individual cells and enables the identification of rare cell populations that are easily overlooked. This review introduces the method of scRNA-seq, summarizes its applications in the field of cardiovascular disease research, and discusses existing limitations and prospects for future applications.}
}
@article{FAN20202034,
title = {Hybrid support vector machines with heuristic algorithms for prediction of daily diffuse solar radiation in air-polluted regions},
journal = {Renewable Energy},
volume = {145},
pages = {2034-2045},
year = {2020},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2019.07.104},
url = {https://www.sciencedirect.com/science/article/pii/S0960148119311243},
author = {Junliang Fan and Lifeng Wu and Xin Ma and Hanmi Zhou and Fucang Zhang},
keywords = {Air pollution, Support vector machines, Extreme gradient boosting, Particle swarm optimization algorithm, Bat algorithm, Whale optimization algorithm},
abstract = {Increasing air pollutants significantly affect the proportion of diffuse (Rd) to global (Rs) solar radiation. This study proposed three new hybrid support vector machines (SVM) with particle swarm optimization algorithm (SVM-PSO), bat algorithm (SVM-BAT) and whale optimization algorithm (SVM-WOA) for predicting daily Rd in air-polluted regions. These models were further compared to standalone SVM, multivariate adaptive regression spline (MARS) and extreme gradient boosting (XGBoost) models. The results showed that models with suspended particulate matter with aerodynamic diameter smaller than 2.5 μm and 10 μm (PM2.5 and PM10) and ozone (O3) produced more accurate daily Rd estimates than those without air pollution parameters, with average relative decreases in root mean square deviation (RMSD) of 11.1%, 10.0% and 10.4% for sunshine duration-based, Rs-based and combined models, respectively. SVM showed better accuracy than XGBoost and MARS. However, compared to SVM, SVM-BAT further enhanced the prediction accuracy and convergence rate in daily Rd modeling, followed by SVM-WOA and SVM-PSO, with relative decreases in RMSD of 2.9%–5.6%, 1.9%–4.9% and 1.1%–3.3%, respectively. The results highlighted the significance of incorporating air pollutants for more accurate estimation of daily Rd in air-polluted regions. Heuristic algorithms, especially BAT, are highly recommended for improving performance of standalone machine learning models.}
}
@article{AHMER20201388,
title = {A unified approach towards performance monitoring and condition-based maintenance in grinding machines},
journal = {Procedia CIRP},
volume = {93},
pages = {1388-1393},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.094},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307289},
author = {Muhammad Ahmer and Pär Marklund and Martin Gustafsson and Kim Berglund},
keywords = {Analytics, Automation, Condition monitoring, Grinding, Machining, Maintenance, Manufacturing, Measurement, Process Monitoring, Sensor},
abstract = {The process controller in a precision grinder for bearing rings puts high performance demands on the machine to achieve desired quality in production. This paper presents a unique approach of adding additional sensors for machine condition monitoring for the purpose of learning and using high fidelity condition indicators. The consolidation of real-time sensor data and the process control signals yields high-dimensional dataset. Automatic segmentation helps optimize the amount of data for processing and data mining ahead of fault diagnosis. The proposed setup is state of the art for prognostics as part of condition-based maintenance in a production machine.}
}
@article{RIVASTABARES2020175,
title = {Self-organizing map of soil properties in the context of hydrological modeling},
journal = {Applied Mathematical Modelling},
volume = {88},
pages = {175-189},
year = {2020},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2020.06.044},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X2030319X},
author = {David Rivas-Tabares and Ángel {de Miguel} and Bárbara Willaarts and Ana M. Tarquis},
keywords = {Soil properties, Self-organizing Maps, Hydrological modeling, SWAT, Soils spatial patterns, SOM},
abstract = {One of the most relevant inputs for hydrological modeling is the soil map. The soil sources and scales for the soil properties are diverse, and the quality of soil mapping is increasing, but soil surveying is time-consuming and large area campaigns are expensive. The taxonomic unit approach for soil mapping is common and limited to one layer of data. This limitation causes errors in simulated water fluxes through the soil when taxonomic units approach is implemented during hydrological modeling analysis. Some strategies using geostatistics and machine learning algorithms such as Kriging and Self-Organizing maps (SOM) are improving the taxonomic units’ approach and could serve as an alternative for soil mapping for hydrological purposes. The aim of this work is to study the influence of different soil maps and resolutions on the main hydrological components of a sub-arid watershed in central Spain. For this, the Soil Water and Assessment Tool (SWAT) was parameterized with three different soil maps. A first one was based on Harmonized World Soil database from FAO, at scale 1:1,000,000 (HWSD). The other two were based on a Kriging interpolation at 100 × 100 m from soil samples. To obtain soil properties map from it, two strategies were applied: one was to average the soil properties following the official taxonomic soil units at 1:400,000 scale (Agricultural Technological Institute of Castilla and Leon - ITACyL) and the other was to applied Self-organizing map (SOM) to create the soil units (SOMM). The results suggest that scale and soil properties mapping influence HRU definition, which in turn affects water flow through the soils. Statistical metrics of model performance were improved from R2 =0.62 and NSE=0.46 with HWSD soil map to R2 =0.86 and NSE=0.84 with SOM and similar values were achieved during validation. Thus, the SOM is presented as an innovative algorithm applied for hydrological modeling with SWAT, significantly increasing the level of model accuracy to stream flow in sub-arid watersheds.}
}
@article{HASSELGREN2020104040,
title = {Blockchain in healthcare and health sciences—A scoping review},
journal = {International Journal of Medical Informatics},
volume = {134},
pages = {104040},
year = {2020},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.104040},
url = {https://www.sciencedirect.com/science/article/pii/S138650561930526X},
author = {Anton Hasselgren and Katina Kralevska and Danilo Gligoroski and Sindre A. Pedersen and Arild Faxvaag},
keywords = {Blockchain, Health systems, Scoping review, Distributed ledger},
abstract = {Background
Blockchain can be described as an immutable ledger, logging data entries in a decentralized manner. This new technology has been suggested to disrupt a wide range of data-driven domains, including the health domain.
Objective
The purpose of this study was to systematically review, assess and synthesize peer-reviewed publications utilizing/proposing to utilize blockchain to improve processes and services in healthcare, health sciences and health education.
Method
A structured literature search on the topic was conducted in October 2018 relevant bibliographic databases.
Result
39 publications fulfilled the inclusion criteria. The result indicates that Electronic Health Records and Personal Health Records are the most targeted areas using blockchain technology. Access control, interoperability, provenance and data integrity are all issues that are meant to be improved by blockchain technology in this field. Ethereum and Hyperledger fabric seem to be the most used platforms/frameworks in this domain.
Conclusion
This study shows that the endeavors of using blockchain technology in the health domain are increasing exponentially. There are areas within the health domain that potentially could be highly impacted by blockchain technology.}
}
@article{CHEN2020192,
title = {Virtual metrology of semiconductor PVD process based on combination of tree-based ensemble model},
journal = {ISA Transactions},
volume = {103},
pages = {192-202},
year = {2020},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2020.03.031},
url = {https://www.sciencedirect.com/science/article/pii/S0019057820301397},
author = {Ching-Hsien Chen and Wei-Dong Zhao and Timothy Pang and Yi-Zheng Lin},
keywords = {Semiconductor, Sequential model-based optimization, Combination of tree-based ensemble models, Virtual metrology},
abstract = {In order to improve the accuracy of semiconductor wafer virtual metrology, and overcome the physical metrology delay of wafer acceptance test, a virtual physical vapor deposition metrology method based on combination of tree-based ensemble models is proposed to conduct online virtual metrology on semiconductor wafer electrical parameters, and use hyperparameter optimization technique to perform model optimization and to achieve real-time alarm on process deviation. This combination of tree-based ensemble model combines Bagging, Boosting, and Stacking techniques. First, based on 4 types of base learner, Random Forest, Extra-Trees, XGBoost, and lightGBM, preliminary virtual metrology is performed on wafer PVD process, and then transforms the predict results of the 4 base learners into meta feature vector as the input of meta learner lightGBM to perform further virtual metrology. The Sequential model-based optimization algorithm is used to improve the accuracy of virtual metrology. First, the initial hyperparameter of the sequential model-based optimization is initialized by using random sampling, then the combination model is approximated by the surrogate model of tree-structured Parzen estimator, and the recommended hyperparameters is obtained by using EI (Expected Improvement), and then the optimized combination model is obtained. Finally, the superiority of the method proposed in this paper is verified by studying the results comparing to the common virtual metrology methods on the PVD process. The experiment shows the result of resistivity metrology using the combination of tree-based ensemble models in the PVD process is significantly better than LASSO regression, partial least squares regression(PLSR), support vector machine(SVR), Gaussian process regression(GPR) and artificial neural network regression(ANN).}
}
@incollection{ALJABERY20207,
title = {2 - Data preprocessing},
editor = {Khalid K. Al-jabery and Tayo Obafemi-Ajayi and Gayla R. Olbricht and Donald C. {Wunsch II}},
booktitle = {Computational Learning Approaches to Data Analytics in Biomedical Applications},
publisher = {Academic Press},
pages = {7-27},
year = {2020},
isbn = {978-0-12-814482-4},
doi = {https://doi.org/10.1016/B978-0-12-814482-4.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128144824000024},
author = {Khalid K. Al-jabery and Tayo Obafemi-Ajayi and Gayla R. Olbricht and Donald C. {Wunsch II}},
keywords = {COBRIT, Computational intelligence, Data imputation, Data Processing, Features selection and extraction, Machine learning, Missing value algorithms, Normalization, Outlier detection},
abstract = {This chapter proposed a general framework for data curation. It covers the different phases of data preprocessing and preparation. The presented general framework fits a broad variety of datasets. Raw data prior to cleansing and curation is usually not ready for distilling correct inferences. This chapter discusses and provides a detailed overview for the most popular algorithms and techniques, which are used in the field of data curation and preparation. This chapter's framework describes techniques for data curation, imputation, feature extraction, correlation analysis, and practical application of these algorithms. We also provided techniques that have been developed from our experience in data processing. Finally, we presented a practical example showing the effect of using different imputation methods on the performance and efficiency of SVM. The chapter describes a methodology for converting raw and messy data to a well-organized data that is ready for applying high level machine learning algorithms or any advanced methods of data analysis.}
}
@article{WANG20201198,
title = {Long-Term Expansion of Pancreatic Islet Organoids from Resident Procr+ Progenitors},
journal = {Cell},
volume = {180},
number = {6},
pages = {1198-1211.e19},
year = {2020},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2020.02.048},
url = {https://www.sciencedirect.com/science/article/pii/S0092867420302257},
author = {Daisong Wang and Jingqiang Wang and Lanyue Bai and Hong Pan and Hua Feng and Hans Clevers and Yi Arial Zeng},
keywords = {pancreatic islets, adult stem cells, organoid, β cells, Procr},
abstract = {Summary
It has generally proven challenging to produce functional β cells in vitro. Here, we describe a previously unidentified protein C receptor positive (Procr+) cell population in adult mouse pancreas through single-cell RNA sequencing (scRNA-seq). The cells reside in islets, do not express differentiation markers, and feature epithelial-to-mesenchymal transition characteristics. By genetic lineage tracing, Procr+ islet cells undergo clonal expansion and generate all four endocrine cell types during adult homeostasis. Sorted Procr+ cells, representing ∼1% of islet cells, can robustly form islet-like organoids when cultured at clonal density. Exponential expansion can be maintained over long periods by serial passaging, while differentiation can be induced at any time point in culture. β cells dominate in differentiated islet organoids, while α, δ, and PP cells occur at lower frequencies. The organoids are glucose-responsive and insulin-secreting. Upon transplantation in diabetic mice, these organoids reverse disease. These findings demonstrate that the adult mouse pancreatic islet contains a population of Procr+ endocrine progenitors.}
}
@article{MENGER2020e00102,
title = {Wide-scope screening of polar contaminants of concern in water: A critical review of liquid chromatography-high resolution mass spectrometry-based strategies},
journal = {Trends in Environmental Analytical Chemistry},
volume = {28},
pages = {e00102},
year = {2020},
issn = {2214-1588},
doi = {https://doi.org/10.1016/j.teac.2020.e00102},
url = {https://www.sciencedirect.com/science/article/pii/S2214158820300301},
author = {Frank Menger and Pablo Gago-Ferrero and Karin Wiberg and Lutz Ahrens},
keywords = {Aquatic environment, Organic, Emerging micropollutant, Non-Target screening, Suspect screening, LC, HRMS, Water analysis},
abstract = {The number of chemicals with potential to reach the environment is still largely unknown, which poses great challenges for both environmental scientists and analytical chemists. Liquid chromatography coupled to high-resolution mass spectrometry (LC-HRMS) is currently the instrumentation of choice for identification of wide-scope polar chemicals of concern (CECs) in water. This review critically evaluates all steps involved in screening for polar CECs in water, including sampling and extraction, analysis by LC-HRMS, data (pre-)treatment, evaluation and reporting. Passive samplers and direct injection, in combination with LC-HRMS, provide new opportunities compared with conventional grab water sampling, as do instrumental advances such as ion-mobility spectrometry coupled to HRMS (IM-HRMS). In this paper, we argue that target, suspect and non-target screening should not be viewed as three separate principles, but rather as conceptual approaches to general data treatment strategies that can be linked together. Due to the large amount of data generated, smart prioritisation strategies are needed, in particular for non-target screening, to reduce complexity and focus on data of high interest. We critically evaluate existing strategies and consider that each prioritisation step will result in data loss (as any other step in a screening study), requiring compromises depending on the research question to be tackled. Many different data treatment strategies have been developed in recent years, but structure elucidation remains a challenging and time-consuming task. We discuss current and potential future trends, e.g. effect-based methods that can be used as future prioritisation tools, technological advances like IM-HRMS and improved software solutions that can enable new data treatment strategies.}
}
@incollection{FAULKNER202023,
title = {2 - System Safety Management},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {23-54},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000140},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Safety Management Paradigms, Data maturity models, Safety requirements, Assurance, Safety justification},
abstract = {A Safety Management System (SMS) is a collective description of the processes used to manage system safety. It can be decomposed into safety management by design (Safety I) and safety management as practised (Safety II). This chapter identifies the nine core elements of an SMS. It discusses the role of data in an SMS. It extends current safety management paradigms to data-centric organisations (Safety II+ and Safety III). Data maturity models are a key element in this management activity. System Safety engineering is about identifying safety requirements on relevant elements of the system and assuring that these requirements have been met. This chapter investigates the use of a set of safety assurance processes to assure the safety characteristics of a data-centric system. To do this it introduces data paths and extends current failure and error classifications to incorporate decision making via data.}
}