@article{WANG201941,
title = {Integration of BIM and GIS in sustainable built environment: A review and bibliometric analysis},
journal = {Automation in Construction},
volume = {103},
pages = {41-52},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518309828},
author = {Hao Wang and Yisha Pan and Xiaochun Luo},
keywords = {BIM, GIS, Integration, AEC, Sustainable built environment},
abstract = {Building information modelling (BIM) and geographical information systems (GIS) provide digital representation of architectural and environmental entities. BIM focuses on micro-level representation of buildings themselves, and GIS provide macro-level representation of the external environments of buildings. Moreover, their combination can establish a comprehensive view of a built environment based on data integrated, which underpins the development and transition of the architecture, engineering and construction (AEC) industries in the digital era. This paper gives a comprehensive review on BIM-GIS integration in sustainable built environments in order to analyse the status quo and practical applications from four viewpoints: technologies for data integration, applications in the life cycle of AEC projects, building energy management, and urban governance. Three typical modes of BIM-GIS integration, namely, “BIM leads and GIS supports”, “GIS leads and BIM supports”, and “BIM and GIS are equally involved”, are categorised based on the different dominant positions of the two technologies. Furthermore, the research trends and future directions for the applications of BIM-GIS integration are discussed. Specifically, we underline that semantic models and third-party integration platforms should be optimised technically, and information about the whole process of AEC projects needs to be improved. Comprehensive information for building energy management should be digitised and quantified to improve its systematic integration and application to the urban built environment. This review can serve as a roadmap for researchers who focus on studies of BIM-GIS integration in the sustainable built environment.}
}
@article{GAO20191,
title = {Computational socioeconomics},
journal = {Physics Reports},
volume = {817},
pages = {1-104},
year = {2019},
note = {Computational Socioeconomics},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0370157319301954},
author = {Jian Gao and Yi-Cheng Zhang and Tao Zhou},
keywords = {Socio-economic systems, Complex networks, Socioeconomic status, Economic development, Data mining, Machine learning},
abstract = {Uncovering the structure of socioeconomic systems and timely estimation of socioeconomic status are significant for economic development. The understanding of socioeconomic processes provides foundations to quantify global economic development, to map regional industrial structure, and to infer individual socioeconomic status. In this review, we will make a brief manifesto about a new interdisciplinary research field named Computational Socioeconomics, followed by detailed introduction about data resources, computational tools, data-driven methods, theoretical models and novel applications at multiple resolutions, including the quantification of global economic inequality and complexity, the map of regional industrial structure and urban perception, the estimation of individual socioeconomic status and demographic, and the real-time monitoring of emergent events. This review, together with pioneering works we have highlighted, will draw increasing interdisciplinary attentions and induce a methodological shift in future socioeconomic studies.}
}
@article{DHANVIJAY2019113,
title = {Internet of Things: A survey of enabling technologies in healthcare and its applications},
journal = {Computer Networks},
volume = {153},
pages = {113-131},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619302695},
author = {Mrinai M. Dhanvijay and Shailaja C. Patil},
keywords = {IoT, WBAN, Healthcare system, Body sensor},
abstract = {Internet of Things (IoT) on the Wireless Body Area Network (WBAN) for healthcare applications is an operative scenario for IoT devices that has gained attention from vast research fields in recent years. The IoT connects all subjects and the healthcare system seamlessly. This paper describes the WBAN based IoT healthcare system and reviews the state-of-the-art of the network architecture topology and applications in the IoT based healthcare solutions. Moreover, this paper analyzes the security and the privacy features consisting of privacy, authentication, energy, power, resource management, Quality of Services and the real-time wireless health monitoring that are quite problematic in many IoT healthcare architectures. Because, system architecture is not well-defined, data restriction and its integrity preservation is still a challenge. At present 90% of the information available is acquired in the recent two years. This survey mainly aims at analyzing healthcare purpose which is based on digital healthcare system. Further, it reports many IoT and the e-healthcare policies and systems that decide how to ease all bearable development. Thus, the overall system provides large possibilities for future research based on IoT healthcare system. Finally, research gaps are reviewed and the possible future aspects have been discussed.}
}
@article{LIU20191102,
title = {Relevance of the intestinal health-related pathways to broiler residual feed intake revealed by duodenal transcriptome profiling},
journal = {Poultry Science},
volume = {98},
number = {3},
pages = {1102-1110},
year = {2019},
issn = {0032-5791},
doi = {https://doi.org/10.3382/ps/pey506},
url = {https://www.sciencedirect.com/science/article/pii/S0032579119303827},
author = {Ranran Liu and Jie Liu and Guiping Zhao and Wei Li and Maiqing Zheng and Jie Wang and Qinghe Li and Huanxian Cui and Jie Wen},
keywords = {RFI, broiler, intestinal health, transcriptome},
abstract = {ABSTRACT
In broiler production, there is a continuous effort to breed feed efficient chickens. Residual feed intake (RFI) is an accurate indicator that has been accepted as an alternative measure of the conventional feed conversion ratio. This study conducted a duodenal transcriptome survey to explore the molecular basis of broiler RFI. Results showed that there are 599 genes that were differentially expressed (DE) in the duodenum between high RFI and low RFI (LRFI) broilers. Functional analysis showed that RFI can be explained by differences in the regulation of the immune system process, complement activation, nutrient digestion, and absorption pathways. Among those processes, the glutathione S-transferase family and serpin family are involved in glutathione metabolism and TGF-β signaling. These genes are involved in complement and coagulation cascade pathways that constitute a new regulatory network to reduce oxidative stress and inflammatory reaction, as well as to improve the defense capability in LRFI broilers. Ten DE genes related to the digestive tract health and digestive function, CCK, MPEG1, EPHB2, SERPINH1, VANGL2, CYFIP2, PCDH19, TGFBI, SCUBE3and CATHL1, were identified as candidate genes related to RFI. In conclusion, the results indicate that there is less oxidative stress, less inflammatory reactions, and better digestion and absorption in the duodenum of the LRFI broilers, which might result in improved intestinal health and contribute to an increase in the efficiency of feed conversion.}
}
@article{FUNG201919,
title = {Using SNOMED CT-encoded problems to improve ICD-10-CM coding—A randomized controlled experiment},
journal = {International Journal of Medical Informatics},
volume = {126},
pages = {19-25},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618311559},
author = {Kin Wah Fung and Julia Xu and S. Trent Rosenbloom and James R. Campbell},
keywords = {SNOMED CT, ICD-10-CM, Inter-terminology mapping, Administrative codes, Coding quality},
abstract = {Objective
Clinical problems in the Electronic Health Record that are encoded in SNOMED CT can be translated into ICD-10-CM codes through the NLM’s SNOMED CT to ICD-10-CM map (NLM Map). This study evaluates the potential benefits of using the map-generated codes to assist manual ICD-10-CM coding.
Methods
De-identified clinic notes taken by the physician during an outpatient encounter were made available on a secure web server and randomly assigned for coding by professional coders with usual coding or map-assisted coding. Map-assisted coding made use of the problem list maintained by the physician and the NLM Map to suggest candidate ICD-10-CM codes to the coder. A gold standard set of codes for each note was established by the coders using a Delphi consensus process. Outcomes included coding time, coding reliability as measured by the Jaccard coefficients between codes from two coders with the same method of coding, and coding accuracy as measured by recall, precision and F-score according to the gold standard.
Results
With map-assisted coding, the average coding time per note reduced by 1.5 min (p = 0.006). There was a small increase in coding reliability and accuracy (not statistical significant). The benefits were more pronounced in the more experienced than less experienced coders. Detailed analysis of cases in which the correct ICD-10-CM codes were not found by the NLM Map showed that most failures were related to omission in the problem list and suboptimal mapping of the problem list terms to SNOMED CT. Only 12% of the failures was caused by errors in the NLM Map.
Conclusion
Map-assisted coding reduces coding time and can potentially improve coding reliability and accuracy, especially for more experienced coders. More effort is needed to improve the accuracy of the map-suggested ICD-10-CM codes.}
}
@article{HAWKINS2019856,
title = {Beyond the local fishing hole: A preliminary study of pan-regional fishing in southern Ontario (ca. 1000 CE to 1750 CE)},
journal = {Journal of Archaeological Science: Reports},
volume = {24},
pages = {856-868},
year = {2019},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2019.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X18305200},
author = {Alicia L. Hawkins and Suzanne Needs-Howarth and Trevor J. Orchard and Eric J. Guiry},
keywords = {Dietary reconstruction, Great Lakes, Landscape archaeology, Ontario Iroquoian archaeology, Stable isotope analysis, Woodland archaeology, Zooarchaeology},
abstract = {During the Late Woodland period in what is now the Canadian province of Ontario, Indigenous peoples met their nutritional needs through a combination of maize horticulture, gathering, hunting, and fishing. Recent research on stable isotopes in human tissue (Pfeiffer et al. 2016) suggests that the protein component in the diet of one of the groups of Iroquoian-speaking peoples in Ontario varies over time and came in part from high trophic level fish taxa. We present a pilot study that examines similar questions by means of zooarchaeological data from >100 previously analysed zooarchaeological assemblages using Geographic Information Systems (GIS). Our findings indicate differences in the consumption of fish through time. In addition, we observe patterned variation across the landscape of southern Ontario. In areas close to Lake Ontario, the primary high tropic level fishes exploited were members of the family Salmonidae. By contrast, in the Lake Erie drainage, Sander spp., in the family Percidae, makes a greater contribution to zooarchaeological samples. These findings suggest that the Indigenous peoples exploiting these fish sources would have faced different challenges with respect to harvest technology and scheduling.}
}
@article{LIU2019302,
title = {Observability quantification of public transportation systems with heterogeneous data sources: An information-space projection approach based on discretized space-time network flow models},
journal = {Transportation Research Part B: Methodological},
volume = {128},
pages = {302-323},
year = {2019},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2019.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0191261518305976},
author = {Jiangtao Liu and Xuesong Zhou},
keywords = {System observability quantification, Information space, Heterogeneous data sources, Public transportation system, Dantzig–Wolfe decomposition},
abstract = {Focusing on how to quantify system observability in terms of different interested states, this paper proposes a modeling framework to systemically account for the multi-source sensor information in public transportation systems. By developing a system of linear equations and inequalities, an information space is generated based on the available data from heterogeneous sensor sources. Then, a number of projection functions are introduced to match the relation between the unique information space and different system states of interest, such as, the passenger flow/density on the platform or in the vehicle at specific time intervals, the path flow of each origin-destination pair, the earning collected from the tickets to different operation companies etc., in urban rail transit systems as our study object. Their corresponding observability represented by state estimate uncertainties is further quantified by calculating its maximum feasible state range in proposed space-time network flow models. All of proposed models are solved as linear programming models by Dantzig–Wolfe decomposition, and a k-shortest-path-based approximation approach is also proposed to solve our models in large-scale networks. Finally, numerical experiments are conducted to demonstrate our proposed methodology and algorithms.}
}
@article{LI2019116040,
title = {City type-oriented modeling electric power consumption in China using NPP-VIIRS nighttime stable light data},
journal = {Energy},
volume = {189},
pages = {116040},
year = {2019},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.116040},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219317347},
author = {Shuyi Li and Liang Cheng and Xiaoqiang Liu and Junya Mao and Jie Wu and Manchun Li},
keywords = {NPP-VIIRS, Electric power consumption, City type, Cluster analysis, Regression model},
abstract = {Accelerating urbanization has created tremendous pressure on the global environment and energy supply, making accurate estimates of energy use of great importance. Most current models for estimating electric power consumption (EPC) from nighttime light (NTL) imagery are oversimplified, ignoring influential social and economic factors. Here we propose first classifying cities by economic focus and then separately estimating each category’s EPC using NTL data. We tested this approach using statistical employment data for 198 Chinese cities, 2015 NTL data from the Visible Infrared Imaging Radiometer Suite (VIIRS), and annual electricity consumption statistics. We used cluster analysis of employment by sector to divide the cities into three types (industrial, service, and technology and education), then established a linear regression model for each city’s NTL and EPC. Compared with the estimation results before city classification (R2: 0.785), the R2 of the separately modeled service cities and technology and education cities increased to 0.866 and 0.830, respectively. However, the results for industrial cities were less consistent due to their more complex energy consumption structure. In general, using classification before modeling helps reflect factors affecting the relationship between EPC and NTL, making the estimation process more reasonable and improving the accuracy of the results.}
}
@article{ODWYER2019581,
title = {Smart energy systems for sustainable smart cities: Current developments, trends and future directions},
journal = {Applied Energy},
volume = {237},
pages = {581-597},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919300248},
author = {Edward O’Dwyer and Indranil Pan and Salvador Acha and Nilay Shah},
keywords = {Smart cities, Computational intelligence, Machine learning, Predictive control, Smart energy systems},
abstract = {Within the context of the Smart City, the need for intelligent approaches to manage and coordinate the diverse range of supply and conversion technologies and demand applications has been well established. The wide-scale proliferation of sensors coupled with the implementation of embedded computational intelligence algorithms can help to tackle many of the technical challenges associated with this energy systems integration problem. Nonetheless, barriers still exist, as suitable methods are needed to handle complex networks of actors, often with competing objectives, while determining design and operational decisions for systems across a wide spectrum of features and time-scales. This review looks at the current developments in the smart energy sector, focussing on techniques in the main application areas along with relevant implemented examples, while highlighting some of the key challenges currently faced and outlining future pathways for the sector. A detailed overview of a framework developed for the EU H2020 funded Sharing Cities project is also provided to illustrate the nature of the design stages encountered and control hierarchies required. The study aims to summarise the current state of computational intelligence in the field of smart energy management, providing insight into the ways in which current barriers can be overcome.}
}
@article{SUPARDIANTO20191308,
title = {The Role of Information Technology Usage on Startup Financial Management and Taxation},
journal = {Procedia Computer Science},
volume = {161},
pages = {1308-1315},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.246},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319581},
author = { Supardianto and Ridi Ferdiana and Selo Sulistyo},
keywords = {Information Technology, Finacial Governance, Digital Transformation, Purposive Sampling, Cloud Computing},
abstract = {The level of participation of taxpayers, particularly Startups in Indonesia, is still low. According to Directorate General of Taxes, two things at least cause the low rate: first is the high turnover rate of Startup; second is the low rate of financial literacy, which correlates with tax administration. Consequently, having good financial governance is a must. Furthermore, technology may significantly help startups in systematizing their financial governance activity; while at the same time may help the startup to progress. The purpose of this study was to investigate the role of the use of information technology on Startup financial governance and taxation. Data were collected through a survey conducted in Yogyakarta using 37 respondent who are small business or startups. A researcher–administered questionnaire survey method was used for data collection. The results reveal that many startups have used technology for financial governance — the use of technology as part of digital transformation such as paperless accounting or digitalizing documents previously were done manually, such as making invoices and other matters related to recording. Startup widely uses cloud-based applications. More than half of startups that conduct financial governance utilize internet-based applications or cloud computing. The applications used by startups are limited to recording transactions, but none of the startups have utilized the taxation application, especially for Final Income Tax for Startups. These finding can enable policymakers to develop information system which can use as financial governance and managing taxation simultaneously.}
}
@article{SRIVASTAVA2019139,
title = {Understanding the adoption and usage of data analytics and simulation among building energy management professionals: A nationwide survey},
journal = {Building and Environment},
volume = {157},
pages = {139-164},
year = {2019},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2019.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0360132319302604},
author = {Charu Srivastava and Zheng Yang and Rishee K. Jain},
keywords = {Building energy management, Energy efficiency, Survey, Decision-making, Data analytics, Simulation},
abstract = {Improving the energy-efficiency of our building stock is critical to meeting our worldwide sustainability goals. In response to this need, two key tools have emerged to help engineers, building scientists and energy managers understand building energy usage and derive energy-efficiency solutions: data analytics and simulation. While both data analytics and simulation hold significant promise, we lack a clear understanding on the use, barriers and expectations of both in the building energy management decision-making process. This study conducts a nationwide survey of 448 building energy management professionals in the United States to help elucidate: 1) what impacts the adoption of data analytics and simulation among building energy management professionals; 2) in what phases of the building energy management decision-making process are data analytics and simulation currently used; and 3) what are the barriers of use for data analytics and simulation and how can they be improved to better support building energy management decision-making. Overall, results indicate that professional domain plays a large role in associating the uses, barriers and expectations for data analytics and simulation. Results also indicate that data analytics and simulation could be coupled to leverage functionality as they are used in similar phases of the decision-making process. Lastly, results point to opportunities for improving the applicability of data analytics and simulation tools as well as training for both. In the end, this study aims to provide a quantitative basis for improving the efficacy and integration of data analytics and simulation in the building energy management domain.}
}
@article{ALOQAILY2019101842,
title = {An intrusion detection system for connected vehicles in smart cities},
journal = {Ad Hoc Networks},
volume = {90},
pages = {101842},
year = {2019},
note = {Recent advances on security and privacy in Intelligent Transportation Systems},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519301131},
author = {Moayad Aloqaily and Safa Otoum and Ismaeel Al Ridhawi and Yaser Jararweh},
keywords = {Smart city, Connected vehicles, Intrusion detection, Vehicular cloud computing, Smart transportation, Service-specific clusters, QoS, QoE},
abstract = {In the very near future, transportation will go through a transitional period that will shape the industry beyond recognition. Smart vehicles have played a significant role in the advancement of intelligent and connected transportation systems. Continuous vehicular cloud service availability in smart cities is becoming a crucial subscriber necessity which requires improvement in the vehicular service management architecture. Moreover, as smart cities continue to deploy diversified technologies to achieve assorted and high-performance cloud services, security issues with regards to communicating entities which share personal requester information still prevails. To mitigate these concerns, we introduce an automated secure continuous cloud service availability framework for smart connected vehicles that enables an intrusion detection mechanism against security attacks and provides services that meet users’ quality of service (QoS) and quality of experience (QoE) requirements. Continuous service availability is achieved by clustering smart vehicles into service-specific clusters. Cluster heads are selected for communication purposes with trusted third-party entities (TTPs) acting as mediators between service requesters and providers. The most optimal services are then delivered from the selected service providers to the requesters. Furthermore, intrusion detection is accomplished through a three-phase data traffic analysis, reduction, and classification technique used to identify positive trusted service requests against false requests that may occur during intrusion attacks. The solution adopts deep belief and decision tree machine learning mechanisms used for data reduction and classification purposes, respectively. The framework is validated through simulations to demonstrate the effectiveness of the solution in terms of intrusion attack detection. The proposed solution achieved an overall accuracy of 99.43% with 99.92% detection rate and 0.96% false positive and false negative rate of 1.53%.}
}
@article{RAMBONNET2019271,
title = {Making citizen science count: Best practices and challenges of citizen science projects on plastics in aquatic environments},
journal = {Marine Pollution Bulletin},
volume = {145},
pages = {271-277},
year = {2019},
issn = {0025-326X},
doi = {https://doi.org/10.1016/j.marpolbul.2019.05.056},
url = {https://www.sciencedirect.com/science/article/pii/S0025326X19304242},
author = {Liselotte Rambonnet and Suzanne C. Vink and Anne M. Land-Zandstra and Thijs Bosker},
keywords = {Citizen science, Microplastics, Macroplastics, Global, Plastic pollution},
abstract = {There is considerable scientific and societal concern about plastic pollution, which has resulted in citizen science projects to study the scale of the issue. Citizen science is a cost-effective way to gather data over a large geographical range while simultaneously raising public awareness on the problem. Because the experiences of researchers involved in these projects are not yet adequately covered, this paper presents the findings from ten semi-structured qualitative interviews with researchers leading a citizen science project on micro- or macroplastics. Our results show it is important to specify the goal(s) of the project and that expertise on communication and data science is needed. Furthermore, simple protocols, quality control, and engagement with volunteers and the public are key elements for successful projects. From these results, a framework with recommendations was drafted, which can be used by anyone who wants to develop or improve citizen science projects.}
}
@article{GREAVES2019570,
title = {Key questions about the future of laboratory medicine in the next decade of the 21st century: A report from the IFCC-Emerging Technologies Division},
journal = {Clinica Chimica Acta},
volume = {495},
pages = {570-589},
year = {2019},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cca.2019.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0009898119318820},
author = {Ronda F. Greaves and Sergio Bernardini and Maurizio Ferrari and Paolo Fortina and Bernard Gouget and Damien Gruson and Tim Lang and Tze Ping Loh and Howard A. Morris and Jason Y. Park and Markus Roessler and Peng Yin and Larry J. Kricka},
keywords = {Emerging technologies, Disruptive technologies, Laboratory medicine},
abstract = {This review advances the discussion about the future of laboratory medicine in the 2020s. In five major topic areas: 1. the “big picture” of healthcare; 2. pre-analytical factors; 3. Analytical factors; 4. post-analytical factors; and 5. relationships, which explores a next decade perspective on laboratory medicine and the likely impact of the predicted changes by means of a number of carefully focused questions that draw upon predictions made since 2013. The “big picture” of healthcare explores the effects of changing patient populations, the brain-to-brain loop, direct access testing, robots and total laboratory automation, and green technologies and sustainability. The pre-analytical section considers the role of different sample types, drones, and biobanks. The analytical section examines advances in point-of-care testing, mass spectrometry, genomics, gene and immunotherapy, 3D-printing, and total laboratory quality. The post-analytical section discusses the value of laboratory medicine, the emerging role of artificial intelligence, the management and interpretation of omics data, and common reference intervals and decision limits. Finally, the relationships section explores the role of laboratory medicine scientific societies, the educational needs of laboratory professionals, communication, the relationship between laboratory professionals and clinicians, laboratory medicine financing, and the anticipated economic opportunities and outcomes in the 2020's.}
}
@article{HUANG20193411,
title = {A novel energy demand prediction strategy for residential buildings based on ensemble learning},
journal = {Energy Procedia},
volume = {158},
pages = {3411-3416},
year = {2019},
note = {Innovative Solutions for Energy Transitions},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2019.01.935},
url = {https://www.sciencedirect.com/science/article/pii/S1876610219309804},
author = {Yao Huang and Yue Yuan and Huanxin Chen and Jiangyu Wang and Yabin Guo and Tanveer Ahmad},
keywords = {Building heating energy demand, Prediction, Machine learning, Ensemble learning},
abstract = {Energy demand prediction is able to improve the energy efficiency and energy savings of the residential buildings. In this article, the prediction models are developed based on ensemble learning methods which combined extreme gradient boosting, extreme learning machine, multiple linear regression with support vector regression. According to the importance analysis of the random forest method, the optimal set of feature variables is selected. Besides, a historical energy comprehensive variable named EWMA was added in the prediction models to improve the prediction accuracy. A ground source heat pump for residential buildings located in Henan, China, is selected as a model for examining 2-hour ahead heating load forecasting. Results showed that the proposed prediction model based on ensemble learning could reduce the MAE of the testing set prediction result, which ranged from 29.1% to 70%.}
}
@incollection{BWALYA2019407,
title = {20 - The smart city of Johannesburg, South Africa},
editor = {Leonidas Anthopoulos},
booktitle = {Smart City Emergence},
publisher = {Elsevier},
pages = {407-419},
year = {2019},
isbn = {978-0-12-816169-2},
doi = {https://doi.org/10.1016/B978-0-12-816169-2.00020-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128161692000201},
author = {Kelvin Joseph Bwalya},
keywords = {Smart city, innovation, strategy, pervasive access, intelligent decisions, Modderfontein, Johannesburg},
abstract = {The city of Johannesburg has stepped-up strategic efforts aimed at rebranding itself as a smart city and overall competitive city at the center of innovation toward improved life experiences. Smart cities are conceptualized upon networked infrastructures, business-led urban development, spatial intelligence, and environmental and/or social sustainability. All the different initiatives in this regard are implemented based on the Johannesburg 2040 Growth and Development Strategy that aims to ensure that Johannesburg has the state-of-the-art socioeconomic infrastructure that will ultimately improve citizens’ experience. The chapter focuses on Johannesburg’s attainment of sustainable urban development and Information and Communications Technology (ICT) maturity and specifically intends to highlight the different initiatives and programs that are being pursued toward smart city realization especially at a strategic and technical level. Out of the realization for the need to develop smart city initiatives, many South African cities, such as Durban, Cape Town, and Port Elizabeth, have jumped onto the bandwagon for designing smart city projects driven by the local contextual setting. The direct implication of many smart city interventions and projects in South Africa is that there will be improved experience of the cities by the citizens and that information will be accessed ubiquitously to make intelligent decisions anywhere and anytime.}
}
@article{NGUYEN201935,
title = {Multi-label classification via label correlation and first order feature dependance in a data stream},
journal = {Pattern Recognition},
volume = {90},
pages = {35-51},
year = {2019},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0031320319300123},
author = {Tien Thanh Nguyen and Thi Thu Thuy Nguyen and Anh Vu Luong and Quoc Viet Hung Nguyen and Alan Wee-Chung Liew and Bela Stantic},
keywords = {Multi-label classification, Multi-label learning, Online learning, Data stream, Concept drift, Label correlation, Feature dependence},
abstract = {Many batch learning algorithms have been introduced for offline multi-label classification (MLC) over the years. However, the increasing data volume in many applications such as social networks, sensor networks, and traffic monitoring has posed many challenges to batch MLC learning. For example, it is often expensive to re-train the model with the newly arrived samples, or it is impractical to learn on the large volume of data at once. The research on incremental learning is therefore applicable to a large volume of data and especially for data stream. In this study, we develop a Bayesian-based method for learning from multi-label data streams by taking into consideration the correlation between pairs of labels and the relationship between label and feature. In our model, not only the label correlation is learned with each arrived sample with ground truth labels but also the number of predicted labels are adjusted based on Hoeffding inequality and the label cardinality. We also extend the model to handle missing values, a problem common in many real-world data. To handle concept drift, we propose a decay mechanism focusing on the age of the arrived samples to incrementally adapt to the change of data. The experimental results show that our method is highly competitive compared to several well-known benchmark algorithms under both the stationary and concept drift settings.}
}
@article{MOLINILLO2019247,
title = {Smart city communication via social media: Analysing residents' and visitors' engagement},
journal = {Cities},
volume = {94},
pages = {247-255},
year = {2019},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0264275118318183},
author = {Sebastian Molinillo and Rafael Anaya-Sánchez and Alastair M. Morrison and J. Andres Coca-Stefaniak},
keywords = {Smart cities, Social media, Engagement, City branding, User-generated content (UGC)},
abstract = {This research applies a unique conceptual model and methodology incorporating popularity, commitment, and virality to measure the social media engagement with residents and visitors of smart cities and how they communicate ‘smart’ elements and their brands. Digital content analysis was applied to a sample of ten Spanish smart cities (including Barcelona, Bilbao, Madrid, Seville and Valencia, among others), with measurable and quantifiable elements of engagement (e.g., likes, shares and comments). The smart cities analysed achieved acceptable, but rudimentary, levels of engagement via social media using Facebook, Twitter and Instagram. However, they displayed weaknesses related to their image and branding as well as the effectiveness with which they communicated their smart characteristics. The main implication of this research is that these Spanish smart cities have considerable scope to improve their use of social media to enhance their communications and branding. Greater emphasis is required on delivering emotional (affective) messages and a higher priority needs to be given to business and business event travellers and those visiting friends and relatives.}
}
@incollection{HEIJLEN201983,
title = {4 - The smart city of Leuven},
editor = {Leonidas Anthopoulos},
booktitle = {Smart City Emergence},
publisher = {Elsevier},
pages = {83-104},
year = {2019},
isbn = {978-0-12-816169-2},
doi = {https://doi.org/10.1016/B978-0-12-816169-2.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128161692000043},
author = {Roel Heijlen and Joep Crompvoets},
keywords = {Leuven, smart city vision, Triple Helix, implementation challenges, SMART model},
abstract = {Like many cities around the globe, Leuven, a Belgian medium-sized city characterized by its knowledge institutions and large presence of students, aims to be a smart city. The vision of Smart City Leuven has recently been outlined by a Triple Helix model. Taking into account the unique selling points and particular needs of Leuven, representatives of the model identified focus areas and actions to guideline Leuven’s road to smartness. This chapter examines the current conceptualization of Smart City Leuven and discusses future challenges regarding implementation of its vision. To evaluate Leuven’s smart city vision the SMART model will be applied.}
}
@article{SCHEMPP2019101143,
title = {A framework to integrate social media and authoritative data for disaster relief detection and distribution optimization},
journal = {International Journal of Disaster Risk Reduction},
volume = {39},
pages = {101143},
year = {2019},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2019.101143},
url = {https://www.sciencedirect.com/science/article/pii/S2212420918309518},
author = {Timothy Schempp and Haoran Zhang and Alexander Schmidt and Minsung Hong and Rajendra Akerkar},
keywords = {Disaster relief, Disaster management, Social media, Optimization},
abstract = {In this paper, we propose an interdisciplinary approach to (natural) disaster relief management. Our framework combines dynamic and static databases, which consist of social media and authoritative data of an afflicted region, respectively, to model rescue demand during a disaster situation. Using Global Particle Swarm Optimization and Mixed-Integer Linear Programming, we then determine the optimal amount and locations of temporal rescue centers. Furthermore, our disaster relief system identifies an efficient distribution of supplies between hospitals and rescue centers and rescue demand points. By leveraging the temporal dimension of the social media data, our framework manages to iteratively optimize the disaster relief distribution.}
}
@article{SCHMIDT2019100552,
title = {Blockchain and supply chain relations: A transaction cost theory perspective},
journal = {Journal of Purchasing and Supply Management},
volume = {25},
number = {4},
pages = {100552},
year = {2019},
issn = {1478-4092},
doi = {https://doi.org/10.1016/j.pursup.2019.100552},
url = {https://www.sciencedirect.com/science/article/pii/S1478409218301298},
author = {Christoph G. Schmidt and Stephan M. Wagner},
keywords = {Digitalization, Blockchain, Supply chain management, Boundary conditions, Transaction cost theory},
abstract = {Blockchain is projected to be the latest revolutionary technology and is gaining increasing attention from academics and practitioners. Blockchain is essentially a distributed and immutable database that enables more efficient and transparent transactions. The consensus-based record validation can eliminate the need for a trusted intermediary. We utilize the transaction cost theory to create a better understanding of how blockchain might influence supply chain relations, specifically in terms of transaction costs and governance decisions. Conceptually developing a set of six propositions, we argue that blockchain limits opportunistic behavior, the impact of environmental and behavioral uncertainty. Blockchain reduces transaction costs, as it allows for transparent and valid transactions. We explore several areas for future research on how blockchain might shape supply chain management in the future.}
}
@article{BRUNELLI2019248,
title = {Deep Learning-based Production Forecasting in Manufacturing: a Packaging Equipment Case Study},
journal = {Procedia Manufacturing},
volume = {38},
pages = {248-255},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920300342},
author = {Luca Brunelli and Chiara Masiero and Diego Tosato and Alessandro Beghi and Gian Antonio Susto},
keywords = {Data Mining, Deep Learning, Equipment Provider, Food Industry, Industry 4.0, Overall Equipment Effectiveness, Packaging},
abstract = {We propose a Deep Learning (DL)-based approach for production performance forecasting in fresh products packaging. On the one hand, this is a very demanding scenario where high throughput is mandatory; on the other, due to strict hygiene requirements, unexpected downtime caused by packaging machines can lead to huge product waste. Thus, our aim is predicting future values of key performance indexes such as Machine Mechanical Efficiency (MME) and Overall Equipment Effectiveness (OEE). We address this problem by leveraging DL-based approaches and historical production performance data related to measurements, warnings and alarms. Different architectures and prediction horizons are analyzed and compared to identify the most robust and effective solutions. We provide experimental results on a real industrial case, showing advantages with respect to current policies implemented by the industrial partner both in terms of forecasting accuracy and maintenance costs. The proposed architecture is shown to be effective on a real case study and it enables the development of predictive services in the area of Predictive Maintenance and Quality Monitoring for packaging equipment providers.}
}
@article{STEPHENSON20192262,
title = {Preconception health in England: a proposal for annual reporting with core metrics},
journal = {The Lancet},
volume = {393},
number = {10187},
pages = {2262-2271},
year = {2019},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(19)30954-7},
url = {https://www.sciencedirect.com/science/article/pii/S0140673619309547},
author = {Judith Stephenson and Christina Vogel and Jennifer Hall and Jayne Hutchinson and Sue Mann and Helen Duncan and Kathryn Woods-Townsend and Simon {de Lusignan} and Lucilla Poston and Janet Cade and Keith Godfrey and Mark Hanson and Geraldine Barrett and Mary Barker and Gabriella Conti and Geordan Shannon and Tim Colbourn},
abstract = {Summary
There is growing interest in preconception health as a crucial period for influencing not only pregnancy outcomes, but also future maternal and child health, and prevention of long-term medical conditions. Successive national and international policy documents emphasise the need to improve preconception health, but resources and action have not followed through with these goals. We argue for a dual intervention strategy at both the public health level (eg, by improving the food environment) and at the individual level (eg, by better identification of those planning a pregnancy who would benefit from support to optimise health before conception) in order to raise awareness of preconception health and to normalise the notion of planning and preparing for pregnancy. Existing strategies that target common risks factors, such as obesity and smoking, should recognise the preconception period as one that offers special opportunity for intervention, based on evidence from life-course epidemiology, developmental (embryo) programming around the time of conception, and maternal motivation. To describe and monitor preconception health in England, we propose an annual report card using metrics from multiple routine data sources. Such a report card should serve to hold governments and other relevant agencies to account for delivering interventions to improve preconception health.}
}
@article{HUOVILA2019141,
title = {Comparative analysis of standardized indicators for Smart sustainable cities: What indicators and standards to use and when?},
journal = {Cities},
volume = {89},
pages = {141-153},
year = {2019},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2019.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0264275118309120},
author = {Aapo Huovila and Peter Bosch and Miimu Airaksinen},
keywords = {Urban indicator, Smart sustainable cities, Standardization, Monitoring, Decision making, Sustainable Development Goals},
abstract = {City managers need indicators for target setting, performance assessment, monitoring, management and decision-making purposes. The choice of the most suitable indicator framework is crucial, but difficult, as it requires expert knowledge. To help cities in their choice, this paper compares seven recently published indicator standards for Smart sustainable cities. A taxonomy was developed to evaluate each of their 413 indicators against five conceptual urban focuses (types of urban sustainability and smartness), ten sectoral application domains (energy, transport, ICT, economy, etc.) and five indicator types (input, process, output, outcome, impact). The results clearly discriminate between indicator standards suited for evaluating the implementation of predominantly smart city approaches versus standards more focused on sustainability assessment. A further distinction is possible in standards almost fully oriented towards impacts reached, and standards that allow for progress evaluation according to steps in the implementation process. Some standards provide a narrow focus on output indicators evaluating the progress in implementing smart urban ICT solutions (e.g. number of smart meters installed). Cities are encouraged to complement such evaluations with impact indicators that demonstrate the effects of those solutions. This paper provides guidance for city managers and policy makers to select the indicators and standard that best correspond to their assessment need and goals, and align with their stage in Smart sustainable city implementation.}
}
@article{MCGHIN201962,
title = {Blockchain in healthcare applications: Research challenges and opportunities},
journal = {Journal of Network and Computer Applications},
volume = {135},
pages = {62-75},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519300864},
author = {Thomas McGhin and Kim-Kwang Raymond Choo and Charles Zhechao Liu and Debiao He},
keywords = {Blockchain, Healthcare industry, Authentication, IoT, Wireless, Vulnerabilities, Survey},
abstract = {Blockchain has a range of built-in features, such as distributed ledger, decentralized storage, authentication, security, and immutability, and has moved beyond hype to practical applications in industry sectors such as Healthcare. Blockchain applications in the healthcare sector generally require more stringent authentication, interoperability, and record sharing requirements, due to exacting legal requirements, such as Health Insurance Portability and Accountability Act of 1996 (HIPAA). Building on existing blockchain technologies, researchers in both academia and industry have started to explore applications that are geared toward healthcare use. These applications include smart contracts, fraud detection, and identity verification. Even with these improvements, there are still concerns as blockchain technology has its own specific vulnerabilities and issues that need to be addressed, such as mining incentives, mining attacks, and key management. Additionally, many of the healthcare applications have unique requirements that are not addressed by many of the blockchain experiments being explored, as highlighted in this survey paper. A number of potential research opportunities are also discussed in this paper.}
}
@article{ZHU2019164,
title = {Understanding an urbanizing planet: Strategic directions for remote sensing},
journal = {Remote Sensing of Environment},
volume = {228},
pages = {164-182},
year = {2019},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0034425719301658},
author = {Zhe Zhu and Yuyu Zhou and Karen C. Seto and Eleanor C. Stokes and Chengbin Deng and Steward T.A. Pickett and Hannes Taubenböck},
keywords = {Urbanization, UN Sustainable Development Goals (SDGs), Review, Urban, Human dimensions, Environmental change},
abstract = {Scientific contributions from remote sensing over the last fifty years have significantly advanced our understanding of urban areas. Key contributions of urban remote sensing include but are not limited to characterization of urban areas, urban land cover changes and thermal remote sensing of urban climates. Today, the proliferation of new sensors, long time series of the satellite record, joint analysis of Earth observation data with ancillary data sets, widespread availability of high-performance computing facilities, and slow but increasing use of remote sensing data and methods in non-remote sensing fields together offer new opportunities to generate scientific knowledge for an urbanizing planet. Simultaneously, the scale and pace of contemporary urbanization require new information about urban areas from both the science and policy communities. This paper examines remote sensing contributions to the scientific understanding of urban areas over the last 50 years until today. Based on this assessment and current needs by user communities, we identify four strategic directions for future urban remote sensing research: high temporal frequency analysis, characterization of urban heterogeneity, characterization of urban form and structure in two and three dimensions, and linking remote sensing with emerging urban data. Advances in these four areas are likely to generate significant new insights that will be useful to both science and policy.}
}
@article{WANG201961,
title = {An industrial missing values processing method based on generating model},
journal = {Computer Networks},
volume = {158},
pages = {61-68},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619301781},
author = {Huan Wang and Zhaolin Yuan and Yibin Chen and Bingyang Shen and Aixiang Wu},
keywords = {DAE, GAN, Generating model, IIOT, MCAR, Missing values},
abstract = {The issue of missing values (MVs) has been found widely in real-world datasets and obstructed the use of many statistical or machine learning algorithms for data analytics due to their incompetence in processing incomplete datasets. Most of the current MVs filling methods are applied to the datasets with certain specific types or low missing rate. This paper proposes a method of missing values processing based on the combination of denoising autoencoder (DAE) and generative adversarial networks (GAN), aiming at the missing completely at random (MCAR) datasets with high missing rate and noise interference in industrial scenes. We execute the training process on a discrete dataset with missing values, in order to ensure the generated dataset is completely similar to the feature distribution of the original dataset. We conduct our experiments for different dimensional datasets to prove the feasibility and efficiency of this method, including three public authority datasets and an industrial production monitoring dataset. The results compared with traditional missing values imputation methods have shown when the missing rate is higher than 30%, our method performs better in robustness and accuracy.}
}
@article{DENG2019105006,
title = {Field detection and classification of citrus Huanglongbing based on hyperspectral reflectance},
journal = {Computers and Electronics in Agriculture},
volume = {167},
pages = {105006},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105006},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919307574},
author = {Xiaoling Deng and Zixiao Huang and Zheng Zheng and Yubin Lan and Fen Dai},
keywords = {Citrus Huanglongbing, Hyperspectral, Band selection, Classification, Machine learning},
abstract = {Citrus Huanglongbing (HLB), also called citrus greening, is the most destructive disease in the citrus industry. Detecting the disease as early as possible and then eradicating infected roots can effectively control its spread. For the Shatangju mandarin cultivar, a non-destructive citrus HLB field detection method based on hyperspectral reflectance is proposed in this study. A characteristic band extraction method based on entropy distance and sequential backward selection is explored. Several machine learning algorithms (logistic regression, decision tree, support vector machine, k-nearest neighbor, linear discriminant analysis, and ensemble learning) were used to discriminate between disease groups: healthy, symptomatic HLB-infected, and asymptomatic HLB-infected, based on leaf reflectance. The results showed that the use of primary hyperspectral reflectance is very feasible for such classification. The band selection method proposed in this study provides an option for dimensionality reduction while still providing high classification accuracy. In three-group classification, the SVM learner achieved 90.8% accuracy, while in two-group classification (healthy vs symptomatic HLB leaves), the accuracy reached to 96%. The results also show that using only a few bands is insufficient for classification. In this study, 13 characteristic bands extracted by the proposed method provided the best performance.}
}
@article{RASHID201948,
title = {Completeness and consistency analysis for evolving knowledge bases},
journal = {Journal of Web Semantics},
volume = {54},
pages = {48-71},
year = {2019},
note = {Managing the Evolution and Preservation of the Data Web},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300623},
author = {Mohammad Rifat Ahmmad Rashid and Giuseppe Rizzo and Marco Torchiano and Nandana Mihindukulasooriya and Oscar Corcho and Raúl García-Castro},
keywords = {Quality assessment, Evolution analysis, Validation, Knowledge base, RDF shape, Machine learning},
abstract = {Assessing the quality of an evolving knowledge base is a challenging task as it often requires to identify correct quality assessment procedures. Since data is often derived from autonomous, and increasingly large data sources, it is impractical to manually curate the data, and challenging to continuously and automatically assess their quality. In this paper, we explore two main areas of quality assessment related to evolving knowledge bases: (i) identification of completeness issues using knowledge base evolution analysis, and (ii) identification of consistency issues based on integrity constraints, such as minimum and maximum cardinality, and range constraints. For the completeness analysis, we use data profiling information from consecutive knowledge base releases to estimate completeness measures that allow predicting quality issues. Then, we perform consistency checks to validate the results of the completeness analysis using integrity constraints and learning models. The approach has been tested both quantitatively and qualitatively by using a subset of datasets from both DBpedia and 3cixty knowledge bases. The performance of the approach is evaluated using precision, recall, and F1 score. From completeness analysis, we observe a 94% precision for the English DBpedia KB and 95% precision for the 3cixty Nice KB. We also assessed the performance of our consistency analysis by using five learning models over three sub-tasks, namely minimum cardinality, maximum cardinality, and range constraint. We observed that the best performing model in our experimental setup is Random Forest, reaching an F1 score greater than 90% for minimum and maximum cardinality and 84% for range constraints.}
}
@incollection{KOTU201919,
title = {Chapter 2 - Data Science Process},
editor = {Vijay Kotu and Bala Deshpande},
booktitle = {Data Science (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {19-37},
year = {2019},
isbn = {978-0-12-814761-0},
doi = {https://doi.org/10.1016/B978-0-12-814761-0.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128147610000022},
author = {Vijay Kotu and Bala Deshpande},
keywords = {CRISP, KDD, data mining process, prior knowledge, modeling, data preparation, evaluation, application},
abstract = {Successfully uncovering patterns using data science is an iterative process. This chapter provides a framework to solve a data science problem. The five-step process outlined in this chapter provides guidelines on gathering subject matter expertise; exploring the data with statistics and visualization; building a model using data science algorithms; testing the model and deploying it in a production environment; and finally reflecting on new knowledge gained in the cycle. Over the years of evolution of data mining and data science practices, different frameworks have been put forward by various academic and commercial bodies, like the Cross Industry Standard Process for Data Mining, knowledge discovery in databases, etc. These data science frameworks exhibit common characteristics, and hence, generic framework closely resembling the CRISP process will be used.}
}
@article{CSOKNYAI20191,
title = {Analysis of energy consumption profiles in residential buildings and impact assessment of a serious game on occupants’ behavior},
journal = {Energy and Buildings},
volume = {196},
pages = {1-20},
year = {2019},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0378778818334790},
author = {Tamás Csoknyai and Jeremy Legardeur and Audrey Abi Akle and Miklós Horváth},
keywords = {Serious game, Smart metering, Occupants’ behavior, Energy consumption trends and profiles, Energy performance of residential buildings, Demand side management},
abstract = {The paper has a focus on energy consumption habits, trends and intervention strategies in residential buildings, mainly through the serious game approach with a combination of direct consumer feedback through smart metering. More than 150 homes in France and Spain have been involved in the research experiment and the consumption habits of approximately 50 homes were deeply analyzed. The applied methods, processes, results and findings of the monitoring data analysis are presented in the paper with two aims. First, consumption profiles and trends were determined for apartment homes with regard to heating, domestic hot water and electric consumption. Second, the impact of a serious game experiment was assessed comparing energy consumption, indoor air temperature and users’ habits (based on questionnaires) before and after launching the experiment.}
}
@article{ZHANG2019e1,
title = {China Kidney Disease Network (CK-NET) 2015 Annual Data Report},
journal = {Kidney International Supplements},
volume = {9},
number = {1},
pages = {e1-e81},
year = {2019},
note = {China Kidney Disease Network (CK-NET) 2015 Annual Data Report},
issn = {2157-1716},
doi = {https://doi.org/10.1016/j.kisu.2018.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2157171618300339},
author = {Luxia Zhang and Ming-Hui Zhao and Li Zuo and Yue Wang and Feng Yu and Hong Zhang and Haibo Wang and Kunhao Bai and Rui Chen and Hong Chu and Lanxia Gan and Bixia Gao and Xiangxiang He and Lili Liu and Jianyan Long and Ying Shi and Zaiming Su and Xiaoyu Sun and Wen Tang and Fang Wang and Haibo Wang and Jinwei Wang and Song Wang and Yue Wang and Chao Yang and Feng Yu and Dongliang Zhang and Hong Zhang and Luxia Zhang and Minghui Zhao and Xinju Zhao and Liren Zheng and Zhiye Zhou and Li Zuo}
}
@article{GHALEM20199,
title = {A probabilistic multivariate copula-based technique for faulty node diagnosis in wireless sensor networks},
journal = {Journal of Network and Computer Applications},
volume = {127},
pages = {9-25},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303680},
author = {Sanaa Kawther Ghalem and Bouabdellah Kechar and Ahcène Bounceur and Reinhardt Euler},
keywords = {Anomalies, Outlier detection, Probabilistic, Multivariate, Copula, Wireless sensor network, Dependency},
abstract = {Wireless sensor networks (WSNs) find extensive applications in various sensitive domains such as tracking, monitoring, environmental data collection and border surveillance. In these cases, the collected data are considered as a critical resource and used to detect any anomalies or abnormal behavior, providing information about an occurring event or a node failure. An outlier detection process must be set up to ensure the proper functioning of the monitoring system. The existing approaches are limited by assumptions on a specific distribution or a predefined data range of the collected data. Often these assumptions do not hold in practice, the data distribution is not known or determining reliable upper and lower bounds for the set of data is not possible. To overcome this, we propose a new copula-based probabilistic multivariate outlier detection method for faulty node detection in wireless sensor networks (WSNs). The joint probability density function of the copula is constructed considering dependency among the captured n−sensed measures without making any assumptions on the distribution of the collected data. The samples having probabilities violating a predetermined control limit are classified to be faulty. The performance of the proposed technique is observed to be better than the existing statistical methods.}
}
@article{LIU2019254,
title = {Smart card data-centric replication of the multi-modal public transport system in Singapore},
journal = {Journal of Transport Geography},
volume = {76},
pages = {254-264},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0966692316307670},
author = {Xiaodong Liu and Yuan Zhou and Andreas Rau},
keywords = {Public transport, Smart card data, Direct assignment, Supply and demand},
abstract = {This paper proposes an innovative method of replicating the multi-modal public transport system in Singapore with high precision using smart card database. It replicates the operation of public transport system with known exogenous passenger demand and provides many operational details, including passenger inter-modal trip chains, operational timetable, and detailed transfer behaviour. The paper elaborates on the methodology of the replication including data cleaning, filtering, processing and converting the collected data to meaningful information such as bus journey trajectories and metro system timetable. Thereafter, actualised passenger trip chains are directly assigned to the replicated public transport supply. The resulting replication covers almost 96% of trips made in public transport in Singapore. It provides solid quantitative information on several aspects to support decision making, including precise temporal and spatial travel demand analysis, transfer pattern analysis, traffic condition investigation and bus utilisation analysis.}
}
@article{LAM2019146,
title = {Potential pitfalls in the development of smart cities and mitigation measures: An exploratory study},
journal = {Cities},
volume = {91},
pages = {146-156},
year = {2019},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2018.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0264275117314294},
author = {Patrick T.I. Lam and Ruiqu Ma},
keywords = {Smart cities, Potential pitfalls, Information insecurity, Privacy leakage, Information islands, Digital divide},
abstract = {The rapid proliferation of smart city (SC) projects is a response to the challenges posed by rapid urbanization such as energy shortage, economic reconstruction (the drive towards higher productivity and efficiency) and demographic increase. Ubiquitous information communication technologies (ICTs) in SCs enable people to understand and manage cities more efficiently and sustainably, thus improving their life quality. However, a number of potential pitfalls have been noted in the development of SC. This study aims at identifying potential pitfalls in the development of SCs and filling a knowledge gap in this domain. Based on an extensive literature review, four major pitfalls are categorized as system information insecurity, personal privacy leakage, information islands, and digital divide. Possible causes and adverse effects of these pitfalls are discussed with the aid of three international case reviews. In addition, this study looks into existing assessment schemes of SC performance that are mainly focused on the positive and functional capability, but sparingly evaluate the possible downsides. It is argued that a SC cannot claim to be successful by solely measuring how much it has done or what it aims to achieve. While most studies focus on the benefits of SCs, this research reveals the challenges facing city planners. It contributes to the body of knowledge in this regard and also provides an insight into the subject matter. A framework for conducting further research on mitigating potential SC pitfalls has been laid. It is intended to inform practitioners, researchers and policymakers to develop proactive solutions concerning both technological and non-technological aspects at an early stage of SC developments.}
}
@article{RUAN2019101804,
title = {Adolescent binge drinking disrupts normal trajectories of brain functional organization and personality maturation},
journal = {NeuroImage: Clinical},
volume = {22},
pages = {101804},
year = {2019},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2019.101804},
url = {https://www.sciencedirect.com/science/article/pii/S2213158219301548},
author = {Hongtao Ruan and Yunyi Zhou and Qiang Luo and Gabriel H. Robert and Sylvane Desrivières and Erin Burke Quinlan and ZhaoWen Liu and Tobias Banaschewski and Arun L.W. Bokde and Uli Bromberg and Christian Büchel and Herta Flor and Vincent Frouin and Hugh Garavan and Penny Gowland and Andreas Heinz and Bernd Ittermann and Jean-Luc Martinot and Marie-Laure Paillère Martinot and Frauke Nees and Dimitri Papadopoulos Orfanos and Luise Poustka and Sarah Hohmann and Juliane H. Fröhner and Michael N. Smolka and Henrik Walter and Robert Whelan and Fei Li and Gunter Schumann and Jianfeng Feng},
keywords = {Adolescent, Binge drinking, Resting state, Personality, Genome, Co-development},
abstract = {Adolescent binge drinking has been associated with higher risks for the development of many health problems throughout the lifespan. Adolescents undergo multiple changes that involve the co-development processes of brain, personality and behavior; therefore, certain behavior, such as alcohol consumption, can have disruptive effects on both brain development and personality maturation. However, these effects remain unclear due to the scarcity of longitudinal studies. In the current study, we used multivariate approaches to explore discriminative features in brain functional architecture, personality traits, and genetic variants in 19-year-old individuals (n = 212). Taking advantage of a longitudinal design, we selected features that were more drastically altered in drinkers with an earlier onset of binge drinking. With the selected features, we trained a hierarchical model of support vector machines using a training sample (n = 139). Using an independent sample (n = 73), we tested the model and achieved a classification accuracy of 71.2%. We demonstrated longitudinally that after the onset of binge drinking the developmental trajectory of improvement in impulsivity slowed down. This study identified the disrupting effects of adolescent binge drinking on the developmental trajectories of both brain and personality.}
}
@article{HUANG2019831,
title = {An assessment of technology forecasting: Revisiting earlier analyses on dye-sensitized solar cells (DSSCs)},
journal = {Technological Forecasting and Social Change},
volume = {146},
pages = {831-843},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.10.031},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517318541},
author = {Ying Huang and Alan L. Porter and Yi Zhang and Xiangpeng Lian and Ying Guo},
keywords = {Technology forecasting, Technology foresight, Technology emergence, Research evaluation, Dye-sensitized solar cells (DSSCs)},
abstract = {The increasingly uncertain dynamics of technological change pose special challenges to traditional technology forecasting tools, which facilitates future-oriented technology analysis (FTA) tools to support the policy processes in the fields of science, technology & innovation (ST&I) and the management of technology (MOT), rather than merely forecasting incremental advances via analyses of continuous trends. Dye-sensitized solar cells are a promising third-generation photovoltaic technology that can add functionality and lower costs to enhance the value proposition of solar power generation in the early years of the 21st century. Through a series of technological forecasting studies analyzing the R&D patterns and trends in Dye-sensitized solar cells technology over the past several years, we have come to realize that validating previous forecasts is useful for improving ST&I policy processes. Yet, rarely do we revisit forecasts or projections to ascertain how well they fared. Moreover, few studies pay much attention to assessing FTA techniques. In this paper, we compare recent technology activities with previous forecasts to reveal the influencing factors that led to differences between past predictions and actual performance. Beyond our main aim of checking accuracy, in this paper we also wish to gain some sense of how valid those studies were and whether they proved useful to others in some ways.}
}
@article{FAN2019105565,
title = {Application of Radiomics in Central Nervous System Diseases: a Systematic literature review},
journal = {Clinical Neurology and Neurosurgery},
volume = {187},
pages = {105565},
year = {2019},
issn = {0303-8467},
doi = {https://doi.org/10.1016/j.clineuro.2019.105565},
url = {https://www.sciencedirect.com/science/article/pii/S0303846719303610},
author = {Yanghua Fan and Ming Feng and Renzhi Wang},
keywords = {Central nervous system, Radiomics, Diagnose, Prognosis},
abstract = {Central nervous system (CNS) diseases are associated with complexity and diversity; as a result, it is urgent to search for a simple approach for effectively improving the clinical decision-making ability and precise treatment currently. Radiomics can collect plenty of quantitative features based on the massive medical image data; meanwhile, related diagnosis and prediction can be performed through quantitative analysis. The main steps of radiomics analysis include image collection as well as reconstruction, segmentation of the region of interest (ROI), feature extraction as well as quantification, and establishment of the predictive as well as prognostic models. Compared with traditional imaging features, radiomics allows to transform the visual image data to the in-depth features, so as to carry out quantitative research. Our findings suggest that radiomics has broad application prospects in the early screening, accurate diagnosis, grading and staging, treatment and prognosis, and molecular characteristics of CNS diseases, which can improve the capacities to diagnose and predict CNS diseases prognosis through complementing and combining with traditional imaging.}
}
@article{BIKKU2019100188,
title = {A novel somatic cancer gene-based biomedical document feature ranking and clustering model},
journal = {Informatics in Medicine Unlocked},
volume = {16},
pages = {100188},
year = {2019},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2019.100188},
url = {https://www.sciencedirect.com/science/article/pii/S2352914819300632},
author = {Thulasi Bikku and Radhika Paturi},
keywords = {Somatic cancer, Genomes, Bioinformatics, Feature selection, Feature ranking, Fuzzy clustering, Document clustering},
abstract = {Background
As the size of somatic genomes in biomedical repositories increases, it is essential to predict cancer related document sets using the machine learning models. Most of the traditional gene-based somatic cancer mining models are independent of somatic gene ranking and feature extraction due to high computational cost and memory for large datasets. A wide range of feature selection and feature extraction strategies are existing, and they are by and large generally utilized in various areas. Every one of these strategies plans to expel repetitive and irrelevant features from the trained datasets with the goal that the arrangement of new document data will be increasingly accurate. Data extraction is the activity of providing relevant data according to an information need from a collection of large resources of data
Results
Ranking consists of sorting the information offers according to some criterion, so that the “best” results appear in the top priority in the provided list. The mapping of somatic genomes and its equivalent words like synonyms to biomedical document ranking is intricate on vast biomedical document data sets. In order to overcome these limitations, a novel feature ranking based fuzzy clustering framework is designed and implemented on large biomedical databases
Conclusion
Experimental results are simulated with different cluster sizes and gene features for somatic document clustering. Experimental results proved that the present model has high computational cluster quality rate with document ranking for somatic gene-based document indexing.}
}
@article{VANDENHEUVEL2019512,
title = {Multiscale Neuroscience of Psychiatric Disorders},
journal = {Biological Psychiatry},
volume = {86},
number = {7},
pages = {512-522},
year = {2019},
note = {Schizophrenia: Genomics to Therapeutics},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2019.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0006322319313848},
author = {Martijn P. {van den Heuvel} and Lianne H. Scholtens and René S. Kahn},
keywords = {Brain network, Connectivity, Cross-scale, Mental disorders, Multiscale, Psychiatry},
abstract = {The human brain comprises a multiscale network with multiple levels of organization. Neurons with dendritic and axonal connections form the microscale fabric of brain circuitry, and macroscale brain regions and white matter connections form the infrastructure for system-level brain communication and information integration. In this review, we discuss the emerging trend of multiscale neuroscience, the multidisciplinary field that brings together data from these different levels of nervous system organization to form a better understanding of between-scale relationships of brain structure, function, and behavior in health and disease. We provide a broad overview of this developing field and discuss recent findings of exemplary multiscale neuroscience studies that illustrate the importance of studying cross-scale interactions among the genetic, molecular, cellular, and macroscale levels of brain circuitry and connectivity and behavior. We particularly consider a central, overarching goal of these multiscale neuroscience studies of human brain connectivity: to obtain insight into how disease-related alterations at one level of organization may underlie alterations observed at other scales of brain network organization in mental disorders. We conclude by discussing the current limitations, challenges, and future directions of the field.}
}
@article{SRINIVAS2019178,
title = {Government regulations in cyber security: Framework, standards and recommendations},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {178-188},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.09.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18316753},
author = {Jangirala Srinivas and Ashok Kumar Das and Neeraj Kumar},
keywords = {Cyber security, Cyber attacks, Information security, Government policies, Standards},
abstract = {Cyber security refers to the protection of Internet-connected systems, such as hardware, software as well as data (information) from cyber attacks (adversaries). A cyber security regulation is needed in order to protect information technology along with computer systems with the purpose of compelling various organizations as well as companies to protect their systems and information from cyber attacks. Several cyber attacks are possible, such as viruses, phishing, Trojan horses, worms, Denial-of-Service (DoS) attacks, illegal access (e.g., stealing intellectual property or confidential information) as well as control system attacks. In this article, we focus on importance of various standards in cyber defense, and architecture of cyber security framework. We discuss the security threats, attacks and measures in cyber security. We then discuss various standardization challenges in cyber security. We also discuss about the cyber security national strategy to secure cyberspace and also various government policies in protecting the cyber security. Finally, we provide some recommendations that are critical to cyber security and cyber defense.}
}
@article{AIN2019113113,
title = {Two decades of research on business intelligence system adoption, utilization and success – A systematic literature review},
journal = {Decision Support Systems},
volume = {125},
pages = {113113},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113113},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301423},
author = {NoorUl Ain and Giovanni Vaia and William H. DeLone and Mehwish Waheed},
keywords = {Business intelligence system, Adoption, Utilization, Success, Systematic literature review},
abstract = {In the recent era of technological advances and hyper-competition, business intelligence (BI) systems have attracted significant attention from executives and decision makers due to their ability to provide complex and competitive information inputs for the decision process. Following the world of practice, research into the adoption, utilization and success of BI systems has grown substantially over the past two decades. The literature suggests that organizations have largely failed to capture the benefits of BI systems to their full extent and are seeking ways to leverage value from the implemented systems. However, prior studies do not have any comprehensive study that discusses the issues and challenges related to adoption, utilization and success of BI systems. In this study, using a systematic literature review, we present comprehensive knowledge about what has been found in the domain of BI system adoption, utilization and success. A total of 111 peer-reviewed studies, covering three categories – adoption, utilization and success – published between 2000 and 2019, were selected. The findings present the research methods, underpinning theories and key factors employed to study BI system adoption, utilization and success. In addition, the review identified the key issues related to BI adoption, utilization and success and highlighted the areas that have attracted more or less attention. This study also suggests future directions for researchers and practitioners in terms of unexplored themes that may help organizations to obtain value from BI systems.}
}
@article{WELLS2019237,
title = {Transcriptional Profiling of Stem Cells: Moving from Descriptive to Predictive Paradigms},
journal = {Stem Cell Reports},
volume = {13},
number = {2},
pages = {237-246},
year = {2019},
issn = {2213-6711},
doi = {https://doi.org/10.1016/j.stemcr.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2213671119302607},
author = {Christine A. Wells and Jarny Choi},
keywords = {pluripotent stem cell, reprogramming, transcriptome, single-cell sequencing, bioinformatics},
abstract = {Transcriptional profiling is a powerful tool commonly used to benchmark stem cells and their differentiated progeny. As the wealth of stem cell data builds in public repositories, we highlight common data traps, and review approaches to combine and mine this data for new cell classification and cell prediction tools. We touch on future trends for stem cell profiling, such as single-cell profiling, long-read sequencing, and improved methods for measuring molecular modifications on chromatin and RNA that bring new challenges and opportunities for stem cell analysis.}
}
@article{SANCHEZREYNOSO2019293,
title = {Improving the Real-Time Searching in the Organizational Memory},
journal = {Procedia Computer Science},
volume = {154},
pages = {293-304},
year = {2019},
note = {Proceedings of the 9th International Conference of Information and Communication Technology [ICICT-2019] Nanning, Guangxi, China January 11-13, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919308129},
author = {María Laura {Sánchez Reynoso} and Mario Diván},
keywords = {Real-Time Searching, Organizational Memory, Measurement Projects, Structural Coefficient},
abstract = {The real-time data processing constitutes a critical area when talking about real-time decision making. Strong decisions are based on recommendations for describing the associated course of actions, but the real-time processing gives a very short time for searching them. The Processing Architecture based on Measurement Metadata is a data stream engine oriented to measurement projects, which supports the decision making through an organizational memory. The search space related to the organizational memory is initially in-memory limited using the structure of the measurement projects. Given a project, the related projects are ordered based on a given scoring from its structural definition. Here, a new structural coefficient based on the text similarity, which is computed from the textual definition of each descriptive attribute of a project is introduced. This allows better scoring of the related projects, even when its definitions could be affected by human errors or multiple definitions. The scoring is critical when in a given situation, a project has not specific experience for recommending, in such context, the recommendations from the near projects are served. The pabmm_sh library is outlined and a simulation on its associated processing times for the similarity computing are introduced based on the token definition for a measurement project. The library adds a new alternative perspective in the processing architecture for driving the searches into the organizational memory. It can update 2000 projects less than 1 second, keeping the individual processing time of each project under 1 millisecond.}
}
@article{VAHDATNEJAD2019321,
title = {Context-aware computing for mobile crowd sensing: A survey},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {321-332},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.052},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18329583},
author = {Hamed Vahdat-Nejad and Elham Asani and Zohreh Mahmoodian and Mohammad Hossein Mohseni},
keywords = {Mobile crowd sensing, Context-awareness, Functionalities, Survey},
abstract = {Today, the distribution of people with smart mobile devices has provided the opportunity for mobile crowd sensing. Several mobile crowd sensing systems have led to the collection of valuable information with a low infrastructural investment. These types of information have been used in context-aware systems to provide high-level services. In this paper, a comprehensive reference framework is proposed to investigate context-aware mobile crowd sensing systems from three viewpoints of concepts, context-awareness, and functionalities. Each of these aspects has one or more parameters, which investigate and classify the existing works. To this end, the paper characterizes domain and cooperation type, context-awareness, incentive mechanisms, data sharing, local analysis and global aggregation of mobile crowd sensing systems. The aim is to thoroughly review the existing works, foster the dissemination of state-of-the-art research, and present future research directions.}
}
@article{LUKOVIC2019247,
title = {An ontology-based module of the information system ScolioMedIS for 3D digital diagnosis of adolescent scoliosis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {178},
pages = {247-263},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719300665},
author = {Vanja Luković and Saša Ćuković and Danijela Milošević and Goran Devedžić},
keywords = {Ontology-based information system, Adolescent idiopathic scoliosis, Protégé-OWL API, Lenke classification},
abstract = {Background and objective
Conventional information systems are built on top of a relational database. The main weakness of these systems is impossibility to define stable data schema ahead when the knowledge of the system is evolving and dynamic. The widely accepted alternatives to relational databases are ontologies that can be used for designing information systems. Many research papers describe various methods for improving reliability and precision in generating the type of the Lenke classification based on the image processing techniques or a computer program, but all of them require radiograph images. The main objective of this paper is to demonstrate the development of an ontology-based module of the information system ScolioMedIS for adolescent idiopathic scoliosis (AIS) diagnosis and monitoring, which uses optical 3D methods to determine the Lenke classification of AIS and to avoid harmful effects of traditional radiation diagnosis.
Methods
For creating an ontology-based module of the ScolioMedIS we used the following steps: specification, conceptualization, formalization and implementation. In the specification and conceptualization phase we performed data collection and analysis to define domain, concepts and relationships for ontology design. In the formalization and implementation stage we developed the OBR-Scolio ontology and the ontology-based module of the ScolioMedIS. The module employs the Protégé-OWL API, as a collection of Java interfaces for the OBR-Scolio ontology, which enables the creating, deleting, and editing of the basic elements of the OBR-Scolio ontology, as well as the querying of the ontology.
Results
The ontology-based module of ScolioMedIS is tested on the datasets of 20 female and 15 male patients with AIS between the ages of 11 and 18, to categorize spinal curvatures and to automatically generate statistical indicators about the frequency of the basic spinal curvatures, degree of progression or regression of deformity and statistical indicators about curvature characteristics according to the Lenke classification system and Lenke scoliosis types. Results are then compared with analysis of the Lenke classification of 315 observed patients, performed using traditional radiation techniques.
Conclusions
This part of the system allows continuous monitoring of the progression/regression of spinal curvatures for each registered patient, which may provide a better management of scoliosis (diagnosis and treatment).}
}
@article{HAYAT20191040,
title = {A signaling game-based approach for Data-as-a-Service provisioning in IoT-Cloud},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {1040-1050},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17306891},
author = {Routaib Hayat and Essaid Sabir and Elarbi Badidi and Mohammed ElKoutbi},
keywords = {IoT, QoD, IoT-Cloud, Signaling game, Multi-attribute decision-making, Entropy},
abstract = {The impressive progress in sensing technology over the last few years has contributed to the proliferation and popularity of the Internet of Things (IoT) paradigm and to the adoption of Sensor Clouds for provisioning smart ubiquitous services. Also, the massive amount of data generated by sensors and smart objects led to a new kind of services known as Data-as-a-Service (DaaS). The quality of these services is highly dependent on the quality of sensed data (QoD), which is characterized by a number of quality attributes. DaaS provisioning is typically governed by a Service Level Agreement (SLA) between data consumers and DaaS providers. In this work, we propose a game-based approach for DaaS Provisioning, which relies on signaling based model for the negotiation of several QoD attributes between DaaS providers and data consumers. We consider that these entities are adaptive, rational, and able to negotiate the QoD offering even in the case of incomplete information about the other party. We use in the negotiation between the two parties a Q-learning algorithm for the signaling model and a Multi Attributes Decision Making (MADM) model to select the best signal. Moreover, we empirically validate the MADM model using Shannon’s entropy. The results obtained in the case of a multi-stages negotiation scenario show the convergence towards the pooling equilibrium.}
}
@article{GURDUR2019153,
title = {Digitalizing Swedish industry: What is next?: Data analytics readiness assessment of Swedish industry, according to survey results},
journal = {Computers in Industry},
volume = {105},
pages = {153-163},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518302987},
author = {Didem Gürdür and Jad El-khoury and Martin Törngren},
keywords = {Data analytics, Data analytics readiness, Swedish industry, Digitalization, Survey},
abstract = {Digitalization refers to enabling, improving, and transforming operations, functions, models, processes, or activities by leveraging digital technologies. Furthermore, digitalization is considered one of the most powerful drivers of innovation with the potential to trigger the next wave of innovation. Today, the importance of digitalization is well-understood in Swedish government agencies and industry. Although there are several initiatives working to actively drive change, one question is key: What is the next step? Data analytics is a promising way to turn information into outcomes, enhance decision-making, make data-driven discoveries, minimize risk, and unearth valuable insights that would otherwise remain hidden. This paper presents survey results on data analytics adoption and usage within Swedish industry, to highlight post-digitalization industry needs. To this end, a questionnaire was designed and distributed. Answers from more than 100 respondents from the manufacturing, technology, engineering, telecommunications, and automotive industries in Sweden were collected and analyzed. The assessment results show that Swedish industry has a high resources readiness score. This suggests that the necessary tools, and human resources are in place. Moreover, its cultural readiness level, which focuses on the acceptance of data-driven decision-making, scores between high and very high. At the same time, the information systems readiness level is in between medium and high, except in the telecommunication domain. However, the organizational readiness level is between medium and low, which shows that the organizations are not structured to enable the adaptation of data analytics and the business impacts of data analytics are not in place yet. These findings suggest that the industry should use the advantages of the current cultural, information systems, and resources readiness capabilities and concentrate efforts on exploring the business impacts of data analytics, ensuring the support from executive managers, and implementing data analytics protocols to improve organizational readiness. Moreover, the industry should consider structural changes in organizations, in addition to systematically initiating proper planning, timing, budgeting, and setting of clear key performance indicators/metrics in order to ameliorate the organizational readiness of data analytics.}
}
@article{XIA2019700,
title = {Reproducibility of functional brain alterations in major depressive disorder: Evidence from a multisite resting-state functional MRI study with 1,434 individuals},
journal = {NeuroImage},
volume = {189},
pages = {700-714},
year = {2019},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.01.074},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919300801},
author = {Mingrui Xia and Tianmei Si and Xiaoyi Sun and Qing Ma and Bangshan Liu and Li Wang and Jie Meng and Miao Chang and Xiaoqi Huang and Ziqi Chen and Yanqing Tang and Ke Xu and Qiyong Gong and Fei Wang and Jiang Qiu and Peng Xie and Lingjiang Li and Yong He},
keywords = {Brain network, Connectome, Depression, Reliability, ALFF, ReHo, Psychoradiology},
abstract = {Resting-state functional MRI (R-fMRI) studies have demonstrated widespread alterations in brain function in patients with major depressive disorder (MDD). However, a clear and consistent conclusion regarding a repeatable pattern of MDD-relevant alterations is still limited due to the scarcity of large-sample, multisite datasets. Here, we address this issue by including a large R-fMRI dataset with 1434 participants (709 patients with MDD and 725 healthy controls) from five centers in China. Individual functional activity maps that represent very local to long-range connections are computed using the amplitude of low-frequency fluctuations, regional homogeneity and distance-related functional connectivity strength. The reproducibility analyses involve different statistical strategies, global signal regression, across-center consistency, clinical variables, and sample size. We observed significant hypoactivity in the orbitofrontal, sensorimotor, and visual cortices and hyperactivity in the frontoparietal cortices in MDD patients compared to the controls. These alterations are not affected by different statistical analysis strategies, global signal regression and medication status and are generally reproducible across centers. However, these between-group differences are partially influenced by the episode status and the age of disease onset in patients, and the brain-clinical variable relationship exhibits poor cross-center reproducibility. Bootstrap analyses reveal that at least 400 subjects in each group are required to replicate significant alterations (an extent threshold of P < .05 and a height threshold of P < .001) at 50% reproducibility. Together, these results highlight reproducible patterns of functional alterations in MDD and relevant influencing factors, which provides crucial guidance for future neuroimaging studies of this disorder.}
}
@article{LEILEI2019e449,
title = {The burden of injury in China, 1990–2017: findings from the Global Burden of Disease Study 2017},
journal = {The Lancet Public Health},
volume = {4},
number = {9},
pages = {e449-e461},
year = {2019},
issn = {2468-2667},
doi = {https://doi.org/10.1016/S2468-2667(19)30125-2},
url = {https://www.sciencedirect.com/science/article/pii/S2468266719301252},
author = {Duan Leilei and Ye Pengpeng and Juanita A Haagsma and Jin Ye and Wang Yuan and Er Yuliang and Deng Xiao and Gao Xin and Ji Cuirong and Wang Linhong and Marlena S Bannick and W Cliff Mountjoy-Venning and Caitlin N Hawley and Zichen Liu and Mari Smith and Spencer L James and Theo Vos and Christopher J L Murray},
abstract = {Summary
Background
A comprehensive evaluation of the burden of injury is an important foundation for selecting and formulating strategies of injury prevention. We present results from the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2017 of non-fatal and fatal outcomes of injury at the national and subnational level, and the changes in burden for key causes of injury over time in China.
Methods
Using the methods and results from GBD 2017, we describe the burden of total injury and the key causes of injury based on the rates of incidence, cause-specific mortality, and disability-adjusted life years (DALYs) in China estimated using DisMod-MR 2.1. We additionally evaluated these results at the provincial level for the 34 subnational locations of China in 2017, measured the change of injury burden from 1990 to 2017, and compared age-standardised DALYs due to injuries at the provincial level against the expected rates based on the Socio-demographic Index (SDI), a composite measure of development of income per capita, years of education, and total fertility rate.
Findings
In 2017, in China, there were 77·1 million (95% uncertainty interval [UI] 72·5–81·6) new cases of injury severe enough to warrant health care and 733 517 deaths (681 254–767 006) due to injuries. Injuries accounted for 7·0% (95% UI 6·6–7·2) of total deaths and 10·0% (9·5–10·5) of all-cause DALYs in China. In 2017, there was a three-times variation in age-standardised injury DALY rates between provinces of China, with the lowest value in Macao and the highest in Yunnan. Between 1990 and 2017, the age-standardised incidence rate of all injuries increased by 50·6% (95% UI 46·6–54·6) in China, whereas the age-standardised mortality and DALY rates decreased by 44·3% (41·1–48·9) and 48·1% (44·6–51·8), respectively. Between 1990 and 2017, all provinces of China experienced a substantial decline in DALY rates from all injuries ranging from 16·3% (3·1–28·6) in Shanghai and 60·4% (53·7–66·1) in Jiangxi. Age-standardised DALY rates for drowning; injuries from fire, heat and hot substances; adverse effects of medical treatments; animal contact; environmental heat and cold exposure; self-harm; and executions and police conflict each declined by more than 60% between 1990 and 2017.
Interpretation
Between 1990 and 2017, China experienced a decrease in the age-standardised DALY and mortality rates due to injury, despite an increase in the age-standardised incidence rate. These trends occurred in all provinces. The divergent trends in terms of incidence and mortality indicate that with rapid sociodemographic improvements, the case fatality of injuries has declined, which could be attributed to an improving health-care system but also to a decreasing severity of injuries over this time period.
Funding
Bill & Melinda Gates Foundation.}
}
@article{ALKHABBAS2019100084,
title = {Characterizing Internet of Things Systems through Taxonomies: A Systematic Mapping Study},
journal = {Internet of Things},
volume = {7},
pages = {100084},
year = {2019},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2019.100084},
url = {https://www.sciencedirect.com/science/article/pii/S2542660519300307},
author = {Fahed Alkhabbas and Romina Spalazzese and Paul Davidsson},
keywords = {Internet of Things (IoT), Characterization of IoT systems, Systematic Mapping Study (SMS), Taxonomies},
abstract = {During the last decade, a large number of different definitions and taxonomies of Internet of Things (IoT) systems have been proposed. This has resulted in a fragmented picture and a lack of consensus about IoT systems and their constituents. To provide a better understanding of this issue and a way forward, we have conducted a Systematic Mapping Study (SMS) of existing IoT System taxonomies. In addition, we propose a characterization of IoT systems synthesized from the existing taxonomies, which provides a more holistic view of IoT systems than previous taxonomies. It includes seventeen characteristics, divided into two groups: elements and quality aspects. Finally, by analyzing the results of the SMS, we draw future research directions.}
}
@incollection{SERRADACRUZ2019439,
title = {Chapter 17 - Enabling Smart City Provenance-Based Applications to Improve Urban Mobility in Brazilian Cities},
editor = {Javier Faulin and Scott E. Grasman and Angel A. Juan and Patrick Hirsch},
booktitle = {Sustainable Transportation and Smart Logistics},
publisher = {Elsevier},
pages = {439-466},
year = {2019},
isbn = {978-0-12-814242-4},
doi = {https://doi.org/10.1016/B978-0-12-814242-4.00017-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012814242400017X},
author = {Sérgio Manuel {Serra da Cruz} and Raimundo José {Macário Costa}},
keywords = {Data provenance, Data quality, Mobile application, Open Government Data, Smart cities, Sustainability, Transportation, Workflows},
abstract = {Open Government Data (OGD) combined with mobile data collected from citizens' smartphones have the potential for innovative urban services to enable smart cities. However, one of the main challenges for the consolidation of smart cities applications in emerging countries is the limited offer of high-quality data and the availability of strategies that increase the interaction and collaboration between government and civil society. Our work aims to offer solutions related to the challenges of sustainable transportation and urban mobility in Brazilian cities. This chapter adopts concepts and methodologies from the emerging field of urban computing. It aims to contribute to the challenges of using OGD enriched with data provenance in mobile applications about public bus transportation. We illustrate our approach with a distributed architecture named BusInRio, which uses standardized interfaces to access curated OGD and implements services to manage the associated metadata. We also present two studies based on users using OGD enriched with retrospective provenance.}
}
@article{ZHONG20196145,
title = {A Case Study of Operation Optimization on A Renewable Energy Building by E-CPS Method: From Both Sides of Supply and Demand},
journal = {Energy Procedia},
volume = {158},
pages = {6145-6151},
year = {2019},
note = {Innovative Solutions for Energy Transitions},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2019.01.496},
url = {https://www.sciencedirect.com/science/article/pii/S1876610219305193},
author = {Shengyuan Zhong and Shuai Deng and Jun Zhao and Yongzhen Wang and Pengwei Su},
keywords = {cyber physical system, energy cyber physical system, building energy system, renewable energy, data mining},
abstract = {The energy system’s operation strategy has a significant influence on the system efficiency. The determination of the optimal operation strategy requires the effective application of a substantial amount of data accumulated from the operation or even forecasting process. However, data processing, which is not obtained based on the physical mechanism, may fail to explain, under certain boundary conditions. Therefore, using E-CPS to combine data mining and physical model to build a more efficient and accurate model or even a real-time model is vital and has received more attention in practical applications. This paper proposes a general process for constructing E-CPS and applies it to the simulation of building energy systems. The E-CPS results in a 70% reduction in the renewable energy waste rate of the building’s energy system and a 57% reduction in the purchase of electricity. Finally, it is concluded that the use of intelligent algorithm-based data mining combined with physical models to build E-CPS is practical and effective.}
}
@article{GHEISARI2019101470,
title = {A context-aware privacy-preserving method for IoT-based smart city using Software Defined Networking},
journal = {Computers & Security},
volume = {87},
pages = {101470},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818313336},
author = {Mehdi Gheisari and Guojun Wang and Wazir Zada Khan and Christian Fernández-Campusano},
keywords = {SDN, IoT, Privacy, Context-aware, Dynamic, Internet of Things, Penetration rate, Software Defined Networking, Smart city, VPN},
abstract = {Smart City is an application of the Internet of Things (IoT) with the aim of managing cities without human intervention. Each IoT device that is producing data may sense a sensitive one that should not be disclosed unintentionally. Due to the existence of a large number of devices in the near future, the possibility of information leakage, privacy breach, is increasing. To prevent this, each device applies a privacy-preserving method. We discover that all of the existing solutions have three major drawbacks: (1) applying one static privacy-preserving method for the entire system, (2) sending whole data at once, and (3) not context-aware. These cause unacceptable privacy-preserving degree. To address them, in this paper, at first, we equip IoT-based smart city with Software Defined Networking paradigm (SDN). Then, we mount an efficient privacy-preserving method on top of it that manages flowing data packets of split IoT device’ data. We have done extensive simulation through MININET-WIFI to show the effectiveness of our approach. Evaluation results show that our method can be widely applied to the smart city application with a superior performance regarding accuracy, overhead, and penetration rate compared to existing privacy-preserving solutions.}
}
@article{MOGAJI201921,
title = {Insight into consumer experience on UK train transportation services},
journal = {Travel Behaviour and Society},
volume = {14},
pages = {21-33},
year = {2019},
issn = {2214-367X},
doi = {https://doi.org/10.1016/j.tbs.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2214367X18300553},
author = {Emmanuel Mogaji and Ismail Erkan},
keywords = {Service experience, Sentiment analysis, Transport service, United Kingdom, Social media},
abstract = {Customers’ experiences are significant in a rapidly changing service context, and this is shaped by the quality of service provided. With social media changing the way consumers engage with service providers, experiences are shared online. This study carried out three analyses of brand-related conversations on Twitter with the aim of exploring consumers’ attitudes to and experiences of train operating companies. Firstly, Python was used for the tweet mining and sentiment analysis (n = 1,914,494 tweets) to investigate the polarity between the opinions of commuters. Secondly, tweets were thematically analysed and grouped to understand how consumers experience the service quality. Lastly, content analysis of the tweets was carried out to identify the variations in service quality. Results indicated that there is overall positive customer experience, however, there are variations in service quality dimension across the different train groups, highlight the need to improve service quality at different touchpoints, especially the tangible features of the trains and presence of responsive and emphatic staff. This study further broadens the context of customer experience through eWOM on social media for service brands, contribute towards related literature on sentiment analysis and service brands, providing significant theoretical and practical implications for researchers and managers.}
}
@article{LINDEMANN2019406,
title = {Methodical Data-Driven Integration Of Perceived Quality Into The Product Development Process},
journal = {Procedia CIRP},
volume = {84},
pages = {406-411},
year = {2019},
note = {29th CIRP Design Conference 2019, 08-10 May 2019, Póvoa de Varzim, Portgal},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.205},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119308492},
author = {Marie Lindemann and Leo Nuy and Kristof Briele and Robert Schmitt},
keywords = {Perceived Quality, Product Development Process, Data Analytics, Customer Driven Innovation},
abstract = {Due to the strong technical and functional similarities of today’s consumer products, the emotional and sensory product characteristics become increasingly important in order to stand out from the competition. To meet the comprehensive customer requirements it is no longer sufficient to develop a product that only focusses on functional aspects – the common understanding of product quality – but one that corresponds to sensory perception as well. The objective of this research is to develop a data-driven methodology in order to make Perceived Quality systematically usable in the product development process. The aim is on the one hand to increase the product value for the customer and on the other hand to improve the development and innovation process. Thus, customer’s sensory perception of product quality is comprehensible and continuously integrated into the product development process. The increasing amount of customer-generated content regarding a product for example published in Social Media are an important source of information for companies. In addition, companies have access to a variety of data such as studies, internal audits, Lead User workshops, claims as well as complaints. These data sources can provide information about the perceived quality of products. Despite the fact that the industry has partly recognized the importance of Perceived Quality, methodical aspects of the implementation into the product development process have yet to be considered systematically. For the comprehensive acquisition of valuable data, various sources of information need to be taken into account. Due to the diversity of different sources, the challenge is to collect useful data and avoid the loss of information. The different sources and data have to be standardized and structured to enable data analytics. By means of clustering provided information can then be transformed in reliable customer requirements.}
}
@article{OSMAN2019514,
title = {A cognitive analytics management framework for the transformation of electronic government services from users’ perspective to create sustainable shared values},
journal = {European Journal of Operational Research},
volume = {278},
number = {2},
pages = {514-532},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719301468},
author = {Ibrahim H. Osman and Abdel Latef Anouze and Zahir Irani and Habin Lee and Tunç D. Medeni and Vishanth Weerakkody},
keywords = {Analytics, Behavioral OR, Data Envelopment Analysis, Multiple criteria analysis, OR in government},
abstract = {Electronic government services (e-services) involve the delivery of information and services to stakeholders via the Internet, Internet of Things and other traditional modes. Despite their beneficial values, the overall level of usage (take-up) remains relatively low compared to traditional modes. They are also challenging to evaluate due to behavioral, economical, political, and technical aspects. The literature lacks a methodology framework to guide the government transformation application to improve both internal processes of e-services and institutional transformation to advance relationships with stakeholders. This paper proposes a cognitive analytics management (CAM) framework to implement such transformations. The ambition is to increase users’ take-up rate and satisfaction, and create sustainable shared values through provision of improved e-services. The CAM framework uses cognition to understand and frame the transformation challenge into analytics terms. Analytics insights for improvements are generated using Data Envelopment Analysis (DEA). A classification and regression tree is then applied to DEA results to identify characteristics of satisfaction to advance relationships. The importance of senior management is highlighted for setting strategic goals and providing various executive supports. The CAM application for the transforming Turkish e-services is validated on a large sample data using online survey. The results are discussed; the outcomes and impacts are reported in terms of estimated savings of more than fifteen billion dollars over a ten-year period and increased usage of improved new e-services. We conclude with future research.}
}
@article{XU2019206,
title = {Generating a series of land covers by assimilating the existing land cover maps},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {147},
pages = {206-214},
year = {2019},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0924271618303204},
author = {Guang Xu and Baozhang Chen},
keywords = {Land cover, Land cover change, Time series, Data assimilation, Data assimilation framework},
abstract = {Land cover (LC), which describes the physical material of land surface, is an important parameter in earth system science. With more and more LC data available, this research aims to establish a data assimilation framework for integrating continuous LC time series based on the existing LC products. The framework is designed by borrowing the basic concept from data assimilation to find out the optimal LC time series based on existing observations. According to the observing system simulation experiments, the assimilation framework showed a good performance against noises and uncertainties. Moreover, after validation with the existing LC reference data, the averaged accuracy of the LC data produced using the assimilation framework was 73.7%, which is a great improvement compared to the existing LC maps considering the temporal continuity. This LC map assimilation framework would be useful for assessing the global LC changes and related researches.}
}
@article{BECKEN201935,
title = {A hybrid is born: Integrating collective sensing, citizen science and professional monitoring of the environment},
journal = {Ecological Informatics},
volume = {52},
pages = {35-45},
year = {2019},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S157495411830150X},
author = {Susanne Becken and Rod M. Connolly and Jinyan Chen and Bela Stantic},
keywords = {Citizen science, Collective sensing, Monitoring, Hybrid system, Coral, Great barrier reef},
abstract = {Including members of the public in the development of effective environmental monitoring systems is gaining traction. This research assesses the potential for a hybrid monitoring system for the case of coral at the Great Barrier Reef. Based on a review of citizen-derived data sources, the paper first develops a framework and then populates it with five datasets. These are then compared based on data volumes, type of data, spatial coverage, and bleaching patterns. The results reveal the inherent difficulties – both in terms of quantity and quality – for collective sensing data (Twitter in this case) and more structured human sensors approaches (Eye on the Reef Sightings). However, more targeted approaches, such as CoralWatch and tourism-operator based data collection, emerged as important contributors to information generation on the state of coral. Citizen-based data that either deliver a high data density per location, a wide geographic coverage, or regular observations over time are particularly valuable. Recommendations are made for developing a hybrid monitoring system that integrates citizen-derived with professionally collected data.}
}
@article{COURTNEYMUSTAPHI2019100228,
title = {Integrating evidence of land use and land cover change for land management policy formulation along the Kenya-Tanzania borderlands},
journal = {Anthropocene},
volume = {28},
pages = {100228},
year = {2019},
issn = {2213-3054},
doi = {https://doi.org/10.1016/j.ancene.2019.100228},
url = {https://www.sciencedirect.com/science/article/pii/S2213305419300396},
author = {Colin J. {Courtney Mustaphi} and Claudia Capitani and Oliver Boles and Rebecca Kariuki and Rebecca Newman and Linus Munishi and Rob Marchant and Paul Lane},
keywords = {Landscape, Multidisciplinary, Science-policy interface, Serengeti, Socio-ecological systems, Policy support},
abstract = {This paper presents an overview of the scientific evidence providing insights into long term ecosystem and social dynamics across the northern Tanzania and southern Kenya borderlands. The data sources covered a range from palaeoenvironmental records and archaeological information to remote sensing and social science studies that examined human-environmental interactions and land use land cover changes (LULCC) in the region. This knowledge map of published LULCC research contributes to current debates about the drivers and dynamics of LULCC. The review aims to facilitate both multidisciplinary LULCC research and evidence-based policy analyses to improve familiarity and engagement between LULCC knowledge producers and end-users and to motivate research integration for land management policy formulation. Improving familiarity among researchers and non-academic stakeholders through the collation and synthesis of the scientific literature is among the challenges hindering policy formulation and land management decision-making by various stakeholders along the Kenya-Tanzania borderlands. Knowledge syntheses are necessary; yet, do not fully bridge the gap between knowledge and policy action. Cooperation across the science-policy interface is fundamental for the co-production of research questions by academics, policy makers and diverse stakeholders aimed at supporting land management decision making. For improved co-development and co-benefitting outcomes, the LULCC scientific community needs to mobilise knowledge for a broader audience and to advance co-development of relevant and meaningful LULCC products.}
}
@article{WANG201966,
title = {Park green spaces, public health and social inequalities: Understanding the interrelationships for policy implications},
journal = {Land Use Policy},
volume = {83},
pages = {66-74},
year = {2019},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2019.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0264837718316545},
author = {Qian Wang and Zili Lan},
keywords = {park green spaces, health outcomes, inequalities, neighborhood socioeconomic status, indicators, greening policy},
abstract = {How to reduce neighborhood socioeconomic status- (SES-) related health inequalities has been prioritized in the recent political literature. Park green spaces (PGSs) are essential neighborhood land use assets, as they are beneficial for population health and thus should mitigate SES-related health inequalities. This paper aims to elaborate the knowledge on the complex interrelationships among PGSs, health outcomes and social inequalities through developing a set of mixed indicators in an integrated manner. The data are obtained at the community level (N=8117) for three health outcomes (cardiopathy, chronic pneumonia and hypertension) and five SES variables. The PGS characteristics are described from three dimensions (coverage, quality and accessibility) at two geographical levels (15-minute walking distance (15 WD) neighborhoods and 30-minute walking distance (30 WD) neighborhoods). Spatial regressions reveal the following: 1) socioeconomically disadvantaged communities enjoy fewer PGSs and limited access to parks of high quality; 2) socioeconomically disadvantaged communities present higher incidences of diseases; and 3) PGS coverage within 30 WD neighborhoods and PGS accessibility within 15 WD neighborhoods are related to health outcomes. Structural equation modeling further confirms that PGSs, especially those of higher quality, could mitigate SES-related health inequalities. The findings of this study highlight the necessity of improving the PGS quality within walking distance of socioeconomically disadvantaged communities. We thus argue that land use policy makers should collaborate with social researchers and health professionals; and through health impact assessment, they can translate the professional knowledge into land use planning and consider health promotion into land use policies.}
}
@article{KOCSIS2019100420,
title = {A conceptual foundation of design and implementation research in accounting information systems},
journal = {International Journal of Accounting Information Systems},
volume = {34},
pages = {100420},
year = {2019},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1467089518301313},
author = {David Kocsis},
keywords = {Literature review, Accounting Information System, Design, Implementation, Text analysis, Framework},
abstract = {This structured literature review focuses on the design and implementation of systems in Accounting Information System (AIS) research. The review has two objectives: 1) To look in depth at AIS research regarding the design and implementation of information systems related to accounting in the last fifteen years; 2) To understand design and implementation issues in AIS. To do so, this research examines the current state of design and implementation research in the domain of AIS, using a structured review of abstracts in top-level Information Systems, Accounting, and AIS journals. Using a design science in IS theoretical framework, the review categorizes themes and trends in AIS literature. Some of the most relevant themes include audit/auditors, enterprise resource planning, monitoring and control, adoption, and decision making. The most relevant issues include training, commitment, investment, culture, and existing business processes. The research includes gaps, limitations, and opportunities for future research.}
}
@incollection{VISVIZI20191,
title = {Chapter 1 - Smart cities research and debate: what is in there?},
editor = {Anna Visvizi and Miltiadis D. Lytras},
booktitle = {Smart Cities: Issues and Challenges},
publisher = {Elsevier},
pages = {1-14},
year = {2019},
isbn = {978-0-12-816639-0},
doi = {https://doi.org/10.1016/B978-0-12-816639-0.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166390000016},
author = {Anna Visvizi and Miltiadis D. Lytras},
keywords = {Conceptual frameworks, Interdisciplinary research agenda, Multidisciplinary research agenda, Smart city, Sustainability},
abstract = {Several developments have led to an increased interest, and a resultant debate, on the possible use of information and communication technology in cities and urban space. As the debate gains on momentum, a great number of new insights populate the field, thus ostensibly redefining it and delineating its disciplinary boundaries anew. Interestingly, if smart cities are the buzzword of the popular debate today, sustainability has become the keyword defining the thrust of that debate. This volume recognizes the scope, the breadth, and the illuminating insights that scholars active in the field have brought to the debate over the past decades, thereby adding to our knowledge and understanding of the variety of issues and topics pertinent to smart cities. Against this backdrop, this chapter dwells on the conceptual caveats inherent in smart cities research and outlines the scope of the book.}
}
@article{SAHMIM20191246,
title = {Edge Computing: Smart Identity Wallet Based Architecture and User Centric},
journal = {Procedia Computer Science},
volume = {159},
pages = {1246-1257},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.294},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314930},
author = {Syrine Sahmim and Hamza Gharsellaoui and Sadok Bouamama},
keywords = {Edge Computing, IDoT, Blockchain, Privacy, Security, Identity, Wallet, Transaction, Consumer, Provider, Behavior, 51% Attack, Sybil Attack},
abstract = {The huge amount of exchanged data in the internet between entities and the quick development of edge computing has increased users frustration about the future generations. We expect a future where transactions will be based on clouds and virtual machines, gathered from sensors and IoT devices and processed by different artificial intelligence algorithms or agents. The speed and backlash change in the world has driven researchers to work on privacy and security for entities’ information and transactions. Different concepts arise and still in their early stage as digital identity, self sovereign identity, global unique identifier and identity of things (IDoT). Therefore, motivated by the recent explosion of interest around Blockchains, we examine whether they make a good fit for the Identity Internet of Things (IDoT) sector. Blockchain a major distributed peer-to-peer network where non-trusting members can interact together without a need for a trusted third party, it actually make available many advantages to providers and consumers and solve data protection features lifelong a transaction existing. As being immutable, transparent and trustful platform the Blockchain allows managing identities and privacy of its nodes information. Our contribution is a new architecture eventually using a public Blockchain and creating a smart identity wallet. It contains standard node data and will also integrate proactive data behavior and the reactive one. The main target is to protect users from sybil attack at early stage. We will define a new digital wallet based on entities behavior in order to prevent 51% attack and Sybil attack}
}
@article{SUN201953,
title = {AtoMixer: Atom-based interactive visual exploration of traffic surveillance data},
journal = {Journal of Computer Languages},
volume = {53},
pages = {53-62},
year = {2019},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X18300065},
author = {Guodao Sun and Yin Zhao and Dizhou Cao and Jianyuan Li and Ronghua Liang and Yipeng Liu},
keywords = {Human-centered computing, Visual analytics, Visual query},
abstract = {Massive traffic surveillance data extracted from vehicle detectors such as cameras provide essential information for revealing urban traffic pattern. However, most existing tools only allow users to analyze the data in specific time periods and regions with particular requirements. In this paper, we work closely with traffic domain experts and investigate a novel way of reframing visual traffic analysis tasks into the combinations of various atom categorical/numerical features and visual presentation. The categorical features contain primitive attributes such as vehicle type, O/D status and driving direction, and the numerical features contains information such as vehicle frequency and speed. The combination of above features includes four basic operations, namely and, or, xor and not to support diversified user requirements. Basic and advanced visualization methods such as trajectory view and flow distribution view are provided to demonstrate the combination results. Through interactive assembling of various atom operations, analysts could derive different query conditions to meet existed and potential upcoming analysis requirements such as locating suspicious vehicles (e.g., fake plate vehicles). Furthermore, AtoMixer, a visual analytic system is developed to support spatio-temporal investigative tasks for traffic surveillance data. We evaluate the effectiveness and scalability of our approach with real world traffic surveillance data.}
}
@article{LI2019100940,
title = {Hybrid data-driven vigilance model in traffic control center using eye-tracking data and context data},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100940},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100940},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619300540},
author = {Fan Li and Ching-Hung Lee and Chun-Hsien Chen and Li Pheng Khoo},
keywords = {Internet of things, Traffic control center, Vigilance detection, Data-driven, Eye movements},
abstract = {Vigilance decrement of traffic controllers would greatly threaten public safety. Hence, extensive studies have been conducted to establish the physiological data-based vigilance model for objectively monitoring or detecting vigilance decrement. Nevertheless, most of them using intrusive devices to collect physiological data and failed to consider context information. Consequently, these models can be used in a laboratory environment while cannot adapt to dynamic working conditions of traffic controllers. The goal of this research is to develop an adaptive vigilance model for monitoring vigilance objectively and non-intrusively. In recent years, with advanced information and communication technology, a massive amount of data can be collected from connected daily use items. Hence, we proposed a hybrid data-driven approach based on connected objects for establishing vigilance model in the traffic control center and provide an elaborated case study to illustrate the method. Specifically, eye movements are selected as the primary inputs of the proposed vigilance model; Bagged trees technique is adapted to generate the vigilance model. The results of case study indicated that (1) eye metrics would be correlated with the vigilance performance subjected to the mental fatigue levels, (2) the bagged trees with the fusion features as inputs achieved a relatively stable performance under the condition of data loss, (3) the proposed method could achieve better performance than the other classic machine learning methods.}
}
@incollection{CZANNER2019171,
title = {Chapter 10 - Statistical analysis and design in ophthalmology: Toward optimizing your data},
editor = {Emanuele Trucco and Tom MacGillivray and Yanwu Xu},
booktitle = {Computational Retinal Image Analysis},
publisher = {Academic Press},
pages = {171-197},
year = {2019},
series = {The Elsevier and MICCAI Society Book Series},
isbn = {978-0-08-102816-2},
doi = {https://doi.org/10.1016/B978-0-08-102816-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081028162000101},
author = {Gabriela Czanner and Catey Bunce},
keywords = {Statistics, Design, Significance, Prediction, Prognosis, Missingness, Likelihood, Missing data, Sample size, Power analysis},
abstract = {This chapter is focused on two pillars of statistics: the study design and analysis of data. The statistical design helps us to understand variety of ways how clinical studies and experiments can be conducted so that the research question at hand can be answered. Data analysis methods are crucial to unravel the knowledge from the collected data. Both design and analysis are important part of statistics, and they are underpinned by the statistical science and theory of probability.}
}
@article{TORTONESI2019888,
title = {Taming the IoT data deluge: An innovative information-centric service model for fog computing applications},
journal = {Future Generation Computer Systems},
volume = {93},
pages = {888-902},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17306702},
author = {Mauro Tortonesi and Marco Govoni and Alessandro Morelli and Giulio Riberto and Cesare Stefanelli and Niranjan Suri},
keywords = {Fog computing, Smart cities, Information-centric networking, Internet-of-Things, Value of information},
abstract = {Fog Computing is a new computation paradigm, recently emerged from the convergence of IoT, WSN, mobile computing, edge computing, and Cloud Computing, which is particularly well suited for Smart City environments. Fog Computing aims at supporting the development of time-sensitive, location-, social-, and context-aware applications by using computational resources in close proximity of information producers and consumers, such as increasingly common cheap and powerful modern hardware platforms. However, realizing Fog Computing solutions for Smart Cities represents a very challenging task, because of the massive amount of data to process, the strict resource and time constraints, and the significant dynamicity and heterogeneity of computation and network resources. These formidable challenges suggest taking into consideration new information and service model solutions that explore several trade-offs between processing speed and accuracy. Along these guidelines, we designed the SPF Fog-as-a-Service platform, which proposes a new information-centric and utility-based service model and allows the definition of self-adaptive and composition-friendly services, which can execute either on edge devices or in the Cloud. In numerous evaluations, SPF proved to be a very effective platform for running Fog services on heterogeneous devices with significantly different computational capabilities while also demonstrating remarkable ease of development and management characteristics.}
}
@article{APPIO20191,
title = {Understanding Smart Cities: Innovation ecosystems, technological advancements, and societal challenges},
journal = {Technological Forecasting and Social Change},
volume = {142},
pages = {1-14},
year = {2019},
note = {Understanding Smart Cities: Innovation ecosystems, technological advancements, and societal challenges},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518319954},
author = {Francesco Paolo Appio and Marcos Lima and Sotirios Paroutis},
keywords = {Smart cities, Hybrid framework, Physical infrastructure, Quality of life, Innovation, Review},
abstract = {Smart Cities initiatives are spreading all around the globe at a phenomenal pace. Their bold ambition is to increase the competitiveness of local communities through innovation while increasing the quality of life for its citizens through better public services and a cleaner environment. Prior research has shown contrasting views and a multitude of dimensions and approaches to look at this phenomenon. In spite of the fact that this can stimulate the debate, it lacks a systematic assessment and an integrative view. The papers in the special issue on “Understanding Smart Cities: Innovation Ecosystems, Technological Advancements, and Societal Challenges” take stock of past work and provide new insights through the lenses of a hybrid framework. Moving from these premises, we offer an overview of the topic by featuring possible linkages and thematic clusters. Then, we sketch a novel research agenda for scholars, practitioners, and policy makers who wish to engage in – and build – a critical, constructive, and conducive discourse on Smart Cities.}
}
@article{BAI2019102833,
title = {Data mining approach to construction productivity prediction for cutter suction dredgers},
journal = {Automation in Construction},
volume = {105},
pages = {102833},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102833},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518313396},
author = {Shuo Bai and Mingchao Li and Rui Kong and Shuai Han and Heng Li and Liang Qin},
keywords = {Cutter suction dredger, Construction productivity, Monitoring data mining, Machine learning},
abstract = {Cutter suction dredgers are important equipment in the dredging engineering. However, dredging construction productivity is affected by many factors such as soil, hydrology, meteorology and underwater sundries, making it difficult to obtain accurate prediction results. This paper presents a new integrated approach for using intelligent data mining algorithms to analyze dredger monitoring data and estimate the effective productivity. Through these combination algorithms, alongside Lasso and Maximal Information Coefficient (MIC), the key features affecting the productivity are filtered out from a 255-dimensional monitoring data set. The continuous mean data cleaning method is proposed according to the characteristics of the filtered data, followed by a clean-up of the feature data to increase smoothness. Four machine learning algorithms, Random Forest, K-Nearest, Naive Bayes and eXtreme Gradient Enhancement (XGBoost), are used to estimate and analyze the productivity of a cutter suction dredger and applied to actual engineering cases. The results show that the prediction accuracy of XGBoost exceeds 90%, which is better than other algorithms used. Finally, the model is compared with the predictive model trained by the characteristics that influence productivity derived from the traditional method. The results still show that the XGBoost model with features obtained from machine learning as input terms has a better prediction effect.}
}
@article{CHEN2019382,
title = {Understanding road performance using online traffic condition data},
journal = {Journal of Transport Geography},
volume = {74},
pages = {382-394},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S096669231830468X},
author = {Song Chen and Xiaoyan Wei and Nan Xia and Zhaojin Yan and Yi Yuan and H. Michael Zhang and Manchun Li and Liang Cheng},
keywords = {Road performance, Traffic conditions, Spatial analysis, Spatiotemporal patterns, Tidal harmonic analysis model},
abstract = {Traffic conditions predominantly follow a number of recurrent spatiotemporal patterns. An understanding of these recurrent patterns is of great importance to judge whether a road section is likely to be congested, which we called ‘road performance’. Traffic condition data, which are freely available online, provide valuable, full-coverage, real-time information on traffic conditions. Here, we propose a method for evaluating road performance using the sample points from traffic condition data acquired from Baidu map service for free. By treating human mobility as tidal currents and daily routines as planetary gravities, we utilize a tidal harmonic analysis to establish a simulation model with MATLAB. This model can deal with traffic simulations for a long experimental period in order to obtain a usual road performance. Data from two spatial scales, covering Yunnan Province in China and its capital city, Kunming, were used to evaluate the accuracy of this model. We used 31 days of the dataset to understand the road performance at each scale via three indices: the expectation, variance, and daily patterns of traffic conditions. Our experimental results demonstrate that the tidal current approach to model traffic pattern changes in small and large spatial scales is feasible and promising.}
}
@article{KHAN2019456,
title = {Mobile crowdsensing: A survey on privacy-preservation, task management, assignment models, and incentives mechanisms},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {456-472},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18315632},
author = {Fazlullah Khan and Ateeq {Ur Rehman} and Jiangbin Zheng and Mian Ahmad Jan and Muhammad Alam},
keywords = {Mobile crowdsensing, Privacy, Security, Task management, Assignment models, Incentives},
abstract = {Mobile crowdsensing is a useful technique to collect detailed information from mobile devices of the participants. The participants need to participate to sense and transmit valuable information to the servers. Due to the technological growth, various components of mobile devices such as accelerometer, gyroscope, camera and inertial, collect vast volumes of data in a quick, efficient, and cost-effective manner. However, a mobile crowdsensing paradigm may result in serious privacy and security breaches by exposing the mobile devices to various threats and vulnerabilities. This leakage of privacy has an adverse impact on the usage and participation of mobile devices. Motivated by these threats and privacy challenges, we investigate the current approaches used for preserving privacy in mobile crowdsensing applications. After a generic description of mobile crowdsensing systems and their components, we discuss critical issues related to privacy preservation, such as task management, task assignment models, and incentive mechanisms. We also discuss various mobile crowdsensing mechanisms available in the literature. Finally, we highlight numerous research challenges that need to be addressed to improve the performance of future privacy-preserving mechanisms for mobile crowdsensing applications.}
}
@article{WANG20191,
title = {Coevolution spreading in complex networks},
journal = {Physics Reports},
volume = {820},
pages = {1-51},
year = {2019},
note = {Coevolution spreading in complex networks},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0370157319302583},
author = {Wei Wang and Quan-Hui Liu and Junhao Liang and Yanqing Hu and Tao Zhou},
keywords = {Complex networks, Coevolution spreading, Critical phenomena, Biological contagions, Social contagions, Resource allocation, Awareness diffusion},
abstract = {The propagations of diseases, behaviors and information in real systems are rarely independent of each other, but they are coevolving with strong interactions. To uncover the dynamical mechanisms, the evolving spatiotemporal patterns and critical phenomena of networked coevolution spreading are extremely important, which provide theoretical foundations for us to control epidemic spreading, predict collective behaviors in social systems, and so on. The coevolution spreading dynamics in complex networks has thus attracted much attention in many disciplines. In this review, we introduce recent progress in the study of coevolution spreading dynamics, emphasizing the contributions from the perspectives of statistical mechanics and network science. The theoretical methods, critical phenomena, phase transitions, interacting mechanisms, and effects of network topology for four representative types of coevolution spreading mechanisms, including the coevolution of biological contagions, social contagions, epidemic–awareness, and epidemic–resources, are presented in detail, and the challenges in this field as well as open issues for future studies are also discussed.}
}
@article{SAUERWEIN2019140,
title = {An analysis and classification of public information security data sources used in research and practice},
journal = {Computers & Security},
volume = {82},
pages = {140-155},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818304978},
author = {Clemens Sauerwein and Irdin Pekaric and Michael Felderer and Ruth Breu},
keywords = {Cyber threat intelligence sharing, Cyber security information source, Taxonomy, Classification, Characteristic, Information security and risk management, Data format, Research, Practice},
abstract = {In order to counteract today’s sophisticated and increasing number of cyber threats the timely acquisition of information regarding vulnerabilities, attacks, threats, countermeasures and risks is crucial. Therefore, employees tasked with information security risk management processes rely on a variety of information security data sources, ranging from inter-organizational threat intelligence sharing platforms to public information security data sources, such as mailing lists or expert blogs. However, research and practice lack a comprehensive overview about these public information security data sources, their characteristics and dependencies. Moreover, comprehensive knowledge about these sources would be beneficial to systematically use and integrate them to information security processes. In this paper, a triangulation study is conducted to identify and analyze public information security data sources. Furthermore, a taxonomy is introduced to classify and compare these data sources based on the following six dimensions: (1) Type of information, (2) Integrability, (3) Timeliness, (4) Originality, (5) Type of Source,and (6) Trustworthiness. In total, 68 public information security data sources were identified and classified. The investigations showed that research and practice rely on a large variety of heterogeneous information security data sources, which makes it more difficult to integrate and use them for information security and risk management processes.}
}
@article{SANCHEZREILLO201989,
title = {How to implement EU data protection regulation for R&D in biometrics},
journal = {Computer Standards & Interfaces},
volume = {61},
pages = {89-96},
year = {2019},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2018.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303161},
author = {Raul Sanchez-Reillo and Ines Ortega-Fernandez and Wendy Ponce-Hernandez and Helga C. Quiros-Sandoval},
keywords = {Anonymization, Biometrics, Coding, Data Protection, Databases, Privacy, User consent},
abstract = {Biometrics R&D has to deal with personal data. From the Universal Declaration of Human Rights, privacy of a human being shall be protected, and this is addressed in different ways in each region of the world. In the case of the European Union, Data Protection Directives, Laws and Regulations have been established, and interpreted in different ways by each European Member State. Such a diversity has pushed the European Union to generate an improved regulation that will be mandatory from May 2018. Biometric R&D shall not only comply with the current Directive, but also has to adapt its work to the new Regulation. This work is intended to describe the situation and provide a recommended procedure when having to acquire personal data. The recommended procedure is illustrated by the implementation of a Biometric Data Acquisition Platform, used to acquire fingerprints from nearly 600 citizens using different sensors.}
}
@incollection{ANTHOPOULOS2019149,
title = {7 - The smart city of Trikala},
editor = {Leonidas Anthopoulos},
booktitle = {Smart City Emergence},
publisher = {Elsevier},
pages = {149-171},
year = {2019},
isbn = {978-0-12-816169-2},
doi = {https://doi.org/10.1016/B978-0-12-816169-2.00007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128161692000079},
author = {Leonidas Anthopoulos},
keywords = {Smart city, smart parking, smart lighting, smart services, smart transformation},
abstract = {The city of Trikala has a long smart history that started back in 2004. During this time frame, this local smart transformation experienced even failures in terms of social acceptance in 2010, which questioned its viability, while it also experienced a dramatic evolution after 2015. This chapter, after presenting in brief the past story of smart interventions in Trikala, analyzes the case with a focus on the scope that the project followed during the last 4 years, which transformed it to a famous international case and to the Greek smart-city flagship. The outcomes show that a smart city can sustain only if it meets the local interest and can put the smart-city evolution on a viable path.}
}
@article{KWONG20194392,
title = {Can routinely collected laboratory and health administrative data be used to assess influenza vaccine effectiveness? Assessing the validity of the Flu and Other Respiratory Viruses Research (FOREVER) Cohort},
journal = {Vaccine},
volume = {37},
number = {31},
pages = {4392-4400},
year = {2019},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2019.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X19307728},
author = {Jeffrey C. Kwong and Sarah A. Buchan and Hannah Chung and Michael A. Campitelli and Kevin L. Schwartz and Natasha S. Crowcroft and Michael L. Jackson and Timothy Karnauchow and Kevin Katz and Allison J. McGeer and J. Dayre McNally and David C. Richardson and Susan E. Richardson and Laura C. Rosella and Andrew Simor and Marek Smieja and George Zahariadis and Aaron Campigotto and Jonathan B. Gubbay},
keywords = {Influenza, Human, Epidemiology, Influenza Vaccines, Data Linkage, Bias},
abstract = {Background
Linking data on laboratory specimens collected during clinical practice with health administrative data permits highly powered vaccine effectiveness (VE) studies to be conducted at relatively low cost, but bias from using convenience samples is a concern. We evaluated the validity of using such data for estimating VE.
Methods
We created the Flu and Other Respiratory Viruses Research (FOREVER) Cohort by linking individual-level data on respiratory virus laboratory tests, hospitalizations, emergency department visits, and physician services. For community-dwelling adults aged > 65 years, we assessed the presence and magnitude of information and selection biases, generated VE estimates under various conditions, and compared our VE estimates with those from other studies.
Results
We included 65,648 unique testing episodes obtained from 54,434 individuals during the 2010–11 to 2015–16 influenza seasons. To examine information bias, we found the proportion testing positive for influenza for patients with unknown interval from illness onset to specimen collection was more similar to patients for whom illness onset date was ≤ 7 days before specimen collection than to patients for whom illness onset was > 7 days before specimen collection. To assess the presence of selection bias, we found the likelihood of influenza testing was comparable between vaccinated and unvaccinated individuals, although the adjusted odds ratios were significantly greater than 1 for some healthcare settings and during some influenza seasons. Over 6 seasons, VE estimates ranged between 36% (95%CI, 27–44%) in 2010–11 and 5% (95%CI, –2, 11%) in 2014–15. VE estimates were similar under a range of conditions, but were consistently higher when accounting for misclassification of vaccination status through a quantitative sensitivity analysis. VE estimates from the FOREVER Cohort were comparable to those from other studies.
Conclusions
Routinely collected laboratory and health administrative data contained in the FOREVER Cohort can be used to estimate influenza VE in community-dwelling older adults.}
}
@article{CHUDINOV20197,
title = {Improvement of peaks identification and dynamic range for bi-polar Single Particle Mass Spectrometer},
journal = {International Journal of Mass Spectrometry},
volume = {436},
pages = {7-17},
year = {2019},
issn = {1387-3806},
doi = {https://doi.org/10.1016/j.ijms.2018.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S1387380618301131},
author = {Alexey Chudinov and Lei Li and Zhen Zhou and Zhengxu Huang and Wei Gao and Jiajun Yu and Sergei Nikiforov and Alexander Pikhtelev and Aygul Bukharina and Viacheslav Kozlovskiy},
keywords = {Delay extraction, Time-of-flight, Single particle, Aerosol, Resolution, Mass accuracy},
abstract = {Delay Extraction (DE) in Single Particle Mass Spectrometry (SPMS) provides substantial resolution enhancement. However, DE has two main drawbacks which are discussed in our article. First, ion peak position in this case becomes very sensitive to initial ion coordinate. Therefore, substantial peak jitter is observed when switching between mass spectra of individual particles. This peak jitter obstructs correct mass spectra accumulation resulting in wide peaks with irregular shapes in the accumulated mass spectrum, which leads to the fact that isotopic pattern identification becomes difficult. In the present article two ways of the peak jitter compensation, Dynamic Calibration and Mass Spectra correction based on Spectra Correlation, are proposed. It was shown that both proposed techniques provide substantial resolution improvement in the summed mass spectrum especially for small peaks with complex isotopic patterns. Second, time delay distorts regular quadratic dependence between time-of-flight and m/z. In our paper we also propose calibration equation with variable exponent for SPMS modified with DE. It is shown that usage of an exponent as a fitting parameter allows to achieve mass accuracy comparable with 4th degree equation proposed earlier.}
}
@article{CHEN201937,
title = {Layered adaptive compression design for efficient data collection in industrial wireless sensor networks},
journal = {Journal of Network and Computer Applications},
volume = {129},
pages = {37-45},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519300025},
author = {Siguang Chen and Shujun Zhang and Xiaoyao Zheng and Xiukai Ruan},
keywords = {Industrial wireless sensor networks, Compressed sensing, Data correlation, Data collection, Recovery error},
abstract = {Existing compressed sensing (CS)-based spatiotemporal data compression schemes can significantly decrease communication consumption for data collection; however, they ignore data correlation among different clusters over spatial dimensions. To explore data correlation among different clusters and satisfy the requirement of high data precision in industrial applications, in this paper, we propose a layered adaptive compression design for efficient data collection (LACD-EDC) in industrial wireless sensor networks (IWSNs). In the proposed scheme, first, we design a multilayer network architecture to support the exploration of spatiotemporal correlations, especially spatial correlation among different clusters. Then, we construct specific projection methods for exploring temporal correlation in sensory nodes, spatial correlation (intracluster) in cluster heads and spatial correlation (intercluster) in processing nodes. In addition, a detailed solution method is developed to recover the original data and achieve approximate data collection in the sink node. Subsequently, sparsifying dictionaries are trained for adapting different types of data and obtaining better sparse representations, which further improves the data recovery accuracy. Our simulation results indicate that the proposed layered adaptive compression scheme offers better recovery performance than conventional clustered compression schemes (i.e., achieving efficient data collection with high quality).}
}
@article{SKLYAR2019450,
title = {Organizing for digital servitization: A service ecosystem perspective},
journal = {Journal of Business Research},
volume = {104},
pages = {450-460},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S014829631930102X},
author = {Alexey Sklyar and Christian Kowalkowski and Bård Tronvoll and David Sörhammar},
keywords = {Digitalization, Servitization, Service ecosystem, Digitization, Centralization, Integration},
abstract = {Harnessing digital technology is of increasing concern as product firms organize for service-led growth. Adopting a service ecosystem perspective, we analyze interfirm and intrafirm change processes taking place as firms pursue digital servitization. The study draws on in-depth interviews with 44 managers involved in organizing activities in two multinational industry leaders. Our findings identify major differences between the two focal firms in terms of digital service-led growth and associated ecosystem-related activities. The study disentangles underlying processes of organizational change in the ecosystem and suggests that within-firm centralization and integration play a key role in the capacity to organize for digital servitization. For managers, the findings highlight the need to foster service-centricity in order to take full advantage of digitalization beyond purely technological benefits.}
}
@article{ZHIXINGCAI2019152,
title = {Automated groove identification and measurement using long short-term memory unit},
journal = {Measurement},
volume = {141},
pages = {152-161},
year = {2019},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2019.03.071},
url = {https://www.sciencedirect.com/science/article/pii/S0263224119303045},
author = {A. {Zhixing Cai} and B. {Lin Li} and C. {Yuping Hu} and D. {Wenting Luo} and E. {Chunmian Lin}},
keywords = {LSTM, Runway groove, Slab joint, Naïve Bayes classifier, GrooveNet},
abstract = {Transverse grooves have been widely used on airport runways to improve their drainage capacity and skid resistance. Therefore, regular measurement and evaluation of groove dimensions on airport runways are significant for runways to improve skid resistance and eliminate hydroplaning risks. However, there are few effective methods that are able to automatically and accurately measure groove dimension. This study introduces a new method for automated groove identification and measurement using Long Short-term Memory (LSTM). The GrooveNet is designed to identify the potential dip and determine two endpoints of the identified dip. Subsequently, dip dimension is calculated according to FAA AC No. 150/5320-12C. Finally, the modified Naïve Bayes classifier is proposed to distinguish grooves and slab joints. Results indicate that the proposed methodology (including GrooveNet and modified Naïve Bayes classifier) is more robust and accurate in runway groove identification and measurement. With the proposed approach, operators of airfield runway pavements have a robust tool to conduct groove safety evaluation and further provide corrective maintenance actions for the unsafe runway grooves.}
}
@article{LYUTOV2019215,
title = {Managing workflow of customer requirements using machine learning},
journal = {Computers in Industry},
volume = {109},
pages = {215-225},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519300636},
author = {Alexey Lyutov and Yilmaz Uygun and Marc-Thorsten Hütt},
keywords = {Documents management, Automation, Classification, Machine learning},
abstract = {Customer requirements – product specifications issued by the customer – organize the dialog between suppliers and customers and, hence, affect the dynamics of supply networks. These large and complex documents are frequently updated over time, while changes are seldom marked by the customers who issue the requirements. The lack of structure and defined responsibilities, thus, demands an expert to manually process the requirements. Here, the possibility to improve the usual workflow with machine learning algorithms is explored. The whole requirements management process has two major bottlenecks, which can be automatized. The first one, detecting changes, can be accomplished via a document comparison tool. The second one, recognizing the responsibilities and assigning them to the right department, can be solved with standard machine learning algorithms. Here, such algorithms are applied to a dataset obtained from a global automotive industry supplier. The proposed method improves the requirements management process by reducing an expert’s workload and thus decreasing the time for processing one document was reduced from 2 weeks to 1 h. Moreover, the method gives a high accuracy of department assignment and can self-improve once implemented into a requirements management system. Although the machine learning methods are very popular nowadays, they are seldom used to improve business processes in real companies, especially in the case of processes that did not require digitalization in the past. Here we show, how such methods can solve some of the management problems and improve their workflow.}
}
@article{GRALER20191348,
title = {Assisted setup of forming processes: architecture for the integration of non-adjustable disturbances},
journal = {Procedia CIRP},
volume = {81},
pages = {1348-1353},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119306390},
author = {Manuel Gräler and Astrid Wallow and Christian Henke and Ansgar Trächtler},
keywords = {design of experiments, disturbances, forming processes, progressive tools, setup assistant},
abstract = {Due to production flexibility, progressive tools are having to produce continually smaller batch sizes. A technician installs these tools in high-speed presses and adjusts several punch-bending operations manually via adjustment screws. Unpredictable disturbances like deviations in material thickness, temperatures, lubrication, or tool wear can all complicate the setup procedure. In this paper, an approach is proposed in order to compensate unpredictable, but measurable disturbances for these tools. For this, an architecture is introduced which is based on conventional statistical methods, which are combined with machine learning approaches, in order to optimize the setup process of the tools continuously.}
}
@incollection{2019375,
title = {Index},
editor = {Eric Delory and Jay Pearlman},
booktitle = {Challenges and Innovations in Ocean In Situ Sensors},
publisher = {Elsevier},
pages = {375-389},
year = {2019},
isbn = {978-0-12-809886-8},
doi = {https://doi.org/10.1016/B978-0-12-809886-8.18001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128098868180019}
}
@article{TERROSOSAENZ20191066,
title = {An open IoT platform for the management and analysis of energy data},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {1066-1079},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17304181},
author = {Fernando Terroso-Saenz and Aurora González-Vidal and Alfonso P. Ramallo-González and Antonio F. Skarmeta},
keywords = {IoT platform, Energy consumption, FIWARE, Data mining},
abstract = {Buildings are key players when looking at end-use energy demand. It is for this reason that during the last few years, the Internet of Things (IoT) has been considered as a tool that could bring great opportunities for energy reduction via the accurate monitoring and control of a large variety of energy-related agents in buildings. However, there is a lack of IoT platforms specifically oriented towards the proper processing, management and analysis of such large and diverse data. In this context, we put forward in this paper the IoT Energy Platform (IoTEP) which attempts to provide the first holistic solution for the management of IoT energy data. The platform we show here (that has been based on FIWARE) is suitable to include several functionalities and features that are key when dealing with energy quality insurance and support for data analytics. As part of this work, we have tested the platform IoTEP with a real use case that includes data and information from three buildings totalizing hundreds of sensors. The platform has exceed expectations proving robust, plastic and versatile for the application at hand.}
}
@article{BHARDWAJ20191066,
title = {PLANET-SNP pipeline: PLants based ANnotation and Establishment of True SNP pipeline},
journal = {Genomics},
volume = {111},
number = {5},
pages = {1066-1077},
year = {2019},
issn = {0888-7543},
doi = {https://doi.org/10.1016/j.ygeno.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0888754318303914},
author = {Archana Bhardwaj and Sumit K. Bag},
keywords = {Annotation, Diploid, Polyploidy, SNP, Machine learning},
abstract = {Acute prediction of SNPs (Single Nucleotide Polymorphisms) from high throughput sequencing data is a challenging problem, having potential to explore possible variation within plants species. For the extraction of profitable information from bulk of data, machine learning (ML) could lead to development of accurate model based on the learning of prior information. We performed state of art, in-depth learning on six different plant species. Comparative evaluation of five different algorithms showed that Random Forest substantially outperformed in selection of potential SNPs, with markedly improved prediction accuracy via 10-fold cross validation technique and integrated in system known as PLANET-SNP. We present the accurate method to extract the potential SNPs with user specific customizable parameters. It will facilitate the identification of efficient and functional SNPs in most easy and intuitive way. PLANET-SNP pipeline is very flexible in terms of data input and output formats. PLANET-SNP Pipeline is available at http://www.ncgd.nbri.res.in/PLANET-SNP-Pipeline.aspx}
}
@article{USMAN20191490,
title = {Comparison of SIRS, qSOFA, and NEWS for the early identification of sepsis in the Emergency Department},
journal = {The American Journal of Emergency Medicine},
volume = {37},
number = {8},
pages = {1490-1497},
year = {2019},
issn = {0735-6757},
doi = {https://doi.org/10.1016/j.ajem.2018.10.058},
url = {https://www.sciencedirect.com/science/article/pii/S0735675718308891},
author = {Omar A. Usman and Asad A. Usman and Michael A. Ward},
keywords = {Sepsis, Triage, Critical care, qSOFA, SIRS, NEWS},
abstract = {Objectives
The increasing use of sepsis screening in the Emergency Department (ED) and the Sepsis-3 recommendation to use the quick Sepsis-related Organ Failure Assessment (qSOFA) necessitates validation. We compared Systemic Inflammatory Response Syndrome (SIRS), qSOFA, and the National Early Warning Score (NEWS) for the identification of severe sepsis and septic shock (SS/SS) during ED triage.
Methods
This was a retrospective analysis from an urban, tertiary-care academic center that included 130,595 adult visits to the ED, excluding dispositions lacking adequate clinical evaluation (n = 14,861, 11.4%). The SS/SS group (n = 930) was selected using discharge diagnoses and chart review. We measured sensitivity, specificity, and area under the receiver-operating characteristic (AUROC) for the detection of sepsis endpoints.
Results
NEWS was most accurate for triage detection of SS/SS (AUROC = 0.91, 0.88, 0.81), septic shock (AUROC = 0.93, 0.88, 0.84), and sepsis-related mortality (AUROC = 0.95, 0.89, 0.87) for NEWS, SIRS, and qSOFA, respectively (p < 0.01 for NEWS versus SIRS and qSOFA). For the detection of SS/SS (95% CI), sensitivities were 84.2% (81.5–86.5%), 86.1% (83.6–88.2%), and 28.5% (25.6–31.7%) and specificities were 85.0% (84.8–85.3%), 79.1% (78.9–79.3%), and 98.9% (98.8–99.0%) for NEWS ≥ 4, SIRS ≥ 2, and qSOFA ≥ 2, respectively.
Conclusions
NEWS was the most accurate scoring system for the detection of all sepsis endpoints. Furthermore, NEWS was more specific with similar sensitivity relative to SIRS, improves with disease severity, and is immediately available as it does not require laboratories. However, scoring NEWS is more involved and may be better suited for automated computation. QSOFA had the lowest sensitivity and is a poor tool for ED sepsis screening.}
}
@article{CREVECOEUR2019930,
title = {Modeling the number of hidden events subject to observation delay},
journal = {European Journal of Operational Research},
volume = {277},
number = {3},
pages = {930-944},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719301924},
author = {Jonas Crevecoeur and Katrien Antonio and Roel Verbelen},
keywords = {Risk management, Occurrence of events, Observation delay, Calendar day effects, Data analytics},
abstract = {This paper considers the problem of predicting the number of events that have occurred in the past, but which are not yet observed due to a delay. Such delayed events are relevant in predicting the future cost of warranties, pricing maintenance contracts, determining the number of unreported claims in insurance and in modeling the outbreak of diseases. Disregarding these unobserved events results in a systematic underestimation of the event occurrence process. Our approach puts emphasis on modeling the time between the occurrence and observation of the event, the so-called observation delay. We propose a granular model for the heterogeneity in this observation delay based on the occurrence day of the event and on calendar day effects in the observation process, such as weekday and holiday effects. We illustrate this approach on a European general liability insurance data set where the occurrence of an accident is reported to the insurer with delay.}
}
@article{ANDROUTSOPOULOU2019358,
title = {Transforming the communication between citizens and government through AI-guided chatbots},
journal = {Government Information Quarterly},
volume = {36},
number = {2},
pages = {358-367},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17304008},
author = {Aggeliki Androutsopoulou and Nikos Karacapilidis and Euripidis Loukis and Yannis Charalabidis},
abstract = {Driven by ‘success stories’ reported by private sector firms, government agencies have also started adopting various Artificial Intelligence (AI) technologies in diverse domains (e.g. health, taxation, and education); however, extensive research is required in order to exploit the full potential of AI in the public sector, and leverage various AI technologies to address important problems/needs. This paper makes a contribution in this direction: it presents a novel approach, as well as the architecture of an ICT platform supporting it, for the advanced exploitation of a specific AI technology, namely chatbots, in the public sector in order to address a crucial issue: the improvement of communication between government and citizens (which has for long time been problematic). The proposed approach builds on natural language processing, machine learning and data mining technologies, and leverages existing data of various forms (such as documents containing legislation and directives, structured data from government agencies' operational systems, social media data, etc.), in order to develop a new digital channel of communication between citizens and government. Making use of appropriately structured and semantically annotated data, this channel enables ‘richer’ and more expressive interaction of citizens with government in everyday language, facilitating and advancing both information seeking and conducting of transactions. Compared to existing digital channels, the proposed approach is appropriate for a wider range of citizens' interactions, with higher levels of complexity, ambiguity and uncertainty. In close co-operation with three Greek government agencies (the Ministry of Finance, a social security organization, and a big local government organization), this approach has been validated through a series of application scenarios.}
}
@article{DANAF201935,
title = {Context-aware stated preferences with smartphone-based travel surveys},
journal = {Journal of Choice Modelling},
volume = {31},
pages = {35-50},
year = {2019},
issn = {1755-5345},
doi = {https://doi.org/10.1016/j.jocm.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1755534518300381},
author = {Mazen Danaf and Bilge Atasoy and Carlos Lima {de Azevedo} and Jing Ding-Mastera and Maya Abou-Zeid and Nathaniel Cox and Fang Zhao and Moshe Ben-Akiva},
keywords = {Stated preferences, Smartphone, Mode choice, Context-aware, Random design, Travel behavior},
abstract = {Stated preferences surveys are most commonly used to provide behavioral insights on hypothetical travel scenarios such as new transportation services or attribute ranges beyond those observed in existing conditions. When designing SP surveys, considerable care is needed to balance the statistical objectives with the realism of the experiment. This paper presents an innovative method for smartphone-based stated preferences (SP) surveys leveraging state-of-the-art smartphone-based survey platforms and their revealed preferences sensing capabilities. A random experimental design generates context-aware SP profiles using user specific socioeconomic characteristics and past travel data along with relevant web data for scenario generation. The generated choice tasks are automatically validated to reduce the number of dominant or inferior alternatives in real-time, then validated using Monte-Carlo simulations offline. In this paper we focus our attention on mode choice and design an experiment that considers a wide range of possible existing mode alternatives along with a new alternative on-demand mobility service that does not exist in real life. This experiment is then used to collect SP data or a sample of 224 respondents in the Greater Boston Area. A discrete mode choice model is estimated to illustrate the benefit of the proposed method in capturing current context-specific preferences in response to the new scenario.}
}
@incollection{2019341,
title = {Index},
editor = {Anna Visvizi and Miltiadis D. Lytras},
booktitle = {Smart Cities: Issues and Challenges},
publisher = {Elsevier},
pages = {341-353},
year = {2019},
isbn = {978-0-12-816639-0},
doi = {https://doi.org/10.1016/B978-0-12-816639-0.20001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012816639020001X}
}
@article{MENNENGA2019637,
title = {Exploring the Opportunities of System of Systems Engineering to Complement Sustainable Manufacturing and Life Cycle Engineering},
journal = {Procedia CIRP},
volume = {80},
pages = {637-642},
year = {2019},
note = {26th CIRP Conference on Life Cycle Engineering (LCE) Purdue University, West Lafayette, IN, USA May 7-9, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119300289},
author = {Mark Mennenga and Felipe Cerdas and Sebastian Thiede and Christoph Herrmann},
keywords = {System of Systems Engineering, Life Cycle Engineering, Sustainable Manufacturing, Framework},
abstract = {Increased digitalization leads to an overlap of technical systems, their surrounding environment and their embedded systems. The concept System of Systems (SoS) progressively emerges, offering the potential to contribute to sustainable development. A SoS bundles the capabilities and resources of its subsystems and, through intelligent collaboration, offers more functionality than the sum of its sub-systems. The current research on SoS is rather fragmented and there is still an open discussion on basic taxonomic and ontological issues. One important topic being discussed is the applicability of SoS Engineering (SoSE) to other related research disciplines. In this paper, we examine the interrelations of sustainable manufacturing, Life Cycle Engineering (LCE) and SoSE. Therefore, we provide a typology and a process-oriented framework for the integration of SoSE as a complementary discipline within the context of sustainable manufacturing and LCE.}
}
@article{PASICHNYI2019546,
title = {Data-driven strategic planning of building energy retrofitting: The case of Stockholm},
journal = {Journal of Cleaner Production},
volume = {233},
pages = {546-560},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.373},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619319158},
author = {Oleksii Pasichnyi and Fabian Levihn and Hossein Shahrokni and Jörgen Wallin and Olga Kordas},
keywords = {Urban energy planning, Building energy retrofitting, Urban building energy modelling, High-resolution metered data, Urban energy efficiency, Stockholm},
abstract = {Limiting global warming to 1.5 °C requires a substantial decrease in the average carbon intensity of buildings, which implies a need for decision-support systems to enable large-scale energy efficiency improvements in existing building stock. This paper presents a novel data-driven approach to strategic planning of building energy retrofitting. The approach is based on the urban building energy model (UBEM), using data about actual building heat energy consumption, energy performance certificates and reference databases. Aggregated projections of the energy performance of each building are used for holistic city-level analysis of retrofitting strategies considering multiple objectives, such as energy saving, emissions reduction and required social investment. The approach is illustrated by the case of Stockholm, where three retrofitting packages (heat recovery ventilation; energy-efficient windows; and a combination of these) were considered for multi-family residential buildings constructed 1946–1975. This identified potential for decreasing heat demand by 334 GWh (18%) and consequent emissions reduction by 19.6 kt-CO2 per year. The proposed method allows the change in total energy demand from large-scale retrofitting to be assessed and explores its impact on the supply side. It thus enables more precisely targeted and better coordinated energy efficiency programmes. The case of Stockholm demonstrates the potential of rich urban energy datasets and data science techniques for better decision making and strategic planning.}
}
@article{VOSOUGH201944,
title = {Visualization approaches for understanding uncertainty in flow diagrams},
journal = {Journal of Computer Languages},
volume = {52},
pages = {44-54},
year = {2019},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X18300168},
author = {Zana Vosough and Dietrich Kammer and Mandy Keck and Rainer Groh},
keywords = {Uncertainty visualization, Flow diagrams, Sankey diagrams, Parallel sets, Product costing, Business intelligence},
abstract = {Business Intelligence applications often handle data sets that contain uncertain values. In this contribution, we focus on product costing, which deals with the average costs of product components – that vary significantly based on many factors such as inflation, exchange rates, and commodity prices. After experts estimate the uncertainty information for single items, decision makers need to quickly ascertain the cost uncertainties within the hierarchical data structure of the complete product. We propose that only a holistic visualization containing both data and uncertainty can provide this kind of quick overview. Such a visualization must be able to visualize tree data structures associated with value attributes. After conducting interviews with product costing experts, we focused on Flow diagrams, which fulfill this basic requirement. However, they need to be extended in order to directly incorporate uncertainty information. We investigated three visualization techniques applicable to the ribbons of Flow diagrams to convey uncertainty information: Color-code, Gradient, and Margin. Moreover, we designed five visual approaches to show the uncertainty on nodes of Flow diagrams that we evaluated with visualization experts. The approaches add different geometries to the nodes such as triangles, blocks, or forks. The preferred solutions for the nodes was adding forks or filled blocks. With regards to the ribbons, we contribute a user study involving the solution of different product costing tasks using the three different visualizations. Although Gradient was considered an intuitive choice to show uncertainty, it yielded the highest error rates. In contrast, Color-code and Margin were superior depending on the performed task. Based on these findings and the subjective feedback, we designed an integrated approach that combines elements from all three distinct techniques and applied it to Sankey diagrams and Parallel sets.}
}
@article{WAGNER2019101589,
title = {Cyber threat intelligence sharing: Survey and research directions},
journal = {Computers & Security},
volume = {87},
pages = {101589},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.101589},
url = {https://www.sciencedirect.com/science/article/pii/S016740481830467X},
author = {Thomas D. Wagner and Khaled Mahbub and Esther Palomar and Ali E. Abdallah},
keywords = {Advanced persistent threat, Cyber threat intelligence, Threat sharing, Relevance, Trust, Anonymity, Literature survey},
abstract = {Cyber Threat Intelligence (CTI) sharing has become a novel weapon in the arsenal of cyber defenders to proactively mitigate increasing cyber attacks. Automating the process of CTI sharing, and even the basic consumption, has raised new challenges for researchers and practitioners. This extensive literature survey explores the current state-of-the-art and approaches different problem areas of interest pertaining to the larger field of sharing cyber threat intelligence. The motivation for this research stems from the recent emergence of sharing cyber threat intelligence and the involved challenges of automating its processes. This work comprises a considerable amount of articles from academic and gray literature, and focuses on technical and non-technical challenges. Moreover, the findings reveal which topics were widely discussed, and hence considered relevant by the authors and cyber threat intelligence sharing communities.}
}
@article{GARCIA2019260,
title = {Process mining techniques and applications – A systematic mapping study},
journal = {Expert Systems with Applications},
volume = {133},
pages = {260-295},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419303161},
author = {Cleiton dos Santos Garcia and Alex Meincheim and Elio Ribeiro {Faria Junior} and Marcelo Rosano Dallagassa and Denise Maria Vecino Sato and Deborah Ribeiro Carvalho and Eduardo Alves Portela Santos and Edson Emilio Scalabrin},
keywords = {Process mining, Workflow mining, Process mining applications, Process mining case studies},
abstract = {Process mining is a growing and promising study area focused on understanding processes and to help capture the more significant findings during real execution rather than, those methods that, only observed idealized process model. The objective of this article is to map the active research topics of process mining and their main publishers by country, periodicals, and conferences. We also extract the reported application studies and classify these by exploration domains or industry segments that are taking advantage of this technique. The applied research method was systematic mapping, which began with 3713 articles. After applying the exclusion criteria, 1278 articles were selected for review. In this article, an overview regarding process mining is presented, the main research topics are identified, followed by identification of the most applied process mining algorithms, and finally application domains among different business segments are reported on. It is possible to observe that the most active research topics are associated with the process discovery algorithms, followed by conformance checking, and architecture and tools improvements. In application domains, the segments with major case studies are healthcare followed by information and communication technology, manufacturing, education, finance, and logistics.}
}
@article{SCHUTZER201982,
title = {Contribution to the development of a Digital Twin based on product lifecycle to support the manufacturing process},
journal = {Procedia CIRP},
volume = {84},
pages = {82-87},
year = {2019},
note = {29th CIRP Design Conference 2019, 08-10 May 2019, Póvoa de Varzim, Portgal},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.212},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119305177},
author = {Klaus Schützer and Júlia de Andrade Bertazzi and Carolina Sallati and Reiner Anderl and Eduardo Zancul},
keywords = {Digital Twin, Manufacturing Process, Development Process, Digitalization},
abstract = {The current manufacture challenges are closely linked to the aim of digitalizing the product, the process and the means of production. In such aspects, information about the production processes is available in real-time, allowing managers to act on digital models and, through them, apply decisions in real systems. Thus, having a mirror model or a Digital Twin enables real-time absorption, simulation and implementation of manufacturing variations from the real environment, allowing faster detection of physical problems, and faster production response. The Digital Twin is a virtual representation of the physical system, which is equipped with sensors and actuators and feed the digital system, where the monitoring of data and simulation of variations, for instance, take place. From the synchronized interactions of both components, it is possible to deliver the mentioned faster production responses. Brazilian and German universities joined efforts to develop a Digital Twin based on product lifecycle to support the Manufacturing Process to address these challenges. The proposed Digital Twin seeks to integrate the product twin and the twin of its development process. It shall represent the manufacturing process, enabling the monitoring and optimization of the real production process. The Digital Twin itself is addressed as a product inside the production system and, therefore, its development process will follow the product lifecycle perspective, from the conception and planning to its implementation and usage. The Digital Twin will be further improved with the introduction of Artificial Intelligence tools, characterizing a Smart Digital Twin of the Manufacturing Process. Thus, this paper aims to present the concepts of a research project that is being developed in a joint Brazilian-German Cooperative Research.}
}
@article{SAHOO201910,
title = {ProvCaRe: Characterizing scientific reproducibility of biomedical research studies using semantic provenance metadata},
journal = {International Journal of Medical Informatics},
volume = {121},
pages = {10-18},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618302697},
author = {Satya S. Sahoo and Joshua Valdez and Matthew Kim and Michael Rueschman and Susan Redline},
keywords = {Scientific reproducibility, Provenance metadata, W3C PROV specifications, ProvCaRe ontology, S3 model, Provenance-based ranking, ProvCaRe knowledge repository},
abstract = {Objective
Reproducibility of research studies is key to advancing biomedical science by building on sound results and reducing inconsistencies between published results and study data. We propose that the available data from research studies combined with provenance metadata provide a framework for evaluating scientific reproducibility. We developed the ProvCaRe platform to model, extract, and query semantic provenance information from 435, 248 published articles.
Methods
The ProvCaRe platform consists of: (1) the S3 model and a formal ontology; (2) a provenance-focused text processing workflow to generate provenance triples consisting of subject, predicate, and object using metadata extracted from articles; and (3) the ProvCaRe knowledge repository that supports “provenance-aware” hypothesis-driven search queries. A new provenance-based ranking algorithm is used to rank the articles in the search query results.
Results
The ProvCaRe knowledge repository contains 48.9 million provenance triples. Seven research hypotheses were used as search queries for evaluation and the resulting provenance triples were analyzed using five categories of provenance terms. The highest number of terms (34%) described provenance related to population cohort followed by 29% of terms describing statistical data analysis methods, and only 5% of the terms described the measurement instruments used in a study. In addition, the analysis showed that some articles included a higher number of provenance terms across multiple provenance categories suggesting a higher potential for reproducibility of these research studies.
Conclusion
The ProvCaRe knowledge repository (https://provcare.case.edu/) is one of the largest provenance resources for biomedical research studies that combines intuitive search functionality with a new provenance-based ranking feature to list articles related to a search query.}
}
@incollection{FAROOQI201931,
title = {Chapter 2 - Wireless sensor networks towards convenient infrastructure in the healthcare industry: A systematic study},
editor = {Nilanjan Dey and Jyotismita Chaki and Rajesh Kumar},
booktitle = {Sensors for Health Monitoring},
publisher = {Academic Press},
pages = {31-46},
year = {2019},
volume = {5},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-819361-7},
doi = {https://doi.org/10.1016/B978-0-12-819361-7.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128193617000026},
author = {Md. Rashid Farooqi and Naiyar Iqbal and Nripendra Kumar Singh and Mohammad Affan and Khalid Raza},
keywords = {Wireless sensor network (WSN), Health monitoring, Body sensors},
abstract = {Wireless sensor network (WSN) technology refers to a group of sensors used for monitoring and recording the physical conditions of the environment and organizing the collected data at a central location. This sensor network can include thousands of smart sensing nodes with processing abilities that are powered by a dedicated battery. A WSN consists of a tiny wireless computer that communicates environmental stimuli, including vibrations, light, and temperature. It is an integral part of such domains as industry, health care, infrastructure, and research and development. As it relates to health care, a WSN can diagnose abnormal conditions in a patient, issue alarms and alerts, and send electronic mail to healthcare providers. This chapter is a discussion of the applications of WSNs in the healthcare industry.}
}
@article{BAHLO2019459,
title = {The role of interoperable data standards in precision livestock farming in extensive livestock systems: A review},
journal = {Computers and Electronics in Agriculture},
volume = {156},
pages = {459-466},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918312699},
author = {Christiane Bahlo and Peter Dahlhaus and Helen Thompson and Mark Trotter},
keywords = {Interoperability, Data standards, Precision livestock farming, Decision support},
abstract = {Livestock industries are increasingly embracing precision farming and decision support tools. As a result, sensors, weather stations, individual animal tracking, feed monitoring and other sources create large data volumes, much of which is used only for a single purpose. There are unrealised potential benefits of making on farm data interoperable and accessible and federating it with public data sources. We reviewed recent literature on precision livestock farming (PLF) technologies in relation to the use of public data, open standards and interoperability. Livestock farms produce rising volumes of disparate private datasets, reflecting a variety of information needs and technological opportunities, but typically lacking interoperable formats and metadata. These as well as large amounts of accessible public datasets are currently underutilised in decision support tools. Tools that demonstrate the use of interoperable standards and bring together public and private data for decision support can enhance the value proposition and help lower barriers to the sharing and re-use of data. This review of interoperable standards in extensive livestock farming systems concludes that there is a need for not only a new type of decision support tool, but also a consensus on data exchange standards to prove the value of shared data at farm scale (commercial benefit) and a regional scale (public good).}
}