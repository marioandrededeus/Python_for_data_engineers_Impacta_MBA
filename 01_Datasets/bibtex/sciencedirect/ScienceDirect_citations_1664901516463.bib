@article{YOUSIF2019100355,
title = {Analysis and forecasting of weather conditions in Oman for renewable energy applications},
journal = {Case Studies in Thermal Engineering},
volume = {13},
pages = {100355},
year = {2019},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2018.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X18303253},
author = {Jabar H. Yousif and Haitham A. Al-Balushi and Hussein A. Kazem and Miqdam T. Chaichan},
keywords = {Climate data, Forecasting models, Renewable energy, Data analysis and visualization, Environment monitoring},
abstract = {This paper examines and analyses weather data in Oman to re-encode the appropriate climate dimension to generate solar energy. It also suggested prediction models that could accurately predict future weather information. The present study aims to help decision-makers take the necessary measures to address the demand for renewable energy generation and solutions to environmental problems by taking advantage of long daylight hours in Oman to increase the production of alternative and clean electricity. There is no doubt that different environmental factors such as temperature, humidity, wind intensity and rain have a significant impact on the amount of solar cells produced. However, accurate forecasting of temperature and humidity helps to select the best weather conditions that can help raise the generation of solar energy and reduce the cost of production, leading to an increase in the economic income of countries. This paper presents various mathematical prediction models based on a multi-boundary score (2, 3, 4), which has the value of the R2 determination factor equal to (0.9335, 0.9603, 0.9977), respectively. The column test results (Prob> F) proved that the null hypothesis was accepted and rejected the alternative hypothesis. Thus, all the results are less than the significant value (0.5), and each variable has an average value or less than the mean value of the test (26). Therefore, there are no significant differences or unusual cases in historical temperature data in Oman from 1991 to 2015. Also, the prediction values corresponding to the actual temperature in the future, which helps to predict and analyze the temperature data at any time.}
}
@article{ZHANG2019117730,
title = {Sustainable maintainability management practices for offshore assets: A data-driven decision strategy},
journal = {Journal of Cleaner Production},
volume = {237},
pages = {117730},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.117730},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619325909},
author = {Shengyue Zhang and Yifei Yan and Peng Wang and Zhiqian Xu and Xiangzhen Yan},
keywords = {Offshore assets, Sustainable practices, Maintainability management, Data mining, Decision-making},
abstract = {Efficient and reliable maintainability management is a basic element behind ensuring sustainable practices for offshore assets. A method for easily implementing offshore asset maintainability management is proposed and described in terms of the maintainability decision-making process based on data-mining technology. A dataset of historical logs from operational systems drawn from offshore oil and gas service firms that have adopted sustainable maintenance management information system practices, including 12 main systems and 91 auxiliary systems, is used to inform a case study describing maintainability management and the decision-making process. The method uses the optimal values from a multicore classification model that adopts a goal-oriented data mining decision-making approach. Based on maintenance feature attributes, fault duration, fault loss, and frequency of occurrence in maintenance management are the three most important predictors of decision objectives. The decision tree classification results also indicate that the total average maintainability of assets in the key assets component is 53.4%, and the total average maintainability of noncritical assets is 37.9%. The five most important characteristic events found during maintenance and configuration processes were flaws in the tubing for A-annulus communication, leakage in the closed position, external leakage, failure to close on demand, and hydraulic failures that cause safety loss. The results provide unique insights into how offshore enterprise operators can improve maintainability management and decision-making performance using a data-driven decision strategy perspective. Furthermore, it provides a solution for visual proactive maintenance management and decision making under a data-driven framework, making it easier to implement maintenance management and decision-making tasks.}
}
@article{NGULUBE2019100966,
title = {Cartographies of research designs in library information science research in Nigeria and South Africa, 2009–2015},
journal = {Library & Information Science Research},
volume = {41},
number = {3},
pages = {100966},
year = {2019},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2019.100966},
url = {https://www.sciencedirect.com/science/article/pii/S0740818819300611},
author = {Patrick Ngulube and Scholastica C. Ukwoma},
abstract = {Research designs are key to the research process and the production of knowledge that supports performance and development. The appropriateness of the methodologies used in research has implications for the trustworthiness and validity of the outcomes of research and practice. The research designs used in library information science (LIS) research in Nigeria and South Africa between 2009 and 2015 were investigated. The objective was to map out the contours of the research designs that are utilised in LIS, particularly to keep the profession abreast of the trends in the field and the patterns in research designs used. Qualitative content analysis was used to examine 104 PhD dissertations, using six taxonomies to categorise research designs used in the two countries. Positivist epistemologies and quantitative methodologies predominated research in LIS. A handful of studies used basic mixed method research designs. Questionnaires and interviews were commonly used for data collection, but the triangulation of methods was not prevalent. The value of this study lies in that it will lead to the accumulation of knowledge of research designs and provide a baseline for studies on methodological practices.}
}
@article{MCCOY201995,
title = {Machine learning applications in minerals processing: A review},
journal = {Minerals Engineering},
volume = {132},
pages = {95-109},
year = {2019},
issn = {0892-6875},
doi = {https://doi.org/10.1016/j.mineng.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0892687518305430},
author = {J.T. McCoy and L. Auret},
keywords = {Machine learning, Artificial intelligence, Machine vision, Fault detection and diagnosis, Data-based modelling},
abstract = {Machine learning and artificial intelligence techniques have an ever-increasing presence and impact on a wide-variety of research and commercial fields. Disappointed by previous hype cycles, researchers and industrial practitioners may be wary of overpromising and underdelivering techniques. This review aims at equipping researchers and industrial practitioners with structured knowledge on the state of machine learning applications in mineral processing: the supplementary material provides a searchable summary of all techniques reviewed, with fields including nature of case study data (synthetic/laboratory/industrial), level of success, area of application (e.g. milling, flotation, etc), and major problem category (data-based modelling, fault detection and diagnosis, and machine vision). Future directions are proposed, including suggestions on data collection, technique comparison, industrial participation, cost-benefit analyses and the future of mineral engineering training.}
}
@article{TRAINI2019177,
title = {Machine Learning Framework for Predictive Maintenance in Milling},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {177-182},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.172},
url = {https://www.sciencedirect.com/science/article/pii/S240589631931122X},
author = {Emiliano Traini and Giulia Bruno and Gianluca D’Antonio and Franco Lombardi},
keywords = {Machine Learning, Industry 4.0, Artificial Intelligence, Predictive Maintenance Condition Monitoring, Milling, Vibration Analysis},
abstract = {In the Industry 4.0 era, artificial intelligence is transforming the manufacturing industry. With the advent of Internet of Things (IoT) and machine learning methods, manufacturing systems are able to monitor physical processes and make smart decisions through realtime communication and cooperation with humans, machines, sensors, and so forth. Artificial intelligence enables manufacturers to reduce equipment downtime, spot production defects, improve the supply chain, and shorten design times by using machine learning technologies which learn from experiences. One of the last application of these technologies is the development of Predictive Maintenance systems. Predictive maintenance combines Industrial IoT technologies with machine learning to forecast the exact time in which manufacturing equipment will need maintenance, allowing problems to be solved and adaptive decisions to be made in a timely fashion. This study will discuss the implementation of a milling Cutting-tool Predictive Maintenance solution (including Wear Monitoring), applied to a real milling data set as validation of the framework. More generally, this work provides a basic framework for creating a tool to monitor the wear level, preventing the breakdown, of a generic manufacturing tool, in order to improve human-machine interaction and optimize the production process.}
}
@article{TANG2019116973,
title = {Comparison of GOCI and Himawari-8 aerosol optical depth for deriving full-coverage hourly PM2.5 across the Yangtze River Delta},
journal = {Atmospheric Environment},
volume = {217},
pages = {116973},
year = {2019},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2019.116973},
url = {https://www.sciencedirect.com/science/article/pii/S1352231019306120},
author = {Die Tang and Dongren Liu and Yulei Tang and Barnabas C. Seyler and Xunfei Deng and Yu Zhan},
keywords = {GOCI, Himawari-8, Aerosol optical depth, Hourly PM, Random forest, Human exposure},
abstract = {The aerosol optical depth (AOD) data from the Geostationary Ocean Color Imager (GOCI) and the Himawari-8 are valuable for deriving hourly ambient PM2.5 concentrations for assessing acute human exposure in East Asia. This study aims to comparatively evaluate the performance of these two AOD datasets for estimating the hourly PM2.5 on a 1-km grid by using the nonparametric approach with two random-forest submodels. The full-coverage AOD dataset was generated with the first submodel, followed by the PM2.5 estimation using the second submodel. For the Yangtze River Delta (YRD) in 2017, the validation R2 of filling AOD gaps in the GOCI and Himawari-8 was 0.992 and 0.978, respectively. Estimating the hourly PM2.5 concentrations by using the GOCI and Himawari-8 had similar performance, with the cross-validation R2 of 0.860 and 0.862, respectively. Because the PM2.5 predictions based on these two AOD datasets were almost identical, they were fused with the inverse-variance-weighting method to analyze the spatiotemporal patterns of PM2.5. The annual average hourly PM2.5 across YRD was the highest around 08:00 (45.9 μg/m3) and the lowest around 16:00 (39.0 μg/m3). The cumulative acute exposure assessment shows that approximately 21% of the YRD population was exposed to ambient PM2.5>250 μg/m3 for more than 10 h during 2017. This study demonstrates that the GOCI and Himawari-8 datasets are equally adequate to estimate 24-h full-coverage PM2.5 concentrations for air quality management and human health risk assessments.}
}
@article{HAINLINE2019130,
title = {A deep learning approach to estimation of subject-level bias and variance in high angular resolution diffusion imaging},
journal = {Magnetic Resonance Imaging},
volume = {59},
pages = {130-136},
year = {2019},
issn = {0730-725X},
doi = {https://doi.org/10.1016/j.mri.2019.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0730725X18304363},
author = {Allison E. Hainline and Vishwesh Nath and Prasanna Parvathaneni and Kurt G. Schilling and Justin A. Blaber and Adam W. Anderson and Hakmook Kang and Bennett A. Landman},
keywords = {HARDI, Q-ball, Bias correction, GFA, Measurement error, Neural network},
abstract = {The ability to evaluate empirical diffusion MRI acquisitions for quality and to correct the resulting imaging metrics allows for improved inference and increased replicability. Previous work has shown promise for estimation of bias and variance of generalized fractional anisotropy (GFA) but comes at the price of computational complexity. This paper aims to provide methods for estimating GFA, bias of GFA and standard deviation of GFA quickly and accurately. In order to provide a method for bias and variance estimation that can return results faster than the previously studied statistical techniques, three deep, fully-connected neural networks are developed for GFA, bias of GFA, and standard deviation of GFA. The results of these networks are compared to the observed values of the metrics as well as those fit from the statistical techniques (i.e. Simulation Extrapolation (SIMEX) for bias estimation and wild bootstrap for variance estimation). Our GFA network provides predictions that are closer to the true GFA values than a Q-ball fit of the observed data (root-mean-square error (RMSE) 0.0077 vs 0.0082, p < .001). The bias network also shows statistically significant improvement in comparison to the SIMEX-estimated error of GFA (RMSE 0.0071 vs. 0.01, p < .001).}
}
@article{TONELLO2019171,
title = {The PAU Survey: Operation and orchestration of multi-band survey data},
journal = {Astronomy and Computing},
volume = {27},
pages = {171-188},
year = {2019},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213133718301380},
author = {N. Tonello and P. Tallada and S. Serrano and J. Carretero and M. Eriksen and M. Folger and C. Neissner and I. Sevilla-Noarbe and F.J. Castander and M. Delfino and J. {De Vicente} and E. Fernandez and J. Garcia-Bellido and E. Gaztanaga and C. Padilla and E. Sanchez and L. Tortorelli},
keywords = {Cosmological survey, Data management, High throughput computing, Relational database, Data modeling, Web application},
abstract = {The Physics of the Accelerating Universe (PAU) Survey is an international project for the study of cosmological parameters associated with Dark Energy. The PAU 18-CCD camera (PAUCam) is installed at the primary focus of the William Herschel Telescope at Observatorio del Roque de los Muchachos (La Palma, Canary Islands). The camera incorporates a unique set of 40 narrow band filters in the optical range from 450 nm to 850 nm and a set of 6 standard broad band filters. It is used to scan part of the northern sky in order to collect low resolution spectral information from millions of galaxies. The PAU data management (PAUdm) team is in charge of treating the data, including data transfer from the observatory to the PAU Survey data center, hosted at Port d’Informació Científica (PIC). PAUdm is also in charge of the storage, data reduction and, finally, of making the results available to the scientific community. We describe the technical solutions adopted to cover different aspects of the PAU Survey data management, from the computing infrastructure to the software tools and web services for the processing orchestration and data exploration. We give special attention to the PAU database, developed to preserve and guarantee the consistency of data and metadata and also used to coordinate the different PAUdm tasks.}
}
@article{LABONNOTE201933,
title = {A climate services perspective on Norwegian stormwater-related databases},
journal = {Climate Services},
volume = {13},
pages = {33-41},
year = {2019},
issn = {2405-8807},
doi = {https://doi.org/10.1016/j.cliser.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2405880718300086},
author = {Nathalie Labonnote and Åshild {Lappegard Hauge} and Edvard Sivertsen},
keywords = {Climate adaptation, Climate services, Databases, Stormwater management, Flooding damage databases},
abstract = {Floods and stormwater events are the costliest natural catastrophes. Costs are expected to increase due to urbanization and climate change. Mitigation is needed. Different stakeholders with different motivations unfortunately often evaluate vulnerability by using fragmented and incomplete data sources. This paper intends to review the different approaches for collecting and analyzing data, and to evaluate their usefulness within the proposed framework for a “smart” use of data. The objectives of this work have been to review qualitatively and quantitatively a selection of Norwegian stormwater-related databases and to propose measures for improvement. The findings are seen according to the climate services literature and show that that data is spread around a heterogeneous community of stakeholders concerned with different motivations, different needs, and different levels of data processing. In general, the needs of the different stakeholders have not been surveyed and defined systematically enough and there is a substantial potential in upgrading from the delivery of passive raw data to the delivery of knowledge-driven decision-support tools.}
}
@article{CURRY2019405,
title = {A Real-time Linked Dataspace for the Internet of Things: Enabling “Pay-As-You-Go” Data Management in Smart Environments},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {405-422},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1732887X},
author = {Edward Curry and Wassim Derguech and Souleiman Hasan and Christos Kouroupetroglou and Umair {ul Hassan}},
keywords = {Smart environments, Data management, Internet of Things, Water management, Energy management, Dataspace, Linked data, Semantic web, Event processing, Distributed systems},
abstract = {As smart environments move from a research vision to concrete manifestations in real-world enabled by the Internet of Things, they are encountering a number of very practical challenges in data management in terms of the flexibility needed to bring together contextual and real-time data, the interface between new digital infrastructures and existing information systems, and how to easily share data between stakeholders in the environment. Therefore, data management approaches for smart environments need to support flexibility, dynamicity, incremental change, while keeping costs to a minimum. A Dataspace is an emerging approach to data management that has proved fruitful for personal information and scientific data management. However, their use within smart environments and for real-time data remains largely unexplored. This paper introduces a Real-time Linked Dataspace (RLD) as an enabling platform for data management within smart environments. This paper identifies common data management requirements for smart energy and water environments, details the RLD architecture and the key support services and their tiered support levels, and a principled approach to “Pay-As-You-Go” data management. The paper presents a dataspace query service for real-time data streams and entities to enable unified entity-centric queries across live and historical stream data. The RLD was validated in 5 real-world pilot smart environments following the OODA (Observe, Orient, Decide, and Act) Loop to build real-time analytics, decisions support, and smart apps for energy and water management. The pilots demonstrate that the RLD enables incremental pay-as-you-go data management with support services that simplify the development of applications and analytics for smart environments. Finally, the paper discusses experiences, lessons learnt, and future directions.}
}
@article{BYE2019103675,
title = {Normalization of maritime accident data using AIS},
journal = {Marine Policy},
volume = {109},
pages = {103675},
year = {2019},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2019.103675},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X1930154X},
author = {Rolf Johan Bye and Petter G. Almklov},
keywords = {Normalization of accident data, AIS, Activity data, Exposure data},
abstract = {The Automatic Identification System (AIS), fitted on most ships today, is primarily used for real time ship monitoring. This paper illustrates how stored AIS data can be used to construct activity data that can be used to normalize accident statistics, to turn recorded numbers of accidents into accident rates. We show, by way of some examples, the potential in using AIS to construct different types of activity data, and discuss the advantage of combining measures based on different activity data when monitoring accident trends or trying to identify accident prone types of vessels. The analysis and discussion are based on a combination of the Norwegian database of maritime accidents and 6 years of recorded AIS data. The paper addresses methodological issues regarding the construction and use of these activity measures, demonstrates how they can provide new knowledge both for researchers and authorities and outlines some directions for further research.}
}
@article{ZHU2019105596,
title = {Forming a new small sample deep learning model to predict total organic carbon content by combining unsupervised learning with semisupervised learning},
journal = {Applied Soft Computing},
volume = {83},
pages = {105596},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105596},
url = {https://www.sciencedirect.com/science/article/pii/S156849461930376X},
author = {Linqi Zhu and Chong Zhang and Chaomo Zhang and Zhansong Zhang and Xin Nie and Xueqing Zhou and Weinan Liu and Xiu Wang},
keywords = {Small sample, Deep learning, Integrated deep learning model, Coarse-detailed feature extraction, Total organic carbon content},
abstract = {The total organic carbon (TOC) content is a parameter that is directly used to evaluate the hydrocarbon generation capacity of a reservoir. For a reservoir, accurately calculating TOC using well logging curves is a problem that needs to be solved. Machine learning models usually yield the most accurate results. Problems of existing machine learning models that are applied to well logging interpretations include poor feature extraction methods and limited ability to learn complex functions. However, logging interpretation is a small sample problem, and traditional deep learning with strong feature extraction ability cannot be directly used; thus, a deep learning model suitable for logging small sample features, namely, a combination of unsupervised learning and semisupervised learning in an integrated DLM (IDLM), is proposed in this paper and is applied to the TOC prediction problem. This study is also the first systematic application of a deep learning model in a well logging interpretation. First, the model uses a stacked extreme learning machine sparse autoencoder (SELM-SAE) unsupervised learning method to perform coarse feature extraction for a large number of unlabeled samples, and a feature extraction layer consisting of multiple hidden layers is established. Then, the model uses the deep Boltzmann machine (DBM) semisupervised learning method to learn a large number of unlabeled samples and a small number of labeled samples (the input is extracted from logging curve values into SELM-SAE extracted features), and the SELM-SAE and DBM are integrated to form a deep learning model (DLM). Finally, multiple DLMs are combined to form an IDLM algorithm through an improved weighted bagging algorithm. A total of 2381 samples with an unlabeled logging response from 4 wells in 2 shale gas areas and 326 samples with determined TOC values are used to train the model. The model is compared with 11 other machine learning models, and the IDLM achieves the highest precision. Moreover, the simulation shows that for the TOC prediction problem, when the number of labeled samples included in the training is greater than 20, even if this number of samples is used to train 10 hidden layer IDLMs, the trained model has a very low overfitting probability and exhibits the potential to exceed the accuracies of other models. Relative to the existing mainstream shallow model, the IDLM based on a DLM provides the most advanced performance and is more effective. This method implements a small sample deep learning algorithm for TOC prediction and can feasibly use deep learning to solve logging interpretation problems and other small sample set problems for the first time. The IDLM achieves high precision and provides novel insights that can aid in oil and gas exploration and development.}
}
@article{EXNER201977,
title = {A method to design Smart Services based on information categorization of industrial use cases},
journal = {Procedia CIRP},
volume = {83},
pages = {77-82},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.02.143},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119303026},
author = {Konrad Exner and Elisa Smolka and Till Blüher and Rainer Stark},
keywords = {Smart Services, use case, design method, information categorization},
abstract = {Smart Services can be described as a specific manifestation of Product-Service Systems, which mainly refer to customer individual solutions based on the utilization of data derived from cyber-physical products and production systems. In this context the usage data is allocated and analyzed on a platform in order to extract valuable information, thus enabling the development and operationalization of new data-driven business models. In order to support the initial design of Smart Services companies have to achieve a thorough understanding of their existing or obtainable data as well as the related potential options to utilize these data. Based on the analysis of 51 industrial Smart Service use cases data flows and areas of application have been derived and clustered. Moreover, the defined categories are interlinked and mapped to the use cases. The findings have been comprised into an applicable Smart Service design method addressing early design stages. Finally, the approach (DAU flow method) has been applied and evaluated with two case companies, indicating sound qualitative results and providing valuable feedback.}
}
@article{MATSUSHITA20194,
title = {The Clinical Innovation Network: a policy for promoting development of drugs and medical devices in Japan},
journal = {Drug Discovery Today},
volume = {24},
number = {1},
pages = {4-8},
year = {2019},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2018.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S1359644618300266},
author = {Shunsuke Matsushita and Keisuke Tachibana and Masuo Kondoh},
abstract = {The continuous increase in the costs of developing new drugs and medical devices drives increases in medical expenses. Seventy to ninety percent of these costs are associated with clinical trials. Therefore, the development of cost-effective methods to perform clinical trials remains a challenge. One approach is to use patient registries, collections of data related to patients with a specific diagnosis, condition, or procedure. Patient registries are used in Denmark, Sweden, and the USA for the enrollment of patients into clinical trials, and to evaluate endpoints. In Japan, a national project for registry-oriented clinical research, termed the ‘Clinical Innovation Network’ (CIN), was initiated in 2016. Here, we provide an overview of the CIN and discuss its impact on drug and device development in Japan.}
}
@article{YU2019111188,
title = {Supplement of the radiance-based method to validate satellite-derived land surface temperature products over heterogeneous land surfaces},
journal = {Remote Sensing of Environment},
volume = {230},
pages = {111188},
year = {2019},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0034425719301920},
author = {Wenping Yu and Mingguo Ma and Hong Yang and Junlei Tan and Xiaolu Li},
keywords = {Heterogeneous land surface, Validation, Land surface temperature, MODIS, HiWATER},
abstract = {Land surface temperature (LST) retrieved from satellite remote sensing data has become a key parameter in research on global environmental change; therefore, the acquisition of accurate satellite-derived LST information is crucial for the diagnosis and analysis of global change. However, it is relatively difficult to obtain the true value of a pixel due to the scale mismatch between in situ measurements and satellite-based observations, especially for commonly heterogeneous and nonisothermal land areas, which greatly increases the difficulty in estimating pixel-representative LST values from in situ measurements for validation of satellite-based LST products. In this study, a supplemented radiance-based (SR-based) validation method was developed to evaluate the latest moderate resolution imaging spectroradiometer (MODIS) Collection 6 Level 2 daily LST/land surface emissivity (LSE) products over a heterogeneous and nonisothermal region of the Heihe Watershed Allied Telemetry Experimental Research (HiWATER) project, West China. In the SR-based framework, pixel-representative LST values are simulated by the MODTRAN model from the corresponding in situ measurements, such as LSE and atmospheric profile measurements, to evaluate the MODIS LST products. The validation results show that the MODIS daytime LST products from the Aqua satellite (MYD11_L2) have a greater accuracy than those from the Terra satellite (MOD11_L2). Analyses of the effect factors indicate a strong correlation between the errors in the MOD11_L2 LST product and the corresponding difference in the MODIS brightness temperature between bands 31 and 32. Although the requirement of synchronous or quasisynchronous in situ measurements for the validated LST products may limit the applicability of the SR-based method, it is still an effective and simple method for validating satellite-derived LST products over mixed pixels. Our method is an indispensable supplement for the validation methods of satellite-derived LST products, and it can be applied in West China and other areas with heterogeneous land surfaces.}
}
@article{AVANCINI2019702,
title = {Energy meters evolution in smart grids: A review},
journal = {Journal of Cleaner Production},
volume = {217},
pages = {702-715},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.01.229},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619302501},
author = {Danielly B. Avancini and Joel J.P.C. Rodrigues and Simion G.B. Martins and Ricardo A.L. Rabêlo and Jalal Al-Muhtadi and Petar Solic},
keywords = {Energy, Internet of things, Prevention, Security, Smart grids, Smart meter},
abstract = {Intelligent Energy Networks are comprised of devices capable of fulfilling their functions in an energy-efficient fashion and with communication and remote control capabilities. Therefore, some of these devices, such as smart energy meters, become attractive for use in the power generation and distribution industry, achieving the vision of Smart Grids. However, many are the challenges that need to be overcome in order to reach a fully-functional and security-aware smart grid. Providing measurement, control, communication, power, display, and synchronization capabilities shall be no easy task for smart meters. In this context, this paper elaborates on a detailed description of the main functionalities that smart meters must provide, along with the analysis of existing solutions that make use of smart meters for smart grids. Moreover, open challenges in the topic are identified and discussed. By the end of this research piece, the reader should be able to have a detailed view of the capabilities already offered by smart meters and the ones they will have available in order to tackle the challenges smart grids present.}
}
@article{SUN2019368,
title = {Mapping the challenges of Artificial Intelligence in the public sector: Evidence from public healthcare},
journal = {Government Information Quarterly},
volume = {36},
number = {2},
pages = {368-383},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17304781},
author = {Tara Qian Sun and Rony Medaglia},
keywords = {Artificial Intelligence, Public sector, Healthcare, Challenges, Framing, China},
abstract = {The nascent adoption of Artificial Intelligence (AI) in the public sector is being assessed in contradictory ways. But while there is increasing speculation about both its dangers and its benefits, there is very little empirical research to substantiate them. This study aims at mapping the challenges in the adoption of AI in the public sector as perceived by key stakeholders. Drawing on the theoretical lens of framing, we analyse a case of adoption of the AI system IBM Watson in public healthcare in China, to map how three groups of stakeholders (government policy-makers, hospital managers/doctors, and Information Technology (IT) firm managers) perceive the challenges of AI adoption in the public sector. Findings show that different stakeholders have diverse, and sometimes contradictory, framings of the challenges. We contribute to research by providing an empirical basis to claims of AI challenges in the public sector, and to practice by providing four sets of guidelines for the governance of AI adoption in the public sector.}
}
@article{PEREZGONZALEZ2019167,
title = {Developing a data analytics platform to support decision making in emergency and security management},
journal = {Expert Systems with Applications},
volume = {120},
pages = {167-184},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418307449},
author = {Carlos J. Pérez-González and Marcos Colebrook and José L. Roda-García and Carlos B. Rosa-Remedios},
keywords = {Data analytics, Decision-making support, Dashboard and visualization tool, Forecasting and classification models},
abstract = {The Emergency and Security Coordinating Centre is the public service responsible for managing all the incidents registered in the Canary Islands. More than 7 million records have been collected in the last decade with more than twenty observed variables for each incident, which comprise more than 140 million data. All these data emanate from different islands and municipalities but with very marked differences. The study in this paper presents complete and novel research about geographical and temporal incident distribution, which may be of interest to emergency services managers and people responsible for designing public policies concerning security and health matters. We have developed an analytical web platform that features several dashboards with statistically significant results, by island, municipality, etc., and incorporating certain external data sources regarding social and economic issues which allow us to study the relationship between these factors and incident distribution at different geographical levels. Certain specific results are presented and illustrated in order to show all dimensions of data analytics that not only significantly improve companies’ and organizations’ processes, but also demonstrate how data analytics competency relates to decision making performance. Several statistical models that forecast and classify the incidents are proposed to illustrate the potential of statistical modelling in the study. The application has become into a crucial strategic tool for the organization because it helps in decision making processes. The information provided is highly modular and allows for the future inclusion of new features in order to provide larger and improved data analyses.}
}
@article{DAMICO20191,
title = {Machine Learning for Sustainable Structures: A Call for Data},
journal = {Structures},
volume = {19},
pages = {1-4},
year = {2019},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2018.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352012418301395},
author = {B. D'Amico and R.J. Myers and J. Sykes and E. Voss and B. Cousins-Jenvey and W. Fawcett and S. Richardson and A. Kermani and F. Pomponi},
keywords = {Sustainable, Structural, Materials, Embodied carbon, Life cycle assessment LCA, Machine learning, Neural networks},
abstract = {Buildings are the world's largest contributors to energy demand, greenhouse gases (GHG) emissions, resource consumption and waste generation. An unmissable opportunity exists to tackle climate change, global warming, and resource scarcity by rethinking how we approach building design. Structural materials often dominate the total mass of a building; therefore, a significant potential for material efficiency and GHG emissions mitigation is to be found in efficient structural design and use of structural materials. To this end, environmental impact assessment methods, such as life cycle assessment (LCA), are increasingly used. However, they risk failing to deliver the expected benefits due to the high number of parameters and uncertainty factors that characterise impacts of buildings along their lifespans. Additionally, effort and cost required for a reliable assessment seem to be major barriers to a more widespread adoption of LCA. More rapid progress towards reducing building impacts seems therefore possible by combining established environmental impact assessment methods with artificial intelligence approaches such as machine learning and neural networks. This short communication will briefly present previous attempts to employ such techniques in civil and structural engineering. It will present likely outcomes of machine learning and neural network applications in the field of structural engineering and – most importantly – it calls for data from professionals across the globe to form a fundamental basis which will enable quicker transition to a more sustainable built environment.}
}
@article{GARCIARODRIGUEZ2019441,
title = {Spanish Public Procurement: legislation, open data source and extracting valuable information of procurement announcements},
journal = {Procedia Computer Science},
volume = {164},
pages = {441-448},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919322513},
author = {Manuel J. {García Rodríguez} and Vicente Rodríguez Montequín and Francisco Ortega Fernández and Joaquín Villanueva Balsera},
keywords = {Public Procurement, E-Procurement, Open Data Government, Open Data Use, Award of Public Contracts, Spanish Tenders},
abstract = {Open Data in Public Administrations and, in particular, the publications of public procurement (tenders) is a source of valuable information for the decision-making procedure. The analysis of public tenders can provide valuable information for the different stakeholders: politicians, public managers, project managers, executives and, indirectly, citizens. Open Data allows the application of Business Intelligent and massive data processing techniques. This study presents the current situation of the Spanish Public Procurement processes and its open data sources available for citizens. The focus of the study is the Request-For-Proposal (RFP) and tender submission related data. The European and Spanish legislation which applies to this topic is collected. The Spanish Public Sector Contracting Platform, which is the web platform where public procurement announcements and their resolutions are published, is explained. The information can be very useful for researchers who want to carry out studies applying massive data processing techniques. A use case is presented using the open data of that web platform with different approaches to demonstrate its usefulness in a Business Intelligent context. Examples describing quantitative, geographic, sectorial competitiveness and interregional mobility analysis are presented illustrating possible applications.}
}
@article{MOSSER20191843,
title = {Mapping diphtheria-pertussis-tetanus vaccine coverage in Africa, 2000–2016: a spatial and temporal modelling study},
journal = {The Lancet},
volume = {393},
number = {10183},
pages = {1843-1855},
year = {2019},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(19)30226-0},
url = {https://www.sciencedirect.com/science/article/pii/S0140673619302260},
author = {Jonathan F Mosser and William Gagne-Maynard and Puja C Rao and Aaron Osgood-Zimmerman and Nancy Fullman and Nicholas Graetz and Roy Burstein and Rachel L Updike and Patrick Y Liu and Sarah E Ray and Lucas Earl and Aniruddha Deshpande and Daniel C Casey and Laura Dwyer-Lindgren and Elizabeth A Cromwell and David M Pigott and Freya M Shearer and Heidi Jane Larson and Daniel J Weiss and Samir Bhatt and Peter W Gething and Christopher J L Murray and Stephen S Lim and Robert C Reiner and Simon I Hay},
abstract = {Summary
Background
Routine childhood vaccination is among the most cost-effective, successful public health interventions available. Amid substantial investments to expand vaccine delivery throughout Africa and strengthen administrative reporting systems, most countries still require robust measures of local routine vaccine coverage and changes in geographical inequalities over time.
Methods
This analysis drew from 183 surveys done between 2000 and 2016, including data from 881 268 children in 49 African countries. We used a Bayesian geostatistical model calibrated to results from the Global Burden of Diseases, Injuries, and Risk Factors Study 2017, to produce annual estimates with high-spatial resolution (5 ×    5 km) of diphtheria-pertussis-tetanus (DPT) vaccine coverage and dropout for children aged 12–23 months in 52 African countries from 2000 to 2016.
Findings
Estimated third-dose (DPT3) coverage increased in 72·3% (95% uncertainty interval [UI] 64·6–80·3) of second-level administrative units in Africa from 2000 to 2016, but substantial geographical inequalities in DPT coverage remained across and within African countries. In 2016, DPT3 coverage at the second administrative (ie, district) level varied by more than 25% in 29 of 52 countries, with only two (Morocco and Rwanda) of 52 countries meeting the Global Vaccine Action Plan target of 80% DPT3 coverage or higher in all second-level administrative units with high confidence (posterior probability ≥95%). Large areas of low DPT3 coverage (≤50%) were identified in the Sahel, Somalia, eastern Ethiopia, and in Angola. Low first-dose (DPT1) coverage (≤50%) and high relative dropout (≥30%) together drove low DPT3 coverage across the Sahel, Somalia, eastern Ethiopia, Guinea, and Angola.
Interpretation
Despite substantial progress in Africa, marked national and subnational inequalities in DPT coverage persist throughout the continent. These results can help identify areas of low coverage and vaccine delivery system vulnerabilities and can ultimately support more precise targeting of resources to improve vaccine coverage and health outcomes for African children.
Funding
Bill & Melinda Gates Foundation.}
}
@article{DING20191137,
title = {Life cycle assessment of car sharing models and the effect on GWP of urban transportation: A case study of Beijing},
journal = {Science of The Total Environment},
volume = {688},
pages = {1137-1144},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.06.111},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719326798},
author = {Ning Ding and Jingjin Pan and Zhan Zhang and Jianxin Yang},
keywords = {Life cycle assessment, Car sharing, Transportation, GWP, GHG reduction, Electric vehicle},
abstract = {Alongside a trend in lower private-vehicle ownership and the growing popularity of the shared use economy, car sharing is emerging as an alternative travel mode. The LCA model of car sharing is proposed, the global warming potential (GWP) of four car sharing models is determined, and the effect on GWP of urban transportation is explored. This study expanded the LCA of products to the LCA of services, by expanding the functional unit to service. In this study, the time dimension was considered during the functional unit setting. It was found that there are large GWP differences among different car sharing models. Electric vehicle car sharing models have less GWP than gasoline vehicle car sharing models. The dispatch distance and the numbers of passengers in one car are two key factors for GWP of car sharing models. When car sharing replaces ~10% and ~50% of private cars, the GWP reduction potentials of urban transportation are ~4% and ~20%, respectively. The overall distribution of car sharing should be set by considering the features of different models in different areas, to achieve the largest environment benefit by using car sharing in cities. Therefore, car sharing can be used as a measure for significant GWP reduction for city transportation.}
}
@article{SCHUHMACHER20192105,
title = {The Art of Virtualizing Pharma R&D},
journal = {Drug Discovery Today},
volume = {24},
number = {11},
pages = {2105-2107},
year = {2019},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1359644619301394},
author = {Alexander Schuhmacher and Oliver Gassmann and Michael Kuss and Markus Hinder}
}
@incollection{MANDREOLI2019235,
title = {Chapter 9 - Dealing With Data Heterogeneity in a Data Fusion Perspective: Models, Methodologies, and Algorithms},
editor = {Marina Cocchi},
series = {Data Handling in Science and Technology},
publisher = {Elsevier},
volume = {31},
pages = {235-270},
year = {2019},
booktitle = {Data Fusion Methodology and Applications},
issn = {0922-3487},
doi = {https://doi.org/10.1016/B978-0-444-63984-4.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444639844000090},
author = {Federica Mandreoli and Manuela Montangero},
keywords = {Data fusion, Data heterogeneity, Data integration, Entity resolution, Life science data sources},
abstract = {Dealing with multiple manifestations of the same real-world entity across several data sources is a very common challenge for many modern applications, including life science applications. This challenge is referenced as data heterogeneity in the data management research field where the final aim is often to get a unified or integrated view of the real-world entities represented in the data sources. Data heterogeneity is a long-standing challenge that has attracted much interest in different computer science disciplines. The main aim of the chapter is to show how data heterogeneity problems that are typical of life science application contexts can be afforded by adopting systematic solutions stemming from the computer science field. To this end, it focusses on the main sources of heterogeneity in the life science context, presents the main problems that arise when dealing with heterogeneity, and provides a review of the solutions proposed in the computer science literature.}
}
@article{VELLIANGIRI2019104,
title = {A Review of Dimensionality Reduction Techniques for Efficient Computation},
journal = {Procedia Computer Science},
volume = {165},
pages = {104-111},
year = {2019},
note = {2nd International Conference on Recent Trends in Advanced Computing ICRTAC -DISRUP - TIV INNOVATION , 2019 November 11-12, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.01.079},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920300879},
author = {S. Velliangiri and S. Alagumuthukrishnan and S Iwin {Thankumar joseph}},
keywords = {Dimensionality reduction, Feature Selection, Feature Reduction, Feature Extraction},
abstract = {Dimensionality Reduction (DR) is the pre-processing step to remove redundant features, noisy and irrelevant data, in order to improve learning feature accuracy and reduce the training time. Dimensionality reductions techniques have been proposed and implemented by using feature selection and extraction method. Principal Component Analysis (PCA) one of the Dimensions reduction techniques which give reduced computation time for the learning process. In this paper presents most widely used feature extraction techniques such as EMD, PCA, and feature selection techniques such as correlation, LDA, forward selection have been analyzed based on high performance and accuracy. These techniques are highly applied in Deep Neural Network for medical image diagnosis and used to improve the classification accuracy. Further, we discussed how dimension reduction is made in deep learning.}
}
@article{AITISSAD2019511,
title = {A comprehensive review of Data Mining techniques in smart agriculture},
journal = {Engineering in Agriculture, Environment and Food},
volume = {12},
number = {4},
pages = {511-525},
year = {2019},
issn = {1881-8366},
doi = {https://doi.org/10.1016/j.eaef.2019.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1881836619301533},
author = {Hassina {Ait Issad} and Rachida Aoudjit and Joel J.P.C. Rodrigues},
keywords = {Data mining, Smart agriculture, Precision agriculture, IoT, WSN},
abstract = {Agriculture remains a vital sector for most countries. It presents the main source of food for the population of the world. However, it faces a big challenge: producing more and better while increasing the sustainability with a reasonable use of natural resources, reducing environmental degradation as well as adapting to climate change. Hence, it is extremely important to switch from traditional agricultural methods to modern agriculture. Smart Agriculture is one of the solutions to deal with the growing demand for food while meeting sustainability requirements. In Smart Agriculture, the role of information is increasing. Information on weather conditions, soils, diseases, insects, seeds, fertilizers, etc. constitutes an important contribution to the economic and sustainable development of this sector. Smart management consists of collecting, transmitting, selecting and analyzing data. As the amount of agricultural data increases significantly, robust analytical techniques capable of processing and analyzing large amounts of data to obtain more reliable information and much more accurate predictions are essential. Data Mining is expected to play an important role in Smart Agriculture for managing real-time data analysis with massive data. The aim of this paper is to review ongoing studies and research on smart agriculture using the recent practice of Data Mining, to solve a variety of agricultural problems.}
}
@incollection{2019365,
title = {Index},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {365-373},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.09990-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376099905}
}
@article{COLLA20191937,
title = {Public Safety Decision-Making in the Context of Smart and Sustainable Cities},
journal = {Procedia Manufacturing},
volume = {39},
pages = {1937-1945},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.238},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920302857},
author = {M. Colla and G.D. Santos},
keywords = {Smart cities, Sustainable cities, Information orientation, Information, Communication Technologies, Decision-making},
abstract = {Prominent urbanization forces governments to rethink their management processes, incorporate new technologies and ensure quality of life through practices aligned with the concepts of smart and sustainable cities. With a qualitative approach through semi-structured interviews with commanders of two local police departments, this study investigated information orientation in the strategic decision-making process in the area of public safety in a small Brazilian city. The police departments have a limited ICT infrastructure to support the strategic decision-making process because their information systems are not connected. The results show that although the city of Pato Branco (Brazil) is considered a smart city in the area of public security, there are limited resources in several aspects of the police departments for the effective management of their ICT infrastructures. The impact of resource constraints reflects throughout the entire information use lifecycle - identification, collection, organization, processing, etc. - which fuels the strategic decision-making process. The implantation of an operations center could significantly reduce the effects of the problems identified in this research and further research may reveal the operational, technical, economic and financial viability of this proposal.}
}
@article{LI201974,
title = {Interval-valued fuzzy inference based on aggregation functions},
journal = {International Journal of Approximate Reasoning},
volume = {113},
pages = {74-90},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X19300143},
author = {Dechao Li and Min Zhu},
keywords = {Interval-valued aggregation, Interval-valued negation, Interval-valued implication, Compositional rule of inference, Quintuple implication principle},
abstract = {Generalized modus ponens (GMP) and generalized modus tollens (GMT), as two basic patterns of approximate reasoning, aim to acquire some reasonable imprecise conclusions from a collection of imprecise premises using some inference rules. To solve the GMP and GMT problems under interval-valued fuzzy setting, an interval-valued A-compositional rule of inference (ACRI) method and quintuple implication principle (QIP) method with interval-valued implication generated by A under any partial order are presented in this paper, where A is an interval-valued aggregation function. In order to develop these methods, we firstly discuss interval-valued negation generated by an interval-valued aggregation function with any partial order. Some properties of interval-valued implications generated by interval-valued aggregation functions with an arbitrary order are then analyzed. We further investigate the ACRI method and quintuple implication principle (QIP) method with interval-valued implication generated by interval-valued aggregations to solve the interval-valued fuzzy modus ponens (IFMP) and interval-valued fuzzy modus tollens (IFMT). Finally, two examples are implemented to illustrate our proposed approaches using some special interval-valued aggregation functions.}
}
@article{CHEN2019361,
title = {Understanding Granular Aspects of Ontology for Blockchain Databases},
journal = {Procedia Computer Science},
volume = {162},
pages = {361-367},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.296},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320083},
author = {Zhengxin Chen},
keywords = {Blockchain databases, Granular aspects, Ontology},
abstract = {Blockchain technology first appeared a decade ago and is gaining momentum in recent years. The role of ontology to blockchain technology has drawn much attention from researchers. In this paper, we explore ontology in blockchain technology from a unique perspective: Since granular computing can be applied to ontology, it would be a good idea to explore granular aspects of ontology in blockchain technology. Continuing our previous examinations on granular aspects on databases, in this paper, we study granular aspects of ontology for blockchain databases. We provide our own observations, and analyse implications of recent research work related the nature of blockchain technology. As shown in our discussion, this kind of exploration not only helps a better understanding on the nature of blockchain technology, but could also advance the study of granular computing itself.}
}
@article{SHI20191020,
title = {Best of both worlds: Mitigating imbalance of crowd worker strategic choices without a budget},
journal = {Knowledge-Based Systems},
volume = {163},
pages = {1020-1031},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118305161},
author = {Peng Shi and Manyu Zhao and Wanyuan Wang and Yifeng Zhou and Jiuchuan Jiang and J. Zhang and Yichuan Jiang and Zhifeng Hao},
keywords = {Crowdsourcing, Imbalance, Price mediation, Social welfare},
abstract = {Crowdsourcing has become a popular paradigm for requesters to hire ubiquitous crowd workers. The worker’s selfish instinct of choosing the most profitable task can cause the imbalance of task completion: some tasks achieve a number of redundant worker choices, while others may receive no worker response. Although budget-based incentives can mitigate the imbalance of crowd workers’ strategic choices, the extra budget makes them less attractive. To mitigate task completion imbalance without a budget, a price mediation mechanism is proposed. This mechanism works by allowing the crowdsourcing platforms to implicitly adjust task prices, thereby eliciting workers to balance their choices. The price adjustment should be carefully designed to satisfy (1) task completion integrity and (2) no extra budget, while it maximizes social welfare. We prove that this optimization problem is NP-hard to solve. By designing bound function and pruning strategies, we propose an optimal branch-and-bound algorithm for small-scale instances. To further improve the scalability for large-scale instances, a heuristic method based on price transfers is proposed. Experimental results on a real dataset show that compared with benchmarks, our approaches are effective for maximizing social welfare and are beneficial to both requesters and workers.}
}
@incollection{2019397,
editor = {W.H. Inmon and Daniel Linstedt and Mary Levins},
booktitle = {Data Architecture (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {397-408},
year = {2019},
isbn = {978-0-12-816916-2},
doi = {https://doi.org/10.1016/B978-0-12-816916-2.09989-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128169162099897}
}
@article{GE201968,
title = {Mesoscience-based virtual process engineering},
journal = {Computers & Chemical Engineering},
volume = {126},
pages = {68-82},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S009813541831216X},
author = {Wei Ge and Li Guo and Xinhua Liu and Fanyong Meng and Ji Xu and Wen Lai Huang and Jinghai Li},
keywords = {Coarse-graining, Mesoscale, Mesoscience, Stability condition, Virtual process engineering, Virtual reality},
abstract = {Accounting for complex mesoscale structures was found to be the key to predicting system performance from elemental properties, and hence a bottleneck for process systems engineering. The development and generalization of the energy-minimization multiscale (EMMS) model may present a continuous attempt to provide this key link, where mesoscale structures are characterized by analyzing the compromise in competition of the dominant mechanisms in the systems studied, and then an accurate and efficient simulation paradigm is established. This paradigm enables the integration of high-fidelity realtime simulation with virtual reality technologies to create a physically realistic digital counterpart of the industrial processes, that is, virtual process engineering (VPE). VPE may present a new research and development platform for process systems engineering. In long term, the seamless integration of physical and virtual experiments, either in situ or remotely, is also possible with VPE.}
}
@incollection{TOMAR201959,
title = {Chapter 2 - Migration of healthcare relational database to NoSQL cloud database for healthcare analytics and management},
editor = {Nilanjan Dey and Amira S. Ashour and Chintan Bhatt and Simon {James Fong}},
booktitle = {Healthcare Data Analytics and Management},
publisher = {Academic Press},
pages = {59-87},
year = {2019},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-815368-0},
doi = {https://doi.org/10.1016/B978-0-12-815368-0.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128153680000026},
author = {Dimpal Tomar and Jai Prakash Bhati and Pradeep Tomar and Gurjit Kaur},
keywords = {SQL, NoSQL, Big Data, RDBMS, Elastic databases, Cloud database, Data model},
abstract = {Due to the exponential increase in the volume of data in the healthcare sector generated from various sources such as ECGs, X-rays, CT scans, ultrasound reports, lab reports, radiology tests, and clinical research in the form of Electronic Health Records and Electronic Medical Records, relational databases cannot reasonably meet the requirements, such as Web service availability and scaling. Handling more users and data means the addition of big servers, and big servers are tremendously complex and extremely expensive. So, as organizations are facing performance problems with relational databases they are currently using for existing or new applications, especially when the number of users increases, they realize the necessity for a database tier that is faster and more flexible. This is the correct time to migrate from relational database systems to a cloud-based architecture for a NoSQL (NOT ONLY SQL) cloud database. The NoSQL cloud database is being adopted in order to scale out more applications and to support more users. NoSQL cloud databases allow an increase in performance and availability of services, so it seems to be favorable to put forward timely information on data migration from Structured Query Language (SQL) databases to NoSQL cloud databases. First, we present the challenges and analysis of data migration and also present data migration techniques from the relational database management system (RDBMS) to NoSQL using NoSQL technology. Second, we discuss the technology background, applications, and challenges related to NoSQL cloud-based technology. Then we present cloud-based architectures for databases, with technology, applications, and challenges of cloud databases and show how data migration techniques are used to migrate a relational database to a NoSQL cloud database.}
}
@article{CHANG2019263,
title = {Social media analytics: Extracting and visualizing Hilton hotel ratings and reviews from TripAdvisor},
journal = {International Journal of Information Management},
volume = {48},
pages = {263-279},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217303389},
author = {Yung-Chun Chang and Chih-Hao Ku and Chun-Hung Chen},
keywords = {Sentiment analysis, Hospitality, Natural language processing, Social media analytics, Visual analytics, Google trends, TripAdvisor},
abstract = {Analyzing and extracting insights from user-generated data has become a topic of interest among businesses and research groups because such data contains valuable information, e.g., consumers’ opinions, ratings, and recommendations of products and services. However, the true value of social media data is rarely discovered due to overloaded information. Existing literature in analyzing online hotel reviews mainly focuses on a single data resource, lexicon, and analysis method and rarely provides marketing insights and decision-making information to improve business’ service and quality of products. We propose an integrated framework which includes a data crawler, data preprocessing, sentiment-sensitive tree construction, convolution tree kernel classification, aspect extraction and category detection, and visual analytics to gain insights into hotel ratings and reviews. The empirical findings show that our proposed approach outperforms baseline algorithms as well as well-known sentiment classification methods, and achieves high precision (0.95) and recall (0.96). The visual analytics results reveal that Business travelers tend to give lower ratings, while Couples tend to give higher ratings. In general, users tend to rate lowest in July and highest in December. The Business travelers more frequently use negative keywords, such as “rude,” “terrible,” “horrible,” “broken,” and “dirty,” to express their dissatisfied emotions toward their hotel stays in July.}
}
@article{HOYER20191727,
title = {Exploring the challenges with applying tracking and tracing technology in the dairy industry},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1727-1732},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.450},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319314314},
author = {Madeleine R. Høyer and Olumide E. Oluyisola and Jan O. Strandhagen and Marco G. Semini},
keywords = {RFID, ubiquitous manufacturing, logistics in manufacturing, intelligent manufacturing systems, manufacturing plant control},
abstract = {The purpose of this research is to identify the various challenges encountered when using tracking and tracing technology in the dairy industry. Based on a systematic literature review, the challenges are reviewed from the supply chain perspective. The findings are then discussed within the context of a large dairy manufacturer that implemented RFID within its supply chain. The paper distinguishes between three different types of challenges regarding tracking and tracing technology: strategic, technical and convenience challenges, and are further categorized as either adoption barriers or implementation barriers. This study also finds that the technical requirements for implementing tracking and tracing technology pose the least difficulty, while organisational change and cyber-security risks are more critical.}
}
@article{KIM201963,
title = {Probabilistic prediction of direct normal irradiance derived from global horizontal irradiance over the Korean Peninsula by using Monte-Carlo simulation},
journal = {Solar Energy},
volume = {180},
pages = {63-74},
year = {2019},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2019.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X19300398},
author = {Chang Ki Kim and Hyun-Goo Kim and Yong-Heack Kang and Chang-Yeol Yun and Shin Young Kim},
keywords = {Decomposition model, Direct normal irradiance, Global horizontal irradiance, Prediction interval, Monte-Carlo simulation},
abstract = {Solar resource assessment is carried out in a feasibility study using reliable meteorological elements including solar irradiance. In concentrating solar power plants, the direct normal irradiance is the key variable in the system operation. However, direct normal irradiance is rarely measured as compared to global horizontal irradiance. There are several models that can be used to derive the direct normal irradiance from global horizontal irradiance. In this study, the Engerer model is used as a decomposition model, then evaluated against in situ observations at three ground stations: Seoul, Buan, and Jeju ground stations. The relative root mean square errors between the observed and direct normal irradiance estimated by the Engerer model are 15.0%, 19.4%, and 17.1% at Seoul, Buan, and Jeju ground stations, respectively. The uncertainty of estimates is represented by the prediction interval from probabilistic prediction through Monte-Carlo simulation that employs the bias between estimation and ground truth for training datasets. The prediction interval for 90% confidence level is 117.9 W m−2 at the Seoul station, resulting from Monte-Carlo simulation. The prediction interval coverage probability is 92.8%, implying that the probability that observed DNI is not included in the prediction interval is 7.2%. The error metrics for probabilistic prediction indicates that Monte-Carlo simulation provides both valid and more informative estimations.}
}
@article{PATEL2019692,
title = {Social implications of smart cities},
journal = {Procedia Computer Science},
volume = {155},
pages = {692-697},
year = {2019},
note = {The 16th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2019),The 14th International Conference on Future Networks and Communications (FNC-2019),The 9th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919310154},
author = {Yash Patel and Nishant Doshi},
keywords = {Smart Cities, End-User Privacy, Applications of Smart Cities, Technologies in Smart Cities},
abstract = {Smart Cities have become the new talk in the community. Citizens and businesses are engaged in an intelligent and connected ecosystem. Being faced by the challenges to meet objectives regarding the quality of life and social development, cities to cater that need are attempting to transform themselves into smart cities. Thus they are the result of knowledge-comprehensive and creative strategies aiming at reinforcing the socio-economic, ecological and competitive performance of cities. This paper aims to understand the social impact potential and the limits of smart cities. The concept of “Smart City” is adapting to the most powerful economic and social forces of our time to the needs of the places where most of us live and work. Along with the development of smart cities comes a large range of IoT devices which creates cybersecurity and privacy concerns. Hence it is very is important to study the implications of the smart cities on the social life of the people.}
}
@article{JAVED201963,
title = {Data analytics for Cooperative Intelligent Transport Systems},
journal = {Vehicular Communications},
volume = {15},
pages = {63-72},
year = {2019},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S221420961830192X},
author = {Muhammad Awais Javed and Sherali Zeadally and Elyes Ben Hamida},
keywords = {Intelligent transport systems, Data analytics, Route guidance application},
abstract = {Cooperative Intelligent Transport System (C-ITS) is a key enabler of future road traffic management systems. The core component of C-ITS includes vehicles, road side units and traffic command centers. They generate a large amount of traffic that is made up of both mobility and service-related data. To extract useful and relevant information out of this data, data analytics will play a crucial role in future C-ITS applications. We present a review of how data analytics can benefit C-ITS applications. We describe the various types of data generated by C-ITS applications and potential dissemination techniques using various wireless technologies. We demonstrate how meaningful results from this data could be beneficial to C-ITS. We also demonstrate the improved reliability of C-ITS applications that can be achieved with data analytics using simulation results. Finally, we discuss future possible applications of data analytics in C-ITS.}
}
@article{KHAN20191735,
title = {Unbiased data analytic strategies to improve biomarker discovery in precision medicine},
journal = {Drug Discovery Today},
volume = {24},
number = {9},
pages = {1735-1748},
year = {2019},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2019.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S1359644619300236},
author = {Saifur R. Khan and Yousef Manialawy and Michael B. Wheeler and Brian J. Cox},
abstract = {Omics technologies promised improved biomarker discovery for precision medicine. The foremost problem of discovered biomarkers is irreproducibility between patient cohorts. From a data analytics perspective, the main reason for these failures is bias in statistical approaches and overfitting resulting from batch effects and confounding factors. The keys to reproducible biomarker discovery are: proper study design, unbiased data preprocessing and quality control analyses, and a knowledgeable application of statistics and machine learning algorithms. In this review, we discuss study design and analysis considerations and suggest standards from an expert point-of-view to promote unbiased decision-making in biomarker discovery in precision medicine.}
}
@article{AGGESTAM2019124,
title = {Setting the stage for a Shared Environmental Information System},
journal = {Environmental Science & Policy},
volume = {92},
pages = {124-132},
year = {2019},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1462901118306944},
author = {Filip Aggestam},
keywords = {Shared Environmental Information System (SEIS), Environmental policy, Reporting obligations, Environmental statistics, Data harmonisation},
abstract = {There has been a significant increase in efforts to improve environmental data sharing practices in the past decade. One such initiative is the Shared Environmental Information System (SEIS), initiated by the European Commission in 2008, as part of a process to facilitate regular environmental assessments and State-of-the-Environment Reporting (SOER). Using SEIS as a case study example, this paper takes its departure from the 8th Environment for Europe (EFE) Ministerial conference to identify ongoing processes and challenges surrounding environmental data and information sharing. The paper relies on data obtained for the 2016 report on progress in establishing SEIS in support of regular reporting in the pan-European region. The article demonstrates a number of gaps with regards to the availability and accessibility of certain environmental datasets and indicators and highlights the suboptimal use of information, where comprehensive data flows and high-quality information is not being used adequately in support of policymaking or where there is selective use of environmental indicators. Against this background, questions arise as to whether applied models for data sharing can be implemented with equal success across different regions and countries that are characterized by heterogeneous and complex data practices and data flows. Most importantly, results from the SEIS progress report demonstrate the pressing need for a better understanding of environmental data types, data packaging and data flows across multiples contexts, epistemic cultures and policy making.}
}
@article{CAMERON2019102,
title = {Education in Process Systems Engineering: Why it matters more than ever and how it can be structured},
journal = {Computers & Chemical Engineering},
volume = {126},
pages = {102-112},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418311773},
author = {Ian T. Cameron and Sebastian Engell and Christos Georgakis and Norbert Asprion and Dominique Bonvin and Furong Gao and Dimitrios I. Gerogiorgis and Ignacio E. Grossmann and Sandro Macchietto and Heinz A. Preisig and Brent R. Young},
abstract = {This position paper is an outcome of discussions that took place at the third FIPSE Symposium in Rhodes, Greece, between June 20–22, 2016 (http://fi-in-pse.org). The FIPSE objective is to discuss open research challenges in topics of Process Systems Engineering (PSE). Here, we discuss the societal and industrial context in which systems thinking and Process Systems Engineering provide indispensable skills and tools for generating innovative solutions to complex problems. We further highlight the present and future challenges that require systems approaches and tools to address not only ‘grand’ challenges but any complex socio-technical challenge. The current state of Process Systems Engineering (PSE) education in the area of chemical and biochemical engineering is considered. We discuss approaches and content at both the unit learning level and at the curriculum level that will enhance the graduates’ capabilities to meet the future challenges they will be facing. PSE principles are important in their own right, but importantly they provide significant opportunities to aid the integration of learning in the basic and engineering sciences across the whole curriculum. This fact is crucial in curriculum design and implementation, such that our graduates benefit to the maximum extent from their learning.}
}
@article{MENDES201913,
title = {Fuzzy control system for variable rate irrigation using remote sensing},
journal = {Expert Systems with Applications},
volume = {124},
pages = {13-24},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.01.043},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419300491},
author = {Willians Ribeiro Mendes and Fábio Meneghetti U. Araújo and Ritaban Dutta and Derek M. Heeren},
keywords = {Fuzzy control, Variable rate irrigation, Speed control, Remote sensing, Decision support system},
abstract = {Variable rate irrigation (VRI) is the capacity to spatially vary the depth of water application in a field to handle different types of soils, crops, and other conditions. Precise management zones must be developed to efficiently apply variable rate technologies. However, there is no universal method to determine management zones. Using speed control maps for the central pivot is one option. Thus, this study aims to develop an intelligent fuzzy inference system based on precision irrigation knowledge, i.e., a system that can create prescriptive maps to control the rotation speed of the central pivot. Satellite images are used in this study because remote sensing offers quick measurements and easy access to information on crops for large irrigation areas. Based on the VRI-prescribed map created using the intelligent decision-making system, the pivot can increase or decrease its speed, reaching the desired depth of application in a certain irrigation zone. Therefore, considering the spatial variability in the crop has made the strategy of speed control more realistic than traditional methods for crop management. The intelligent irrigation system pointed out areas with lower leaf development, indicating that the pivot must reduce its speed, thus increasing the water layer applied to that area. The existence of well-divided zones could be observed; each zone provides a specific value for the speed that the pivot must develop for decreasing or increasing the application of the water layer to the crop area. Three quarters of the total crop area had spatial variations during water application. The set point built by the developed system pointed out zones with a decreased speed in the order of 50%. From the viewpoint of a traditional control, the relay from pivot percent timer should have been adjusted from 70% to 35% whenever the central pivot passed over that specific area. The proposed system obtained values of 37% and 47% to adjust the pivot percent timer. Therefore, it is possible to affirm that traditional control models used for central-pivot irrigators do not support the necessary precision to meet the demands of speed control determined by the developed VRI systems. Results indicate that data from the edaphoclimatic variables when well-fitted to the fuzzy logic can solve uncertainties and non-linearities of an irrigation system and establish a control model for high-precision irrigation.}
}
@article{KOZJEK2019809,
title = {Data mining for fault diagnostics: A case for plastic injection molding},
journal = {Procedia CIRP},
volume = {81},
pages = {809-814},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.204},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119305098},
author = {Dominik Kozjek and Rok Vrabič and David Kralj and Peter Butala and Nada Lavrač},
keywords = {fault diagnostics, plastic injection molding, data analytics, data mining, industrial data},
abstract = {In manufacturing processes the automated identification of faulty operating conditions that might lead to insufficient product quality and reduced availability of the equipment is an important and challenging task. This paper proposes a data mining approach to the identification of complex faults, i.e. unplanned machine stops in plastic injection molding. Several data mining methods are considered, with a focus on the abilities to reveal patterns of faulty operating conditions and on the interpretation of the induced models with the objective to find the data mining method that best corresponds to the nature of the plastic-injection-molding process and the related data. Well-known data mining methods, i.e. J48, random forests, JRip rules, naïve Bayes, and k-nearest neighbors are applied to real industrial data. The results show that tested data mining methods can be effectively used to reveal patterns related to faulty operating conditions. The interpretation capacity of the tested methods, their ability to describe the operating conditions, and to reveal patterns related to faulty operating conditions, are demonstrated and discussed.}
}
@article{ZANG2019112515,
title = {Disentangling residential self-selection from impacts of built environment characteristics on travel behaviors for older adults},
journal = {Social Science & Medicine},
volume = {238},
pages = {112515},
year = {2019},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2019.112515},
url = {https://www.sciencedirect.com/science/article/pii/S027795361930509X},
author = {Peng Zang and Yi Lu and Jing Ma and Bo Xie and Ruoyu Wang and Ye Liu},
keywords = {Travel behaviors, Residential self-selection, Older adults, Built environment},
abstract = {In the context of population ageing in many developed and developing countries, encouraging active transport behaviors of older adults, is a key public health priority. However, many cross-sectional studies assessing the impact of built environment characteristics on travel behavior fail to address residential self-selection bias, and hence the causal relationship is uncertain. A large-scale public housing scheme provided this study with a unique research opportunity to distinguish residential self-selection from the effects of built environment characteristics on the travel behaviors of older adults (N = 13,468 and 3,961 in two analyses respectively) in Hong Kong, because public housing residents have little freedom to choose their residential locations. The results showed that the elderly living in public housing estates generally have fewer trips, shorter overall travel times and distances, and fewer motorized trips including those by rail or private car than those living in private housing estates. In addition, the results for walking, walking times, numbers of trips, and travel distance for elderly people in public and private housing all exhibited markedly different associations with built environment characteristics. Strength of built environment-travel behavior associations dropped by approximately 30–50% after controlling for the effect of residential self-selection. The results indicate that both built environment characteristics and residential self-selection affect travel behaviors.}
}
@article{TANG2019117015,
title = {Aldehyde dehydrogenase-2 acts as a potential genetic target for renal fibrosis},
journal = {Life Sciences},
volume = {239},
pages = {117015},
year = {2019},
issn = {0024-3205},
doi = {https://doi.org/10.1016/j.lfs.2019.117015},
url = {https://www.sciencedirect.com/science/article/pii/S0024320519309427},
author = {Simin Tang and Teng Huang and Huan Jing and Zhenxing Huang and Hongtao Chen and Youling Fan and Jiying Zhong and Jun Zhou},
keywords = {Unilateral ureteral obstruction, Cisplatin, Ischemia-reperfusion injury, Renal fibrosis, Bioinformatics, Aldh2, Glycolysis pathway},
abstract = {Obstructive renal injury and drug-induced nephrotoxicity are the two most common causes of renal fibrosis diseases. However, whether these two different pathogeny induced same pathological outcomes contain common genetic targets or signaling pathway, the current research has not paid great attention. GSE121190 and GSE35257 were downloaded from the Gene Expression Omnibus (GEO) database. While GSE121190 represents a differential expression profile in kidney of mice with unilateral ureteral obstruction (UUO) model, GSE35257 represents cisplatin nephrotoxicity model. By using GEO2R, 965 differential expression genes (DEGs) in GSE121190 and 930 DEGs in GSE35257 were identified. 43 co-DEGs were shared and were extracted for protein-protein interaction (PPI) analysis. Subsequently, three shared pathways including glycolysis/gluconeogenesis, fatty acid degradation and pathways in cancer were involved in two models with Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis. We reconfirmed that these three pathways have relatively high scores by using Gene Set Enrichment Analysis (GSEA) software. Additionally, further bioinformatic analysis showed that Aldehyde dehydrogenase-2 (Aldh2) involved in the progression of renal fibrosis by mediating glycolysis pathway. Then real-time PCR and western blotting were performed to validate the expression of Aldh2 in kidney tissue after three different etiologies that caused renal fibrosis. Basically consistent with our bioinformatics results, our experiment showed that the expression of Aldh2 is the most significantly decreased in the UUO model, followed by ischemia-reperfusion injury (IRI) model and finally the cisplatin-induced model. Thus, Aldh2 can act as a common potential genetic target for different renal fibrosis diseases.}
}
@article{VAHDATNEJAD2019321,
title = {Context-aware computing for mobile crowd sensing: A survey},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {321-332},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.052},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18329583},
author = {Hamed Vahdat-Nejad and Elham Asani and Zohreh Mahmoodian and Mohammad Hossein Mohseni},
keywords = {Mobile crowd sensing, Context-awareness, Functionalities, Survey},
abstract = {Today, the distribution of people with smart mobile devices has provided the opportunity for mobile crowd sensing. Several mobile crowd sensing systems have led to the collection of valuable information with a low infrastructural investment. These types of information have been used in context-aware systems to provide high-level services. In this paper, a comprehensive reference framework is proposed to investigate context-aware mobile crowd sensing systems from three viewpoints of concepts, context-awareness, and functionalities. Each of these aspects has one or more parameters, which investigate and classify the existing works. To this end, the paper characterizes domain and cooperation type, context-awareness, incentive mechanisms, data sharing, local analysis and global aggregation of mobile crowd sensing systems. The aim is to thoroughly review the existing works, foster the dissemination of state-of-the-art research, and present future research directions.}
}
@incollection{NISHTALA2019391,
title = {Sources of Data Used in Pharmacoepidemiology and Pharmacovigilance},
editor = {Zaheer-Ud-Din Babar},
booktitle = {Encyclopedia of Pharmacy Practice and Clinical Pharmacy},
publisher = {Elsevier},
address = {Oxford},
pages = {391-399},
year = {2019},
isbn = {978-0-12-812736-0},
doi = {https://doi.org/10.1016/B978-0-12-812735-3.00206-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128127353002065},
author = {Prasad S. Nishtala and Sharmin S. Bala},
keywords = {Pharmacoepidemiology, Pharmacovigilance, Data sources, Databases, Registry, Medical records},
abstract = {The utilization of healthcare information databases in pharmacoepidemiology has expanded in the last two decades, and several larger data resources have become available for use. Selection of the appropriate database to address a specific research question is challenging at times. This chapter delineates the current primary and secondary sources of data used in pharmacoepidemiology and pharmacovigilance, along with a summary of their relative advantages and disadvantages.}
}
@article{ISMAGILOVA201988,
title = {Smart cities: Advances in research—An information systems perspective},
journal = {International Journal of Information Management},
volume = {47},
pages = {88-100},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218312738},
author = {Elvira Ismagilova and Laurie Hughes and Yogesh K. Dwivedi and K. Ravi Raman},
keywords = {Information systems, Smart cities, Literature review, Sustainable development goals},
abstract = {Smart cities employ information and communication technologies to improve: the quality of life for its citizens, the local economy, transport, traffic management, environment, and interaction with government. Due to the relevance of smart cities (also referred using other related terms such as Digital City, Information City, Intelligent City, Knowledge-based City, Ubiquitous City, Wired City) to various stakeholders and the benefits and challenges associated with its implementation, the concept of smart cities has attracted significant attention from researchers within multiple fields, including information systems. This study provides a valuable synthesis of the relevant literature by analysing and discussing the key findings from existing research on issues related to smart cities from an Information Systems perspective. The research analysed and discussed in this study focuses on number of aspects of smart cities: smart mobility, smart living, smart environment, smart citizens, smart government, and smart architecture as well as related technologies and concepts. The discussion also focusses on the alignment of smart cities with the UN sustainable development goals. This comprehensive review offers critical insight to the key underlying research themes within smart cities, highlighting the limitations of current developments and potential future directions.}
}
@incollection{NUR201993,
title = {8 - The benefits of accessing transport data to support intelligent mobility},
editor = {Pierluigi Coppola and Domokos Esztergár-Kiss},
booktitle = {Autonomous Vehicles and Future Mobility},
publisher = {Elsevier},
pages = {93-111},
year = {2019},
isbn = {978-0-12-817696-2},
doi = {https://doi.org/10.1016/B978-0-12-817696-2.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128176962000081},
author = {Khalid Nur and Tim Gammons},
keywords = {Intelligent mobility (IM), Data platforms, Business models, Revenue models, oneTRANSPORT, Data value chain, Direct benefits, Indirect benefits},
abstract = {The delivery of intelligent mobility services and the realization of its benefits require access to multi-sourced, multi-modal, and multi-regional transport data. While many major cities around the world are providing access to such data through cloud-based data platforms, challenges face the opening of transport data outside the boundaries of such cities. This chapter identifies and evaluates the benefits, direct (cashable) and indirect (economic, environmental and social), of accessing transport data through data platforms from a context that goes beyond large “smart” cities to include other urban and rural areas and support a multi-regional approach. Some of the identified benefits are demonstrated through the oneTRANSPORT initiative, which developed a data platform and market place for transport-related data in the United Kingdom with the aim of delivering Intelligent Mobility within and beyond large cities through an economical and scalable approach to accessing multi-regional transport data.}
}
@article{PERAKSLIS2019e8,
title = {Is health-care data the new blood?},
journal = {The Lancet Digital Health},
volume = {1},
number = {1},
pages = {e8-e9},
year = {2019},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(19)30001-9},
url = {https://www.sciencedirect.com/science/article/pii/S2589750019300019},
author = {Eric Perakslis and Andrea Coravos}
}
@article{MANCINI201953,
title = {Agrochemicals in the wild: Identifying links between pesticide use and declines of nontarget organisms},
journal = {Current Opinion in Environmental Science & Health},
volume = {11},
pages = {53-58},
year = {2019},
note = {Environmental Pollution: Wildlife},
issn = {2468-5844},
doi = {https://doi.org/10.1016/j.coesh.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2468584419300297},
author = {Francesca Mancini and Ben A. Woodcock and Nick J.B. Isaac},
keywords = {Occupancy-detection models, Pollinators, Sustainable agriculture, Biological recording, Pesticide surveillance},
abstract = {Agricultural pesticides are a key component of the toolbox of most agricultural systems and are likely to continue to play a role in meeting the challenge of feeding a growing global population. However, pesticide use has well documented and often significant consequences for populations of native wildlife. Although rigorous, regulatory processes for the approval of new chemicals for agronomic use do have limitations which may fail to identify real-world negative effects of products. Here, we describe a possible approach to complement the existing regulatory process, which is to combine long-term and national-scale data sets on native wildlife with pesticide use data to understand long-term and large-scale impacts of agrochemicals on wildlife populations.}
}
@article{SHAHID2019638,
title = {Computational intelligence techniques for medical diagnosis and prognosis: Problems and current developments},
journal = {Biocybernetics and Biomedical Engineering},
volume = {39},
number = {3},
pages = {638-672},
year = {2019},
issn = {0208-5216},
doi = {https://doi.org/10.1016/j.bbe.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0208521619300452},
author = {Afzal Hussain Shahid and M.P. Singh},
keywords = {Computational intelligence, Disease diagnosis, Prediction, Detection, Uncertainty, Medical data},
abstract = {Diagnosis, being the first step in medical practice, is very crucial for clinical decision making. This paper investigates state-of-the-art computational intelligence (CI) techniques applied in the field of medical diagnosis and prognosis. The paper presents the performance of these techniques in diagnosing different diseases along with the detailed description of the data used. This paper includes basic as well as hybrid CI techniques that have been used in recent years so as to know the current trends in medical diagnosis domain. The paper presents the merits and demerits of different techniques in general as well as application specific context. This paper discusses some critical issues related to the medical diagnosis and prognosis such as uncertainties in the medical domain, problems in the medical data especially dealing with time-stamped (temporal) data, and knowledge acquisition. Moreover, this paper also discusses the features of good CI techniques in medical diagnosis. Overall, this review provides new insight for future research requirements in the medical diagnosis domain.}
}
@article{2019iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {159},
pages = {iii-xvii},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(19)31644-8},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919316448}
}
@article{SHEN201948,
title = {Construction of a drought monitoring model using deep learning based on multi-source remote sensing data},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {79},
pages = {48-57},
year = {2019},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0303243418307803},
author = {Runping Shen and Anqi Huang and Bolun Li and Jia Guo},
keywords = {Drought, Remote sensing, Deep learning},
abstract = {Drought is a popular scientific issue in global climate change research. Accurate monitoring of drought has important implications for the sustainable development of regional agriculture in the context of increasingly complex global climate change. Deep learning is a widely used technique in the field of artificial intelligence. However, ongoing on drought monitoring using deep learning is relatively scarce. In this paper, the various hazard factors in drought development were comprehensively considered based on satellite data including Moderate Resolution Imaging Spectroradiometer (MODIS) and tropical rainfall measuring mission (TRMM) as multi-source remote sensing data. By using the deep learning technique, a comprehensive drought monitoring model was constructed and tested in Henan Province of China as an example. The results showed that the comprehensive drought model has good applicability in the monitoring of meteorological drought and agricultural drought. There was a significant positive correlation between the drought indicators of the model output and the comprehensive meteorological drought index (CI) measured at the site scale. The consistency rate of the drought grade of the two models was 85.6% and 79.8% for the training set and the test set, respectively. The correlation coefficient between the drought index of the model and the standard precipitation evapotranspiration index (SPEI) was between 0.772 and 0.910 (P < 0.01), which indicated a strong level of significance. The correlation coefficient between the drought index of the model and the soil relative moisture at a 10 cm depth was greater than 0.550 (P < 0.01), and there was a good correlation between them. This study provides a new method for the comprehensive assessment of regional drought.}
}
@article{LENSU201953,
title = {Big maritime data for the Baltic Sea with a focus on the winter navigation system},
journal = {Marine Policy},
volume = {104},
pages = {53-65},
year = {2019},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2019.02.038},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X18305335},
author = {Mikko Lensu and Floris Goerlandt},
keywords = {AIS data, Databases, Data integration, Marine information, Marine models, Ice conditions, Ice navigation, Baltic Sea},
abstract = {The automatic identification system (AIS) has become a key element in maritime domains of inquiry and the number of related articles has increased rapidly. The systematic integration of AIS data with other datatypes has received less attention and has mostly resulted in application-specific datasets that are small relative to the available AIS data. This work presents an accumulating multi-purpose database for the northern Baltic Sea that combines nine years of AIS data with marine environmental data. The main application is winter navigation research, for which purpose the environmental data is from ice charts and ice drift models. The AIS data is from terrestrial stations and amounts to 6 billion messages. It has a full update rate which is also required for the analysis of ice navigation as this involves close encounters, icebreaker assistance, convoy operations, and rapid speed changes. To identify and study such traffic features, distances between ships that are close to each other are included in the database. Application examples are given for spatial traffic statistics, reduction of ship speed with increasing ice thickness, and for icebreaker assistance.}
}
@article{CONVERTINO2019258,
title = {Information-theoretic portfolio decision model for optimal flood management},
journal = {Environmental Modelling & Software},
volume = {119},
pages = {258-274},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218312362},
author = {Matteo Convertino and Antonio Annis and Fernando Nardi},
keywords = {River basin management, Floods, Systemic risk, MaxEnt, Portfolio decision model, MCDA},
abstract = {The increasing impact of flooding urges more effective flood management strategies to guarantee sustainable ecosystem development. Recent catastrophes underline the importance of avoiding local flood management, but characterizing large scale basin wide approaches for systemic flood risk management. Here we introduce an information-theoretic Portfolio Decision Model (iPDM) for the optimization of a systemic ecosystem value at the basin scale by evaluating all potential flood risk mitigation plans. iPDM calculates the ecosystem value predicted by all feasible combinations of flood control structures (FCS) considering environmental, social and economical asset criteria. A multi-criteria decision analytical model evaluates the benefits of all FCS portfolios at the basin scale weighted by stakeholder preferences for assets’ criteria as ecosystem services. The risk model is based on a maximum entropy model (MaxEnt) that predicts the flood susceptibility, the risk of floods based on the exceedance probability distribution, and its most important drivers. Information theoretic global sensitivity and uncertainty analysis is used to select the simplest and most accurate model based on a flood return period. A stochastic optimization algorithm optimizes the ecosystem value constrained to the budget available and provides Pareto frontiers of optimal FCS plans for any budget level. Pareto optimal solutions maximize FCS diversity and minimize the criticality of floods manifested by the scaling exponent of the Pareto distribution of flood size that links management and hydrogeomorphological patterns. The proposed model is tested on the 17,000 km2 Tiber river basin in Italy. iPDM allows stakeholders to identify optimal FCS plans in river basins for a comprehensive evaluation of flood effects under future ecosystem trajectories.}
}
@incollection{TZOREFBRILL201979,
title = {Chapter Two - Advances in Combinatorial Testing},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {112},
pages = {79-134},
year = {2019},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300542},
author = {Rachel Tzoref-Brill},
keywords = {Combinatorial testing, Combinatorial interaction testing, Combinatorial test design, Interaction coverage, Combinatorial modeling, Covering arrays},
abstract = {Since their introduction into software testing in the mid-1980s, combinatorial methods for test design gathered popularity as a testing best practice and as a prominent software testing research area. This chapter reviews recent advances in combinatorial testing, with special focus on the research since 2011. It provides a brief background on the theory behind combinatorial testing and on its use in practice. Requirements from industry usage have led to advances in various areas examined in this chapter, including constraints handling in combinatorial algorithms, support for the combinatorial modeling process, and studies on metrics to support the effectiveness of combinatorial testing. We also highlight recent case studies describing novel use cases for test and field quality improvement in the context of system test, and for optimization of test data. Finally, we examine recent developments in advanced topics such as utilization of existing tests, test case prioritization, fault localization, and evolution of combinatorial models.}
}
@incollection{SARKHEYLI2019269,
title = {Chapter 19 - Smart Megaprojects in Smart Cities, Dimensions, and Challenges},
editor = {Danda B. Rawat and Kayhan Zrar Ghafoor},
booktitle = {Smart Cities Cybersecurity and Privacy},
publisher = {Elsevier},
pages = {269-277},
year = {2019},
isbn = {978-0-12-815032-0},
doi = {https://doi.org/10.1016/B978-0-12-815032-0.00019-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128150320000196},
author = {Azadeh Sarkheyli and Elnaz Sarkheyli},
keywords = {Smart city, Megaproject, Sustainable development, Information management, Information security},
abstract = {Smart cities providing a large amount of data and information in different dimensions make the complicated process of impact assessment and management of megaprojects feasible. The management and control of megaprojects as costly, long-term, and large-scale developments are critical to their success. In addition, they can facilitate data management in smart cities as they usually use new and innovative technologies, and can provide the integrable data. This chapter tries to propose a framework for megaprojects’ management in smart cities via reviewing the dimensions and the existing data categories and domains in smart cities to enable the megaprojects to smartly react to the risks; smartly manage the energy, service and space usage; and smartly control their footprint. Thus, the concept of smart megaprojects is introduced, and the related challenges in safety and security are discussed.}
}
@article{TANDON2019A1,
title = {Machine learning in psychiatry- standards and guidelines},
journal = {Asian Journal of Psychiatry},
volume = {44},
pages = {A1-A4},
year = {2019},
issn = {1876-2018},
doi = {https://doi.org/10.1016/j.ajp.2019.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S1876201819308810},
author = {Neeraj Tandon and Rajiv Tandon}
}
@incollection{BAYLEY2019121,
title = {Chapter 6 - New Advances in Benthic Monitoring Technology and Methodology},
editor = {Charles Sheppard},
booktitle = {World Seas: an Environmental Evaluation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {121-132},
year = {2019},
isbn = {978-0-12-805052-1},
doi = {https://doi.org/10.1016/B978-0-12-805052-1.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128050521000061},
author = {Daniel T.I. Bayley and Andrew O.M. Mogg},
keywords = {Benthic ecology, Ecosystem management, Methods, Monitoring, Technological advances, Structural modeling},
abstract = {The last few decades have seen dramatic increases in the abundance and variety of data that can be collected within benthic marine environments to monitor species and ecosystems. These developments are now allowing ecologists to view their study systems in ever greater detail and over scales ranging from microscopic to global. Here we give an overview of current widely used techniques, touch on a number of recent advances in both technologies and methodologies, and focus on photogrammetry. We recommend that greater synergies, standardization, and automation of a range of these methods could further improve many monitoring programs’ effectiveness.}
}
@article{CORDEIRO2019751,
title = {Factors associated with occupational and non-occupational viral hepatitis infections in Brazil between 2007–2014},
journal = {Annals of Hepatology},
volume = {18},
number = {5},
pages = {751-756},
year = {2019},
issn = {1665-2681},
doi = {https://doi.org/10.1016/j.aohep.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1665268119302108},
author = {Técia Maria S.C. Cordeiro and Raymundo P. {Ferreira Filho} and Argemiro {D’Oliveira Júnior}},
keywords = {Communicable diseases, Epidemiology, Disease notification, Work},
abstract = {Introduction and Objectives
Viral hepatitis is an endemic and epidemic disease of relevance in public health. This study estimated the frequency of viral hepatitis by occupational and non-occupational infections and analyzed the factors associated with case notifications in Brazil from 2007 to 2014.
Material and methods
This was an exploratory epidemiological study using the Notifiable Diseases Information System database. Descriptive and multivariate analyses were performed.
Results
The frequency of viral hepatitis by occupational infections was 0.7%, of which 1.3% were due to hepatitis A virus (HAV), 45.1% hepatitis B virus (HBV), and 45.3% hepatitis C virus (HCV). There was a significant association of the disease with female sex [AOR=1.31; P=0.048], schooling [AOR=1.71; P<0.001], occupation [AOR=2.74; P<0.001], previous contact with an HBV or HCV-infected patient [AOR=5.77; P<0.001], exposure to accidents with biological materials [AOR=99.82; P<0.001], and hepatitis B vaccination [AOR=0.73; P=0.033].
Conclusion
While there was a low frequency of viral hepatitis by occupational infections in Brazil from 2007 to 2014, these findings might be underreported and have been associated with individual and occupational characteristics. This reinforces the need for the adoption of prevention strategies in the workplace and for completeness of case notifications.}
}
@article{SALMINEN2019203,
title = {Machine learning approach to auto-tagging online content for content marketing efficiency: A comparative analysis between methods and content type},
journal = {Journal of Business Research},
volume = {101},
pages = {203-217},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319302607},
author = {Joni Salminen and Vignesh Yoganathan and Juan Corporan and Bernard J. Jansen and Soon-Gyo Jung},
keywords = {Machine learning, Auto-tagging, Web content, Content marketing, Neural network, Digital marketing},
abstract = {As complex data becomes the norm, greater understanding of machine learning (ML) applications is needed for content marketers. Unstructured data, scattered across platforms in multiple forms, impedes performance and user experience. Automated classification offers a solution to this. We compare three state-of-the-art ML techniques for multilabel classification - Random Forest, K-Nearest Neighbor, and Neural Network - to automatically tag and classify online news articles. Neural Network performs the best, yielding an F1 Score of 70% and provides satisfactory cross-platform applicability on the same organisation's YouTube content. The developed model can automatically label 99.6% of the unlabelled website and 96.1% of the unlabelled YouTube content. Thus, we contribute to marketing literature via comparative evaluation of ML models for multilabel content classification, and cross-channel validation for a different type of content. Results suggest that organisations may optimise ML to auto-tag content across various platforms, opening avenues for aggregated analyses of content performance.}
}
@article{HULL2019670,
title = {Drinking Water Microbiome Project: Is it Time?},
journal = {Trends in Microbiology},
volume = {27},
number = {8},
pages = {670-677},
year = {2019},
issn = {0966-842X},
doi = {https://doi.org/10.1016/j.tim.2019.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0966842X19300757},
author = {Natalie M. Hull and Fangqiong Ling and Ameet J. Pinto and Mads Albertsen and H. Grace Jang and Pei-Ying Hong and Konstantinos T. Konstantinidis and Mark LeChevallier and Rita R. Colwell and Wen-Tso Liu},
keywords = {microbiome, meta-omics, drinking water, microbial ecology},
abstract = {Now is an opportune time to foster collaborations across sectors and geographical boundaries to enable development of best practices for drinking water (DW) microbiome research, focusing on accuracy and reproducibility of meta-omic techniques (while learning from past microbiome projects). A large-scale coordinated effort that builds on this foundation will enable the urgently needed comprehensive spatiotemporal understanding and control of DW microbiomes by engineering interventions to protect public health. This opinion paper highlights the need to initiate and conduct a large-scale coordinated DW microbiome project by addressing key knowledge gaps and recommends a roadmap for this effort.}
}
@article{BOOKHEIMER2019335,
title = {The Lifespan Human Connectome Project in Aging: An overview},
journal = {NeuroImage},
volume = {185},
pages = {335-348},
year = {2019},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918319682},
author = {Susan Y. Bookheimer and David H. Salat and Melissa Terpstra and Beau M. Ances and Deanna M. Barch and Randy L. Buckner and Gregory C. Burgess and Sandra W. Curtiss and Mirella Diaz-Santos and Jennifer Stine Elam and Bruce Fischl and Douglas N. Greve and Hannah A. Hagy and Michael P. Harms and Olivia M. Hatch and Trey Hedden and Cynthia Hodge and Kevin C. Japardi and Taylor P. Kuhn and Timothy K. Ly and Stephen M. Smith and Leah H. Somerville and Kâmil Uğurbil and Andre {van der Kouwe} and David {Van Essen} and Roger P. Woods and Essa Yacoub},
keywords = {Neuroimaging, Brain, MRI, Connectivity, Connectomics, fMRI, Diffusion imaging, Morphometry, Functional connectivity},
abstract = {The original Human Connectome Project yielded a rich data set on structural and functional connectivity in a large sample of healthy young adults using improved methods of data acquisition, analysis, and sharing. More recent efforts are extending this approach to include infants, children, older adults, and brain disorders. This paper introduces and describes the Human Connectome Project in Aging (HCP-A), which is currently recruiting 1200 + healthy adults aged 36 to 100+, with a subset of 600 + participants returning for longitudinal assessment. Four acquisition sites using matched Siemens Prisma 3T MRI scanners with centralized quality control and data analysis are enrolling participants. Data are acquired across multimodal imaging and behavioral domains with a focus on factors known to be altered in advanced aging. MRI acquisitions include structural (whole brain and high resolution hippocampal) plus multiband resting state functional (rfMRI), task fMRI (tfMRI), diffusion MRI (dMRI), and arterial spin labeling (ASL). Behavioral characterization includes cognitive (such as processing speed and episodic memory), psychiatric, metabolic, and socioeconomic measures as well as assessment of systemic health (with a focus on menopause via hormonal assays). This dataset will provide a unique resource for examining how brain organization and connectivity changes across typical aging, and how these differences relate to key characteristics of aging including alterations in hormonal status and declining memory and general cognition. A primary goal of the HCP-A is to make these data freely available to the scientific community, supported by the Connectome Coordination Facility (CCF) platform for data quality assurance, preprocessing and basic analysis, and shared via the NIMH Data Archive (NDA). Here we provide the rationale for our study design and sufficient details of the resource for scientists to plan future analyses of these data. A companion paper describes the related Human Connectome Project in Development (HCP-D, Somerville et al., 2018), and the image acquisition protocol common to both studies (Harms et al., 2018).}
}
@article{FAROOQUE2019882,
title = {Circular supply chain management: A definition and structured literature review},
journal = {Journal of Cleaner Production},
volume = {228},
pages = {882-900},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.04.303},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619314003},
author = {Muhammad Farooque and Abraham Zhang and Matthias Thürer and Ting Qu and Donald Huisingh},
keywords = {Supply chain management, Circular economy, Circular supply chain, Circular supply chain management, Sustainable supply chain, Sustainability},
abstract = {Circular economy is increasingly recognized as a better alternative to the dominant linear (take, make, and dispose) economic model. Circular Supply Chain Management (CSCM), which integrates the philosophy of the circular economy into supply chain management, offers a new and compelling perspective to the supply chain sustainability domain. Consequently, there is increasing research interest. However, a review of the extant literature shows that a comprehensive integrated view of CSCM is still absent in the extant literature. This prohibits a clear distinction compared to other supply chain sustainability concepts and hinders further progress of the field. In response, this research first classifies various terminologies related to supply chain sustainability and conceptualizes a unifying definition of CSCM. Using this definition as a base, it then conducts a structured literature review of 261 research articles on the current state of CSCM research. Based on the review results, the researchers call for further studies in the following directions that are important but received little or no attention: design for circularity, procurement and CSCM, biodegradable packaging, circular supply chain collaboration and coordination, drivers and barriers of CSCM, circular consumption, product liabilities and producer's responsibility, and technologies and CSCM.}
}
@article{WANG2019103,
title = {Smart solutions shape for sustainable low-carbon future: A review on smart cities and industrial parks in China},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {103-117},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518312174},
author = {Yuanping Wang and Hong Ren and Liang Dong and Hung-Suck Park and Yuepeng Zhang and Yanwei Xu},
keywords = {Smart city, Smart industrial park, Low-carbon society, Demand side management, China},
abstract = {To promote sustainable urban development and green industrial process are critical solutions for sustainable and low-carbon society transition in China, considering the significant environmental impacts derived from the industrialization and surging urbanization. Under this background, China adopts top-down programs on smart cities and smart industrial parks to forward the above efforts. While practices and lessons from these programs will be valuable to enlighten other regions and practitioners, to date, rather few studies have paid attentions to this issue. Particularly, the emerging smart technologies strongly support the practice, via offering smart solutions like better renewable energy projection, low-carbon life styles transformation, as well as energy planning and management. However, there has been a lack of discussing their future role in-depth. With this circumstance, this paper conducts an integrated and in-depth review on China's promotion on smart cities and smart industrial parks. In detail, the national pilots, key technical innovations, incentives and policies framework, as well as spatial features were discussed in-depth. Particularly, we further explored how the smart solutions can contribute to better decision making on low-carbon urban and industrial system planning. Finally, we highlighted policy recommendations targeting on future smart cities and industrial parks promotion, focusing on the perspectives of technological and social system innovations, innovative decision support tools, and smart management framework. Our results expect to offer critical enlightenments for policy makers to address future concerns on smart cities and industrial parks promotion and management.}
}
@article{WANG2019103668,
title = {Impacts of spatial clustering of urban land cover on land surface temperature across Köppen climate zones in the contiguous United States},
journal = {Landscape and Urban Planning},
volume = {192},
pages = {103668},
year = {2019},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2019.103668},
url = {https://www.sciencedirect.com/science/article/pii/S0169204619301033},
author = {Chuyuan Wang and Yubin Li and Soe W. Myint and Qunshan Zhao and Elizabeth A. Wentz},
keywords = {Spatial clustering, Moran’s , Urban land cover, Land surface temperature, Köppen climate classification},
abstract = {This study examines the effects of spatial clustering of urban land cover types on land surface temperature (LST). The potential impact of the background regional climate is also taken into consideration. To study this relationship, multiple cities, each representing a major Köppen climate region in the U.S., namely Portland, Los Angeles, Chicago, Denver, Kansas City, Orlando, and Phoenix, were selected. Urban land cover types were derived from the 2011 National Land Cover Database (NLCD); summer mean LST from 2011 was calculated using the Moderate Resolution Imaging Spectroradiometer (MODIS) LST products. Spatial clustering was quantified using Moran’s I, and was analyzed against LST using correlation and multivariate regression analyses. The results indicate that in most climate regions, clustered impervious surfaces can elevate LST for both daytime and nighttime. The cooling effect of clustered vegetation cover was only found significant in regions with dry and warm summers, such as in Phoenix and Portland. Clustered water bodies have a strong cooling effect during the daytime but have a warming effect at night, except for cities such as Los Angeles and Phoenix, which have scant large water bodies. Furthermore, policy recommendations were put forward to suggest that reducing the spatial clustering of impervious surfaces, having more spatially clustered greenspaces, and having spatially dispersed water bodies with clustered greenspaces nearby are potential strategies to reduce urban warming in most cities in the contiguous U.S.}
}
@article{LINDSTROM2019880,
title = {Towards intelligent and sustainable production systems with a zero-defect manufacturing approach in an Industry4.0 context},
journal = {Procedia CIRP},
volume = {81},
pages = {880-885},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.218},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119305232},
author = {John Lindström and Erik Lejon and Petter Kyösti and Massimo Mecella and Dominic Heutelbeck and Matthias Hemmje and Mikael Sjödahl and Wolfgang Birk and Bengt Gunnarsson},
keywords = {continuous quality control, Industry4.0, intelligent, online predictive maintenance, production, sustainable, zero-defect manufacturing},
abstract = {The paper addresses intelligent and sustainable production achieved through combination and integration of online predictive maintenance, monitoring of process parameters and continuous quality control of both input materials and output from the process. This enables production systems, within both manufacturing and process industries, to move towards zero-defect manufacturing. Such a zero-defect manufacturing approach allows for earlier identification of problems or issues, which will or already negatively affect the output. The paper outlines the first part of the second cycle of an action research effort at Gestamp HardTech AB in Sweden, whose objective is to keep its position as a world-leading provider of press-hardened vehicle parts. In order to fully implement the zero-defect manufacturing approach, 4-6 action research cycles are expected to be needed in order to iteratively refine the approach. During the first cycle, various methods and solutions for some of the individual issues/problems have been conceptualized, realized and initially tested. The selected design criteria for the action research efforts were: simplicity, low cost, robustness, high-quality output and future-proofing. The result from the research in the second cycle so far is an action plan for the technical change and a set of challenges/problems which need additional investigation.}
}
@article{LIAO2019168,
title = {A Semantic Concast service for data discovery, aggregation and processing on NDN},
journal = {Journal of Network and Computer Applications},
volume = {125},
pages = {168-178},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303394},
author = {Zhuhua Liao and Zengde Teng and Jian Zhang and Yizhi Liu and Hao Xiao and Aiping Yi},
keywords = {Named Data Networking, Semantic Concast, Data discovery, Data aggregation, Distributed processing},
abstract = {Offering a flexible paradigm for intelligently discovering, aggregating and processing big distributed data is a crucial requirement in large content-centric Internet. However, the major hindrances to this paradigm are network's dynamic feature, traffic balance, wired forwarding and the absence of cooperation between communications and computations. In this paper, we present a scalable Semantic Concast service on Named Data Networking (NDN) being considered as a promising paradigm for the future Internet. The service enables cooperation between data discovering, aggregating and processing among intermediate nodes for a user's Interest that contained a hierarchical name and semantic constraints. Specifically, multiple types and strategies of data aggregation and processing for combining and processing the positive data and suppressing the negative, futile data, as well as a determination of response completeness are introduced for enhancing relevant results recall and sharing. The experimentation demonstrated the Semantic Concast service can effectively improve service quality, reduce network traffic and shorten response time.}
}
@article{HEATON201932,
title = {A conceptual framework for the alignment of infrastructure assets to citizen requirements within a Smart Cities framework},
journal = {Cities},
volume = {90},
pages = {32-41},
year = {2019},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2019.01.041},
url = {https://www.sciencedirect.com/science/article/pii/S0264275118304384},
author = {James Heaton and Ajith Kumar Parlikad},
keywords = {Smart Cities, Asset management, Building information modelling, BIM, Smart Cities framework, Citizen requirements, Smart assets},
abstract = {With the predicted world population growth of 83 million people per year (increasing 1.09% year on year) compounded with a strong trend for migration to urban centres, there is a developing interest by academics, industry and government to the digitalisation of the built environment and its potential impact on private enterprises, public services and the broader context of society. The governments around the world and others are aiming to guide and standardise this process by developing an array of standards to support this digitalisation, most notably on Building Information Modelling (BIM) and Smart Cities Framework. Furthermore, the advancement of the Internet of Things (IoT) is creating a highly flexible, dynamic and accessible platform for the exchange capture and of information. There is a risk that this information on the built environment is quickly becoming unmanageable, and the value of that information is quickly becoming lost. This paper presents a smart asset alignment framework that creates an alignment between the information captured at the infrastructure asset level and citizen requirements within a Smart City framework. The framework contributes to the debate on designing and developing Smart City solutions in a way that will deliver value to the citizens.}
}
@article{2019iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {151},
pages = {iii-xii},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(19)30705-7},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307057}
}
@article{THOMAS2019101401,
title = {Data management maturity assessment of public sector agencies},
journal = {Government Information Quarterly},
volume = {36},
number = {4},
pages = {101401},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2019.101401},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X19300656},
author = {Manoj A. Thomas and Joseph Cipolla and Bob Lambert and Lemuria Carter},
keywords = {Data management maturity, Stage model, DMM model, DMM index, DMMI, Government agencies, Public sector},
abstract = {To determine how critical data assets are conceptualized and managed in the public sector, we conducted a large-scale empirical study at 15 government agencies. We use the Data Management Maturity (DMM) reference model framework to conduct a systematic multi-level analysis (inter-agency, intra-agency, and cross-case analysis). To aid the comparative assessment of multiple independent agencies, we propose and test the DMM Index. The study not only examines the maturity of data management practices in government agencies, but also provides guidance on how an enterprise-wide, systematic assessment may be conducted. The approach presented in the paper can be replicated at other large government entities and private conglomerates. Public and private sector agencies may apply the approach to develop custom roadmaps for data management improvements that align with the organization's business goals.}
}
@article{GENITSARIDI2019353,
title = {Standardised profiling for tinnitus research: The European School for Interdisciplinary Tinnitus Research Screening Questionnaire (ESIT-SQ)},
journal = {Hearing Research},
volume = {377},
pages = {353-359},
year = {2019},
issn = {0378-5955},
doi = {https://doi.org/10.1016/j.heares.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0378595518304684},
author = {Eleni Genitsaridi and Marta Partyka and Silvano Gallus and Jose A. Lopez-Escamez and Martin Schecklmann and Marzena Mielczarek and Natalia Trpchevska and Jose L. Santacruz and Stefan Schoisswohl and Constanze Riha and Matheus Lourenco and Roshni Biswas and Nuwan Liyanage and Christopher R. Cederroth and Patricia Perez-Carpena and Jana Devos and Thomas Fuller and Niklas K. Edvall and Matilda Prada Hellberg and Alessia D'Antonio and Stefania Gerevini and Magdalena Sereda and Andreas Rein and Theodore Kypraios and Derek J. Hoare and Alain Londero and Rüdiger Pryss and Winfried Schlee and Deborah A. Hall},
keywords = {Heterogeneity, Classification, Data collection, Self report, Translations},
abstract = {Background
The heterogeneity of tinnitus is substantial. Its numerous pathophysiological mechanisms and clinical manifestations have hampered fundamental and treatment research significantly. A decade ago, the Tinnitus Research Initiative introduced the Tinnitus Sample Case History Questionnaire, a case history instrument for standardised collection of information about the characteristics of the tinnitus patient. Since then, a number of studies have been published which characterise individuals and groups using data collected with this questionnaire. However, its use has been restricted to a clinical setting and to the evaluation of people with tinnitus only. In addition, it is limited in the ability to capture relevant comorbidities and evaluate their temporal relationship with tinnitus.
Method
Here we present a new case history instrument which is comprehensive in scope and can be answered by people with and without tinnitus alike. This ‘European School for Interdisciplinary Tinnitus Research Screening Questionnaire’ (ESIT-SQ) was developed with specific attention to questions about potential risk factors for tinnitus (including demographics, lifestyle, general medical and otological histories), and tinnitus characteristics (including perceptual characteristics, modulating factors, and associations with co-existing conditions). It was first developed in English, then translated into Dutch, German, Italian, Polish, Spanish, and Swedish, thus having broad applicability and supporting international collaboration.
Conclusions
With respect to better understanding tinnitus profiles, we anticipate the ESIT-SQ to be a starting point for comprehensive multi-variate analyses of tinnitus. Data collected with the ESIT-SQ can allow establishment of patterns that distinguish tinnitus from non-tinnitus, and definition of common sets of tinnitus characteristics which might be indicated by the presence of otological or comorbid systemic diseases for which tinnitus is a known symptom.}
}
@article{CHAIR2019271,
title = {Towards A Social Media-Based Framework for Disaster Communication},
journal = {Procedia Computer Science},
volume = {164},
pages = {271-278},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.183},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919322227},
author = {Sarra Chair and Malika Charrad and Narjes Bellamine {Ben Saoud}},
keywords = {Social media, Disaster management, Floods, Recommendation, Social media analysis},
abstract = {During disasters, social media platforms such as Twitter, Facebook, and YouTube are widely used by people to share information, opinions, experience, and request for urgent needs. These platforms provide tremendous opportunities to detect disaster situations and give insight into their severity. Since disaster happens suddenly, people face a significant challenge to find credible information and take suitable reactions. The focus here is to reach the affected persons with personalized recommendations to prevent them and save their lives. In this paper, we propose a framework of a social media-based platform that aims to detect emergent disaster and improve communication with citizens before, during and after a disaster by generating personalized recommendations in a timely manner.}
}
@article{CHAKRABORTY201981,
title = {Data-driven parallelizable traffic incident detection using spatio-temporally denoised robust thresholds},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {105},
pages = {81-99},
year = {2019},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18314141},
author = {Pranamesh Chakraborty and Chinmay Hegde and Anuj Sharma},
keywords = {Freeway incident detection, Threshold denoising, Total variation, Bilateral filter, MapReduce},
abstract = {Automatic incident detection (AID) is crucial for reducing non-recurrent congestion caused by traffic incidents. In this paper, we propose a data-driven AID framework that can leverage large-scale historical traffic data along with the inherent topology of the traffic networks to obtain robust traffic patterns. Such traffic patterns can be compared with the real-time traffic data to detect traffic incidents in the road network. Our AID framework consists of two basic steps for traffic pattern estimation. First, we estimate a robust univariate speed threshold using historical traffic information from individual sensors. This step can be parallelized using MapReduce framework thereby making it feasible to implement the framework over large networks. Our study shows that such robust thresholds can improve incident detection performance significantly compared to traditional threshold determination. Second, we leverage the knowledge of the topology of the road network to construct threshold heatmaps and perform image denoising to obtain spatio-temporally denoised thresholds. We used two image denoising techniques, bilateral filtering and total variation for this purpose. Our study shows that overall AID performance can be improved significantly using bilateral filter denoising compared to the noisy thresholds or thresholds obtained using total variation denoising.}
}
@article{HO2019329,
title = {Governance of automated image analysis and artificial intelligence analytics in healthcare},
journal = {Clinical Radiology},
volume = {74},
number = {5},
pages = {329-337},
year = {2019},
issn = {0009-9260},
doi = {https://doi.org/10.1016/j.crad.2019.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0009926019301151},
author = {C.W.L. Ho and D. Soon and K. Caals and J. Kapur},
abstract = {The hype over artificial intelligence (AI) has spawned claims that clinicians (particularly radiologists) will become redundant. It is still moot as to whether AI will replace radiologists in day-to-day clinical practice, but more AI applications are expected to be incorporated into the workflows in the foreseeable future. These applications could produce significant ethical and legal issues in healthcare if they cause abrupt disruptions to its contextual integrity and relational dynamics. Sustaining trust and trustworthiness is a key goal of governance, which is necessary to promote collaboration among all stakeholders and to ensure the responsible development and implementation of AI in radiology and other areas of clinical work. In this paper, the nature of AI governance in biomedicine is discussed along with its limitations. It is argued that radiologists must assume a more active role in propelling medicine into the digital age. In this respect, professional responsibilities include inquiring into the clinical and social value of AI, alleviating deficiencies in technical knowledge in order to facilitate ethical evaluation, supporting the recognition, and removal of biases, engaging the “black box” obstacle, and brokering a new social contract on informational use and security. In essence, a much closer integration of ethics, laws, and good practices is needed to ensure that AI governance achieves its normative goals.}
}
@article{ROITSCH20192,
title = {Review: New sensors and data-driven approaches—A path to next generation phenomics},
journal = {Plant Science},
volume = {282},
pages = {2-10},
year = {2019},
note = {The 4th International Plant Phenotyping Symposium},
issn = {0168-9452},
doi = {https://doi.org/10.1016/j.plantsci.2019.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0168945217310506},
author = {Thomas Roitsch and Llorenç Cabrera-Bosquet and Antoine Fournier and Kioumars Ghamkhar and José Jiménez-Berni and Francisco Pinto and Eric S. Ober},
keywords = {Imaging, IPPN, Metadata, Next generation phenomics, Plant phenotyping, Sensor development, Trait value},
abstract = {At the 4th International Plant Phenotyping Symposium meeting of the International Plant Phenotyping Network (IPPN) in 2016 at CIMMYT in Mexico, a workshop was convened to consider ways forward with sensors for phenotyping. The increasing number of field applications provides new challenges and requires specialised solutions. There are many traits vital to plant growth and development that demand phenotyping approaches that are still at early stages of development or elude current capabilities. Further, there is growing interest in low-cost sensor solutions, and mobile platforms that can be transported to the experiments, rather than the experiment coming to the platform. Various types of sensors are required to address diverse needs with respect to targets, precision and ease of operation and readout. Converting data into knowledge, and ensuring that those data (and the appropriate metadata) are stored in such a way that they will be sensible and available to others now and for future analysis is also vital. Here we are proposing mechanisms for “next generation phenomics” based on our learning in the past decade, current practice and discussions at the IPPN Symposium, to encourage further thinking and collaboration by plant scientists, physicists and engineering experts.}
}
@article{YANG2019104071,
title = {A generic framework to analyse the spatiotemporal variations of water quality data on a catchment scale},
journal = {Environmental Modelling & Software},
volume = {122},
pages = {104071},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216305941},
author = {Qinli Yang and Miklas Scholz and Junming Shao and Guoqing Wang and Xiaofang Liu},
keywords = {Spatiotemporal analysis, Environmental data, Cluster analysis, Dynamic time warping},
abstract = {Most spatiotemporal studies treat spatial and temporal analysis separately. However, spatial and temporal changes occur simultaneously and are correlated. In this study, we propose a generic framework to simultaneously analyse the spatial and temporal variations of water quality on a catchment scale. Specifically, we analyse the heterogeneity of temporal evolution of water quality data among different sampling sites, and the heterogeneity of spatial distribution of water quality data over different sampling times, respectively, by integrating the techniques of normalized mutual information, dynamic time wrapping and cluster analysis. To bring deep insight into the spatiotemporal variations, inter-change and intra-change are further defined and distinguished, respectively. Taking the Fuxi River catchment as a case study, results indicate that the proposed framework is intuitive and efficient. Beyond this, the generic framework can be expanded for other catchments and various environmental data.}
}
@article{AHMAD20191187,
title = {Intelligent algorithms and standards for interoperability in Internet of Things},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {1187-1191},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18328516},
author = {Awais Ahmad and Salvatore Cuomo and Wei Wu and Gwanggil Jeon},
keywords = {Intelligent algorithms, Internet of Things, Semantic interoperability solutions, Monitoring performances and QoS, IoT implementations, Low cost interoperability in IoT},
abstract = {Research associated with intelligent algorithm and interoperability in internet of things became important topic over the next few years. The topic includes work on (i) infrastructures, platforms, architectures and designs supporting interoperability in IoT, (ii) environment at various levels (data, device, middleware, networking, and application service), (iii) intelligent semantic interoperability solutions, (iv) messaging protocols within IoT to enhance the interoperability of various interactive systems, (v) security- and privacy-aware IoT, monitoring performances and QoS, (vi) intelligent standards and algorithms for Interoperability between different IoT implementations, and (vii) survey on suitability of intelligent standards and algorithms for interoperability and heterogeneity in IoT infrastructures. The papers of this special issue address a variety of issues and concerns in interoperability in IoT: searching and processing IoT, implementing and modelling event and workflow systems, visualization modelling and simulation based on innovative list of solutions.}
}
@article{JIA2019111,
title = {Adopting Internet of Things for the development of smart buildings: A review of enabling technologies and applications},
journal = {Automation in Construction},
volume = {101},
pages = {111-126},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518307064},
author = {Mengda Jia and Ali Komeily and Yueren Wang and Ravi S. Srinivasan},
keywords = {Internet of Things (IoT), Smart buildings, Built environment, State of the art review, Application assessment},
abstract = {The 21st century is witnessing a fast-paced digital revolution. A significant trend is that cyber and physical environments are being unprecedentedly entangled with the emergence of Internet of Things (IoT). IoT has been widely immersed into various domains in the industry. Among those areas where IoT would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities, and moving towards sustainable development goals. So far, IoT itself has entered an ambiguous phase for industrial utilization, and there are limited number of studies focusing on the application of IoT in the building industry. Given the promising future impact of IoT technologies on buildings, and the increasing interests in interdisciplinary research among academics, this paper investigates the state-of-the-art projects and adoptions of IoT for the development of smart buildings within both academia and industry contexts. The wide-ranging IoT concepts are provided, covering the necessary breadth as well as relevant topic depth that directly relates to smart buildings. Current enabling technologies of IoT, especially those applied to buildings and related areas are summarized, which encompasses three different layers based on the conventional IoT architecture. Afterwards, several recent applications of IoT technologies on buildings towards the critical goals of smart buildings are selected and presented. Finally, the priorities and challenges of successful and seamless IoT integration for smart buildings are discussed. Besides, this paper discusses the future research questions to advance the implementation of IoT technologies in both building construction and operation phases. The paper argues that a mature adoption of IoT technologies in the building industry is not yet realized and, therefore, calls for more attention from researchers in the relevant fields from the application perspective.}
}
@article{GROSSMAN2019223,
title = {Data Lakes, Clouds, and Commons: A Review of Platforms for Analyzing and Sharing Genomic Data},
journal = {Trends in Genetics},
volume = {35},
number = {3},
pages = {223-234},
year = {2019},
issn = {0168-9525},
doi = {https://doi.org/10.1016/j.tig.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0168952518302257},
author = {Robert L. Grossman},
keywords = {data commons, data clouds, data sharing, cancer genomics clouds},
abstract = {Data commons collate data with cloud computing infrastructure and commonly used software services, tools, and applications to create biomedical resources for the large-scale management, analysis, harmonization, and sharing of biomedical data. Over the past few years, data commons have been used to analyze, harmonize, and share large-scale genomics datasets. Data ecosystems can be built by interoperating multiple data commons. It can be quite labor intensive to curate, import, and analyze the data in a data commons. Data lakes provide an alternative to data commons and simply provide access to data, with the data curation and analysis deferred until later and delegated to those that access the data. We review software platforms for managing, analyzing, and sharing genomic data, with an emphasis on data commons, but also cover data ecosystems and data lakes.}
}
@article{NAGY2019118464,
title = {Application of artificial neural networks for Process Analytical Technology-based dissolution testing},
journal = {International Journal of Pharmaceutics},
volume = {567},
pages = {118464},
year = {2019},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2019.118464},
url = {https://www.sciencedirect.com/science/article/pii/S0378517319305046},
author = {Brigitta Nagy and Dulichár Petra and Dorián László Galata and Balázs Démuth and Enikő Borbás and György Marosi and Zsombor Kristóf Nagy and Attila Farkas},
keywords = {Dissolution prediction, Real-time release testing, Artificial neural network, Process Analytical Technology, Raman spectroscopy, NIR spectroscopy},
abstract = {This work proposes the application of artificial neural networks (ANN) to non-destructively predict the in vitro dissolution of pharmaceutical tablets from Process Analytical Technology (PAT) data. An extended release tablet formulation was studied, where the dissolution was influenced by the composition of the tablets and the tableting compression force. NIR and Raman spectra of the intact tablets were measured, and the dissolution of the tablets was modeled directly from the spectral data. Partial Least Square (PLS) regression and ANN models were developed for the different spectroscopic measurements individually as well as by combining them together. ANN provided up to 3% lower root mean square error for prediction (RMSEP) than the PLS models, due to its capability of modeling non-linearity between the process parameters and dissolution curves. The ANN model using reflection NIR spectra provided the most accurate predictions with 6.5 and 63 mean f1 and f2 values between the computed and measured dissolution curves, respectively. Furthermore, ANN served as a straightforward data fusion method without the need for additional preprocessing steps. The method could significantly advance data processing in the PAT environment, contribute to an enhanced real-time release testing procedure and hence the increased efficacy of dissolution testing.}
}
@article{LEE20198,
title = {Intelligent positive computing with mobile, wearable, and IoT devices: Literature review and research directions},
journal = {Ad Hoc Networks},
volume = {83},
pages = {8-24},
year = {2019},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2018.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S157087051830619X},
author = {Uichin Lee and Kyungsik Han and Hyunsung Cho and Kyong-Mee Chung and Hwajung Hong and Sung-Ju Lee and Youngtae Noh and Sooyoung Park and John M. Carroll},
keywords = {Positive computing, Persuasive technologies, Mobile and wearable technologies, Internet-of-Things (IoT), Evidence-based design and intervention},
abstract = {The use of mobile, wearable, and Internet of Things (IoT) technologies fosters unique opportunities for designing novel intelligent positive computing services that address various health and well-being issues such as stress and depression. As positive computing research is often cross-disciplinary, it is difficult to acquire holistic perspectives on the design, implementation, and evaluation of intelligent positive computing systems with mobile, wearable, and IoT technologies. To bridge this gap, we propose a conceptual framework and review the key components to provide guidelines for intelligent positive computing systems research. We also present several practical service scenarios and provide useful insights on opportunities and challenges. By critically reflecting on the literature and scenarios, we suggest several research directions on the core topics in intelligent positive computing systems research. In addition, we discuss concerns and challenges such as technology dependence, abandonment, side effects, privacy, and ethical issues.}
}
@article{ZHOU2019100870,
title = {An operational parameter optimization method based on association rules mining for chiller plant},
journal = {Journal of Building Engineering},
volume = {26},
pages = {100870},
year = {2019},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2019.100870},
url = {https://www.sciencedirect.com/science/article/pii/S2352710219300622},
author = {Xuan Zhou and Bingwen Wang and Liequan Liang and Junwei Yan and Dongmei Pan},
keywords = {Chiller plant, Operational parameters optimization, Unsupervised data mining, ARM},
abstract = {The traditional mechanism models of a chiller plant of HVAC are complicated with multiple variables and many constraints, so that it’s burdensome to optimize those operational parameters. Moreover, the optimization methods based on mechanism models are unpractical to be applied in the engineering projects. Therefore, an operational parameter optimization method based on the unsupervised data mining technology is proposed in this paper and verified with a large number of field operational data of the chiller plant of a shopping mall in the subtropical area. The unsupervised data mining procedure is illustrated in detail, including data preparation, data partitioning, strong association rules extraction by Apriori algorithm and so on. The definition, selection and discretization methods of external and operational parameters are described to determine the mining target and divide typical operating conditions. At last, 54 and 70 strong association rules, respectively, for the single larger chiller operating mode and the single smaller one under typical operating conditions are extracted. Simulation results shows that the energy consumption of the chiller plant is reduced by 11.60% and 13.33% after optimization, respectively, on the studied days in summer and that in winter. The analysis results mean that after optimization, the energy performance of a chiller plant was significantly improved. The strong association rules are easily utilized in the engineering projects, in terms of their simplicity and feasibility. And this method can also be used in other fields when there are enough effective operational data.}
}
@article{JAREMKO2019107,
title = {Canadian Association of Radiologists White Paper on Ethical and Legal Issues Related to Artificial Intelligence in Radiology},
journal = {Canadian Association of Radiologists Journal},
volume = {70},
number = {2},
pages = {107-118},
year = {2019},
issn = {0846-5371},
doi = {https://doi.org/10.1016/j.carj.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0846537119300063},
author = {Jacob L. Jaremko and Marleine Azar and Rebecca Bromwich and Andrea Lum and Li Hsia {Alicia Cheong} and Martin Gibert and François Laviolette and Bruce Gray and Caroline Reinhold and Mark Cicero and Jaron Chong and James Shaw and Frank J. Rybicki and Casey Hurrell and Emil Lee and An Tang},
keywords = {Artificial intelligence, Machine learning, Ethics, Legal, Radiology, Imaging},
abstract = {Artificial intelligence (AI) software that analyzes medical images is becoming increasingly prevalent. Unlike earlier generations of AI software, which relied on expert knowledge to identify imaging features, machine learning approaches automatically learn to recognize these features. However, the promise of accurate personalized medicine can only be fulfilled with access to large quantities of medical data from patients. This data could be used for purposes such as predicting disease, diagnosis, treatment optimization, and prognostication. Radiology is positioned to lead development and implementation of AI algorithms and to manage the associated ethical and legal challenges. This white paper from the Canadian Association of Radiologists provides a framework for study of the legal and ethical issues related to AI in medical imaging, related to patient data (privacy, confidentiality, ownership, and sharing); algorithms (levels of autonomy, liability, and jurisprudence); practice (best practices and current legal framework); and finally, opportunities in AI from the perspective of a universal health care system.
Résumé
Les logiciels d’intelligence artificielle (IA) analysant les images médicales sont de plus en plus prévalents. Contrairement aux générations précédentes de logiciels d’IA qui reposaient sur un savoir d’expert pour identifier les caractéristiques d’une image, les approches d’apprentissage machine permettent aux systèmes d’apprendre à reconnaître ces caractéristiques. Toutefois, la promesse d’une médecine personnalisée précise ne peut être remplie qu’avec un accès à de très grandes quantités de données médicales de patients. Ces données pourraient servir à prédire les maladies, à les diagnostiquer, à optimiser les traitements et à établir un pronostic. La radiologie se trouve en position de chef de file pour le développement et la mise en œuvre des algorithmes d’IA, ainsi que pour la gestion des défis éthiques et légaux qui leur sont associés. Ce livre blanc de l’Association canadienne des radiologistes (CAR) propose un cadre d’étude pour les problèmes légaux et éthiques en rapport avec l’utilisation de l’IA dans l’imagerie médicale pour les données des patients (vie privée, confidentialité, propriété et partage), les algorithmes (degrés d’autonomie, responsabilité et jurisprudence), la pratique médicale (meilleures pratiques et cadre réglementaire actuel) et les opportunités offertes par l’IA du point de vue d’un système de soins de santé universel.}
}
@article{BENNETT201912,
title = {Data Science for Child Health},
journal = {The Journal of Pediatrics},
volume = {208},
pages = {12-22},
year = {2019},
issn = {0022-3476},
doi = {https://doi.org/10.1016/j.jpeds.2018.12.041},
url = {https://www.sciencedirect.com/science/article/pii/S0022347618318158},
author = {Tellen D. Bennett and Tiffany J. Callahan and James A. Feinstein and Debashis Ghosh and Saquib A. Lakhani and Michael C. Spaeder and Stanley J. Szefler and Michael G. Kahn},
keywords = {forecasting, classification, decision support systems, clinical, data mining, machine learning, phenotype, neural networks, data warehousing}
}
@incollection{GOPALAKRISHNAN2019247,
title = {Chapter 18 - Regulatory Perspectives on the Use of Biomarkers and Personalized Medicine in CNS Drug Development: The FDA Viewpoint},
editor = {George G. Nomikos and Douglas E. Feltner},
series = {Handbook of Behavioral Neuroscience},
publisher = {Elsevier},
volume = {29},
pages = {247-258},
year = {2019},
booktitle = {Translational Medicine in CNS Drug Development},
issn = {1569-7339},
doi = {https://doi.org/10.1016/B978-0-12-803161-2.00018-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128031612000187},
author = {Mathangi Gopalakrishnan and Jogarao V.S. Gobburu},
keywords = {Imaging biomarkers, Accelerated approval, Biomarker qualification program (BQP), Real-world evidence (RWE), Critical path innovations meetings (CPIM)},
abstract = {Drug development for central nervous system (CNS) disorders is challenging and has faced a mixed bag of successes. Failures to identify safe and effective therapies for conditions such as Alzheimer's disease and Parkinson's disease have been attributed to lack of good preclinical disease models, absences of link between biomarkers, and clinical end points and adequate heterogeneity in disease progression. Understanding reasons for the disease heterogeneity and identifying early markers for treatment effect have been recognized as important factors for successful drug development for CNS disorders. Recent scientific advances in genetic sequencing and imaging technologies offer promising ways to accelerate the development of therapies. Toward achieving this goal, several FDA regulatory initiatives, such as biomarker qualification program (BQP), Critical Path Innovations Meeting (CPIM), and expedited drug approval pathways, could potentially lead to successful and more targeted CNS therapies in the future.}
}
@article{DING2019129,
title = {A survey on data fusion in internet of things: Towards secure and privacy-preserving fusion},
journal = {Information Fusion},
volume = {51},
pages = {129-144},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253518304731},
author = {Wenxiu Ding and Xuyang Jing and Zheng Yan and Laurence T. Yang},
keywords = {Data fusion, Internet of Things, Data privacy, Security, Smart home, Smart grid, Smart transportation},
abstract = {Internet of Things (IoT) aims to create a world that enables the interconnection and integration of things in physical world and cyber space. With the involvement of a great number of wireless sensor devices, IoT generates a diversity of datasets that are massive, multi-sourcing, heterogeneous, and sparse. By taking advantage of these data to further improve IoT services and offer intelligent services, data fusion is always employed first to reduce the size and dimension of data, optimize the amount of data traffic and extract useful information from raw data. Although there exist some surveys on IoT data fusion, the literature still lacks comprehensive insight and discussion on it with regard to different IoT application domains by paying special attention to security and privacy. In this paper, we investigate the properties of IoT data, propose a number of IoT data fusion requirements including the ones about security and privacy, classify the IoT applications into several domains and then provide a thorough review on the state-of-the-art of data fusion in main IoT application domains. In particular, we employ the requirements of IoT data fusion as a measure to evaluate and compare the performance of existing data fusion methods. Based on the thorough survey, we summarize open research issues, highlight promising future research directions and specify research challenges.}
}
@article{ZVARA2019578,
title = {Optimizing distributed data stream processing by tracing},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {578-591},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.06.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17325141},
author = {Zoltán Zvara and Péter G.N. Szabó and Barnabás Balázs and András Benczúr},
keywords = {Distributed data processing, Data stream processing, Distributed tracing, Data provenance, Apache Spark},
abstract = {Heterogeneous mobile, sensor, IoT, smart environment, and social networking applications have recently started to produce unbounded, fast, and massive-scale streams of data that have to be processed “on the fly”. Systems that process such data have to be enhanced with detection for operational exceptions and with triggers for both automated and manual operator actions. In this paper, we illustrate how tracing in distributed data processing systems can be applied to detecting changes in data and operational environment to maintain the efficiency of heterogeneous data stream processing systems under potentially changing data quality and distribution. By the tracing of individual input records, we can (1) identify outliers in a web crawling and document processing system and use the insights to define URL filtering rules; (2) identify heavy keys, such as NULL, that should be filtered before processing; (3) give hints to improve the key-based partitioning mechanisms; and (4) measure the limits of overpartitioning if heavy thread-unsafe libraries are imported. By using Apache Spark as illustration, we show how various data stream processing efficiency issues can be mitigated or optimized by our distributed tracing engine. We describe and qualitatively compare two different designs, one based on reporting to a distributed database and another based on trace piggybacking. Our prototype implementation consists of wrappers suitable for JVM environments in general, with minimal impact on the source code of the core system. Our tracing framework is the first to solve tracing in multiple systems across boundaries and to provide detailed performance measurements suitable for automated optimization, not just debugging.}
}
@incollection{2019289,
title = {Chapter 6 - From Sensor to User—Interoperability of Sensors and Data Systems},
editor = {Eric Delory and Jay Pearlman},
booktitle = {Challenges and Innovations in Ocean In Situ Sensors},
publisher = {Elsevier},
pages = {289-337},
year = {2019},
isbn = {978-0-12-809886-8},
doi = {https://doi.org/10.1016/B978-0-12-809886-8.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128098868000065}
}
@article{GONDAL2019462,
title = {Development of a bariatric surgery specific risk assessment tool for perioperative myocardial infarction},
journal = {Surgery for Obesity and Related Diseases},
volume = {15},
number = {3},
pages = {462-468},
year = {2019},
issn = {1550-7289},
doi = {https://doi.org/10.1016/j.soard.2018.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S155072891831102X},
author = {Amlish Bilal Gondal and Chiu-Hsieh Hsu and Rostam Khoubyari and Iman Ghaderi},
keywords = {Bariatric surgery, Perioperative myocardial infarction, Mortality, Risk stratification},
abstract = {Background
Perioperative myocardial infarction (PMI) is a feared complication after surgery. Bariatric surgery, due to its intraabdominal nature, is traditionally considered an intermediate risk procedure. However, there are limited data on MI rates and its predictors in patients undergoing bariatric surgery.
Objectives
To enumerate the prevalence of PMI after bariatric surgery and develop a risk assessment tool.
Setting
Bariatric surgery centers, United States.
Methods
Patients undergoing bariatric surgery were identified from the MBSAQIP participant use file (PUF) 2016. Preoperative characteristics, which correlated with PMI were identified by multivariable regression analysis. PUF 2015 was used to validate the scoring tool developed from PUF 2016.
Results
We identified 172,017 patients from PUF 2016. Event rate for MI within 30 days of the operation was .03%; with a mortality rate of 17.3% in patients with a PMI. Four variables correlated with PMI on regression, including history of a previous MI (odds ratio [OR] = 8.57, confidence interval [CI] = 3.4–21.0), preoperative renal insufficiency (OR = 3.83, CI = 1.2–11.4), hyperlipidemia (OR = 2.60, CI = 1.3–5.1), and age >50 (OR = 2.15, CI = 1.1–4.2). Each predicting variable was assigned a score and event rate for MI was assessed with increasing risk score in PUF 2015; the rate increased from 9.5 per 100,000 operations with a score of 0 to 3.2 per 100 with a score of 5.
Conclusion
The prevalence of MI after bariatric surgery is lower than other intraabdominal surgeries. However, mortality with PMI is high. This scoring tool can be used by bariatric surgeons to identify patients who will benefit from focused perioperative cardiac workup.}
}
@article{JETTEN2019156,
title = {The role of CRIS’s in the research life cycle. A case study on implementing a FAIR RDM policy at Radboud University, the Netherlands},
journal = {Procedia Computer Science},
volume = {146},
pages = {156-165},
year = {2019},
note = {14th International Conference on Current Research Information Systems, CRIS2018, FAIRness of Research Information},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.090},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930095X},
author = {Mijke Jetten and Ed Simons and Jan Rijnders},
keywords = {CRIS, Metis, Data life cycle, Research Data Management, RDM, FAIR, Research data policy, Project registration, Data management plans, DMP, Archiving data, Narcis, DANS data archive, Donders repository},
abstract = {In 2015, Radboud University (Nijmegen, the Netherlands) started a project to extend its CRIS (Metis) with functionalities that allow researchers to register (metadata) and archive (uploading files) their research data, while at the same time making the data available for reuse in a FAIR way (via national Dutch data archive DANS). The new functionality was integrated with already existing functions in the CRIS, thus offering a one-stop-shop interface to researchers in which registration and archiving of data is combined with registration of publications, the uploading of full text to the university’s repository, the linking of datasets and publications and the creation of researcher’s profile (CV) pages. Next to the functional extension of the CRIS, the project also included an organizational element: the establishment of support and management structures and workflows, including data curation processes, in order to assure the quality of the data registration process and to foster the FAIRness of the research data. In the period up to now, we continued to transform the university’s CRIS, by bringing it in line with the research life cycle perspective and policy changes in Research Data Management (RDM), including a Data Management Plan (DMP) module and FAIR data. In this paper, it will be argued and demonstrated that both for researchers and research institutes, a CRIS oriented approach to RDM brings added value. We also point to future use cases that put a central role for CRIS’s even earlier within the research life cycle, e.g. at pre-registration of research questions and informed consent/ethics approval procedures. We further envision our CRIS to play a linking pin function between storage and service locations of data during research and at publication. The paper will use Radboud University as a good practice of past, present and future use of CRIS’s in the research life cycle that universities and research institutes as well as researchers and research support desks are currently dealing with in the FAIR data era.}
}
@article{LIU2019100984,
title = {Edge-cloud orchestration driven industrial smart product-service systems solution design based on CPS and IIoT},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100984},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100984},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619305579},
author = {Bufan Liu and Yingfeng Zhang and Geng Zhang and Pai Zheng},
keywords = {Product-service systems, Edge-cloud orchestration, Cyber-physical systems, Industrial Internet of Things, Data-driven value co-creation},
abstract = {The rapid booming of advanced information and communication technologies (ICT) has promoted an encouraging smart, connected product (SCP) market that further triggers the development of manufacturing towards the servitization proposition, viz. smart product-service systems (PSS). Smart PSS aims to provide a solution (product-service) with high satisfaction and less environmental influence by leveraging SCP as the media tool. Its solution design should not just focus on the physical world nor only be enabled by the cloud side, while the cyber world and the edge side must be included in the Industry 4.0. However, only few current researches investigate about the smart PSS, let alone an overall cyber-physical and edge-cloud discussion to support its solution design. In order to fill this gap, this work proposes an edge-cloud orchestration driven solution design based on the cyber-physical systems (CPS) and industrial Internet of Things (IIoT). To make our ideas concrete, a real-life milling process was conducted as an illustrative example. It is hoped that this study can furnish industrial enterprises with meaningful sights in the process of servitization and value co-creation.}
}
@article{LAWSON2019100028,
title = {Translation software: An alternative to transit data standards},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {2},
pages = {100028},
year = {2019},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2019.100028},
url = {https://www.sciencedirect.com/science/article/pii/S2590198219300284},
author = {Catherine T. Lawson and Paul Tomchik and Alex Muro and Eric Krans},
keywords = {Archived Intelligent Transportation Systems (ITS) data, General Transit Feed Specifications (GTFS), Service Interface for Real Time Information (SIRI)},
abstract = {Data standardization is recognized in many disciplines as a critical aspect of data stewardship. Establishing and implementing data specifications increases the usefulness of data collection efforts and facilitates analysis techniques. With the advent of large quantities of machine-generated data, the use of standardized data formats feeds opportunities for visualization and advanced applications with machine-learning and Artificial Intelligence (AI). The transportation industry made substantial progress with data format specifications in the late 1990s, primarily for highway traffic. Unfortunately, establishing data standards has been an on-going challenge for the transit community. Archived Intelligent Transportation Systems (ITS) transit data (e.g., Automatic Vehicle Location (AVL), Automatic Passenger Counters (APCs), Automatic Fare Card (AFC)) still lack industry standards for data formats. Recent advancements in electronic transit scheduling (e.g., General Transit Feed Specifications (GTFS)) met a portion of this challenge with Open Data specifications. Now GTFS provides transit riders with agile information on services available at any location where the data is provided to developers of mobile device application (apps). Due to system and vendor limitations, the Metropolitan Transportation Authority (MTA), serving the New York City region, publishes its real-time subway system data in GTFS-R and its bus data in SIRI. This research develops an Application Programming Interface (API) to translate GTFS-R into SIRI to overcome the lack of standards making it possible to harmonize the subway and bus systems for the New York region. This solution offers the opportunity to develop a novel set of analytical tools, including pseudo-surveillance data for performance metrics.}
}
@article{PIAO2019158,
title = {Privacy-preserving governmental data publishing: A fog-computing-based differential privacy approach},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {158-174},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18300773},
author = {Chunhui Piao and Yajuan Shi and Jiaqi Yan and Changyou Zhang and Liping Liu},
keywords = {Governmental statistical data publishing, Privacy-preserving, Fog computing, Differential privacy, MaxDiff histogram},
abstract = {With the growing availability of public open data, the protection of citizens’ privacy has become a vital issue for governmental data publishing. However, there are a large number of operational risks in the current government cloud platforms. When the cloud platform is attacked, most existing privacy protection models for data publishing cannot resist the attacks if the attacker has prior background knowledge. Potential attackers may gain access to the published statistical data, and identify specific individual’s background information, which may cause the disclosure of citizens’ private information. To address this problem, we propose a fog-computing-based differential privacy approach for privacy-preserving data publishing in this paper. We discuss the risk of citizens’ privacy disclosure related to governmental data publishing, and present a differential privacy framework for publishing governmental statistical data based on fog computing. Based on the framework, a data publishing algorithm using a MaxDiff histogram is developed, which can be used to realize the function of preserving user privacy based on fog computing. Applying the differential method, Laplace noises are added to the original data set, which prevents citizens’ privacy from disclosure even if attackers get strong background knowledge. According to the maximum frequency difference, the adjacent data bins are grouped, then the differential privacy histogram with minimum average error can be constructed. We evaluate the proposed approach by computational experiments based on the real data set of Philippine families’ income and expenditures provided by Kaggle. It shows that the proposed data publishing approach can not only effectively protect citizens’ privacy, but also reduce the query sensitivity and improve the utility of the data published.}
}
@article{BEUNZA2019103257,
title = {Comparison of machine learning algorithms for clinical event prediction (risk of coronary heart disease)},
journal = {Journal of Biomedical Informatics},
volume = {97},
pages = {103257},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103257},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419301765},
author = {Juan-Jose Beunza and Enrique Puertas and Ester García-Ovejero and Gema Villalba and Emilia Condes and Gergana Koleva and Cristian Hurtado and Manuel F. Landecho},
keywords = {Machine learning, Supervised machine learning, Support vector machines, Research techniques, Area under curve, Diagnostic techniques and procedures},
abstract = {Aim
The aim of this study is to compare the utility of several supervised machine learning (ML) algorithms for predicting clinical events in terms of their internal validity and accuracy. The results, which were obtained using two statistical software platforms, were also compared.
Materials and methods
The data used in this research come from the open database of the Framingham Heart Study, which originated in 1948 in Framingham, Massachusetts as a prospective study of risk factors for cardiovascular disease. Through data mining processes, three data models were elaborated and a comparative methodological study between the different ML algorithms – decision tree, random forest, support vector machines, neural networks, and logistic regression – was carried out. The global selection criterium for choosing the right set of hyperparameters and the type of data manipulation was the area under a curve (AUC). The software tools used to analyze the data were R-Studio® and RapidMiner®.
Results
The Framingham study open database contains 4240 observations. The algorithm that yielded the greatest AUC when analyzing the data in R-Studio was neural network applied to a model that excluded all observations in which there was at least one missing value (AUC = 0.71); when analyzing the data in RapidMiner and applying the same model, the best algorithm was support vector machines (AUC = 0.75).
Conclusions
ML algorithms can reinforce the diagnostic and prognostic capacity of traditional regression techniques. Differences between the applicability of those algorithms and the results obtained with them were a function of the software platforms used in the data analysis.}
}
@article{ZHU20191072,
title = {A dynamic approach to energy efficiency estimation in the large-scale chemical plant},
journal = {Journal of Cleaner Production},
volume = {212},
pages = {1072-1085},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.11.186},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618335881},
author = {Li Zhu and Junghui Chen},
keywords = {Energy efficiency estimation models, Industrial energy efficiency, Nonlinear modeling method},
abstract = {With the increasing pressures from the energy price and environmental protection, large-scale chemical plants pay more attention to the implementation of energy efficiency estimation to improve its economic benefit and environmental performance. Because of the stochastic and dynamic characteristics of the actual data, traditional estimation methods fail to satisfy the requirement of real-time evaluation. To cope with this limitation, a novel energy efficiency estimation method combining just-in-time (JIT) learning and subspace model identification (SMI) with noise elimination, called e-JITSMI method, is proposed. First, the state space model is constructed to describe the dynamic performance of production processes. This integration method can select the appropriate sampling data, estimate noise effect, and build the corresponding dynamic model. With the built model, not only are the relationships between production and supplied energy built, but the energy efficiency tendency is also predicted at the next moment. In addition, with the arrival of the new sampling data, the dynamic evaluation model is automatically updated. The effectiveness and accuracy of the proposed method are demonstrated through a practical large-scale chemical process. The results present the average accuracy of energy efficiency prediction can reach 88.9% and the tendency of energy efficiency is 100% correct even if the working conditions change.}
}
@article{JOBE20199,
title = {Off-Label Drugs in Neonatology: Analyses Using Large Data Bases},
journal = {The Journal of Pediatrics},
volume = {208},
pages = {9-11},
year = {2019},
issn = {0022-3476},
doi = {https://doi.org/10.1016/j.jpeds.2019.01.038},
url = {https://www.sciencedirect.com/science/article/pii/S002234761930126X},
author = {Alan H. Jobe},
keywords = {off-label, neonatology, surfactant}
}
@article{HAOYU201969,
title = {An IoMT cloud-based real time sleep apnea detection scheme by using the SpO2 estimation supported by heart rate variability},
journal = {Future Generation Computer Systems},
volume = {98},
pages = {69-77},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18326980},
author = {Li Haoyu and Li Jianxing and N. Arunkumar and Ahmed Faeq Hussein and Mustafa Musa Jaber},
keywords = {Sleep apnea, Wearable device, Oxygen saturation, Intelligent classification, HRV, Cloud computing},
abstract = {Obstructive sleep apnea refers to a highly rampant sleep-related breathing disorder. The gold standard examination for diagnosis is polysomnography. Even though it provides highly accurate results, this multi-parametric test is time consuming and expensive. It also does not align with the new trend in health care, where focus is shifted to wellness and prevention. One possible way to address this problem is home health care, through the use of minimal invasive devices, higher accessibility, and provision of low cost diagnosis. To manage this, an automated and portable sleep apnea detector was formulated and assessed. The device utilizes one SpO2 sensor for estimating the heart rate and the oxygen blood level as well. The basis of the proposed analysis method is the connection between heart rate variability and oxygen saturation with d apnea events. The measured signals were then transferred to a cloud-based system architecture to diagnose and warn the remote patients. This solution was used to process the data and display it on both the mobile phone and personal computer. Testing of the proposed algorithms was done using the St. Vincents University Hospital/University College Dublin sleep apnea database. Apart from this database, the researchers utilized data gathered from 10 apnea patient volunteers. The performance of the proposed scheme algorithm achieved an average accuracy, specificity, and sensitivity of 98.54, 98.95%, and 97.05%, respectively.}
}