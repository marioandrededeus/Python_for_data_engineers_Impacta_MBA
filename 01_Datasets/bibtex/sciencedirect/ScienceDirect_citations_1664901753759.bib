@article{ZHAO2021101580,
title = {Understanding the key factors and configurational paths of the open government data performance: Based on fuzzy-set qualitative comparative analysis},
journal = {Government Information Quarterly},
volume = {38},
number = {3},
pages = {101580},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101580},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000162},
author = {Yupan Zhao and Bo Fan},
keywords = {OGD, Implementation performance, fsQCA, Configurational path},
abstract = {The governments worldwide have attached great importance to open government data (OGD), and many OGD projects have emerged in recent years. However, the performance of OGD greatly differs in various districts and governments. Therefore, the influencing factors of OGD performance should be explored. However, the existing research has not yet established a systematic analytical framework for OGD performance, and the explanation degree of performance differences in OGD implementation is limited. Thus, this study takes technical management capacity, financial resource, organization arrangement, rules and regulations, organization culture, public demand, and inter-government competition as antecedent conditions under the perspective of technology–organization–environment framework and resource-based theory. From the cases of 16 provincial OGD practice in China, we employ fuzzy-set qualitative comparative analysis to explore the influencing mechanism of the interaction and coordination of multiple conditions on OGD performance. Results indicate that OGD performance depends on the integration of the total effect of various factors. Moreover, four configurational paths could be utilized to achieve high OGD performance, namely, organization–balanced path, organization–environment path, balanced path, and organization–technology path. Furthermore, a substitution relationship exists among different conditional variables, which points out the direction and focus of the implementation of OGD for governments with different endowment characteristics. This study enriches the existing studies of OGD implementation and provides references for OGD practice.}
}
@article{NITSCHKE2021642,
title = {Conceptualizing the Internet of Things Data Supply},
journal = {Procedia Computer Science},
volume = {181},
pages = {642-649},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.213},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002568},
author = {Patrick Nitschke and Susan P. Williams},
keywords = {Internet of Things, Data Supply, Value Capturing},
abstract = {By enmeshing the digital and physical worlds, the Internet of Things is envisioned to generate a wealth of real-world data which enables new ways of creating value based on data. Organizations and researchers in the Information Systems community have already begun to transform existing approaches by applying concepts such as Nonownership Business Models that use data as the new main resource to create value. However, as our critical literature review shows, there has been limited attention to the actual characteristics and challenges of data supply in the IoT. Based on the critical review, two distinct themes were identified regarding the conceptualization of data and Things. Data is conceptualized either as Shallow Data or as Provenance Data, whereas things are conceptualized in terms of Things as Sensors or Things as Agents. Based on these themes, two research implications are developed. Firstly, things are more than sensors, they inherit agency and intention from their respective owners. Secondly, the provenance of data is essential. Data is considered a resource, quality control, assessment of legality are essential to securely rely on data to create value.}
}
@article{AKTER2021258,
title = {How to Build an AI Climate-Driven Service Analytics Capability for Innovation and Performance in Industrial Markets?},
journal = {Industrial Marketing Management},
volume = {97},
pages = {258-273},
year = {2021},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2021.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0019850121001437},
author = {Shahriar Akter and Samuel Fosso Wamba and Marcello Mariani and Umme Hani},
keywords = {AI climate, Cognitive service analytics capability, Service innovation, Market performance, B2B markets},
abstract = {AI climate-driven service analytics capability has been anecdotally argued as a viable strategy to enhance service innovation and market performance in B2B markets. While AI climate refers to the shared perceptions of policies, procedures, and practices to support AI initiatives, cognitive service analytics capability refers to the analytical insights driven by AI climate and augmented by both machines and humans to make marketing decisions. However, there is limited knowledge on the antecedents of such analytics capabilities and their overall effects on service innovation and market performance. Drawing on service analytics literature and the microfoundations of dynamic capability theory, this study fills this research gap using in-depth interviews (n = 30) and a survey (n = 276) of service analytics managers within the AI climate in Australia. The findings confirm the five microfoundations of cognitive service analytics capabilities (cognitive technology, cognitive information, cognitive problem solving, cognitive knowledge & skills, cognitive training & development). The findings also highlight the significant mediating effect of service innovation in the relationship between analytics climate and market performance and cognitive service analytics capability and market performance.}
}
@article{YIN2021103579,
title = {Development of cultural tourism platform based on FPGA and convolutional neural network},
journal = {Microprocessors and Microsystems},
volume = {80},
pages = {103579},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103579},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120307298},
author = {Xinzhe Yin and Jinghua Li},
keywords = {Data mining, Field programmable gate array (FPGA) using xilinx, Predictive Modeling, Association Analysis},
abstract = {Data mining can be described as a typical analysis of large datasets to investigate early unknown types, styles, and interpersonal relationships to generate the right decision information. It improves their markets and today to maintain control over whether these companies are forced into the data mining tools and technologies they use to develop and manage tourism products and services in the market. It is falling out of the favorable situation of the travel and tourism industry. Objective work is to provide and display its application in data mining and tourism. Advances in mobile technology provide an opportunity to obtain real-time information of travelers, such as time and space behavior, at the destination they visit. This study analyzed a large-scale mobile phone data set to capture the mobile phone traces of international tourists who visited South Korea. We adopt the trajectory data mining method to understand tourism activities’ spatial structure in three different destinations. The research reveals tourist destinations and multiple “hot spots” (or popular areas) that interact spatially in these places through spatial cluster analysis and sequential pattern mining. Therefore, this article provides the planning of spatial model destinations to integrate important tourism influences, which is based on tourism design. The proposed system is modelled in Field Programmable Gate Array (FPGA) using Xilinx software.}
}
@incollection{CAVALIERE2021479,
title = {Chapter 23 - Molecular Docking: A Contemporary Story About Food Safety},
editor = {Mohane S. Coumar},
booktitle = {Molecular Docking for Computer-Aided Drug Design},
publisher = {Academic Press},
pages = {479-492},
year = {2021},
isbn = {978-0-12-822312-3},
doi = {https://doi.org/10.1016/B978-0-12-822312-3.00025-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128223123000254},
author = {Francesca Cavaliere and Giulia Spaggiari and Pietro Cozzini},
keywords = {Big data, Consensus scoring, Database, Food safety, In silico methods, Molecular docking, Virtual screening},
abstract = {The application of computational methods (repository or database design, screening, and molecular docking) in food safety is a relatively recent challenge. Docking/scoring techniques could be applied to a wide range of food safety problems. An important milestone for screening/docking approaches is the availability of a three-dimensional database to collect the huge amount of food contact chemicals to make possible testing these compounds otherwise unfeasible with traditional in vitro tests. In silico applications could be applied to predict the interaction between food contact chemicals and different receptors/targets involved in human diseases and/or to decipher their mechanism of binding. Another important purpose is the design of chemosensors for mycotoxins detection. The use of docking techniques as an alternative to animal tests is an emerging field and we will illustrate these concepts using recently published cases.}
}
@article{XU2021102482,
title = {Application of training data affects success in broad-scale local climate zone mapping},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {103},
pages = {102482},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102482},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421001896},
author = {Chunxue Xu and Perry Hystad and Rui Chen and Jamon {Van Den Hoek} and Rebecca A. Hutchinson and Steve Hankey and Robert Kennedy},
keywords = {Local climate zone, Machine learning, Training areas, Crowdsourced data, Spatial autocorrelation},
abstract = {Satellite imagery has been widely used to map urbanization processes. To address the urgent need for urban landscape mapping that goes beyond urban footprint analysis, the local climate zone (LCZ) scheme has been increasingly used to reveal the urban forms and functions important to urban heat islands and micro-climates across the globe. As with most supervised classification strategies, proper application of training data is critical for the success of LCZ classification models. However, the collection and application of LCZ training areas brings with it two challenges that may affect mapping success. First, because digitizing training areas is a time-consuming task, there is a broad effort in the LCZ mapping community to create a crowdsourced data collection among different experts. However, this strategy likely leads to inconsistencies in labels that could weaken models. Second, the LCZ labeling process typically involves the delineation of large zones from which multiple training samples are drawn, but those samples are likely spatially autocorrelated and lead to overly optimistic estimates of model accuracy. Although both effects -- inconsistent labeling and spatial autocorrelation -- are theoretically possible, it is unknown whether they substantially affect accuracy. We investigated both issues, specifically asking: (i) how do the discrepancies of LCZ labeling by different experts impact broad-scale LCZ mapping? (ii) to what extent does spatial correlation affect model prediction power? We used two classifiers (Random Forests and ResNets) to map eight metropolitan areas in the US into LCZs, comparing training areas drawn by different or consistent interpreters, and data splitting strategy using rules that allow or reduce spatial autocorrelation. We found large discrepancies among results built from crowdsourced training areas digitized by different experts; improving the consistency of labels can lead to substantial improvements in LCZ classification accuracy. Second, we found that spatial autocorrelation can boost the apparent accuracy of the classifier by 16% to 21%, leading to erroneous interpretation of mapping results. The two effects interplay as well: spatial autocorrelation in the raw data can lead to an underestimation of the model’s predictive error when modeling with crowdsourced training areas of high inconsistency. Due to the uncertainty in the labeling process and spatial autocorrelation in derived training data, broad-scale LCZ mapping results should be interpreted with caution.}
}
@article{JAVAID2021209,
title = {Internet of Things (IoT) enabled healthcare helps to take the challenges of COVID-19 Pandemic},
journal = {Journal of Oral Biology and Craniofacial Research},
volume = {11},
number = {2},
pages = {209-214},
year = {2021},
issn = {2212-4268},
doi = {https://doi.org/10.1016/j.jobcr.2021.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S2212426821000154},
author = {Mohd Javaid and Ibrahim Haleem Khan},
keywords = {Internet of things (IoT), COVID-19, Information technology applications, Healthcare, Smart hospital},
abstract = {Background/objectives
The Internet of Things (IoT) can create disruptive innovation in healthcare. Thus, during COVID-19 Pandemic, there is a need to study different applications of IoT enabled healthcare. For this, a brief study is required for research directions.
Methods
Research papers on IoT in healthcare and COVID-19 Pandemic are studied to identify this technology’s capabilities. This literature-based study may guide professionals in envisaging solutions to related problems and fighting against the COVID-19 type pandemic.
Results
Briefly studied the significant achievements of IoT with the help of a process chart. Then identifies seven major technologies of IoT that seem helpful for healthcare during COVID-19 Pandemic. Finally, the study identifies sixteen basic IoT applications for the medical field during the COVID-19 Pandemic with a brief description of them.
Conclusions
In the current scenario, advanced information technologies have opened a new door to innovation in our daily lives. Out of these information technologies, the Internet of Things is an emerging technology that provides enhancement and better solutions in the medical field, like proper medical record-keeping, sampling, integration of devices, and causes of diseases. IoT’s sensor-based technology provides an excellent capability to reduce the risk of surgery during complicated cases and helpful for COVID-19 type pandemic. In the medical field, IoT’s focus is to help perform the treatment of different COVID-19 cases precisely. It makes the surgeon job easier by minimising risks and increasing the overall performance. By using this technology, doctors can easily detect changes in critical parameters of the COVID-19 patient. This information-based service opens up new healthcare opportunities as it moves towards the best way of an information system to adapt world-class results as it enables improvement of treatment systems in the hospital. Medical students can now be better trained for disease detection and well guided for the future course of action. IoT’s proper usage can help correctly resolve different medical challenges like speed, price, and complexity. It can easily be customised to monitor calorific intake and treatment like asthma, diabetes, and arthritis of the COVID-19 patient. This digitally controlled health management system can improve the overall performance of healthcare during COVID-19 pandemic days.}
}
@article{PEI2021207,
title = {GIScience and remote sensing in natural resource and environmental research: Status quo and future perspectives},
journal = {Geography and Sustainability},
volume = {2},
number = {3},
pages = {207-215},
year = {2021},
issn = {2666-6839},
doi = {https://doi.org/10.1016/j.geosus.2021.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666683921000389},
author = {Tao Pei and Jun Xu and Yu Liu and Xin Huang and Liqiang Zhang and Weihua Dong and Chengzhi Qin and Ci Song and Jianya Gong and Chenghu Zhou},
keywords = {Natural resource, Environmental science, GIScience, Remote sensing, Information technology},
abstract = {Geographic information science (GIScience) and remote sensing have long provided essential data and methodological support for natural resource challenges and environmental problems research. With increasing advances in information technology, natural resource and environmental science research faces the dual challenges of data and computational intensiveness. Therefore, the role of remote sensing and GIScience in the fields of natural resources and environmental science in this new information era is a key concern of researchers. This study clarifies the definition and frameworks of these two disciplines and discusses their role in natural resource and environmental research. GIScience is the discipline that studies the abstract and formal expressions of the basic concepts and laws of geography, and its research framework mainly consists of geo-modeling, geo-analysis, and geo-computation. Remote sensing is a comprehensive technology that deals with the mechanisms of human effects on the natural ecological environment system by observing the earth surface system. Its main areas include sensors and platforms, information processing and interpretation, and natural resource and environmental applications. GIScience and remote sensing provide data and methodological support for resource and environmental science research. They play essential roles in promoting the development of resource and environmental science and other related technologies. This paper provides forecasts of ten future directions for GIScience and eight future directions for remote sensing, which aim to solve issues related to natural resources and the environment.}
}
@article{ZIEGENBEIN2021869,
title = {Data-based quality analysis in machining production: Influence of data pre-processing on the results of machine learning models},
journal = {Procedia CIRP},
volume = {104},
pages = {869-874},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.146},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010441},
author = {Amina Ziegenbein and Joachim Metternich},
keywords = {Machine Learning, Part Quality, Drilling},
abstract = {Quality assurance as a non-value-adding process is constantly reviewed for cost optimisation and potential savings. In the pursuit of utilising advanced data analysis and machine learning methods to improve efficiency of quality assurance in machining processes there are several influencing factors severely impacting the performance and hence the value of said methods. Especially data preparation is a time consuming task requiring both domain and data expert knowledge and yielding various options for data preparation. In this paper, the impact of different input data sets for predicting part quality in a drilling process is investigated, using machine control data.}
}
@article{KALUZNY2021970,
title = {Data analytics in military human performance: Getting in the game: Summary of a keynote address},
journal = {Journal of Science and Medicine in Sport},
volume = {24},
number = {10},
pages = {970-974},
year = {2021},
issn = {1440-2440},
doi = {https://doi.org/10.1016/j.jsams.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1440244021000992},
author = {Bohdan L. Kaluzny},
keywords = {Data analytics, Operations research, Military human performance},
abstract = {Objectives
The rise of data analytics has been not only central to the digital transformation of many industries and governments, but is now ubiquitous in daily life. But what is it? Researchers in military human performance may very well ask themselves: What is new? After all, aren't they already collecting, analysing, interpreting, and presenting data? Do they need to adapt?
Discussion
Defence and security have often been at the forefront of new technologies, but has lagged other industries with respect to data analytics. Sports science is one of the industries that are on the leading edge and this presents an opportunity that researchers in military human performance must seize.
Conclusions
Researchers must embrace data analytics and seek opportunities to ‘operationalize’ their research via data science: responsible analytics respecting scientific development supporting decision making at the necessary speed of relevance.}
}
@incollection{2021321,
title = {Glossary},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {321-329},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.09986-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150099862}
}
@article{ZHANG2021119139,
title = {Nondestructive evaluation of soluble solids content in tomato with different stage by using Vis/NIR technology and multivariate algorithms},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {248},
pages = {119139},
year = {2021},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2020.119139},
url = {https://www.sciencedirect.com/science/article/pii/S1386142520311185},
author = {Dongyan Zhang and Yi Yang and Gao Chen and Xi Tian and Zheli Wang and Shuxiang Fan and Zhenghua Xin},
keywords = {Vis/NIR, Soluble solids content, Tomato, PLS, LS-SVM, Effective wavelength},
abstract = {In this study Vis/NIR spectroscopy was applied to evaluate soluble solids content (SSC) of tomato. A total of 168 tomato samples with five different maturity stages, were measured by two developed systems with the wavelength ranges of 500–930 nm and 900–1400 nm, respectively. The raw spectral data were pre-processed by first derivative and standard normal variate (SNV), respectively, and then the effective wavelengths were selected using competitive adaptive reweighted sampling (CARS) and random frog (RF). Partial least squares (PLS) and least square-support vector machines (LS-SVM) were employed to build the prediction models to evaluate SSC in tomatoes. The prediction results revealed that the best performance was obtained using the PLS model with the optimal wavelengths selected by CARS in the range of 900–1400 nm (Rp = 0.820 and RMSEP = 0.207 °Brix). Meanwhile, this best model yielded desirable results with Rp and RMSEP of 0.830 and 0.316 °Brix, respectively, in 60 samples of the independent set. The method proposed from this study can provide an effective and quick way to predict SSC in tomato.}
}
@article{HOJBRASEN20211918,
title = {Comparison between data maturity and maintenance strategy: A case sutdy},
journal = {Procedia CIRP},
volume = {104},
pages = {1918-1923},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.324},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121012221},
author = {Lucas Peter {Høj Brasen} and Oliver Fuglsan Groos and Torben Tambo},
keywords = {manufacturing, asset management, data maturity, sensor networks, predictive maintenance, Internet-of-Things, SME},
abstract = {With the rise of Industry 4.0, there has been a substantial drive towards sensor networks for enabling predictive maintenance as an essential component of asset management. This study analyses sensor data maturity and asset management strategy. A model is proposed for establishing a best-fit correlation between data maturity and maintenance strategy, both for the current situation and as a guide for future development. The findings are based on the literature and case studies for small and medium-sized enterprises. The research implication is to view enterprise strategy as a balance between the chosen maturity and operational needs. The practical implication is the possibility to sustain or improve and qualify investment planning.}
}
@article{HAO2021127,
title = {Semi-supervised disentangled framework for transferable named entity recognition},
journal = {Neural Networks},
volume = {135},
pages = {127-138},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304159},
author = {Zhifeng Hao and Di Lv and Zijian Li and Ruichu Cai and Wen Wen and Boyan Xu},
keywords = {Named entity recognition, Semi-supervised learning, Transfer learning, Disentanglement},
abstract = {Named entity recognition (NER) for identifying proper nouns in unstructured text is one of the most important and fundamental tasks in natural language processing. However, despite the widespread use of NER models, they still require a large-scale labeled data set, which incurs a heavy burden due to manual annotation. Domain adaptation is one of the most promising solutions to this problem, where rich labeled data from the relevant source domain are utilized to strengthen the generalizability of a model based on the target domain. However, the mainstream cross-domain NER models are still affected by the following two challenges (1) Extracting domain-invariant information such as syntactic information for cross-domain transfer. (2) Integrating domain-specific information such as semantic information into the model to improve the performance of NER. In this study, we present a semi-supervised framework for transferable NER, which disentangles the domain-invariant latent variables and domain-specific latent variables. In the proposed framework, the domain-specific information is integrated with the domain-specific latent variables by using a domain predictor. The domain-specific and domain-invariant latent variables are disentangled using three mutual information regularization terms, i.e., maximizing the mutual information between the domain-specific latent variables and the original embedding, maximizing the mutual information between the domain-invariant latent variables and the original embedding, and minimizing the mutual information between the domain-specific and domain-invariant latent variables. Extensive experiments demonstrated that our model can obtain state-of-the-art performance with cross-domain and cross-lingual NER benchmark data sets.}
}
@article{JIN2021102059,
title = {Bus network assisted drone scheduling for sustainable charging of wireless rechargeable sensor network},
journal = {Journal of Systems Architecture},
volume = {116},
pages = {102059},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102059},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121000497},
author = {Yong Jin and Jia Xu and Sixu Wu and Lijie Xu and Dejun Yang and Kaijian Xia},
keywords = {Wireless rechargeable sensor network, Bus network, Drone scheduling, Traveling salesman path problem, Submodular orienteering problem},
abstract = {Wireless Rechargeable Sensor Network (WRSN) is largely used in monitoring of environment and traffic, video surveillance and medical care, etc., and helps to improve the quality of urban life. However, it is challenging to provide the sustainable energy for sensors deployed in buildings, soil or other places, where it is hard to harvest the energy from environment. To address this issue, we design a new wireless charging system, which levers the bus network assisted drone in urban areas. We formulate the drone scheduling problem based on this new wireless charging system to minimize the total time cost of drone subject to all sensors can be charged under the energy constraint of drone. Then, we propose an approximation algorithm DSA for the energy tightened drone scheduling problem. To make the tasks of WRSN sustainable, we further formulate the drone scheduling problem with deadlines of sensors, and present the approximation algorithm DDSA to find the drone schedule with the maximal number of sensors charged by the drone before deadlines. Through the extensive simulations, we demonstrate that DSA can reduce the total time cost by 84.83% compared with Greedy Replenished Energy algorithm, and uses at most 5.98 times of the total time cost of optimal solution on average. Then, we also demonstrate that DDSA can increase the survival rate of sensors by 51.95% compared with Deadline Greedy Replenished Energy algorithm, and can obtain 77.54% survival rate of optimal solution on average.}
}
@article{FENZA2021107366,
title = {Data set quality in Machine Learning: Consistency measure based on Group Decision Making},
journal = {Applied Soft Computing},
volume = {106},
pages = {107366},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107366},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621002891},
author = {Giuseppe Fenza and Mariacristina Gallo and Vincenzo Loia and Francesco Orciuoli and Enrique Herrera-Viedma},
keywords = {Consistency, Learning to Rank, Group Decision Making, Training data consistency, Data set quality},
abstract = {Performance of Machine Learning models heavily depends on the quality of the training dataset. Among others, the quality of training data relies on the consistency of the labels assigned to similar items. Indeed, the labels should be coherently assigned (or collected) by avoiding inconsistencies for increasing the performance of the machine learning model. This study focuses on evaluating training data consistency for machine learning algorithms dealing with ranking problems, i.e., the Learning to Rank methods (LTR). This work defines a training data consistency measure based on the consensus value introduced in Group Decision Making. It investigates the statistical relationship between the proposed consistency measure and the performance of a deep neural network implementing an LTR method. This measure could drive data filtering at the training stage and guide model update decisions. Experimentation reveals a strong correlation between the proposed consistency measure and the performance of the model.}
}
@article{LI2021104262,
title = {Robust CSEM data processing by unsupervised machine learning},
journal = {Journal of Applied Geophysics},
volume = {186},
pages = {104262},
year = {2021},
issn = {0926-9851},
doi = {https://doi.org/10.1016/j.jappgeo.2021.104262},
url = {https://www.sciencedirect.com/science/article/pii/S0926985121000094},
author = {Guang Li and Zhushi He and Juzhi Deng and Jingtian Tang and Youyao Fu and Xiaoqiong Liu and Changming Shen},
keywords = {CSEM data processing, Periodic signal de-noising, Signal-noise identification, Fuzzy -means clustering (FCM), Machine Learning, Correlation Analysis},
abstract = {The ambient noise in controlled-source electromagnetic (CSEM) data seriously affects the accuracy and reliability of the exploration result. Traditional correlation-based data selection method requires manually setting the threshold. To overcome the deficiency, we analyze the typical noises in CSEM data and find that normalized cross-correlation (NCC), absolute maximum value of the amplitude (Max), and detrend fluctuation analysis (DFA) can be used to accurately identify high-quality time series. Based on this discovery, we replace traditional manually intervention with unsupervised machine learning and propose a novel CSEM data processing method. We applied the newly proposed method to synthetic and measured CSEM data to verify the feasibility and effectiveness. Experimental results demonstrate that the newly proposed method is superior to the conventional data selection method because it accurately selects the best data fragments from noisy data automatically. The newly proposed method requires no human intervention which makes the results obtained free of subjective distortion caused by the operator.}
}
@incollection{CROMPTON202193,
title = {5 - Data management from the DCS to the historian and HMI},
editor = {Patrick Bangert},
booktitle = {Machine Learning and Data Science in the Power Generation Industry},
publisher = {Elsevier},
pages = {93-122},
year = {2021},
isbn = {978-0-12-819742-4},
doi = {https://doi.org/10.1016/B978-0-12-819742-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197424000056},
author = {Jim Crompton},
keywords = {Machine learning, Value chain optimization, Incremental value chain economics, Digital platform, Asset utilization, Grow new markets, Integrated strategies, Innovation},
abstract = {In today's global and fast changing business environment, the urgency with multinational companies to find readily implementable digital solutions has increased significantly as the underlying science yielding operational and business improvements has matured over the past decade. Companies lacking innovative ways to extract incremental value from existing and new business ventures will be left behind. Manufacturing plants, in general, manage billion dollars’ worth of raw and finished products daily across the United States and globally. The quantities of data analyzed in a product lifecycle, capturing cost to produce, logistics, working capital, and other ancillary costs are massive and difficult to both consolidate and integrate. Many production companies still rely heavily on manual processes to evaluate opportunities and economics.}
}
@article{DOMINIEK2021102988,
title = {Exploring variation in the performance of planned birth: A mixed method study},
journal = {Midwifery},
volume = {98},
pages = {102988},
year = {2021},
issn = {0266-6138},
doi = {https://doi.org/10.1016/j.midw.2021.102988},
url = {https://www.sciencedirect.com/science/article/pii/S026661382100067X},
author = {Coates Dominiek and Henry Amanda and Chambers Georgina and Paul Repon and Makris Angela and Clerke Teena and Natasha Donnolley},
keywords = {Unwarranted variation, Induction of labour, Planned caesarean section, Clinician perspectives},
abstract = {Objective: Variation in practice in relation to indications and timing for both induction of labour (IOL) and planned caesarean section (CS) clearly exists. However, the extent of this variation, and how this variation is explained by clinicians remains unclear. The aim of this study was to map the variation in IOL and planned CS at eight Australian hospitals, and understand why variation occurs from the perspective of clinicians at these hospitals. Our ultimate aim was to identify opportunities for improvement as evidenced by hospital data, clinician experiences, and feedback. Design: A two-phased mixed method study using sequential explanatory study design. The first phase consisted of an analysis of routinely collected patient data to map variation between hospitals. The second phase consisted of focus groups with clinicians to gain their perspectives on the reasons for variation. Setting and Participants: Patient data consisted of routine data from 19,073 women giving birth at eight Sydney hospitals between November 2017 and October 2018. Focus groups were attended by a total of 61 medical staff and 121 midwives. Results: Hospital data analysis found substantial variation, before and after adjustment for case-mix, in rates of both IOL (adjusted rates 27.6%–42%) and planned CS (adjusted rate 15.4%–22.6%). Planned CS by gestation also showed variation, although after restricting analysis to term (≥37 weeks gestation) births, variation was reduced. At focus groups, five main themes explaining variation emerged: local guidelines, policies and procedures (inconsistency and ambiguity); uncertainty of the evidence/what is best practice (contradictory research and different interpretations of evidence); clinician preferences, beliefs and values; the culture of the unit; and organisational influences (access to specialised clinics, theatre time). Key conclusions: Considerable variation in IOL and planned CS, even after case-mix adjustment, was found in this sample of Australian hospitals. Engagement with hospital clinicians identified likely sources of this variation and enabled clinicians at each hospital to consider appropriate local responses to address variation, such as more detailed review of their planned birth cases. Implications for practice: At a macro level, measures to reduce unwarranted variation should initially focus on consistent national guidelines, while supporting equitable access to operating theatres for optimal CS timing, and shared decision-making training to reduce influence of clinician preference.}
}
@article{ABUSALIH2021103076,
title = {Domain-specific knowledge graphs: A survey},
journal = {Journal of Network and Computer Applications},
volume = {185},
pages = {103076},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103076},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000990},
author = {Bilal Abu-Salih},
keywords = {Knowledge graph, Domain-specific knowledge graph, Knowledge graph construction, Knowledge graph embeddings, Knowledge graph evaluation, Domain ontology, Survey},
abstract = {Knowledge Graphs (KGs) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the KG which underpins a better comprehension, reasoning and interpretation of knowledge for both human and machine. Therefore, KGs continue to be used as the main means of tackling a plethora of real-life problems in various domains. However, there is no consensus in regard to a plausible and inclusive definition of a domain-specific KG. Further, in conjunction with several limitations and deficiencies, various domain-specific KG construction approaches are far from perfect. This survey is the first to offer a comprehensive definition of a domain-specific KG. Also, the paper presents a thorough review of the state-of-the-art approaches drawn from academic works relevant to seven domains of knowledge. An examination of current approaches reveals a range of limitations and deficiencies. At the same time, uncharted territories on the research map are highlighted to tackle extant issues in the literature and point to directions for future research.}
}
@article{CARETTATEIXEIRA20211097,
title = {Proposal for a health information management model based on Lean thinking},
journal = {Procedia Computer Science},
volume = {181},
pages = {1097-1104},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003550},
author = {Jéssica Cristina {Caretta Teixeira} and Filipe Andrade Bernardi and Rui Pedro Charters {Lopes Rijo} and Domingos Alves},
keywords = {Lean Healthcare, Health Information Systems, Quality Improvement},
abstract = {Although assessing quality in the health field is a prominent challenge, there is unanimity among managers that it is necessary to select appropriate assessment systems and methods to assist the administration of services and provide decision-making with the least degree of uncertainty possible. Lean, also known as lean philosophy, is a management model that has been used in the area of Health. The management of data, knowledge, and health services must be carefully performed, so that quality care can be offered. at all levels of care. In this way, when implementing Lean strategies in Information Technology, it is necessary to evaluate all its processes within the institution to eliminate waste, structure functions within the applied methodology and measure improvement at all levels of the organization. Thus, the general objective of this article is that of a study that leads to a health information management model based on Lean thinking in the municipality of Ituverava. The highly heterogeneous, and sometimes ambiguous, nature of the medical language and its constant evolution, the high amount of data generated constantly by the automation of processes and the emergence of new technologies constitute the foundation for the inevitable computerization of health to promote the production and management of knowledge. Adopting Lean thinking in health may seem a challenge initially for managers and team members, but as the first results begin to appear, profound and concrete changes are visible for positive transformation for improvements in the quality of the service provided, until the culture can be learned completely in order to have the perfect care.}
}
@article{MULLAHY2021e103,
title = {Embracing Uncertainty: The Value of Partial Identification in Public Health and Clinical Research},
journal = {American Journal of Preventive Medicine},
volume = {61},
number = {2},
pages = {e103-e108},
year = {2021},
issn = {0749-3797},
doi = {https://doi.org/10.1016/j.amepre.2021.01.041},
url = {https://www.sciencedirect.com/science/article/pii/S0749379721001707},
author = {John Mullahy and Atheendar Venkataramani and Daniel L. Millimet and Charles F. Manski},
abstract = {Introduction
This paper describes the methodology of partial identification and its applicability to empirical research in preventive medicine and public health.
Methods
The authors summarize findings from the methodologic literature on partial identification. The analysis was conducted in 2020–2021.
Results
The applicability of partial identification methods is demonstrated using 3 empirical examples drawn from published literature.
Conclusions
Partial identification methods are likely to be of considerable interest to clinicians and others engaged in preventive medicine and public health research.}
}
@article{ZEISER2021597,
title = {Requirements towards optimizing analytics in industrial processes},
journal = {Procedia Computer Science},
volume = {184},
pages = {597-605},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007079},
author = {Alexander Zeiser and Bas van Stein and Thomas Bäck},
keywords = {Industry 4.0, Predictive Modelling, Optimizing analytics, Information fusion, Machine Learning, Industrial processes},
abstract = {Modern production systems are composed of complex manufacturing processes with highly technology specific cause-effect relationships. Developments in sensor technology and computational science allow for data-driven decision making that facilitate effcient and objective production management. However, process data may only be beneficial if it is enriched with meta information and process expertise, reduced to relevant information and modelling results interpreted correctly. The importance of data integration in the heterogeneous industrial environment rises at the same momentum as new metrology techniques are deployed. In this paper, we focus on optimizing analytics, containing data-driven decision making for predictive quality and maintenance. We summarize key requirements for data analytics and machine learning application in industrial processes. With a use case from automotive component manufacturing we characterize industrial production, categorize process data and put requirements in context to a real-world example.}
}
@article{TRUANT2021121173,
title = {Digitalisation boosts company performance: an overview of Italian listed companies},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121173},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121173},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006065},
author = {Elisa Truant and Laura Broccardo and Léo-Paul Dana},
keywords = {Digitalization, Company performance, Listed company, Benefit, Obstacle},
abstract = {Digitalisation has become embedded in products and services, and it increasingly supports corporate business processes. However, few empirical studies have analysed the state of digitalisation and its implementation within companies, and the extant literature has painted an inconsistent picture concerning the effects of digitalisation. This survey-based study explores the diffusion of digitalisation, the advantages and difficulties in the practical transition to digitalisation, and its impact on performance. The sample includes Italian listed companies across diverse industries. The results highlight the still embryonic adoption of digital tools to support daily company operations; however, the impacts of digitalisation on company performance are noticeable. This research contributes to the literature on digitalisation and performance, breaks new ground by focusing on listed companies, and has implications for management investment in digitalisation for value creation.}
}
@article{MARTINNOGUEROL2021317,
title = {Artificial intelligence in radiology: relevance of collaborative work between radiologists and engineers for building a multidisciplinary team},
journal = {Clinical Radiology},
volume = {76},
number = {5},
pages = {317-324},
year = {2021},
issn = {0009-9260},
doi = {https://doi.org/10.1016/j.crad.2020.11.113},
url = {https://www.sciencedirect.com/science/article/pii/S0009926020306024},
author = {T. Martín-Noguerol and F. Paulano-Godino and R. López-Ortega and J.M. Górriz and R.F. Riascos and A. Luna},
abstract = {The use of artificial intelligence (AI) algorithms in the field of radiology is becoming more common. Several studies have demonstrated the potential utility of machine learning (ML) and deep learning (DL) techniques as aids for radiologists to solve specific radiological challenges. The decision-making process, the establishment of specific clinical or radiological targets, the profile of the different professionals involved in the development of AI solutions, and the relation with partnerships and stakeholders are only some of the main issues that have to be faced and solved prior to starting the development of radiological AI solutions. Among all the players in this multidisciplinary team, the communication between radiologists and data scientists is essential for a successful collaborative work. There are specific skills that are inherent to radiological and medical training that are critical for identifying anatomical or clinical targets as well as for segmenting or labelling lesions. These skills would then have to be transferred, explained, and taught to the data science experts to facilitate their comprehension and integration into ML or DL algorithms. On the other hand, there is a wide range of complex software packages, deep neural-network architectures, and data transfer processes for which radiologists need the expertise of software engineers and data scientists in order to select the optimal manner to analyse and post-process this amount of data. This paper offers a summary of the top five challenges faced by radiologists and data scientists including tips and tricks to build a successful AI team.}
}
@article{CHAKRABORTY2021106410,
title = {Machine learning based digital twin for dynamical systems with multiple time-scales},
journal = {Computers & Structures},
volume = {243},
pages = {106410},
year = {2021},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2020.106410},
url = {https://www.sciencedirect.com/science/article/pii/S0045794920302133},
author = {S. Chakraborty and S. Adhikari},
keywords = {Digital twin, Multi-timescale dynamics, Mixture of experts, Gaussian process, Frequency},
abstract = {Digital twin technology has a huge potential for widespread applications in different industrial sectors such as infrastructure, aerospace, and automotive. However, practical adoptions of this technology have been slower, mainly due to a lack of application-specific details. Here we focus on a digital twin framework for linear single-degree-of-freedom structural dynamic systems evolving in two different operational time scales in addition to its intrinsic dynamic time-scale. Our approach strategically separates into two components – (a) a physics-based nominal model for data processing and response predictions, and (b) a data-driven machine learning model for the time-evolution of the system parameters. The physics-based nominal model is system-specific and selected based on the problem under consideration. On the other hand, the data-driven machine learning model is generic. For tracking the multi-timescale evolution of the system parameters, we propose to exploit a mixture of experts as the data-driven model. Within the mixture of experts model, Gaussian Process (GP) is used as the expert model. The primary idea is to let each expert track the evolution of the system parameters at a single time-scale. For learning the hyperparameters of the ‘mixture of experts using GP’, an efficient framework that exploits expectation-maximization and sequential Monte Carlo sampler is used. Performance of the digital twin is illustrated on a multi-timescale dynamical system with stiffness and/or mass variations. The digital twin is found to be robust and yields reasonably accurate results. One exciting feature of the proposed digital twin is its capability to provide reasonable predictions at future time-steps. Aspects related to the data quality and data quantity are also investigated.}
}
@article{JI2021365,
title = {Blog text quality assessment using a 3D CNN-based statistical framework},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {365-370},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330028},
author = {Fang Ji and Heqing Zhang and Zijiang Zhu and Weihuang Dai},
keywords = {Blog data, VQA, 3D CNN, Video-related text, Bi-LSTM, Text quality evaluation},
abstract = {Aiming at the problem that blog texts are the streaming data captured by different acquisition modality, each kind of which has its particular quality evaluation mode, this paper proposes a text quality evaluation (TQA) model based on 3D CNN correlated with blog text data. In order to achieve accurate TQA value, the model adopted a Bi-LSTM-based architecture to process video-related blog text as auxiliary part to provide additional information for our TQA architecture. First, the auxiliary part constructs feature vector for each video-related text by the model originating from Bi-LSTM and Seq2Seq. Then, the feature vector was feed to a well-trained decoder to reconstruct the original input data. Then, the feature vector complied with the blog textual data are inputted into end-to-end TQA modal based on the 3D CNN straightly. Comprehensive experimental results on the blog text/video dataset from the well-known truism website “http://www.mafengwo.cn/” have shown that the proposed model reflects the subjective quality of online texts more accurately, and has better overall blog TQA assessment performance than the other state-of-the-art non-reference methods.}
}
@article{ZHANG2021116452,
title = {A review of machine learning in building load prediction},
journal = {Applied Energy},
volume = {285},
pages = {116452},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116452},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921000209},
author = {Liang Zhang and Jin Wen and Yanfei Li and Jianli Chen and Yunyang Ye and Yangyang Fu and William Livingood},
keywords = {Building energy system, Building load prediction, Building energy forecasting, Machine learning, Feature engineering, Data engineering},
abstract = {The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.}
}
@article{DING2021100662,
title = {Towards the next generation of the LinkedGeoData project using virtual knowledge graphs},
journal = {Journal of Web Semantics},
volume = {71},
pages = {100662},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100662},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000378},
author = {Linfang Ding and Guohui Xiao and Albulen Pano and Claus Stadler and Diego Calvanese},
keywords = {LinkedGeoData, Virtual knowledge graph, GeoSPARQL, Ontop, OpenStreetMap},
abstract = {With the advancement of Semantic Technologies, large geospatial data sources have been increasingly published as Linked data on the Web. The LinkedGeoData project is one of the most prominent such projects to create a large knowledge graph from OpenStreetMap (OSM) with global coverage and interlinking of other data sources. In this paper, we report on the ongoing effort of exposing the relational database in LinkedGeoData as a SPARQL endpoint using Virtual Knowledge Graph (VKG) technology. Specifically, we present two realizations of VKGs, using the two systems Sparqlify and Ontop. In order to improve compliance with the OGC GeoSPARQL standard, we have implemented GeoSPARQL support in Ontop v4. Moreover, we have evaluated the VKG-powered LinkedGeoData in the test areas of Italy and Germany. Our experiments demonstrate that such system supports complex GeoSPARQL queries, which confirms that query answering in the VKG approach is efficient.}
}
@article{GAJJAM2021,
title = {Key challenges and research direction in cloud storage},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.01.609},
url = {https://www.sciencedirect.com/science/article/pii/S221478532100701X},
author = {Mr. Nikhil S. Gajjam and Dr. T. Gunasekhar},
keywords = {Cloud Computing, Cloud database, Availability, Cloud Storage, Consistency},
abstract = {Cloud computing has gained the importance in recent time because of its unique characteristics like elasticity, resilience in delivering compute, storage, network, and resource management in form of utility services over the Internet. Cloud computing area has attracted attention and interest due to the ever-increasing demand for reliable and cost-effective infrastructure and service delivery. This paper primarily focuses on the survey of issues and open challenges of cloud database in cloud computing storage. The aim of this survey is to provide a perspective to a researcher a general overview of issues, open challenges, and future trends in cloud storage with primarily focuses on cloud databases. The paper also tabulates comparison of different methods and approaches of researchers in tackling issues pertaining to data availability through data replication, data partitioning, data management, and data placement under the context of cloud enabled technologies.}
}
@article{LIANG2021100249,
title = {Data Price Determinants Based on a Hedonic Pricing Model},
journal = {Big Data Research},
volume = {25},
pages = {100249},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100249},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000666},
author = {Ji Liang and Chunhui Yuan},
keywords = {Data market, Pricing management, Hedonic, Data pricing model},
abstract = {Data have become an emerging factor of production; the development of the data trading market is accelerating, but data pricing remains in the initial exploration stage. Based on hedonic price theory, this paper analyzes the factors influencing data prices. From the perspective of data transactions, this paper proposes construction of a data price characteristic index system from three aspects: data objects, data demanders and data suppliers. An empirical analysis is conducted based on 1010 data commodity sample data points from the Jingdong Vientiane platform. Through testing and estimation, it is considered that the hedonic price models can be fitted by a logarithmic function, taken as the function form of data hedonic price analysis. According to the analysis of the sample data sources from the data market, the number of free trials, data specification and quantity, data attention (number of data visits), data demand (sales volume), and preferential policies for businesses are negatively correlated with data price. Data evaluation satisfaction (number of collected users), request parameters (business), return parameters (business), data range, and response speed are positively correlated with data price. In addition, further analysis is made on the reason of the processing complexity variable of the data algorithm which does not conform to the expected estimation.}
}
@article{ZUIDERWIJK2021101577,
title = {Implications of the use of artificial intelligence in public governance: A systematic literature review and a research agenda},
journal = {Government Information Quarterly},
volume = {38},
number = {3},
pages = {101577},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101577},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000137},
author = {Anneke Zuiderwijk and Yu-Che Chen and Fadi Salem},
keywords = {Public governance, Artificial intelligence, Artificial intelligence for government, Public sector, Digital government, Systematic literature review, Research agenda},
abstract = {To lay the foundation for the special issue that this research article introduces, we present 1) a systematic review of existing literature on the implications of the use of Artificial Intelligence (AI) in public governance and 2) develop a research agenda. First, an assessment based on 26 articles on this topic reveals much exploratory, conceptual, qualitative, and practice-driven research in studies reflecting the increasing complexities of using AI in government – and the resulting implications, opportunities, and risks thereof for public governance. Second, based on both the literature review and the analysis of articles included in this special issue, we propose a research agenda comprising eight process-related recommendations and seven content-related recommendations. Process-wise, future research on the implications of the use of AI for public governance should move towards more public sector-focused, empirical, multidisciplinary, and explanatory research while focusing more on specific forms of AI rather than AI in general. Content-wise, our research agenda calls for the development of solid, multidisciplinary, theoretical foundations for the use of AI for public governance, as well as investigations of effective implementation, engagement, and communication plans for government strategies on AI use in the public sector. Finally, the research agenda calls for research into managing the risks of AI use in the public sector, governance modes possible for AI use in the public sector, performance and impact measurement of AI use in government, and impact evaluation of scaling-up AI usage in the public sector.}
}
@article{ZHAO2021116175,
title = {Data-driven framework for large-scale prediction of charging energy in electric vehicles},
journal = {Applied Energy},
volume = {282},
pages = {116175},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116175},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920315798},
author = {Yang Zhao and Zhenpo Wang and Zuo-Jun Max Shen and Fengchun Sun},
keywords = {Charging energy, Large-scale prediction, Machine learning, Electric vehicle},
abstract = {Large-scale and high-precision predictions of the charging energy required for electric vehicles (EVs) are essential to ensure the safety of EVs and provide reliable inputs for grid-load calculations. However, the complex and dynamic operating conditions of EVs make it challenging to accurately predict the charging energy under real-world conditions, especially for large-scale EV utilization. In this study, a novel data-driven framework for large-scale charging energy predictions is developed by individually controlling the strongly linear and weakly nonlinear contributions. The proposed framework concurrently addresses the overfitting of nonlinear networks using a low proportion of training data as well as the poorly descriptive ability of linear networks under complex environments. For each charging session, the charging energy predictions appropriately account for important factors such as the variations in the state of charge (SOC) of the battery, ambient temperatures, charging rates, and total driving distances. The results suggest that, compared with existing prediction models (such as the random forest, xgboost, and neural network), the proposed framework persists with evidently higher accuracy and stability over a wide range of the ratio between the number of EVs used for testing and training; its mean absolute percentage error (MAPE) is maintained at 2.5–3.8% when the ratio ranges from 0.1 to 1000. The proposed models can be further utilized for cloud-based battery diagnoses and large-scale forecasting of the energy demands of EVs.}
}
@article{OLIVEIRA2021893,
title = {Steps towards an Healthcare Information Model based on openEHR},
journal = {Procedia Computer Science},
volume = {184},
pages = {893-898},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007900},
author = {Daniela Oliveira and Rui Miranda and Francini Hak and Nuno Abreu and Pedro Leuschner and António Abelha and José Machado},
keywords = {Electronic Health Record, Healthcare Information Systems, OpenEHR, Clinical Information model, Reference Model},
abstract = {During COVID-19 pandemic crisis, healthcare institutions globally were experiencing a VUCA - Volatile, Uncertain, Complex, and Ambiguous - environment. Effcient clinical and administrative management had never been so emergent. To achieve this goal, different components of the Healthcare Information System (HIS) must cooperate and interoperate flawlessly. Data standardization is a necessary step towards normalization and interoperability between existing Legacy Systems (LSs), and provides for longitudinal, highly reliable and persistent Electronic Health Records (EHRs). The openEHR standard was chosen for its overall dual domain architecture, where the more dynamic clinical information model may evolve independently from the relatively stable Reference Model (RM). Its Information Model (IM) comprises demographic, administrative and clinical systems. Critical clinical terms have been aligned to the FHIR HL7 standard, as to further support interoperability.}
}
@article{KHAN2021130,
title = {Robustness of AI-based prognostic and systems health management},
journal = {Annual Reviews in Control},
volume = {51},
pages = {130-152},
year = {2021},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2021.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1367578821000195},
author = {Samir Khan and Seiji Tsutsumi and Takehisa Yairi and Shinichi Nakasuka},
keywords = {Prognostics and system health management, Robust AI, Machine learning, PHM, Fault diagnosis},
abstract = {Prognostic and systems Health Management (PHM) is an integral part of a system. It is used for solving reliability problems that often manifest due to complexities in design, manufacturing, operating environment and system maintenance. For safety-critical applications, using a model-based development process for complex systems might not always be ideal but it is equally important to establish the robustness of the solution. The information revolution has allowed data-driven methods to diffuse within this field to construct the requisite process (or system models) to cope with the so-called big data phenomenon. This is supported by large datasets that help machine-learning models achieve impressive accuracy. AI technologies are now being integrated into many PHM related applications including aerospace, automotive, medical robots and even autonomous weapon systems. However, with such rapid growth in complexity and connectivity, a systems’ behaviour is influenced in unforeseen ways by cyberattacks, human errors, working with incorrect or incomplete models and even adversarial phenomena. Many of these models depend on the training data and how well the data represents the test data. These issues require fine-tuning and even retraining the models when there is even a small change in operating conditions or equipment. Yet, there is still ambiguity associated with their implementation, even if the learning algorithms classify accordingly. Uncertainties can lie in any part of the AI-based PHM model, including in the requirements, assumptions, or even in the data used for training and validation. These factors lead to sub-optimal solutions with an open interpretation as to why the requirements have not been met. This warrants the need for achieving a level of robustness in the implemented PHM, which is a challenging task in a machine learning solution. This article aims to present a framework for testing the robustness of AI-based PHM. It reviews some key milestones achieved in the AI research community to deal with three particular issues relevant for AI-based PHM in safety-critical applications: robustness to model errors, robustness to unknown phenomena and empirical evaluation of robustness during deployment. To deal with model errors, many techniques from probabilistic inference and robust optimisation are often used to provide some robustness guarantee metric. In the case of unknown phenomena, techniques include anomaly detection methods, using causal models, the construction of ensembles and reinforcement learning. It elicits from the authors’ work on fault diagnostics and robust optimisation via machine learning techniques to offer guidelines to the PHM research community. Finally, challenges and future directions are also examined; on how to better cope with any uncertainties as they appear during the operating life of an asset.}
}
@article{SUN2021180,
title = {Learning hierarchical face representation to enhance HCI among medical robots},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {180-186},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330314},
author = {Dianmin Sun and Honghua Zhao and Tao Song and Aiqin Liu and Jinling Cheng and Zhi Liu and Xin Zhao},
keywords = {Face recognition, Deep learning, Hierarchical neural network, Medical robot HCI},
abstract = {In this paper, we propose a hierarchical framework for face recognition by learning deep representation. In order to exploit key patches for face recognition, we separate the entire image into several patches including eyes, nose, and mouth. A binary facegrid is generated to indicate the accurate position of the key patches in face image. The patches are fed into the hierarchical framework to learn the deep representation of the image. We leverage the PCA and SVM method for face recognition. Our face representation can enhance many medical robot applications. Comprehensive experiments have demonstrated that our proposed method can effectively recognize real human faces from fake samples.}
}
@article{GUO2021105522,
title = {Internet court's challenges and future in China},
journal = {Computer Law & Security Review},
volume = {40},
pages = {105522},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105522},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920301278},
author = {Meirong Guo},
keywords = {Internet court, Adjudication model, Civil jurisdiction, Internet technology},
abstract = {China established the world's first Internet court in Hangzhou in August 2017. Subsequently in 2018 Internet courts in Beijing and Guangzhou were established respectively. With the official establishment of these three Internet courts, China's electronic litigation advanced to a new stage.. Internet courts offer many advantages, and this innovative adjudication model has earned widespread approval for both its speedy acceptance of cases and speedy hearing of cases. This article analyzes the questions and challenges faced by Internet courts, proposes solutions such as compliance with three basic legal ethical principles, re-establishing the sense of presence and ritual of litigation, establishment of risk mitigation mechanisms between the legal system and technological systems to develop the ability for the construction of Internet courts in China.}
}
@article{CATO2021837,
title = {The Accuracy of Medication Administration Data in the Emergency Department: Why Does It Matter?},
journal = {Journal of Emergency Nursing},
volume = {47},
number = {6},
pages = {837-838},
year = {2021},
issn = {0099-1767},
doi = {https://doi.org/10.1016/j.jen.2021.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0099176721002464},
author = {Kenrick Cato}
}
@article{JAVAID2021100109,
title = {Significance of Quality 4.0 towards comprehensive enhancement in manufacturing sector},
journal = {Sensors International},
volume = {2},
pages = {100109},
year = {2021},
issn = {2666-3511},
doi = {https://doi.org/10.1016/j.sintl.2021.100109},
url = {https://www.sciencedirect.com/science/article/pii/S2666351121000309},
author = {Mohd Javaid and Abid Haleem and Ravi {Pratap Singh} and Rajiv Suman},
keywords = {Quality 4.0, Industry 4.0, Quality revolution, Quality control, Sensors, Technologies},
abstract = {Quality 4.0 corresponds to the growing digitisation of industry, which uses advanced technologies to enhance the quality of manufacturing and services. This fourth quality revolution is envisaged to digitise the entire quality systems and subsequently improve the existing quality approaches. Innovative industries adopt cloud-based quality 4.0 innovations in the controlled production process. It is used to resolve quality problems satisfactorily when they emerge and carry out real-time quality analyses to improve competitiveness and use them. Various ongoing challenges are take-over by Quality 4.0 technologies, such as automated root cause analysis, machine-to-machine connectivity to parameter auto adjustment, simulation of real-time processes and more. Quality 4.0 is a modern form of quality management. Digital technologies paired with more sophisticated methods and smarter processes will allow high-performance teams to provide consumers with high-performance and quality goods reliably. Sensors play an essential role in improving the quality of manufacturing and services. These can improve protection, increased internal productivity and sustainable operations. This paper provides how quality 4.0 will have a significant impact in the field of manufacturing. Various Key Aspects and enablers of Quality 4.0 for Manufacturing are discussed, finally, Identified and discussed eighteen significant applications of Quality 4.0 in the field of manufacturing. Quality 4.0 not only concerns the things happening inside a factory; it also includes the complete supply chain from Research and Development (R&D), manufacturing, development, distribution, sales, and service after-sales.}
}
@article{LEE2021101428,
title = {Understanding digital transformation in advanced manufacturing and engineering: A bibliometric analysis, topic modeling and research trend discovery},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101428},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101428},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001804},
author = {Ching-Hung Lee and Chien-Liang Liu and Amy J.C. Trappey and John P.T. Mo and Kevin C. Desouza},
keywords = {Digital transformation, Advanced manufacturing and engineering, Bibliometric analysis, Topic modeling, Systematic review},
abstract = {Digital transformation (DT) is the process of combining digital technologies with sound business models to generate great value for enterprises. DT intertwines with customer requirements, domain knowledge, and theoretical and empirical insights for value propagations. Studies of DT are growing rapidly and heterogeneously, covering the aspects of product design, engineering, production, and life-cycle management due to the fast and market-driven industrial development under Industry 4.0. Our work addresses the challenge of understanding DT trends by presenting a machine learning (ML) approach for topic modeling to review and analyze advanced DT technology research and development. A systematic review process is developed based on the comprehensive DT in manufacturing systems and engineering literature (i.e., 99 articles). Six dominant topics are identified, namely smart factory, sustainability and product-service systems, construction digital transformation, public infrastructure-centric digital transformation, techno-centric digital transformation, and business model-centric digital transformation. The study also contributes to adopting and demonstrating the ML-based topic modeling for intelligent and systematic bibliometric analysis, particularly for unveiling advanced engineering research trends through domain literature.}
}
@article{CAO2021106004,
title = {An automated zizania quality grading method based on deep classification model},
journal = {Computers and Electronics in Agriculture},
volume = {183},
pages = {106004},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106004},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921000223},
author = {Jingjun Cao and Tan Sun and Wenrong Zhang and Ming Zhong and Bo Huang and Guomin Zhou and Xiujuan Chai},
keywords = {Zizania, Automatic grading, Convolutional neural network, Deep learning, Object classification},
abstract = {Zizania, one of aquatic vegetables, needs to be graded before entering market for assuring the product quality. However, it is time-consuming, tedious, labor-intensive, inaccurate and expensive to assess qualitatively and grade zizanias manually. This paper gives an effective solution to automatically classify fresh zizania into two categories, high quality and defective quality, by using the deep learnt features from the appearances. A new architecture of convolutional neural network, called LightNet, has been proposed and described. Specifically, it is composed of many compressed blocks, which is designed to reduce the computation complexity mainly by converting serial down-sampling and convolution operation to parallel structure. We evaluate the proposed architecture on the zizania image dataset collected by ourselves and integrate the algorithm in the automatic grading device. The experimental results show that the accuracy rate achieves 95.62% and the speed of inference quality is around 47 ms per zizania image. The proposed LightNet for automate classification has less parameters and lower computation complexity than popular networks, while maintaining the comparable accuracy in the task of grading zizanias. It obtains 99.31% accuracy in the task of grading apples. The result proves that it can be extended to other tasks about classification.}
}
@article{ANUPAMA2021,
title = {A comprehensive review on the crop prediction algorithms},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.01.549},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321006404},
author = {C.G. Anupama and C. Lakshmi},
keywords = {Crop prediction, Crop recommendation, Data mining, Machine learning, Agriculture recommendation systems, Crop yield, Factors affecting crop yield},
abstract = {Crop prediction based on various underlying factors like Soil, Climate, Yielding ability, Physiographic, Socio economic etc., has been a potential area of research. Data Mining and Machine Learning Algorithms are used to excerpt the significant features that assist in accurately predicting or recommending the suitable crop. Though the algorithms perform well in accurately predicting the crop, the factors used, the heterogeneity of sources and features selected plays an important role in precision and error rate. There is a lot of scope for further enhancements and improvement. The proposed survey give a detailed analysis of the different sources selected, tools and techniques employed, algorithms used, and observations.}
}
@article{LO2021110164,
title = {Techno-economic analysis for biomass supply chain: A state-of-the-art review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {135},
pages = {110164},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.110164},
url = {https://www.sciencedirect.com/science/article/pii/S136403212030455X},
author = {Shirleen Lee Yuen Lo and Bing Shen How and Wei Dong Leong and Sin Yong Teng and Muhammad Akbar Rhamdhani and Jaka Sunarso},
keywords = {Biomass properties, Optimization, Supply chain risks, Supply chain uncertainties, Bioenergy},
abstract = {Given the increasing risk of climate change and depletion of non-renewable energy sources, countries around the world have been looking into energy profile diversification whereby biomass represents one of the most appealing alternatives for energy production feedstock. To attract interest and more investment from industry players into biomass-based industries, comprehensive techno-economic analysis has to be performed. In addition, various uncertainties related to supply chain such as biomass attainability, demand variation, and material price fluctuation, have to be considered in the evaluation to yield more accurate and reliable feasibility estimation. This review paper aims to: (i) provide an overview on the different types of methods or approaches used in the feasibility evaluation of biomass-based industries from the techno-economic point of view, and (ii) outline the supply chain uncertainties that should be incorporated into the evaluation model using a Malaysian case study to illustrate the impact of these uncertainties. Apart from that, some of the unquantifiable uncertainties and risks are critically reviewed in this paper. It was found that 78 % of the reviewed articles opted for mathematical modelling evaluation method with the majority leaning towards mathematical modelling with optimization (i.e., deterministic and stochastic optimization). Furthermore, only a minority had performed stochastic evaluation that incorporates biomass supply chain uncertainties. This review discussed six quantifiable uncertainties, include: (i) biomass availability, (ii) biomass quality, (iii) transportation cost, (iv) market demand, (v) fluctuation of pricing, and (vi) wages of workers.}
}
@article{KRAUS2021109,
title = {A valve closing body as a central sensory-utilizable component},
journal = {Procedia CIRP},
volume = {100},
pages = {109-114},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004765},
author = {Benjamin Kraus and Florian Schmitt and Kay-Eric Steffan and Eckhard Kirchner},
keywords = {Sensing machine elements (SME), Cyber-physical-system (CPS), Sensory-utilizable machine elements (SuME), Product development},
abstract = {Enhancing traditional machine elements by using their embodied physical effects as an additional sensorial function to create so called sensing machine elements (SME) has been gaining momentum as a novel research topic. Utilizing those machine elements has the potential to enable a wide digitization regarding the creation of cyber-physical systems (CPS) in all mechanical engineering fields, since the basic components of every machine are being enhanced. These basic machine elements are often located close to the point of interest and therefore enable the user to collect data directly within the process itself. This design idea for the location of sensors or sensory functions is called in-situ measurement. While the enhancement of machine elements to SMEs seems promising the question arises if not other components and their physical characteristics can be used as sensors themselves. Ideally, to minimize the distance between the desired target variable and the actual measurand, those sensory-utilizable components are integral components for the main function of the machine. One of those central components is the closing body of a valve. During the concept phase of a new valve a concept was chosen while also regarding the future possible integration of sensory functions. With this in mind it was possible to enhance the chosen concept doing a small tweak during the design phase with a sensory function, that detects the current position of the valves closing body, using its physical properties. The basic idea behind the concept and design phase as well as the gathered data from a first prototype is presented in this paper.}
}
@article{NGOCLANHUYNH2021117193,
title = {Novel short-term solar radiation hybrid model: Long short-term memory network integrated with robust local mean decomposition},
journal = {Applied Energy},
volume = {298},
pages = {117193},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117193},
url = {https://www.sciencedirect.com/science/article/pii/S030626192100619X},
author = {Anh {Ngoc-Lan Huynh} and Ravinesh C. Deo and Mumtaz Ali and Shahab Abdulla and Nawin Raj},
keywords = {Short-term solar radiation prediction, Robust Local Mean Decomposition, Deep Learning},
abstract = {Data-intelligent algorithms tailored for short-term energy forecasting can generate meaningful information on the future variability of solar energy developments. Traditional forecasting methods find it relatively difficult to obtain a reliable solar energy monitoring system because of the inherent nonlinearities in solar radiation and the related atmospheric input variables to any forecasting system. This paper proposes a new artificial intelligence-based hybrid model by employing the robust version of local mean decomposition (RLMD) and Long Short-term Memory (LSTM) network denoted as RLMD-LSTM. The objective model (i.e., RLMD-LSTM) is built near real-time, half-hourly ground-based solar radiation dataset for the solar rich, metropolitan study sites in Vietnam with all of the forecasting results being benchmarked through classical modelling approaches (i.e., Support Vector Regression SVR, Long Short-term Memory LSTM, Multivariate Adaptive Regression Spline MARS, Persistence) as well as the other alternative hybrid methods (i.e., RLMD-MARS, RLMD-Persistence and RLMD-SVR). Verified by statistical metrics and visual infographics, the present results demonstrate that the proposed model can generate satisfactory predictions, outperforming several counterpart methods. The predictive performance is stable for all study sites that the root-mean-square error remained profoundly lower for RLMD-LSTM (19–20%) compared with RLMD-MARS (20–21%), RLMD-SVR (29–35%), RLMD- Persistence (29–51%), LSTM (25–48%), MARS (21–51%) and SVR (23–85%), Persistence (29–51%). The Legates and McCabe’s Index, yielding a value of approximately 0.7988–0.9256 for RLMD-LSTM compared with 0.765–0.8142, 0.4917–0.5711, 0.6900–0.7482, 0.6914–0.7646, 0.4349–0.7170 respectively, for the RLMD-MARS, RLMD-SVR, RLMD-Persistence, LSTM, MARS, SVR, Persistence models, also confirms the outstanding performance of RLMD-LSTM model. Accordingly, the study ascertains that the newly designed approach can be a potential candidate for real-time energy management, renewable energy integration into a power grid and other decisions to optimise the overall system's scheduling and performance.}
}
@article{FORSTMANN2021100001,
title = {NeuroImage: Reports: A new member of the NeuroImage family embracing negative findings, replication studies and registered reports},
journal = {Neuroimage: Reports},
volume = {1},
number = {1},
pages = {100001},
year = {2021},
issn = {2666-9560},
doi = {https://doi.org/10.1016/j.ynirp.2020.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2666956020300015},
author = {Birte U. Forstmann and Christopher D. Chambers and Michael X. Cohen and Gesa Hartwigsen and Peter Kochunov and Dimitri {Van De Ville} and Chao-Gan Yan}
}
@article{GE2021100150,
title = {Semi-supervised data modeling and analytics in the process industry: Current research status and challenges},
journal = {IFAC Journal of Systems and Control},
volume = {16},
pages = {100150},
year = {2021},
issn = {2468-6018},
doi = {https://doi.org/10.1016/j.ifacsc.2021.100150},
url = {https://www.sciencedirect.com/science/article/pii/S2468601821000080},
author = {Zhiqiang Ge},
keywords = {Semi-supervised data, Data driven modeling, Machine learning, Process data analytics},
abstract = {Semi-supervised data are quite common in the process industry, which has caught much attention in recent years. The semi-supervised feature of process data not only has a great impact on data mining and analytics, but also matters in feature extraction and knowledge discovery in the process. In this paper, the framework of semi-supervised data modeling and applications is formulated for the process industry. First, the semi-supervised data structure is introduced, including the causes of semi-supervised data structure, the main feature of the semi-supervised data, and its effects on data modeling and applications in the process industry. Second, detailed research statuses on semi-supervised data modeling and applications in the process industry are illustrated, with introductions of some representative approaches. Third, several challenges and promising issues on modeling and application of semi-supervised data are discussed and highlighted for future research.}
}
@article{JAYAWARDENE2021100202,
title = {The role of data and information quality during disaster response decision-making},
journal = {Progress in Disaster Science},
volume = {12},
pages = {100202},
year = {2021},
issn = {2590-0617},
doi = {https://doi.org/10.1016/j.pdisas.2021.100202},
url = {https://www.sciencedirect.com/science/article/pii/S2590061721000624},
author = {Vimukthi Jayawardene and Thomas J. Huggins and Raj Prasanna and Bapon Fakhruddin},
keywords = {Disaster response, Decision making, Data and information quality},
abstract = {Massive amounts of data and information are exchanged during the response phase of disaster management. A large body of contemporary research has indicated that most of these data and information have severe quality related concerns, meaning that they may not be suitable for critical decision-making. The current paper addresses these issues by identifying how certain features of data and information quality function, to support specific, naturalistic decision-making processes during disaster response. These functions are used to revise and consolidate pre-existing definitions of data and information quality, for use in further disaster response research.}
}
@incollection{CROMPTON202183,
title = {Chapter 5 - Data Management from the DCS to the Historian},
editor = {Patrick Bangert},
booktitle = {Machine Learning and Data Science in the Oil and Gas Industry},
publisher = {Gulf Professional Publishing},
pages = {83-110},
year = {2021},
isbn = {978-0-12-820714-7},
doi = {https://doi.org/10.1016/B978-0-12-820714-7.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207147000054},
author = {Jim Crompton},
keywords = {data management, oil and gas industry, sensor data, digital operations, simulation, automation},
abstract = {The oil and gas industry produces great volumes of data on an ongoing basis. This data must be acquired, stored, curated, and made available properly for data science to be able to take place. This process starts at the sensor and continues via systems such as the control system and the historian. In this journey, information and operational technologies converge. Sensor data is acquired and transmitted through various systems to arrive at the control system, which then makes it available through various protocols for consumption by analysis software. Historians save the data and make it available to the human user as diagrams. Documents and simulation data can also be integrated into the picture. This complex landscape is discussed in this chapter.}
}
@article{YU2021100383,
title = {A node optimization model based on the spatiotemporal characteristics of the road network for urban traffic mobile crowd sensing},
journal = {Vehicular Communications},
volume = {31},
pages = {100383},
year = {2021},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2021.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2214209621000528},
author = {Haiyang Yu and Jing Fang and Shuai Liu and Yilong Ren and Jian Lu},
keywords = {Urban traffic mobile crowd sensing, Spatiotemporal characteristics, Dynamic accessibility, Node selection, Utility maximization},
abstract = {Urban traffic mobile crowd sensing (Urban Traffic MCS) has emerged as a new effective paradigm of sensing and collecting data by means of vehicles equipped with various sensors in urban areas. In an Urban Traffic MCS system, the utility directly reflects the effectiveness of the sensing results, and it is essential to maximize the utility of the collected data. Some studies have shown that utility can be effectively improved by optimizing the selection of sensing nodes. However, most previous research has considered only the coverage and critical links of the road network while neglecting the spatiotemporal characteristics of the traffic flow, although the latter are essential for node selection optimization and significantly impact the utility of Urban Traffic MCS. Therefore, most existing methods are not suitable for Urban Traffic MCS systems. In this paper, a novel node optimization model based on the spatiotemporal characteristics of the road network is proposed. First, we introduce the Urban Traffic MCS system, and dynamic accessibility is introduced to describe the spatiotemporal characteristics of the whole road network. Then, the utility function for Urban Traffic MCS is redefined based on the effective coverage and dynamic accessibility to consider both the topological structure of the road network and the dynamic changes in traffic flow. On this basis, a node selection method with the aim of maximizing the utility of Urban Traffic MCS is proposed. Finally, the results of simulation experiments show that the node selection method in this paper can effectively achieve increased utility for an Urban Traffic MCS system.}
}
@article{HEATH2021112372,
title = {Preface},
journal = {Food and Chemical Toxicology},
volume = {155},
pages = {112372},
year = {2021},
issn = {0278-6915},
doi = {https://doi.org/10.1016/j.fct.2021.112372},
url = {https://www.sciencedirect.com/science/article/pii/S0278691521004051},
author = {David Heath and Milena Horvat and Nives Ogrinc}
}
@article{SONG2021106521,
title = {The use of real-world data/evidence in regulatory submissions},
journal = {Contemporary Clinical Trials},
volume = {109},
pages = {106521},
year = {2021},
issn = {1551-7144},
doi = {https://doi.org/10.1016/j.cct.2021.106521},
url = {https://www.sciencedirect.com/science/article/pii/S1551714421002573},
author = {Fuyu Song and Chenxuan Zang and Xinyi Ma and Sifan Hu and Qiqing Sun and Shein-Chung Chow and Hongqiang Sun},
keywords = {Real-world data, Real-world evidence, Substantial evidence, Gap analysis, Fit-for-purpose, Regulatory submission},
abstract = {The 21st Century Cures Act passed by the United States (US) Congress in December 2016 requires the US Food and Drug Administration (FDA) shall establish a program to evaluate the potential use of real-world evidence (RWE) which is generated from real-world data (RWD) to (i) support approval of new indication for a drug approved under section 505 (c) and (ii) satisfy post-approval study requirements. RWE offers the opportunities to develop robust evidence using high-quality data and sophisticated methods for producing causal-effect estimates regardless randomization is feasible. In this article, we have demonstrated that the assessment of treatment effect (RWE) based on RWD could be biased due to the potential selection and information biases of RWD. Although fit-for-purpose RWE may meet regulatory standards under certain assumptions, it is not the same as substantial evidence (current regulatory standard in support of approval of regulatory submission). In practice, it is then suggested that when there are gaps between fit-for-purpose RWE and substantial evidence, we should make efforts to fill these gaps based on a comprehensive evaluation of the treatment effect. We also review two RWE examples to show some potential use of RWE in clinical studies.}
}
@article{CUNHA2021100403,
title = {A survey of privacy-preserving mechanisms for heterogeneous data types},
journal = {Computer Science Review},
volume = {41},
pages = {100403},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100403},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000435},
author = {Mariana Cunha and Ricardo Mendes and João P. Vilela},
keywords = {Privacy, Privacy taxonomy, Privacy-preserving mechanisms, Heterogeneous data types, Privacy tools},
abstract = {Due to the pervasiveness of always connected devices, large amounts of heterogeneous data are continuously being collected. Beyond the benefits that accrue for the users, there are private and sensitive information that is exposed. Therefore, Privacy-Preserving Mechanisms (PPMs) are crucial to protect users’ privacy. In this paper, we perform a thorough study of the state of the art on the following topics: heterogeneous data types, PPMs, and tools for privacy protection. Building from the achieved knowledge, we propose a privacy taxonomy that establishes a relation between different types of data and suitable PPMs for the characteristics of those data types. Moreover, we perform a systematic analysis of solutions for privacy protection, by presenting and comparing privacy tools. From the performed analysis, we identify open challenges and future directions, namely, in the development of novel PPMs.}
}
@article{FIAIDHI2021107187,
title = {Prognosis analysis of thick data: Clustering heart diseases risk groups case study},
journal = {Computers & Electrical Engineering},
volume = {92},
pages = {107187},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107187},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621001889},
author = {J. Fiaidhi and S. Mohammed},
keywords = {Thick Data, Prognosis Analysis, Fuzzy Clustering, Small Datasets, Healthcare Data, Risk Analysis, Machine Learning},
abstract = {Analyzing clinical data differs from other machine learning data analysis as most of the clinical data are relatively small requiring more qualitative techniques to bring focus to the context and then to predict important indicators like the patient risk in developing heart disease. The strength of qualitative analytics lies in data thickness as they can work on small samples and corpuses (“small data”). However, working with thick data analytics requires involving patient characteristics (e.g. socioeconomic status, family background, working conditions, social support, psycho-social characteristics, lifestyle risk factors, age group, gender and social capital) and their weights in a particular clinical practice. Therefore, the role of patient characteristics is not only a dominant factor in thick data analytics but it is also linked to predicting the prognosis of patient cases. A Fuzzy C-Means algorithm is presented as technique for prognostic predictions to identify risk groups associated with Cardiovascular Disease (CVD) conditions.}
}
@article{RUKANOVA2021101496,
title = {Identifying the value of data analytics in the context of government supervision: Insights from the customs domain},
journal = {Government Information Quarterly},
volume = {38},
number = {1},
pages = {101496},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101496},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20302756},
author = {Boriana Rukanova and Yao-Hua Tan and Micha Slegt and Marcel Molenhuis and Ben {van Rijnsoever} and Jonathan Migeotte and Mathieu L.M. Labare and Krunoslav Plecko and Bora Caglayan and Gavin Shorten and Otis {van der Meij} and Suzanne Post},
keywords = {Framework, Data analytics, Value, Government, Supervision, eCommerce, Capabilities, Collective capability},
abstract = {eCommerce, Brexit, new safety and security concerns are only a few examples of the challenges that government organisations, in particular customs administrations, face today when controlling goods crossing borders. To deal with the enormous volumes of trade customs administrations rely more and more on information technology (IT) and risk assessment, and are starting to explore the possibilities that data analytics (DA) can offer to support their supervision tasks. Driven by customs as our empirical domain, we explore the use of DA to support the supervision role of government. Although data analytics is considered to be a technological breakthrough, there is so far only a limited understanding of how governments can translate this potential into actual value and what are barriers and trade-offs that need to be overcome to lead to value realisation. The main question that we explore in this paper is: How to identify the value of DA in a government supervision context, and what are barriers and trade-offs to be considered and overcome in order to realise this value? Building on leading models from the information system (IS) literature, and by using case studies from the customs domain, we developed the Value of Data Analytics in Government Supervision (VDAGS) framework. The framework can help managers and policy-makers to gain a better understanding of the benefits and trade-offs of using DA when developing DA strategies or when embarking on new DA projects. Future research can examine the applicability of the VDAGS framework in other domains of government supervision.}
}
@article{LIU2021110601,
title = {A data mining-based framework for the identification of daily electricity usage patterns and anomaly detection in building electricity consumption data},
journal = {Energy and Buildings},
volume = {231},
pages = {110601},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110601},
url = {https://www.sciencedirect.com/science/article/pii/S0378778820333879},
author = {Xue Liu and Yong Ding and Hao Tang and Feng Xiao},
keywords = {Building energy management, Time series clustering, Decision tree, Knowledge discovery, Electricity usage pattern, Data mining},
abstract = {With the development of advanced information techniques, ﻿smart energy meters have made a considerable amount of real-time electricity consumption data available. These data provide a promising way to understand energy usage patterns and improve building energy management. However, previous studies have paid more attention to methodologies for the identification of energy usage patterns and are limited in the interpretability and applications of the patterns. In this context, this paper proposes a general data mining-based framework that can extract typical electricity load patterns (TELPs) and discover insightful information hidden in the patterns. The framework integrates multiple data mining techniques and mainly consists of three phases: data preparation, identification of TELPs and knowledge discovery in the patterns. A new clustering method with a two-step clustering analysis is proposed to identify the TELPs at the individual building level. Before clustering, five statistical features that represent the shapes of electricity load profiles are first defined to reduce the dimensions of daily electricity load profiles. The first clustering step aims at detecting outliers of daily electricity load profiles (DELPs) by using the density-based spatial clustering application with noise (DBSCAN) algorithm clustering technique, which addresses the data quality issues for electricity consumption data derived from energy consumption monitoring platforms (ECMPs). The second clustering step aims at grouping similar DELPs by means of the k-means algorithm to extract TELPs. The effectiveness of the proposed clustering method is demonstrated by a comparison with two single-step clustering techniques. Furthermore, a classification and regression tree (CART) algorithm is employed to discover insightful knowledge on TELPs and improve the interpretability of clustering results, namely, to explain the relations between dynamic influencing factors related to electricity consumption and TELPs. The proposed framework is applied to analyze the time-series electricity consumption data of three practical office buildings in Chongqing, and its effectiveness has been confirmed. A potential application of discovered knowledge is presented: early fault detection of anomalous electricity load profiles. The proposed framework can provide building managers with an efficient way to understand the characteristics of building electricity usage patterns and detect anomalies therein.}
}
@article{AMOS2021100331,
title = {The NanoInformatics Knowledge Commons: Capturing spatial and temporal nanomaterial transformations in diverse systems},
journal = {NanoImpact},
volume = {23},
pages = {100331},
year = {2021},
issn = {2452-0748},
doi = {https://doi.org/10.1016/j.impact.2021.100331},
url = {https://www.sciencedirect.com/science/article/pii/S2452074821000409},
author = {Jaleesia D. Amos and Yuan Tian and Zhao Zhang and Greg V. Lowry and Mark R. Wiesner and Christine Ogilvie Hendren},
keywords = {Database, Nanoinformatics, Nanomaterials, Environmental nanotechnology, Transformations},
abstract = {The empirical necessity for integrating informatics throughout the experimental process has become a focal point of the nano-community as we work in parallel to converge efforts for making nano-data reproducible and accessible. The NanoInformatics Knowledge Commons (NIKC) Database was designed to capture the complex relationship between nanomaterials and their environments over time in the concept of an ‘Instance’. Our Instance Organizational Structure (IOS) was built to record metadata on nanomaterial transformations in an organizational structure permitting readily accessible data for broader scientific inquiry. By transforming published and on-going data into the IOS we are able to tell the full transformational journey of a nanomaterial within its experimental life cycle. The IOS structure has prepared curated data to be fully analyzed to uncover relationships between observable phenomenon and medium or nanomaterial characteristics. Essential to building the NIKC database and associated applications was incorporating the researcher's needs into every level of development. We started by centering the research question, the query, and the necessary data needed to support the question and query. The process used to create nanoinformatic tools informs usability and analytical capability. In this paper we present the NIKC database, our developmental process, and its curated contents. We also present the Collaboration Tool which was built to foster building new collaboration teams. Through these efforts we aim to: 1) elucidate the general principles that determine nanomaterial behavior in the environment; 2) identify metadata necessary to predict exposure potential and bio-uptake; and 3) identify key characterization assays that predict outcomes of interest.}
}
@article{WANG2021629,
title = {GAPIT Version 3: Boosting Power and Accuracy for Genomic Association and Prediction},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {19},
number = {4},
pages = {629-640},
year = {2021},
note = {Bioinformatics Commons},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022921001777},
author = {Jiabo Wang and Zhiwu Zhang},
keywords = {GWAS, Genomic selection, Software, R, GAPIT},
abstract = {Genome-wide association study (GWAS) and genomic prediction/selection (GP/GS) are the two essential enterprises in genomic research. Due to the great magnitude and complexity of genomic and phenotypic data, analytical methods and their associated software packages are frequently advanced. GAPIT is a widely-used genomic association and prediction integrated tool as an R package. The first version was released to the public in 2012 with the implementation of the general linear model (GLM), mixed linear model (MLM), compressed MLM (CMLM), and genomic best linear unbiased prediction (gBLUP). The second version was released in 2016 with several new implementations, including enriched CMLM (ECMLM) and settlement of MLMs under progressively exclusive relationship (SUPER). All the GWAS methods are based on the single-locus test. For the first time, in the current release of GAPIT, version 3 implemented three multi-locus test methods, including multiple loci mixed model (MLMM), fixed and random model circulating probability unification (FarmCPU), and Bayesian-information and linkage-disequilibrium iteratively nested keyway (BLINK). Additionally, two GP/GS methods were implemented based on CMLM (named compressed BLUP; cBLUP) and SUPER (named SUPER BLUP; sBLUP). These new implementations not only boost statistical power for GWAS and prediction accuracy for GP/GS, but also improve computing speed and increase the capacity to analyze big genomic data. Here, we document the current upgrade of GAPIT by describing the selection of the recently developed methods, their implementations, and potential impact. All documents, including source code, user manual, demo data, and tutorials, are freely available at the GAPIT website (http://zzlab.net/GAPIT).}
}
@article{TENG2021110208,
title = {Recent advances on industrial data-driven energy savings: Digital twins and infrastructures},
journal = {Renewable and Sustainable Energy Reviews},
volume = {135},
pages = {110208},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.110208},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120304974},
author = {Sin Yong Teng and Michal Touš and Wei Dong Leong and Bing Shen How and Hon Loong Lam and Vítězslav Máša},
keywords = {Digital twins, Data-driven energy savings, Artificial intelligence (AI), Blockchain, Internet of things (IoT), Cyber-physical production systems (CPPS)},
abstract = {Data-driven models for industrial energy savings heavily rely on sensor data, experimentation data and knowledge-based data. This work reveals that too much research attention was invested in making data-driven models, as supposed to ensuring the quality of industrial data. Furthermore, the true challenge within the Industry 4.0 is with data communication and infrastructure problems, not so significantly on developing modelling techniques. Current methods and data infrastructures for industrial energy savings were comprehensively reviewed to showcase the potential for a more accurate and effective digital twin-based infrastructure for the industry. With a few more development in enabling technologies such as 5G developments, Internet of Things (IoT) standardization, Artificial Intelligence (AI) and blockchain 3.0 utilization, it is but a matter of time that the industry will transition towards the digital twin-based approach. Global government efforts and policies are already inclining towards leveraging better industrial energy efficiencies and energy savings. This provides a promising future for the development of a digital twin-based energy-saving system in the industry. Foreseeing some potential challenges, this paper also discusses the importance of symbiosis between researchers and industrialists to transition from traditional industry towards a digital twin-based energy-saving industry. The novelty of this work is the current context of industrial energy savings was extended towards cutting-edge technologies for Industry 4.0. Furthermore, this work proposes to standardize and modularize industrial data infrastructure for smart energy savings. This work also serves as a concise guideline for researchers and industrialists who are looking to implement advanced energy-saving systems.}
}
@article{LEE202127,
title = {Strava Metro data for bicycle monitoring: a literature review},
journal = {Transport Reviews},
volume = {41},
number = {1},
pages = {27-47},
year = {2021},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2020.1798558},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722000435},
author = {Kyuhyun Lee and Ipek Nese Sener},
keywords = {Strava, bicycle, crowdsourced data, fitness tracking application, emerging travel data},
abstract = {ABSTRACT
Monitoring bicycle trips is no longer limited to traditional sources, such as travel surveys and counts. Strava, a popular fitness tracker, continuously collects human movement trajectories, and its commercial data service, Strava Metro, has enriched bicycle research opportunities over the last five years. Accrued knowledge from colleagues who have already utilised Strava Metro data can be valuable for those seeking expanded monitoring options. To convey such knowledge, this paper synthesises a data overview, extensive literature review on how the data have been applied to deal with drivers’ bicycle-related issues, and implications for future work. The review results indicate that Strava Metro data have the potential—although finite—to be used to identify various travel patterns, estimate travel demand, analyse route choice, control for exposure in crash models, and assess air pollution exposure. However, several challenges, such as the under-representativeness of the general population, bias towards and away from certain groups, and lack of demographic and trip details at the individual level, prevent researchers from depending entirely on the new data source. Cross-use with other sources and validation of reliability with official data could enhance the potentiality.}
}
@article{NEGROCALDUCH2021104507,
title = {Technological progress in electronic health record system optimization: Systematic review of systematic literature reviews},
journal = {International Journal of Medical Informatics},
volume = {152},
pages = {104507},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104507},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621001337},
author = {Elsa Negro-Calduch and Natasha Azzopardi-Muscat and Ramesh S. Krishnamurthy and David Novillo-Ortiz},
keywords = {Medical informatics, Electronic health records, eHealth, Artificial intelligence, Blockchain, Phenotyping, Deep learning, Natural language processing},
abstract = {Background
The recent, rapid development of digital technologies offers new possibilities for more efficient implementation of electronic health record (EHR) and personal health record (PHR) systems. A growing volume of healthcare data has been the hallmark of this digital transformation. The large healthcare datasets' complexity and their dynamic nature pose various challenges related to processing, analysis, storage, security, privacy, data exchange, and usability.
Materials and Methods
We performed a systematic review of systematic reviews to assess technological progress in EHR and PHR systems. We searched MEDLINE, Cochrane, Web of Science, and Scopus for systematic literature reviews on technological advancements that support EHR and PHR systems published between January 1, 2010, and October 06, 2020.
Results
The searches resulted in a total of 2,448 hits. Of these, we finally selected 23 systematic reviews. Most of the included papers dealt with information extraction tools and natural language processing technology (n = 10), followed by studies that assessed the use of blockchain technology in healthcare (n = 8). Other areas of digital technology research included EHR and PHR systems in austere settings (n = 1), de-identification methods (n = 1), visualization techniques (n = 1), communication tools within EHR and PHR systems (n = 1), and methodologies for defining Clinical Information Models that promoted EHRs and PHRs interoperability (n = 1).
Conclusions
Technological advancements can improve the efficiency in the implementation of EHR and PHR systems in numerous ways. Natural language processing techniques, either rule-based, machine-learning, or deep learning-based, can extract information from clinical narratives and other unstructured data locked in EHRs and PHRs, allowing secondary research (i.e., phenotyping). Moreover, EHRs and PHRs are expected to be the primary beneficiaries of the blockchain technology implementation on Health Information Systems. Governance regulations, lack of trust, poor scalability, security, privacy, low performance, and high cost remain the most critical challenges for implementing these technologies.}
}
@article{XU2021103828,
title = {Spatiotemporal forecasting in earth system science: Methods, uncertainties, predictability and future directions},
journal = {Earth-Science Reviews},
volume = {222},
pages = {103828},
year = {2021},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2021.103828},
url = {https://www.sciencedirect.com/science/article/pii/S0012825221003299},
author = {Lei Xu and Nengcheng Chen and Zeqiang Chen and Chong Zhang and Hongchu Yu},
keywords = {Spatiotemporal forecasting, Artificial intelligence, Physical model, Uncertainty modeling, Predictability},
abstract = {Spatiotemporal forecasting (STF) extends traditional time series forecasting or spatial interpolation problem to space and time dimensions. Here, we review the statistical, physical and artificial intelligence (AI) methods, data and model uncertainties, predictability and future directions for STF problems. Statistical STF methods have limitations in high-level feature extractions and long-term memory modeling. Physical models are computationally intensive and are imperfect in model structure and parameterization. AI models lack the interpretability and require elaborate training but can model complex nonlinear and non-Gaussian problems. Integrating data-driven and physical model-driven methods could facilitate the improvement of interpretability and forecasting accuracy. The predictive uncertainty comes from data and models, which could be measured by probability distribution and Bayesian inference, respectively. The predictive uncertainty is generally missing in AI models and could be resolved by incorporating Bayesian frameworks. The predictability of dynamic earth systems is spatiotemporally heterogeneous and is generally examined by diagnostic and prognostic approaches. Diagnostic methods analyze the predictability empirically from a theoretical perspective, while prognostic methods investigate the predictability through real experiments. Unraveling the predictability in space and time and the predictability sources will greatly improve earth system understanding and operational forecasting development. Current STF systems are largely not user-friendly to provide probabilistic and understandable forecasting services in near real-time. Intelligent STF systems should automatically prepare various data sources, train the models in a self-adaptative way and provide timely predictive information services for users to make decisions. This review provides state-of-the-art advances in forecasting sciences and highlights new directions for new-generation STF systems.}
}
@article{CABRERA2021105069,
title = {Future of dairy farming from the Dairy Brain perspective: Data integration, analytics, and applications},
journal = {International Dairy Journal},
volume = {121},
pages = {105069},
year = {2021},
issn = {0958-6946},
doi = {https://doi.org/10.1016/j.idairyj.2021.105069},
url = {https://www.sciencedirect.com/science/article/pii/S0958694621000972},
author = {Victor E. Cabrera and Liliana Fadul-Pacheco},
abstract = {Data integration is one of the biggest challenges the dairy industry faces nowadays due to increased number of technologies and data overflow at the farm. Here we review the current situation of precision dairy farming technologies that use integrated data and its application on the management decision process at the dairy farm. The most common data connections were those from activity monitors, dairy herd improvement records, herd management, and milking recordings. Algorithms used can be defined in general as artificial intelligence and machine learning approaches. Most of the 22 revised papers, research or review, demonstrated that applying different algorithms to integrated data provides additional and complementary insights to improve decision-making tools and therefore enhance economic, management and animal welfare, and hence the sustainability of dairy farms. All revised studies acknowledge the importance of live data integration to develop relevant decision support tools to improve decision making.}
}
@article{BARATA2021101624,
title = {The fourth industrial revolution of supply chains: A tertiary study},
journal = {Journal of Engineering and Technology Management},
volume = {60},
pages = {101624},
year = {2021},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2021.101624},
url = {https://www.sciencedirect.com/science/article/pii/S0923474821000138},
author = {João Barata},
keywords = {4SC, Supply Chain 4.0, Industry 4.0, Fourth Industrial Revolution, Tertiary study},
abstract = {This paper unfolds the ongoing fourth revolution of supply chains (4SC) and proposes guidelines for future research. The review of sixty-five literature reviews follows three stages: bibliometric analysis of Industry 4.0, its synergies with supply chain transformation, and state-of-the-art assessment. 4SC is a context-bound technological change driven by organizational and cultural priorities, aiming to create more sustainable networks to serve the customers and support responsible decisions in the supply lifecycle. The proposed framework can assist future literature reviews and digital transformation proposals for 4SC that need to frame their context and incorporate functions to endure change.}
}
@incollection{2021vii,
title = {Contents},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {vii-xii},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00015-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000152}
}
@incollection{2021347,
title = {Index},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {347-355},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.09992-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150099928}
}
@incollection{CHINOY2021,
title = {Use of technology for real-world sleep and circadian research},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2021},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-822963-7.00200-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229637002000},
author = {Evan D. Chinoy and Rachel R. Markwald},
keywords = {Actigraphy, Consumer sleep technology, Mobile EEG, Naturalistic settings, Nearables, Non-contact sensors, Polysomnography, Sleep monitoring, Wearables, Validation},
abstract = {Recent advances in technology and demand for biometric data have led to the creation of personal devices that track sleep and other physiological and behavioral patterns with increasing accuracy. Although such technologies have widespread use among the general population, applications for sleep and circadian research show much promise, but their current adoption has been slow in part due to the need for validation versus standard research methodologies. This article outlines the current state of sleep and circadian technologies for real-world research, their strengths and limitations, recommended standards for use, current operational use cases, and future directions for real-world applications.}
}
@article{BAG2021107844,
title = {Industry 4.0 adoption and 10R advance manufacturing capabilities for sustainable development},
journal = {International Journal of Production Economics},
volume = {231},
pages = {107844},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107844},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320302103},
author = {Surajit Bag and Shivam Gupta and Sameer Kumar},
keywords = {Industry 4.0, Circular economy, Sustainable development, Practice based view, Dynamic capability view, Advanced manufacturing, I4.0 delivery system},
abstract = {Industry 4.0 technologies provide digital solutions for the automation of manufacturing. In circular economy-based models, the resources stay in the system as it experiences one of the 10 R (Refuse, Rethink, Reduce, Reuse, Repair, Refurbish, Remanufacture, Repurpose, Recycle, and Recover) processes. These 10 R processes require the development of advanced manufacturing capabilities; however, 10 R processes suffer from various challenges and can be effectively overcome through Industry 4.0 technological applications. Although literature has indicated the use of various Industry 4.0 technologies, little information is available about firms’ views on the degree of Industry 4.0 application in the 10 R based advanced manufacturing area and its ability to achieve sustainable development. The current study aspires to examine how great an effect Industry 4.0 adoption has on 10 R advanced manufacturing capabilities and its outcome on sustainable development under the moderating effect of an Industry 4.0 delivery system. Practice-based view and Dynamic capability view theories are used to conceptualise the theoretical model. The research team statistically validated the theoretical model considering 124 data points that were collected using an online survey with a structured questionnaire. The findings point out that the path degree of Industry 4.0 adoption and 10 R advanced manufacturing capabilities are statistically significant. 10 R advanced manufacturing capabilities are found to have a positive influence on sustainable development outcomes. Industry 4.0 delivery system has a moderating effect on the path degree of I4.0 implementation and 10 R advanced manufacturing capabilities. The study concludes with key take away points for managers.}
}
@article{HALEEM202171,
title = {Quality 4.0 technologies to enhance traditional Chinese medicine for overcoming healthcare challenges during COVID-19},
journal = {Digital Chinese Medicine},
volume = {4},
number = {2},
pages = {71-80},
year = {2021},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2589377721000161},
author = {Abid Haleem and Mohd Javaid and Ravi Pratap Singh and Rajiv Suman},
keywords = {Quality 4.0, COVID-19, Digital healthcare, Traditional Chinese medicine (TCM), Industry 4.0, Quality revolution},
abstract = {The Quality 4.0 concept is derived from the industrial fourth revolution, i.e., Industry 4.0. Quality 4.0 is the future of quality, where new digital and disruptive technologies are used to maintain quality in organizations. It is also suitable for traditional Chinese medicine (TCM) to maintain quality. This quality revolution aims to improve industrial and service sectors’ quality by incorporating emerging technologies to connect physical systems with the natural world. The proposed digital philosophy can update and enhance the entire TCM treatment methodology to become more effective and attractive in the current competitive structure of the pharmaceutical and clinical industries. Thus, in healthcare, this revolution empowers quality treatment during the COVID-19 pandemic. There is a major requirement in healthcare to maintain the quality of medical tools, equipment, and treatment processes during a pandemic. Digital technologies can widely be used to provide innovative products and services with excellent quality for TCM. In this paper, we discuss the significant role of Quality 4.0 and how it can be used to maintain healthcare quality and fulfill challenges during the pandemic. Additionally, we discuss 10 significant applications of Quality 4.0 in healthcare during the COVID-19 pandemic. These technologies will provide unique benefits to maintain the quality of TCM throughout the treatment process. With Quality 4.0, quality can be maintained using innovative and advanced digital technologies.}
}
@article{LI2021,
title = {Entropy-based redundancy analysis and information screening},
journal = {Digital Communications and Networks},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864821000985},
author = {Yang Li and Jiachen Yang and Jiabao Wen},
keywords = {Sampling, Quality assessment, Data mining, Low-shot, Few-shot},
abstract = {The ongoing data explosion introduced unprecedented challenges to the information security of communication networks. As images are one of the most commonly used information transmission carriers; therefore, their data redundancy analysis and screening are of great significance. However, most of the current research focus on the algorithm improvement of commonly used image datasets. Thus, we should consider an important question: Is there data redundancy in the open datasets? Considering the factors of model structures and data distribution to ensure the generalization, we conducted extensive experiments to compare the average accuracy based on few random data to the baseline accuracy based on all data. The results show serious data redundancy in the open datasets from different domains. For instance, with the aid of deep model, only 20% data can achieve more than 90% of the baseline accuracy. Further, we proposed a novel entropy-based information screening method, which outperforms the random sampling under many experimental conditions. In particular, considering 20% of data, for the shallow model, the improvement is approximately 10%, and for the deep model, the ratio to the baseline accuracy increases to greater than 95%. Moreover, this work can also serve as a new way of learning from a few valuable samples, compressing the size of existing datasets and guiding the construction of high-quality datasets in the future.}
}
@article{REIS2021107529,
title = {Data-centric process systems engineering: A push towards PSE 4.0},
journal = {Computers & Chemical Engineering},
volume = {155},
pages = {107529},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107529},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421003070},
author = {Marco S. Reis and Pedro M. Saraiva},
keywords = {Process systems engineering 4.0, Data-centric PSE, Data science, Industry 4.0, Artificial intelligence, Applied statistics},
abstract = {Process Systems Engineering (PSE) is now a mature field with a well-established body of knowledge, computational-oriented frameworks and methodologies designed and implemented for addressing chemical processes related problems spanning a wide range of scales in time and space. A common feature of many PSE approaches relies in their mostly deductive nature, based on a deep understanding of the underlying Chemical Engineering Science. Given the current data-intensive industrial and societal contexts, new sources of process or product information are now easily made available and should be exploited to complement and expand the classical PSE paradigm with inductive data-driven reasoning and knowledge discovery methodologies. In this article, based upon our over 25 years of research and teaching experience in the field, we discuss the scope and trends of this PSE evolution, refer to several relevant Data-Centric PSE approaches, and identify the main components, applications and future opportunities of this PSE 4.0 perspective.}
}
@article{JESSE20218,
title = {Data Strategy and Data Trust – Drivers for Business Development},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {8-12},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.409},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321018462},
author = {Norbert Jesse},
keywords = {Innovation Management, Intelligent Systems, Applications},
abstract = {Data is the new oil – which has often been heard since the end of the 90th. However, many companies took too long to exploit the new technological options for their business. It has been proven that the digital disruption led to seemingly stable and well-known brands disappearing from their market. Insufficient data competence is one of the reasons why companies failed in the escalating process of creative destruction. In this document we address three competence dimensions: data architecture, data preparation and the interchange of data. The competence in these fields is a precondition for a company’s decision-making process to lead successfully to profitability.}
}
@article{SIMONART202169,
title = {Epidemiologic evolution of common cutaneous infestations and arthropod bites: A Google Trends analysis},
journal = {JAAD International},
volume = {5},
pages = {69-75},
year = {2021},
issn = {2666-3287},
doi = {https://doi.org/10.1016/j.jdin.2021.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666328721000651},
author = {Thierry Simonart and Xuân-Lan {Lam Hoai} and Viviane {De Maertelaer}},
keywords = {bed bugs, head lice, infodemiology, pubic lice, scabies, ticks, Google Trends},
abstract = {Background
Common cutaneous infestations and arthropod bites are not reportable conditions in most countries. Their worldwide epidemiologic evolution and distribution are mostly unknown.
Objective
To explore the evolution and geographic distribution of common cutaneous infestations and arthropod bites through an analysis of Google Trends.
Methods
Search trends from 2004 through March 2021 for common cutaneous infestations and arthropod bites were extracted from Google Trends, quantified, and analyzed.
Results
Time series decomposition showed that total search term volume for pubic lice decreased worldwide over the study period, while the interest for ticks, pediculosis, insect bites, scabies, lice, and bed bugs increased (in increasing order). The interest for bed bugs was more pronounced in the former Union of Soviet Socialist Republics countries, interest for lice in Near East and Middle East countries, and interest for pubic lice in South American countries. Internet searches for bed bugs, insect bites, and ticks exhibited the highest seasonal patterns.
Limitations
Retrospective analysis limits interpretation.
Conclusion
Surveillance systems based on Google Trends may enhance the timeliness of traditional surveillance systems and suggest that, while most cutaneous infestations increase worldwide, pubic lice may be globally declining.}
}
@article{HUANG2021586,
title = {Toward a research framework to conceptualize data as a factor of production: The data marketplace perspective},
journal = {Fundamental Research},
volume = {1},
number = {5},
pages = {586-594},
year = {2021},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2021.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S2667325821001515},
author = {Lihua Huang and Yifan Dou and Yezheng Liu and Jinzhao Wang and Gang Chen and Xiaoyang Zhang and Runyin Wang},
keywords = {Data marketplace, Factor of production, Data lifecycle, Asset specificity},
abstract = {The widespread use of machine learning techniques and artificial intelligence algorithms has highlighted the strategic role of data. To acquire data for training algorithms and eventually empowering the digital transformation, data marketplaces are often required to support and coordinate cross-organizational data transactions. However, the prior industry practices have suggested that the transaction costs in the data marketplaces are severely high, and the supporting infrastructure is far from mature. This paper proposes a data attributes-affected data exchange (DADE) conceptual model to understand the challenges and directions for developing data marketplaces. Specifically, our model framework is built upon two dimensions, data lifecycle maturity and data asset specificity. Based on the DADE model, we propose four approaches for developing data marketplaces and discuss future research directions with an overview of computational methods as potential technical solutions.}
}
@article{GHANEM2021615,
title = {Comment on: High acquisition rate and internal validity in the Scandinavian Obesity Surgery Registry},
journal = {Surgery for Obesity and Related Diseases},
volume = {17},
number = {3},
pages = {615-617},
year = {2021},
issn = {1550-7289},
doi = {https://doi.org/10.1016/j.soard.2020.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1550728920306572},
author = {Omar M. Ghanem and Joseph N. Badaoui}
}
@article{LIM2021100013,
title = {State of data platforms for connected vehicles and infrastructures},
journal = {Communications in Transportation Research},
volume = {1},
pages = {100013},
year = {2021},
issn = {2772-4247},
doi = {https://doi.org/10.1016/j.commtr.2021.100013},
url = {https://www.sciencedirect.com/science/article/pii/S2772424721000135},
author = {Kai Li Lim and Jake Whitehead and Dongyao Jia and Zuduo Zheng},
keywords = {Connected mobility, Vehicular networks, Data platforms, Electric vehicles},
abstract = {The continuing expansion of connected and electro-mobility products and services has led to their ability to rapidly generate very large amounts of data, leading to a demand for effective data management solutions. This is further catalysed through the need for society to make informed policies and decisions that can properly support their emerging growth. While data systems and platforms exist, they are often proprietary, being only compatible to the products that they are designed for. Given the products and services generate energy and spatial-temporal data that can often correlate, a lack of interoperability between these systems would impede decision making, as data from each system must be considered independently. By studying currently available data platforms and frameworks, this paper weighs the problems that these products address, and identifies necessary gaps for a more cohesive platform to exist. This is performed through a top-down approach, whereby broader vehicle-to-everything approaches are first studied, before moving to the components that could comprise a data platform to integrate and ingest these various data feeds. Finally, potential design considerations for a data platform is presented, along with examples of application benefits that would enable users to make more informed and holistic decisions about current mobility options.}
}
@article{BABAR2021100992,
title = {Energy aware smart city management system using data analytics and Internet of Things},
journal = {Sustainable Energy Technologies and Assessments},
volume = {44},
pages = {100992},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.100992},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821000023},
author = {Muhammad Babar and Akmal Saeed Khattak and Mian Ahmad Jan and Muhammad Usman Tariq},
keywords = {Smart city, Internet of Things, Energy management, Data analytics},
abstract = {The rise of Internet of Things (IoT) concept founded the realization of a smart city. Energy management has lately turn out to be a vital concern for the services of a smart city as IoT devices consume massive energy constantly. This concern needs to be addressed to devise a practical method. Efficient energy utilization aims to promise smart city sustainability. Moreover, IoT devices produce enormous data that is required to be processed efficiently. In this article, a framework is proposed that assures the energy efficiency of IoT devices along with data analysis for cities. This article proposes a general design for smart city energy management that assures the energy efficiency of IoT devices along with data analysis. The proposed model includes three different components that are energy management, data processing, and service management. The energy management component is dependent on infrastructure optimization. The energy-efficient clustering, peak load shaving, optimized scheduling, and load balancing algorithms are integrated to achieve efficient energy management. The data processing is performed using distributed framework. The service management is performed using rules and thresholds. The experiments are performed using authentic datasets and the result highlights the efficiency of the proposed model.}
}
@article{MARROQUINRIVERA2021110,
title = {Implementing a Redcap-based research data collection system for mental health},
journal = {Revista Colombiana de Psiquiatría (English ed.)},
volume = {50},
pages = {110-115},
year = {2021},
issn = {2530-3120},
doi = {https://doi.org/10.1016/j.rcpeng.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S2530312021000588},
author = {Arturo {Marroquin Rivera} and Juan Camilo Rosas-Romero and Sergio Mario Castro and Fernando Suárez-Obando and Jeny Aguilera-Cruz and Sophia Marie Bartels and Sena Park and William Chandler Torrey and Carlos Gómez-Restrepo},
keywords = {Data collection, REDCap, Mental health, Recolección de datos, REDCap, Salud mental},
abstract = {Background
The implementation of new technologies in medical research, such as novel big storage systems, has recently gained importance. Electronic data capture is a perfect example as it powerfully facilitates medical research. However, its implementation in resource-limited settings, where basic clinical resources, internet access, and human resources may be reduced might be a problem.
Methods
In this paper we described our approach for building a network architecture for data collection to achieve our objectives using a REDCap® tool in Colombia and provide guidance for data collection in similar settings.
Conclusions
REDCap is a feasible and efficient electronic data capture software to use in similar contexts to Colombia. The software facilitated the whole data management process and is a way to build research capacities in resourced-limited settings.
Resumen
Contexto
La implementación de nuevas tecnologías en la investigación médica, como los nuevos sistemas de gran almacenamiento de datos, recientemente han ganado importancia. El almacenamiento electrónico de datos es un ejemplo perfecto ya que facilita poderosamente la investigación médica. Sin embargo, su implementación en ambientes con recursos limitados, donde los recursos básicos clínicos, el acceso a internet y el recurso humano podrían ser reducidos, suponen un problema.
Métodos
En este artículo describimos nuestro acercamiento para construir una red arquitectónica para la recolección de datos, en aras de alcanzar nuestros objetivos mediante la utilización de la herramienta REDCap® en Colombia y proveer una guía para la recolección de datos en condiciones similares.
Conclusiones
REDCap es un software de almacenamiento electrónico de datos eficiente y encontramos que resulta posible su utilización en contextos similares a Colombia. Este software facilitó el proceso del manejo de los datos y es una manera de construir capacidades investigativas en contextos donde los recursos son limitados.}
}
@article{GOKALP2021527,
title = {Data-driven manufacturing: An assessment model for data science maturity},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {527-546},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001485},
author = {Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Altan Koçyiğit and P. Erhan Eren},
keywords = {Smart manufacturing, Industry 4.0, Maturity model, Data science, Maturity assessment, Process improvement},
abstract = {Today, data science presents immense opportunities by turning raw data into manufacturing intelligence in data-driven manufacturing that aims to improve operational efficiency and product quality together with reducing costs and risks. However, manufacturing firms face difficulties in managing their data science endeavors for reaping these potential benefits. Maturity models are developed to guide organizations by providing an extensive roadmap for improvement in certain areas. Therefore, this paper seeks to address this problem by proposing a theoretically grounded Data Science Maturity Model (DSMM) for manufacturing organizations to assess their existing strengths and weaknesses, perform a gap analysis, and draw a roadmap for continuous improvements in their progress towards data-driven manufacturing. DSMM comprises six maturity levels from “Not Performed” to” Innovating” and twenty-eight data science processes categorized under six headings: Organization, Strategy Management, Data Analytics, Data Governance, Technology Management, and Supporting. The applicability and usefulness of DSMM are validated through multiple case studies conducted in manufacturing organizations of various sizes, industries, and countries. The case study results indicate that DSMM is applicable in different settings and is able to reflect the organizations’ current data science maturity levels and provide significant insights to improve their data science capabilities.}
}
@incollection{2021295,
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {295-299},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000164}
}
@article{CHEN2021115699,
title = {Application of data-driven models to predictive maintenance: Bearing wear prediction at TATA steel},
journal = {Expert Systems with Applications},
volume = {186},
pages = {115699},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115699},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421010836},
author = {X. Chen and Jos {Van Hillegersberg} and E. Topan and S. Smith and M. Roberts},
keywords = {Predictive maintenance, Industry 4.0, Data-driven, Machine learning},
abstract = {Industries that are in transition to Industry 4.0 often face challenges in applying data-driven methods to improve performance. While ample methods are available in literature, knowledge on how to select and apply them is scarce. This study aims to address this gap reported on the design and implementation of data-driven models for predictive maintenance at TATA Steel, Shotton. The objective of the project is to predict the wearing behaviour of the components in the steel production line for maintenance activity decision support. To achieve the predictive maintenance goal, the approach applied can be summarized as follows: 1. business understanding and data collection, 2. literature review, 3. data preparation and exploration, 4. modelling and result analysis and 5. conclusion and recommendation. The data-driven methods that were analysed and compared are: Partial Least Squares Regression (PLSR), Artificial Neu- ral Network (ANN) and Random Forest(RF). After cleaning and analysing the production line data, predictive maintenance with the current available data in TATA Steel, Shotton is best feasible with PLSR. The study further concludes that, predictive maintenance is likely to be feasible in similar industries that are in transition to industry 4.0 and have growing volumes of production data with varying quality and detail. However, as illustrated in this case study, careful understanding of the industrial process, thorough modeling and cleaning of the data as well as careful method selection and tuning are required. Moreover, the resulting model needs to be packaged in a user friendly way to find its way to the job floor.}
}
@article{BELTRAMI2021127733,
title = {Industry 4.0 and sustainability: Towards conceptualization and theory},
journal = {Journal of Cleaner Production},
volume = {312},
pages = {127733},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127733},
url = {https://www.sciencedirect.com/science/article/pii/S095965262101951X},
author = {Mirjam Beltrami and Guido Orzes and Joseph Sarkis and Marco Sartor},
keywords = {Industry 4.0, Fourth industrial revolution, Internet of things, Sustainability, Conceptual framework},
abstract = {Both Industry 4.0 and sustainability have gained momentum in the academic, managerial and policy debate. Despite the relevance of the topics, the relation between Industry 4.0 and sustainability – revealed by many authors – is still unclear; literature is fragmented. This paper seeks to overcome this limit by developing a systematic literature review of 117 peer-reviewed journal articles. After descriptive and content analyses, the work presents a conceptualization and theoretical framework. The paper contributes to both theory and practice by advancing current understanding of Industry 4.0 and sustainability, especially the impact of Industry 4.0 technologies on sustainability practices and performance.}
}
@article{DAI2021110480,
title = {Advanced battery management strategies for a sustainable energy future: Multilayer design concepts and research trends},
journal = {Renewable and Sustainable Energy Reviews},
volume = {138},
pages = {110480},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.110480},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120307668},
author = {Haifeng Dai and Bo Jiang and Xiaosong Hu and Xianke Lin and Xuezhe Wei and Michael Pecht},
keywords = {Lithium-ion batteries, Battery management technologies, Multilayer design concepts, Safety and aging, Data and intelligence},
abstract = {Lithium-ion batteries are promising energy storage devices for electric vehicles and renewable energy systems. However, due to complex electrochemical processes, potential safety issues, and inherent poor durability of lithium-ion batteries, it is essential to monitor and manage batteries safely and efficiently. This study reviews the development of battery management systems during the past periods and introduces a multilayer design architecture for advanced battery management, which consists of three progressive layers. The foundation layer focuses on the system physical basis and theoretical principle, the algorithm layer aims at providing a comprehensive understanding of battery, and the application layer ensures a safe and efficient battery system through sufficient management. A comprehensive overview of each layer is presented from both academic and engineering perspectives. Future trends in research and development of next-generation battery management are discussed. Based on data and intelligence, the next-generation battery management will achieve better safety, performance, and interconnectivity.}
}
@article{HADJSASSI202129,
title = {Knowledge Management Process for Air Quality Systems based on Data Warehouse Specification},
journal = {Procedia Computer Science},
volume = {192},
pages = {29-38},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014915},
author = {Mohamed Saifeddine {Hadj Sassi} and Lamia {Chaari Fourati} and Manel Zekri and Sadok {Ben Yahia}},
keywords = {Knowledge Management, Air Quality, Data Warehouse, Conceptual Data Model, Multidimensional Design, Ontology.},
abstract = {Even though several systems for Air Quality (AQ) monitoring have been in existence for over a decade, a research model for Knowledge Management (KM) of AQ data has to be created in order to enhance the decision-making and organize the air quality data collected from the Internet of Things (IoT) consumer devices. This model should be made more performant by ensuring greater flexibility and interoperability between devices and emerging technologies. In this context, we propose an approach for representing Data WareHouse (DWH) schema based on an ontology that captures the multidimensional knowledge of tools, techniques, and technologies used for novel AQ systems. This enhances decision-making by coping with potential problems such as data sources heterogeneity and covering the various phases of the decision-making life cycle.}
}
@article{RAKHRA2021,
title = {Smart data in innovative farming},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.01.237},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321003266},
author = {Manik Rakhra and Ramandeep Singh},
keywords = {Smart agriculture, Innovative farming, Ecosystem, Business, System},
abstract = {Smart agriculture is an advancement that aims the use of cyber-physical agricultural management of knowledge and communication technologies. This development will be leveraged and introduced more artificial intelligence and robotics in agriculture by new technologies like smart phones such as the Internet of Things. The vast quantities of data, which can be stored, analyzed and can be further used for decision-support system of the Smart Data Phenomena. The main focus of this study is to think about smart technologies and other challenges. A philosophical research framework has been established using a systematic methodology that in future studies on this subject may also be used. The analysis reveals that the scale of Smart Data technologies in the field of innovative farming goes beyond primary production. Smart information is used to provide predictive knowledge into agricultural system, to make organizational decisions in real time, and to reinvent business processes with evolving business models. The stakeholder ecosystem shows an exciting game involving large development firms, risk investors and also tiny beginning businesses and new entrants. Simultaneously, many public institutions post open data, so that the protection of people is ensured. In a continuous series of two extreme scenarios the future of Innovative farming may unravel: 1) closing proprietary systems in which farmers are included in the Hoch integrated food supply chain or 2) partnership structures that flexibly define business associates for the development and food processing industries for all the stakeholders in the chain process. In the battle among these situations, the growth and the operational adoption of knowledge and technology systems may play an important role. The authors propose to focus on gathering the data from the farmers and creation of effective smart information system for the exchange of Equipment’s and tools in different supply chain situations from a social-economic perspective.}
}
@article{GAN2021100845,
title = {Capturing the swarm intelligence in truckers: The foundation analysis for future swarm robotics in road freight},
journal = {Swarm and Evolutionary Computation},
volume = {62},
pages = {100845},
year = {2021},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2021.100845},
url = {https://www.sciencedirect.com/science/article/pii/S2210650221000067},
author = {Mi Gan and Qiujun Qian and Dandan Li and Yi Ai and Xiaobo Liu},
keywords = {Swarm Intelligence, Prediction, Trucker Trajectory, Machine learning},
abstract = {A group of individual truckers can be regarded as a swarm intelligence system without central management. With the development of autonomous driving technology, trucker groups will be replaced by driverless vehicles. At that point, a swarm of truckers will become a swarm robotics system. Therefore, considering the design and control of an efficient swarm robotics system, it is essential to investigate the properties and model the behaviors of a swarm of truckers in advance. In this study, we probe the characteristics of both individual truckers and a swarm of truckers using trajectory data of truckers. First, the trajectory data were map matched based on the geographic scale of cities and administrative regions. Then, the properties of the division of labor, pattern formation, and swarm synchronization were obtained through an analysis of the spatiotemporal distribution of radius of gyration, travel distance, and the number of visited places. Because predicting the next visit locations of individuals of a swarm is a measure for modeling swarm behaviors, the prediction model can be used to predict future swarm robotics (driverless trucks) behaviors. Thus, we apply several machine learning models to predict the next locations of truckers. The results show that there are common characteristics and routines embodied in the behavior of the truckers; the swarm shows consistency and regularity. Moreover, the peak predictability of the entire group reached 94%, indicating that our model can predict the behavior of groups and individuals. Our findings provide basis supporting to the future efficient swarm robotics system.}
}
@article{MEI2021134,
title = {Effects of obstructive sleep apnoea severity on neurocognitive and brain white matter alterations in children according to sex: a tract-based spatial statistics study},
journal = {Sleep Medicine},
volume = {82},
pages = {134-143},
year = {2021},
issn = {1389-9457},
doi = {https://doi.org/10.1016/j.sleep.2020.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1389945720303853},
author = {Lin Mei and Xiaodan Li and Guifei Zhou and Tingting Ji and Jun Chen and Zhifei Xu and Yun Peng and Yue Liu and Hongbin Li and Jie Zhang and Shengcai Wang and Yamei Zhang and Wentong Ge and Yongli Guo and Yue Qiu and Xinbei Jia and Jinghong Tian and Li Zheng and Jiangang Liu and Jun Tai and Xin Ni},
keywords = {Diffusion tensor imaging, Obstructive sleep apnoea, Children, Tract-based spatial statistics, White matter},
abstract = {Objectives
To investigate alterations in neurocognitive, attention, paediatric sleep questionnaire (PSQ) scores and whole brain white matter (WM) integrity between children with mild and severe obstructive sleep apnoea (OSA) according to sex and whether these changes are associated with OSA severity.
Methods
Fifty-seven children (36 males and 21 females) diagnosed with OSA were recruited for this study. Children of both sexes were divided into mild (male-MG, female-MG) and severe (male-SG, female-SG) groups according to OSA severity. Polysomnography (PSG), neurocognitive, attention and PSQ tests were compared between groups by one-way samples analysis of variance (ANOVA) F test. Diffusion tensor imaging (DTI) was scanned using a 3T GE MRI scanner and analysed by Tract-based Spatial Statistics (TBSS). Spearman correlation was calculated between DTI Eigenvalues and clinical characteristics.
Results
Compared to mild OSA patients, severe OSA patients presented greater severity of obstructive apnoea hypopnea index (OAHI), neurocognition, PSQ and attention tests in both male and female patients. Brain WM integrity in the male-SG, compared to the male-MG, demonstrated significantly reduced fractional anisotropy (FA) values in the right middle frontal gyrus and the right frontal sub-gyral regions and increased axial diffusivity (AD) values in the right inferior frontal gyrus, left parietal angular gyrus and sub-gyral regions, while no differences were found between the female-MG and female-SG. Alterations in male-SG brain regions were observably correlated with severity in male OSA patients.
Conclusions
The integrity of WM, which regulates autonomic, cognitive, and attention functions, is impaired in male, but not female, children with severe OSA.}
}
@article{SCHROER2021526,
title = {A Systematic Literature Review on Applying CRISP-DM Process Model},
journal = {Procedia Computer Science},
volume = {181},
pages = {526-534},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.199},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002416},
author = {Christoph Schröer and Felix Kruse and Jorge Marx Gómez},
keywords = {CRISP-DM, Literature Review, Data Mining, Process Methodology, Deployment},
abstract = {CRISP-DM is the de-facto standard and an industry-independent process model for applying data mining projects. Twenty years after its release in 2000, we would like to provide a systematic literature review of recent studies published in IEEE, ScienceDirect and ACM about data mining use cases applying CRISP-DM. We give an overview of the research focus, current methodologies, best practices and possible gaps in conducting the six phases of CRISP-DM. The main findings are that CRISP-DM is still a de-factor standard in data mining, but there are challenges since the most studies do not foresee a deployment phase. The contribution of our paper is to identify best practices and process phases in which data mining analysts can be better supported. Further contribution is a template for structuring and releasing CRISP-DM studies.}
}
@incollection{AZEROUAL2021169,
title = {18 - Trustworthy or not? Research data on COVID-19 in data repositories},
editor = {David Baker and Lucy Ellis},
booktitle = {Libraries, Digital Information, and COVID},
publisher = {Chandos Publishing},
pages = {169-182},
year = {2021},
series = {Chandos Digital Information Review},
isbn = {978-0-323-88493-8},
doi = {https://doi.org/10.1016/B978-0-323-88493-8.00027-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884938000276},
author = {Otmane Azeroual and Joachim Schöpfel},
keywords = {Research data, Data repository, Data quality, Trustworthiness, Open access, Open science, COVID-19},
abstract = {The outburst of the COVID-19 pandemic has boosted the need for seamless, unrestricted, fast, and free access to the latest research results on the virus, on its treatment, prevention, protocols, and so on. Open access to publications and research data, suddenly, became self-evident, not only for researchers in life and medical sciences but also for politicians, journalists, and society as a whole. At the same time, this sudden awareness triggered another debate on the quality and, moreover, the trustworthiness of this mass of information made available most often without any form of quality control (peer review). Thousands of datasets from research on COVID-19 and related topics have already been deposited on data repositories. Our chapter discusses the issue of the quality and trustworthiness of research data in data repositories using examples from the ongoing pandemic. It offers insights into some fundamental concepts and summarizes recommendations for quality assurance and evaluation of research data.}
}
@article{LINKE2021579,
title = {Shared and Anxiety-Specific Pediatric Psychopathology Dimensions Manifest Distributed Neural Correlates},
journal = {Biological Psychiatry},
volume = {89},
number = {6},
pages = {579-587},
year = {2021},
note = {Early Developmental Antecedents of Mood Disorders},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2020.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0006322320320436},
author = {Julia O. Linke and Rany Abend and Katharina Kircanski and Michal Clayton and Caitlin Stavish and Brenda E. Benson and Melissa A. Brotman and Olivier Renaud and Stephen M. Smith and Thomas E. Nichols and Ellen Leibenluft and Anderson M. Winkler and Daniel S. Pine},
keywords = {Anxiety, Disruptive behavior, Intrinsic brain connectivity, Irritability, Joint canonical correlation and independent component analysis, Youth},
abstract = {Background
Imaging research has not yet delivered reliable psychiatric biomarkers. One challenge, particularly among youth, is high comorbidity. This challenge might be met through canonical correlation analysis designed to model mutual dependencies between symptom dimensions and neural measures. We mapped the multivariate associations that intrinsic functional connectivity manifests with pediatric symptoms of anxiety, irritability, and attention-deficit/hyperactivity disorder (ADHD) as common, impactful, co-occurring problems. We evaluate the replicability of such latent dimensions in an independent sample.
Methods
We obtained ratings of anxiety, irritability, and ADHD, and 10 minutes of resting-state functional magnetic resonance imaging data, from two independent cohorts. Both cohorts (discovery: n = 182; replication: n = 326) included treatment-seeking youth with anxiety disorders, with disruptive mood dysregulation disorder, with ADHD, or without psychopathology. Functional connectivity was modeled as partial correlations among 216 brain areas. Using canonical correlation analysis and independent component analysis jointly we sought maximally correlated, maximally interpretable latent dimensions of brain connectivity and clinical symptoms.
Results
We identified seven canonical variates in the discovery and five in the replication cohort. Of these canonical variates, three exhibited similarities across datasets: two variates consistently captured shared aspects of irritability, ADHD, and anxiety, while the third was specific to anxiety. Across cohorts, canonical variates did not relate to specific resting-state networks but comprised edges interconnecting established networks within and across both hemispheres.
Conclusions
Findings revealed two replicable types of clinical variates, one related to multiple symptom dimensions and a second relatively specific to anxiety. Both types involved a multitude of broadly distributed, weak brain connections as opposed to strong connections encompassing known resting-state networks.}
}
@article{PENZEL2021619,
title = {New Paths in Respiratory Sleep Medicine: Consumer Devices, e-Health, and Digital Health Measurements},
journal = {Sleep Medicine Clinics},
volume = {16},
number = {4},
pages = {619-634},
year = {2021},
note = {Measuring Sleep},
issn = {1556-407X},
doi = {https://doi.org/10.1016/j.jsmc.2021.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1556407X2100062X},
author = {Thomas Penzel and Sarah Dietz-Terjung and Holger Woehrle and Christoph Schöbel},
keywords = {E-health, Out-of-center testing, Health apps, Longtime monitoring, Diagnostics, Sleep-disordered breathing}
}
@article{GEORGIOU2021111089,
title = {An empirical study of COVID-19 related posts on Stack Overflow: Topics and technologies},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111089},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111089},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001862},
author = {Konstantinos Georgiou and Nikolaos Mittas and Alexandros Chatzigeorgiou and Lefteris Angelis},
keywords = {COVID-19, Pandemic, StackOverflow, Knowledge-sharing},
abstract = {The COVID-19 outbreak, also known as the coronavirus pandemic, has left its mark on every aspect of our lives and at the time of this writing is still an ongoing battle. Beyond the immediate global-wide health response, the pandemic has triggered a significant number of IT initiatives to track, visualize, analyze and potentially mitigate the phenomenon. For individuals or organizations interested in developing COVID-19 related software, knowledge-sharing communities such as Stack Overflow proved to be an effective source of information for tackling commonly encountered problems. As an additional contribution to the investigation of this unprecedented health crisis and to assess how fast and how well the community of developers has responded, we performed a study on COVID-19 related posts in Stack Overflow. In particular, we profiled relevant questions based on key post features and their evolution, identified the most prominent technologies adopted for developing COVID-19 software and their interrelations and focused on the most persevering problems faced by developers. For the analysis of posts we employed descriptive statistics, Association Rule Graphs, Survival Analysis and Latent Dirichlet Allocation. The results reveal that the response of the developers’ community to the pandemic was immediate and that the interest of developers on COVID-19 related challenges was sustained after its initial peak. In terms of the problems addressed, the results show a clear focus on COVID-19 data collection, analysis and visualization from/to the web, in line with the general needs for monitoring the pandemic.}
}
@article{YANG2021126442,
title = {Current advances and future challenges of AIoT applications in particulate matters (PM) monitoring and control},
journal = {Journal of Hazardous Materials},
volume = {419},
pages = {126442},
year = {2021},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2021.126442},
url = {https://www.sciencedirect.com/science/article/pii/S0304389421014072},
author = {Chao-Tung Yang and Ho-Wen Chen and En-Jui Chang and Endah Kristiani and Kieu Lan Phuong Nguyen and Jo-Shu Chang},
abstract = {Air pollution is at the center of pollution-control discussion due to the significant adverse health effects on individuals and the environment. Research has shown the association between unsafe environments and different sizes of particulate matter (PM), highlighting the importance of pollutant monitoring to mitigate its detrimental effect. By monitoring air quality with low-cost monitoring devices that collect massive observations, such as Air Box, a comprehensive collection of ground-level PM concentration is plausible due to the simplicity and low-cost, propelling applications in agriculture, aquaculture, and air quality, water resources, and disaster prevention. This paper aims to view IoT-based systems with low-cost microsensors at the sensor, network, and application levels, along with machine learning algorithms that improve sensor networks’ precision, providing better resolution. From the analysis at the three levels, we analyze current PM monitoring methods, including the use of sensors when collecting PM concentrations, demonstrate the use of IoT-based systems in PM monitoring and its challenges, and finally present the integration of AI and IoT (AIoT) in PM monitoring, indoor air quality control, and future directions. In addition, the inclusion of Taiwan as a site analysis was illustrated to show an example of AIoT in PM-control policy-making potential directions.}
}
@article{ANTONOV202142,
title = {Façade deterioration prediction with the use of machine learning methods, based on objective parameters and e-participation data},
journal = {Procedia Computer Science},
volume = {193},
pages = {42-51},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020469},
author = {Aleksandr Antonov and Ivan Khodnenko and Sergei Kudinov},
keywords = {machine learning, logistic regression, predictive data, e-participation data, quality of urban environment, infrastructure deterioration},
abstract = {Condition monitoring and timely repair of residential buildings is an important task when ensuring a comfortable life in cities. In the case of large metropolitan areas, it is a difficult task to perform continuous objective condition monitoring for tens of thousands of residential buildings by efforts of experts. However, residential infrastructure health can be predicted on the basis of indirect data. These can be objective building parameters or subjective data on citizens’ complaints about deterioration. In cities today, it is possible to collect such data in machine-readable form from various information systems. This article proposes a method to predict external deterioration of buildings on the basis of indirect data, using machine learning and SMILE Low-coding platform. Based on the results of method approbation, which used data of a metropolis, the significance of electronic participation data and objective parameters of objects for façade deterioration forecast was assessed. Options for further research are proposed to improve the quality of deterioration predicting by using data on citizens’ complaints about infrastructure damage.}
}
@article{ZAHID2021104420,
title = {A systematic review of emerging information technologies for sustainable data-centric health-care},
journal = {International Journal of Medical Informatics},
volume = {149},
pages = {104420},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104420},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621000460},
author = {Arnob Zahid and Jennifer Kay Poulsen and Ravi Sharma and Stephen C. Wingreen},
keywords = {Emerging technologies, Data modelling, Data analytics, Data-centric health-care},
abstract = {Background
Of the Sustainable Development Goals (SDGs), the third presents the opportunity for a predictive universal digital healthcare ecosystem, capable of informing early warning, assisting in risk reduction and guiding management of national and global health risks. However, in reality, the existing technology infrastructure of digital healthcare systems is insufficient, failing to satisfy current and future data needs.
Objective
This paper systematically reviews emerging information technologies for data modelling and analytics that have potential to achieve Data-Centric Health-Care (DCHC) for the envisioned objective of sustainable healthcare. The goal of this review is to: 1) identify emerging information technologies with potential for data modelling and analytics, and 2) explore recent research of these technologies in DCHC.
Findings
A total of 1619 relevant papers have been identified and analysed in this review. Of these, 69 were probed deeply. Our analysis found that the extant research focused on elder care, rehabilitation, chronic diseases, and healthcare service delivery. Use-cases of the emerging information technologies included providing assistance, monitoring, self-care and self-management, diagnosis, risk prediction, well-being awareness, personalized healthcare, and qualitative and/or quantitative service enhancement. Limitations identified in the studies included vendor hardware specificity, issues with user interface and usability, inadequate features, interoperability, scalability, and compatibility, unjustifiable costs and insufficient evaluation in terms of validation.
Conclusion
Achievement of a predictive universal digital healthcare ecosystem in the current context is a challenge. State-of-the-art technologies demand user centric design, data privacy and protection measures, transparency, interoperability, scalability, and compatibility to achieve the SDG objective of sustainable healthcare by 2030.}
}
@article{ROSA2021106219,
title = {Using digital technologies in clinical trials: Current and future applications},
journal = {Contemporary Clinical Trials},
volume = {100},
pages = {106219},
year = {2021},
issn = {1551-7144},
doi = {https://doi.org/10.1016/j.cct.2020.106219},
url = {https://www.sciencedirect.com/science/article/pii/S1551714420302974},
author = {Carmen Rosa and Lisa A. Marsch and Erin L. Winstanley and Meg Brunner and Aimee N.C. Campbell},
keywords = {Clinical trials, Digital technology, Smartphones, Electronic health records, Artificial intelligence},
abstract = {In 2015, we provided an overview of the use of digital technologies in clinical trials, both as a methodological tool and as a mechanism to deliver interventions. At that time, there was limited guidance and limited use of digital technologies in clinical research. However, since then smartphones have become ubiquitous and digital health technologies have exploded. This paper provides an update to our earlier publication and an overview of how technology has been used in the past five years in clinical trials, providing examples with varying levels of technological integration and across different health conditions. Digital technology integration ranges from the incorporation of artificial intelligence in diagnostic devices to the use of real-world data (e.g., electronic health records) for study recruitment. Clinical trials can now be conducted entirely virtually, eliminating the need for in-person interaction. Much of the published research demonstrates how digital approaches can improve the design and implementation of clinical trials. While challenges remain, progress over the last five years is encouraging, and barriers can be overcome with careful planning.}
}
@article{LI2021104245,
title = {Review of tourism forecasting research with internet data},
journal = {Tourism Management},
volume = {83},
pages = {104245},
year = {2021},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2020.104245},
url = {https://www.sciencedirect.com/science/article/pii/S0261517720301710},
author = {Xin Li and Rob Law and Gang Xie and Shouyang Wang},
keywords = {Internet data, Tourism forecasting, Search engine, Social media, Systematic review},
abstract = {Internet techniques significantly influence the tourism industry and Internet data have been used widely used in tourism and hospitality research. However, reviews on the recent development of Internet data in tourism forecasting remain limited. This work reviews articles on tourism forecasting research with Internet data published in academic journals from 2012 to 2019. Then, the findings ae synthesized based on the following Internet data classifications: search engine, web traffic, social media, and multiple sources. Results show that among such classifications, search engine data are most widely incorporated into tourism forecasting. Time series and econometric forecasting models remain dominant, whereas artificial intelligence methods are still developing. For unstructured social media and multi-source data, methodological advancements in text mining, sentiment analysis, and social network analysis are required to transform data into time series for forecasting. Combined Internet data and forecasting models will help in improving forecasting accuracy further in future research.}
}
@incollection{PLOTKIN2021xvii,
title = {Introduction},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {xvii-xxii},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000176},
author = {David Plotkin}
}
@article{CUI2021104726,
title = {Bidirectional cross-modality unsupervised domain adaptation using generative adversarial networks for cardiac image segmentation},
journal = {Computers in Biology and Medicine},
volume = {136},
pages = {104726},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104726},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521005205},
author = {Hengfei Cui and Chang Yuwen and Lei Jiang and Yong Xia and Yanning Zhang},
keywords = {Generative adversarial networks, Unsupervised domain adaptation, Cardiac segmentation, Self-attention mechanism, Knowledge distillation loss},
abstract = {Background
A novel Generative Adversarial Networks (GAN) based bidirectional cross-modality unsupervised domain adaptation (GBCUDA) framework is developed for cardiac image segmentation, which can effectively tackle the problem of network's segmentation performance degradation when adapting to the target domain without ground truth labels.
Method
GBCUDA uses GAN for image alignment, applies adversarial learning to extract image features, and gradually enhances the domain invariance of extracted features. The shared encoder performs an end-to-end learning task in which features that differ between the two domains complement each other. The self-attention mechanism is incorporated to the GAN network, which can generate details based on the prompts of all feature positions. Furthermore, spectrum normalization is implemented to stabilize the training of GAN, and knowledge distillation loss is introduced to process high-level feature-maps in order to better complete the cross-mode segmentation task.
Results
The effectiveness of our proposed unsupervised domain adaptation framework is tested over the Multi-Modality Whole Heart Segmentation (MM-WHS) Challenge 2017 dataset. The proposed method is able to improve the average Dice from 74.1% to 81.5% for the four cardiac substructures, and reduce the average symmetric surface distance (ASD) from 7.0 to 5.8 over CT images. For MRI images, our proposed framework trained on CT images gives the average Dice of 59.2% and reduces the average ASD from 5.7 to 4.9.
Conclusions
The evaluation results demonstrate our method's effectiveness on domain adaptation and the superiority to the current state-of-the-art domain adaptation methods.}
}
@article{ULLAH2021120743,
title = {Risk management in sustainable smart cities governance: A TOE framework},
journal = {Technological Forecasting and Social Change},
volume = {167},
pages = {120743},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120743},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100175X},
author = {Fahim Ullah and Siddra Qayyum and Muhammad Jamaluddin Thaheem and Fadi Al-Turjman and Samad M.E. Sepasgozar},
keywords = {Smart city, Sustainability, Risk management, Smart city governance, Technology-organisation-environment (TOE) framework},
abstract = {Sustainable smart cities are confronted by technological, organisational and external risks, making their governance difficult and susceptible to manipulation. Based on a comprehensive literature review of 796 systematically retrieved articles, the current study proposes a multilayered technology-organisation-environment (TOE-based) risk management framework for sustainable smart city governance. A total of 56 risks are identified and grouped into TOE categories. There are 17 technological risks, including IoT networks, public internet management and user safety concerns, with a 38.7% contribution to smart city governance risks. With a 15.6% share, there are 11 organisational risks, including user data security and cloud management. There are 28 external risks with a contribution of 46.7% to the smart city governance and consist of smart city's environment, governance, integration and security risks. A multilayered TOE-based risk management framework is proposed to identify and manage the risks associated with smart city governance in the current study. The framework links smart citizens to each other through the smart city governance team and the integrated TOE layers. The iterative risk management process of identification, analysis, evaluation, monitoring and response planning is carried out in the TOE layers, both at the external layer levels and internal management levels. The proposed framework operationalises the risk management process for smart city governance by presenting the collection of pertinent risks and their thematic TOE categorisation. The criticality of the identified risks in line with the study's rankings can help researchers and practitioners understand the top risks of smart city governance. These risks present investment opportunities for city governance bodies to develop critical and effective responses as well as provide safety, security and enhanced privacy for citizens.}
}