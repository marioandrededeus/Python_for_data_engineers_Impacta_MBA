@article{SHARDT2020104,
title = {Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {104-113},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.103},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320303591},
author = {Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov},
keywords = {data quality assessment, system identification, big data, Industry 4.0, soft sensors},
abstract = {As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.}
}
@article{MIKALEF2020103237,
title = {Big data and business analytics: A research agenda for realizing business value},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103237},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103237},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619310687},
author = {Patrick Mikalef and Ilias O. Pappas and John Krogstie and Paul A. Pavlou}
}
@article{DO2020100018,
title = {Data quality analysis of interregional travel demand: Extracting travel patterns using matrix decomposition},
journal = {Asian Transport Studies},
volume = {6},
pages = {100018},
year = {2020},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2020.100018},
url = {https://www.sciencedirect.com/science/article/pii/S2185556020300183},
author = {Canh Xuan Do and Makoto Tsukai and Akimasa Fujiwara},
keywords = {Interregional travel survey, Web-based survey, Mobile phone data, Data quality, Nonnegative matrix factorization},
abstract = {The Interregional Travel Survey in Japan (formerly the Net Passenger Transportation Survey [NPTS]) still has some limitations. New data sources have recently emerged, e.g., massive data from web-based surveys (WEB) or collecting passive mobile phone data (MOBI). Using or not using these data sources have been questioned for data integration or model estimation and validation. Therefore, as an initial step, the data quality of new data sources was evaluated to identify the potential for data integration with NPTS or new data collection methods to replace NPTS. This study focused on finding out the similarities in travel patterns extracted from these data sources using a nonnegative matrix factorization method. This study found that origin–destination pairs in the MOBI travel patterns were significantly different from those of NPTS and WEB, while there were some similarities between NPTS and WEB. However, some issues have been remaining and should be resolved in the future.}
}
@article{LIU2020123,
title = {Urban big data fusion based on deep learning: An overview},
journal = {Information Fusion},
volume = {53},
pages = {123-133},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519301393},
author = {Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang},
keywords = {Urban computing, Big data, Data fusion, Deep learning},
abstract = {Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.}
}
@article{ELIA2020617,
title = {A multi-dimension framework for value creation through Big Data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {617-632},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120302212},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{MAJOR202056,
title = {Using big data in pediatric oncology: Current applications and future directions},
journal = {Seminars in Oncology},
volume = {47},
number = {1},
pages = {56-64},
year = {2020},
note = {Pediatric Oncology},
issn = {0093-7754},
doi = {https://doi.org/10.1053/j.seminoncol.2020.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0093775420300063},
author = {Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum},
keywords = {Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics},
abstract = {Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.}
}
@article{CARRA2020300,
title = {Data-driven ICU management: Using Big Data and algorithms to improve outcomes},
journal = {Journal of Critical Care},
volume = {60},
pages = {300-304},
year = {2020},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2020.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0883944120306791},
author = {Giorgia Carra and Jorge I.F. Salluh and Fernando José {da Silva Ramos} and Geert Meyfroidt},
keywords = {Big data, Data mining, Machine learning, Predictive modeling, Intensive care unit},
abstract = {The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term “Big Data” can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.}
}
@article{VALENCIAPARRA2020101180,
title = {Unleashing Constraint Optimisation Problem solving in Big Data environments},
journal = {Journal of Computational Science},
volume = {45},
pages = {101180},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101180},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320304816},
author = {Álvaro Valencia-Parra and Ángel Jesús Varela-Vaca and Luisa Parody and María Teresa Gómez-López},
keywords = {Big Data, Optimisation problem, Constraint programming, Distributed data, Heterogeneous data format},
abstract = {The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.}
}
@article{CARPIOPINEDO2020102859,
title = {Consumption and symbolic capital in the metropolitan space: Integrating ‘old’ retail data sources with social big data},
journal = {Cities},
volume = {106},
pages = {102859},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102859},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120312075},
author = {José Carpio-Pinedo and Javier Gutiérrez},
keywords = {Commercial space, Retail, Retail geography, Symbolic capital, Big data, Foursquare, Madrid},
abstract = {While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.}
}
@article{SBAI2020938,
title = {A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems},
journal = {Procedia Computer Science},
volume = {176},
pages = {938-947},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319876},
author = {Ines Sbai and Saoussen Krichen},
keywords = {Big data analytic, Decision Support System, DVRP, S-GA, Spark},
abstract = {Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark’s in-memory computing ability (as a master-slave distribution computing) and GA’s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.}
}
@article{BINDER2020307,
title = {Big Data Management Using Ontologies for CPQ Solutions},
journal = {Procedia Manufacturing},
volume = {52},
pages = {307-312},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321958},
author = {Alexander Binder and Eva-Maria Iwer and Werner Quint},
keywords = {CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality},
abstract = {In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called "ontology-based data matching", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.}
}
@article{WILKIN2020100470,
title = {Big data prioritization in SCM decision-making: Its role and performance implications},
journal = {International Journal of Accounting Information Systems},
volume = {38},
pages = {100470},
year = {2020},
note = {2019 UW CISA Symposium},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2020.100470},
url = {https://www.sciencedirect.com/science/article/pii/S1467089520300385},
author = {Carla Wilkin and Aldónio Ferreira and Kristian Rotaru and Luigi Red Gaerlan},
keywords = {Big data, Big data availability, Big data prioritization, Supply chain management, Performance},
abstract = {Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.}
}
@article{KHALAJZADEH2020100964,
title = {An end-to-end model-based approach to support big data analytics development},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100964},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300241},
author = {Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He},
keywords = {Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools},
abstract = {We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.}
}
@article{SILVA2020893,
title = {Identification of Patterns of Fatal Injuries in Humans through Big Data},
journal = {Procedia Computer Science},
volume = {170},
pages = {893-898},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305524},
author = {Jesus Silva and Jack Zilberman and Ligia Romero and Omar Bonerge Pineda and Yaneth Herazo-Beltran},
keywords = {Recognition of automated standards, mining, decision trees},
abstract = {External cause injuries are defined as intentionally or unintentionally harm or injury to a person, which may be caused by trauma, poisoning, assault, accidents, etc., being fatal (fatal injury) or not leading to death (non-fatal injury). External injuries have been considered a global health problem for two decades. This work aims to determine criminal patterns using data mining techniques to a sample of patients from Mumbai city in India.}
}
@article{PAL2020100869,
title = {Big data in biology: The hope and present-day challenges in it},
journal = {Gene Reports},
volume = {21},
pages = {100869},
year = {2020},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2020.100869},
url = {https://www.sciencedirect.com/science/article/pii/S2452014420302831},
author = {Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh},
keywords = {Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning},
abstract = {The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.}
}
@article{ZARRINPAR2020599,
title = {What Can We Learn About Drug Safety and Other Effects in the Era of Electronic Health Records and Big Data That We Would Not Be Able to Learn From Classic Epidemiology?},
journal = {Journal of Surgical Research},
volume = {246},
pages = {599-604},
year = {2020},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2019.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0022480419306985},
author = {Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo},
keywords = {Electronic health record, Big data, Drug safety, Health care database, Cancer risk},
abstract = {As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.}
}
@article{NASHIPUDIMATH2020100033,
title = {An efficient integration and indexing method based on feature patterns and semantic analysis for big data},
journal = {Array},
volume = {7},
pages = {100033},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300187},
author = {Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain},
keywords = {Big data, Integration, Feature patterns, Indexing, Semantic analysis},
abstract = {Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.}
}
@article{ZHAO20201624,
title = {Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling},
journal = {Drug Discovery Today},
volume = {25},
number = {9},
pages = {1624-1638},
year = {2020},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620302646},
author = {Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu},
abstract = {Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.}
}
@article{KARIM2020942,
title = {Big data management in participatory sensing: Issues, trends and future directions},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {942-955},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311627},
author = {Ahmad Karim and Aisha Siddiqa and Zanab Safdar and Maham Razzaq and Syeda Anum Gillani and Huma Tahir and Sana Kiran and Ejaz Ahmed and Muhammad Imran},
keywords = {Participatory sensing, Big data, Big data management, Big data analytics, Mobile cloud computing},
abstract = {Participatory sensing has become an emerging technology of this era owing to its low cost in big sensor data collection. Prior to participatory sensing, large-scale deployment complexities were found in wireless sensor networks when collecting data from widespread resources. Participatory sensing systems employ handheld devices as sensors to collect data from communities and transmit to the cloud, where data are further analyzed by expert systems. The processes involved in participatory sensing, such as data collection, transmission, analysis, and visualization, exhibit certain management issues. This study aims to identify big data management issues that must be addressed at the cloud side during data processing and storing and at the participant side during data collection and visualization. It then proposes a framework for big data management in participatory sensing to resolve the contemporary big data management issues on the basis of suggested principles. Moreover, this work presents case studies to elaborate the existence of the highlighted issues. Finally, the limitations, recommendations, and future research directions for academia and industry in the domain of participatory sensing are discussed.}
}
@article{MIKALEF2020103169,
title = {Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities},
journal = {Information & Management},
volume = {57},
number = {2},
pages = {103169},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618301022},
author = {Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou},
keywords = {Big data analytics, Dynamic capabilities, Operational capabilities, Business value, Resource-based view},
abstract = {A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm’s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.}
}
@article{BALTI2020101136,
title = {A review of drought monitoring with big data: Issues, methods, challenges and research directions},
journal = {Ecological Informatics},
volume = {60},
pages = {101136},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101136},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120300868},
author = {Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle},
keywords = {Drought monitoring, Artificial intelligence, Big data, Machine learning, Statistical approach, Remote sensing},
abstract = {Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.}
}
@article{LV2020101937,
title = {Achieving secure big data collection based on trust evaluation and true data discovery},
journal = {Computers & Security},
volume = {96},
pages = {101937},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101937},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820302133},
author = {Denglong Lv and Shibing Zhu},
keywords = {Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network},
abstract = {Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on ω-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.}
}
@article{HUGHES2020120300,
title = {Sowing the seeds of value? Persuasive practices and the embedding of big data analytics},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120300},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120300},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311264},
author = {Jeffrey Hughes and Kirstie Ball},
keywords = {Big data analytics, Persuasion, Practice, Capabilities, Value},
abstract = {This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.}
}
@article{SU2020138984,
title = {Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis},
journal = {Science of The Total Environment},
volume = {733},
pages = {138984},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.138984},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720325018},
author = {Yuan Su and Yanni Yu and Ning Zhang},
keywords = {Big data, Streaming data, Carbon emission, Environmental management, Bibliometric analysis, Net-work analysis},
abstract = {Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.}
}
@article{HUANCHUN2020102024,
title = {Analyzing the Influencing Factors of Urban Thermal Field Intensity Using Big-Data-Based GIS},
journal = {Sustainable Cities and Society},
volume = {55},
pages = {102024},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102024},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720300111},
author = {Huang Huanchun and Yang Hailin and Deng Xin and Hao Cui and Liu Zhifeng and Liu Wei and Zeng Peng},
keywords = {land-surface temperature, thermal field pattern, POI data, GIS, air temperature},
abstract = {The effects of human activities and land cover changes on urban thermal field patterns are closely related to the land surface temperature (LST) and air temperature. At present, the number of studies on the quantitative relationship between these two indexes and the effect of the observational scale on their influence is insufficient. In this study, spatial analysis methods such as geographic modeling were combined with remote sensing images, meteorological data, and points of insert and used to investigate the composition and scale of the factors influencing the temperature field in Beijing. The results showed that there are differences in the positive and negative correlations between LST and air temperature and various influencing factors. At a spatial resolution of 90 m, LST had a strong linear relationship with the average air temperature. Indicators reflecting elements of human activity, such as buildings, roads, and entertainment, were easily measured by meteorological stations at a small scale, and the natural green space ratio could also be easily captured by satellite thermal sensors at small scales. These results have substantial implications for environmental impact assessments in areas experiencing an increasing urban heat island effect due to rapid urbanization.}
}
@article{HAJLI2020135,
title = {Understanding market agility for new product success with big data analytics},
journal = {Industrial Marketing Management},
volume = {86},
pages = {135-143},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118304735},
author = {Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem},
keywords = {Big data analytics, Customer agility, Effective use of data, New product success},
abstract = {The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.}
}
@incollection{MISHRA20201,
title = {Chapter 1 - Analysis of the role and scope of big data analytics with IoT in health care domain},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar and Manju Khari},
booktitle = {Handbook of Data Science Approaches for Biomedical Engineering},
publisher = {Academic Press},
pages = {1-23},
year = {2020},
isbn = {978-0-12-818318-2},
doi = {https://doi.org/10.1016/B978-0-12-818318-2.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128183182000015},
author = {Sushruta Mishra and Brojo Kishore Mishra and Hrudaya Kumar Tripathy and Arijit Dutta},
keywords = {Big data analytics, Cloud, Health care domain, Internet of Things (IoT), Sensors},
abstract = {Data analytics play an active role in medical applications to extract relevant information from heaps of data samples. Internet of things (IoT) technology has slowly captured the market and is entering the health care sector too. With the help of big data analytics, various IoT-based devices can auto-monitor the health conditions of patients and can send the status to concerned physicians and family members. Thus, the integration of big data analytics with IoT technology forms a favorable combination in the health care domain. In this chapter, we discuss the two latest trends that include big data analytics and IoT with respect to its relevance in medical fields. We also analyze a health care monitoring system, which is an IoT-based model integrated with big data analytics. The system integrates patient specific information over the cloud. In the more developed model, the implementation was made to monitor the health status of the patients. The developed model was found to be faster and thus it can be easily implemented into a real time patient health monitoring and status management system. Medical experts can take advantage of this system model, thereby providing appropriate information to appropriate patient and doctors at appropriate time.}
}
@incollection{KRISHNAN2020175,
title = {10 - Building the big data application},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {175-197},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000107},
author = {Krish Krishnan},
keywords = {Big data, Business continuity, Research project, Software, Storyboard, User interface},
abstract = {This chapter will discuss the building and delivery of the application. We will look at all aspects of what needs to be done to complete the process from requirements to outcomes, program management, testing, methodology, and all risks and pitfalls. We will discuss KANBAN, budgets and finances, governance, timeline, increase of efficiency, maintenance, support, and application implementation.}
}
@incollection{FAULKNER202081,
title = {4 - Data Fundamentals},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {81-92},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000176},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Data ecosystems, Data context, Data architectures, Data Integrity, Data Quality},
abstract = {Data is one component in a system. It has value. The economics of increasingly data-centric systems is explored. There is a growing reliance on data to create systems that are larger scale, have wider scope and are more complex. As reliance on the data component increases, a self-reinforcing problem of implementing checks and balances necessary to enforce appropriate levels of risk reduction arises. This chapter introduces data architecture elements of container and content. It places this architecture within an appropriate data context. It introduces the concepts of data quality and data integrity. Data integrity is placed within the Safety Management and Safety Assurance processes. Big data and machine learning are considered in this context.}
}
@incollection{DAS2020127,
title = {Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare},
editor = {Amit Banerjee and Basabi Chakraborty and Hiroshi Inokawa and Jitendra {Nath Roy}},
booktitle = {Terahertz Biomedical and Healthcare Technologies},
publisher = {Elsevier},
pages = {127-143},
year = {2020},
isbn = {978-0-12-818556-8},
doi = {https://doi.org/10.1016/B978-0-12-818556-8.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185568000070},
author = {Debashis Das and Chinmay Chakraborty and Sourav Banerjee},
keywords = {3D imaging, Big data, Genomic expression, Healthcare technology, Medical sensors, Personal medical information, Radiology images},
abstract = {This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, “big data” is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary “V's” of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.}
}
@article{ATITALLAH2020100303,
title = {Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions},
journal = {Computer Science Review},
volume = {38},
pages = {100303},
year = {2020},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100303},
url = {https://www.sciencedirect.com/science/article/pii/S1574013720304032},
author = {Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala},
keywords = {Internet of Things, Deep Learning, Smart city, Big data analytics, Review},
abstract = {The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.}
}
@incollection{VIDHYALAKSHMI20201,
title = {Chapter 1 - Medical big data mining and processing in e-health care},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar},
booktitle = {An Industrial IoT Approach for Pharmaceutical Industry Growth},
publisher = {Academic Press},
pages = {1-30},
year = {2020},
isbn = {978-0-12-821326-1},
doi = {https://doi.org/10.1016/B978-0-12-821326-1.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213261000012},
author = {A. Vidhyalakshmi and C. Priya},
keywords = {Big data, IoT, health care, telemedicine, WHO, image processing},
abstract = {Health care is thought to be one of the business fields with the largest big data potential. Based on the prevailing definition, big data has a large amount of data which can be processed easily and can be modified or updated easily. These data can be quickly stored, processed, and transformed into valuable information using older technologies. At present, many new trends regarding new data resources and innovative data analysis are followed in medicine and health care. In practice, electronic health-care records, free open-source data, and the “quantified self” provide new approaches for analyzing data. Some of these advancements have been made in information extraction from the text data based on analytics, which is useful in data unlocking for analytics purposes from clinical documentation. Choosing big data approaches in the medicine and health-care fields has been lagging. This has led to the rise specific problems regarding data complexity and organizational, legal, and ethical challenges. With the growth of the uptake of big data in general, and medicine and health care in specific, innovative ideas and solutions are expected. Telemedicine is a new opportunity for the Internet of Things (IoT). This enables the specialist to consult a patient despite them being in different places. Medical image segmentation is needed for the analysis, storage, and protection of medical images in telemedicine. Telemedicine is defined by the World Health Organization (WHO) as “the practice of medical care using interactive audiovisual and data communications. This includes the delivery of medical care services, diagnosis, consultation, treatment, as well as health education and the transfer of medical data.” IoT-based applications mainly include remote patient monitoring and clinical monitoring. In addition, preventive measures-based applications are also part of smart health care. These applications require image processing-based technologies which could be integrated into medical health-care systems. Various types of input taken from cameras and processing of CT and MRI images could be integrated into IoT-based medical applications.}
}
@article{HECKMAN202019,
title = {The Role of Physicians in the Era of Big Data},
journal = {Canadian Journal of Cardiology},
volume = {36},
number = {1},
pages = {19-21},
year = {2020},
issn = {0828-282X},
doi = {https://doi.org/10.1016/j.cjca.2019.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0828282X19312826},
author = {George A. Heckman and John P. Hirdes and Robert S. McKelvie}
}
@article{HOSEINZADEH2020101518,
title = {Quality of location-based crowdsourced speed data on surface streets: A case study of Waze and Bluetooth speed data in Sevierville, TN},
journal = {Computers, Environment and Urban Systems},
volume = {83},
pages = {101518},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101518},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302519},
author = {Nima Hoseinzadeh and Yuandong Liu and Lee D. Han and Candace Brakewood and Amin Mohammadnazar},
keywords = {Location-based data, Crowdsourced data, Waze, Bluetooth, Big data, Smart cities, Surface streets},
abstract = {Obtaining accurate speed and travel time information is a challenge for researchers, geographers, and transportation agencies. In the past, traffic data were usually acquired and disseminated by government agencies through fixed-location sensors. High costs, infrastructure demands, and low coverage levels of these sensor devices require agencies and researchers to look beyond the traditional approaches. With the emergence of smartphones and navigation apps, location-based and crowdsourced Big Data are receiving increased attention. In this regard, location-based big data (LocBigData) collected from probe vehicles and road users can be used to provide speed and travel time information in different locations. Examining the quality of crowdsourced data is essential for researchers and agencies before using them. This study assessed the quality of Waze speed data from surface streets and conducted a case study in Sevierville, Tennessee. Typically, examining the quality of these data in surface streets and arterials is more challenging than freeways data. This research used Bluetooth speed data as the ground truth, which is independent of Waze data. In this study, three steps of methodology were used. In the first step, Waze speed data was compared to Bluetooth data in terms of accuracy, mean difference, and distribution similarity. In the second step, a k-means algorithm was used to categorize Waze data quality, and a multinomial logistics regression model was performed to explore the significant factors that impact data quality. Finally, in the third step, machine learning techniques were conducted to predict the data quality in different conditions. The result of the comparison showed a similar pattern and a slight difference between datasets, which verified the quality of Waze speed data. The statistical model indicates that that Waze speed data are more accurate in peak hours than in night hours. Also, the traffic speed, traffic volume, and segment length have a significant association on the accuracy of Waze data on surface streets. Finally, the result of machine learning prediction showed that a KNN method performed the highest prediction accuracy of 84.5% and 82.9% of the time for training and test datasets, respectively. Overall, the study results suggest that Waze speed data is a promising data source for surface streets.}
}
@article{SHAMIM2020120315,
title = {Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120315},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120315},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311410},
author = {Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia},
keywords = {Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets},
abstract = {This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.}
}
@article{HOLMLUND2020356,
title = {Customer experience management in the age of big data analytics: A strategic framework},
journal = {Journal of Business Research},
volume = {116},
pages = {356-365},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320300345},
author = {Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki},
keywords = {Customer experience, Customer experience management, Customer experience insight, Big data analytics},
abstract = {Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.}
}
@article{SILVA2020111,
title = {Ion beam analysis and big data: How data science can support next-generation instrumentation},
journal = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
volume = {478},
pages = {111-115},
year = {2020},
issn = {0168-583X},
doi = {https://doi.org/10.1016/j.nimb.2020.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0168583X2030272X},
author = {Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães},
keywords = {Ion beam analysis, Big data, Data quality assurance, Artificial intelligence},
abstract = {With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.}
}
@incollection{TONG2020107,
title = {Chapter 5 - Machine learning for spatiotemporal big data in air pollution},
editor = {Lixin Li and Xiaolu Zhou and Weitian Tong},
booktitle = {Spatiotemporal Analysis of Air Pollution and Its Application in Public Health},
publisher = {Elsevier},
pages = {107-134},
year = {2020},
isbn = {978-0-12-815822-7},
doi = {https://doi.org/10.1016/B978-0-12-815822-7.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158227000054},
author = {Weitian Tong},
keywords = {Air pollution, Fine particulate matter, Spatiotemporal interpolation, Machine learning, Deep learning},
abstract = {An accurate understanding of air pollutants in a continuous space-time domain is critical for meaningful assessment of the quantitative relationship between the adverse health effects and the concentrations of air pollutants. Traditional interpolation methods, including various statistic and nonstatistic regression models, typically involve restrictive assumptions regarding independence of observations and distributions of outcomes. Moreover, a set of relationships among variables need to be defined strictly in advance. Machine learning opens a new door to understand the air pollution data based on the exposing data-driven relationships and predicting outcomes without empirical models. In this chapter, the state-of-the-art machine learning methods will be introduced to unlock the full potential of the air pollutant data, that is, to estimate the PM2.5 concentration more accurately in the spatiotemporal domain. The methods can be extended to the other air pollutants.}
}
@article{LAREYRE2020e575,
title = {Artificial Intelligence in Vascular Surgery: Moving from Big Data to Smart Data},
journal = {Annals of Vascular Surgery},
volume = {67},
pages = {e575-e576},
year = {2020},
issn = {0890-5096},
doi = {https://doi.org/10.1016/j.avsg.2020.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0890509620303411},
author = {Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort}
}
@article{LIU2020113381,
title = {Minimizing the data quality problem of information systems: A process-based method},
journal = {Decision Support Systems},
volume = {137},
pages = {113381},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113381},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301366},
author = {Qi Liu and Gengzhong Feng and Xi Zhao and Wenlong Wang},
keywords = {Data quality, Information system, Petri net, Optimization model, Process model},
abstract = {The low quality of data in information systems poses enormous risks to business operations and decision making. In this paper, a single-period resource allocation problem for controlling the information system's data quality problem is considered. We develop a Data-Quality-Petri net to capture the process through which data quality problem generates, propagates, and accumulates in the information system. The net considers not only the factors leading to the production of the data quality problem by the data operation nodes and the data flow structure, but also the data transfer ratio of the nodes. Then, we propose a nonlinear programming optimization model with control resource constraints. The result of the model provides an optimal strategy to allocate resources for minimizing the expected data quality problem of an information system. Further, we examine the impact of the data flow structure on optimal resource allocation. The result shows that the optimal resource input level for a data operation node is proportional to its potential for downstream propagation. A warehouse management system of an e-commerce company is utilized to illustrate the model. Our study provides a method for data managers to control the information system's data quality problem by employing a process perspective.}
}
@article{BOLDOSOVA2020122,
title = {Telling stories that sell: The role of storytelling and big data analytics in smart service sales},
journal = {Industrial Marketing Management},
volume = {86},
pages = {122-134},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118303353},
author = {Valeriia Boldosova},
keywords = {Storytelling, Big data analytics, Smart service, Customer reference, Customer-supplier relationships},
abstract = {The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.}
}
@article{CORTEREAL2020103141,
title = {Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103141},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308662},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira},
keywords = {Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory},
abstract = {Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.}
}
@article{LV2020103,
title = {Analysis of healthcare big data},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {103-110},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20304829},
author = {Zhihan Lv and Liang Qiao},
keywords = {Big data, Health care, Privacy security risk, Privacy measures},
abstract = {In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China’s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.}
}
@article{ELAGGOUNE2020465,
title = {A fuzzy agent approach for smart data extraction in big data environments},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {4},
pages = {465-478},
year = {2020},
note = {Emerging Software Systems},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819302010},
author = {Zakarya Elaggoune and Ramdane Maamri and Imane Boussebough},
keywords = {Big data, Multi-agent systems, Wireless sensor network, Fuzzy logic, Smart data},
abstract = {The era of big data has brought new challenges in data processing ad management. Existing analytical tools are now close to facing ongoing challenges thus providing satisfactory results at a reasonable cost. However, the velocity at which new data are flooded and the noise generated from such a large volume leads to various new challenges. The present research combines two artificial intelligence fields the represented by multi-agent technologies and fuzzy logic inference systems in order to extract the needed smart data from big noisy ones. A multi-fuzzy agent-based large-scale wireless sensor network has been used to demonstrate the effectiveness of the proposed approach. It handles sensors as autonomous fuzzy agents to measure the relevance of the collected data and eliminate the irrelevant ones. The results of the simulation exhibit a high quality of the data with a decrease in the sensors energy consumption, leading to a longer lifetime of the network.}
}
@article{GILL202051,
title = {Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model},
journal = {Clinics in Laboratory Medicine},
volume = {40},
number = {1},
pages = {51-59},
year = {2020},
note = {Direct-to-Consumer Testing: The Role of Laboratory Medicine},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300927},
author = {Emily L. Gill and Stephen R. Master},
keywords = {Big Data, Laboratory medicine, Machine learning, Direct-to-consumer testing, DTC, Harmonization}
}
@article{ARBEX2020102671,
title = {Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data},
journal = {Journal of Transport Geography},
volume = {85},
pages = {102671},
year = {2020},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2020.102671},
url = {https://www.sciencedirect.com/science/article/pii/S0966692319300092},
author = {Renato Arbex and Claudio B. Cunha},
keywords = {Public transport, Accessibility, Smart card data, In-vehicle crowding, Travel time reliability},
abstract = {Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. São Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.}
}
@article{VIEIRA2020125,
title = {Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio},
journal = {Procedia Manufacturing},
volume = {42},
pages = {125-131},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.093},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306582},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Industry 4.0},
abstract = {The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders’ interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.}
}
@article{DAISSAOUI2020161,
title = {IoT and Big Data Analytics for Smart Buildings: A Survey},
journal = {Procedia Computer Science},
volume = {170},
pages = {161-168},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304506},
author = {Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath},
keywords = {Smart buildings, IoT, Big data analytics, Reactif systems, Complex event processing},
abstract = {The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.}
}
@article{TU2020101428,
title = {Portraying the spatial dynamics of urban vibrancy using multisource urban big data},
journal = {Computers, Environment and Urban Systems},
volume = {80},
pages = {101428},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2019.101428},
url = {https://www.sciencedirect.com/science/article/pii/S0198971519302674},
author = {Wei Tu and Tingting Zhu and Jizhe Xia and Yulun Zhou and Yani Lai and Jincheng Jiang and Qingquan Li},
keywords = {Urban vibrancy, Geographically weighted regression, mobile phone data, Social media, Points-of-interest, Big data},
abstract = {Understanding urban vibrancy aids policy-making to foster urban space and therefore has long been a goal of urban studies. Recently, the emerging urban big data and urban analytic methods have enabled us to portray citywide vibrancy. From the social sensing perspective, this study presents a comprehensive and comparative framework to cross-validate urban vibrancy and uncover associated spatial effects. Spatial patterns of urban vibrancy indicated by multisource urban sensing data (points-of-interest, social media check-ins, and mobile phone records) were investigated. A comprehensive urban vibrancy metric was formed by adaptively weighting these metrics. The association between urban vibrancy and demographic, economic, and built environmental factors was revealed with global regression models and local regression models. An empirical experiment was conducted in Shenzhen. The results demonstrate that four urban vibrancy metrics are all higher in the special economic zone (SEZ) and lower in non-SEZs but with different degrees of spatial aggregation. The influences of employment and road density on all vibrancy metrics are significant and positive. However, the effects of metro stations, land use mix, building footprints, and distance to district center depend on the vibrancy indicator and location. These findings unravel the commonalities and differences in urban vibrancy metrics derived from multisource urban big data and the hidden spatial dynamics of the influences of associated factors. They further suggest that urban policies should be proposed to foster vibrancy in Shenzhen therefore benefit social wellbeing and urban development in the long term. They also provide valuable insights into the reliability of urban big data-driven urban studies.}
}
@article{ELIA2020508,
title = {A multi-dimension framework for value creation through big data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {508-522},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118307600},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{SHOUMY2020102447,
title = {Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals},
journal = {Journal of Network and Computer Applications},
volume = {149},
pages = {102447},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102447},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519303078},
author = {Nusrat J. Shoumy and Li-Minn Ang and Kah Phooi Seng and D.M.Motiur Rahaman and Tanveer Zia},
keywords = {Affective computing, Multimodal fusion, Sentiment databases, Sentiment analysis, Affective applications},
abstract = {Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.}
}
@article{VIEIRA2020101985,
title = {On the use of simulation as a Big Data semantic validator for supply chain management},
journal = {Simulation Modelling Practice and Theory},
volume = {98},
pages = {101985},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.101985},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19301182},
author = {António AC Vieira and Luís MS Dias and Maribel Y Santos and Guilherme AB Pereira and José A Oliveira},
keywords = {Simulation, Big Data, Data issues, Semantic validation, Supply chain management, Industry 4.0},
abstract = {Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.}
}
@article{JIN202024,
title = {Big Data in food safety- A review},
journal = {Current Opinion in Food Science},
volume = {36},
pages = {24-32},
year = {2020},
note = {Food Safety},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214799320301260},
author = {Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin},
abstract = {The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.}
}
@article{GHOLIZADEH2020120640,
title = {A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data},
journal = {Journal of Cleaner Production},
volume = {258},
pages = {120640},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.120640},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620306879},
author = {Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh},
keywords = {logistics, Robust fuzzy stochastic programming, Sustainable procurement, Big data, Hybrid uncertainty, ε-Constraint},
abstract = {Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented ε-constraint method is utilized. The model performance is investigated in a comprehensive computational study.}
}
@article{JHA2020113382,
title = {A note on big data analytics capability development in supply chain},
journal = {Decision Support Systems},
volume = {138},
pages = {113382},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113382},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301378},
author = {Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai},
keywords = {Big data, Analytics, Capability development, Qualitative study, Supply chain},
abstract = {Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.}
}
@article{URBAN2020117792,
title = {Application of big data analysis technique on high-velocity airblast atomization: Searching for optimum probability density function},
journal = {Fuel},
volume = {273},
pages = {117792},
year = {2020},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2020.117792},
url = {https://www.sciencedirect.com/science/article/pii/S0016236120307870},
author = {András Urbán and Axel Groniewsky and Milan Malý and Viktor Józsa and Jan Jedelský},
keywords = {Big data, Airblast, Rapeseed oil, PDA, Probability density function, Likelihood},
abstract = {In this paper, the droplet size distributions of high-velocity airblast atomization were analyzed. The spray measurement was performed by a Phase-Doppler anemometer at several points and different diameters across the spray for diesel oil, light heating oil, crude rapeseed oil, and water. The atomizing gauge pressure and the liquid preheating temperature varied from 0.3 to 2.4 bar and 25 to 100 °C, respectively. Approximately 400 million individual droplets were recorded; therefore, a big data evaluation technique was applied. 18 of the most commonly used probability density functions (PDF) were fitted to the histogram of each measuring point and evaluated by their relative log-likelihood. Among the three-parameter PDFs, Generalized Extreme Value and Burr PDFs provided the most desirable result to describe a complete drop size distribution. With restriction to two-parameter PDFs, the Nakagami PDF unexpectedly outperformed all the others, including Weibull (Rosin-Rammler) PDF, which is commonly used in atomization. However, if the spray is characterized by a single value, such as the Sauter Mean Diameter, i.e. an expected value-like parameter is of primary importance over the distribution, Gamma PDF is the best option, used in several papers of the atomization literature.}
}
@article{KEZUNOVIC2020106788,
title = {Big data analytics for future electricity grids},
journal = {Electric Power Systems Research},
volume = {189},
pages = {106788},
year = {2020},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2020.106788},
url = {https://www.sciencedirect.com/science/article/pii/S0378779620305915},
author = {Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa},
keywords = {Electricity grids, Analytics, Big data, Decision-making},
abstract = {This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.}
}
@article{PERAKIS2020107035,
title = {CYBELE – Fostering precision agriculture & livestock farming through secure access to large-scale HPC enabled virtual industrial experimentation environments fostering scalable big data analytics},
journal = {Computer Networks},
volume = {168},
pages = {107035},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.107035},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619305353},
author = {Konstantinos Perakis and Fenareti Lampathaki and Konstantinos Nikas and Yiannis Georgiou and Oskar Marko and Jarissa Maselyne},
keywords = {Precision agriculture, Precision livestock farming, High performance computing, Big data analytics},
abstract = {According to McKinsey & Company, about a third of food produced is lost or wasted every year, amounting to a $940 billion economic hit. Inefficiencies in planting, harvesting, water use, reduced animal contributions, as well as uncertainty about weather, pests, consumer demand and other intangibles contribute to the loss. Precision Agriculture (PA) and Precision Livestock Farming (PLF) come to assist in optimizing agricultural and livestock production and minimizing the wastes and costs aforementioned. PA is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. PLF is also a technology-enabled, data-driven approach to livestock production management, which exploits technology to quantitatively measure the behavior, health and performance of animals. Big data delivered by a plethora of data sources related to these domains, has a multitude of payoffs including precision monitoring of fertilizer and fungicide levels to optimize crop yields, risk mitigation that results from monitoring when temperature and humidity levels reach dangerous levels for crops, increasing livestock production while minimizing the environmental footprint of livestock farming, ensuring high levels of welfare and health for animals, and more. By adding analytics to these sensor and image data, opportunities also exist to further optimize PA and PLF by having continuous data on how a field or the livestock is responding to a protocol. For these domains, two main challenges exist: 1) to exploit this multitude of data facilitating dedicated improvements in performance, and 2) to make available advanced infrastructure so as to harness the power of this information in order to benefit from the new insights, practices and products, efficiently time-wise, lowering responsiveness down to seconds so as to cater for time-critical decisions. The current paper aims to introduce CYBELE, a platform aspiring to safeguard that the stakeholders involved in the agri-food value chain (research community, SMEs, entrepreneurs, etc.) have integrated, unmediated access to a vast amount of very large scale datasets of diverse types and coming from a variety of sources, and that they are capable of actually generating value and extracting insights out of these data, by providing secure and unmediated access to large-scale High Performance Computing (HPC) infrastructures supporting advanced data discovery, processing, combination and visualization services, solving computationally-intensive challenges modelled as mathematical algorithms requiring very high computing power and capability.}
}
@article{CUI2020101861,
title = {Manufacturing big data ecosystem: A systematic literature review},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {62},
pages = {101861},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101861},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300559},
author = {Yesheng Cui and Sami Kara and Ka C. Chan},
keywords = {Smart manufacturing, Big data, Cloud computing, Cloud manufacturing, Internet of things, NoSQL},
abstract = {Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.}
}
@article{NORTHCOTT202096,
title = {Big data and prediction: Four case studies},
journal = {Studies in History and Philosophy of Science Part A},
volume = {81},
pages = {96-104},
year = {2020},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368119300652},
author = {Robert Northcott},
keywords = {Big data, Prediction, Case studies, Explanation, Elections, Weather},
abstract = {Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.}
}
@incollection{LEI202029,
title = {2 - Fundamentals of big data in radio astronomy},
editor = {Linghe Kong and Tian Huang and Yongxin Zhu and Shenghua Yu},
booktitle = {Big Data in Astronomy},
publisher = {Elsevier},
pages = {29-58},
year = {2020},
isbn = {978-0-12-819084-5},
doi = {https://doi.org/10.1016/B978-0-12-819084-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128190845000109},
author = {Jiale Lei and Linghe Kong},
keywords = {Big data, Astronomy, Statistical challenges, Astronomical data analysis, Platforms for big data process},
abstract = {Large digital sky surveys are becoming the dominant source of data in astronomy. There are more than 100 terabytes of data in major archives, and that amount is growing rapidly. A typical sky survey archive has approximately 10 terabytes of image data and a billion detected sources (stars, galaxies, quasars, etc.), with hundreds of measured attributes per source. These surveys span the full range of wavelengths, radio through gamma ray, yet they are just a taste of the much larger datasets to come. Yearly advances in electronics bring new instruments that double the amount of data collected each year and lead to the exponential growth of information in astronomy. Thus, datasets that are orders of magnitude larger, more complex, and more homogeneous than in the past are on the horizon. In comparison, the size of the human genome is about 1 gigabyte and that of the Library of Congress is about 20 terabytes. Truly, astronomy has come to the big data era.}
}
@article{TRIPATHI20201245,
title = {Big-data driven approaches in materials science: A survey},
journal = {Materials Today: Proceedings},
volume = {26},
pages = {1245-1249},
year = {2020},
note = {10th International Conference of Materials Processing and Characterization},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.02.249},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320310026},
author = {Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi},
keywords = {Material science, Big data, Machine learning, Data analytics, Predictive Algorithms},
abstract = {The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.}
}
@article{SALVETAT2020101602,
title = {Data determinants of the activity of SMEs automobile dealers},
journal = {Journal of Engineering and Technology Management},
volume = {58},
pages = {101602},
year = {2020},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2020.101602},
url = {https://www.sciencedirect.com/science/article/pii/S0923474820300503},
author = {David Salvetat and Jean-Sébastien Lacam},
keywords = {Big data, Smart data, Development, Automobile, SME},
abstract = {Many SMEs still seem reluctant to accept the management of large datasets, which still appear to be too complex for them. However, our study reveals that the majority of small French car dealers are developing Big data and Smart data policies to improve the quality of their offers, the dynamism of their sales and their access to new opportunities. However, not every policy has the same effects on the development of their business. Whereas Big data improves all the components of SME development in a global, short-term and operational way, Smart data presents itself as a more targeted, prospective and strategic approach.}
}
@article{SHARMA2020538,
title = {MR-I MaxMin-scalable two-phase border based knowledge hiding technique using MapReduce},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {538-550},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.05.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17322173},
author = {Shivani Sharma and Durga Toshniwal},
keywords = {Big data, Computational cost, Knowledge hiding techniques, MapReduce, Privacy preservation, Scalability},
abstract = {Border based Knowledge hiding techniques (BB-KHT) are widely adopted form of privacy preservation techniques of data mining. These approaches are used to hide sensitive knowledge (confidential information) present in a dataset before sharing or analyzing it. BB-KHT primarily rely on border theory and maximum criterion method for preserving privacy and perpetuating good data quality of sanitized dataset but costs high computational complexity. Further, due to sequential nature, these approaches are particularly felicitous for small datasets and become infeasible while dealing with large scale datasets. Therefore, to subjugate the identified challenges of infeasibility and high computational complexity, a scalable two-phase improved MaxMin BB-KHT using MapReduce framework (MR-I MaxMin) is proposed. The proposed scheme requires only two database scans throughout the hiding process and hence, is computationally inexpensive. Moreover, the scheme also commits to preserve good data quality of sanitized dataset. The MapReduce version of proposed approach helps in achieving the feasibility by processing large voluminous data in a parallel fashion. Quantitative experiments and evaluations have been performed over a number of real and synthetically generated large-scale datasets. It is shown that the proposed MR-I MaxMin technique outperforms the similar existing approaches and vanquishes the identified challenges along with much-needed privacy preservation.}
}
@article{GHALLAB2020131,
title = {Detection outliers on internet of things using big data technology},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {3},
pages = {131-138},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519301616},
author = {Haitham Ghallab and Hanan Fahmy and Mona Nasr},
keywords = {Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs},
abstract = {Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.}
}
@article{MIKALEF2020103361,
title = {The role of information governance in big data analytics driven innovation},
journal = {Information & Management},
volume = {57},
number = {7},
pages = {103361},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103361},
url = {https://www.sciencedirect.com/science/article/pii/S0378720620302998},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS},
abstract = {The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm’s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm’s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC’s and a firm’s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.}
}
@article{ABOELMAGED2020102234,
title = {Influencing models and determinants in big data analytics research: A bibliometric analysis},
journal = {Information Processing & Management},
volume = {57},
number = {4},
pages = {102234},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102234},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319313366},
author = {Mohamed Aboelmaged and Samar Mouakket},
keywords = {Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks},
abstract = {Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars’ attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.}
}
@article{AITHAMMOU2020102122,
title = {Towards a real-time processing framework based on improved distributed recurrent neural network variants with fastText for social big data analytics},
journal = {Information Processing & Management},
volume = {57},
number = {1},
pages = {102122},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102122},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319305163},
author = {Badr {Ait Hammou} and Ayoub {Ait Lahcen} and Salma Mouline},
keywords = {Big data, FastText, Recurrent neural networks, LSTM, BiLSTM, GRU, Natural language processing, Sentiment analysis, Social big data analytics},
abstract = {Big data generated by social media stands for a valuable source of information, which offers an excellent opportunity to mine valuable insights. Particularly, User-generated contents such as reviews, recommendations, and users’ behavior data are useful for supporting several marketing activities of many companies. Knowing what users are saying about the products they bought or the services they used through reviews in social media represents a key factor for making decisions. Sentiment analysis is one of the fundamental tasks in Natural Language Processing. Although deep learning for sentiment analysis has achieved great success and allowed several firms to analyze and extract relevant information from their textual data, but as the volume of data grows, a model that runs in a traditional environment cannot be effective, which implies the importance of efficient distributed deep learning models for social Big Data analytics. Besides, it is known that social media analysis is a complex process, which involves a set of complex tasks. Therefore, it is important to address the challenges and issues of social big data analytics and enhance the performance of deep learning techniques in terms of classification accuracy to obtain better decisions. In this paper, we propose an approach for sentiment analysis, which is devoted to adopting fastText with Recurrent neural network variants to represent textual data efficiently. Then, it employs the new representations to perform the classification task. Its main objective is to enhance the performance of well-known Recurrent Neural Network (RNN) variants in terms of classification accuracy and handle large scale data. In addition, we propose a distributed intelligent system for real-time social big data analytics. It is designed to ingest, store, process, index, and visualize the huge amount of information in real-time. The proposed system adopts distributed machine learning with our proposed method for enhancing decision-making processes. Extensive experiments conducted on two benchmark data sets demonstrate that our proposal for sentiment analysis outperforms well-known distributed recurrent neural network variants (i.e., Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Unit (GRU)). Specifically, we tested the efficiency of our approach using the three different deep learning models. The results show that our proposed approach is able to enhance the performance of the three models. The current work can provide several benefits for researchers and practitioners who want to collect, handle, analyze and visualize several sources of information in real-time. Also, it can contribute to a better understanding of public opinion and user behaviors using our proposed system with the improved variants of the most powerful distributed deep learning and machine learning algorithms. Furthermore, it is able to increase the classification accuracy of several existing works based on RNN models for sentiment analysis.}
}
@article{WANG2020119299,
title = {Big data driven Hierarchical Digital Twin Predictive Remanufacturing paradigm: Architecture, control mechanism, application scenario and benefits},
journal = {Journal of Cleaner Production},
volume = {248},
pages = {119299},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119299},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619341691},
author = {Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu},
keywords = {multi-life-cycle remanufacturing, Sustainable products, Big data, CPS-Digital-twin(CPSDT), IoT-cloud, Reconfiguration},
abstract = {Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.}
}
@article{SFAXI2020101862,
title = {DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects},
journal = {Data & Knowledge Engineering},
volume = {130},
pages = {101862},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101862},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X19303830},
author = {Lilia Sfaxi and Mohamed Mehdi Ben Aissa},
keywords = {Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality},
abstract = {Decision making is the lifeblood of the enterprise — from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being “data driven” is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.}
}
@incollection{BIRKIN2020303,
title = {Big Data},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-311},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10616-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102295510616X},
author = {Mark Birkin},
keywords = {Administrative data, Crowdsourcing, Data ethics, Data quality, Data sharing, Representation and bias, Social media, Spatial analysis, Value, Variety, Velocity, Volume, Volunteered geographical information},
abstract = {Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.}
}
@article{DREMEL2020103121,
title = {Actualizing big data analytics affordances: A revelatory case study},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103121},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308522},
author = {Christian Dremel and Matthias M. Herterich and Jochen Wulf and Jan {vom Brocke}},
keywords = {Big data analytics, Affordance theory, Socio-technical approach, Organizational transformation, Organizational benefits, Affordance actualization},
abstract = {Drawing on a revelatory case study, we identify four big data analytics (BDA) actualization mechanisms: (1) enhancing, (2) constructing, (3) coordinating, and (4) integrating, which manifest in actions on three socio-technical system levels, i.e., the structure, actor, and technology levels. We investigate the actualization of four BDA affordances at an automotive manufacturing company, i.e., establishing customer-centric marketing, provisioning vehicle-data-driven services, data-driven vehicle developing, and optimizing production processes. This study introduces a theoretical perspective to BDA research that explains how organizational actions contribute to actualizing BDA affordances. We further provide practical implications that can help guide practitioners in BDA adoption.}
}
@article{SURBAKTI2020103146,
title = {Factors influencing effective use of big data: A research framework},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103146},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308649},
author = {Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq},
keywords = {Big data, Effective use, Factors, Framework},
abstract = {Information systems (IS) research has explored “effective use” in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.}
}
@article{RAHUL2020364,
title = {Data Life Cycle Management in Big Data Analytics},
journal = {Procedia Computer Science},
volume = {173},
pages = {364-371},
year = {2020},
note = {International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920315465},
author = {Kumar Rahul and Rohitash Kumar Banyal},
keywords = {Data life cycle, Data creation, Data usability, Healthcare, Big data},
abstract = {Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data’s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.}
}
@article{LI2020100608,
title = {Network analysis of big data research in tourism},
journal = {Tourism Management Perspectives},
volume = {33},
pages = {100608},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2019.100608},
url = {https://www.sciencedirect.com/science/article/pii/S2211973619301400},
author = {Xin Li and Rob Law},
keywords = {Big data, Tourism studies, Co-citation analysis, Network analysis, Research trends},
abstract = {This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.}
}
@article{WANG2020120175,
title = {Tension in big data using machine learning: Analysis and applications},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120175},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120175},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520310015},
author = {Huamao Wang and Yumei Yao and Said Salhi},
keywords = {Big data, Machine learning, Data size, Prediction accuracy, Social media},
abstract = {The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.}
}
@article{MATHIS2020582,
title = {Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group},
journal = {Journal of Cardiothoracic and Vascular Anesthesia},
volume = {34},
number = {3},
pages = {582-585},
year = {2020},
issn = {1053-0770},
doi = {https://doi.org/10.1053/j.jvca.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053077019311590},
author = {Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren}
}
@incollection{WHELAN2020365,
title = {Chapter 27 - Genetics, imaging, and cognition: big data approaches to addiction research},
editor = {Antonio Verdejo-Garcia},
booktitle = {Cognition and Addiction},
publisher = {Academic Press},
pages = {365-377},
year = {2020},
isbn = {978-0-12-815298-0},
doi = {https://doi.org/10.1016/B978-0-12-815298-0.00027-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152980000277},
author = {Robert Whelan and Zhipeng Cao and Laura O'Halloran and Brian Pennie},
keywords = {Addiction, Big Data, Cognition, Genetics, Machine learning, Neuroimaging, Online methods},
abstract = {The etiology and trajectory of addictions is complex, caused and moderated by individual differences in cognition that are themselves a function of genetics and of environment. In this chapter, we discuss how “Big Data” can shed light on the cognitive correlates of addiction. Big Data is primarily data-driven, using algorithms that search for patterns in data, with accurate prediction on previously unseen data as the metric of success. In this chapter, we introduce and provide practical advice on Big Data approaches for addiction. In the first part of this chapter, we describe how online methods of data collection facilitate the collection of large datasets. In the second section, we outline some recent advances in neuroimaging, with a focus on prediction of substance use using machine learning methods. In the final section, we present advances in genetics—meta- and megaanalyses—which may provide breakthroughs in our understanding of the genetics of addiction.}
}
@article{GAO2020668,
title = {Big data analytics for smart factories of the future},
journal = {CIRP Annals},
volume = {69},
number = {2},
pages = {668-692},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301359},
author = {Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti},
keywords = {Digital manufacturing system, Information, Learning},
abstract = {Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.}
}
@article{ZHANG2020483,
title = {Linking big data analytical intelligence to customer relationship management performance},
journal = {Industrial Marketing Management},
volume = {91},
pages = {483-494},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308762},
author = {Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han},
keywords = {Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance},
abstract = {This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.}
}
@article{HAUSLADEN2020101721,
title = {Towards a maturity model for big data analytics in airline network planning},
journal = {Journal of Air Transport Management},
volume = {82},
pages = {101721},
year = {2020},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2019.101721},
url = {https://www.sciencedirect.com/science/article/pii/S0969699718304988},
author = {Iris Hausladen and Maximilian Schosser},
keywords = {Maturity model, Network planning, Big data analytics, Airlines, Case study},
abstract = {The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.}
}
@article{SHAH2020106970,
title = {Feature engineering in big data analytics for IoT-enabled smart manufacturing – Comparison between deep learning and statistical learning},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {106970},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106970},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420300363},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-Things, Smart manufacturing, Big data, Data analytics, Feature engineering, Deep learning, Statistical learning},
abstract = {As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.}
}
@article{SELLAMI2020102732,
title = {On the use of big data frameworks for big service composition},
journal = {Journal of Network and Computer Applications},
volume = {166},
pages = {102732},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102732},
url = {https://www.sciencedirect.com/science/article/pii/S108480452030206X},
author = {Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid},
keywords = {Big data, Big service, Big service composition, Quality of big services, Fuzzy RCA, Spark},
abstract = {Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called “Big Services”. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.}
}
@article{SILVA2020107828,
title = {Can big data explain yield variability and water productivity in intensive cropping systems?},
journal = {Field Crops Research},
volume = {255},
pages = {107828},
year = {2020},
issn = {0378-4290},
doi = {https://doi.org/10.1016/j.fcr.2020.107828},
url = {https://www.sciencedirect.com/science/article/pii/S0378429019318039},
author = {João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma},
keywords = {Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands},
abstract = {Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.}
}
@article{MCISAAC2020510,
title = {Real-world evaluation of enhanced recovery after surgery: big data under the microscope},
journal = {British Journal of Anaesthesia},
volume = {124},
number = {5},
pages = {510-512},
year = {2020},
issn = {0007-0912},
doi = {https://doi.org/10.1016/j.bja.2020.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0007091220300593},
author = {Daniel I. McIsaac},
keywords = {big data, enhanced recovery, epidemiology, orthopedic surgery, postoperative outcome, study design}
}
@article{LIU2020123646,
title = {Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123646},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123646},
url = {https://www.sciencedirect.com/science/article/pii/S095965262033691X},
author = {Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He},
keywords = {Big data, Blockchain, Agri-food supply chain, Investment decision, Coordination},
abstract = {Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.}
}
@article{LI202018,
title = {Teaching Natural Language Processing through Big Data Text Summarization with Problem-Based Learning},
journal = {Data and Information Management},
volume = {4},
number = {1},
pages = {18-43},
year = {2020},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2020-0003},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000572},
author = {Liuqing Li and Jack Geissinger and William A. Ingram and Edward A. Fox},
keywords = {information system education, computer science education, problem-based learning, natural language processing, NLP, big data text analytics, machine learning, deep learning},
abstract = {Natural language processing (NLP) covers a large number of topics and tasks related to data and information management, leading to a complex and challenging teaching process. Meanwhile, problem-based learning is a teaching technique specifically designed to motivate students to learn efficiently, work collaboratively, and communicate effectively. With this aim, we developed a problem-based learning course for both undergraduate and graduate students to teach NLP. We provided student teams with big data sets, basic guidelines, cloud computing resources, and other aids to help different teams in summarizing two types of big collections: Web pages related to events, and electronic theses and dissertations (ETDs). Student teams then deployed different libraries, tools, methods, and algorithms to solve the task of big data text summarization. Summarization is an ideal problem to address learning NLP since it involves all levels of linguistics, as well as many of the tools and techniques used by NLP practitioners. The evaluation results showed that all teams generated coherent and readable summaries. Many summaries were of high quality and accurately described their corresponding events or ETD chapters, and the teams produced them along with NLP pipelines in a single semester. Further, both undergraduate and graduate students gave statistically significant positive feedback, relative to other courses in the Department of Computer Science. Accordingly, we encourage educators in the data and information management field to use our approach or similar methods in their teaching and hope that other researchers will also use our data sets and synergistic solutions to approach the new and challenging tasks we addressed.}
}
@article{XIANG2020106538,
title = {Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence},
journal = {Computers & Industrial Engineering},
volume = {145},
pages = {106538},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106538},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220302722},
author = {Zehua Xiang and Minli Xu},
keywords = {Supply chain management, Big Data marketing, Technological innovation, Closed-loop supply chain, Overconfidence},
abstract = {In the “Internet+” era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP’s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a “win–win” situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer’s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP’s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP’s overconfidence and cost-sharing strategies may damage the supplier’s profit, the total profit of the CLSC increases.}
}
@article{KONG2020123142,
title = {A systematic review of big data-based urban sustainability research: State-of-the-science and future directions},
journal = {Journal of Cleaner Production},
volume = {273},
pages = {123142},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123142},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620331875},
author = {Lingqiang Kong and Zhifeng Liu and Jianguo Wu},
keywords = {Big data, Social media data, Urban landscape sustainability, Smart city, Urban planning},
abstract = {The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.}
}
@article{LIU202053,
title = {Semantic-aware data quality assessment for image big data},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {53-65},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302304},
author = {Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu},
keywords = {Semantic-aware, Quality assessment, Image big data, IDSTH, SHR},
abstract = {Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on “fitness for requirement”, can arouse the user’s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user’s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user’s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.}
}
@article{AMANULLAH2020495,
title = {Deep learning and big data technologies for IoT security},
journal = {Computer Communications},
volume = {151},
pages = {495-517},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419315361},
author = {Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran},
keywords = {Deep learning, Big data, IoT security},
abstract = {Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.}
}
@incollection{REIS2020179,
title = {3.10 - Data Quality and Denoising: A Review☆},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {179-204},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.14874-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472148747},
author = {M.S. Reis and P.M. Saraiva and B.R. Bakshi},
keywords = {Bayesian estimation, Data quality, Data rectification, Filtering, Fourier analysis, Gaussian and non-Gaussian noise, Kalman filtering, Model-based denoising, Multiscale analysis, Off-line and online denoising, Outliers, Smoothing, Wavelet analysis, Wavelet thresholding, Windowed Fourier analysis},
abstract = {This article introduces the methods of Fourier and wavelet analysis for enhancing data quality in typical chemometric and process analytics applications. Fourier analysis has been popular for many decades but is best suited for enhancing signals where most features are localized in frequency. In contrast, wavelet analysis is appropriate for signals that contain features localized in both time and frequency domains. It also retains the benefits of Fourier analysis such as orthonormality and computational efficiency. Practical algorithms for off-line and on-line denoising are described and compared via simple examples. These algorithms can be used for off-line or on-line applications in order to mitigate the impact of additive Gaussian as well as non-Gaussian noise.}
}
@article{SEDLMAYR202081,
title = {Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen für die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen},
journal = {Zeitschrift für Evidenz, Fortbildung und Qualität im Gesundheitswesen},
volume = {158-159},
pages = {81-91},
year = {2020},
issn = {1865-9217},
doi = {https://doi.org/10.1016/j.zefq.2020.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1865921720301744},
author = {Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr},
keywords = {Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases},
abstract = {Zusammenfassung
Hintergrund
In Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden können. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept für den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-geförderten Projekts „BIDA-SE“ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einfließen können.
Methode
Ziel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, ökonomischer Implikationen sowie Grenzen und Barrieren für dessen mittelfristige Umsetzung zu evaluieren. Für die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N = 9 Ärzt*innen, N = 69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N = 14 IT-Expert*innen und N = 21 Versorgungsforscher*innen durchgeführt. Für die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte primär deskriptiv durch eine Analyse von Häufigkeiten, Mittelwerten und Standardabweichungen.
Ergebnisse
Die Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (Ärzt*innen, Patient*innen/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erfährt. Aus Sicht der Ärzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen hätte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektorenübergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung würden sich aus Sicht der Ärzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios müsste jedoch eine Anpassung der Vergütungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren für eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualität, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verfügbarkeit sowie (7) Gewohnheiten und Präferenzen/Arztrolle.
Diskussion
Mit der vorliegenden Studie wurde ein erstes fachübergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zukünftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erfährt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) nützlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren für dessen mittelfristige Umsetzung überwunden werden müssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gewährleiten und den Zugang zu den Zentren für Seltene Erkrankungen zukünftig zu kanalisieren.
Schlussfolgerung
Auf nationaler Ebene wurden zahlreiche Aktivitäten angestoßen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts „BIDA-SE“ entwickelte Szenario ergänzt diese Forschungsaktivitäten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden können, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu können.
Introduction
In Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project “BIDA-SE”, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.
Methods
The aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October/November 2019 amongst a total of N = 9 physicians, N = 69 patients with rare diseases/patient representatives, N = 14 IT experts and N = 21 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.
Results
The results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician’s and health care researcher’s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards/data sources/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's role.
Discussion
With the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.
Conclusion
Many activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the “BIDA-SE” project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.}
}
@article{YADEGARIDEHKORDI2020100921,
title = {The impact of big data on firm performance in hotel industry},
journal = {Electronic Commerce Research and Applications},
volume = {40},
pages = {100921},
year = {2020},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2019.100921},
url = {https://www.sciencedirect.com/science/article/pii/S1567422319300985},
author = {Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang}},
keywords = {Firm performance, Big data, Hotel industry, Fuzzy logic, Structural equation modelling},
abstract = {Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.}
}
@article{MARIANI2020338,
title = {Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies},
journal = {Journal of Business Research},
volume = {121},
pages = {338-352},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320305956},
author = {Marcello M. Mariani and Samuel {Fosso Wamba}},
keywords = {Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data},
abstract = {The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers’ evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.}
}
@article{KAZMIERSKA202043,
title = {From multisource data to clinical decision aids in radiation oncology: The need for a clinical data science community},
journal = {Radiotherapy and Oncology},
volume = {153},
pages = {43-54},
year = {2020},
note = {Physics Special Issue: ESTRO Physics Research Workshops on Science in Development},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2020.09.054},
url = {https://www.sciencedirect.com/science/article/pii/S016781402030829X},
author = {Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine Røe Redalen and Alberto Traverso},
keywords = {Artificial intelligence, Big data, Data science, Personalized treatment, Radiotherapy, Shared decision making},
abstract = {Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.}
}
@article{CAISSIE2020e773,
title = {Radiotherapy (RT) Patterns Of Practice Variability Identified As A Challenge To Real-World Big Data: Recommendations From The Learning From Analysis Of Multicenter Big Data Aggregation (LAMBDA) Consortium},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {108},
number = {3, Supplement },
pages = {e773-e774},
year = {2020},
note = {Proceedings of the American Society for Radiation Oncology},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2020.07.224},
url = {https://www.sciencedirect.com/science/article/pii/S0360301620316436},
author = {A.L. Caissie and M.L. Mierzwa and C.D. Fuller and M. Rajaraman and A. Lin and A.M. McDonald and R.A. Popple and Y. Xiao and L. {van Dijk} and P. Balter and H. Fong and H. Ping and M. Kovoor and J. Lee and A. Rao and M.K. Martel and R.F. Thompson and B. Merz and J. Yao and C. Mayo}
}
@article{ZHAO2020132,
title = {Privacy-preserving clustering for big data in cyber-physical-social systems: Survey and perspectives},
journal = {Information Sciences},
volume = {515},
pages = {132-155},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309764},
author = {Yaliang Zhao and Samwel K. Tarus and Laurence T. Yang and Jiayu Sun and Yunfei Ge and Jinke Wang},
keywords = {CPSS, Big data, Cloud computing, Privacy preserving, Clustering},
abstract = {Clustering technique plays a critical role in data mining, and has received great success to solve application problems like community analysis, image retrieval, personalized recommendation, activity prediction, etc. This paper first reviews the traditional clustering and the emerging multiple clustering methods, respectively. Although the existing methods have superior performance on some small or certain datasets, they fall short when clustering is performed on CPSS big data because of the high cost of computation and storage. With the powerful cloud computing, this challenge can be effectively addressed, but it brings enormous threat to individual or company’s privacy. Currently, privacy preserving data mining has attracted widespread attention in academia. Compared to other reviews, this paper focuses on privacy preserving clustering technique, guiding a detailed overview and discussion. Specifically, we introduce a novel privacy-preserving tensor-based multiple clustering, propose a privacy-preserving tensor-based multiple clustering analytic and service framework, and give an illustrated case study on the public transportation dataset. Furthermore, we indicate the remaining challenges of privacy preserving clustering and discuss the future significant research in this area.}
}
@article{VIEIRA2020132,
title = {Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context},
journal = {Procedia Manufacturing},
volume = {42},
pages = {132-139},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305825},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Data issues, Industry 4.0},
abstract = {Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.}
}
@article{LI2020106143,
title = {Ensemble-based deep learning for estimating PM2.5 over California with multisource big data including wildfire smoke},
journal = {Environment International},
volume = {145},
pages = {106143},
year = {2020},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2020.106143},
url = {https://www.sciencedirect.com/science/article/pii/S0160412020320985},
author = {Lianfa Li and Mariam Girguis and Frederick Lurmann and Nathan Pavlovic and Crystal McClure and Meredith Franklin and Jun Wu and Luke D. Oman and Carrie Breton and Frank Gilliland and Rima Habre},
keywords = {PM, Machine learning, Air pollution exposure, Wildfires, Remote sensing, California, High spatiotemporal resolution},
abstract = {Introduction
Estimating PM2.5 concentrations and their prediction uncertainties at a high spatiotemporal resolution is important for air pollution health effect studies. This is particularly challenging for California, which has high variability in natural (e.g, wildfires, dust) and anthropogenic emissions, meteorology, topography (e.g. desert surfaces, mountains, snow cover) and land use.
Methods
Using ensemble-based deep learning with big data fused from multiple sources we developed a PM2.5 prediction model with uncertainty estimates at a high spatial (1 km × 1 km) and temporal (weekly) resolution for a 10-year time span (2008–2017). We leveraged autoencoder-based full residual deep networks to model complex nonlinear interrelationships among PM2.5 emission, transport and dispersion factors and other influential features. These included remote sensing data (MAIAC aerosol optical depth (AOD), normalized difference vegetation index, impervious surface), MERRA-2 GMI Replay Simulation (M2GMI) output, wildfire smoke plume dispersion, meteorology, land cover, traffic, elevation, and spatiotemporal trends (geo-coordinates, temporal basis functions, time index). As one of the primary predictors of interest with substantial missing data in California related to bright surfaces, cloud cover and other known interferences, missing MAIAC AOD observations were imputed and adjusted for relative humidity and vertical distribution. Wildfire smoke contribution to PM2.5 was also calculated through HYSPLIT dispersion modeling of smoke emissions derived from MODIS fire radiative power using the Fire Energetics and Emissions Research version 1.0 model.
Results
Ensemble deep learning to predict PM2.5 achieved an overall mean training RMSE of 1.54 μg/m3 (R2: 0.94) and test RMSE of 2.29 μg/m3 (R2: 0.87). The top predictors included M2GMI carbon monoxide mixing ratio in the bottom layer, temporal basis functions, spatial location, air temperature, MAIAC AOD, and PM2.5 sea salt mass concentration. In an independent test using three long-term AQS sites and one short-term non-AQS site, our model achieved a high correlation (>0.8) and a low RMSE (<3 μg/m3). Statewide predictions indicated that our model can capture the spatial distribution and temporal peaks in wildfire-related PM2.5. The coefficient of variation indicated highest uncertainty over deciduous and mixed forests and open water land covers.
Conclusion
Our method can be generalized to other regions, including those having a mix of major urban areas, deserts, intensive smoke events, snow cover and complex terrains, where PM2.5 has previously been challenging to predict. Prediction uncertainty estimates can also inform further model development and measurement error evaluations in exposure and health studies.}
}