@article{ZHANG201938,
title = {Public participation in the Geoweb era: Defining a typology for geo-participation in local governments},
journal = {Cities},
volume = {85},
pages = {38-50},
year = {2019},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0264275118313441},
author = {Shanqi Zhang},
keywords = {Public participation, Geoweb, PPGIS, VGI, Local government},
abstract = {Advancements in citizen sensing and geospatial big data have enabled new opportunities for government-citizen interactions and have played important roles in developing smart(er) cities. In addition to city governments and citizens actively using maps to communicate spatial planning issues, the increasing capabilities of citizens generating spatial data either actively or passively allow city governments to collect local spatial knowledge with unprecedented breadth at finer spatiotemporal resolutions. New methods for citizens collaborating with city governments are also emerging to enhance citizen engagement and to spur social innovation. By synthesizing recent advancements in geo-enabled citizen participation, this paper proposed a new typology for classifying and characterizing concepts and practices related to geospatial technology-mediated public participation in local governments (e.g. city and municipal governments). Practical examples are used to illustrate how new dynamics between local governments and citizens are formed, new methods of collecting local spatial knowledge are enabled, and new opportunities for improving the openness and operational efficiency of local governments have emerged. The proposed conceptualization and examples give rise to emerging needs of advancing geo-participation by developing geospatial methods and infrastructure and by investigating the social and spatial implications of geo-participation.}
}
@article{SHAH2019562,
title = {An Internet-of-things Enabled Smart Manufacturing Testbed},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {1},
pages = {562-567},
year = {2019},
note = {12th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.06.122},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319302083},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-things, smart manufacturing, big data, data analytics, statistical analysis, vibration, soft sensor, process monitoring},
abstract = {The emergence of the industrial Internet of Things (IoT) and ever advancing computing and communication technologies have fueled a new industrial revolution which is happening worldwide to make current manufacturing systems smarter, safer, and more efficient. Although many general frameworks have been proposed for IoT enabled systems for industrial application, there is limited literature on demonstrations or testbeds of such systems. In addition, there is a lack of systematic study on the characteristics of IoT sensors and data analytics challenges associated with IoT sensor data. This study is an attempt to help fill this gap by exploring the characteristics of IoT vibration sensors and show how IoT sensors and big data analytics can be used to develop real time monitoring frameworks.}
}
@article{PASIDIS2019301,
title = {Congestion by accident? A two-way relationship for highways in England},
journal = {Journal of Transport Geography},
volume = {76},
pages = {301-314},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300704},
author = {Ilias Pasidis},
keywords = {Accidents, Traffic congestion, Big data, Highways, England},
abstract = {This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access ‘big data’ of highway traffic and accidents in England for the period 2012–2014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the ‘rubbernecking’ effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.}
}
@article{ANTOINE201998,
title = {Use of metrics to quantify IMRT and VMAT treatment plan complexity: A systematic review and perspectives},
journal = {Physica Medica},
volume = {64},
pages = {98-108},
year = {2019},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2019.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S1120179719301310},
author = {Mikaël Antoine and Flavien Ralite and Charles Soustiel and Thomas Marsac and Paul Sargos and Audrey Cugny and Jérôme Caron},
keywords = {Modulation indices, Plan complexity, Volumetric modulated arc therapy},
abstract = {Purpose
Fixed-field intensity modulated radiation therapy (FF-IMRT) or volumetric modulated arc therapy (VMAT) beams complexity is due to fluence fluctuation. Pre-treatment Quality Assurance (PTQA) failure could be linked to it. Several plan complexity metrics (PCM) have been published to quantify this complexity but in a heterogeneous formalism. This review proposes to gather different PCM and to discuss their eventual PTQA failure identifier abilities.
Methods and materials
A systematic literature search and outcome extraction from MEDLINE/PubMed (National Center for Biotechnology Information, NCBI) was performed. First, a list and a synthesis of available PCM is made in a homogeneous formalism. Second, main results relying on the link between PCM and PTQA results but also on other uses are listed.
Results
A total of 163 studies were identified and n = 19 were selected after inclusion and exclusion criteria application. Difference is made between fluence and degree of freedom (DOF)-based PCM. Results about the PCM potential as PTQA failure identifier are described and synthesized. Others uses are also found in quality, big data, machine learning and audit procedure.
Conclusions
A state of the art is made thanks to this homogeneous PCM classification. For now, PCM should be seen as a planning procedure quality indicator although PTQA failure identifier results are mitigated. However limited clinical use seems possible for some cases. Yet, addressing the general PTQA failure prediction case could be possible with the big data or machine learning help.}
}
@article{HE2019320,
title = {Network-wide identification of turn-level intersection congestion using only low-frequency probe vehicle data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {108},
pages = {320-339},
year = {2019},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18312543},
author = {Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen},
keywords = {Big data, Floating car data, Urban road network, Traffic congestion, Road intersection},
abstract = {Locating the bottlenecks in cities where traffic congestion usually occurs is essential prior to solving congestion problems. Therefore, this paper proposes a low-frequency probe vehicle data (PVD)-based method to identify turn-level intersection traffic congestion in an urban road network. This method initially divides an urban area into meter-scale square cells and maps PVD into those cells and then identifies the cells that correspond to road intersections by taking advantage of the fixed-location stop-and-go characteristics of traffic passing through intersections. With those rasterized road intersections, the proposed method recognizes probe vehicles’ turning directions and provides preliminary analysis of traffic conditions at all turning directions. The proposed method is map-independent (i.e., no digital map is needed) and computationally efficient and is able to rapidly screen most of the intersections for turn-level congestion in a road network. Thereby, this method is expected to greatly decrease traffic engineers’ workloads by providing information regarding where and when to investigate and solve traffic congestion problems.}
}
@article{SAYAD2019130,
title = {Predictive modeling of wildfires: A new dataset and machine learning approach},
journal = {Fire Safety Journal},
volume = {104},
pages = {130-146},
year = {2019},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0379711218303941},
author = {Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}},
keywords = {Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining, Artificial intelligence},
abstract = {Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.}
}
@article{YANG2019277,
title = {Ontology: Footstone for Strong Artificial Intelligence},
journal = {Chinese Medical Sciences Journal},
volume = {34},
number = {4},
pages = {277-280},
year = {2019},
issn = {1001-9294},
doi = {https://doi.org/10.24920/003701},
url = {https://www.sciencedirect.com/science/article/pii/S1001929420300080},
author = {Xiaolin Yang and Zhe Wang and Hongjie Pan and Yan Zhu},
keywords = {ontology, artificial intelligence, biomedicine, big data},
abstract = {Abstract
In the past ten years, the application of artificial intelligence (AI) in biomedicine has increased rapidly, which roots in the rapid growth of biomedicine data, the improvement of computing performance, and the development of deep learning methods. At present, there are great difficulties in front of AI for solving complex and comprehensive medical problems. Ontology can play an important role in how to make machines have stronger intelligence and has wider applications in the medical field. By using ontologies, (meta) data can be standardized so that data quality is improved and more data analysis methods can be introduced, data integration can be supported by the semantics relationships which are specified in ontologies, and effective logic expression in nature language can be better understood by machine. This can be a pathway to stronger AI. Under this circumstance, the Chinese Conference on Biomedical Ontology and Terminology was held in Beijing in autumn 2019, with the theme “Making Machine Understand Data”. The success of this conference further improves the development of ontology in the field of biomedical information in China, and will promote the integration of Chinese ontology research and application with the international standards and the findability, accessibility, interoperability, and reusability(FAIR) Data Principle.}
}
@article{YANG2019103960,
title = {Revisiting the problem of sediment motion threshold},
journal = {Continental Shelf Research},
volume = {187},
pages = {103960},
year = {2019},
issn = {0278-4343},
doi = {https://doi.org/10.1016/j.csr.2019.103960},
url = {https://www.sciencedirect.com/science/article/pii/S0278434319303437},
author = {Yang Yang and Shu Gao and Ya Ping Wang and Jianjun Jia and Jilian Xiong and Liang Zhou},
keywords = {Sediment threshold, Shields curves, Critical shear stress, Turbulence structures, Scale effects, Continental shelf environments},
abstract = {The definition of the threshold of sediment motion is critical for continental shelf sediment dynamics. The work by A. Shields laid the foundation for this research direction, leading to the well-known Shields curve. Here we review the most widely used threshold curves that have followed from the original Shields curve over the last 80 years, and propose that in terms of physical processes the threshold (critical Shields parameter) is a function of at least six variables, i.e. grain Reynolds number, grain size distribution, sphericity, roundness, particle cohesiveness and the scale effects of turbulence. Identifying these key factors, we paid a special attention to the role of the scale effects of turbulence. Turbulence was thought to be a random process, but the improvement of measurement techniques revealed that it has both temporal and spatial structures: the magnitude of instantaneous velocity fluctuations varies in time and in location, which can cause the deviation between in situ measurements and flume experiments. In coastal and shelf waters, in situ measurements of tidal currents and suspended sediment concentrations have revealed that resuspension takes place even though the bed shear stress is well below the Shields curve. Further process and mechanism studies are required to improve the theoretical framework regarding the turbulence structures and their interplay with sediment threshold. The scientific problems for future studies include the establishment of laboratory experiments, in situ measurements and process-based modelling under different water depths and hydrodynamic conditions to quantify the scale effects of turbulence; the development of new observation techniques for higher resolution and for extreme environments; development of new data processing methods, including big data methods to analyse turbulence structures; and the quantification of the effects of biological contributions and non-particle components on the family of Shields curves.}
}
@article{WOODALL201972,
title = {Potential Problem Data Tagging: Augmenting information systems with the capability to deal with inaccuracies},
journal = {Decision Support Systems},
volume = {121},
pages = {72-83},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300740},
author = {Philip Woodall and Vaggelis Giannikas and Wenrong Lu and Duncan McFarlane},
keywords = {Data quality, Information quality, Accuracy, Metadata, Data analytics, Data tags},
abstract = {Data quality tags are a means of informing decision makers about the quality of the data they use from information systems. Unfortunately, data quality tags have not been successfully adopted despite their potential to assist decision makers. One reason for the non-adoption is that maintaining the tags is expensive and time-consuming: having a tag that represents accuracy, for example, would be massively time-consuming to measure because it requires some physical observation of reality to check the true value. We argue that a useful surrogate tag for accuracy can be created—without having to physically measure it—by counting the number of times the data has been exposed to an event that could cause it to become inaccurate. Experimental results show that the tags can help to avoid problems caused by inaccuracies, and also to help find the inaccuracies themselves.}
}
@article{YANASE2019413,
title = {The seven key challenges for the future of computer-aided diagnosis in medicine},
journal = {International Journal of Medical Informatics},
volume = {129},
pages = {413-422},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619300632},
author = {Juri Yanase and Evangelos Triantaphyllou},
keywords = {Computer-aided diagnosis (CAD), Segmentation, Feature extraction/selection, Classification, PRISMA approach, Big data},
abstract = {Background
Computer-aided diagnosis (CAD) can assist physicians in effective and efficient diagnostic decision-making. CAD systems are currently essential tools in some areas of clinical practice. In addition, it is one of the established fields of study in the interface of medicine and computer science. There are, however, still some critical challenges that CAD systems face.
Methods
This paper first describes a new literature review protocol, the Dynamic PRISMA approach based on the well-known PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) approach. This new approach enhances the traditional approach by integrating a feedback mechanism module. As a result of the literature review, this paper identifies seven major challenges that occur today in CAD and inhibit the next major developments.
Results
The seven challenges described in this paper involve some technical weaknesses in the interface of medicine and computer science. These challenges are related to various algorithmic limitations, the difficulty of medical professionals to adopt new systems, problems when dealing with patient data, and the lack of guidelines and standardization regarding many aspects of CAD. This paper also describes some of the recent research developments towards these challenges.
Conclusion
If these seven key challenges are addressed properly, then the ways for dealing with them will become the R&D pillars needed to bring CAD to the next level. This would require additional well-coordinated collaboration between researchers and practitioners in the fields of medicine and computer science.}
}
@article{DANIEL2019104804,
title = {Initializing a hospital-wide data quality program. The AP-HP experience.},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104804},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718306242},
author = {Christel Daniel and Patricia Serre and Nina Orlova and Stéphane Bréant and Nicolas Paris and Nicolas Griffon},
keywords = {Data accuracy, Data quality, Electronic health records, Data warehousing, Observational Studies as Topic},
abstract = {Background and Objectives
Data Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.
Materials and Methods
As part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets – the PI data set containing 8.8 M patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.
Results
Eleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n = 6), conformance (n = 3) and plausibility (n = 2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.
Conclusion
DQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.}
}
@article{PAIS2019100194,
title = {An automated workflow for MALDI-ToF mass spectra pattern identification on large data sets: An application to detect aneuploidies from pregnancy urine},
journal = {Informatics in Medicine Unlocked},
volume = {16},
pages = {100194},
year = {2019},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2019.100194},
url = {https://www.sciencedirect.com/science/article/pii/S2352914819300851},
author = {Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles},
keywords = {MALDI-ToF, Pattern recognition, Quality control, Comparative intensity data, Automated processing},
abstract = {Urine from first trimester pregnancies has been found to be rich in information related to aneuploidies and other clinical conditions. Mass spectral analysis derived from matrix assisted laser desorption ionization (MALDI) time of flight (ToF) data has been proven to be a cost effective method for clinical diagnostics. However, urine mass spectra are complex and require data modelling frameworks. Therefore, computational approaches that systematically analyse big data generated from MALDI-ToF mass spectra are essential. To address this issue, we developed an automated workflow that successfully processed large data sets from MALDI-ToF which is 100-fold faster than using a common software tool. Our method performs accurate data quality control decisions, and generates a comparative analysis to extract peak intensity patterns from a data set. We successfully applied our framework to the identification of peak intensity patterns for Trisomy 21 and Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in the UK and China. The results from our automated comparative analysis have shown characteristic patterns associated with aneuploidies in the first trimester pregnancy. Moreover, we have shown that the intensity patterns depended on the population origin, gestational age, and MALDI-ToF instrument.}
}
@article{JEONG2019358,
title = {Cohort profile: Beyond birth cohort study – The Korean CHildren's ENvironmental health Study (Ko-CHENS)},
journal = {Environmental Research},
volume = {172},
pages = {358-366},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0013935118306388},
author = {Kyoung Sook Jeong and Suejin Kim and Woo Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha},
keywords = {Ko-CHENS, Children, Environment, Cohort profile, Birth cohort},
abstract = {The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.}
}
@article{BLANDFORD201941,
title = {HCI for health and wellbeing: Challenges and opportunities},
journal = {International Journal of Human-Computer Studies},
volume = {131},
pages = {41-51},
year = {2019},
note = {50 years of the International Journal of Human-Computer Studies. Reflections on the past, present and future of human-centred technologies},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1071581919300771},
author = {Ann Blandford},
keywords = {Digital health, Medical devices, Health IT, Patient empowerment, Patient safety, Human factors, Complex adaptive systems},
abstract = {In terms of Human–Computer Interaction, healthcare presents paradoxes: on the one hand, there is substantial investment in innovative health technologies, particularly around “big data” analytics and personal health technologies; on the other hand, most interactive health technologies that are currently deployed at scale are difficult to use and few innovative technologies have achieved significant market penetration. We live in a time of change, with a shift from care being delivered by professionals towards people being expected to be actively engaged and involved in shared decision making. Technically, this shift is supported by novel health technologies and information resources; culturally, the pace of change varies across contexts. In this paper, I present a “space” of interactive health technologies, users and uses, and interdependencies between them. Based on a review of the past and present, I highlight opportunities for and challenges to the application of HCI methods in the design and deployment of digital health technologies. These include threats to privacy, patient trust and experience, and opportunities to deliver healthcare and empower people to manage their health and wellbeing in ways that better fit their lives and values.}
}
@article{POLYVYANYY2019345,
title = {A systematic approach for discovering causal dependencies between observations and incidents in the health and safety domain},
journal = {Safety Science},
volume = {118},
pages = {345-354},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518316230},
author = {Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede}},
keywords = {Big data, Data mining, Process mining, Proximity of events, Causality, Health and safety, Cause of incidents},
abstract = {The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company’s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization’s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.}
}
@article{MORAN201942,
title = {Curious Feature Selection},
journal = {Information Sciences},
volume = {485},
pages = {42-54},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519301100},
author = {Michal Moran and Goren Gordon},
keywords = {Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Big data, Data science, Feature selection},
abstract = {In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.}
}
@article{FRONZETTICOLLADON2019113075,
title = {Using social network and semantic analysis to analyze online travel forums and forecast tourism demand},
journal = {Decision Support Systems},
volume = {123},
pages = {113075},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113075},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301046},
author = {Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella},
keywords = {Tourism forecasting, Social network analysis, Semantic analysis, Online community, Text mining, Big data},
abstract = {Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network – i.e. the presence of eminent contributors – were the variables that contributed more to the forecasting of international airport arrivals.}
}
@article{GUNTHER2019583,
title = {Data quality assessment for improved decision-making: a methodology for small and medium-sized enterprises},
journal = {Procedia Manufacturing},
volume = {29},
pages = {583-591},
year = {2019},
note = {“18th International Conference on Sheet Metal, SHEMET 2019”“New Trends and Developments in Sheet Metal Processing”},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.02.114},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919301477},
author = {Lisa C. Günther and Eduardo Colangelo and Hans-Hermann Wiendahl and Christian Bauer},
keywords = {Data quality assessment, Data quality control, Information quality, Benchmarking, Production planning, control},
abstract = {Industrial enterprises rely on prediction of market behavior, monitoring of performance measures, evaluation of production processes and other data analyses to support strategic and operational decisions. However, although an adequate data quality (DQ) is essential for any data analysis and several methodologies for DQ assessment exist, not all organizations consider DQ in decision-making processes. E.g., inaccurate and delayed data acquisition leads to imprecise master data and poor knowledge of machine utilization. While these aspects should influence production planning and control, current approaches to data evaluation are too complex to use them on a-day-to-day basis. In this paper, we propose a methodology that simplifies the execution of DQ evaluations and improves the understandability of its results. One of its main concerns is to make DQ assessment usable to small and medium-sized enterprises (SME). The approach takes selected, context related structured or semi-structured data as input and uses a set of generic test criteria applicable to different tasks and domains. It combines data and domain driven aspects and can be partly executed automated and without context specific domain knowledge. The results of the assessment can be summarized into quality dimensions and used for benchmarking. The methodology is validated using data from the enterprise resource planning (ERP) and manufacturing execution system (MES) of a sheet metal manufacturer covering a year of time. The particular application aims at calculating logistic key performance indicators. Based on these conditions, data requirements are defined and the available data is evaluated considering domain specific characteristics.}
}
@article{SETER201959,
title = {The data driven transport research train is leaving the station. Consultants all aboard?},
journal = {Transport Policy},
volume = {80},
pages = {59-69},
year = {2019},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X17305589},
author = {Hanne Seter and Petter Arnesen and Odd André Hjelkrem},
abstract = {This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently – what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.}
}
@article{FRONTONI2019267,
title = {Sharing health data among general practitioners: The Nu.Sa. project},
journal = {International Journal of Medical Informatics},
volume = {129},
pages = {267-274},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619305532},
author = {Emanuele Frontoni and Adriano Mancini and Marco Baldi and Marina Paolanti and Sara Moccia and Primo Zingaretti and Vincenzo Landro and Paolo Misericordia},
keywords = {Electronic health record, Cybersecurity, Privacy, Cloud, Information sharing, General practitioners, e-Health},
abstract = {Today, e-health has entered the everyday work flow in the form of a variety of healthcare providers. General practitioners (GPs) are the largest category in the public sanitary service, with about 60,000 GPs throughout Italy. Here, we present the Nu.Sa. project, operating in Italy, which has established one of the first GP healthcare information systems based on heterogeneous data sources. This system connects all providers and provides full access to clinical and health-related data. This goal is achieved through a novel technological infrastructure for data sharing based on interoperability specifications recognised at the national level for messages transmitted from GP providers to the central domain. All data standards are publicly available and subjected to continuous improvement. Currently, the system manages more than 5,000 GPs with about 5,500,000 patients in total, with 4,700,000 pharmacological e-prescriptions and 1,700,000 e-prescriptions for laboratory exams per month. Hence, the Nu.Sa. healthcare system that has the capacity to gather standardised data from 16 different form of GP software, connecting patients, GPs, healthcare organisations, and healthcare professionals across a large and heterogeneous territory through the implementation of data standards with a strong focus on cybersecurity. Results show that the application of this scenario at a national level, with novel metrics on the architecture's scalability and the software's usability, affect the sanitary system and on GPs’ professional activities.}
}
@article{BOHNSACK2019799,
title = {What the hack? A growth hacking taxonomy and practical applications for firms},
journal = {Business Horizons},
volume = {62},
number = {6},
pages = {799-818},
year = {2019},
note = {Digital Transformation & Disruption},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681319301247},
author = {René Bohnsack and Meike Malena Liesner},
keywords = {Growth hacking, Digital transformation, Lean startup, Digital marketing, Big data},
abstract = {As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.}
}
@article{PASICHNYI2019486,
title = {Energy performance certificates — New opportunities for data-enabled urban energy policy instruments?},
journal = {Energy Policy},
volume = {127},
pages = {486-499},
year = {2019},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2018.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S0301421518307894},
author = {Oleksii Pasichnyi and Jörgen Wallin and Fabian Levihn and Hossein Shahrokni and Olga Kordas},
keywords = {Energy performance certificate (EPC), Building energy efficiency, Data applications, Data quality, Sweden},
abstract = {Energy performance certificates (EPC) were introduced in European Union to support reaching energy efficiency targets by informing actors in the building sector about energy efficiency in buildings. While EPC have become a core source of information about building energy, the domains of its applications have not been studied systematically. This partly explains the limitation of conventional EPC data quality studies that fail to expose the essential problems and secure effective use of the data. This study reviews existing applications of EPC data and proposes a new method for assessing the quality of EPCs using data analytics. Thirteen application domains were identified from systematic mapping of 79 papers, revealing increases in the number and complexity of studies and advances in applied data analysis techniques. The proposed data quality assurance method based on six validation levels was tested using four samples of EPC dataset for the case of Sweden. The analysis showed that EPC data can be improved through adding or revising the EPC features and assuring interoperability of EPC datasets. In conclusion, EPC data have wider applications than initially intended by the EPC policy instrument, placing stronger requirements on the quality and content of the data.}
}
@article{MARSDEN2019113172,
title = {Perspectives on numerical data quality in IS research},
journal = {Decision Support Systems},
volume = {126},
pages = {113172},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113172},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619302015},
author = {James R. Marsden and David E. Pingry and Jason B. Thatcher}
}
@article{YOUSEFI2019103005,
title = {Exploration information systems – A proposal for the future use of GIS in mineral exploration targeting},
journal = {Ore Geology Reviews},
volume = {111},
pages = {103005},
year = {2019},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2019.103005},
url = {https://www.sciencedirect.com/science/article/pii/S0169136819301490},
author = {Mahyar Yousefi and Oliver P. Kreuzer and Vesa Nykänen and Jon M.A. Hronsky},
keywords = {Exploration information systems (EIS), Mineral exploration targeting, Geographic information systems (GIS), Mineral systems approach},
abstract = {The advent of modern data collection and storage technologies has brought about a huge increase in data volumes with both traditional and machine learning tools struggling to effectively handle, manage and analyse the very large data quantities that are now available. The mineral exploration industry is by no means immune to this big data issue. Exploration decision-making has become much more complex in the wake of big data, in particular with respect to questions about how to best manage and use the data to obtain information, generate knowledge and gain insight. One of the ways in which the mineral exploration industry works with big data is by using a geographic information system (GIS). For example, GIS platforms are often used for integration, interrogation and interpretation of diverse geoscience and mineral exploration data with the goal of refining and prioritising known and identifying new targets. Here we (i) briefly discuss the importance of carefully translating conceptual ore deposit models into effective exploration targeting maps, (ii) propose and describe what we term exploration information systems (EIS): a new idea for an information system designed to better integrate the conceptual mineral deposit model (i.e., the critical and constituent processes of the targeted mineral system) with data available to support exploration targeting, and (iii) discuss how best to categorise mineral systems in an EIS as scale-dependent subsystems to form mineral deposits. Our vision for the future use of EIS in exploration targeting is one whereby the mappable ingredients of a targeted mineral system are translated and combined into a set of weighted evidence (or proxy) maps automatically, resulting in an auto-generated mineral prospectivity map and a series of ranked exploration targets. We do not envisage the EIS replacing human input and ingenuity; rather we envisage the EIS as an additional tool in the exploration toolbox and as an intelligence amplifying system in which humans are making use of machines to achieve the best possible results.}
}
@article{WEI20191062,
title = {A survey on quality-assurance approximate stream processing and applications},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1062-1080},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19314414},
author = {Xiaohui Wei and Yuanyuan Liu and Xingwang Wang and Bingyi Sun and Shang Gao and Jon Rokne},
keywords = {Approximate computing, Online data streams, Distributed stream processing, Quality assurance},
abstract = {The massive growth of data now being made available from a variety of sources leads to an increased demand for fast data processing to extract value from the data. In data streams, processing data requires computational power and data storage capabilities that have not kept pace with the data collection abilities. For these reasons approximate computations have been developed to handle both computational issues as well as the storage issues especially related to real-time data streams. In this paper, we first propose a comprehensive study of approximate computing techniques for data streams. We classify common approximate techniques as data-driven and computing-driven methods, and also discuss the combination of the two methods in emerging distributed processing environments. Based on existing approximate methods, we then detail the research on data quality management including quality evaluation and monitoring. The challenges are grouped into several research themes including pre-evaluation, data learning, approximation processing, and quality measurement. The aim of the paper is to provide researchers with a guide for how to make effective systematic strategies for approximate stream processing.}
}
@article{LNENICKA2019129,
title = {Big and open linked data analytics ecosystem: Theoretical background and essential elements},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {129-144},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18302545},
author = {Martin Lnenicka and Jitka Komarkova},
keywords = {Big and open linked data, Ecosystem approach, Dimensions, Data analytics lifecycle, Stakeholders, Conceptual framework},
abstract = {Big and open linked data are often mentioned together because storing, processing, and publishing large amounts of these data play an increasingly important role in today's society. However, although this topic is described from the political, economic, and social points of view, a technical dimension, which is represented by big data analytics, is insufficient. The aim of this review article was to provide a theoretical background of big and open linked data analytics ecosystem and its essential elements. First, the key terms were introduced including related dimensions. Then, the key lifecycle phases were defined and involved stakeholders were identified. Finally, a conceptual framework was proposed. In contrast to previous research, the new ecosystem is formed by interactions of stakeholders in the following dimensions and their sub-dimensions: transparency, engagement, legal, technical, social, and economic. These relationships are characterized by the most important requisites and public policy choices affecting the data analytics ecosystem together with the key phases and activities of the data analytics lifecycle. The findings should contribute to relevant initiatives, strategies, and policies and their effective implementation.}
}
@article{ABRAHAM2019424,
title = {Data governance: A conceptual framework, structured review, and research agenda},
journal = {International Journal of Information Management},
volume = {49},
pages = {424-438},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219300787},
author = {Rene Abraham and Johannes Schneider and Jan {vom Brocke}},
keywords = {Data governance, Information governance, Conceptual framework, Literature review, Research agenda},
abstract = {Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.}
}
@article{CHALVATZIS2019381,
title = {Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {381-393},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517315147},
author = {Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary},
keywords = {Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming},
abstract = {Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.}
}
@incollection{ONIK2019197,
title = {Chapter 8 - Blockchain in Healthcare: Challenges and Solutions},
editor = {Nilanjan Dey and Himansu Das and Bighnaraj Naik and Himansu Sekhar Behera},
booktitle = {Big Data Analytics for Intelligent Healthcare Management},
publisher = {Academic Press},
pages = {197-226},
year = {2019},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-818146-1},
doi = {https://doi.org/10.1016/B978-0-12-818146-1.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128181461000088},
author = {Md. Mehedi Hassan Onik and Satyabrata Aich and Jinhong Yang and Chul-Soo Kim and Hee-Cheol Kim},
keywords = {Blockchain, Big data, Healthcare, Data privacy, EHR security},
abstract = {The main challenge in distributing electronic health records (EHRs) for patient-centered research, market analysis, medicine investigation, healthcare data mining etc., is data privacy. Handling the large-scale data and preserving the privacy of patients has been a challenge to researchers for a long period of time. On the contrary, blockchain technology has alleviated some of the problems by providing a protected and distributed platform. Sadly, existing electronic health record (EHR) management systems suffer from data manipulation, delayed communication, and trustless cooperation in data collection, storage, and distribution. This chapter discusses the current issues of healthcare data privacy and existing and upcoming regulations on this sector. This chapter also includes an overview of the architecture, existing issues, and future scope of blockchain technology for successfully handling privacy and management of current and future medical records. This chapter also presents few blockchain solutions that advocate the future research scopes in healthcare, big data, and blockchain.}
}
@article{LNENICKA2019124,
title = {Developing a government enterprise architecture framework to support the requirements of big and open linked data with the use of cloud computing},
journal = {International Journal of Information Management},
volume = {46},
pages = {124-141},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218305942},
author = {Martin Lnenicka and Jitka Komarkova},
keywords = {Government enterprise architecture framework, Design science research, Big data, Open linked data, Cloud computing, Quality attributes, ATAM, Methodology},
abstract = {Governmental and local authorities are facing many new information and communication technologies challenges. The amount of data is rapidly increasing. The data sets are published in different formats. New services are based on linking and processing differently structured data from various sources. Users expect openness of public data, fast processing, and intuitive visualisation. The article addresses the challenges and proposes a new government enterprise architecture framework. The following partial architectures are included: big and open linked data storage, processing, and publishing using cloud computing. At first, the key concepts are defined. Next, the basic architectural roles and components are specified. The components result from the decomposition of related frameworks. The main part of the article deals with the detailed proposal of the architecture framework and partial views on architecture (sub-architectures). A methodology, including a proposal of appropriate steps, solutions and responsibilities for them, is described in the next step - after the verification and validation of the new framework with respect to the attributes of quality. The new framework responds to emerging ICT trends in order to evolve government enterprise architecture continually and represent current architectural components and their relationships.}
}
@article{LU201968,
title = {Oil and Gas 4.0 era: A systematic review and outlook},
journal = {Computers in Industry},
volume = {111},
pages = {68-90},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519302064},
author = {Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang},
keywords = {Oil and Gas 4.0, Big data, Digitization, IIoT, Intelligentization},
abstract = {Recently, with the development of “Industry 4.0”, “Oil and Gas 4.0” has also been put on the agenda in the past two years. Some companies and experts believe that “Oil and Gas 4.0” can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the “Oil and Gas 4.0” is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of “Oil and Gas 4.0”, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of “Oil and Gas 4.0” is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the “Oil and Gas 4.0” era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the “Oil and Gas 4.0” era. Finally, relevant policy recommendations are proposed.}
}
@article{NASHAAT2019131,
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
journal = {Information and Software Technology},
volume = {113},
pages = {131-145},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301247},
author = {Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad Marston},
keywords = {Big data, Machine learning, Business-to-business, User trust, Case study},
abstract = {Context
The need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.}
}
@article{AMENGUALGUAL201931,
title = {Status epilepticus prevention, ambulatory monitoring, early seizure detection and prediction in at-risk patients},
journal = {Seizure},
volume = {68},
pages = {31-37},
year = {2019},
note = {Pediatric Convulsive Status Epilepticus},
issn = {1059-1311},
doi = {https://doi.org/10.1016/j.seizure.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1059131118304059},
author = {Marta Amengual-Gual and Adriana Ulate-Campos and Tobias Loddenkemper},
keywords = {Epilepsy, Status epilepticus, Closed-loop systems, Machine learning, Seizure detection sensors, Automated seizure detection},
abstract = {Purpose
Status epilepticus is an often apparently randomly occurring, life-threatening medical emergency which affects the quality of life in patients with epilepsy and their families. The purpose of this review is to summarize information on ambulatory seizure detection, seizure prediction, and status epilepticus prevention.
Method
Narrative review.
Results
Seizure detection devices are currently under investigation with regards to utility and feasibility in the detection of isolated seizures, mainly in adult patients with generalized tonic-clonic seizures, in long-term epilepsy monitoring units, and occasionally in the outpatient setting. Detection modalities include accelerometry, electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors, surface electromyography, video detection systems, gyroscope, peripheral temperature, photoplethysmography, and respiratory sensors, among others. Initial detection results are promising, and improve even further, when several modalities are combined. Some portable devices have already been U.S. FDA approved to detect specific seizures. Improved seizure prediction may be attainable in the future given that epileptic seizure occurrence follows complex patient-specific non-random patterns. The combination of multimodal monitoring devices, big data sets, and machine learning may enhance patient-specific detection and predictive algorithms. The integration of these technological advances and novel approaches into closed-loop warning and treatment systems in the ambulatory setting may help detect seizures sooner, and tentatively prevent status epilepticus in the future.
Conclusions
Ambulatory monitoring systems are being developed to improve seizure detection and the quality of life in patients with epilepsy and their families.}
}
@article{SERRANO2019122,
title = {Deep neural network architectures for social services diagnosis in smart cities},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {122-131},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301918},
author = {Emilio Serrano and Javier Bajo},
keywords = {Social exclusion, Social services, Deep learning, Data analysis, Machine learning, Data mining},
abstract = {Social services intend to aid disadvantaged, distressed, or vulnerable persons or groups. Machine Learning (ML) and Deep Learning (DL), which are important technologies to leverage Internet of Things and Big Data, have not been considered to support intelligent social services in Smart Cities. Using technology to achieve more responsive, efficient, and proactive social services is a must in Smart Cities because it will lead to a more fair and egalitarian society. This research work contributes with the evaluation of a thousand Neural Networks architectures for the automatic diagnosis of chronic social exclusion. Some of them outperform previous models in quality metrics such as accuracy and F-score. Beyond the improvement in predicting this specific social condition, to the best of the authors’ knowledge, this paper open the research line of applying these methods for the general social services diagnosis in Smart Cities. Finally, the advantages of using the DL paradigm over other ML alternatives in this scope are discussed.}
}
@article{DEY20191317,
title = {Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review},
journal = {Journal of the American College of Cardiology},
volume = {73},
number = {11},
pages = {1317-1335},
year = {2019},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2018.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S0735109719302360},
author = {Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick},
keywords = {artificial intelligence, cardiovascular imaging, deep learning, machine learning},
abstract = {Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.}
}
@article{VONBULTZINGSLOWEN2019361,
title = {Swedish Quality Registry for Caries and Periodontal Diseases – a framework for quality development in dentistry},
journal = {International Dental Journal},
volume = {69},
number = {5},
pages = {361-368},
year = {2019},
issn = {0020-6539},
doi = {https://doi.org/10.1111/idj.12481},
url = {https://www.sciencedirect.com/science/article/pii/S0020653920323066},
author = {Inger {von Bültzingslöwen} and Hans Östholm and Lars Gahnberg and Dan Ericson and Jan L. Wennström and Jörgen Paulander},
keywords = {Odontology, epidemiology, oral health, big data, quality registry},
abstract = {ObjectivesL There is a need for monitoring dental health and healthcare, as support for quality development, allocation of resources and long-term planning of dental care. The aim of this paper is to describe the concept and implementation of the Swedish Quality Registry for Caries and Periodontal Diseases (SKaPa). Materials and methods: The SKaPa receives information by automatic transfer of data daily from electronic patient dental records via secure connections from affiliated dental care organisations (DCOs). The registry stores information about DCOs, dental professionals and patients. Information on a patient level includes personal identifier, gender, age, living area, dental status, risk assessments for caries and periodontitis, and dental care provided. In addition, data generated from a global question on patient-perceived oral health are uploaded. In total, more than 400 variables are transferred to the registry and updated daily. Results: In 2018, all of the 21 public DCOs and the largest private DCO in Sweden were affiliated to SKaPa, representing a total of 1,089 public and 234 private dental clinics. The accumulated amount of information on dental healthcare covers 6.9 million individuals out of the total Swedish population of 10 million. SKaPa produces reports on de-identified data, both cross-sectional and longitudinal. Conclusion: As a nationwide registry based on automatic retrieval of data directly from patient records, SKaPa offers the basis for a new era of systematic evaluation of oral health and quality of dental care. The registry supports clinical and epidemiological research, data mining and external validation of results from randomised controlled trials}
}
@article{HUI2019113136,
title = {A reporting guideline for IS survey research},
journal = {Decision Support Systems},
volume = {126},
pages = {113136},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113136},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301654},
author = {Wendy Hui and Siu Man Lui and Wai Kwong Lau},
keywords = {Reporting guidelines, Survey, Replicability},
abstract = {As a research method, the survey is known to be subject to various types of biases. Nevertheless, a survey is often the most direct and cost-effective way to solicit people's opinions. We emphasize that, regardless of the data collection method, it is impossible to remove all potential biases in a research setting. Aside from adhering to research design best practice, authors are responsible for providing sufficient transparency in the reporting of their work to enhance replicability and to allow others to evaluate the validity of their research. In this paper, we develop a reporting guideline for IS survey research. Researchers conducting IS survey research can use this guide as a checklist when they prepare their manuscripts, and peer reviewers can use it to evaluate research quality and the sufficiency of reporting. We hope that similar guidelines can be developed for other IS research methods and that their use and endorsement by research outlets can motivate researchers to pay greater attention to research design and data quality.}
}
@article{NAM2019411,
title = {Business analytics adoption process: An innovation diffusion perspective},
journal = {International Journal of Information Management},
volume = {49},
pages = {411-423},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218311435},
author = {Dalwoo Nam and Junyeong Lee and Heeseok Lee},
keywords = {Business analytics, Innovation diffusion, Adoption process, Data infrastructure, Data quality management, Analytics centralization},
abstract = {Although business analytics (BA) have been increasingly adopted into businesses, there is limited empirical research examining the drivers of each stage of BA adoption in organizations. Drawing upon technological-organizational-environmental framework and innovation diffusion process, we developed an integrative model to examine BA adoption processes and tested with 170 Korean firms. The analysis shows data-related technological characteristics derive all stages of BA adoption: initiation, adoption and assimilation. While organizational characteristics are associated with adoption and assimilation stage, only competition intensity in environmental characteristics is associated with initiation stage. Our findings help practitioners and researchers to understand what factors can enable companies to adopt BA in each stage.}
}
@article{GENGLER20195756,
title = {Symposium review: Challenges and opportunities for evaluating and using the genetic potential of dairy cattle in the new era of sensor data from automation},
journal = {Journal of Dairy Science},
volume = {102},
number = {6},
pages = {5756-5763},
year = {2019},
issn = {0022-0302},
doi = {https://doi.org/10.3168/jds.2018-15711},
url = {https://www.sciencedirect.com/science/article/pii/S0022030219302796},
author = {N. Gengler},
keywords = {dairy cattle, management, breeding, genome-guided management},
abstract = {ABSTRACT
Sensor data from automation are becoming available on an increasingly large scale, and associated research is slowly starting to appear. This new era of sensor data from automation leads to many challenges but also new opportunities for assessing and maximizing the genetic potential of dairy cattle. The first challenge is data quality, because all uses of sensor data require careful data quality validation, potentially using external references. The second issue is data accessibility. Indeed, sensor data generated from automation are often designed to be available on-farm in a given system. However, to make these data useful—for genetic improvement for example—the data must also be made available off-farm. By nature, sensor data often are very complex and diverse; therefore, a data consolidation and integration layer is required. Moreover, the traits we want to select have to be defined precisely when generated from these raw data. This approach is obviously also beneficial to limit the challenge of extremely high data volumes generated by sensors. An additional challenge is that sensors will always be deployed in a context of herd management; therefore, any efforts to make them useful should focus on both breeding and management. However, this challenge also leads to opportunities to use genomic predictions based on these novel data for breeding and management. Access to relevant phenotypes is crucial for every genomic evaluation system. The automatic generation of training data, on both the phenotypic and genomic levels, is a major opportunity to access novel, precise, continuously updated, and relevant data. If the challenges of bidirectional data transfer between farms and external databases can be solved, new opportunities for continuous genomic evaluations integrating genotypes and the most current local phenotypes can be expected to appear. Novel concepts such as federated learning may help to limit exchange of raw data and, therefore, data ownership issues, which is another important element limiting access to sensor data. Accurate genome-guided decision-making and genome-guided management of dairy cattle should be the ultimate way to add value to sensor data from automation. This could also be the major driving force to improve the cost–benefit relationship for sensor-based technologies, which is currently one of the major obstacles for large-scale use of available technologies.}
}
@article{LAU2019357,
title = {A survey of data fusion in smart city applications},
journal = {Information Fusion},
volume = {52},
pages = {357-374},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519300326},
author = {Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan},
keywords = {Data fusion, Sensor fusion, Smart city, Big data, Internet of things, Multi-perspectives classification},
abstract = {The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.}
}
@article{ZHANG201973,
title = {Prediction of geological conditions for a tunnel boring machine using big operational data},
journal = {Automation in Construction},
volume = {100},
pages = {73-83},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518308628},
author = {Qianli Zhang and Zhenyu Liu and Jianrong Tan},
keywords = {Geological conditions prediction, Rock mass type change, Big data analytics, Data mining, Data compression, Classification and prediction},
abstract = {This paper presents a comprehensive procedure to predict geological conditions (i.e., rock mass types) for a tunneling boring machine (TBM) based on big operational data including four channels: cutterhead speed, cutterhead torque, thrust, and advance rate. To handle the big operational data, a Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) algorithm is adopted to effectively compress 12,038,636 TBM operational data to only 5014 leaf node entries. A K-means++ algorithm is used to find potential rock mass types in the TBM operational data. By comparing three kinds of classifiers, a support vector classifier (SVC) with an average precision of 98.6% is selected as the best geological conditions prediction model. Test results on historical TBM operational segments show that most adjacent operational segments have the same rock mass type. The change in rock mass type is a dynamic process, which first fluctuates between two rock mass types and gradually stabilizes at the latter type. In addition, the cutterhead torque and thrust are found to better reflect the change of rock mass types compared with the advance rate and cutterhead speed. Test results on a water conveyance tunnel show that using only 20% of TBM training data the developed prediction model can generate 84.4% precision and 88.8% recall performance for the remaining 80% testing data. Hence, the proposed procedure could be applied to big TBM operational data to accurately detect, characterize, and predict rock mass types, which is of critical importance to safe and efficient tunneling.}
}
@incollection{DZIUBANY2019239,
title = {Chapter 11 - Machine learning-based artificial nose on a low-cost IoT-hardware},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {239-257},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.00011-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376000117},
author = {Matthias Dziubany and Marcel Garling and Anke Schmeink and Guido Burger and Guido Dartmann and Stefan Naumann and Klaus-Uwe Gollmer},
keywords = {Artificial Nose, PCA, SVM, Feature selection, low cost},
abstract = {In order to make Internet of things applications easily available and cost-effective, we aim at using low-cost hardware for typical measurement tasks, and in return putting more effort into the signal processing and data analysis. By the example of beverage recognition with a low-cost temperature-modulated gas sensor, we demonstrate the benefits of processing techniques in big data such as feature selection and dimensionality reduction. Specifically, we determine a subset of temperatures that yields good support vector machine classification results and thereby shortens the measurement process.}
}
@article{LOPEZROBLES201922,
title = {30 years of intelligence models in management and business: A bibliometric review},
journal = {International Journal of Information Management},
volume = {48},
pages = {22-38},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S026840121730244X},
author = {J.R. López-Robles and J.R. Otegi-Olaso and I. {Porto Gómez} and M.J. Cobo},
keywords = {Business Intelligence, Competitive Intelligence, Strategic Intelligence, Science information management, Mapping analysis},
abstract = {The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization’s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.}
}
@article{BELLINI2019521,
title = {Data quality and blockchain technology},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {38},
number = {5},
pages = {521-522},
year = {2019},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2018.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352556818305368},
author = {Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami},
keywords = {Machine learning, Artificial intelligence, Blockchain technology}
}
@article{ROJO2019160,
title = {Near-ground effect of height on pollen exposure},
journal = {Environmental Research},
volume = {174},
pages = {160-169},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119302439},
author = {Jesús Rojo and Jose Oteros and Rosa Pérez-Badia and Patricia Cervigón and Zuzana Ferencova and A. Monserrat Gutiérrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodríguez-De la Cruz} and Estefanía Sánchez-Reyes and José Sánchez-Sánchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calderón and César Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fernández-González and José García and M. Mar Trigo and Dorota Myszkowska and Santiago Fernández-Rodríguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Donát Magyar and László Makra and Orsolya Udvardy and Purificación Alcázar and Carmen Galán and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skjøth and Predrag Radišić and Branko Šikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters},
keywords = {Height, Pollen, Aerobiology, Monitoring network, Big data},
abstract = {The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50 m above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7–2.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10 m AGL). We show that pollen concentrations measured at >10 m are representative for background near-ground levels.}
}
@article{LIU201990,
title = {Constructing Large Scale Cohort for Clinical Study on Heart Failure with Electronic Health Record in Regional Healthcare Platform: Challenges and Strategies in Data Reuse},
journal = {Chinese Medical Sciences Journal},
volume = {34},
number = {2},
pages = {90-102},
year = {2019},
issn = {1001-9294},
doi = {https://doi.org/10.24920/003579},
url = {https://www.sciencedirect.com/science/article/pii/S1001929419300318},
author = {Daowen Liu and Liqi Lei and Tong Ruan and Ping He},
keywords = {electronic health records, clinical terminology knowledge graph, clinical special disease case repository, evaluation of data quality, large scale cohort study},
abstract = {Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.}
}
@article{LIU2019102,
title = {Reinforcement learning-based cell selection in sparse mobile crowdsensing},
journal = {Computer Networks},
volume = {161},
pages = {102-114},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619303706},
author = {Wenbin Liu and Leye Wang and En Wang and Yongjian Yang and Djamal Zeghlache and Daqing Zhang},
keywords = {Mobile crowdsensing, Cell selection, Reinforcement learning, Compressive sensing},
abstract = {Sparse Mobile Crowdsensing (MCS) is a novel MCS paradigm which allows us to use the mobile devices to collect sensing data from only a small subset of cells (sub-areas) in the target sensing area while intelligently inferring the data of other cells with quality guarantee. Since selecting sensed data from different cell sets will probably lead to diverse levels of inference data quality, cell selection (i.e., choosing which cells in the target area to collect sensed data from participants) is a critical issue that will impact the total amount of data that requires to be collected (i.e., data collection costs) for ensuring a certain level of data quality. To address this issue, this paper proposes the reinforcement learning-based cell selection algorithm for Sparse MCS. First, we model the key concepts in reinforcement learning including state, action, and reward, and then propose a Q-learning based cell selection algorithm. To deal with the large state space, we employ the deep Q-network to learn the Q-function that can help decide which cell is a better choice under a certain state during cell selection. Then, we modify the Q-network to a deep recurrent Q-network with LSTM to catch the temporal patterns and handle partial observability. Furthermore, we leverage the transfer learning techniques to relieve the dependency on a large amount of training data. Experiments on various real-life sensing datasets verify the effectiveness of our proposed algorithms over the state-of-the-art mechanisms in Sparse MCS by reducing up to 20% of sensed cells with the same data inference quality guarantee.}
}
@article{JETLEY2019113137,
title = {Electronic health records in IS research: Quality issues, essential thresholds and remedial actions},
journal = {Decision Support Systems},
volume = {126},
pages = {113137},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113137},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301666},
author = {Gaurav Jetley and He Zhang},
keywords = {Electronic health records, Data quality issues, Quality thresholds, Remedial actions},
abstract = {The increase in adoption of Electronic Health Record (EHR) systems by healthcare organizations has led to the proliferation of the use of EHR as a secondary data source in both IS and supporting fields. It is imperative that EHR data is exploited appropriately, which would lead to high quality results and enhanced reproducibility. However, the quality of the EHR data being used can vary significantly and can have repercussions for research outcomes. In this paper, we first discuss four major data quality issues present in EHR data. These issues are: (a) non-standard coding schemes, (b) missing data, (c) inconsistencies and (d) aggregation and augmentation of EHR data. Then, we discuss quality thresholds that need to be met in order to avoid the negative impacts of quality issues. Lastly, we discuss some remedial actions that researchers can take to enhance the quality of EHR data to meet the quality thresholds. The discussed issues, thresholds and remedial actions can also apply to a much wider set of data sources when used as secondary data in research.}
}
@article{HAGEMANN2019160,
title = {Hybrid Artificial Intelligence System for the Design of Highly-Automated Production Systems},
journal = {Procedia Manufacturing},
volume = {28},
pages = {160-166},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313684},
author = {Simon Hagemann and Atakan Sünnetcioglu and Rainer Stark},
keywords = {Artificial Intelligence, Machine Learning, Automotive, Body-in-White, Production System Design, Data Analytics, Pattern Recognition, Process Automatization, Robotics, Industrial Data Quality},
abstract = {The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype.}
}
@incollection{CAVALCANTE2019265,
title = {Chapter 4-2 - Novel Bioinformatics Methods for Toxicoepigenetics},
editor = {Shaun D. McCullough and Dana C. Dolinoy},
booktitle = {Toxicoepigenetics},
publisher = {Academic Press},
pages = {265-288},
year = {2019},
isbn = {978-0-12-812433-8},
doi = {https://doi.org/10.1016/B978-0-12-812433-8.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124338000125},
author = {Raymond G. Cavalcante and Tingting Qin and Maureen A. Sartor},
keywords = {Epigenomic analysis, Toxicoepigenomics, Bisulfite sequencing, Chromatin accessibility, ChIP-seq, Integrative analysis, Chromosomal interactions},
abstract = {The use of high-throughput, genome-wide assays in toxicoepigenetics is rapidly developing and expanding. With recent advances in experimental technologies, a great amount of multiomics epigenomic data has been generated requiring the development of correspondingly advanced bioinformatics approaches to analyze and interpret such big data. This chapter discusses analysis methods for current epigenomic assays available for use in toxicoepigenetic and novel bioinformatics approaches to interpret, visualize, and integrate a variety of epigenomic data and data resources. The epigenomic features covered include DNA methylation, DNA hydroxymethylation, histone modification, chromatin accessibility, and chromatin interaction. For each type of assay used to interrogate those features, bioinformatics tools for data quality control, epigenetic mark detection, comparative analysis, data visualization, functional analysis, and integrative analysis are suggested. Looking forward, it is anticipated that researchers in toxicoepigenomics will adopt newer techniques such as single-cell assays and the bioinformatics methods will continue to evolve.}
}
@article{LI20191234,
title = {Internet of Things to network smart devices for ecosystem monitoring},
journal = {Science Bulletin},
volume = {64},
number = {17},
pages = {1234-1245},
year = {2019},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095927319304013},
author = {Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou},
keywords = {Ecosystem monitoring, Fragile ecosystem, Internet of Things, Wireless sensor network, Smart device},
abstract = {Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and δ13C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.}
}
@article{MA201936,
title = {Investigating the barriers faced by stakeholders in open data development: A study on Hong Kong as a “smart city”},
journal = {Cities},
volume = {92},
pages = {36-46},
year = {2019},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0264275118308205},
author = {Ruiqu Ma and Patrick T.I. Lam},
keywords = {Open data, Barriers, Stakeholders, Social network analysis (SNA), Smart cities},
abstract = {The drive towards open data aims at improving government transparency, motivating citizen participation and unlocking commercial innovation. However, various intertwining barriers hinder the adoption of open data. They are stemming from legislation and licensing, technology and operation, use level, institution and governance, as well as economic considerations. Through the use of social network analysis (SNA), this study identified 43 barriers faced by stakeholders in an open data project in Hong Kong and investigated their interdependencies. Hong Kong was selected as a representative case due to its relatively low ranking in the Global Open Data Index (24th) and poor data quality. It was found that the lack of an open data policy should be tackled as a matter of priority to provide technical guidance for the public sector, ensure data quality and achieve expected outcomes. It is also necessary to improve the IT literacy/mindset of the public sector, refine the governance structure relating to the delivery of open data initiatives, encourage engagement from private entities and provide a feedback loop for users. This study explored the interrelationships between various barriers to open data adoption and proposes practical recommendations to enhance open data development in the context of emerging “smart cities”.}
}
@article{VIAL2019113133,
title = {Reflections on quality requirements for digital trace data in IS research},
journal = {Decision Support Systems},
volume = {126},
pages = {113133},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113133},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301629},
author = {Gregory Vial},
keywords = {Digital trace data, Data quality, GitHub},
abstract = {In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.}
}
@article{CORRALESGARAY201977,
title = {Knowledge areas, themes and future research on open data: A co-word analysis},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {77-87},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18303216},
author = {Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-María Mora-Valentín},
keywords = {Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge areas, Most-studied themes, Future trends},
abstract = {This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.}
}
@article{LOOTEN2019104825,
title = {What can millions of laboratory test results tell us about the temporal aspect of data quality? Study of data spanning 17 years in a clinical data warehouse},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104825},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718307089},
author = {Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne Landau-Loriot and Benoit Vedie and Jean-Louis Paul and Laëtitia Mauge and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and Bastien Rance},
keywords = {Quality control, Computational biology/methods*, Information storage and retrieval, Humans, Clinical laboratory information systems},
abstract = {Objective
To identify common temporal evolution profiles in biological data and propose a semi-automated method to these patterns in a clinical data warehouse (CDW).
Materials and Methods
We leveraged the CDW of the European Hospital Georges Pompidou and tracked the evolution of 192 biological parameters over a period of 17 years (for 445,000 + patients, and 131 million laboratory test results).
Results
We identified three common profiles of evolution: discretization, breakpoints, and trends. We developed computational and statistical methods to identify these profiles in the CDW. Overall, of the 192 observed biological parameters (87,814,136 values), 135 presented at least one evolution. We identified breakpoints in 30 distinct parameters, discretizations in 32, and trends in 79.
Discussion and conclusion
our method allowed the identification of several temporal events in the data. Considering the distribution over time of these events, we identified probable causes for the observed profiles: instruments or software upgrades and changes in computation formulas. We evaluated the potential impact for data reuse. Finally, we formulated recommendations to enable safe use and sharing of biological data collection to limit the impact of data evolution in retrospective and federated studies (e.g. the annotation of laboratory parameters presenting breakpoints or trends).}
}
@article{AMUTHABALA2019233,
title = {Robust analysis and optimization of a novel efficient quality assurance model in data warehousing},
journal = {Computers & Electrical Engineering},
volume = {74},
pages = {233-244},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618318470},
author = {P. Amuthabala and R. Santhosh},
keywords = {Data warehouse, Distributed, Data complexity, Data quality, Quality assurance, Optimization, Machine learning},
abstract = {The significance of distributed data warehouses is to initiate the proliferation of various analytical applications. However, with the increase of ubiquitous devices, it is likely that massive volumes of data will be generated, which poses further problems based on the degradation of data quality. The practical reasons for the degradation of data quality in distributed warehouses are identified as heterogeneous data, uncertain inferior data which further affect predictions. The proposed system presents an integrated optimization model to address all the quality degradation problems and to provide a better computational model which effectively incorporates a higher degree of quality assurance. An analytical methodology is adopted in order to develop the proposed quality assurance model for distributed data warehouses.}
}
@article{DENG2019712,
title = {Exploring spatial–temporal relations via deep convolutional neural networks for traffic flow prediction with incomplete data},
journal = {Applied Soft Computing},
volume = {78},
pages = {712-721},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618306082},
author = {Shaojiang Deng and Shuyuan Jia and Jing Chen},
keywords = {Traffic flow prediction, Deep learning, Intelligent transportation systems},
abstract = {Traffic flow prediction is a fundamental component in intelligent transportation systems. Various computational methods have been applied in this field, among which machine learning based methods are believed to be promising and scalable for big data. In general, most of machine learning based methods encounter three fundamental issues: feature representation of traffic patterns, learning from single location or network, and data quality. In order to address these three issues, in this work we present a deep architecture for traffic flow prediction that learns deep hierarchical feature representation with spatio-temporal relations over the traffic network. Furthermore, we design an ensemble learning strategy via random subspace learning to make the model be able to tolerate incomplete data. Accordingly the contributions of this work are summarized as the three points. First, we transform the time series analysis problem into the task of image-like analysis. Benefitting from the image-like data form, we can jointly explore spatio-temporal relations simultaneously by the two-dimension convolution operator. In addition, the proposed model can tolerate the incomplete data, which is very common in traffic application field. Finally, we propose an improved random search based on uniform design in order to optimize hyper-parameters for deep Convolutional Neural Networks (deep CNN). A large range of experiments with various traffic conditions have been performed on the traffic data originated from the California Freeway Performance Measurement System (PeMS). The experimental results corroborate the effectiveness of the proposed approach compared with the state of the art.}
}
@article{WONG2019602,
title = {Bootstrap standard error estimations of nonlinear transport models based on linearly projected data},
journal = {Transportmetrica A Transport Science},
volume = {15},
number = {2},
pages = {602-630},
year = {2019},
issn = {2324-9935},
doi = {https://doi.org/10.1080/23249935.2018.1519647},
url = {https://www.sciencedirect.com/science/article/pii/S2324993522002524},
author = {Wai Wong and S.C. Wong and Henry X. Liu},
keywords = {Big data era, linear data projection, heteroscedasticity, bootstrap standard error, macroscopic fundamental diagram},
abstract = {ABSTRACT
Linear data projection is a commonly leveraged data scaling method for unbiased traffic data estimation. However, recent studies have shown that model estimations based on linearly projected data would certainly result in biased standard errors. Although methods have been developed to remove such biases for linear regression models, many transport models are nonlinear regression models. This study outlines the practical difficulties of the traditional approach to standard error estimation for generic nonlinear transport models, and proposes a bootstrapping mean value restoration method to accurately estimate the parameter standard errors of all nonlinear transport models based on linearly projected data. Comprehensive simulations with different settings using the most commonly adopted nonlinear functions in modeling traffic flow demonstrate that the proposed method outperforms the conventional method and accurately recovers the true standard errors. A case study of estimating a macroscopic fundamental diagram that illustrates situations necessitating the proposed method is presented.}
}
@article{SUSHA2019112,
title = {Data driven social partnerships: Exploring an emergent trend in search of research challenges and questions},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {112-128},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17302708},
author = {Iryna Susha and Åke Grönlund and Rob {Van Tulder}},
keywords = {Data partnership, Data collaborative, Data philanthropy, Data donation, Big data, Collaboration},
abstract = {The volume of data collected by multiple devices, such as mobile phones, sensors, satellites, is growing at an exponential rate. Accessing and aggregating different sources of data, including data outside the public domain, has the potential to provide insights for many societal challenges. This catalyzes new forms of partnerships between public, private, and nongovernmental actors aimed at leveraging different sources of data for positive societal impact and the public good. In practice there are different terms in use to label these partnerships but research has been lagging behind in systematically examining this trend. In this paper, we deconstruct the conceptualization and examine the characteristics of this emerging phenomenon by systematically reviewing academic and practitioner literature. To do so, we use the grounded theory literature review method. We identify several concepts which are used to describe this phenomenon and propose an integrative definition of “data driven social partnerships” based on them. We also identify a list of challenges which data driven social partnerships face and explore the most urgent and most cited ones, thereby proposing a research agenda. Finally, we discuss the main contributions of this emerging research field, in relation to the challenges, and systematize the knowledge base about this phenomenon for the research community.}
}
@article{HA2019101078,
title = {Where WTS meets WTB: A Blockchain-based Marketplace for Digital Me to trade users’ private data},
journal = {Pervasive and Mobile Computing},
volume = {59},
pages = {101078},
year = {2019},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2019.101078},
url = {https://www.sciencedirect.com/science/article/pii/S1574119218305947},
author = {Miyeon Ha and Sujeong Kwon and Yeon Joo Lee and Yealin Shim and Jinwoo Kim},
keywords = {Personal data, Data market, Blockchain, Privacy concern, Moral hazard},
abstract = {People are reluctant to provide their personal data, making it difficult for corporations to collect such information. The Personal Data Market presents a promising solution to this problem, but current offerings must overcome several challenges. This paper proposes a Blockchain-based Data Market framework, “Decentralized Data Marketplace for Digital Me”, in which personal data are anonymized in accordance with the General Data Protection Regulation to protect the owner’s privacy. In addition, this paper presents a user scenario for the framework in the form of Financial Me, which provides a reasonable price range and adequate incentives for data quality.}
}
@article{SHENG2019321,
title = {Technology in the 21st century: New challenges and opportunities},
journal = {Technological Forecasting and Social Change},
volume = {143},
pages = {321-335},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517311319},
author = {Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang},
keywords = {Business intelligence, Big data, Big data analytics, Advanced techniques, Decision-making},
abstract = {Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.}
}
@article{ANDRADE2019102352,
title = {Cognitive security: A comprehensive study of cognitive science in cybersecurity},
journal = {Journal of Information Security and Applications},
volume = {48},
pages = {102352},
year = {2019},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2214212618307804},
author = {Roberto O Andrade and Sang Guun Yoo},
keywords = {Cognitive security, Cognitive science, Situation awareness, Cyber operations},
abstract = {Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.}
}
@article{MONTANS2019845,
title = {Data-driven modeling and learning in science and engineering},
journal = {Comptes Rendus Mécanique},
volume = {347},
number = {11},
pages = {845-855},
year = {2019},
note = {Data-Based Engineering Science and Technology},
issn = {1631-0721},
doi = {https://doi.org/10.1016/j.crme.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1631072119301809},
author = {Francisco J. Montáns and Francisco Chinesta and Rafael Gómez-Bombarelli and J. Nathan Kutz},
keywords = {Data-driven science, Data-driven modeling, Artificial intelligence, Machine learning, Data-science, Big data},
abstract = {In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.}
}
@article{CEPNI2019555,
title = {Nowcasting and forecasting GDP in emerging markets using global financial and macroeconomic diffusion indexes},
journal = {International Journal of Forecasting},
volume = {35},
number = {2},
pages = {555-572},
year = {2019},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0169207018301985},
author = {Oguzhan Cepni and I. Ethem Güney and Norman R. Swanson},
keywords = {Diffusion index, Dimension reduction methods, Emerging markets, Factor model, Forecasting, Variable selection},
abstract = {This paper contributes to the nascent literature on nowcasting and forecasting GDP in emerging market economies using big data methods. This is done by analyzing the usefulness of various dimension-reduction, machine learning and shrinkage methods, including sparse principal component analysis (SPCA), the elastic net, the least absolute shrinkage operator, and least angle regression when constructing predictions using latent global macroeconomic and financial factors (diffusion indexes) in a dynamic factor model (DFM). We also utilize a judgmental dimension-reduction method called the Bloomberg Relevance Index (BRI), which is an index that assigns a measure of importance to each variable in a dataset depending on the variable’s usage by market participants. Our empirical analysis shows that, when specified using dimension-reduction methods (particularly BRI and SPCA), DFMs yield superior predictions relative to both benchmark linear econometric models and simple DFMs. Moreover, global financial and macroeconomic (business cycle) diffusion indexes constructed using targeted predictors are found to be important in four of the five emerging market economies that we study (Brazil, Mexico, South Africa, and Turkey). These findings point to the importance of spillover effects across emerging market economies, and underscore the significance of characterizing such linkages parsimoniously when utilizing high-dimensional global datasets.}
}
@article{OZKAN2019208,
title = {Criminology in the age of data explosion: New directions},
journal = {The Social Science Journal},
volume = {56},
number = {2},
pages = {208-219},
year = {2019},
issn = {0362-3319},
doi = {https://doi.org/10.1016/j.soscij.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0362331918301514},
author = {Turgut Ozkan},
keywords = {Social science, Big data, Crime, Social media, Data-driven social science},
abstract = {This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate ‘big data’, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.}
}
@article{LIONO2019196,
title = {QDaS: Quality driven data summarisation for effective storage management in Internet of Things},
journal = {Journal of Parallel and Distributed Computing},
volume = {127},
pages = {196-208},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S074373151830220X},
author = {Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim},
keywords = {Quality of data, Storage management, Internet of Things (IoT), Cloud computing, Quality of service, Data summarisation},
abstract = {The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.}
}
@article{RUMSON2019598,
title = {Innovations in the use of data facilitating insurance as a resilience mechanism for coastal flood risk},
journal = {Science of The Total Environment},
volume = {661},
pages = {598-612},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.01.114},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719301317},
author = {Alexander G. Rumson and Stephen H. Hallett},
keywords = {Risk analytics, Adaptation, Remote sensing, Big Data},
abstract = {Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.}
}
@article{SINGH20191147,
title = {A Systemic Cybercrime Stakeholders Architectural Model},
journal = {Procedia Computer Science},
volume = {161},
pages = {1147-1155},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.227},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319386},
author = {Manmeet Mahinderjit Singh and Anizah Abu Bakar},
keywords = {Big data, Internet of Things (IoT), Cyberspace, Routine Activity Theory, Systemic},
abstract = {The increased of cybercrime incidents taking place in the world is at its perilous magnitude causing losses in term of money and trust. Even though there are various cybersecurity solutions in place; the threat of cybercrime is still a hard problem. Exploration of cybercrime challenges, especially the preventions and detections of the cybercrime should be investigated by composing all the stakeholders and players of a cybercrime issue. In this paper; an exploration of several cybercrime stakeholders is done. It is argued that cybercrime is a systemic threat and cannot be tackled with cybersecurity and legal systems. The architectural model proposed is significant and should become one of the considered milestones in designing security control in tackling cybercrime globally.}
}
@article{OMAR2019100127,
title = {Business analytics in manufacturing: Current trends, challenges and pathway to market leadership},
journal = {Operations Research Perspectives},
volume = {6},
pages = {100127},
year = {2019},
issn = {2214-7160},
doi = {https://doi.org/10.1016/j.orp.2019.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2214716019300934},
author = {Yamila M. Omar and Meysam Minoufekr and Peter Plapper},
keywords = {Manufacturing, Business analytics, Big data, Industry 4.0},
abstract = {The manufacturing sector is under constant pressure to increase profitability in a growingly competitive international market in which differentiation is not tied to manufactured products or utilized technologies but to business processes optimization. In this context, business analytics offer the opportunity to harness the knowledge and value hidden within enterprise information systems to revolutionize innovation, enhance supply chain management and production, accurately target marketing and sales efforts, as well as develop and manage profitable after-sales services. While the literature to date presents numerous specific applications in which business analytics techniques were successfully deployed to improve specific business units, it is evident that a comprehensive enterprise approach is missing. In the present work, a pathway to attain market leadership through the effective use of business analytics is defined suggesting focus must center on three increasingly challenging barriers. Firstly, “standardization” of collection, aggregation and storage of data must be accomplished. Then, an “organizational culture evolution” that outgrows intuition and embraces data-driven decision-making is needed to create the perfect ecosystem for business analytics to produce actionable results and recommendations. In turn, these must guide “business model innovation” efforts to tackle new value creation, and capture and secure market leadership.}
}
@article{ZENG2019289,
title = {Comparative study of data driven methods in building electricity use prediction},
journal = {Energy and Buildings},
volume = {194},
pages = {289-300},
year = {2019},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0378778818303682},
author = {Aaron Zeng and Sheng Liu and Yao Yu},
keywords = {Energy prediction, Data driven, Electricity consumption, Big data, Comparative},
abstract = {The energy use prediction of building systems is crucial to design a high-efficiency building and maintain low energy consumption operation, which is also important in optimizing building system control and retrofitting. This paper demonstrates a comparative study of four data-driven methods used in online building energy predictions involving large-scale data extracted from several types of buildings. The characteristics of building electricity use and data reliability were addressed through the data pre-treatment process including visualization, cleaning, parsing and filtering. Mathematical algorithms and their applications in previous studies were summarized and compared, and evaluation methods were developed. The performance and suitable application scenarios of the proposed algorithms were conducted via the comparison of monitoring data and predicted results. The study indicates that the most complex method which requires the highest computation ability, i.e., the Artificial Neural Network (ANN), does not lead to the highest accuracy, while as the fastest computation method, Gaussian Process Regression (GPR) usually has the results with the lowest accuracy. Support Vector Machine (SVM) and Multivariate Linear Regression (MLR) methods usually perform better in the case scenarios studied. All the prediction accuracies can meet the requirements of RMSE <30% and NMBE <10% proposed by ASHRAE, and the computation time varies from less than 1 s to 22 s per prediction. All these methods/algorithms worked well for buildings with stable energy use patterns. For buildings with complex and unstable occupancy schedules and energy use patterns, MLR and SVM methods have the ability to achieve a high accuracy with fast computation speed.}
}
@article{AKTER201985,
title = {Analytics-based decision-making for service systems: A qualitative study and agenda for future research},
journal = {International Journal of Information Management},
volume = {48},
pages = {85-95},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218312696},
author = {Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos},
keywords = {Big data analytics, Decision-making, Service systems},
abstract = {While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.}
}
@article{HANDFIELD2019194,
title = {Shifts in buyer-seller relationships: A retrospective on Handfield and Bechtel (2002)},
journal = {Industrial Marketing Management},
volume = {83},
pages = {194-206},
year = {2019},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0019850119300835},
author = {Robert Handfield},
abstract = {One of the most highly cited papers in the Industrial Marketing Management journal was published 17 years ago, and proposed a strong linkage in the elements of buyer-seller trust, asset specificity, contracts, and supply chain performance. In this paper, explore the question of “what has changed”? We note that the emergence of 1) real-time analytic technologies, 2) new governance models that span multiple parties across organizations in a supply chain network, and 3) new digital innovation requiring partnering with new entities are required to produce end to end analytical capabilities. We offer three new propositions that provide some insights towards future research areas, and we also note that although interpersonal buyer-seller relationships will remain important, digital transformation is changing the nature of how these will unfold. Our propositions provide insights on how the role of technology and other shifts in the supply chain ecosystem is shifting the role of buyers and sellers in the industrial landscape. I offer these insights in the hope that they may provide a basis for future researchers to engage in research in the field of emerging industrial buyer-seller relationships, and devote this paper to the memory of Christian.}
}
@article{CROWN2019587,
title = {Real-World Evidence, Causal Inference, and Machine Learning},
journal = {Value in Health},
volume = {22},
number = {5},
pages = {587-592},
year = {2019},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1098301519301329},
author = {William H. Crown},
keywords = {big data, causal inference, econometrics, epidemiology, machine learning, real-world evidence, targeted maximum likelihood estimator},
abstract = {The current focus on real world evidence (RWE) is occurring at a time when at least two major trends are converging. First, is the progress made in observational research design and methods over the past decade. Second, the development of numerous large observational healthcare databases around the world is creating repositories of improved data assets to support observational research.
Objective
This paper examines the implications of the improvements in observational methods and research design, as well as the growing availability of real world data for the quality of RWE. These developments have been very positive. On the other hand, unstructured data, such as medical notes, and the sparcity of data created by merging multiple data assets are not easily handled by traditional health services research statistical methods. In response, machine learning methods are gaining increased traction as potential tools for analyzing massive, complex datasets.
Conclusions
Machine learning methods have traditionally been used for classification and prediction, rather than causal inference. The prediction capabilities of machine learning are valuable by themselves. However, using machine learning for causal inference is still evolving. Machine learning can be used for hypothesis generation, followed by the application of traditional causal methods. But relatively recent developments, such as targeted maximum likelihood methods, are directly integrating machine learning with causal inference.}
}
@article{ELKASSAR2019483,
title = {Green innovation and organizational performance: The influence of big data and the moderating role of management commitment and HR practices},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {483-498},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517315226},
author = {Abdul-Nasser El-Kassar and Sanjay Kumar Singh},
keywords = {Green innovation, Corporate environmental ethics, Large scale data, Human resource practices, Management commitment, Environmental and economic performance},
abstract = {Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.}
}
@article{WONG2019169,
title = {On the estimation of connected vehicle penetration rate based on single-source connected vehicle data},
journal = {Transportation Research Part B: Methodological},
volume = {126},
pages = {169-191},
year = {2019},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0191261518309834},
author = {Wai Wong and Shengyin Shen and Yan Zhao and Henry X. Liu},
keywords = {Era of big data and IoT, Connected vehicle, Penetration rate, Single-source data, Fundamental diagram},
abstract = {With more connected vehicles (CVs) in the networks, the big data era leads to the availability of abundant data from CVs. CV penetration rate is the fundamental building block of tremendous applications, such as traffic data estimation, CV-based adaptive signal control and origin-destination estimation. While CV penetration rate is a random variable unknown in nature, the current estimation method of penetration rate mainly relies on two sources of data —detector and CV data. Penetration rate across the link is computed as CV flow divided by all traffic flow over a certain period of time. However, the current method is constrained by availability and quality of detector data. This paper proposes a simple, analytical, non-parametric, and most importantly, unbiased single-source data penetration rate (SSDPR) estimation method for estimating penetration rate solely based on CV data. It subtly and simultaneously fuses two estimation mechanisms—(1) the measurement of the probability of the first vehicle in a queue being a CV and (2) the direct estimation of the penetration rate of a sample queue—to constitute a single estimator to handle all the possible sample queue patterns. Applicability of the proposed method is not confined to a specific arrival pattern. It solely utilizes the number of the observed CVs and the number of vehicles before the last observed CV in a sample queue. Combining with bridging the queue algorithm, the proposed SSDPR estimation method is extended to overflow or oversaturated conditions. Simulation results show that the proposed method is able to accurately estimate penetration rate as low as 0.1% for all the situations considered. To illustrate the applicability of the proposed method, a case study of fundamental diagram estimation of a link without being installed with any detector is presented.}
}
@article{OLEARY2019113139,
title = {Technology life cycle and data quality: Action and triangulation},
journal = {Decision Support Systems},
volume = {126},
pages = {113139},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113139},
url = {https://www.sciencedirect.com/science/article/pii/S016792361930168X},
author = {Daniel E. O'Leary},
keywords = {Technology life cycle, Data quality, Non-stationary data, Hype cycle, Data biases, Data phase triangulation},
abstract = {Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of “data phase triangulation,” which is contrasted with “methodology triangulation” and “data (collection) triangulation.” In addition, we discuss the importance of being able to use the findings from life cycle-based research to “push” a technology from one phase to another phase.}
}
@article{YEBENES2019614,
title = {Towards a Data Governance Framework for Third Generation Platforms},
journal = {Procedia Computer Science},
volume = {151},
pages = {614-621},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.082},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305447},
author = {Juan Yebenes and Marta Zorrilla},
keywords = {Data governance, Industry 4.0, data bus architecture, cloud computing, IoT, big data},
abstract = {The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.}
}
@article{SOLANKI2019476,
title = {Towards a knowledge driven framework for bridging the gap between software and data engineering},
journal = {Journal of Systems and Software},
volume = {149},
pages = {476-484},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302772},
author = {Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan},
keywords = {Ontologies, Data engineering, Software engineering, Alignment, Integration},
abstract = {In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.}
}
@article{LAKIND2019302,
title = {ExpoQual: Evaluating measured and modeled human exposure data},
journal = {Environmental Research},
volume = {171},
pages = {302-312},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119300465},
author = {Judy S. LaKind and Cian O’Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman},
keywords = {ExpoQual, BEES-C, Exposure, Human, Quality, Fit-for-purpose, Instrument, Biomonitoring, Model uncertainty},
abstract = {Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).}
}
@article{KOCHOVSKI2019747,
title = {Trust management in a blockchain based fog computing platform with trustless smart oracles},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {747-759},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301281},
author = {Petar Kochovski and Sandi Gec and Vlado Stankovski and Marko Bajec and Pavel D. Drobintsev},
keywords = {Trust, Fog, Blockchain, Smart contract, Smart oracle},
abstract = {Trust is a crucial aspect when cyber-physical systems have to rely on resources and services under ownership of various entities, such as in the case of Edge, Fog and Cloud computing. The DECENTER’s Fog Computing Platform is developed to support Big Data pipelines, which start from the Internet of Things (IoT), such as cameras that provide video-streams for subsequent analysis. It is used to implement Artificial Intelligence (AI) algorithms across the Edge-Fog-Cloud computing continuum which provide benefits to applications, including high Quality of Service (QoS), improved privacy and security, lower operational costs and similar. In this article, we present a trust management architecture for DECENTER that relies on the use of blockchain-based Smart Contracts (SCs) and specifically designed trustless Smart Oracles. The architecture is implemented on Ethereum ledger (testnet) and three trust management scenarios are used for illustration. The scenarios (trust management for cameras, trusted data flow and QoS based computing node selection) are used to present the benefits of establishing trust relationships among entities, services and stakeholders of the platform.}
}
@article{XIANG20191180,
title = {Dynamic cooperation strategies of the closed-loop supply chain involving the internet service platform},
journal = {Journal of Cleaner Production},
volume = {220},
pages = {1180-1193},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.01.310},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619303373},
author = {Zehua Xiang and Minli Xu},
keywords = {Big data marketing, Differential game, Closed-loop supply chain, Internet service platform},
abstract = {In the age of “Internet+”, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has “free rider” tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.}
}
@article{YE2019936,
title = {Improved population mapping for China using remotely sensed and points-of-interest data within a random forests model},
journal = {Science of The Total Environment},
volume = {658},
pages = {936-946},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2018.12.276},
url = {https://www.sciencedirect.com/science/article/pii/S0048969718351489},
author = {Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia},
keywords = {Points of interest, Population, Random forests, Nighttime light, China},
abstract = {Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.}
}
@article{ALSHAER2019792,
title = {IBRIDIA: A hybrid solution for processing big logistics data},
journal = {Future Generation Computer Systems},
volume = {97},
pages = {792-804},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1830606X},
author = {Mohammed AlShaer and Yehia Taher and Rafiqul Haque and Mohand-Saïd Hacid and Mohamed Dbouk},
keywords = {Realtime processing, Clustering, Big data, Internet of Things, Logistics, Hierarchical clustering algorithm},
abstract = {Internet of Things (IoT) is leading to a paradigm shift within the logistics industry. Logistics services providers use sensor technologies such as GPS or telemetry to track and manage their shipment processes. Additionally, they use external data that contain critical information about events such as traffic, accidents, and natural disasters. Correlating data from different sensors and social media and performing analysis in real-time provide opportunities to predict events and prevent unexpected delivery delay at run-time. However, collecting and processing data from heterogeneous sources foster problems due to the variety and velocity of data. In addition, processing data in real-time is heavily challenging that it cannot be dealt with using conventional logistics information systems. In this paper, we present a hybrid framework for processing massive volume of data in batch style and real-time. Our framework is built upon Johnson’s hierarchical clustering (HCL) algorithm which produces a dendrogram that represents different clusters of data objects.}
}
@article{EYOB201927,
title = {Ensuring safe surgical care across resource settings via surgical outcomes data & quality improvement initiatives},
journal = {International Journal of Surgery},
volume = {72},
pages = {27-32},
year = {2019},
note = {Endoscopic Surgery},
issn = {1743-9191},
doi = {https://doi.org/10.1016/j.ijsu.2019.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S174391911930189X},
author = {Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger},
keywords = {Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality improvement, Big data},
abstract = {Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.}
}
@article{KATARIA2019101429,
title = {Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {33},
number = {4},
pages = {101429},
year = {2019},
note = {How to Investigate: Very Early Inflammatory Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2019.101429},
url = {https://www.sciencedirect.com/science/article/pii/S1521694219300981},
author = {Suchitra Kataria and Vinod Ravindran},
keywords = {Artificial intelligence, Big data, Machine learning, Data analytics, Wearable devices, Robotics, Digital health},
abstract = {Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.}
}
@article{QUINONESGRUEIRO201956,
title = {Data-driven monitoring of multimode continuous processes: A review},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {189},
pages = {56-71},
year = {2019},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2019.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169743918306774},
author = {Marcos Quiñones-Grueiro and Alberto Prieto-Moreno and Cristina Verde and Orestes Llanes-Santiago},
keywords = {Process monitoring, Multimode process, Data-driven modeling},
abstract = {The Internet of Things benefits connectivity and functionality in industrial environments, while Cloud Computing boosts computational capability. Hence, historical data from processes allows developing automatic strategies for improving efficiency and security. Automatic process monitoring tasks are important due to the complexity of real processes where multiple operating conditions must be considered for achieving a satisfactory performance. In the last 20 years, over 100 publications on data-driven monitoring for multimode continuous processes have been released. Therefore, this work provides a review on this topic. The data-driven modeling problem for monitoring multimode continuous processes is introduced. The analysis of the clustering methods and monitoring approaches is the main contribution of the review. This study includes advantages and drawbacks of every analyzed strategy. Finally, promising research directions towards the Industry 4.0 and the Big Data era are discussed.}
}
@article{MOLL2019100833,
title = {The role of internet-related technologies in shaping the work of accountants: New directions for accounting research},
journal = {The British Accounting Review},
volume = {51},
number = {6},
pages = {100833},
year = {2019},
note = {Innovative Governance and Sustainable Pathways in a Disruptive Environment},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0890838919300459},
author = {Jodie Moll and Ogan Yigitbasioglu},
keywords = {Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence},
abstract = {This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.}
}
@article{MIETH2019868,
title = {Framework for the usage of data from real-time indoor localization systems to derive inputs for manufacturing simulation},
journal = {Procedia CIRP},
volume = {81},
pages = {868-873},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.216},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119305219},
author = {Carina Mieth and Anne Meyer and Michael Henke},
keywords = {real-time indoor localization system, input data management, cyber-physical system, manufacturing simulation, digital twin},
abstract = {Discrete event simulation is becoming increasingly important in the planning and operation of complex manufacturing systems. A major problem with today’s approach to manufacturing simulation studies is the collection and processing of data from heterogeneous sources, because the data is often of poor quality and does not contain all the necessary information for a simulation. This work introduces a framework that uses a real-time indoor localization systems (RTILS) as a central main data harmonizer, that is designed to feed production data into a manufacturing simulation from a single source of truth. It is shown, based on different data quality dimensions, how this contributes to a better overall data quality in manufacturing simulation. Furthermore, a detailed overview on which simulation inputs can be derived from the RTILS data is given.}
}
@article{GRABOWSKI201921,
title = {A Primer on Data Analytics in Functional Genomics: How to Move from Data to Insight?},
journal = {Trends in Biochemical Sciences},
volume = {44},
number = {1},
pages = {21-32},
year = {2019},
issn = {0968-0004},
doi = {https://doi.org/10.1016/j.tibs.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0968000418302263},
author = {Piotr Grabowski and Juri Rappsilber},
keywords = {data integration, data science, functional genomics, machine learning, systems biology},
abstract = {High-throughput methodologies and machine learning have been central in developing systems-level perspectives in molecular biology. Unfortunately, performing such integrative analyses has traditionally been reserved for bioinformaticians. This is now changing with the appearance of resources to help bench-side biologists become skilled at computational data analysis and handling large omics data sets. Here, we show an entry route into the field of omics data analytics. We provide information about easily accessible data sources and suggest some first steps for aspiring computational data analysts. Moreover, we highlight how machine learning is transforming the field and how it can help make sense of biological data. Finally, we suggest good starting points for self-learning and hope to convince readers that computational data analysis and programming are not intimidating.}
}
@article{SAEZ2019104954,
title = {Guest editorial: Special issue in biomedical data quality assessment methods},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104954},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719309174},
author = {Carlos Sáez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez}
}
@article{ETTEHADTAVAKKOL2019293,
title = {A data analytic workflow to forecast produced water from Marcellus shale},
journal = {Journal of Natural Gas Science and Engineering},
volume = {61},
pages = {293-302},
year = {2019},
issn = {1875-5100},
doi = {https://doi.org/10.1016/j.jngse.2018.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S1875510018305201},
author = {Amin Ettehadtavakkol and Ali Jamali},
keywords = {Data analytics, Marcellus shale gas, Produced water management, Spatiotemporal analysis, Statistical imputation, Large-scale low-integrity big data problems},
abstract = {Water and gas production and potential water treatment facility requirements for the Marcellus formation are discussed using data analytic methods. These methods aim to handle dataset diversity and scale, and apply data analytics for statistical imputation, estimating future drilling activity, fluids production, and the optimization of water recycling facility locations and size. The objective of this study is to quantify and predict the volumes of produced fluids in the short- and medium-term for the Marcellus shale. The paper accomplishes this objective for the Pennsylvania section comprising 10,000 wells. The application of data analytics to large-scale, data-intensive, low-integrity public environmental databases is illustrated, and challenges of implementation methods are discussed and resolved. In addition, a special class of data analytic tools and workflows for spatiotemporal analysis (spatially correlated variation of parameters with time) is discussed and implemented. The results quantify the prospect of future drilling activity, and water and gas production for all Pennsylvania counties in the Marcellus. Finally, several practical problems of interest on applications of predictive analytics and management support are proposed and solved. The limitations of the proposed workflow are briefly discussed.}
}
@article{RIESENER2019304,
title = {Framework for defining information quality based on data attributes within the digital shadow using LDA},
journal = {Procedia CIRP},
volume = {83},
pages = {304-310},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.131},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119304366},
author = {Michael Riesener and Christian Dölle and Günther Schuh and Christian Tönnes},
keywords = {Digital Shadow, Information quality, Data quality, Latent dirichlet allocation (LDA)},
abstract = {The amount of data, which is created in companies is increasing due to modern communication technologies and decreasing costs for storing data. This leads to an advancement of methods for data analyses as well as to an increasing awareness of benefits resulting from data-based knowledge. In the context of product service systems and product development, there are two major concepts for providing product information. The digital twin collects every information possible, while the digital shadow provides a sufficient and content-related picture of the product. Since these concepts merge data from different sources, comprehension about information quality and its relation to the data quality becomes immanently important. This paper introduces a framework to determine information quality with respect to data-related and system-related attributes. An extensive literature review with focus on “information quality” and “data quality” identifies the important approaches for describing information and data quality. A latent dirichlet allocation (LDA) algorithm is applied on 371 definitions and identify 12 data-related and system-related attributes for information quality. Those attributes are assigned to six dimensions for information quality. So the proposed framework depicts the relationships between data attributes and the influence on information quality.}
}
@article{NEWHART2019498,
title = {Data-driven performance analyses of wastewater treatment plants: A review},
journal = {Water Research},
volume = {157},
pages = {498-513},
year = {2019},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2019.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S0043135419302490},
author = {Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath},
keywords = {Wastewater treatment, Big data, Statistical process control, Process optimization, Monitoring},
abstract = {Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.}
}
@article{KLERKX2019100315,
title = {A review of social science on digital agriculture, smart farming and agriculture 4.0: New contributions and a future research agenda},
journal = {NJAS - Wageningen Journal of Life Sciences},
volume = {90-91},
pages = {100315},
year = {2019},
issn = {1573-5214},
doi = {https://doi.org/10.1016/j.njas.2019.100315},
url = {https://www.sciencedirect.com/science/article/pii/S1573521419301769},
author = {Laurens Klerkx and Emma Jakku and Pierre Labarthe},
keywords = {Robotic farming, Precision agriculture, Digitalization, Digital social science, Data science, Responsible research and innovation, Agricultural knowledge and innovation systems},
abstract = {While there is a lot of literature from a natural or technical sciences perspective on different forms of digitalization in agriculture (big data, internet of things, augmented reality, robotics, sensors, 3D printing, system integration, ubiquitous connectivity, artificial intelligence, digital twins, and blockchain among others), social science researchers have recently started investigating different aspects of digital agriculture in relation to farm production systems, value chains and food systems. This has led to a burgeoning but scattered social science body of literature. There is hence lack of overview of how this field of study is developing, and what are established, emerging, and new themes and topics. This is where this article aims to make a contribution, beyond introducing this special issue which presents seventeen articles dealing with social, economic and institutional dynamics of precision farming, digital agriculture, smart farming or agriculture 4.0. An exploratory literature review shows that five thematic clusters of extant social science literature on digitalization in agriculture can be identified: 1) Adoption, uses and adaptation of digital technologies on farm; 2) Effects of digitalization on farmer identity, farmer skills, and farm work; 3) Power, ownership, privacy and ethics in digitalizing agricultural production systems and value chains; 4) Digitalization and agricultural knowledge and innovation systems (AKIS); and 5) Economics and management of digitalized agricultural production systems and value chains. The main contributions of the special issue articles are mapped against these thematic clusters, revealing new insights on the link between digital agriculture and farm diversity, new economic, business and institutional arrangements both on-farm, in the value chain and food system, and in the innovation system, and emerging ways to ethically govern digital agriculture. Emerging lines of social science enquiry within these thematic clusters are identified and new lines are suggested to create a future research agenda on digital agriculture, smart farming and agriculture 4.0. Also, four potential new thematic social science clusters are also identified, which so far seem weakly developed: 1) Digital agriculture socio-cyber-physical-ecological systems conceptualizations; 2) Digital agriculture policy processes; 3) Digitally enabled agricultural transition pathways; and 4) Global geography of digital agriculture development. This future research agenda provides ample scope for future interdisciplinary and transdisciplinary science on precision farming, digital agriculture, smart farming and agriculture 4.0.}
}
@article{CHI201910,
title = {A supernetwork-based online post informative quality evaluation model},
journal = {Knowledge-Based Systems},
volume = {168},
pages = {10-24},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118306221},
author = {Yuxue Chi and Xianyi Tang and Ying Lian and Xuefan Dong and Yijun Liu},
keywords = {Online public opinion, Informative quality, Super-network, Machine learning},
abstract = {In an era of big data, explosive growth of online posts makes the judgment of their qualities harder while more important. In many cases, people want to quickly identify those most informative posts, which contain details, insights or in-depth criticisms, to help them make better decisions. To meet this demand, a three-stage model is proposed. First, a super-network model is introduced to accommodate the multidimensional attributes of online posts, including keywords, user ID, emotions and the related event. Second, a corpus updating mechanism is introduced to generate event specific corpora, which help to discriminate the informative quality of posts in the next stage. Third, machine learning algorithms are applied, where the posts are first filtered by a linear discriminant classifier and then assessed by a multilayer perceptron neural network. To test the model, we chose six online public opinion events that fell into two categories: major public safety crisis and online controversies about public policies. Experimental results showed the effectiveness of the proposed model, where majority of errors are less than 0.05, on a 0–1 measuring scale. In the future, this model may also be adapted to areas including evaluation of informative quality of websites, product reviews and answers in question and answer communities.}
}
@incollection{CHUI2019111,
title = {Chapter 7 - Smart city is a safe city: information and communication technology–enhanced urban space monitoring and surveillance systems: the promise and limitations},
editor = {Anna Visvizi and Miltiadis D. Lytras},
booktitle = {Smart Cities: Issues and Challenges},
publisher = {Elsevier},
pages = {111-124},
year = {2019},
isbn = {978-0-12-816639-0},
doi = {https://doi.org/10.1016/B978-0-12-816639-0.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166390000077},
author = {Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu},
keywords = {Cyber security, Ethics, Policy-making, Security, Surveillance},
abstract = {Urban space monitoring and surveillance systems are present almost everywhere in various forms of sensing devices such as closed-circuit television, smartphone, and camera. This requires a robust and easy-to-manage information and communication technology (ICT) infrastructure that is generally comprises sensors, protocols, networks, and steps. Smart adoption of such systems could influence, manage, direct, and protect human beings and property. Nevertheless, it may create problems of government support, data quality, privacy, and security. Today's computational world allows implementation of artificial intelligence models for big data analytics to bring cities smart (with intelligence and optimal improvement). This chapter will discuss the applications of urban space monitoring and surveillance systems via ICT. The typical limitations of the current research are discussed in detail.}
}
@article{VELOSO2019457,
title = {Distributed Trust & Reputation Models using Blockchain Technologies for Tourism Crowdsourcing Platforms},
journal = {Procedia Computer Science},
volume = {160},
pages = {457-460},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.065},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931765X},
author = {Bruno Veloso and Fátima Leal and Benedita Malheiro and Fernando Moreira},
keywords = {Trust, Reputation, Data Integrity, Blockchain, Social Computing},
abstract = {Crowdsourced repositories have become an increasingly important source of information for users and businesses in multiple domains. Everyday examples of tourism crowdsourcing platforms focusing on accommodation, food or travelling in general, influence consumer behaviour in modern societies. These repositories, due to their intrinsic openness, can strongly benefit from independent data quality modelling mechanisms. In this context, building trust & reputation models of contributors and storing crowdsourced data using distributed ledger technology allows not only to ascertain the quality of crowdsourced contributions, but also ensures the integrity of the built models. This paper presents a survey on distributed trust & reputation modelling using blockchain technology and, for the specific case of tourism crowdsourcing platforms, discusses the open research problems and identifies future lines of research.}
}
@article{TANDON201970,
title = {Using machine learning to explain the heterogeneity of schizophrenia. Realizing the promise and avoiding the hype},
journal = {Schizophrenia Research},
volume = {214},
pages = {70-75},
year = {2019},
note = {Machine Learning in Schizophrenia},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2019.08.032},
url = {https://www.sciencedirect.com/science/article/pii/S0920996419303883},
author = {Neeraj Tandon and Rajiv Tandon},
keywords = {Machine learning, Schizophrenia, Methods, Research, Neuroscience, Computational psychiatry, -omics, Big data, Hype, Promise},
abstract = {Despite extensive research and prodigious advances in neuroscience, our comprehension of the nature of schizophrenia remains rudimentary. Our failure to make progress is attributed to the extreme heterogeneity of this condition, enormous complexity of the human brain, limitations of extant research paradigms, and inadequacy of traditional statistical methods to integrate or interpret increasingly large amounts of multidimensional information relevant to unravelling brain function. Fortunately, the rapidly developing science of machine learning appears to provide tools capable of addressing each of these impediments. Enthusiasm about the potential of machine learning methods to break the current impasse is reflected in the steep increase in the number of scientific publication about the application of machine learning to the study of schizophrenia. Machine learning approaches are, however, poorly understood by schizophrenia researchers and clinicians alike. In this paper, we provide a simple description of the nature and techniques of machine learning and their application to the study of schizophrenia. We then summarize its potential and constraints with illustrations from six studies of machine learning in schizophrenia and address some common misconceptions about machine learning. We suggest some guidelines for researchers, readers, science editors and reviewers of the burgeoning machine learning literature in schizophrenia. In order to realize its enormous promise, we suggest the need for the disciplined application of machine learning methods to the study of schizophrenia with a clear recognition of its capability and challenges accompanied by a concurrent effort to improve machine learning literacy among neuroscientists and mental health professionals.}
}
@article{NITOSLAWSKI2019101770,
title = {Smarter ecosystems for smarter cities? A review of trends, technologies, and turning points for smart urban forestry},
journal = {Sustainable Cities and Society},
volume = {51},
pages = {101770},
year = {2019},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101770},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719307644},
author = {Sophie A. Nitoslawski and Nadine J. Galle and Cecil Konijnendijk {Van Den Bosch} and James W.N. Steenberg},
keywords = {Sustainability, Trees, Urban forests, Urban technology, Green infrastructure, Green space, Ecological engineering, Smart monitoring, Smart cities},
abstract = {Smart cities are increasingly part of urban sustainability discourses. There is a growing interest in understanding how citizen engagement, connected technology, and data analytics can support sustainable development. Evidence has also repeatedly shown that green infrastructure such as urban forests address diverse urban challenges and are critical components of urban sustainability and resilience. Nevertheless, it is unclear whether green space and urban forest management are gaining significant traction in smart city planning. It is thus timely to consider whether and to what extent urban forests and other green spaces can be effectively integrated into smart city planning, to maximize green benefits for all city dwellers. We address this gap by exploring current and emerging smart city trends and technologies, and highlight practical applications for urban forest and green space management. Current “smart urban forest” projects reveal a focus on novel monitoring techniques using sensors and Internet of Things (IoT) technologies, as well as open data and citizen engagement, particularly through the use of mobile devices, applications (“apps”), and open-source mapping platforms. We propose a definition and promising approach to “smart urban forest management”, emphasizing both the potential of digital infrastructure to enhance forest benefits and the facilitation of citizen stewardship and empowerment in green space planning. Cities are getting faster and smarter – can (and should) the trees, and their managers, do the same?}
}
@article{CHANDRA20191,
title = {Crowdsourcing-based traffic simulation for smart freight mobility},
journal = {Simulation Modelling Practice and Theory},
volume = {95},
pages = {1-15},
year = {2019},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19300383},
author = {Shailesh Chandra and R. Thirumaleswara Naik and Jose Jimenez},
keywords = {Crowdsourcing, Smart freight, Routing, Mobility, Exit ramp, Congestion},
abstract = {Crowdsourcing is emerging as a powerful tool in transportation applications as it aims to provides possible solutions to problems by soliciting inputs from the crowd - usually people, objects or entities on an individual level. The primary sources of ‘big data’ in transportation (which includes social media, mobile applications and connected vehicles) facilitate this evolution through crowdsourcing; however, limited applications of crowdsourcing have been found in literature for the freight operations. In this paper, a simulation framework is developed combined with probabilistic modeling to make freight truck a ‘smart freight’ truck - with improved mobility by being able to detour to avoid a downstream congestion on the path. Smart freight trucks have access to crowdsourced data on an imminent congestion on its route and can detour leveraging this privilege. A discrete-time Markov chain (DTMC)-based simulation model is developed which describes the detour process through the nearest exit ramp of a freeway. The detour occurs with the help of crowdsourced information on a downstream congestion location. Simulation analyses carried out with both average daily passenger cars and truck traffic volumes using interstate 405 (I-405) and I-605 in the Southern California Region substantiate the effectiveness of the model. Simulation analysis shows that the efficiency improvements in detouring freight trucks under free flow traffic conditions and congested scenarios are 52% and 31%, respectively. Since connected vehicle technology (CVT) relies heavily on crowdsourced information from other vehicles in the fleet, simulation findings of this research can have large-scale implications in determining the success of CVT applications in freight operations. This crowdsourced data-based simulation framework can be used within traffic simulation software packages to enhance both passenger and truck freight operations.}
}