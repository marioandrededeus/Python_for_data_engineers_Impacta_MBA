@article{KICKBUSCH20211727,
title = {The Lancet and Financial Times Commission on governing health futures 2030: growing up in a digital world},
journal = {The Lancet},
volume = {398},
number = {10312},
pages = {1727-1776},
year = {2021},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(21)01824-9},
url = {https://www.sciencedirect.com/science/article/pii/S0140673621018249},
author = {Ilona Kickbusch and Dario Piselli and Anurag Agrawal and Ran Balicer and Olivia Banner and Michael Adelhardt and Emanuele Capobianco and Christopher Fabian and Amandeep {Singh Gill} and Deborah Lupton and Rohinton P Medhora and Njide Ndili and Andrzej Ryś and Nanjira Sambuli and Dykki Settle and Soumya Swaminathan and Jeanette Vega Morales and Miranda Wolpert and Andrew W Wyckoff and Lan Xue and Aferdita Bytyqi and Christian Franz and Whitney Gray and Louise Holly and Micaela Neumann and Lipsa Panda and Robert D Smith and Enow Awah {Georges Stevens} and Brian Li Han Wong}
}
@article{WANG2021101371,
title = {Crowdsourcing the perceived urban built environment via social media: The case of underutilized land},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101371},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101371},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001245},
author = {Yan Wang and Shangde Gao and Nan Li and Siyu Yu},
keywords = {Built environment, Crowdsourcing, Social media, Urban analytics, Underutilized land},
abstract = {Crowdsourcing the public’s perceptions of the built environment in real time enables more responsive and agile infrastructure and land use planning. Social media has emerged to be an effective platform for citizens, engineers, and planners to communicate opinions and feelings transparently. However, a comprehensive terminological resource of the perceived built environment (BE) for consistent data collection and a specified analytical framework are still lacking, particularly for different underutilized land issues. To fill this knowledge gap, we demonstrate a BE-specific term construction and expansion method specifically for collecting Twitter data and propose a Geo-Topic-Sentiment analytical framework for retrieving and analyzing relevant tweets. We conduct a demonstrative study on un(der)utilized land-related BE terms across ten metropolitan statistical areas in the U.S. Findings reveal spatial variations in contents and sentiments about underutilized land environments, and more localized efforts may be required to address specific land use issues across different urban contexts. The research demonstrates Twitter as a useful platform in crowdsourcing perceived BE and sentiments at fine temporal and spatial scales in a timely manner. It contributes to engineering informatics by investigating the role of social media in environmental planning and proposing integrated domain-specific data analytic approaches for engineering practices.}
}
@incollection{USHA2021281,
title = {Chapter 17 - Deciphering the animal genomics using bioinformatics approaches},
editor = {Sukanta Mondal and Ram Lakhan Singh},
booktitle = {Advances in Animal Genomics},
publisher = {Academic Press},
pages = {281-297},
year = {2021},
isbn = {978-0-12-820595-2},
doi = {https://doi.org/10.1016/B978-0-12-820595-2.00017-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205952000175},
author = {Talambedu Usha and Prachurjya Panda and Arvind Kumar Goyal and Shivani Sukhralia and Sarah Afreen and H.P. {Prashanth Kumar} and Dhivya Shanmugarajan and Sushil Kumar Middha},
keywords = {Animal genomes, Food, India, Sequencing techniques},
abstract = {Animal genomics is gaining popularity among researchers due to its utility-driven approaches. The major advantage lies in the understanding of how genes function and get expressed within various animal populations. Genomic understanding can propionate the thriving yield in farm animals to bioengineer innovative materials, enhanced productivity of livestock, xenotransplantation, and several other animal-based populous items for consumptions and even nonconsumption-based products like fabrics, silk. In this chapter, we present an introductory commentary on techniques and databases available to deduce animal genomes, a rapidly developing genome project resource, completed genomes summary of various domestic animals such as buffalo, sheep, and goat and the latest progress in the field. We have also flagged a concern with regard to resources and updates concerning farm livestock genome projects, especially in India, as compared to growing population and food demands across the globe within subsequent decades.}
}
@article{KLAPWIJK2021100902,
title = {Opportunities for increased reproducibility and replicability of developmental neuroimaging},
journal = {Developmental Cognitive Neuroscience},
volume = {47},
pages = {100902},
year = {2021},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2020.100902},
url = {https://www.sciencedirect.com/science/article/pii/S1878929320301511},
author = {Eduard T. Klapwijk and Wouter {van den Bos} and Christian K. Tamnes and Nora M. Raschle and Kathryn L. Mills},
keywords = {Development, Open science, Sample size, Cognitive neuroscience, Transparency, Preregistration},
abstract = {Many workflows and tools that aim to increase the reproducibility and replicability of research findings have been suggested. In this review, we discuss the opportunities that these efforts offer for the field of developmental cognitive neuroscience, in particular developmental neuroimaging. We focus on issues broadly related to statistical power and to flexibility and transparency in data analyses. Critical considerations relating to statistical power include challenges in recruitment and testing of young populations, how to increase the value of studies with small samples, and the opportunities and challenges related to working with large-scale datasets. Developmental studies involve challenges such as choices about age groupings, lifespan modelling, analyses of longitudinal changes, and data that can be processed and analyzed in a multitude of ways. Flexibility in data acquisition, analyses and description may thereby greatly impact results. We discuss methods for improving transparency in developmental neuroimaging, and how preregistration can improve methodological rigor. While outlining challenges and issues that may arise before, during, and after data collection, solutions and resources are highlighted aiding to overcome some of these. Since the number of useful tools and techniques is ever-growing, we highlight the fact that many practices can be implemented stepwise.}
}
@article{SHEN2021258,
title = {Discovery of marageing steels: machine learning vs. physical metallurgical modelling},
journal = {Journal of Materials Science & Technology},
volume = {87},
pages = {258-268},
year = {2021},
issn = {1005-0302},
doi = {https://doi.org/10.1016/j.jmst.2021.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S1005030221002504},
author = {Chunguang Shen and Chenchong Wang and Pedro E.J. Rivera-Díaz-del-Castillo and Dake Xu and Qian Zhang and Chi Zhang and Wei Xu},
keywords = {Machine learning, Physical metallurgy, Small sample problem, Marageing steel},
abstract = {Physical metallurgical (PM) and data-driven approaches can be independently applied to alloy design. Steel technology is a field of physical metallurgy around which some of the most comprehensive understanding has been developed, with vast models on the relationship between composition, processing, microstructure and properties. They have been applied to the design of new steel alloys in the pursuit of grades of improved properties. With the advent of rapid computing and low-cost data storage, a wealth of data has become available to a suite of modelling techniques referred to as machine learning (ML). ML is being emergingly applied in materials discovery while it requires data mining with its adoption being limited by insufficient high-quality datasets, often leading to unrealistic materials design predictions outside the boundaries of the intended properties. It is therefore required to appraise the strength and weaknesses of PM and ML approach, to assess the real design power of each towards designing novel steel grades. This work incorporates models and datasets from well-established literature on marageing steels. Combining genetic algorithm (GA) with PM models to optimise the parameters adopted for each dataset to maximise the prediction accuracy of PM models, and the results were compared with ML models. The results indicate that PM approaches provide a clearer picture of the overall composition-microstructure-properties relationship but are highly sensitive to the alloy system and hence lack on exploration ability of new domains. ML conversely provides little explicit physical insight whilst yielding a stronger prediction accuracy for large-scale data. Hybrid PM/ML approaches provide solutions maximising accuracy, while leading to a clearer physical picture and the desired properties.}
}
@incollection{WANG2021125,
title = {Chapter 4 - Battery state estimation methods},
editor = {Shunli Wang and Yongcun Fan and Daniel-Ioan Stroe and Carlos Fernandez and Chunmei Yu and Wen Cao and Zonghai Chen},
booktitle = {Battery System Modeling},
publisher = {Elsevier},
pages = {125-156},
year = {2021},
isbn = {978-0-323-90472-8},
doi = {https://doi.org/10.1016/B978-0-323-90472-8.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323904728000019},
author = {Shunli Wang and Yongcun Fan and Daniel-Ioan Stroe and Carlos Fernandez and Chunmei Yu and Wen Cao and Zonghai Chen},
keywords = {State of charge, State of energy, State of power, State of health, Remaining useful life, Influencing factors, Extended Kalman filtering, Support vector machine, Neural network, Particle filtering},
abstract = {The battery state estimation is a very important task in its management system. The state of charge represents the battery’s remaining energy ratio after a period of use or a long period of disuse, which can reflect the battery life or the battery remaining use time. As for the battery operation, the state parameter reflects its working conditions. The estimation methods are described for the battery state estimation of different working conditions. Before the battery state estimation, the definition of its state parameters is conducted, including state of charge, state of energy, state of power, state of health, and remaining useful life. After that, the main state influencing factors are analyzed as well as algorithm fusion and comparison. The parameter measurement technology is then introduced into the balancing control theory analysis and temperature adjustment. For the estimation method analysis, the foundational methods are analyzed in advance, including open-circuit voltage and ampere hour integral. The smart algorithms are introduced such as extended Kalman filtering, support vector machine, neural network, and particle filtering.}
}
@article{CHEN2021101713,
title = {Evaluation of occupational stress management for improving performance and productivity at workplaces by monitoring the health, well-being of workers},
journal = {Aggression and Violent Behavior},
pages = {101713},
year = {2021},
issn = {1359-1789},
doi = {https://doi.org/10.1016/j.avb.2021.101713},
url = {https://www.sciencedirect.com/science/article/pii/S1359178921001671},
author = {Ming Chen and Bin Ran and Xiaoying Gao and Guilan Yu and Jing Wang and J. Jagannathan},
keywords = {Occupational stress, Improving performance, Productivity, Stress, Health, Information technology},
abstract = {Competence lack, inadequate social support at work leads to the inability of workers since they are suffering from occupational stress. This will cause distress, burnout or psychosomatic difficulties, decreases in quality of life and service provision. Some of them may connect to work in an individual's personal life, both as managers, recognize stressors in their department, and respond on a departmental basis or individually. Many workers say that their employee utilization monitoring is not sufficient until computer counting involves. In addition, the systems are associated with higher stress, health hazards, and work unhappiness among supervised personnel. Monitoring these problems can increase employee awareness of personal productivity, providing performance information more promptly and frequently. Interventions are based on an examination of the variables that impact the performance of health workers. The article for employee stress management and health monitoring using information technology (SMHM-IT) gives better working conditions, motivation, retention, etc. Evaluation of occupational risks is a framework introduced to manage health and safety implications associated with preventative measures for improving and protecting the highest physical, social, or emotional working skills. Statistical data analysis is introduced to compare a medical specialty which includes analysis of employee's details. Results are compared with assessments shows that architecture offers successful in-time accessibility of performance 98.12% is achieved.}
}
@article{CABITZA2021104510,
title = {The need to separate the wheat from the chaff in medical informatics: Introducing a comprehensive checklist for the (self)-assessment of medical AI studies},
journal = {International Journal of Medical Informatics},
volume = {153},
pages = {104510},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104510},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621001362},
author = {Federico Cabitza and Andrea Campagner},
keywords = {Medical artificial intelligence, Machine learning, Checklist, Quality auditing},
abstract = {This editorial aims to contribute to the current debate about the quality of studies that apply machine learning (ML) methodologies to medical data to extract value from them and provide clinicians with viable and useful tools supporting everyday care practices. We propose a practical checklist to help authors to self assess the quality of their contribution and to help reviewers to recognize and appreciate high-quality medical ML studies by distinguishing them from the mere application of ML techniques to medical data.}
}
@article{BELLAPU2021,
title = {Developing and evaluation of machine learning models in the insurance sector},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.12.866},
url = {https://www.sciencedirect.com/science/article/pii/S221478532040584X},
author = {Rajendra Prasad Bellapu and S. Mylsamy and S. {Rama Krishna} and Pankaj Kundu},
keywords = {Insurance, ML, GLM, Random forest, Decision tree},
abstract = {Since the insurance sector is highly data oriented, machine learning (ML) is not surprisingly well incorporated into the industry. Though GLMs are still the comfort area of most of the actuaries, machine learning algorithms have increased in recent years. This paper concentrates on the creation and evaluation of three models of machine learning based on trees, starting from simple decision-making trees and developing random and gradient boosting machines with the more sophisticated ensemble methods. In a case study based on data given by an insurance provider, we forecast the claim frequency for an all-risk insurance fee. The gradient boost and random forests surpass the individual decision trees and in addition, we use visualization software to detect and learn from models.}
}
@incollection{CARLETTO20214407,
title = {Chapter 81 - Agricultural data collection to minimize measurement error and maximize coverage},
editor = {Christopher B. Barrett and David R. Just},
series = {Handbook of Agricultural Economics},
publisher = {Elsevier},
volume = {5},
pages = {4407-4480},
year = {2021},
booktitle = {Handbook of Agricultural Economics},
issn = {1574-0072},
doi = {https://doi.org/10.1016/bs.hesagr.2021.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1574007221000086},
author = {Calogero Carletto and Andrew Dillon and Alberto Zezza},
keywords = {Agriculture, Measurement error, Sampling error, Survey design, Data collection},
abstract = {Advances in agricultural data production provide ever-increasing opportunities for pushing the research frontier in agricultural economics and designing better agricultural policy. As new technologies present opportunities to create new and integrated data sources, researchers face tradeoffs in survey design that may reduce measurement error or increase coverage. In this chapter, we first review the econometric and survey methodology literatures that focus on the sources of measurement error and coverage bias in agricultural data collection. Second, we provide examples of how agricultural data structure affects testable empirical models. Finally, we review the challenges and opportunities offered by technological innovation to meet old and new data demands and address key empirical questions, focusing on the scalable data innovations of greatest potential impact for empirical methods and research.}
}
@article{GASHI2021103505,
title = {Dealing with missing usage data in defect prediction: A case study of a welding supplier},
journal = {Computers in Industry},
volume = {132},
pages = {103505},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103505},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001123},
author = {Milot Gashi and Patrick Ofner and Helmut Ennsbrunner and Stefan Thalmann},
keywords = {Defect prediction, End-of-line testing, Welding industry, Predictive maintenance, Multi-component systems},
abstract = {End-of-line (EoL) testing is performed to determine product quality by ensuring reliable performance. Even though low-quality products may pass EoL testing, they have a high probability of failure over time. Analyzing product usage data can help to improve EoL testing in this regard. However, current approaches do not consider usage data for this purpose. The major challenge for manufacturers is that they do not have access to comprehensive usage data for their products because customers are unwilling to provide usage data. However, manufacturers obtain some usage data from their sales and service departments i.e., contextual data. In this paper, we introduce an alternative approach to improving EoL testing when usage data from customers are missing. We discuss whether it is possible to predict low-quality products from EoL testing data when only contextual information is available (i.e., historical service data and location data of shipped products). We find that a simple, duration-based product usage threshold is sufficient to separate products affected by the production process (low-quality products) from those affected primarily by usage and environmental factors (long-term influence). Low-quality products could only be predicted by combining EoL data and contextual data. Additionally, we identify frequent patterns of maintained components to tackle the challenge of having limited data and promote user acceptance of our predictive model. Finally, we demonstrate our approach by conducting a case study in the welding industry. Our approach can identify frequent component failures and improve product reliability in countries with varying environmental conditions and rates of product usage. We expect that our findings will improve EoL testing protocols in welding and other industries while improving defect prediction models in general.}
}
@article{DOBBELAERE20211201,
title = {Machine Learning in Chemical Engineering: Strengths, Weaknesses, Opportunities, and Threats},
journal = {Engineering},
volume = {7},
number = {9},
pages = {1201-1211},
year = {2021},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S2095809921002010},
author = {Maarten R. Dobbelaere and Pieter P. Plehiers and Ruben {Van de Vijver} and Christian V. Stevens and Kevin M. {Van Geem}},
keywords = {Artificial intelligence, Machine learning, Reaction engineering, Process engineering},
abstract = {Chemical engineers rely on models for design, research, and daily decision-making, often with potentially large financial and safety implications. Previous efforts a few decades ago to combine artificial intelligence and chemical engineering for modeling were unable to fulfill the expectations. In the last five years, the increasing availability of data and computational resources has led to a resurgence in machine learning-based research. Many recent efforts have facilitated the roll-out of machine learning techniques in the research field by developing large databases, benchmarks, and representations for chemical applications and new machine learning frameworks. Machine learning has significant advantages over traditional modeling techniques, including flexibility, accuracy, and execution speed. These strengths also come with weaknesses, such as the lack of interpretability of these black-box models. The greatest opportunities involve using machine learning in time-limited applications such as real-time optimization and planning that require high accuracy and that can build on models with a self-learning ability to recognize patterns, learn from data, and become more intelligent over time. The greatest threat in artificial intelligence research today is inappropriate use because most chemical engineers have had limited training in computer science and data analysis. Nevertheless, machine learning will definitely become a trustworthy element in the modeling toolbox of chemical engineers.}
}
@article{AN2021104776,
title = {Deep convolutional neural network for automatic fault recognition from 3D seismic datasets},
journal = {Computers & Geosciences},
volume = {153},
pages = {104776},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104776},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421000807},
author = {Yu An and Jiulin Guo and Qing Ye and Conrad Childs and John Walsh and Ruihai Dong},
keywords = {Fault recognition, Seismic interpretation, Deep learning, Computer vision, Image processing},
abstract = {With the explosive growth in seismic data acquisition and the successful application of deep convolutional neural networks (DCNN) to various image processing tasks within multidisciplinary fields, many researchers have begun to research DCNN based automatic seismic interpretation techniques. Due to the vast number of parameters considered in deep neural networks, deep learning methods usually require a large amount of data for training. However, collecting a large number of expert interpretations is very time consuming, so related research usually uses synthetic datasets and ignores the practical problems of field datasets. In this paper, we open-source a multi-gigabyte expert-labelled field dataset in response to the challenge of accessing large-scale expert-labelled field datasets. We show that 2D fault recognition within this dataset is an image segmentation or edge detection problem in the computer vision field, that can be expressed as a pixel-level fault/non-fault binary classification. Both types of DCNNs are compared, and we propose a novel fault recognition workflow, which involves processing and screening of seismic images and labels, training DCNNs and automatic numerical evaluation. We have also demonstrated for three case study datasets that effective image augmentation methods can reduce the required labelled crosslines while maintaining satisfactory performance. Our experimental results show that our workflow not only outperforms two state-of-the-art DCNN solutions but also achieves performance comparable to humans on an expert labelled image dataset, even predicting subtle faults that an expert interpreter did not annotate. We suggest that the proposed workflow could reduce the fault interpretation life cycle from months to hours and improve the quality, and define the confidence, of fault interpretation results.}
}
@article{DAVIDOVIC2021109533,
title = {Application of artificial intelligence for detection of chemico-biological interactions associated with oxidative stress and DNA damage},
journal = {Chemico-Biological Interactions},
volume = {345},
pages = {109533},
year = {2021},
issn = {0009-2797},
doi = {https://doi.org/10.1016/j.cbi.2021.109533},
url = {https://www.sciencedirect.com/science/article/pii/S0009279721001691},
author = {Lazar M. Davidovic and Darko Laketic and Jelena Cumic and Elena Jordanova and Igor Pantic},
keywords = {Reactive oxygen species, Radiation, Machine learning, Non-coding DNA, Aging},
abstract = {In recent years, various AI-based methods have been developed in order to uncover chemico-biological interactions associated with DNA damage and oxidative stress. Various decision trees, bayesian networks, random forests, logistic regression models, support vector machines as well as deep learning tools, have great potential in the area of molecular biology and toxicology, and it is estimated that in the future, they will greatly contribute to our understanding of molecular and cellular mechanisms associated with DNA damage and repair. In this concise review, we discuss recent attempts to build machine learning tools for assessment of radiation – induced DNA damage as well as algorithms that can analyze the data from the most frequently used DNA damage assays in molecular biology. We also review recent works on the detection of antioxidant proteins with machine learning, and the use of AI-related methods for prediction and evaluation of noncoding DNA sequences. Finally, we discuss previously published research on the potential application of machine learning tools in aging research.}
}
@incollection{2021xxxi,
title = {Author Biographies},
editor = {David Baker and Lucy Ellis},
booktitle = {Future Directions in Digital Information},
publisher = {Chandos Publishing},
pages = {xxxi-xxxix},
year = {2021},
series = {Chandos Digital Information Review},
isbn = {978-0-12-822144-0},
doi = {https://doi.org/10.1016/B978-0-12-822144-0.09989-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221440099894}
}
@article{ASHRAF2021114913,
title = {Strategic-level performance enhancement of a 660 MWe supercritical power plant and emissions reduction by AI approach},
journal = {Energy Conversion and Management},
volume = {250},
pages = {114913},
year = {2021},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.114913},
url = {https://www.sciencedirect.com/science/article/pii/S019689042101089X},
author = {Waqar Muhammad Ashraf and Ghulam Moeen Uddin and Syed Muhammad Arafat and Jaroslaw Krzywanski and Wang Xiaonan},
keywords = {Combustion power plant, Fuel management, GHG emission reduction, Artificial intelligence},
abstract = {Power plant heat rate is a plant level performance parameter that indicates the economy of power production, equipment’s safety, and availability. In this paper, seven operating parameters, including the performance indices of integrated energy devices and the environmental conditions are incorporated for modeling the power plant heat rate by Artificial Neural Network (ANN), Support Vector Machine (SVM), and automated machine learning (AutoML) approach. The parametric significance order is determined by ANN and SVM-based Monte Carlo analytics and other machine learning-driven algorithms. Subsequently, the best-performing model is selected based on the external validation test and deployed for knowledge mining purposes. The improvement in the power plant heat rate by the parametric adjustment is achieved and subsequently, up to 3.12 percentage point (pp) increase in the thermal efficiency of the power plant is confirmed. Moreover, the fuel savings corresponding to the improved power plant heat rate are also calculated at three power generation modes. Their equivalence to an annual reduction in emissions is quantified. It is estimated that the accumulated reduction in CO2, SO2, CH4, N2O, and Hg emissions, i.e., 288.2 kilo tons / year (kt/y), can be achieved under 3.15% improvement in the power plant heat rate, corresponding to 75% power generation mode.}
}
@article{SONG2021633,
title = {High-throughput phenotyping: Breaking through the bottleneck in future crop breeding},
journal = {The Crop Journal},
volume = {9},
number = {3},
pages = {633-645},
year = {2021},
note = {Rice as a model crop: genetics, genomics and breeding},
issn = {2214-5141},
doi = {https://doi.org/10.1016/j.cj.2021.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S2214514121000829},
author = {Peng Song and Jinglu Wang and Xinyu Guo and Wanneng Yang and Chunjiang Zhao},
keywords = {High-throughput phenotyping, Crop breeding, Crop phenomics, Phenotyping platform, Data analysis},
abstract = {With the rapid development of genetic analysis techniques and crop population size, phenotyping has become the bottleneck restricting crop breeding. Breaking through this bottleneck will require phenomics, defined as the accurate, high-throughput acquisition and analysis of multi-dimensional phenotypes during crop growth at organism-wide levels, ranging from cells to organs, individual plants, plots, and fields. Here we offer an overview of crop phenomics research from technological and platform viewpoints at various scales, including microscopic, ground-based, and aerial phenotyping and phenotypic data analysis. We describe recent applications of high-throughput phenotyping platforms for abiotic/biotic stress and yield assessment. Finally, we discuss current challenges and offer perspectives on future phenomics research.}
}
@article{GONZALEZ2021100858,
title = {Urban climate and resiliency: A synthesis report of state of the art and future research directions},
journal = {Urban Climate},
volume = {38},
pages = {100858},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100858},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521000882},
author = {Jorge E. González and Prathap Ramamurthy and Robert D. Bornstein and Fei Chen and Elie R. Bou-Zeid and Masoud Ghandehari and Jeffrey Luvall and Chandana Mitra and Dev Niyogi},
keywords = {Urban climate resiliency, Extreme urban weather, Climate adaptation, Modeling and observations of extreme urban weather, Knowledge transfer of urban climate data, Cyber-systems for urban climate and weather},
abstract = {The Urban Climate and Resiliency-Science Working Group (i.e., The WG) was convened in the summer of 2018 to explore the scientific grand challenges related to climate resiliency of cities. The WG leveraged the presentations at the 10th International Conference on Urban Climate (ICUC10) held in New York City (NYC) on 6–10 August 2018 as input forum. ICUC10 was a collaboration between the International Association of Urban Climate, American Meteorological Society, and World Meteorological Organization. It attracted more than 600 participants from more than 50 countries, resulting in close to 700 oral and poster presentations under the common theme of “Sustainable & Resilient Urban Environments”. ICUC10 covered topics related to urban climate and weather processes with far-reaching implications to weather forecasting, climate change adaptation, air quality, health, energy, urban planning, and governance. This article provides a synthesis of the analysis of the current state of the art and of the recommendations of the WG for future research along each of the four Grand Challenges in the context of urban climate and weather resiliency; Modeling, Observations, Cyber-Informatics, and Knowledge Transfer & Applications.}
}
@incollection{MLADENOVIC202112,
title = {Mobility as a Service},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {12-18},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10607-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717106074},
author = {Miloš N. Mladenović},
keywords = {Emerging technology, Mobility governance, Mobility on demand, Mobility system, Responsible innovation, Social justice, Transportation network company, Transport policy},
abstract = {Mobility as a Service (MaaS) is a relatively fast-growing emerging technology based on the vision of integration (incl., policy, operational, informational, and transactional levels) and customization in transport systems. The user is expected to receive information, book, and pay for a choice of different mobility services by accessing a “one-stop-shop” or “mobility platform” via digital interfaces. Although highly uncertain, MaaS has significant potential to exert a considerable impact on the socio-technical domains in and beyond mobility. Such implications are also in terms of the composition of actors, institutions, and patterns of interactions among those, along with the associated innovation processes. Technological transition from niche to regime for MaaS depends on associated rhetoric as well as a wider set of converging socio-technical factors of societal automation, digitalization, and reregulation. Ultimately, transport policy and governance institutions will have to reflect and act on the potential undesired consequences from the depolitization of MaaS technological development.}
}
@article{ZAMORA2021138,
title = {Massive data screening is a second opportunity to improve the management of patients with familial hypercholesterolemia phenotype},
journal = {Clínica e Investigación en Arteriosclerosis (English Edition)},
volume = {33},
number = {3},
pages = {138-147},
year = {2021},
issn = {2529-9123},
doi = {https://doi.org/10.1016/j.artere.2020.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S2529912321000231},
author = {Alberto Zamora and Guillem Paluzie and Joan García-Vilches and Oriol Alonso Gisbert and Ana Inés {Méndez Martínez} and Núria Plana and Cèlia Rodríguez-Borjabad and Daiana Ibarretxe and Anabel Martín-Urda and Luis Masana},
keywords = {Familial hypercholesterolemia, Massive data screening, Profiling of patients, Hipercolesterolemia familiar, Rastreo masivo de datos, Perfilado de pacientes},
abstract = {Introduction
Familial Hypercholesterolemia (FH) is an autosomal dominant disease with an estimated prevalence between 1/200–250. It is under-treated and underdiagnosed. Massive data screening can increase the detection of patients with FH.
Methods
Study population: Residents in the health coverage area (N: 195.000 inhabitants) and with at least one determination of cholesterol linked to low-density lipoproteins (LDLC) carried out between January 1, 2010 and December 30, 2019. The highest LDL-C values were selected. Exclusion criteria: nephrotic syndrome, hypothyroidism, Hypothyroid treatment or triglycerides > 400 mg/dL. Seven algorithms suggestive of Familial Hypercholesterolemia Phenotype (HF-P) were analyzed, selecting the most efficient algorithm that could easily be translated into clinical practice.
Results
Based on 6.264.877 assistances and 288.475 patients, after applying the inclusionexclusion criteria, 504.316 tests were included, corresponding to 106.382 adults and 10.509 < 18 years. The selected algorithm presented a prevalence of 0.62%. 840 patients with HF-P were detected, 55.8% being women and 178 < 18 years old, 9.3% had a history of cardiovascular disease (CVD) and 16.4% had died. 65% of the patients in primary prevention had LDL-C values > 130 mg/dL and 83% in secondary prevention values > 70 mg/dL. A ratio of 7.64 (1–18) patients with HF-P per analytical requesting physician was obtained.
Conclusions
Massive data screening and patient profiling are effective tools and easily applicable in clinical practice for the detection of patients with FH.
Resumen
Introducción
La Hipercolesterolemia Familiar (HF) es una enfermedad autósómica dominante con una prevalencia estimada entre 1/200–250. Se encuentra infratratada e infradiagnosticada. El rastreo masivo de datos puede incrementar la detección de pacientes con HF.
Métodos
Población a estudio: Residentes en la zona sanitaria de cobertura (N: 195.000 habitantes) y con al menos una determinación de colesterol ligado a lipoproteínas de baja densidad (C-LDL) realizada entre el 1 de Enero de 2010 y el 30 de Diciembre de 2019. Se seleccionaron los valores más altos de C-LDL. Criterios de exclusión: síndrome nefrótico, hipotiroidismo, tratamiento hipotiroideo o triglicéridos > 400 mg/dL. Se analizaron 7 algoritmos sugestivos de fenotipo de Hipercolesterolemia Familiar (FHF). Se seleccionó el algoritmo más eficaz y de fácil traslación a la práctica clínica.
Resultados
Partiendo de 6.264.877 asistencias y 288.475 pacientes tras aplicar los criterios de inclusión-exclusión se incluyeron 504.316 analíticas correspondiendo a 106.382 adultos y 10.509 < 18 años.El algoritmo seleccionado presentó una prevalencia de 0.62%.Se detectaron 840 pacientes con fenotipo de Hipercolestereolemia Familiar (FHF) siendo el 55.8% mujeres y 178 < 18 años, El 9.3% tenían antecedentes de enfermedad cardio-vascular (ECV) y 16.4% habían fallecido.El 65% de los pacientes en prevención primaria presentaron valores de C-LDL > 130 mg/dL y el 83% en prevención secundaria valores > 70 mg/dL.Se obtuvo una ratio de 7.64 (1–18) pacientes con HF-P por médico solicitante de analítica.
Conclusiones
El rastreo masivo de datos y el perfilado de pacientes son herramientas eficaces y fácilmente aplicables en práctica clínica para la detección de pacientes con HF.}
}
@article{PAUDEL2021103016,
title = {Machine learning for large-scale crop yield forecasting},
journal = {Agricultural Systems},
volume = {187},
pages = {103016},
year = {2021},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2020.103016},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X20308775},
author = {Dilli Paudel and Hendrik Boogaard and Allard {de Wit} and Sander Janssen and Sjoukje Osinga and Christos Pylianidis and Ioannis N. Athanasiadis},
keywords = {Crop yield prediction, Machine learning, Modularity, Reusability, Large-scale crop yield forecasting},
abstract = {Many studies have applied machine learning to crop yield prediction with a focus on specific case studies. The data and methods they used may not be transferable to other crops and locations. On the other hand, operational large-scale systems, such as the European Commission's MARS Crop Yield Forecasting System (MCYFS), do not use machine learning. Machine learning is a promising method especially when large amounts of data are being collected and published. We combined agronomic principles of crop modeling with machine learning to build a machine learning baseline for large-scale crop yield forecasting. The baseline is a workflow emphasizing correctness, modularity and reusability. For correctness, we focused on designing explainable predictors or features (in relation to crop growth and development) and applying machine learning without information leakage. We created features using crop simulation outputs and weather, remote sensing and soil data from the MCYFS database. We emphasized a modular and reusable workflow to support different crops and countries with small configuration changes. The workflow can be used to run repeatable experiments (e.g. early season or end of season predictions) using standard input data to obtain reproducible results. The results serve as a starting point for further optimizations. In our case studies, we predicted yield at regional level for five crops (soft wheat, spring barley, sunflower, sugar beet, potatoes) and three countries (the Netherlands (NL), Germany (DE), France (FR)). We compared the performance with a simple method with no prediction skill, which either predicted a linear yield trend or the average of the training set. We also aggregated the predictions to the national level and compared with past MCYFS forecasts. The normalized RMSE (NRMSE) for early season predictions (30 days after planting) were comparable for NL (all crops), DE (all except soft wheat) and FR (soft wheat, spring barley, sunflower). For example, NRMSE was 7.87 for soft wheat (NL) (6.32 for MCYFS) and 8.21 for sugar beet (DE) (8.79 for MCYFS). In contrast, NRMSEs for soft wheat (DE), sugar beet (FR) and potatoes (FR) were twice as much compared to MCYFS. NRMSEs for end of season were still comparable to MCYFS for NL, but worse for DE and FR. The baseline can be improved by adding new data sources, designing more predictive features and evaluating different machine learning algorithms. The baseline will motivate the use of machine learning in large-scale crop yield forecasting.}
}
@article{DU2021113721,
title = {Effects of the joint prevention and control of atmospheric pollution policy on air pollutants-A quantitative analysis of Chinese policy texts},
journal = {Journal of Environmental Management},
volume = {300},
pages = {113721},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.113721},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721017837},
author = {Huibin Du and Yaqian Guo and Zhongguo Lin and Yueming Qiu and Xiao Xiao},
keywords = {Joint prevention and control of atmospheric pollution policy, Policy quantification, Emission reduction, Policy effectiveness},
abstract = {Joint prevention and control of atmospheric pollution (JPCAP) policies play a vital role in alleviating regional pollution. Based on Latent Dirichlet Allocation (LDA) model, we construct two policy strength measures of effectiveness and number, and investigate the effects of policy strength on air pollutant emissions for four types of JPCAP policies. The results show that the effects of economic incentive policy tools and supporting policy tools on emission reduction deviate significantly from policy preferences. Economic incentive policy tools are the most effective in promoting emission reductions in SO2, NOx and dust, but their effectiveness are the lowest in reality. Supporting policy tools, with the highest strength, have little effect on emission reduction. Command-control policies and persuasion policies are both relatively high in quantity and effectiveness. In addition, policy strength plays a more important role in reducing air pollutants in key regions than in non-key regions. JPCAP policies have gradually changed from a single policy tool to multiple policy tools, and the government shifted its attention to improving the legal effectiveness of policies after 2015. Finally, we propose some policy implications to optimize JPCAP policies and address regional air pollution problem.}
}
@article{WEI2021191,
title = {Construction of super-resolution model of remote sensing image based on deep convolutional neural network},
journal = {Computer Communications},
volume = {178},
pages = {191-200},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002462},
author = {Zikang Wei and Yunqing Liu},
keywords = {Remote sensing image, Super resolution, Convolution algorithm, Cloud computing},
abstract = {With the continuous improvement of satellite remote sensing technology, using super-resolution image reconstruction technology to reconstruct remote sensing images has important application significance for social development. In the generator model proposed in this paper, the standard convolution layer in the residual network structure is replaced by empty convolution to improve the overall performance of the model while keeping the number of parameters unchanged and the receptive field of convolution at each stage unchanged. By analyzing the advantages of residual network, dense connection network, and cavity convolution in the field of image super resolution, an optimized super-resolution reconstruction model of GAN image with cavity convolution is constructed with dense connection block of cavity residue as a generator component. The cloud computing-based service model is introduced into the image reconstruction system, and the background management module is built through the cloud service system, which is responsible for model training, image transmission, image processing request and database reading. Through experimental analysis, it is proved that the whole automatic data processing from automatic matching data to processing data can be completed, and the performance is better than the traditional service mode, which can produce great economic benefits.}
}
@article{SAKKA2021101875,
title = {A profile-aware methodological framework for collaborative multidimensional modeling},
journal = {Data & Knowledge Engineering},
volume = {131-132},
pages = {101875},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101875},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000021},
author = {Amir Sakka and Sandro Bimonte and Stefano Rizzi and Lucile Sautot and François Pinet and Michela Bertolotto and Aurélien Besnard and Noura Rouillier},
keywords = {Data warehouse design, Collaborative systems, Quality dimensions},
abstract = {Multidimensional modeling, i.e., the design of cube schemata, has a key role in data warehouse (DW) projects, in self-service business intelligence, and in general to let users analyze data via the OLAP paradigm. Though an effective involvement of users in multidimensional modeling is crucial in these projects, not much has been said about how to establish a fruitful collaboration in projects involving numerous users with different skills, reputations, and degrees of authority. This issue is especially relevant in citizen science projects, where several volunteers can contribute their requirements despite not being formally-trained experts in the application domain. To fill this gap, we propose a framework for collaborative multidimensional modeling that can adapt itself to the profiles and skills of the actors involved. We first classify users depending on their authoritativeness, skills, and engagement in the project. Then, following this classification, we identify four possible methodological scenarios and propose a profile-aware methodology supported by two sets of quality attributes. Finally, we describe a Group Decision Support System that implements our methodological framework and present some experiments carried out on a real case study.}
}
@article{ZHAO2021545,
title = {A polygenic methylation prediction model associated with response to chemotherapy in epithelial ovarian cancer},
journal = {Molecular Therapy - Oncolytics},
volume = {20},
pages = {545-555},
year = {2021},
issn = {2372-7705},
doi = {https://doi.org/10.1016/j.omto.2021.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S2372770521000309},
author = {Lanbo Zhao and Sijia Ma and Linconghua Wang and Yiran Wang and Xue Feng and Dongxin Liang and Lu Han and Min Li and Qiling Li},
keywords = {ovarian cancer, bioinformatics, DNA methylation, chemotherapy response, prediction model, AGR2, HSPA2, ACAT2},
abstract = {To identify potential aberrantly differentially methylated genes (DMGs) correlated with chemotherapy response (CR) and establish a polygenic methylation prediction model of CR in epithelial ovarian cancer (EOC), we accessed 177 (47 chemo-sensitive and 130 chemo-resistant) samples corresponding to three DNA-methylation microarray datasets from the Gene Expression Omnibus and 306 (290 chemo-sensitive and 16 chemo-resistant) samples from The Cancer Genome Atlas (TCGA) database. DMGs associated with chemotherapy sensitivity and chemotherapy resistance were identified by several packages of R software. Pathway enrichment and protein-protein interaction (PPI) network analyses were constructed by Metascape software. The key genes containing mRNA expressions associated with methylation levels were validated from the expression dataset by the GEO2R platform. The determination of the prognostic significance of key genes was performed by the Kaplan-Meier plotter database. The key genes-based polygenic methylation prediction model was established by binary logistic regression. Among accessed 483 samples, 457 (182 hypermethylated and 275 hypomethylated) DMGs correlated with chemo resistance. Twenty-nine hub genes were identified and further validated. Three genes, anterior gradient 2 (AGR2), heat shock-related 70-kDa protein 2 (HSPA2), and acetyltransferase 2 (ACAT2), showed a significantly negative correlation between their methylation levels and mRNA expressions, which also corresponded to prognostic significance. A polygenic methylation prediction model (0.5253 cutoff value) was established and validated with 0.659 sensitivity and 0.911 specificity.}
}
@article{ARUNTHAVANATHAN2021107197,
title = {An analysis of process fault diagnosis methods from safety perspectives},
journal = {Computers & Chemical Engineering},
volume = {145},
pages = {107197},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107197},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420312400},
author = {Rajeevan Arunthavanathan and Faisal Khan and Salim Ahmed and Syed Imtiaz},
keywords = {Process safety, Process risk management, Process failure analysis, Process fault diagnosis, Process automation},
abstract = {Industry 4.0 provides substantial opportunities to ensure a safer environment through online monitoring, early detection of faults, and preventing the faults to failures transitions. Decision making is an important step in abnormal situation management. Assigning risk based on the consequences may provide additional information for abnormal situation management decisions to prevent the accident before it occurs. This paper analyzes the interconnections between the three essential aspects of process safety: fault detection and diagnosis (FDD), risk assessment (RA), and abnormal situation management (ASM) in the context of the current and next generation of process systems. The authors present their thoughts on research directions in process safety in Industry 4.0. This article aims to serve as a road map for the next generation of process safety research to enable safer and sustainable process operations and development.}
}
@article{WANG2021331,
title = {A data driven approach to assessing the reliability of using taxicab as probes for Real-Time route selections},
journal = {Journal of Intelligent Transportation Systems},
volume = {25},
number = {4},
pages = {331-342},
year = {2021},
issn = {1547-2450},
doi = {https://doi.org/10.1080/15472450.2019.1617142},
url = {https://www.sciencedirect.com/science/article/pii/S1547245022002961},
author = {Zheng Wang and Wei-Hua Lin and Wangtu Xu},
keywords = {Data driven approach, route choice, taxi service, travel time estimation},
abstract = {Taxi service is one of the most important modes for urban transportation. In recent years, many taxi companies have been routinely collecting data to track the movement of each taxi for improving security, coordination, and service performance. This paper is intended to use the GPS vehicle positioning data to assess the route choice behavior of taxi drivers and explore if the routes selected by taxi drivers can be incorporated into a traveler information system. It is often perceived that taxi drivers have the ability to select quality routes assuming that: (1) they tend to be more knowledgeable about alternative routes and time-dependent traffic conditions than general public, including some publicly available route guidance systems due to the nature of their profession; and (2) they are typically more motivated to incorporate their knowledge about traffic conditions into their route choice decisions. An experimental study is conducted to examine the validity of these two assumptions. We have developed a framework that can effectively process the data into information about routes selected by taxi drivers and their associated travel times. The performance of the routes selected by taxi drivers is compared with the performance of those recommended by e-maps. Our results indicate that the routes selected by taxi drivers are generally more efficient than the routes recommended by some major e-maps, suggesting that taxi drivers are more active in selecting routes to avoid congestion.}
}
@article{LI2021102974,
title = {Inferring the trip purposes and uncovering spatio-temporal activity patterns from dockless shared bike dataset in Shenzhen, China},
journal = {Journal of Transport Geography},
volume = {91},
pages = {102974},
year = {2021},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2021.102974},
url = {https://www.sciencedirect.com/science/article/pii/S0966692321000272},
author = {Shaoying Li and Caigang Zhuang and Zhangzhi Tan and Feng Gao and Zhipeng Lai and Zhifeng Wu},
keywords = {Activity inference, Gravity model, Bayesian rules, Travel patterns, Dockless shared bikes},
abstract = {Trip purpose is closely related to travel patterns and plays an important role in urban planning and transportation management. Recently, there has been a growing interest in investigating the spatio-temporal patterns of dockless shared-bike usage and its influencing mechanisms. Few, however, have focused on revealing the travel patterns by inferring the purpose of dockless shared-bike trips at the individual level. We present a framework for inferring the purpose of dockless shared-bike users, based on gravity model and Bayesian rules, and conduct it in Shenzhen, China. We consider the comprehensive factors including distance, time, environment, activity type proportion, and service capacity of points of interest (POIs), the last two factors of which were usually neglected in previous transport studies. Especially, we integrated areas of interest (AOIs) and Tencent User density (TUD) social media data characterize the service capacity of POIs, which reflect the area and scale differences of different POI categories. Through the comparison between two improved models and the basic model, it is demonstrated that the introduction of activity type proportion and service capacity of POIs can improve the effectiveness of model for inferring the purposes of dockless shared-bike trips. Based on the obtained trip purposes, we further explore the spatio-temporal patterns of different activities and gain some insights into bike travel demand, which can inform scientific decisions for bicycle infrastructure planning and dockless shared- bike management.}
}
@article{YANG2021100175,
title = {COSMOS next generation – A public knowledge base leveraging chemical and biological data to support the regulatory assessment of chemicals},
journal = {Computational Toxicology},
volume = {19},
pages = {100175},
year = {2021},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100175},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000232},
author = {C. Yang and M.T.D. Cronin and K.B. Arvidson and B. Bienfait and S.J. Enoch and B. Heldreth and B. Hobocienski and K. Muldoon-Jacobs and Y. Lan and J.C. Madden and T. Magdziarz and J. Marusczyk and A. Mostrag and M. Nelms and D. Neagu and K. Przybylak and J.F. Rathman and J. Park and A-N Richarz and A.M. Richard and J.V. Ribeiro and O. Sacher and C. Schwab and V. Vitcheva and P. Volarath and A.P. Worth},
keywords = {Toxicity, Database, Public database, Knowledge hub, Study reliability, Analogue selection, Guided workflow},
abstract = {The COSMOS Database (DB) was originally established to provide reliable data for cosmetics-related chemicals within the COSMOS Project funded as part of the SEURAT-1 Research Initiative. The database has subsequently been maintained and developed further into COSMOS Next Generation (NG), a combination of database and in silico tools, essential components of a knowledge base. COSMOS DB provided a cosmetics inventory as well as other regulatory inventories, accompanied by assessment results and in vitro and in vivo toxicity data. In addition to data content curation, much effort was dedicated to data governance – data authorisation, characterisation of quality, documentation of meta information, and control of data use. Through this effort, COSMOS DB was able to merge and fuse data of various types from different sources. Building on the previous effort, the COSMOS Minimum Inclusion (MINIS) criteria for a toxicity database were further expanded to quantify the reliability of studies. COSMOS NG features multiple fingerprints for analysing structure similarity, and new tools to calculate molecular properties and screen chemicals with endpoint-related public profilers, such as DNA and protein binders, liver alerts and genotoxic alerts. The publicly available COSMOS NG enables users to compile information and execute analyses such as category formation and read-across. This paper provides a step-by-step guided workflow for a simple read-across case, starting from a target structure and culminating in an estimation of a NOAEL confidence interval. Given its strong technical foundation, inclusion of quality-reviewed data, and provision of tools designed to facilitate communication between users, COSMOS NG is a first step towards building a toxicological knowledge hub leveraging many public data systems for chemical safety evaluation. We continue to monitor the feedback from the user community at support@mn-am.com.}
}
@article{GREGORIADES2021115546,
title = {Supporting digital content marketing and messaging through topic modelling and decision trees},
journal = {Expert Systems with Applications},
volume = {184},
pages = {115546},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115546},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421009532},
author = {Andreas Gregoriades and Maria Pampaka and Herodotos Herodotou and Evripides Christodoulou},
keywords = {Topic modelling, Cultural and economic distance, Decision trees, Shapley additive explanation, Tourists’ reviews},
abstract = {This paper presents a machine learning approach involving tourists’ electronic word of mouth (eWOM) to support destination marketing campaigns. This approach enhances optimisation of a critical aspect of marketing campaigns, that is, the communication of the right content to the right consumers. The proposed method further considers aggregate cultural and economic-related information of the tourists’ country of origin with topic modelling and Decision Tree (DT) models. Each DT addresses different dimensions of culture and purchasing power and the way these dimensions are associated with the topics discussed in eWOM, thus revealing patterns relating tourists’ experiences with potential explanations for their dissatisfaction/satisfaction. The method is implemented in a case study in the context of tourism in Cyprus focusing on two hotel groups (2/3 and 4/5 stars) to account for their differences. Patterns emerged from the extraction of rules from DTs illuminate combinations of variables associated with tourist experience (negative or positive) for each of the two hotel categories and verify the asymmetric relationship between service performance and satisfaction. The approach can be used by management during marketing campaigns to design messages to better address the desires and needs of tourists from different cultural and economic backgrounds, as these emerge from the data analysis.}
}
@article{ELBADAWI2021745,
title = {Disrupting 3D printing of medicines with machine learning},
journal = {Trends in Pharmacological Sciences},
volume = {42},
number = {9},
pages = {745-757},
year = {2021},
issn = {0165-6147},
doi = {https://doi.org/10.1016/j.tips.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S016561472100119X},
author = {Moe Elbadawi and Laura E. McCoubrey and Francesca K.H. Gavins and Jun J. Ong and Alvaro Goyanes and Simon Gaisford and Abdul W. Basit},
keywords = {additive manufacturing, 3D Printed drug products and formulations, Industry 4.0 and digital health, personalized oral drug delivery systems and medical devices, biomedical engineering and pharmaceutical sciences, translational pharmaceutics},
abstract = {3D printing (3DP) is a progressive technology capable of transforming pharmaceutical development. However, despite its promising advantages, its transition into clinical settings remains slow. To make the vital leap to mainstream clinical practice and improve patient care, 3DP must harness modern technologies. Machine learning (ML), an influential branch of artificial intelligence, may be a key partner for 3DP. Together, 3DP and ML can utilise intelligence based on human learning to accelerate drug product development, ensure stringent quality control (QC), and inspire innovative dosage-form design. With ML’s capabilities, streamlined 3DP drug delivery could mark the next era of personalised medicine. This review details how ML can be applied to elevate the 3DP of pharmaceuticals and importantly, how it can expedite 3DP’s integration into mainstream healthcare.}
}
@article{WANG2021112622,
title = {Multi-factor and multi-level predictive models of building natural period},
journal = {Engineering Structures},
volume = {242},
pages = {112622},
year = {2021},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2021.112622},
url = {https://www.sciencedirect.com/science/article/pii/S0141029621007720},
author = {Zetao Wang and Jun Chen and Jiaxu Shen},
keywords = {Natural period of building, Database, Maximal information coefficient, Kruskal–Wallis ANOVA, Empirical formula, Confidence interval, Rational scope},
abstract = {A comprehensive database containing data on approximately 2700 buildings and 6000 full-scale measured period samples was constructed through massive literature searching and stringent data filtering. The newly emerged maximal information coefficient method, which is suitable for large data set statistical analysis, was adopted in conjunction with Kruskal–Wallis analysis of variance to identify factors that significantly affect a building’s fundamental period. It was quantitatively verified that height, predominant structural material, and lateral-force resisting system are the three most important influencing factors. Subsequently, height was used as the dominant regression variable, and material and lateral-force resisting system were used as categorical variables, predictive models in combination with confidence intervals of the fundamental period are provided for multi-factors, including four material types and three structural types. In addition, multi-level empirical formulas of the natural period in other five modes (two translational and three torsional) are provided on the basis of the regression results of the fundamental period. All these predictive models can effectively reflect the tendency of the median and the rational scope of variability of the natural period of buildings.}
}
@article{SPANAKI2021102350,
title = {AI applications of data sharing in agriculture 4.0: A framework for role-based data access control},
journal = {International Journal of Information Management},
volume = {59},
pages = {102350},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102350},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000438},
author = {Konstantina Spanaki and Erisa Karafili and Stella Despoudi},
keywords = {Agriculture 4.0, Design science, Artificial intelligence, Data sharing, Role-based access control},
abstract = {Industry 4.0 and the associated IoT and data applications are evolving rapidly and expand in various fields. Industry 4.0 also manifests in the farming sector, where the wave of Agriculture 4.0 provides multiple opportunities for farmers, consumers and the associated stakeholders. Our study presents the concept of Data Sharing Agreements (DSAs) as an essential path and a template for AI applications of data management among various actors. The approach we introduce adopts design science principles and develops role-based access control based on AI techniques. The application is presented through a smart farm scenario while we incrementally explore the data sharing challenges in Agriculture 4.0. Data management and sharing practices should enforce defined contextual policies for access control. The approach could inform policymaking decisions for role-based data management, specifically the data-sharing agreements in the context of Industry 4.0 in broad terms and Agriculture 4.0 in specific.}
}
@article{LI2021100483,
title = {Robust Gaussian process regression based on iterative trimming},
journal = {Astronomy and Computing},
volume = {36},
pages = {100483},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2021.100483},
url = {https://www.sciencedirect.com/science/article/pii/S2213133721000378},
author = {Zhao-Zhou Li and Lu Li and Zhengyi Shao},
keywords = {Gaussian process, Robust regression, Outlier detection, Ridge line, Star clusters},
abstract = {The Gaussian process (GP) regression can be severely biased when the data are contaminated by outliers. This paper presents a new robust GP regression algorithm that iteratively trims the most extreme data points. While the new algorithm retains the attractive properties of the standard GP as a nonparametric and flexible regression method, it can greatly improve the model accuracy for contaminated data even in the presence of extreme or abundant outliers. It is also easier to implement compared with previous robust GP variants that rely on approximate inference. Applied to a wide range of experiments with different contamination levels, the proposed method significantly outperforms the standard GP and the popular robust GP variant with the Student-t likelihood in most test cases. In addition, as a practical example in the astrophysical study, we show that this method can precisely determine the main-sequence ridge line in the color–magnitude diagram of star clusters.}
}
@article{KUFNER2021103389,
title = {Vertical data continuity with lean edge analytics for industry 4.0 production},
journal = {Computers in Industry},
volume = {125},
pages = {103389},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103389},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306230},
author = {Thomas Küfner and Stefan Schönig and Richard Jasinski and Andreas Ermer},
keywords = {Edge analytics, Industry 4.0, Smart sensors, Machine learning},
abstract = {Industry 4.0 is characterized by the digitization and networking of machines and systems in production. The amount of data in production is increasing, providing information about processes and thus enables the autonomous monitoring, control and optimization of value creation processes. However, there have been several open challenges and current research questions identified. In particular, new solutions need to be scalable and high-performing to deal with the growing volumes of data close to real-time. The work at hand tackles these research gaps by presenting an approach to realize vertical data continuity by combining signal acquisition and simultaneous data evaluation in a decentralized system without the use of time-consuming external cloud solutions. The approach has been evaluated in laboratory as well as in industrial settings.}
}
@article{MANTELERO2021105561,
title = {An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems},
journal = {Computer Law & Security Review},
volume = {41},
pages = {105561},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105561},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000340},
author = {Alessandro Mantelero and Maria Samantha Esposito},
keywords = {Artificial intelligence, Human rights, Human Rights Impact Assessment, Data protection, AI regulation, Data ethics},
abstract = {Different approaches have been adopted in addressing the challenges of Artificial Intelligence (AI), some centred on personal data and others on ethics, respectively narrowing and broadening the scope of AI regulation. This contribution aims to demonstrate that a third way is possible, starting from the acknowledgement of the role that human rights can play in regulating the impact of data-intensive systems. The focus on human rights is neither a paradigm shift nor a mere theoretical exercise. Through the analysis of more than 700 decisions and documents of the data protection authorities of six countries, we show that human rights already underpin the decisions in the field of data use. Based on empirical analysis of this evidence, this work presents a methodology and a model for a Human Rights Impact Assessment (HRIA). The methodology and related assessment model are focused on AI applications, whose nature and scale require a proper contextualisation of HRIA methodology. Moreover, the proposed models provide a more measurable approach to risk assessment which is consistent with the regulatory proposals centred on risk thresholds. The proposed methodology is tested in concrete case-studies to prove its feasibility and effectiveness. The overall goal is to respond to the growing interest in HRIA, moving from a mere theoretical debate to a concrete and context-specific implementation in the field of data-intensive applications based on AI.}
}
@article{FAVA2021124,
title = {The bioeconomy in Italy and the new national strategy for a more competitive and sustainable country},
journal = {New Biotechnology},
volume = {61},
pages = {124-136},
year = {2021},
issn = {1871-6784},
doi = {https://doi.org/10.1016/j.nbt.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1871678420302041},
author = {Fabio Fava and Lucia Gardossi and Patrizia Brigidi and Piergiuseppe Morone and Daniela A.R. Carosi and Andrea Lenzi},
keywords = {Bioeconomy strategy, Circular bioeconomy, Italian bioeconomy},
abstract = {Italy has the third largest bioeconomy in Europe (€330 billion annual turnover, 2 million employees), making it a core pillar of the national economy. Its sectors of excellence are food and biobased products, and it is a consistent presence in research and innovation projects funded by the EU Horizon 2020 programme (Societal Challenges 2) and the European Public Private Partnership “Biobased industry” (BBI-JU). The bioeconomy reduces dependence on fossil fuels and finite materials, loss of biodiversity and changing land use. It contributes to environmental regeneration, spurs economic growth and supports jobs in rural, coastal and abandoned industrial areas, leveraging local contexts and traditions. In 2017 the Italian government promoted the development of a national Bioeconomy Strategy (BIT), recently updated (BIT II) to interconnect more efficiently the pillars of the national bioeconomy: production of renewable biological resources, their conversion into valuable food/feed, biobased products and bio-energy, and transformation and valorization of bio-waste streams. BIT II aims to improve coordination between Ministries and Italian regions in alignment of policies, regulations, R&I funding programmes and infrastructures investment. The goal is a 15 % increase in turnover and employment in the Italian bioeconomy by 2030. Based on Italy’s strategic geopolitical position in the Mediterranean basin, BIT II also includes actions to improve sustainable productivity, social cohesion and political stability through the implementation of bioeconomy strategies in this area. This paper provides an insight into these strategies and discusses the strengths and weaknesses of the sectors involved and the measures, regulatory initiatives and monitoring actions undertaken.}
}
@article{PARK2021115691,
title = {The risk of hip fractures in individuals over 50 years old with prediabetes and type 2 diabetes – A longitudinal nationwide population-based study},
journal = {Bone},
volume = {142},
pages = {115691},
year = {2021},
issn = {8756-3282},
doi = {https://doi.org/10.1016/j.bone.2020.115691},
url = {https://www.sciencedirect.com/science/article/pii/S8756328220304713},
author = {Ho Youn Park and Kyoungdo Han and Youngwoo Kim and Yoon Hwan Kim and Yoo Joon Sur},
keywords = {Diabetes mellitus, Prediabetic state, Diabetic complications, Hip fractures, Risk assessment, Cohort studies},
abstract = {Background
The present study aimed to investigate the association between type 2 diabetes mellitus (T2DM) and hip fractures using a large-scale nationwide population-based cohort that is representative of the Republic of Korea. We determined the risks of hip fractures in individuals with prediabetes and T2DM with different diabetes durations, and compared them with the risks of hip fractures in individuals without T2DM.
Methods
A total of 5,761,785 subjects over 50 years old who underwent the National Health Insurance Service medical checkup in 2009–2010 were included. Subjects were classified into 5 groups based on the diabetes status; Normal, Prediabetes, Newly-diagnosed T2DM, T2DM less than 5 years, and T2DM more than 5 years. They were followed from the date of the medical checkup to the end of 2016. The endpoint was a new development of hip fracture during follow-up. The hazard ratios (HRs) and 95% confidence intervals (CIs) of hip fractures for each group were analyzed using Cox proportional hazard regression models after adjusting for age, sex, smoking, alcohol drinking, regular exercise, body mass index, hypertension, dyslipidemia, and chronic kidney disease.
Results
The HRs of hip fractures were 1 in the Normal group, 1.032 (95% CI: 1.009, 1.056) in the Prediabetes group, 1.168 (95% CI: 1.113, 1.225) in the Newly-diagnosed T2DM2, 1.543 (95% CI: 1.495, 1.592) in the T2DM less than 5 years and 2.105 (95% CI: 2.054, 2.157) in the T2DM more than 5 years. The secular trend of the HRs of hip fractures according to the duration of T2DM was statistically significant (P < .001). Subgroup analyses also showed the same increasing pattern of the HRs of hip fractures according to the duration of T2DM in both sexes and all age groups (50–64 years, 65–74 years, over 75 years).
Conclusions
In summary, this large-scale, retrospective, longitudinal, nationwide population-based cohort study of 5,761,785 subjects demonstrated that the risks of hip fractures started to increase in prediabetes and was associated linearly with the duration of T2DM. The secular trend of risks of hip fractures according to the duration of T2DM was consistent in both sexes and all age groups.}
}
@article{SUJON2021103844,
title = {Application of weigh-in-motion technologies for pavement and bridge response monitoring: State-of-the-art review},
journal = {Automation in Construction},
volume = {130},
pages = {103844},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103844},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002958},
author = {Mohhammad Sujon and Fei Dai},
keywords = {Weigh-in-motion, Review, Response monitoring, Structural health monitoring, Weight enforcement},
abstract = {Overweight vehicles may cause damage and premature deterioration of pavement and bridge structures. Weigh-in-Motion (WIM) is efficient in avoiding structural damage and ensuring successful weight enforcement by measuring a vehicle's weight in a dynamic state. WIM additionally provides information such as traffic volume, vehicle's speed, axle spacing, equivalent single axle load (ESAL), individual axle and gross vehicle weight (GVW), which is of value to planning, design, construction, and operations of transportation infrastructures. This paper reviewed the state of practice and research in WIM with focuses on its potential, limitations, cost-effectiveness, and data usage. Discussion was made on identifying needs and challenges for further development. This review provides the research community with a holistic view of available WIM techniques, their limitations, cost-effectiveness, and the need for future research on usage of the WIM data that might lead to wider adoption of WIM in transportation applications.}
}
@article{DURRANT2021100493,
title = {How might technology rise to the challenge of data sharing in agri-food?},
journal = {Global Food Security},
volume = {28},
pages = {100493},
year = {2021},
issn = {2211-9124},
doi = {https://doi.org/10.1016/j.gfs.2021.100493},
url = {https://www.sciencedirect.com/science/article/pii/S2211912421000031},
author = {Aiden Durrant and Milan Markovic and David Matthews and David May and Georgios Leontidis and Jessica Enright},
keywords = {Data Trusts, Data sharing, AI Technologies, Agri-food supply chains},
abstract = {Data sharing is often hindered by a number of real word challenges caused by a mixture of technological and social factors. To date, the agri-food sector significantly lags behind other sectors in overcoming these challenges. However, the benefits of data sharing are too great to be ignored as they have a potential to address many historical failings such as issues related to food safety, traceability and transparency, and must be carefully considered as the sector is undergoing a widespread digitalisation. In this article, we explore the potential of different technologies in addressing the challenges presented by data sharing in the agri-food sector, and how the use of these technologies in the narrative of a Data Trust may address many of these obstacles. We argue the importance of utilising semantic web technologies, distributed ledger technologies, machine learning, and privacy preserving technologies to enable future transformative data sharing infrastructures in the agri-food sector. The utilisation of holistic statistical analysis of the shared data is also discussed, vital in supporting many of the sectors optimisation and sustainability goals.}
}
@article{YANG20211000,
title = {Research and applications of artificial neural network in pavement engineering: A state-of-the-art review},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {8},
number = {6},
pages = {1000-1021},
year = {2021},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2021.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095756421001008},
author = {Xu Yang and Jinchao Guan and Ling Ding and Zhanping You and Vincent C.S. Lee and Mohd Rosli {Mohd Hasan} and Xiaoyun Cheng},
keywords = {Pavement engineering, Pavement design, Artificial neural network, Deep learning, Pavement life cycle, Health inspection and monitoring},
abstract = {Given the great advancements in soft computing and data science, artificial neural network (ANN) has been explored and applied to handle complicated problems in the field of pavement engineering. This study conducted a state-of-the-art review for surveying the recent progress of ANN application at different stages of pavement engineering, including pavement design, construction, inspection and monitoring, and maintenance. This study focused on the papers published over the last three decades, especially the studies conducted since 2013. Through literature retrieval, a total of 683 papers in this field were identified, among which 143 papers were selected for an in-depth review. The ANN architectures used in these studies mainly included multi-layer perceptron neural network (MLPNN), convolutional neural network (CNN) and recurrent neural network (RNN) for processing one-dimensional data, two-dimensional data and time-series data. CNN-based pavement health inspection and monitoring attracted the largest research interest due to its potential to replace human labor. While ANN has been proved to be an effective tool for pavement material design, cost analysis, defect detection and maintenance planning, it is facing huge challenges in terms of data collection, parameter optimization, model transferability and low-cost data annotation. More attention should be paid to bring multidisciplinary techniques into pavement engineering to tackle existing challenges and widen future opportunities.}
}
@article{GUO2021109256,
title = {Improved kinematic interpolation for AIS trajectory reconstruction},
journal = {Ocean Engineering},
volume = {234},
pages = {109256},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109256},
url = {https://www.sciencedirect.com/science/article/pii/S002980182100682X},
author = {Shaoqing Guo and Junmin Mou and Linying Chen and Pengfei Chen},
keywords = {AIS, Ship trajectory, Trajectory reconstruction, Kinematic interpolation},
abstract = {Ship trajectory information has made a significant contribution to the data-based research in analyzing maritime transportation and has facilitated the improvement of maritime safety. However, the AIS data, which consists of ship trajectory, inevitably contains noises or missing data that can interfere with the conclusion. In this paper, an improved kinematic interpolation is presented for AIS trajectory reconstruction, which integrates data preprocessing and interpolation that considers the ships' kinematic information. The improved kinematic reconstruction method includes four steps: (1) data preprocessing, (2) analysis of time interval distribution, (3) abnormal data detection and removal, (4) kinematic interpolation that takes the kinematic feature of ships (i.e., velocity and acceleration) into account, adding forward and backward track points to help correct the acceleration function of reconstruction points. The proposed method is tested on the AIS dataset of Zhoushan Port and was compared with traditional ship trajectory reconstruction methods. The comparison indicates that the proposed method can effectively reconstruct the ship trajectory with higher performance on a single ship trajectory and a large AIS data set of certain water areas.}
}
@article{FRIKHA202198,
title = {Reinforcement and deep reinforcement learning for wireless Internet of Things: A survey},
journal = {Computer Communications},
volume = {178},
pages = {98-113},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002681},
author = {Mohamed Said Frikha and Sonia Mettali Gammar and Abdelkader Lahmadi and Laurent Andrey},
keywords = {Internet of Things, Reinforcement learning, Deep reinforcement learning, Wireless Networks},
abstract = {Nowadays, many research studies and industrial investigations have allowed the integration of the Internet of Things (IoT) in current and future networking applications by deploying a diversity of wireless-enabled devices ranging from smartphones, wearables, to sensors, drones, and connected vehicles. The growing number of IoT devices, the increasing complexity of IoT systems, and the large volume of generated data have made the monitoring and management of these networks extremely difficult. Numerous research papers have applied Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) techniques to overcome these difficulties by building IoT systems with effective and dynamic decision-making mechanisms, dealing with incomplete information related to their environments. The paper first reviews pre-existing surveys covering the application of RL and DRL techniques in IoT communication technologies and networking. The paper then analyzes the research papers that apply these techniques in wireless IoT to resolve issues related to routing, scheduling, resource allocation, dynamic spectrum access, energy, mobility, and caching. Finally, a discussion of the proposed approaches and their limits is followed by the identification of open issues to establish grounds for future research directions proposal.}
}
@article{CAI2021128792,
title = {The need for urban form data in spatial modeling of urban carbon emissions in China: A critical review},
journal = {Journal of Cleaner Production},
volume = {319},
pages = {128792},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128792},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621029905},
author = {Meng Cai and Yuan Shi and Chao Ren and Takahiro Yoshida and Yoshiki Yamagata and Chao Ding and Nan Zhou},
keywords = {Urban carbon emissions, Spatial modeling, Systematic review, Urban form, China},
abstract = {Cities produce over 70% of global carbon emissions and are thus crucial in driving climate change. Urban carbon emissions may continue to increase especially in those less-developed countries and regions which are still under rapid urban development. Policymakers need to find ways to effectively control and reduce carbon emissions. Thus, spatial modeling methods to map and predict urban carbon emissions have been developed to meet these needs. This paper examines the progress of the spatial modeling of carbon emissions and the relationship between urban form and carbon emissions in China by reviewing more than 100 peer-reviewed journal articles in the Scopus database. The latest prediction methods and techniques are described in the paper. Their advantages and limitations are then discussed. Urban forms have a significant influence on carbon emissions and have been applied in spatial modeling studies in other countries. However, this review has identified the lack of urban form data and high-resolution inventories from existing studies in China. Future developments in the spatial modeling in China should therefore have a fine spatial resolution and incorporate open and high-quality urban form data, including urban morphology and land use/land cover.}
}
@article{RUMSON2021109214,
title = {The application of fully unmanned robotic systems for inspection of subsea pipelines},
journal = {Ocean Engineering},
volume = {235},
pages = {109214},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109214},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821006442},
author = {Alexander G. Rumson},
keywords = {Automation, Inspection, Marine robotics, Underwater structures, Unmanned underwater technology, Underwater vehicles},
abstract = {This paper focuses on recent innovations in the methods used for external remote subsea pipeline inspection. An unmanned method is revealed, in which an Autonomous Underwater Vehicle (AUV), paired with an Unmanned Surface Vessel (USV) was utilised to complete inspections. Results are presented from a recent project, in which existing hardware and software were integrated and combined with automated workflows to form a solution, involving an AUV operated from a USV, with operations remotely controlled from shore. As inspection data was acquired, self-actuating workflows ran onboard the AUV, enabling data processing tasks to commence and QC messages/alerts to be transmitted to the control centre, this allowed execution of in-water mission adjustments. The primary focus of this paper is on the development and implementation of an automated, fully unmanned system for subsea inspection operations. Initially, a brief review is presented of recent developments in this field. Links are drawn between these wider developments and progress made within the project, and areas are highlighted where further work is required for realisation of a comprehensive unmanned pipeline inspection solution.}
}
@article{FURNESS2021129127,
title = {Building the ‘Bio-factory’: A bibliometric analysis of circular economies and Life Cycle Sustainability Assessment in wastewater treatment},
journal = {Journal of Cleaner Production},
volume = {323},
pages = {129127},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129127},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621033163},
author = {Madeline Furness and Ricardo Bello-Mendoza and Jonatan Dassonvalle and Rolando Chamy-Maggi},
keywords = {Life cycle sustainability assessment, Resource recovery, Wastewater treatment, Circular economies, Biofactory},
abstract = {The “Biofactory” is a circular economy-based concept for wastewater treatment that improves water quality, promotes efficient use of materials and energy, while also recovering resources and decreasing both emissions and costs. Due to socio-economic bottlenecks, such as typical high costs and low public acceptance of novel resource recovery scenarios in wastewater treatment, realizing the Biofactory goals becomes a difficult task. Decision makers are currently unable to appreciate the environmental, social, and economic benefits of the Biofactory, as most decision-making tools focus on mainly technical and economic aspects. This is the first review is to use bibliometric analysis of publication trends in life cycle-based modelling for circular economies in wastewater treatment across the globe, focusing on Life Cycle Assessment (LCA), Life Cycle Costing (LCC) and Social Life Cycle Assessment (SLCA). The integration of LCA, LCC and SLCA for the development of a Life Cycle Sustainability Assessment (LCSA) decision making framework is recommended, while practical implications for the methodological development of this tool are compiled. The goal and scope of LCSA for the Biofactory must explore multi-product functional units for the recovery of different resources from wastewater, where system boundaries must include techno-environmental, social and economic systems together. Internal loops, feedbacks loops, avoided products and co-product allocation methodologies within the integrated system boundaries must be considered. Innovation is required for building LCA, LCC and SLCA inventories, where life cycle data management is important for implementing the Biofactory into the future.}
}
@incollection{2021467,
title = {Index},
editor = {Cynthia J. Girman and Mary Elizabeth Ritchey},
booktitle = {Pragmatic Randomized Clinical Trials},
publisher = {Academic Press},
pages = {467-476},
year = {2021},
isbn = {978-0-12-817663-4},
doi = {https://doi.org/10.1016/B978-0-12-817663-4.20001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128176634200019}
}
@article{CARTWRIGHT2021120,
title = {Managing relationships on social media in business-to-business organisations},
journal = {Journal of Business Research},
volume = {125},
pages = {120-134},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320307815},
author = {Severina Cartwright and Iain Davies and Chris Archer-Brown},
keywords = {Social media, B2B marketing, Relationship marketing, Business networks},
abstract = {Social media (SM) constitutes a valuable source of market intelligence, characterised by great ease and efficiency of interactions between networked partners, and by facilitation of individual expressions of self and brand engagement. Thus, SM can enable interaction, collaboration, and networking, thereby strengthening the relationships between actors within networks. Nonetheless, research into B2B organisations ́ usage of SM for relationship management remains limited, fragmented and lacking strategic direction. To expand the current state of theory, we draw upon twelve case studies of SM management concerning tactics for acquiring new and potential relationships, building a reputation online, and engaging with business partners. Results show four distinct engagement strategies that organisations tend to employ when implementing SM marketing strategies. The four strategies provide an insight into current approaches to SM marketing within B2B organisations, and how organisations are structuring themselves and managing their resources to respond to this opportunity.}
}
@article{MEI20212358,
title = {Web resources facilitate drug discovery in treatment of COVID-19},
journal = {Drug Discovery Today},
volume = {26},
number = {10},
pages = {2358-2366},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S135964462100204X},
author = {Long-Can Mei and Yin Jin and Zheng Wang and Ge-Fei Hao and Guang-Fu Yang},
keywords = {Bioinformatics, SARS-CoV-2, Sequence and structure, Drug design, Vaccines, Monoclonal antibodies},
abstract = {The infectious disease Coronavirus 2019 (COVID-19) continues to cause a global pandemic and, thus, the need for effective therapeutics remains urgent. Global research targeting COVID-19 treatments has produced numerous therapy-related data and established data repositories. However, these data are disseminated throughout the literature and web resources, which could lead to a reduction in the levels of their use. In this review, we introduce resource repositories for the development of COVID-19 therapeutics, from the genome and proteome to antiviral drugs, vaccines, and monoclonal antibodies. We briefly describe the data and usage, and how they advance research for therapies. Finally, we discuss the opportunities and challenges to preventing the pandemic from developing further.}
}
@article{IPPOLITO2021108300,
title = {Improving facies prediction by combining supervised and unsupervised learning methods},
journal = {Journal of Petroleum Science and Engineering},
volume = {200},
pages = {108300},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.108300},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520313541},
author = {Marco Ippolito and John Ferguson and Fred Jenson},
keywords = {Facies classification, Machine learning, Supervised learning, Unsupervised learning, Bias, Joint PDF},
abstract = {Facies classification from well logs is an indispensable part of seismic interpretation and is important in the determination of sequence stratigraphy and ultimately reservoir characterization. Although there have been improvements in the tools used to perform this task, it remains laborious, subjective, and error-prone. Achieving a proper classification is complicated by increasing dataset sizes as well as the need for correlated multidisciplinary models. Recent developments in machine learning provide an opportunity to assist interpreters in accomplishing this task while also improving the accuracy of classification results. Applications of machine learning methods for automating facies classification from well logs have previously been explored, however these have largely focused on evaluations or comparisons of individual algorithms or of ensembles of homogeneous agents. The proposed method combines heterogeneous agents to enhance prediction accuracy. Specifically, supervised learning, which provides a direct mapping between the data domain and the solution domain while introducing bias to generalize the mapping, is combined with unsupervised learning, which does not depend on similar generalization bias or training data but also does not provide a direct mapping between the data and solution domains. The combination is accomplished via the joint probability density function (PDF) of the supervised classification, which is used to guide identification of clusters delineated by unsupervised learning. This multi-agent approach can reduce bias introduced during training and provides a basis for generating a probability distribution for each sample rather than a discrete classification. The distribution, in turn, can be used to more accurately model the continuous nature of well log signals, which reflects continuity in lithological regimes.}
}
@article{WANG2021325,
title = {Early event detection in a deep-learning driven quality prediction model for ultrasonic welding},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {325-336},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001333},
author = {Baicun Wang and Yang Li and Ying Luo and Xingyu Li and Theodor Freiheit},
keywords = {Ultrasonic welding, Quality prediction, Deep-learning, Long short-term memory, Event detection},
abstract = {A goal in ultrasonic welding (USW) process monitoring is to accurately predict quality outcomes based on monitored signals. However, in most cases, knowing only that the USW process has failed is insufficient. Modern process automation should assess signal information and intercede to rectify process problems. Identification of when a process signal deviates from an acceptable final quality outcome, i.e., the time at which an abnormal event starts, facilitates control action or root cause analysis to bring it back to compliance. A long short-term memory (LSTM) recurrent neural network is proposed to monitor USW and other time-series signals and identify this point. This deep neural network is trained to classify quality outcomes from continuous signals. The process monitoring signals and their sampling time are divided into finite segments as input to this network. The time segment at which the process signal first converges to the final quality class prediction is identified using cross-entropy of the classification probabilities. This procedure is demonstrated using USW quality monitoring algorithms and robot motion failure detection. The examples show an LSTM network not only provides high accuracy for USW quality prediction, but also that the time of classification convergence is consistent with variance observed in USW weld quality factors. Moreover, classification convergence time was shown to be associated to specific robot motion failures, useful as input to adaptive learning. This work realizes deep-learning driven quality prediction and early event detection for quality classification problems, and provides the information necessary for adaptive control algorithms.}
}
@article{LIN2021108362,
title = {Uncertainty quantification and software risk analysis for digital twins in the nearly autonomous management and control systems: A review},
journal = {Annals of Nuclear Energy},
volume = {160},
pages = {108362},
year = {2021},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2021.108362},
url = {https://www.sciencedirect.com/science/article/pii/S0306454921002383},
author = {Linyu Lin and Han Bao and Nam Dinh},
keywords = {Digital twin, Autonomous control, Uncertainty quantification, Software risk analysis},
abstract = {A nearly autonomous management and control (NAMAC) system is designed to furnish recommendations to operators for achieving particular goals based on NAMAC’s knowledge base. As a critical component in a NAMAC system, digital twins (DTs) are used to extract information from the knowledge base to support decision-making in reactor control and management during all modes of plant operations. With the advancement of artificial intelligence and data-driven methods, machine learning algorithms are used to build DTs of various functions in the NAMAC system. To evaluate the uncertainty of DTs and its impacts on the reactor digital instrumentation and control systems, uncertainty quantification (UQ) and software risk analysis is needed. As a comprehensive overview of prior research and a starting point for new investigations, this study selects and reviews relevant UQ techniques and software hazard and software risk analysis methods that may be suitable for DTs in the NAMAC system.}
}
@article{ZHENG202110,
title = {Characterizing urban land changes of 30 global megacities using nighttime light time series stacks},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {173},
pages = {10-23},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000022},
author = {Qiming Zheng and Qihao Weng and Ke Wang},
keywords = {Nighttime light imagery, Land use and land cover changes, Urbanization, Time series analysis, Megacities, Urban areas, Sustainability},
abstract = {Worldwide urbanization has brought about diverse types of urban land use and land cover (LULC) changes. The diversity of urban land changes, however, have been greatly under studied, since the major focus of past research has been on urban growth. In this study, we proposed a framework to characterize diverse urban land changes of 30 global megacities using monthly nighttime light time series from VIIRS data. First, we developed a Logistic-Harmonic model to fit VIIRS time series. Second, by leveraging the uniqueness of urban land change and nighttime light data, we incorporated temporal information of VIIRS time series and proposed a new classification scheme to produce monthly maps of built-up areas and to disentangle urban land changes into five categories. Third, we provided an in-depth analysis and comparison of urban land change patterns of the selected megacities. Results demonstrated that the Logistic-Harmonic model yielded a robust performance in fitting VIIRS time series. Temporal features based classification can not only significantly improve the mapping accuracy of built-up areas, especially for regions with heterogeneous built-up and non-built-up landscapes, but also promoted temporal consistency and classification efficiency. Urban land changes occurred in 51% of the built-up pixels of the megacities. Compared with urban growth, other types of urban land change, particularly land use intensification, contributed to an unexpectedly large proportion of the changes (83%). The findings of this study offer an insightful understanding on global urbanization processes in megacities, and evoke further investigation on the environmental and ecological implications of urban land changes.}
}
@article{PIAO2021102651,
title = {Privacy preserving in blockchain-based government data sharing: A Service-On-Chain (SOC) approach},
journal = {Information Processing & Management},
volume = {58},
number = {5},
pages = {102651},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102651},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321001400},
author = {Chunhui Piao and Yurong Hao and Jiaqi Yan and Xuehong Jiang},
keywords = {Consortium blockchain, Government data sharing, Privacy preserving, Smart contract},
abstract = {Sharing government data is of great significance to social development, but insecure and inappropriate sharing may lead to privacy breaches. Data sharing in consortium blockchain has provided a promising direction for efficient privacy preserving government data sharing. However, since government data is not owned by a single person or any government employers, it is hard to attribute participants’ responsibility for motivating data sharing behavior in blockchain systems. Furthermore, the scope and scale of government data to be shared among departments is unclear, as the purposes for retrieving shared data are dynamically changing. In order to solve these problems, we propose a Service-On-Chain (or simply, SOC) approach. The SOC approach can effectively identify different departments’ data retrieving requirements while efficiently sharing government data with trustworthiness in data content and controllability in data ownership. In particular, we utilize smart contracts to provide an onchain service to define data sharing agreements between government departments, which can identify ambiguous data retrieving requirements and formalize the process logic. We apply the SOC approach to a real world scenario and demonstrate that the SOC approach provides a feasible solution for secure and efficient sharing of data among government departments.}
}
@incollection{BIBAULT2021361,
title = {Chapter 18 - Artificial intelligence in oncology},
editor = {Lei Xing and Maryellen L. Giger and James K. Min},
booktitle = {Artificial Intelligence in Medicine},
publisher = {Academic Press},
pages = {361-381},
year = {2021},
isbn = {978-0-12-821259-2},
doi = {https://doi.org/10.1016/B978-0-12-821259-2.00018-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212592000181},
author = {Jean-Emmanuel Bibault and Anita Burgun and Laure Fournier and André Dekker and Philippe Lambin},
keywords = {Oncology, cancer, artificial intelligence, deep learning, machine learning, prediction},
abstract = {Medical decisions can rely on a very large number of parameters, but it is traditionally considered that our cognitive capacity can only integrate up to five factors in order to take a decision. Oncologists will need to combine vast amount of clinical, biological, and imaging data to achieve state-of-the-art treatments. Data science and artificial intelligence (AI) will have an important role in the generation of models to predict outcome and guide treatments. A new paradigm of data-driven decision-making, reusing routine health-care data to provide decision support is emerging. This chapter explores the studies published in imaging, medical and radiation oncology and explains the technical challenges that need to be addressed before AI can be routinely used to treat cancer patients.}
}
@article{RAJKUMARREDDY2021107334,
title = {Developing a blockchain framework for the automotive supply chain: A systematic review},
journal = {Computers & Industrial Engineering},
volume = {157},
pages = {107334},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107334},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221002382},
author = {Kotha {Raj Kumar Reddy} and Angappa Gunasekaran and P. Kalpana and V. {Raja Sreedharan} and S {Arvind Kumar}},
keywords = {Automotive supply chain, Blockchain, Systematic literature review, VUCA world},
abstract = {As world is affected by demand volatility; process uncertainty; supply chain complexity and information ambiguity forming a VUCA world. To manage this scenario, industries are adopting emerging technologies for business excellence and one among them is Blockchain. Blockchain technology (BCT) is a distributed ledger technology (DLT) that stores transactional records in a tamper-proof and immutable way; it is a promising solution for incorporating transparency and traceability in traditional ecosystem. As automotive industries are facing a Volatile environment, Uncertain schedules & information; Complex supply chain networks, and Ambiguous decisions that cripples the automotive supply chain (ASC). Therefore, BCT can be used to address issues related to ASC in VUCA world. Keeping this in mind, study reported a systematic literature review (SLR) of BCT applications in ASC. More than seventy research papers were reviewed based on different BCT characteristics and applications. Through content analysis, study explored how to link supply chain visibility, information transparency with BCT for an efficient ASC in VUCA world. Moreover, a BCT implementation framework is proposed for ASC, to provide a decision-making approach for practitioners in VUCA world.}
}
@article{YANG2021e690,
title = {Associations between HIV infection and clinical spectrum of COVID-19: a population level analysis based on US National COVID Cohort Collaborative (N3C) data},
journal = {The Lancet HIV},
volume = {8},
number = {11},
pages = {e690-e700},
year = {2021},
issn = {2352-3018},
doi = {https://doi.org/10.1016/S2352-3018(21)00239-3},
url = {https://www.sciencedirect.com/science/article/pii/S2352301821002393},
author = {Xueying Yang and Jing Sun and Rena C Patel and Jiajia Zhang and Siyuan Guo and Qulu Zheng and Amy L Olex and Bankole Olatosi and Sharon B Weissman and Jessica Y Islam and Christopher G Chute and Melissa Haendel and Gregory D Kirk and Xiaoming Li and Richard Moffitt and Hana Akelsrod and Keith A Crandall and Nora Francheschini and Evan French and Teresa {Po-Yu Chiang} and G Caleb-Alexander and Kathleen M Andersen and Amanda J Vinson and Todd T Brown and Roslyn B Mannon},
abstract = {Summary
Background
Evidence of whether people living with HIV are at elevated risk of adverse COVID-19 outcomes is inconclusive. We aimed to investigate this association using the population-based National COVID Cohort Collaborative (N3C) data in the USA.
Methods
We included all adult (aged ≥18 years) COVID-19 cases with any health-care encounter from 54 clinical sites in the USA, with data being deposited into the N3C. The outcomes were COVID-19 disease severity, hospitalisation, and mortality. Encounters in the same health-care system beginning on or after January 1, 2018, were also included to provide information about pre-existing health conditions (eg, comorbidities). Logistic regression models were employed to estimate the association of HIV infection and HIV markers (CD4 cell count, viral load) with hospitalisation, mortality, and clinical severity of COVID-19 (multinomial). The models were initially adjusted for demographic characteristics, then subsequently adjusted for smoking, obesity, and a broad range of comorbidities. Interaction terms were added to assess moderation effects by demographic characteristics.
Findings
In the harmonised N3C data release set from Jan 1, 2020, to May 8, 2021, there were 1 436 622 adult COVID-19 cases, of these, 13 170 individuals had HIV infection. A total of 26 130 COVID-19 related deaths occurred, with 445 among people with HIV. After adjusting for all the covariates, people with HIV had higher odds of COVID-19 death (adjusted odds ratio 1·29, 95% CI 1·16–1·44) and hospitalisation (1·20, 1·15–1·26), but lower odds of mild or moderate COVID-19 (0·61, 0·59–0·64) than people without HIV. Interaction terms revealed that the elevated odds were higher among older age groups, male, Black, African American, Hispanic, or Latinx adults. A lower CD4 cell count (<200 cells per μL) was associated with all the adverse COVID-19 outcomes, while viral suppression was only associated with reduced hospitalisation.
Interpretation
Given the COVID-19 pandemic's exacerbating effects on health inequities, public health and clinical communities must strengthen services and support to prevent aggravated COVID-19 outcomes among people with HIV, particularly for those with pronounced immunodeficiency.
Funding
National Center for Advancing Translational Sciences, National Institute of Allergy and Infectious Diseases, National Institutes of Health, USA.}
}
@incollection{MARTYR2021469,
title = {Chapter 14 - Data handling and modeling},
editor = {Anthony J. Martyr and David R. Rogers},
booktitle = {Engine Testing (Fifth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Fifth Edition},
address = {Oxford},
pages = {469-509},
year = {2021},
isbn = {978-0-12-821226-4},
doi = {https://doi.org/10.1016/B978-0-12-821226-4.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212264000140},
author = {Anthony J. Martyr and David R. Rogers},
keywords = {Data-science, machine-learning, artificial-intelligence, clustering, regression, prediction, model, data handling},
abstract = {In this chapter an overview of the industry standard for powertrain test data analytics will be provided, along with the emergence of the application of data science for engineering data. This is in order to provide the facility engineering with information of the value of modern data science in powertrain test and development environments. The material serves to provide those who are new to powertrain test data analysis mechanisms, the basic practices of data acquisition, analysis, and utilization as they currently exist within the industry. In addition, providing information for those seeking deeper information on sophisticated statistical and machine-learning tools within the context of powertrain engineering, along with the types of problems to which they are suited. As a summary, the chapter compares and contrasts conventional data practice and emerging processes, making observations on the possible future directions for the industry in this rapidly developing field.}
}
@article{MOTHUKURI2021619,
title = {A survey on security and privacy of federated learning},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {619-640},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329848},
author = {Viraaji Mothukuri and Reza M. Parizi and Seyedamin Pouriyeh and Yan Huang and Ali Dehghantanha and Gautam Srivastava},
keywords = {Artificial intelligence, Machine learning, Distributed learning, Federated learning, Federated machine learning, Security, Privacy},
abstract = {Federated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FL’s security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios.}
}
@article{GARG2021120407,
title = {Measuring the perceived benefits of implementing blockchain technology in the banking sector},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120407},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120407},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312336},
author = {Poonam Garg and Bhumika Gupta and Ajay Kumar Chauhan and Uthayasankar Sivarajah and Shivam Gupta and Sachin Modgil},
keywords = {Blockchain, Banking sector, Perceived benefits, Instrument development, AMOS},
abstract = {This study aims to measure the perceived business benefits of blockchain technology implementation in the banking sector and establish factors to measure these benefits. Concerns regarding security, values, and standards are essential to banking operations. Data was collected from 291 respondents who are either blockchain consultants, blockchain marketing experts, or CEOs/business heads of banks that are in the process of advising, consulting, or implementing blockchain technology. Confirmatory factor analysis (CFA) was carried out to assess the reliability and validity of the proposed instrument. The results support the proposed instrument and its five constructs. The scale emerging from this study indicates a good degree of reliability, validity and unidimensionality in each of its constructs. Technologies like blockchain are in their initial stages, and recent advances in blockchain technology may impact our findings. The developed instrument could help give decision makers a foundational view to measure the benefits of implementing blockchain technology before they choose to integrate it in their existing system. The scientific and societal significance of the study based on its practical and theoretical applications is presented at the end.}
}
@article{DETERWANGNE2021105497,
title = {Council of Europe convention 108+: A modernised international treaty for the protection of personal data},
journal = {Computer Law & Security Review},
volume = {40},
pages = {105497},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105497},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920301023},
author = {Cécile {de Terwangne}},
keywords = {Data protection, Council of Europe Convention 108, Modernised Convention 108, Personal data, Informational autonomy, Data subject’s rights, Data security, Transborder data flows, Supervisory authority, Convention Committee},
abstract = {Summary
The Council of Europe has modernized its Convention 108 for the protection of individuals with regard to automatic processing of personal data: in 2018 it adopted Convention 108+. The modernised version of Convention 108 seeks to respond to the challenges posed, in terms of human rights, by the use of new information and communication technologies. This article presents a detailed analysis of this new international text. Convention 108+ contains important innovations: it proclaims the importance of protecting the right to informational autonomy and human dignity in the face of technological developments. It consolidates the proportionality requirement for data processing and strengthens the arsenal of rights of the data subjects. It reinforces the responsibility of those in charge of data processing as well as its transparency. It requires notification of security breaches. It strengthens the independence, powers and means of action of the supervisory authorities. It also strengthens the mechanism to ensure its effective implementation by entrusting the Committee set up by the Convention with the task of verifying compliance with the commitments made by Parties.}
}
@article{OLDHAM20212040,
title = {NHLBI-CMREF Workshop Report on Pulmonary Vascular Disease Classification: JACC State-of-the-Art Review},
journal = {Journal of the American College of Cardiology},
volume = {77},
number = {16},
pages = {2040-2052},
year = {2021},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2021.02.056},
url = {https://www.sciencedirect.com/science/article/pii/S0735109721005660},
author = {William M. Oldham and Anna R. Hemnes and Micheala A. Aldred and John Barnard and Evan L. Brittain and Stephen Y. Chan and Feixiong Cheng and Michael H. Cho and Ankit A. Desai and Joe G.N. Garcia and Mark W. Geraci and Susan D. Ghiassian and Kathryn T. Hall and Evelyn M. Horn and Mohit Jain and Rachel S. Kelly and Jane A. Leopold and Sara Lindstrom and Brian D. Modena and William C. Nichols and Christopher J. Rhodes and Wei Sun and Andrew J. Sweatt and Rebecca R. Vanderpool and Martin R. Wilkins and Beth Wilmot and Roham T. Zamanian and Joshua P. Fessel and Neil R. Aggarwal and Joseph Loscalzo and Lei Xiao},
keywords = {drug repurposing, integrative omics, master clinical trial protocol, precision medicine, pulmonary hypertension, systems biology},
abstract = {The National Heart, Lung, and Blood Institute and the Cardiovascular Medical Research and Education Fund held a workshop on the application of pulmonary vascular disease omics data to the understanding, prevention, and treatment of pulmonary vascular disease. Experts in pulmonary vascular disease, omics, and data analytics met to identify knowledge gaps and formulate ideas for future research priorities in pulmonary vascular disease in line with National Heart, Lung, and Blood Institute Strategic Vision goals. The group identified opportunities to develop analytic approaches to multiomic datasets, to identify molecular pathways in pulmonary vascular disease pathobiology, and to link novel phenotypes to meaningful clinical outcomes. The committee suggested support for interdisciplinary research teams to develop and validate analytic methods, a national effort to coordinate biosamples and data, a consortium of preclinical investigators to expedite target evaluation and drug development, longitudinal assessment of molecular biomarkers in clinical trials, and a task force to develop a master clinical trials protocol for pulmonary vascular disease.}
}
@article{SEE2021736881,
title = {Aquaculture efficiency and productivity: A comprehensive review and bibliometric analysis},
journal = {Aquaculture},
volume = {544},
pages = {736881},
year = {2021},
issn = {0044-8486},
doi = {https://doi.org/10.1016/j.aquaculture.2021.736881},
url = {https://www.sciencedirect.com/science/article/pii/S0044848621005445},
author = {Kok Fong See and Rabiatul Adawiyah Ibrahim and Kim Huat Goh},
keywords = {Aquaculture, Bibliometric analysis, Data envelopment analysis, Efficiency, Productivity, Stochastic frontier analysis},
abstract = {The scientific research on aquaculture efficiency and productivity has been increasing over the years. This study aims to identify the publication trends and growth potential of aquaculture efficiency and productivity studies. A bibliometric analysis was employed for a sample of 85 scientific articles published during the 1998–2020 period. The findings show that authors and institutions have close groups in collaboration networks. Through the citation analysis, three clusters were obtained that were related to the use of stochastic frontier analysis in an empirical application, Norwegian salmon aquaculture, and efficiency studies associated to freshwater aquaculture. For the temporal evolution of the keywords, earlier existing studies adopted a stochastic translog production function to assess the technical efficiency of aquaculture production, whereas later studies used data envelopment analysis, which focused on more diverse research objectives. The farms or subsectors of aquaculture in Norway, Bangladesh, and Vietnam have been analyzed in-depth for the efficiency and productivity in the existing studies. Education, experience, and age of farmers are often used as determinants to explain the variations in technical efficiency. The present study concludes that aquaculture efficiency and productivity research is not moving toward a mature stage. Several of the discovered issues are only focused on specific countries, and there is still room for methodological improvement in assessing aquaculture efficiency and productivity. Nevertheless, research collaborations are growing, and new research trends that are related to environmental regulation and pollution show great interest in aquaculture efficiency and productivity. This study provides a clear roadmap for researchers and practitioners to understand the publication patterns and hotspots in the research field.}
}
@article{SONG2021102656,
title = {Self-help cognitive behavioral therapy application for COVID-19-related mental health problems: A longitudinal trial},
journal = {Asian Journal of Psychiatry},
volume = {60},
pages = {102656},
year = {2021},
issn = {1876-2018},
doi = {https://doi.org/10.1016/j.ajp.2021.102656},
url = {https://www.sciencedirect.com/science/article/pii/S187620182100112X},
author = {Jiaqi Song and Ronghuan Jiang and Nan Chen and Wei Qu and Dan Liu and Meng Zhang and Hongzhen Fan and Yanli Zhao and Shuping Tan},
keywords = {Cognitive behavioral therapy, Depression, Anxiety, Insomnia, COVID-19},
abstract = {Background and aim
Recently, the availability and usefulness of mobile self-help mental health applications have increased, but few applications deal with COVID-19-related psychological problems. This study explored the intervention efficacy of a mobile application on addressing psychological problems related to COVID-19.
Methods
A longitudinal control trial involving 129 Chinese participants with depression symptoms was conducted through the mobile application “Care for Your Mental Health and Sleep during COVID-19” (CMSC) based on WeChat. Participants were divided into two groups: mobile internet cognitive behavioral therapy (MiCBT) and wait-list. The primary outcome was improvement in depression symptoms. Secondary outcomes included improvement in anxiety and insomnia. The MiCBT group received three self-help CBT intervention sessions in one week via CMSC.
Results
The MiCBT group showed significant improvement in depression and insomnia (allP < 0.05) compared with the wait-list group. Although both groups showed significant improvement in anxiety at the intervention’s end, compared with the wait-list group, the MiCBT group had no significant advantage. Correlation analysis showed that improvement in depression and anxiety had a significant positive association with education level. Changes in insomnia were significantly negatively correlated with anxiety of COVID-19 at the baseline. CMSC was considered helpful (n=68, 81.9 %) and enjoyable (n=54, 65.9 %) in relieving depression and insomnia during the COVID-19 outbreak.
Conclusions
CMSC is verified to be effective and convenient for improving COVID-19-related depression and insomnia symptoms. A large study with sufficient evidence is required to determine its continuous effect on reducing mental health problems during the pandemic.}
}
@article{CALAFIORE2021101539,
title = {A geographic data science framework for the functional and contextual analysis of human dynamics within global cities},
journal = {Computers, Environment and Urban Systems},
volume = {85},
pages = {101539},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101539},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302726},
author = {Alessia Calafiore and Gregory Palmer and Sam Comber and Daniel Arribas-Bel and Alex Singleton},
keywords = {Foursquare, Geographic data science, Urban analytics},
abstract = {This study develops a Geographic Data Science framework that transforms the Foursquare check-in locations and user origin-destination flows data into knowledge about the emerging forms and characteristics of cities' neighbourhoods. We employ a longitudinal mobility dataset describing human interactions with Foursquare venues in ten global cities: Chicago, Istanbul, Jakarta, London, Los Angeles, New York, Paris, Seoul, Singapore, Tokyo. This social media data provides spatio-temporally referenced digital traces left by human use of urban environments, giving us access to the intangible aspects of urban life, such as people behaviours and preferences. Our framework capitalizes on these new data sources, bringing about a novel Geographic Data Science and human-centered methodological approach. Combining network science – a study area with great promise for the analysis of cities and their structure – with geospatial analysis methods, we model cities as a series of global urban networks. Through a spatially weighted community detection algorithm, we uncover functional neighbourhoods for the ten global cities. Each neighbourhood is linked to hyper-local characterisations of their built environment for the Foursquare venues that compose them, and complemented with a range of measures describing their diversity, morphology and mobility. This information is used in a clustering exercise that uncovers a set of four functional neighbourhood types. Our results enable the profiling and comparison of functional neighbourhoods, based on human dynamics and their contexts, across the sample of global cities. The framework is portable to other geographic contexts where interaction data are available to bind different localities into functional agglomerations, and provide insight into their contextual and human dynamics.}
}
@article{YAN2021100011,
title = {Emerging approaches applied to maritime transport research: Past and future},
journal = {Communications in Transportation Research},
volume = {1},
pages = {100011},
year = {2021},
issn = {2772-4247},
doi = {https://doi.org/10.1016/j.commtr.2021.100011},
url = {https://www.sciencedirect.com/science/article/pii/S2772424721000111},
author = {Ran Yan and Shuaian Wang and Lu Zhen and Gilbert Laporte},
keywords = {Maritime transport, Shipping, Port, Data-driven modeling, Digitalization in the maritime industry},
abstract = {Maritime transport is the backbone of international trade and globalization. Maritime transport research can be roughly divided into two categories, namely the shipping side and the port side. Most of the classic approaches adopted to address practical problems in these research topics are based on long-term observations and expert knowledge, while few of them are based on historical data accumulated from practice. In recent years, emerging approaches, which we refer to as machine learning and deep learning techniques in this essay, have been receiving a wider attention to solve practical problems. As a relatively conservative industry, there are some initial trials of applying the emerging approaches to solve practical problems in the maritime sector. The objective of this essay is to review the application of emerging approaches to maritime transport research. The main research topics in maritime transport and classic methods developed to solve them are first presented. The introduction of emerging approaches and their suitability to be applied in maritime transport research is then discussed. Related existing studies are then reviewed according to problem settings, main data sources, and emerging approaches adopted. Challenges and solutions in the process are also discussed from the perspectives of data, model, users, and targets. Finally, promising future research directions are identified. This essay is the first to give a comprehensive review of existing studies on developing machine learning and deep learning models together with popular data sources used to address practical problems in maritime transport.}
}
@article{YANG2021102384,
title = {Digital transformation solutions of entrepreneurial SMEs based on an information error-driven T-spherical fuzzy cloud algorithm},
journal = {International Journal of Information Management},
pages = {102384},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102384},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000773},
author = {Zaoli Yang and Jinping Chang and Lucheng Huang and Abbas Mardani},
keywords = {Digital transformation, Entrepreneurial SMEs, Evaluation and selection, T-spherical fuzzy cloud, T-spherical fuzzy cloud weighted Heronian mean operator},
abstract = {The digital transformation of enterprises has become an inevitable development trend and one of the key driving forces that promotes the sustainable development of enterprises. However, due to the many obstacles of financial burdens, technical thresholds, and talent shortages, digital transformation has become a challenging task for entrepreneurial Small and Medium-Sized Enterprises (SMEs). Additionally, many competitive digital transformation solutions on the market cause confusion when enterprises must choose one. This study drew a new information error-driven T-spherical fuzzy cloud algorithm to evaluate digital transformation solutions of entrepreneurial SMEs and support its selection. First, an evaluation index system for the digital transformation solution of entrepreneurial SMEs was established from four aspects. Then, a new concept of a T-spherical fuzzy cloud was defined to represent the evaluation information of the indicators. Additionally, a T-spherical Fuzzy Cloud Weighted Heronian Mean (T-SFCWHM) operator was used to aggregate the evaluation information. Afterward, an evaluation and selection decision framework for the digital transformation solution of entrepreneurial SMEs based on the T-SFCWHM operator was developed. Further, a practical example was given to illustrate the effectiveness of the proposed method. Finally, a discussion of the findings in our study was conducted, and the conclusions were summarized.}
}
@article{ZHANG2021114261,
title = {Using CatBoost algorithm to identify middle-aged and elderly depression, national health and nutrition examination survey 2011–2018},
journal = {Psychiatry Research},
volume = {306},
pages = {114261},
year = {2021},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2021.114261},
url = {https://www.sciencedirect.com/science/article/pii/S0165178121005564},
author = {Chenyang Zhang and Xiaofei Chen and Song Wang and Junjun Hu and Chunpeng Wang and Xin Liu},
keywords = {Depression, Machine learning, Middle-aged and elderly, NHANES},
abstract = {Depression is one of the most common mental health problems in middle-aged and elderly people. The establishment of risk factor-based depression risk assessment model is conducive to early detection and early treatment of high-risk groups of depression. Five machine learning models (logistic regression (LR); back propagation (BP); random forest (RF); support vector machines (SVM); category boosting (CatBoost) were used to evaluate the depression among 8374 middle-aged people and 4636 elderly people in the NHANES database from 2011 to 2018. In the 2011–2018 cycle, the estimated prevalence of depression was 8.97% in the middle-aged participants and 8.02% in the elderly participants. Among the middle-aged and elderly participants, CatBoost was the best model to identify depression, and its area under the working characteristic curve (AUC) reaches the highest. The second is LR model and SVM model, while the performance of BP and RF model was slightly worse. The primary influencing factor of depression in middle-aged male is alanine aminotransferase. All five machine learning models can identify the occurrence of depression in the NHANES data set through social demographics, lifestyle, laboratory data and other data of middle-aged and elderly people, and among five models, the CatBoost model performed best.}
}
@article{BROWN2021107694,
title = {Opportunities for improving recognition of coastal wetlands in global ecosystem assessment frameworks},
journal = {Ecological Indicators},
volume = {126},
pages = {107694},
year = {2021},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2021.107694},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X21003599},
author = {Christopher J. Brown and Maria F. Adame and Christina A. Buelow and Marieke A. Frassl and Shing Yip Lee and Brendan Mackey and Eva C. McClure and Ryan M. Pearson and Anusha Rajkaran and Thomas S. Rayner and Michael Sievers and Chantal A. {Saint Ange} and Ana I. Sousa and Vivitskaia J.D. Tulloch and Mischa P. Turschwell and Rod M. Connolly},
keywords = {Seagrass, Saltmarsh, Mangrove, Fish nursery, Ecosystem condition, System of environmental-economic accounting indicators, Biodiversity, Health index},
abstract = {Vegetated coastal wetlands, including seagrass, saltmarsh and mangroves, are threatened globally, yet the need to avert these losses is poorly recognized in international policy, such as in the Convention on Biological Diversity and the United Nations (UN) Sustainable Development Goals. Identifying the impact of overlooking coastal wetlands in ecosystem assessment frameworks could help prioritize research efforts to fill these gaps. Here, we examine gaps in the recognition of coastal wetlands in globally applicable ecosystem assessments. We address both shortfalls in assessment frameworks when it comes to assessing wetlands, and gaps in data that limit widespread application of assessments. We examine five assessment frameworks that track fisheries, greenhouse gas emissions, ecosystem threats, and ecosystem services. We found that these assessments inform management decisions, but that the functions provided by coastal wetlands are incompletely represented. Most frameworks had sufficient complexity to measure wetland status, but limitations in data meant they were incompletely informed about wetland functions and services. Incomplete representation of coastal wetlands may lead to them being overlooked by research and management. Improving the coverage of coastal wetlands in ecosystem assessments requires improving global scale mapping of wetland trends, developing global-scale indicators of wetland function and synthesis to quantitatively link animal population dynamics to wetland trends. Filling these gaps will help ensure coastal wetland conservation is properly informed to manage them for the outstanding benefits they bring humanity.}
}
@article{ZHONG2021107767,
title = {Characteristics of vegetation response to drought in the CONUS based on long-term remote sensing and meteorological data},
journal = {Ecological Indicators},
volume = {127},
pages = {107767},
year = {2021},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2021.107767},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X21004325},
author = {Shaobo Zhong and Ziheng Sun and Liping Di},
keywords = {Drought, Standardized precipitation evapotranspiration index (SPEI), Vegetation condition index (VCI), Vegetation response, CONUS},
abstract = {Drought is one of the billion-dollar natural disasters and hard to trace and measure. In recent years drought monitoring becomes much easier with remote sensing. However, it is still difficult to pin vegetation variances on drought because of the delay of the caused vegetation stress. To assess vegetative drought, it is important to first understand the relationship between meteorological condition and vegetation condition, and measure the vegetation responses to meteorological drought. It would be very helpful for effective early warning about agricultural drought. This study uses the CONUS as the study area, and utilizes remote sensing products such as NDVI/VCI (normalized difference vegetation index/vegetation condition index) and SPI/SPEI (standardized precipitation index/standardized precipitation evapotranspiration index) to give a comparative evaluation to the vegetation’s drought response. The used vegetation products and meteorological data are ensured to be consistent. The scale and lag of vegetation response to drought for various vegetation types and aridity levels were thoroughly investigated. The results show that: The AVHRR and MODIS NDVI series pairs and the meteorological drought index series pairs (SPEI and SPI) have fairly good consistencies. Among them, 69.5% and 84% have rho (correlation coefficient) values greater than 0.8 respectively. For the NDVI series pairs, the maximum rho value is 0.98, the minimum rho value is −0.47, and the mean rho value is 0.79, which are 0.97, 0.68, and 0.87 respectively for the meteorological index series pairs. Compared to rho values of the meteorological index series pairs, the rho values of the NDVI series pairs have more outliers indicating instability. The correlation between SPEI and VCI significantly relies on time lags and has high spatial inhomogeneity. 1- and 2-month lags of SPEI have more significant positive correlation with VCI in Arid, Semi-Arid and Dry sub-humid areas of central west CONUS with less precipitation and lower temperature. For various time scales (time scale is SPEI reference range), the most significant positive correlation between SPEI and VCI happens in the time scales of 6- to 12-month in summer, the time scales of 3- and 6-month in spring and autumn, and the time scales of 2- and 3-month in winter regardless of time lags. Despite of the different vegetation types and aridity levels, the maximum correlations between SPEI and VCI are observed in Hyper arid regions in January, Arid regions in April, Semi-arid regions in July, and Dry sub-humid regions in October. Shrub has prominent responses in January and April, grass responses appear in July and October, and Evergreen forest shows least responses in all seasons. The results add more insights of the connection between vegetation and climate, and guide the development of new technology leveraging remote sensing data for vegetation drought monitoring and early-warning. The results are also helpful to provide important references for studying large-scale physiological and phenological properties of the vegetation under different climate conditions.}
}
@article{YUAN2021401,
title = {Spatiotemporal attention mechanism-based deep network for critical parameters prediction in chemical process},
journal = {Process Safety and Environmental Protection},
volume = {155},
pages = {401-414},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2021.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S0957582021004900},
author = {Zhuang Yuan and Zhe Yang and Yiqun Ling and Chuanpeng Wu and Chuankun Li},
keywords = {Chemical processes, Parameters prediction, Deep networks, Spatiotemporal attention mechanism, Feature representation},
abstract = {In chemical processes, grasping the changing trend of critical parameters can help field operators take appropriate adjustments to eliminate potential fluctuations. Thus, deep networks, renowned for its revolutionary feature representation capability, have been gradually exploited for building reliable prediction models from massive data embraced tremendously nonlinearities and dynamics. Because of the inherent complexity, the process trajectories over the whole running duration make distinctive contributions to the ultimate targets. Specifically, features extracted from different secondary variables at different previous instants have diverse impacts on the current state of primary variables. However, this spatiotemporal relevance discrepancy is rarely considered, which may lead to deterioration of prediction performance. Therefore, this paper seamlessly integrates the spatiotemporal attention (STA) mechanism with convolutional neural networks (CNN) and bi-directional long short-term memory (BiLSTM), and proposes a novel predictive model, namely STA-ConvBiLSTM. Using the deep framework composed of CNN and BiLSTM, the integrated model can, not only automatically explore the esoteric spatial correlations among high-dimensional variables at each time step, but also adaptively excavate beneficial temporal characteristics across all time steps. Meanwhile, STA is further introduced to assign corresponding weights to information with dissimilar importance, so as to prevent high target-relevant interactions from being discarded due to overlong sequences and excessive features. STA-ConvBiLSTM is applied in the case of furnace tube temperature prediction of a delayed coking unit, which exhibits a significant improvement of the prediction accuracy.}
}
@article{WU20213318,
title = {Iterative tomography with digital adaptive optics permits hour-long intravital observation of 3D subcellular dynamics at millisecond scale},
journal = {Cell},
volume = {184},
number = {12},
pages = {3318-3332.e17},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421005328},
author = {Jiamin Wu and Zhi Lu and Dong Jiang and Yuduo Guo and Hui Qiao and Yi Zhang and Tianyi Zhu and Yeyi Cai and Xu Zhang and Karl Zhanghao and Hao Xie and Tao Yan and Guoxun Zhang and Xiaoxu Li and Zheng Jiang and Xing Lin and Lu Fang and Bing Zhou and Peng Xi and Jingtao Fan and Li Yu and Qionghai Dai},
keywords = {long-term high-speed imaging, adaptive optics, light-field microscopy, phototoxicity, intravital, migrasome, retraction fiber, tumor metastasis, calcium imaging},
abstract = {Summary
Long-term subcellular intravital imaging in mammals is vital to study diverse intercellular behaviors and organelle functions during native physiological processes. However, optical heterogeneity, tissue opacity, and phototoxicity pose great challenges. Here, we propose a computational imaging framework, termed digital adaptive optics scanning light-field mutual iterative tomography (DAOSLIMIT), featuring high-speed, high-resolution 3D imaging, tiled wavefront correction, and low phototoxicity with a compact system. By tomographic imaging of the entire volume simultaneously, we obtained volumetric imaging across 225 × 225 × 16 μm3, with a resolution of up to 220 nm laterally and 400 nm axially, at the millisecond scale, over hundreds of thousands of time points. To establish the capabilities, we investigated large-scale cell migration and neural activities in different species and observed various subcellular dynamics in mammals during neutrophil migration and tumor cell circulation.}
}
@article{LI2021103955,
title = {Applications of distributed ledger technology (DLT) and Blockchain-enabled smart contracts in construction},
journal = {Automation in Construction},
volume = {132},
pages = {103955},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103955},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004064},
author = {Jennifer Li and Mohamad Kassem},
keywords = {Blockchain, Building Information Modelling (BIM), Construction sector, Distributed ledger technology (DLT), Smart contracts, Systematic review, Thematic analysis},
abstract = {The contribution of distributed ledger technology (DLT) (e.g. blockchain) and smart contracts to the digitalisation and digital transformation of the construction sector is nascent but rapidly gaining traction. ‘Systematic reviews’ of DLT and smart contract applications that are specific to the construction sector are missing. This paper performs an extensive systematic review of 153 DLT and smart contract papers specific to the design, construction and operation of built assets. The protocols and processes of a systematic review were adopted to ensure full transparency, accountability, reproducibility, and updateability of the results. Through thematic analysis, we identify eight distinct themes of applications for DLT and smart contracts in construction: information management, payments, procurement, supply chain management, regulations and compliance, construction management and delivery, dispute resolution, and technological systems. Each theme identified was analysed to understand current capabilities, applications, and future developments. A cross-themes discussion revealed that DLT and smart contracts are ‘supplementary’ technologies that are used in combination with other technologies (e.g. BIM, IoT, cloud computing) as part of ‘technological systems’ that need to co-evolve in order to enable the themes' applications identified. Research into DLT and smart contracts in construction is rapidly moving from theoretical insights and frameworks into developing proofs-of-concept studies (27 studies) and testing them in case studies (20 studies). The next stage of research involving wider academic communities and industry-wide engagement is expected to begin uncovering the anticipated benefits of DLT and smart contracts through investments into technological systems and testing in real-world pilot studies. The discussion of the themes identified from technology, policy, process, and society perspectives exposed the need for an extended socio-technical approach to the solution in order to deliver the necessary change and impact from the adoption of DLT and smart contracts at speed and scale. The results of this systematic review provide a noteworthy reference point for academics, practitioners and policy makers interested in the future development of DLT and smart contract applications in construction.}
}
@article{LIU2021112364,
title = {Production of global daily seamless data cubes and quantification of global land cover change from 1985 to 2020 - iMap World 1.0},
journal = {Remote Sensing of Environment},
volume = {258},
pages = {112364},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112364},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721000821},
author = {Han Liu and Peng Gong and Jie Wang and Xi Wang and Grant Ning and Bing Xu},
keywords = {Global land cover mapping, Seamless data cube, Daily, Analysis ready data, Cloud computing, Intelligent mapping, Amazon web Services},
abstract = {Longer time high-resolution, high-frequency, consistent, and more detailed land cover data are urgently needed in order to achieve sustainable development goals on food security, high-quality habitat construction, biodiversity conservation and planetary health, and for the understanding, simulation and management of the Earth system. However, due to technological constraints, it is difficult to provide simultaneously high spatial resolution, high temporal frequency, and high quality observation data. Existing mapping solutions are limited by traditional remotely sensed data, that have shorter observation periods, poor spatio-temporal consistency and comparability. Therefore, a new mapping paradigm is needed. This paper develops a framework for intelligent mapping (iMap) of land cover based on state-of-the-art technologies such as cloud computing, artificial intelligence, virtual constellations, and spatio-temporal reconstruction and fusion. Under this framework, we built an automated, serverless, end-to-end data production chain and parallel mapping system based on Amazon Web Services (AWS) and produced the first 30 m global daily seamless data cubes (SDC), and annual to seasonal land cover maps for 1985–2020. The SDC was produced through a multi-source spatio-temporal data reconstruction and fusion workflow based on Landsat, MODIS, and AVHRR virtual constellations. Independent validation results show that the relative mean error of the SDC is less than 2.14%. As analysis ready data (ARD), it can lay a foundation for high-precision quantitative remote sensing information extraction. From this SDC, we produced 36-year long, 30 m resolution global land cover map data set by combining strategies of sample migration, machine learning, and spatio-temporal adjustment. The average overall accuracy of our annual land cover maps over multiple periods of time is 80% for level 1 classification and over 73% for level 2 classification (29 and 33 classes). Based on an objective validation sample consisting of FLUXNET sites, our map accuracy is 10% higher than that of existing global land cover datasets including Globeland30. Our results show that the average global land cover change rate is 0.36%/yr. Global forest decreased by 1.47 million km2 from 38.44 million km2, cropland increased by 0.84 million km2 from 12.49 million km2 and impervious surface increased by 0.48 million km2 from 0.57 million km2 during 1985– 2020.}
}
@article{MINBAEVA2021100820,
title = {Disrupted HR?},
journal = {Human Resource Management Review},
volume = {31},
number = {4},
pages = {100820},
year = {2021},
note = {Navigating the shifting landscapes of HRM},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2020.100820},
url = {https://www.sciencedirect.com/science/article/pii/S1053482220300930},
author = {Dana Minbaeva},
keywords = {Strategic HR, Disruption, Flexible workforce, Digitalization, Analytics},
abstract = {In this paper, I discuss possible avenues for future research aimed at bridging the research-practice gap on the topic of disruptions in human resources (HR). I focus on three global mega-trends—the flexible workforce, the digitalization of business models, and artificial intelligence and machine learning—and examine their influence on the field of human resource management (HRM) in general and in the context of the COVID-19 pandemic. I discuss why HRM research has overlooked potential paradigm-shifting possibilities that could ultimately equip HR practitioners with the knowledge needed to respond to disruptions caused by these mega-trends.}
}
@article{VANSANTEN2021264,
title = {Microbial natural product databases: moving forward in the multi-omics era},
journal = {Natural Product Reports},
volume = {38},
number = {1},
pages = {264-278},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d0np00053a},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822003555},
author = {Jeffrey A. {van Santen} and Satria A. Kautsar and Marnix H. Medema and Roger G. Linington},
abstract = {Covering: 2010–2020 The digital revolution is driving significant changes in how people store, distribute, and use information. With the advent of new technologies around linked data, machine learning and large-scale network inference, the natural products research field is beginning to embrace real-time sharing and large-scale analysis of digitized experimental data. Databases play a key role in this, as they allow systematic annotation and storage of data for both basic and advanced applications. The quality of the content, structure, and accessibility of these databases all contribute to their usefulness for the scientific community in practice. This review covers the development of databases relevant for microbial natural product discovery during the past decade (2010–2020), including repositories of chemical structures/properties, metabolomics, and genomic data (biosynthetic gene clusters). It provides an overview of the most important databases and their functionalities, highlights some early meta-analyses using such databases, and discusses basic principles to enable widespread interoperability between databases. Furthermore, it points out conceptual and practical challenges in the curation and usage of natural products databases. Finally, the review closes with a discussion of key action points required for the field moving forward, not only for database developers but for any scientist active in the field.}
}
@article{THOMAS2021100427,
title = {SPARTAN: Maximizing the use of spectro-photometric observational data during template fitting},
journal = {Astronomy and Computing},
volume = {34},
pages = {100427},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2020.100427},
url = {https://www.sciencedirect.com/science/article/pii/S2213133720300810},
author = {R. Thomas},
keywords = {Galaxy, Fitting, Observations, Spectroscopy, Photometry},
abstract = {SPARTAN [Spectroscopic And photometRic fitting Tool for Astronomical aNalysis] is a tool designed to perform the fitting of galaxy observations either using photometry and low resolution spectroscopy separately or simultaneously. Based on a grid search χ2 fitting method, SPARTAN was tailored to UV-to-NIR data and designed for well calibrated data. The first version of this tool allows the use of the low resolution models of Bruzual & Charlot (2003) and include the treatment of the intergalactic medium as a free parameter. It has been designed to be an user-friendly environment where people do not need to know how to code to perform the fit. SPARTAN includes everything needed to perform the fit, from the galaxy models creation, to the visualization of the results through the graphical interface. SPARTAN is a fully open source software made with Python 3. It is published under the GNU general public license (v3) and is available in a Github repository. It can be installed directly from the python official repository (pypi) and the documentation is available through a github repository.}
}
@article{BUYUKOZKAN2021108309,
title = {Evaluating Blockchain requirements for effective digital supply chain management},
journal = {International Journal of Production Economics},
volume = {242},
pages = {108309},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108309},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321002851},
author = {Gülçin Büyüközkan and Gizem Tüfekçi and Deniz Uztürk},
keywords = {Blockchain, Digitalization, Supply chain, House of quality, Intuitionistic fuzzy sets, Incomplete preferences},
abstract = {Collapsing traditional supply chain (SC) nodes into digitally interconnected networks creates several opportunities for efficiency, cost savings, and SC traceability. Blockchain is one of the pioneering transformative 4.0 technologies and is seen as the backbone of digital SCs (DSCs). Several attempts have been made to create explorative designs or models, however, forming a strategic Blockchain vision remains a challenge. This paper specifies those customer needs and design requirements that should be prioritized with a systematic approach for effective Blockchain integration into SCs. The proposed functional methodology provides a practical roadmap for practitioners. It is based on the House of Quality (HoQ) method, with its customer-focused design aspect, utilizing a group decision making (GDM) approach with an incomplete intuitionistic fuzzy relation (IIFR) extension. The GDM approach is employed to overcome the biases of decision making, while IIFRs deal with different or complementary focus centers in qualitative data. The usefulness of this methodology is tested with an application, and the results are validated by experts in the field. The results indicate that a Blockchain-based DSC (BC-DSC) is expected to deliver continuing financial benefit, efficient use of time, and the ability to support. Automation, effective coordination, and conformity are identified as highly critical design requirements. GDM and IIFR are combined for the first time in the literature. The originality of this paper comes from its generation of critical factors geared to obtaining an effective BC-DSC structure and the implementation of HoQ as an efficient tool for design.}
}
@article{WANG202141,
title = {3D Printing, Computational Modeling, and Artificial Intelligence for Structural Heart Disease},
journal = {JACC: Cardiovascular Imaging},
volume = {14},
number = {1},
pages = {41-60},
year = {2021},
issn = {1936-878X},
doi = {https://doi.org/10.1016/j.jcmg.2019.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20305155},
author = {Dee Dee Wang and Zhen Qian and Marija Vukicevic and Sandy Engelhardt and Arash Kheradvar and Chuck Zhang and Stephen H. Little and Johan Verjans and Dorin Comaniciu and William W. O’Neill and Mani A. Vannan},
keywords = {3D printing, artificial intelligence, computational modeling, computed tomography, left atrial appendage, structural heart disease, transcatheter aortic valve replacement, transcatheter mitral valve replacement, transesophageal echocardiogram},
abstract = {Structural heart disease (SHD) is a new field within cardiovascular medicine. Traditional imaging modalities fall short in supporting the needs of SHD interventions, as they have been constructed around the concept of disease diagnosis. SHD interventions disrupt traditional concepts of imaging in requiring imaging to plan, simulate, and predict intraprocedural outcomes. In transcatheter SHD interventions, the absence of a gold-standard open cavity surgical field deprives physicians of the opportunity for tactile feedback and visual confirmation of cardiac anatomy. Hence, dependency on imaging in periprocedural guidance has led to evolution of a new generation of procedural skillsets, concept of a visual field, and technologies in the periprocedural planning period to accelerate preclinical device development, physician, and patient education. Adaptation of 3-dimensional (3D) printing in clinical care and procedural planning has demonstrated a reduction in early-operator learning curve for transcatheter interventions. Integration of computation modeling to 3D printing has accelerated research and development understanding of fluid mechanics within device testing. Application of 3D printing, computational modeling, and ultimately incorporation of artificial intelligence is changing the landscape of physician training and delivery of patient-centric care. Transcatheter structural heart interventions are requiring in-depth periprocedural understanding of cardiac pathophysiology and device interactions not afforded by traditional imaging metrics.}
}
@article{TIEDE2021112163,
title = {Investigating ESA Sentinel-2 products' systematic cloud cover overestimation in very high altitude areas},
journal = {Remote Sensing of Environment},
volume = {252},
pages = {112163},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112163},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720305368},
author = {Dirk Tiede and Martin Sudmanns and Hannah Augustin and Andrea Baraldi},
keywords = {Sentinel-2 cloud mask, Cloud cover overestimation, Big EO data image selection, Geographical bias},
abstract = {Cloud detection in optical remote sensing imagery is crucial because undetected clouds can produce misleading results in analyses. Almost all optical remote sensing data access portals rely to some degree on a cloud cover filter. Here we show that cirrus as well as opaque cloud cover in Sentinel-2 Level-1C (L1C) and Level-2A (L2A) imagery is systematically and significantly overestimated in very high altitude areas (e.g. Himalayas, Andes). We argue that this systematic bias is created by applying simple thresholds to single bands instead of using a multi-band spectral signature in the cloud detection process. This results in a lot of “hidden” data for very high altitude areas when each image's estimated cloud cover is used as an automated selection criterion for analysis (e.g. global analyses, cloud-free mosaic production). We show geographic locations exemplifying this overestimation, and compare the L1C and L2A cloud masks produced by ESA to cloud masks generated by an expert system that uses comprehensive spectral signatures, showing that reliable cloud estimations are possible in very high altitudes. Based on this comparison, we argue for changes to L1C and L2A cloud detection algorithms in order to improve initial querying and selection of big EO data, where reliable yet automated quality indicators are necessary to handle an overwhelming data volume and velocity. Our contribution raises awareness of potential bias when pre-selecting images based on reported cloud cover in very high altitude areas for researchers and users of Sentinel-2 imagery in the environmental domain.}
}
@article{EMMENS2021114975,
title = {The promises and perils of Automatic Identification System data},
journal = {Expert Systems with Applications},
volume = {178},
pages = {114975},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114975},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421004164},
author = {Ties Emmens and Chintan Amrit and Asad Abdi and Mayukh Ghosh},
keywords = {AIS data, Data mining, Navigation safety, Ship behavior analysis, Environmental evaluation, Advanced applications of AIS data},
abstract = {Automatic Identification System (AIS) is used to identify vessels in maritime navigation. Currently, it is used for various commercial purposes. However, the abundance and lack of quality of AIS data make it difficult to capitalize on its value. Therefore, an understanding of both the limitations of AIS data and the opportunities is important to maximize its value, but these have not been clearly stated in the existing literature. This study aims to help researchers and practitioners understand AIS data by identifying both the promises and perils of AIS data. We identify the different applications and limitations of AIS data in the literature and build upon them in a sequential mixed-design study. We first identify the promises and perils that exist in the literature. We then analyze AIS data from the port of Amsterdam quantitatively to detect noise and to find the perils researchers and practitioners could encounter. Our results incorporate quantitative findings with qualitative insights obtained from interviewing domain experts. This study extends the literature by considering multiple limitations of AIS data across different domains at the same time. Our results show that the amount of noise in AIS data depends on factors such as the equipment used, external factors, humans, dense traffic etc. The contribution that our paper makes is in combining and making a comprehensive list of both the promises and perils of AIS data. Consequently, this study helps researchers and practitioners to (i) identify the sources of noise, (ii) to reduce the noise in AIS data and (iii) use it for the benefits of their research or the optimization of their operations.}
}
@article{KHALLAF2021103760,
title = {Classification and analysis of deep learning applications in construction: A systematic literature review},
journal = {Automation in Construction},
volume = {129},
pages = {103760},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103760},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002119},
author = {Rana Khallaf and Mohamed Khallaf},
keywords = {Systematic literature review, Deep learning, Construction, Damage detection},
abstract = {In recent years, the construction industry has experienced an expansion in the multitude of projects and emergent information. With the advent of deep learning, new opportunities have emerged for utilizing this vast amount of data to solve construction-related issues. While the use of deep learning has been increasing in construction, there has been no review on these applications to date. Therefore, this paper presents a Systematic Literature Review on the use of deep learning applications in construction. A total of 80 journal papers were identified and analyzed. Among these papers, six application-based topics were identified: equipment tracking, crack detection, construction work management, sewer assessment, 3D point cloud enhancement, and miscellaneous topics. Analysis shows that deep learning has been beneficial in leveraging data in areas such as crack detection and segmentation of infrastructure and sewers; equipment and worker detection and; and analysis and reporting on construction-related operations. Additionally, a discussion of the various deep learning techniques is provided as well as a contrast between deep learning, machine learning, and artificial intelligence.}
}
@article{SHAKORSHAHABI2021102337,
title = {Application of data mining in Iran's Artisanal and Small-Scale mines challenges analysis},
journal = {Resources Policy},
volume = {74},
pages = {102337},
year = {2021},
issn = {0301-4207},
doi = {https://doi.org/10.1016/j.resourpol.2021.102337},
url = {https://www.sciencedirect.com/science/article/pii/S0301420721003469},
author = {Reza ShakorShahabi and Ali Nouri Qarahasanlou and Seyed Reza Azimi and Adel Mottahedi},
keywords = {Data mining, Artisanal and small-scale mines, Clustering, Decision tree},
abstract = {Most of the mines operating in Iran are classified into Artisanal and Small-scale mines (ASM). ASM accounts for 98.3% of the country's 10,000 mines, more than 80% of employment, and about 65% of the mining sector production. However, these mines face liquidity, legal and administrative issues, sales market, infrastructure, and investment. Though, their activation and restoration require many limited resources compared to large mines. Therefore, it is undeniable to use this sector's capacity to create sustainable employment and development in deprived areas of the country (due to ASM's geographical extent) and help supply raw materials. Hence, in this paper, in the first step, identifying and troubleshooting in these mines was done based on field information and organ documents such as Ministry of Industry, Mine and Trade, Iranian Mines and Mining Industries Development and Renovation Organization (IMIDRO), Iran Minerals Procurement and Production Company, etc. A database consisting of 313 mines from 29 provinces of the country was formed and evaluated using a data mining approach. In this study, two data mining methods, including clustering and decision tree, were used. As a result, appropriate divisions were presented based on available information without any previous hypotheses or backgrounds. The purpose of these divisions was to provide an appropriate classification of mines by applying different estimators to make strategic decisions. Because at present, in most decisions, mines are divided solely based on an estimator such as geographical distance, mineral genus, annual production.}
}
@article{DWIVEDI2021102168,
title = {Setting the future of digital and social media marketing research: Perspectives and research propositions},
journal = {International Journal of Information Management},
volume = {59},
pages = {102168},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102168},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220308082},
author = {Yogesh K. Dwivedi and Elvira Ismagilova and D. Laurie Hughes and Jamie Carlson and Raffaele Filieri and Jenna Jacobson and Varsha Jain and Heikki Karjaluoto and Hajer Kefi and Anjala S. Krishen and Vikram Kumar and Mohammad M. Rahman and Ramakrishnan Raman and Philipp A. Rauschnabel and Jennifer Rowley and Jari Salo and Gina A. Tran and Yichuan Wang},
keywords = {Artificial intelligence, Augmented reality marketing, Digital marketing, Ethical issues, eWOM, Mobile marketing, Social media marketing},
abstract = {The use of the internet and social media have changed consumer behavior and the ways in which companies conduct their business. Social and digital marketing offers significant opportunities to organizations through lower costs, improved brand awareness and increased sales. However, significant challenges exist from negative electronic word-of-mouth as well as intrusive and irritating online brand presence. This article brings together the collective insight from several leading experts on issues relating to digital and social media marketing. The experts’ perspectives offer a detailed narrative on key aspects of this important topic as well as perspectives on more specific issues including artificial intelligence, augmented reality marketing, digital content management, mobile marketing and advertising, B2B marketing, electronic word of mouth and ethical issues therein. This research offers a significant and timely contribution to both researchers and practitioners in the form of challenges and opportunities where we highlight the limitations within the current research, outline the research gaps and develop the questions and propositions that can help advance knowledge within the domain of digital and social marketing.}
}
@article{BRANDT2021379,
title = {Prescriptive analytics in public-sector decision-making: A framework and insights from charging infrastructure planning},
journal = {European Journal of Operational Research},
volume = {291},
number = {1},
pages = {379-393},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.09.034},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720308389},
author = {Tobias Brandt and Sebastian Wagner and Dirk Neumann},
keywords = {Decision support systems, Public value, Prescriptive analytics, Smart city, Electric mobility},
abstract = {In this work, we investigate the challenges public-sector organizations face when seeking to leverage prescriptive analytics and provide insights into the public value such data-driven tools and methods can provide. Using the strategic triangle of value, legitimacy, and operational capacity as a starting point, we derive a framework to assess public-sector prescriptive analytics initiatives, along with six guiding questions that structure the assessment process. We present a case study applying prescriptive analytics to the placement of charge points in urban areas, a critical challenge many municipalities are currently facing in the transition towards electric mobility. Reflecting on the analytics application as well as its development and implementation process through the guiding questions, we derive key lessons for public-sector organizations seeking to apply prescriptive analytics.}
}
@article{RUNESON2021111088,
title = {Open Data Ecosystems — An empirical investigation into an emerging industry collaboration concept},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111088},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111088},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001850},
author = {Per Runeson and Thomas Olsson and Johan Linåker},
keywords = {Open data, Open data ecosystem, Open innovation, Empirical study},
abstract = {Software systems are increasingly depending on data, particularly with the rising use of machine learning, and developers are looking for new sources of data. Open Data Ecosystems (ODE) is an emerging concept for data sharing under public licenses in software ecosystems, similar to Open Source Software (OSS). It has certain similarities to Open Government Data (OGD), where public agencies share data for innovation and transparency. We aimed to explore open data ecosystems involving commercial actors. Thus, we organized five focus groups with 27 practitioners from 22 companies, public organizations, and research institutes. Based on the outcomes, we surveyed three cases of emerging ODE practice to further understand the concepts and to validate the initial findings. The main outcome is an initial conceptual model of ODEs’ value, intrinsics, governance, and evolution, and propositions for practice and further research. We found that ODE must be value driven. Regarding the intrinsics of data, we found their type, meta-data, and legal frameworks influential for their openness. We also found the characteristics of ecosystem initiation, organization, data acquisition and openness be differentiating, which we advise research and practice to take into consideration.}
}
@article{HUSSAIN2021e156,
title = {First American College of Surgeons National Surgical Quality Improvement Program Report from a Low-Middle-Income Country: A 1-Year Outcome Analysis of Neurosurgical Cases},
journal = {World Neurosurgery},
volume = {155},
pages = {e156-e167},
year = {2021},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2021.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1878875021011967},
author = {Mustafa Mushtaq Hussain and Farida Bibi and Shafqat Shah and Rida Mitha and Muhammad Shahzad Shamim and Afsheen Ziauddin and Hasnain Zafar},
keywords = {30-Day complications, ACS-NSQIP, Developing world, Low-middle-income country, Neurosurgery, Pakistan, Postoperative outcomes},
abstract = {Background
Low-middle-income countries (LMICs) share a substantial proportion of global surgical complications. This is compounded by the seemingly deficient documentation of postsurgical complications and the lack of a national average for comparison. In this context, the implementation of the American College of Surgeons (ACS) National Surgical Quality Improvement Program (NSQIP) that compares hospital performance based on postsurgical complication data provided by a wide array of centers, could be a major initiative in a resource-challenged setting. Implementation of the NSQIP has provenly mitigated postoperative morbidity and mortality across many centers all over the world. To our knowledge, this report is the first from an LMIC to report its postoperative neurosurgical complications in comparison with international benchmarks.
Methods
Our hospital joined the NSQIP in 2019. Through a standardized ACS protocol, ACS-trained surgical clinical reviewers (SCRs) reviewed and extracted data from randomly assigned neurosurgical patients’ medical records from preoperative to postoperative (30-day) data using validated, standardized data definitions. SCRs entered deidentified data in an online Health Insurance Portability and Accountability Act web-based secure platform. The validated data were then consigned to the ACS NSQIP head office in the United States where the data were analyzed and compared with similar data from other centers registered with the NSQIP. In this way, our hospital was rated for each of the variables related to postsurgical complications after both spinal and cranial procedures, and the results were sent back to us in the form of text, tables, and graphs.
Results
Our initial report suggested a relatively higher odds ratio for sepsis and readmissions after spinal procedures at our hospital, and a similarly higher odds ratio for morbidity, sepsis, urinary tract infection, and surgical site infection for cranial procedures. For these variables, our hospital fell in the needs improvement category of the NSQIP. For the rest of the variables studied for both spinal and cranial procedures, the hospital fell in the as expected category of the NSQIP.
Conclusions
Implementation of the NSQIP is an important first step in creating a culture of transparency, safety, and quality. This is the first report of NSQIP implementation in an LMIC, and we have shown comparable results to developed countries.}
}
@article{LU2021117446,
title = {Review of meta-heuristic algorithms for wind power prediction: Methodologies, applications and challenges},
journal = {Applied Energy},
volume = {301},
pages = {117446},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117446},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921008369},
author = {Peng Lu and Lin Ye and Yongning Zhao and Binhua Dai and Ming Pei and Yong Tang},
keywords = {Meta-heuristic algorithms, Wind power forecasting, Combined approach, Multiple time horizons, Multiple error evaluation metrics},
abstract = {The integration of large-scale wind power introduces issues in modern power systems operations due to its strong randomness and volatility. These issues can be resolved via wind power forecasting that can provide comprehensive future information about wind power uncertainties. This paper presents a timely and comprehensive review of meta-heuristic algorithms in the framework of wind power forecasting. The framework is based on the auxiliary layer, forecasting base layer, and core layer. The auxiliary layer, such as the data-decomposition layer, decomposes the wind power time series into many relatively stationary subseries, and uses prediction models, including artificial neural networks (ANNs) and machine learning (ML). The core layer is based on meta-heuristic algorithms, which include evolutionary-based algorithms, physics-based algorithms, human-based algorithms, swarm-based algorithms, hybrid algorithms, and multi-objective optimization algorithms. These algorithms aim to search for the optimal solutions under constraints, which is highly significant for optimizing the key parameters of the prediction models. Besides, multiple error evaluation metrics, e.g., deterministic, uncertainty, and testing methods used in the field of wind power prediction are described. A quantitative analysis focusing on their advantages, disadvantages, forecasting accuracy, and computational costs are also provided. Finally, a few open research issues and trends related to the topic are discussed, which can contribute to improving the understanding of each wind power forecasting method. In general, this review paper provides valuable insights to wind power engineers.}
}
@article{WANG2021127042,
title = {Using sustainable performance prediction in data-scarce scenarios: A study of park-level integrated microgrid projects in Tianjin, China},
journal = {Journal of Cleaner Production},
volume = {304},
pages = {127042},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127042},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621012610},
author = {Yuan Wang and Shuquan Li and Xiuyu Wu and Yan Zhang and Baoluo Li and Lei Gao},
keywords = {Integrated energy system, Sustainable performance prediction, Conditional generative adversarial networks, Long short-term memory, Park level integrated energy system},
abstract = {Time series data of project performance in park-level integrated energy system projects are non-linear, difficult to collect and store, and scarce. Thus, it is difficult to carry out real-time prediction of project performance. In the context of energy and environmental crises, a real-time prediction method for the sustainable development performance of IES projects based on conditional generative adversarial networks-long short-term memory neural networks was proposed after a full study of the park-level IES projects in Tianjin, China. In this study, an evaluation system for the sustainable development performance of IES projects, such as “integrated energy efficiency,” was established to collect the monthly performance index values of 638 IES projects in Tianjin in 2017. The monthly performance evaluation index was calculated using the entropy weight method. After sorting, a binary method was applied to form the monthly performance evaluation label values,"1″ corresponding to the top 50% of the project, “0″ corresponding to the bottom 50% projects, establishing a database of historical project performance. The generator in CGAN game training was initially used to learn the mapping relationship between the noise distribution under the predicted conditions and the historical IES project performance data set, resulting in 6220 project data with similar distribution, with improved generalization ability of online data mining and accuracy of the stabilization algorithm. LSTM was then used to capture the time dependence in IES project performance data characteristics to predict monthly sustainable performance after 12 months of project operation. Compared with other machine learning models, this method is time-adaptive and the model structure is simple. The average response time of performance prediction for the same park was shortened to 2.76 months, and the prediction accuracy increased to 98.75%. Three schemes were designed to verify the effectiveness of the proposed method by comparing the real data of the park-level IES project in Tianjin with the predicted results. These results have practical significance for strengthening the real-time control of integrated energy projects and for effectively promoting the sustainable development of the integrated energy industry.}
}
@article{RODRIGUEZ2021106442,
title = {IoT-Agro: A smart farming system to Colombian coffee farms},
journal = {Computers and Electronics in Agriculture},
volume = {190},
pages = {106442},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106442},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921004592},
author = {Jhonn Pablo Rodríguez and Ana Isabel Montoya-Munoz and Carlos Rodriguez-Pabon and Javier Hoyos and Juan Carlos Corrales},
keywords = {Smart Farming, Internet of Things, Data Analytics, Outlier Detection, BPMN, Coffee Farm},
abstract = {Currently, the adoption of smart technologies for sustainable farming systems creates a distinct competitive edge for farmers, extension services, agri-business, and policy-makers. However, selecting the most appropriate technologies from a wide range of options is never an easy job. In this context, several authors consider Smart Farming as the best solution. However, they fall short in providing more information to recommend the most appropriate IoT technology, the options to manage the IoT infrastructure, and the services to crop management plans and crop production estimation. This paper implements a Smart Farming System based on a three-layered architecture (Agriculture Perception, Edge Computing, and Data Analytics). In the Agriculture Perception Layer, we evaluated Omicron, Libelium, and Intel technologies under criteria such as the price, the number of inputs for sensor connection, communication protocols, portability, battery life, and harvesting energy system photovoltaic panel. We evaluated edge-based management mechanisms in the Edge Layer to provide data reliability, focusing on outlier detection and treatment using Machine Learning and Interpolation algorithms. We recommend the Isolation Forest algorithm for classifying outliers in the monthly temperature dataset (99% of precision) and the Cubic Spline technique for effectively replacing the data classified as outliers (RMSE lower than 0.085). In the Data Analytics Layer, we evaluated different machine learning algorithms to estimate coffee production. The results show that the measured error values of the XGBOOST algorithm keep the values lower than the other models (RMSE 0.008, MAE 0.032, and RSE 0.585). The www.iot-agro.com platform offers farmer services such as weather variables monitoring, coffee production estimating, and IoT infrastructure setting. Finally, stakeholders, researchers, and engineers validated our Smart Farming Solution through a Colombian coffee farm case study. The test evaluated the usability, the straightforward interpretation of data, and the look feel of the web application.}
}
@article{STINGONE2021111019,
title = {Interdisciplinary data science to advance environmental health research and improve birth outcomes},
journal = {Environmental Research},
volume = {197},
pages = {111019},
year = {2021},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2021.111019},
url = {https://www.sciencedirect.com/science/article/pii/S0013935121003133},
author = {Jeanette A. Stingone and Sofia Triantafillou and Alexandra Larsen and Jay P. Kitt and Gary M. Shaw and Judit Marsillach},
keywords = {Preterm birth, Environmental mixtures, Multiple exposures, Public health data science},
abstract = {Rates of preterm birth and low birthweight continue to rise in the United States and pose a significant public health problem. Although a variety of environmental exposures are known to contribute to these and other adverse birth outcomes, there has been a limited success in developing policies to prevent these outcomes. A better characterization of the complexities between multiple exposures and their biological responses can provide the evidence needed to inform public health policy and strengthen preventative population-level interventions. In order to achieve this, we encourage the establishment of an interdisciplinary data science framework that integrates epidemiology, toxicology and bioinformatics with biomarker-based research to better define how population-level exposures contribute to these adverse birth outcomes. The proposed interdisciplinary research framework would 1) facilitate data-driven analyses using existing data from health registries and environmental monitoring programs; 2) develop novel algorithms with the ability to predict which exposures are driving, in this case, adverse birth outcomes in the context of simultaneous exposures; and 3) refine biomarker-based research, ultimately leading to new policies and interventions to reduce the incidence of adverse birth outcomes.}
}
@article{NOMURA2021446,
title = {Pain Management in Clinical Practice Research Using Electronic Health Records},
journal = {Pain Management Nursing},
volume = {22},
number = {4},
pages = {446-454},
year = {2021},
issn = {1524-9042},
doi = {https://doi.org/10.1016/j.pmn.2021.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S152490422100031X},
author = {Aline Tsuma Gaedke Nomura and Lisiane Pruinelli and Luciana Nabinger Menna Barreto and Murilo dos Santos Graeff and Elizabeth A. Swanson and Thamiris Silveira and Miriam de Abreu Almeida},
abstract = {Background: The use of electronic health record (EHR) systems encourages and facilitates the use of data for the development and surveillance of quality indicators, including pain management. Aim: to conduct an integrative review on pain management research using data extracted from EHR in order to synthesize and analyze the following elements: pain management (assessments, interventions, and outcomes) and study results with potential clinical implications, data source, clinical sample characteristics, and method description. Design: An integrative review of the literature was undertaken to identify exemplars of scientific research studies that explore pain management using data from EHR, using Cooper’s framework. Results: Our search of 1,061 records from PubMed, Scopus, and Cinahl was narrowed down to 28 eligible articles to be analyzed. Conclusion: Results of this integrative review will make a critical contribution, assisting others in developing research proposals and sound research methods, as well as providing an overview of such studies over the past 10 years. Through this review it is therefore possible to guide new research on clinical pain management using EHR.}
}
@article{ZHAO2021103465,
title = {Effect of an agency’s resources on the implementation of open government data},
journal = {Information & Management},
volume = {58},
number = {4},
pages = {103465},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103465},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000392},
author = {Yupan Zhao and Bo Fan},
keywords = {OGD, Resource-based theory, Resource allocation, Influencing factors},
abstract = {Open government data (OGD) exhibits substantial political, economic, cultural, and social values that have gained considerable attention globally. Based on the investigation and analysis of the OGD practice, this study raises the research question, “Why do considerable differences exist in the degree of OGD implementation among different agencies under the same local government?” Our study takes resource-based theory as theoretical foundation to explore the factors that affect OGD implementation of constituent agencies within the same local government. A questionnaire survey is conducted to analyze the effect of factors including technical capacity, organizational awareness, organizational arrangement, and rules and regulations on OGD implementation. Results show that the technical capacity, organizational arrangement, and rules and regulations of government agencies have a direct positive effect on OGD implementation. Notably, rules and regulations moderate the relationship between technical capacity and OGD implementation. Besides, the matching degree of technical capacity and other organizational factors in a government agency exerts a positive influence on OGD implementation. Finally, our study proposes policy suggestions that emphasize the direction and focus for OGD implementation.}
}
@article{WANG2021192,
title = {Quantifying the dynamics between environmental information disclosure and firms’ financial performance using functional data analysis},
journal = {Sustainable Production and Consumption},
volume = {28},
pages = {192-205},
year = {2021},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2021.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S2352550921000993},
author = {Deqing Wang and Xuemei Li and Sihua Tian and Lingyun He and Yan Xu and Xu Wang},
keywords = {Environmental information disclosure, Firm financial performance, Data smoothing, Functional regression analysis, Moderating effect},
abstract = {Environmental information disclosure (EID) is an important way for firms to communicate to the government and the public to fulfill their environmental protection responsibilities. Essentially, the dynamic impacts of firms’ activities on the ecological environment are evolving continuously. We aim to introduce functional data analysis (FDA) for exploring the dynamics in the relationship between environmental information disclosure (EID) and firms’ financial performance. Based on continuous curves smoothed from 75 Chinese listed firms of pollution-intensive industries, this study examined the dynamic effect and its structural break of EID on firms' financial performance. Furthermore, moderating effects of public attention, government subsidy and ratio of profits to total cost were tested within a functional framework. The results revealed that the positive effect of EID on firms' financial performance is constantly significant, whereas there existed a structural break in 2015 due to the implementation of new Environmental Protection Law. Moreover, the positive moderating effect of the ratio of profits to total cost is significant only before 2015, while both the main effect and moderating effect of government subsidy are not significant. Surprisingly, although the main effects of public attention are not significant, its positive moderating effect is statistically significant. We contributed in introducing FDA as a useful toolkit for quantifying the time-dynamics in ecological economics, and our findings could offer guidance for stakeholders seeking to improve EID.}
}
@article{GRAHAM2021113061,
title = {Use of spatio-temporal habitat suitability modelling to prioritise areas for common carp biocontrol in Australia using the virus CyHV-3},
journal = {Journal of Environmental Management},
volume = {295},
pages = {113061},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.113061},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721011233},
author = {K. Graham and D. Gilligan and P. Brown and R.D. {van Klinken} and K.A. McColl and P.A. Durr},
keywords = {Bayesian networks, Damage thresholds, Expert opinion, Invasive freshwater fish, Murray-Darling basin, Vertebrate pest species},
abstract = {Common carp (Cyprinus carpio) are an invasive species of the rivers and waterways of south-eastern Australia, implicated in the serious decline of many native fish species. Over the past 50 years a variety of control options have been explored, all of which to date have proved either ineffective or cost prohibitive. Most recently the use of cyprinid herpesvirus 3 (CyHV-3) has been proposed as a biocontrol agent, but to assess the risks and benefits of this, as well as to develop a strategy for the release of the virus, a knowledge of the fundamental processes driving carp distribution and abundance is required. To this end, we developed a novel process-based modelling framework that integrates expert opinion with spatio-temporal datasets via the construction of a Bayesian Network. The resulting weekly networks thus enabled an estimate of the habitat suitability for carp across a range of hydrological habitats in south-eastern Australia, covering five diverse catchment areas encompassing in total a drainage area of 132,129 km2 over a period of 17–27 years. This showed that while suitability for adult and subadult carp was medium-high across most habitats throughout the period, nevertheless the majority of habitats were poorly suited for the recruitment of larvae and young-of-year (YOY). Instead, high population abundance was confirmed to depend on a small number of recruitment hotspots which occur in years of favourable inundation. Quantification of the underlying ecological drivers of carp abundance thus makes possible detailed planning by focusing on critical weaknesses in the population biology of carp. More specifically, it permits the rational planning for population reduction using the biocontrol agent, CyHV-3, targeting areas where the total population density is above a “damage threshold” of approximately 100 kg/ha.}
}
@article{LUO202183,
title = {A multi-task deep learning model for short-term taxi demand forecasting considering spatiotemporal dependences},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {8},
number = {1},
pages = {83-94},
year = {2021},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S209575641830521X},
author = {Huimin Luo and Jianming Cai and Kunpeng Zhang and Ruihang Xie and Liang Zheng},
keywords = {Traffic engineering, Short-term traffic prediction, Deep learning, Multi-task model, Spatiotemporal dependences},
abstract = {Short-term taxi demand forecasting is of great importance to incentivize vacant cars moving from over-supply regions to over-demand regions, which can minimize the wait time for passengers and drivers. With the consideration of spatiotemporal dependences, this study proposes a multi-task deep learning (MTDL) model to predict short-term taxi demand in multi-zone level. The nonlinear Granger causality test is applied to explore the causality relationships among various traffic zones, and long short-term memory (LSTM) is used as the core neural unit to construct the framework of the multi-task deep learning model. In addition, several hyperparameter optimization methods (e.g., grid search, random search, Bayesian optimization, hyperopt) are used to tune the model. Using the taxi trip data in New York City for validation, the multi-task deep learning model considering spatiotemporal dependences (MTDL∗) is compared with the single-task deep learning model (STDL), the full-connected multi-task deep learning model (MTDL#) and other benchmark algorithms (such as LSTM, support vector machine (SVM) and k-nearest neighbors (k-NN)). The experiment results show that the proposed MTDL model is promising to predict short-term taxi demand in multi-zone level, the nonlinear Granger causality analysis is able to capture the spatiotemporal correlations among various traffic zones, and the Bayesian optimization is superior to the other three methods, which verified the feasibility and adaptability of the proposed method.}
}
@article{MALANDRI2021103341,
title = {MEET-LM: A method for embeddings evaluation for taxonomic data in the labour market},
journal = {Computers in Industry},
volume = {124},
pages = {103341},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103341},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305753},
author = {Lorenzo Malandri and Fabio Mercorio and Mario Mezzanzanica and Navid Nobani},
keywords = {Embeddings evaluation, Taxonomies, Semantic hierarchies, Labour market, ICT},
abstract = {Taxonomies are the mainstay of the semantic web as they aim at organising knowledge in concepts linked by IS-A relationships. However, keeping such hierarchies updated and able to represent the domain from which they have been drawn is still a time-consuming, costly and error prone activity. Here, word embeddings have proven to be effective in catching lexicon and semantic similarities to enrich taxonomies from text data. This, in turn, would require to evaluate the generated embeddings to estimate the extent to which they encode the semantic similarity derived from the hierarchy itself. In this paper, we propose and implement MEET-LM, a methodology that aims at generating and evaluating embeddings from a text corpus preserving the co-hyponymy relations synthesised from a domain-specific taxonomy. We apply MEET-LM to a real-life dataset of 2M+ vacancies related to ICT-jobs, framed within the research activities of an EU project that collects millions of Online Job Vacancies and classifies them within the European standard hierarchy ESCO. To show MEET-LM is useful in practice, we also trained a neural network to classify co-hyponym relations using the selected embeddings as features. Our experiments reach 99.4% of accuracy and 86.5% of f1-score.}
}
@article{KRELLENBERG2021126986,
title = {What to do in, and what to expect from, urban green spaces – Indicator-based approach to assess cultural ecosystem services},
journal = {Urban Forestry & Urban Greening},
volume = {59},
pages = {126986},
year = {2021},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2021.126986},
url = {https://www.sciencedirect.com/science/article/pii/S161886672100011X},
author = {Kerstin Krellenberg and Martina Artmann and Celina Stanley and Robert Hecht},
keywords = {Demand and supply, Indicator conceptualization and operationalization, Multi-step assessment approach, Open data, Recreational urban ecosystem services, Site-level},
abstract = {Literature on urban ecosystem services (UESS) is vast, particularly on cultural ecosystem services. However, due to a lack of knowledge on individual urban green spaces on the site level, further research on enhanced methods is needed to underpin existing assumptions about the reasons why people are visiting urban green spaces and what kinds of ecosystem services they can expect, with a focus on recreational activities. We argue for enhanced methods to assess supply of and demand on cultural UESS that should include the direct work with urban green space users. With the overall aim of developing a Spatial Decision Support System for visiting urban green spaces, we are applying a set of different quantitative methods to gather information on peoples’ needs and perceptions as well as data on what existing green spaces offer them. We present a two-step approach 1) linking green space criteria with recreational activities (demand-side) based on a linear series of three online surveys and 2) conducting a spatial mapping of urban green space criteria based on activity-driven indicators (supply-side). In the course of exemplified indicators operationalized by using open and local authorities′ geospatial data in an explorative study in Dresden and Heidelberg (Germany), we discuss the strengths and weaknesses of the approach.}
}
@article{PETERSEN20213594,
title = {CellExplorer: A framework for visualizing and characterizing single neurons},
journal = {Neuron},
volume = {109},
number = {22},
pages = {3594-3608.e2},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321006565},
author = {Peter C. Petersen and Joshua H. Siegle and Nicholas A. Steinmetz and Sara Mahallati and György Buzsáki},
keywords = {electrophysiology, extracellular electrodes, framework, graphical interface, standardized processing and data structure, single cell analysis},
abstract = {Summary
The large diversity of neuron types provides the means by which cortical circuits perform complex operations. Neuron can be described by biophysical and molecular characteristics, afferent inputs, and neuron targets. To quantify, visualize, and standardize those features, we developed the open-source, MATLAB-based framework CellExplorer. It consists of three components: a processing module, a flexible data structure, and a powerful graphical interface. The processing module calculates standardized physiological metrics, performs neuron-type classification, finds putative monosynaptic connections, and saves them to a standardized, yet flexible, machine-readable format. The graphical interface makes it possible to explore the computed features at the speed of a mouse click. The framework allows users to process, curate, and relate their data to a growing public collection of neurons. CellExplorer can link genetically identified cell types to physiological properties of neurons collected across laboratories and potentially lead to interlaboratory standards of single-cell metrics.}
}
@article{LIAO2021127132,
title = {A comparison of global and regional open datasets for urban greenspace mapping},
journal = {Urban Forestry & Urban Greening},
volume = {62},
pages = {127132},
year = {2021},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2021.127132},
url = {https://www.sciencedirect.com/science/article/pii/S1618866721001576},
author = {Yiming Liao and Qi Zhou and Xuanqiao Jing},
keywords = {FROM-GLC10, Land-use, Land-cover, OpenStreetMap, Park, Urban atlas, Vegetation},
abstract = {Greenspace has positive influences on urban environment and human health, and thus it is desirable to acquire data for (urban) greenspace mapping. Nowadays, global and regional open land-use/land-cover datasets have become essential sources for greenspace mapping, but few studies have quantitatively compared them. To fill this gap, this study carries out a quantitative comparison of six global and regional open datasets (CGLS-LC100, CLC, GLC30, UA, FROM-GLC10 and OSM) for greenspace mapping. First of all, the most appropriate land-use/land-cover classes selected as greenspace are analyzed for each open dataset; then, different open datasets are evaluated and compared in terms of five measures (accuracy, precision, recall, F1-score and green coverage rate). Five urban areas in UK are chosen as study areas. Two categories of reference datasets are used for evaluation, including an Ordnance Survey (OS) greenspace dataset in UK and a number of sampling points classified by referring to Google Earth. Results show that: the OSM dataset performs the best, while comparing with the OS dataset (characterized by a narrowly interpreted greenspace); and the FROM-GLC10 dataset performs the best, while comparing with the sampling points (characterized by a broadly interpreted greenspace). Moreover, by using these two open datasets, most quantitative results are close to or higher than 80 %, in terms of the accuracy, precision, recall and F1-score; in most cases there also is the smallest difference between using these two open datasets and corresponding reference datasets, in terms of the green coverage rate. These findings have benefits for researchers and planners to choose an appropriate open dataset for greenspace mapping.}
}