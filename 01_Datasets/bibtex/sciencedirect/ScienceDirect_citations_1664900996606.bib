@article{GOTZ20152188,
title = {On Scalable Data Mining Techniques for Earth Science},
journal = {Procedia Computer Science},
volume = {51},
pages = {2188-2197},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.494},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915013022},
author = {Markus Götz and Matthias Richerzhagen and Christian Bodenstein and Gabriele Cavallaro and Philipp Glock and Morris Riedel and Jón Atli Benediktsson},
keywords = {Data Mining, Machine Learning, HPC, DBSCAN, SVM, MPI},
abstract = {One of the observations made in earth data science is the massive increase of data volume (e.g, higher resolution measurements) and dimensionality (e.g. hyper-spectral bands). Traditional data mining tools (Matlab, R, etc.) are becoming redundant in the analysis of these datasets, as they are unable to process or even load the data. Parallel and scalable techniques, though, bear the potential to overcome these limitations. In this contribution we therefore evaluate said techniques in a High Performance Computing (HPC) environment on the basis of two earth science case studies: (a) Density-based Spatial Clustering of Applications with Noise (DBSCAN) for automated outlier detection and noise reduction in a 3D point cloud and (b) land cover type classification using multi-class Support Vector Machines (SVMs) in multi- spectral satellite images. The paper compares implementations of the algorithms in traditional data mining tools with HPC realizations and ’big data’ technology stacks. Our analysis reveals that a wide variety of them are not yet suited to deal with the coming challenges of data mining tasks in earth sciences.}
}
@article{SKRIPCAK2014303,
title = {Creating a data exchange strategy for radiotherapy research: Towards federated databases and anonymised public datasets},
journal = {Radiotherapy and Oncology},
volume = {113},
number = {3},
pages = {303-309},
year = {2014},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2014.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167814014004071},
author = {Tomas Skripcak and Claus Belka and Walter Bosch and Carsten Brink and Thomas Brunner and Volker Budach and Daniel Büttner and Jürgen Debus and Andre Dekker and Cai Grau and Sarah Gulliford and Coen Hurkmans and Uwe Just and Mechthild Krause and Philippe Lambin and Johannes A. Langendijk and Rolf Lewensohn and Armin Lühr and Philippe Maingon and Michele Masucci and Maximilian Niyazi and Philip Poortmans and Monique Simon and Heinz Schmidberger and Emiliano Spezi and Martin Stuschke and Vincenzo Valentini and Marcel Verheij and Gillian Whitfield and Björn Zackrisson and Daniel Zips and Michael Baumann},
keywords = {Data pooling, Interoperability, Data exchange, Large scale studies, Public data, Radiotherapy},
abstract = {Disconnected cancer research data management and lack of information exchange about planned and ongoing research are complicating the utilisation of internationally collected medical information for improving cancer patient care. Rapidly collecting/pooling data can accelerate translational research in radiation therapy and oncology. The exchange of study data is one of the fundamental principles behind data aggregation and data mining. The possibilities of reproducing the original study results, performing further analyses on existing research data to generate new hypotheses or developing computational models to support medical decisions (e.g. risk/benefit analysis of treatment options) represent just a fraction of the potential benefits of medical data-pooling. Distributed machine learning and knowledge exchange from federated databases can be considered as one beyond other attractive approaches for knowledge generation within “Big Data”. Data interoperability between research institutions should be the major concern behind a wider collaboration. Information captured in electronic patient records (EPRs) and study case report forms (eCRFs), linked together with medical imaging and treatment planning data, are deemed to be fundamental elements for large multi-centre studies in the field of radiation therapy and oncology. To fully utilise the captured medical information, the study data have to be more than just an electronic version of a traditional (un-modifiable) paper CRF. Challenges that have to be addressed are data interoperability, utilisation of standards, data quality and privacy concerns, data ownership, rights to publish, data pooling architecture and storage. This paper discusses a framework for conceptual packages of ideas focused on a strategic development for international research data exchange in the field of radiation therapy and oncology.}
}
@article{REUTER2016545,
title = {Improving Data Consistency in Production Control by Adaptation of Data Mining Algorithms},
journal = {Procedia CIRP},
volume = {56},
pages = {545-550},
year = {2016},
note = {The 9th International Conference on Digital Enterprise Technology – Intelligent Manufacturing in the Knowledge Economy Era},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.10.107},
url = {https://www.sciencedirect.com/science/article/pii/S221282711631099X},
author = {Christina Reuter and Felix Brambring and Jan Weirich and Arne Kleines},
keywords = {Production Planning and Control, Manufacturing Planning and Decision Making, Data Mining},
abstract = {Manufacturing companies are increasingly exposed to volatile market conditions. In this environment, ensuring a reliable adherence to promised delivery dates, allows for a considerable competitive advantage. However, due to dynamically changing production circumstances and high varieties in production programs, manufacturing companies regularly fail in reaching this logistical target. A main prerequisite for mastering this challenge are excellent Production Planning and Control processes. The quality of transactional data of production processes are a commonly ignored root cause for inadequate detailed scheduling plans although a vast volume of these data are used for updating production job statuses and short-term production plans, deriving conclusions for immediate control interventions as well as monitoring production efficiency. Typically, measures for improving data quality involve implementing integrity constraints in databases and setting up data quality processes as well as dedicated organizational structures. Evidently, these classic approaches do not successfully prevent manufacturing companies from dealing with inadequate data quality in their PPC processes. Consequently, this paper presents a model for increasing the quality of data relevant for production processes by adapting data mining algorithms. This new approach allows to estimate probable values for typical data inconsistencies in transactional data of PPC processes. Several adapted algorithms are benchmarked on real-world data sets of German mid-sized manufacturing companies and evaluated towards their power and efficiency.}
}
@article{CHAARI2016260,
title = {Cyber-physical systems clouds: A survey},
journal = {Computer Networks},
volume = {108},
pages = {260-278},
year = {2016},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2016.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1389128616302699},
author = {Rihab Chaâri and Fatma Ellouze and Anis Koubâa and Basit Qureshi and Nuno Pereira and Habib Youssef and Eduardo Tovar},
keywords = {Cloud computing, Cloud robotics, Cloud sensors, Vehicular cloud networks},
abstract = {Cyber-Physical Systems (CPSs) represent systems where computations are tightly coupled with the physical world, meaning that physical data is the core component that drives computation. Industrial automation systems, wireless sensor networks, mobile robots and vehicular networks are just a sample of cyber-physical systems. Typically, CPSs have limited computation and storage capabilities due to their tiny size and being embedded into larger systems. With the emergence of cloud computing and the Internet-of-Things (IoT), there are several new opportunities for these CPSs to extend their capabilities by taking advantage of the cloud resources in different ways. In this survey paper, we present an overview of research efforts on the integration of cyber-physical systems with cloud computing and categorize them into three areas: (1) remote brain, (2) big data manipulation, (3) and virtualization. In particular, we focus on three major CPSs namely mobile robots, wireless sensor networks and vehicular networks.}
}
@article{WATT2016423,
title = {Privacy Matters – Issues within Mechatronics},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {21},
pages = {423-430},
year = {2016},
note = {7th IFAC Symposium on Mechatronic Systems MECHATRONICS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.641},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316322534},
author = {Steve Watt and Chris Milne and David Bradley and David Russell and Peter Hehenberger and Jorge Azorin-Lopez},
keywords = {Privacy, Users, Big Data, Security, Mechatronics, Cyber-Physical Systems, Internet of Things},
abstract = {Abstract:
As mechatronic devices and components become increasingly integrated with and within wider systems concepts such as Cyber-Physical Systems and the Internet of Things, designer engineers are faced with new sets of challenges in areas such as privacy. The paper looks at the current, and potential future, of privacy legislation, regulations and standards and considers how these are likely to impact on the way in which mechatronics is perceived and viewed. The emphasis is not therefore on technical issues, though these are brought into consideration where relevant, but on the soft, or human centred, issues associated with achieving user privacy.}
}
@incollection{BERMAN2013183,
title = {Chapter 13 - Legalities},
editor = {Jules J. Berman},
booktitle = {Principles of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {183-199},
year = {2013},
isbn = {978-0-12-404576-7},
doi = {https://doi.org/10.1016/B978-0-12-404576-7.00013-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045767000137},
author = {Jules J. Berman},
keywords = {Feist Publishing, Inc. v. Rural Telephone Service Co., Data Quality Act, Freedom of Information Act, limited data use agreements, tort, patents, intellectual property, informed consent, data ownership, copyright, infringement, fair use},
abstract = {Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of Big Data as a resource suitable for its intended purposes. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four problems never seem to go away.}
}
@article{YU201633,
title = {Advances and challenges in building engineering and data mining applications for energy-efficient communities},
journal = {Sustainable Cities and Society},
volume = {25},
pages = {33-38},
year = {2016},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2015.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2210670715001067},
author = {Zhun (Jerry) Yu and Fariborz Haghighat and Benjamin C.M. Fung},
keywords = {Data Mining, Building Energy Use, Occupant Behavior, Big Data, Review},
abstract = {The rapidly growing and gigantic body of stored data in the building field, coupled with the need for data analysis, has generated an urgent need for powerful tools that can extract hidden but useful knowledge of building performance improvement from large data sets. As an emerging subfield of computer science, data mining technologies suit this need well and have been proposed for relevant knowledge discovery in the past several years. Aimed to highlight recent advances, this paper provides an overview of the studies undertaking the two main data mining tasks (i.e. predictive tasks and descriptive tasks) in the building field. Based on the overview, major challenges and future research trends are also discussed.}
}
@incollection{HUGHES2016293,
title = {Chapter 13 - Surface Solutions Using Data Virtualization and Big Data},
editor = {Ralph Hughes},
booktitle = {Agile Data Warehousing for the Enterprise},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {293-327},
year = {2016},
isbn = {978-0-12-396464-9},
doi = {https://doi.org/10.1016/B978-0-12-396464-9.00013-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964649000138},
author = {Ralph Hughes},
keywords = {Agile enterprise data warehousing, surface solutions, backfilling the architecture, shadow IT, data virtualization, big data, Hadoop, HDFS, Map/Reduce, Hive},
abstract = {Without investing in exotic data modeling techniques, EDW teams can achieve fast delivery using “surface solutions.” Surface solutions allow developers to first solve business problems with data taken from landing areas and then steadily “backfill” the DW/BI reference architecture to provide progressively more complete and robust solutions. Teams can create surface solutions by leveraging shadow IT, using data virtualization, and tapping a big data platform. When leveraging shadow IT, the EDW team delivers progressively richer data sets to departmental staff members, who build their own temporary BI solutions using that information. The data virtualization strategy relies on a “superoptimizer” that can create views across databases and data types, even including semistructured data as needed. The big data strategy employs a new category of products such as Hadoop’s HDFS, MapReduce, and Hive to provide access to new data, whether it be very large, poorly structured, and/or just unfamiliar to IT and the business users.}
}
@article{FUCHS2014198,
title = {Big data analytics for knowledge generation in tourism destinations – A case from Sweden},
journal = {Journal of Destination Marketing & Management},
volume = {3},
number = {4},
pages = {198-209},
year = {2014},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X14000353},
author = {Matthias Fuchs and Wolfram Höpken and Maria Lexhagen},
keywords = {Big data analytics, Tourism destination, Destination management information system, Business intelligence, Data mining, Online Analytical Processing (OLAP)},
abstract = {This paper presents a knowledge infrastructure which has recently been implemented as a genuine novelty at the leading Swedish mountain tourism destination, Åre. By applying a Business Intelligence approach, the Destination Management Information System Åre (DMIS-Åre) drives knowledge creation and application as a precondition for organizational learning at tourism destinations. Schianetz, Kavanagh, and Lockington’s (2007) concept of the ‘Learning Tourism Destination’ and the ‘Knowledge Destination Framework’ introduced by Höpken, Fuchs, Keil, and Lexhagen (2011) build the theoretical fundament for the technical architecture of the presented Business Intelligence application. After having introduced the development process of indicators measuring destination performance as well as customer behaviour and experience, the paper highlights how DMIS-Åre can be used by tourism managers to gain new knowledge about customer-based destination processes focused on pre- and post-travel phases, like “Web-Navigation”, “Booking” and “Feedback”. After a concluding discussion about the various components building the prototypically implemented BI-based DMIS infrastructure with data from destination stakeholders, the agenda of future research is sketched. The agenda considers, for instance, the application of real-time Business Intelligence to gain real-time knowledge on tourists’ on-site behaviour at tourism destinations.}
}
@incollection{BERANGER2016167,
title = {3 - Management and Governance of Personal Health Data},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {167-236},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500038},
author = {Jérôme Béranger},
keywords = {Controlled regulation, Data lifecycle, Environmental digital ecosystem, Governance, Management, Quality control, Regulatory and organizational aspects, Relational and cultural aspects, Strategic and methodological aspects, Structural and technological aspects},
abstract = {Abstract:
Every company has its own culture, its organization, its governance mode and its project management models. Nevertheless, a number of significant and universal principles concerning governance can be identified, both at the approach level as well as that of actors and of responsibilities. Data governance is one of the key factors to success in the protection of information. It is one of the components that defines the rules, guides and charters of good practice, establishes references and policies (management, classification, storage, and conservation of personal data), and further describes the responsibilities and controls their application. Therefore, it becomes paramount to understand: how can the complexity around personal data management be apprehended and in particular in health fields? In addition, what are the possible mechanisms to process these data pools to turn them into consistent and relevant information? To this end, it is essential to have a detailed and an accurate understanding of algorithmic governance, of the environmental numerical ecosystem, of safety and protection, and of the lifecycle of Big Data.}
}
@incollection{MILOSEVIC201639,
title = {Chapter 2 - Real-Time Analytics},
editor = {Rajkumar Buyya and Rodrigo N. Calheiros and Amir Vahid Dastjerdi},
booktitle = {Big Data},
publisher = {Morgan Kaufmann},
pages = {39-61},
year = {2016},
isbn = {978-0-12-805394-2},
doi = {https://doi.org/10.1016/B978-0-12-805394-2.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053942000027},
author = {Z. Milosevic and W. Chen and A. Berry and F.A. Rabhi},
keywords = {Real-time analytics, Complex event processing, Streaming, Event processing, Advanced analytics, Data analysis, Machine learning, Finance, EventSwarm},
abstract = {Real-time analytics is a special kind of Big Data analytics in which data elements are required to be processed and analyzed as they arrive in real time. It is important in situations where real-time processing and analysis can deliver important insights and yield business value. This chapter provides an overview of current processing and analytics platforms needed to support such analysis, as well as analytics techniques that can be applied in such environments. The chapter looks beyond traditional event processing system technology to consider a broader big data context that involves “data at rest” platforms and solutions. The chapter includes a case study showing the use of EventSwarm complex event processing engine for a class of analytics problems in finance. The chapter concludes with several research challenges, such as the need for new approaches and algorithms required to support real-time data filtering, data exploration, statistical data analysis, and machine learning.}
}
@article{UCHIHIRA2015173,
title = {Service innovation structure analysis for recognizing opportunities and difficulties of M2M businesses},
journal = {Technology in Society},
volume = {43},
pages = {173-182},
year = {2015},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X15000731},
author = {Naoshi Uchihira and Hirokazu Ishimatsu and Shigeaki Sakurai and Yoshiteru Kageyama and Yuji Kakutani and Kazunori Mizushima and Hiroshi Naruse and Susumu Yoneda},
keywords = {Machine-to-machine service, Business model, Big data, Backcasting},
abstract = {With the popularization of high-speed and high-capacity communication infrastructure, Machine-to-Machine (M2M) communication has received significant attention. However, even though the related technologies have been actively investigated, creating new businesses based on M2M communication is not easy. This study proposes a service innovation structure that visualizes the opportunities and difficulties of M2M service businesses. In our proposal, opportunities are classified as two types of value proposition (optimization and identification values) using the Sharing-Connecting-Analyzing-Identifying (SCAI) model. In addition, difficulties are discussed using a fishbone diagram. The SCAI model pays particular attention to the identification value, which tends to be ignored in other models. Opportunities and difficulties are structured as a map according to backcasting from a desired future M2M infrastructure. The backcasting approach is effective to untangle the intertwined difficulties. Using this opportunity-difficulty map, we can discuss and model M2M service businesses more clearly and strategically by recognizing the opportunities and the difficulties with stakeholders. A smart home case is used for explaining the effectiveness of our proposed model.}
}
@article{CHRISTY2015209,
title = {Cluster Based Outlier Detection Algorithm for Healthcare Data},
journal = {Procedia Computer Science},
volume = {50},
pages = {209-215},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915005591},
author = {A. Christy and G. Meera Gandhi and S. Vaithyasubramanian},
keywords = {Cluster-Based, Outlier Detection, Outlier Score, F-Measure, Likelihood Ratio, etc.},
abstract = {Outliers has been studied in a variety of domains including Big Data, High dimensional data, Uncertain data, Time Series data, Biological data, etc. In majority of the sample datasets available in the repository, atleast 10% of the data may be erroneous, missing or not available. In this paper, we utilize the concept of data preprocessing for outlier reduction. We propose two algorithms namely Distance-Based outlier detection and Cluster-Based outlier algorithm for detecting and removing outliers using a outlier score. By cleaning the dataset and clustering based on similarity, we can remove outliers on the key attribute subset rather than on the full dimensional attributes of dataset. Experiments were conducted using 3 built-in Health care dataset available in R package and the results show that the cluster-based outlier detection algorithm providing better accuracy than distance based outlier detection algorithm.}
}
@article{MILLER2013296,
title = {Beyond sharing: cultivating cooperative transportation systems through geographic information science},
journal = {Journal of Transport Geography},
volume = {31},
pages = {296-308},
year = {2013},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2013.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0966692313000719},
author = {Harvey J. Miller},
keywords = {Transportation, Cooperation, Game theory, Big Data},
abstract = {Transportation systems are facing unprecedented challenges in the 21st century. Increasing the efficiency of transportation systems alone will not solve these problems and may exacerbate them. Instead, we must extract new transportation capabilities related to more cooperative decision-making across a wide range of time horizons, spatial scales and decision contexts. This paper discusses the role of sensed transportation, geographic information science and social media to cultivate transportation systems where participants share, cooperate and act collectively to solve operational, tactical and strategic mobility and accessibility problems. This paper also provides a vision of the future by imaging a seamless multimodal transportation system combined with a virtual environment where data streams are fused, interpreted and made available with tools for human engagement and shared decision making. This paper concludes by outlining a GIScience-centric research agenda.}
}
@article{WANG2015413,
title = {What is the level of volatility in instantaneous driving decisions?},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {58},
pages = {413-427},
year = {2015},
note = {Big Data in Transportation and Traffic Engineering},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2014.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15000029},
author = {Xin Wang and Asad J. Khattak and Jun Liu and Golnush Masghati-Amoli and Sanghoon Son},
keywords = {Instantaneous driving decision, Big data, Volatility, Acceleration, Speed},
abstract = {Driving styles can be broadly characterized as calm or volatile, with significant implications for traffic safety, energy consumption and emissions. How to quantify the extent of calm or volatile driving and explore its correlates is a key research question investigated in the study. This study contributes by leveraging a large-scale behavioral database to analyze short-term driving decisions and develop a new driver volatility index to measure the extent of variations in driving. The index captures variation in driving behavior constrained by the performance of the vehicle from a decision-making perspective. Specifically, instantaneous driving decisions include maintaining speed, accelerating, decelerating, maintaining acceleration/deceleration, or jerks to vehicle, i.e., the decision to change marginal rate of acceleration or deceleration. A fundamental understanding of instantaneous driving behavior is developed by categorizing vehicular jerk reversals (acceleration followed by deceleration), jerk enhancements (increasing accelerations or decelerations), and jerk mitigations (decreasing accelerations or decelerations). Volatility in driving decisions, captured by jerky movements, is quantified using data collected in Atlanta, GA during 2011. The database contains 51,370 trips and their associated second-by-second speed data, totaling 36 million seconds. Rigorous statistical models explore correlates of volatility that include socioeconomic variables, travel context variables, and vehicle types. The study contributes by proposing a framework that is based on defining instantaneous driving decisions in a quantifiable way using big data generated by in-vehicle GPS devices and behavioral surveys.}
}
@article{GUNDLA2016460,
title = {Creating NoSQL Biological Databases with Ontologies for Query Relaxation},
journal = {Procedia Computer Science},
volume = {91},
pages = {460-469},
year = {2016},
note = {Promoting Business Analytics and Quantitative Management of Technology: 4th International Conference on Information Technology and Quantitative Management (ITQM 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.120},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916313138},
author = {Naresh Kumar Gundla and Zhengxin Chen},
keywords = {NoSQL databases, Query Relaxation, Ontology, MongoDB, AllgroGraph},
abstract = {The complexity of building biological databases is well-known and ontologies play an extremely important role in biological databases. However, much of the emphasis on the role of ontologies in biological databases has been on the construction of databases. In this paper, we explore a somewhat overlooked aspect regarding ontologies in biological databases, namely, how ontologies can be used to assist better database retrieval. In particular, we show how ontologies can be used to revise user submitted queries for query relaxation. In addition, since our research is conducted at today's “big data” era, our investigation is centered on NoSQL databases which serve as a kind of “representatives” of big data. This paper contains two major parts: First we describe our methodology of building two NoSQL application databases (MongoDB and AllegroGraph) using GO ontology, and then discuss how to achieve query relaxation through GO ontology. We report our experiments and show sample queries and results. Our research on query relaxation on NoSQL databases is complementary to existing work in big data and in biological databases and deserves further exploration.}
}
@article{YANG20153,
title = {Emerging information technologies for enhanced healthcare},
journal = {Computers in Industry},
volume = {69},
pages = {3-11},
year = {2015},
note = {Special Issue: Information Technologies for Enhanced Healthcare},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2015.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0166361515000226},
author = {Ji-Jiang Yang and Jianqiang Li and Jacob Mulder and Yongcai Wang and Shi Chen and Hong Wu and Qing Wang and Hui Pan},
keywords = {Healthcare, Health sensing, Big data analysis, Cloud computing, Empowerment},
abstract = {The appropriate collection and consumption of electronic health information about an individual patient or population is the bedrock of modern healthcare, where electronic medical records (EMR) serve as the main carrier. This paper first introduces the main goal of this special issue and gives a brief guideline. Then, the present situation of the adoption of EMRs is reviewed. After that, the emerging information technologies are presented which have a great impact on the healthcare provision. These include health sensing for medical data collection, medical data analysis and utilization for accurate detection and prediction. Next, cloud computing is discussed, as it may provide scalable and cost-effective delivery of healthcare services. Accordingly, the current state of academic research is documented on emerging information technologies for new paradigms of healthcare service. At last, conclusions are made.}
}
@article{HUARD201518,
title = {The data quality paradox},
journal = {Network Security},
volume = {2015},
number = {6},
pages = {18-20},
year = {2015},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(15)30051-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485815300519},
author = {Boris Huard},
abstract = {After its people, data is arguably an organisation's most valuable asset. According to recent figures by the Networked Systems and Services department at SINTEF, some 90% of all the data in the world has been created in the past two years. That shouldn't be too much of a surprise to us given the data-driven world we now live in; but what is promising is that companies are increasingly switching on to its strategic and commercial value. The challenge is how do we extract value from this data in a way that empowers people and organisations alike?}
}
@article{BLASCH20141299,
title = {Static Versus Dynamic Data Information Fusion Analysis Using DDDAS for Cyber Security Trust},
journal = {Procedia Computer Science},
volume = {29},
pages = {1299-1313},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.117},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914002944},
author = {Erik Blasch and Youssif Al-Nashif and Salim Hariri},
abstract = {Information fusion includes signals, features, and decision -level analysis over various types of data including imagery, te xt, and cyber security detection. With the maturity of data processing, the e xplosion of big data, and the need fo r user acceptance; the Dynamic Data-Driven Application System (DDDAS) philosophy fosters insights into the usability of information systems solutions. In this paper, we e xp lore a notion of an adaptive adjustment of secure communication trust analysis that seeks a balance between standard static solutions versus dynamic -data driven updates. A use case is provided in determin ing trust for a cyber security scenario exp loring comparisons of Bayesian versus evidential reasoning for dynamic security detection updates. Using the evidential reasoning proportional conflict redistribution (PCR) method, we demonstrate improved trust for dynamically changing detections of denial of service attacks.}
}
@article{ZHOU2016212,
title = {Energy Internet: The business perspective},
journal = {Applied Energy},
volume = {178},
pages = {212-222},
year = {2016},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2016.06.052},
url = {https://www.sciencedirect.com/science/article/pii/S0306261916308273},
author = {Kaile Zhou and Shanlin Yang and Zhen Shao},
keywords = {Energy Internet, Business perspective, Business values, Service models, Research agendas},
abstract = {Energy Internet is a new development form of energy system. It realizes the integration of energy flow, information flow and business flow. More and more business model and service model innovations are stimulated in Energy Internet. In this paper, we present a systemic study of Energy Internet from the business perspective. We first propose the evolution stages of energy systems. Since they were invented in the second industrial revolution period, energy systems have mainly experienced four stages, i.e., decentralized energy system, centralized energy system, distributed energy system and smart & connected energy system. Energy Internet is the innovative representation of energy systems in the fourth development stage. We also introduce some key concepts in Energy Internet, including prosumer, microgrid, Virtual Power Plant (VPP), smart grid and smart energy. Then the business values of Energy Internet are discussed from the energy big data analytics perspective. Finally, the business research agendas of Energy Internet are pointed out from five aspects, i.e., strategic issues, data issues, behavioral issues, security issues and regulatory issues.}
}
@article{KARIM2016214,
title = {Maintenance Analytics – The New Know in Maintenance},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {28},
pages = {214-219},
year = {2016},
note = {3rd IFAC Workshop on Advanced Maintenance Engineering, Services and Technology AMEST 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316324612},
author = {Ramin Karim and Jesper Westerberg and Diego Galar and Uday Kumar},
keywords = {big data, maintenance analytics, eMaintenance, Knowledge discovery, maintenance decision support},
abstract = {Abstract:
Decision-making in maintenance has to be augmented to instantly understand and efficiently act, i.e. the new know. The new know in maintenance needs to focus on two aspects of knowing: 1) what can be known and 2) what must be known, in order to enable the maintenance decision-makers to take appropriate actions. Hence, the purpose of this paper is to propose a concept for knowledge discovery in maintenance with focus on Big Data and analytics. The concept is called Maintenance Analytics (MA). MA focuses in the new knowledge discovery in maintenance. MA addresses the process of discovery, understanding, and communication of maintenance data from four time-related perspectives, i.e. 1) “Maintenance Descriptive Analytics (monitoring)”; 2) “Maintenance Diagnostic Analytics”; 3) “Maintenance Predictive Analytics”; and 4) “Maintenance Prescriptive analytics”.}
}
@incollection{ONUKWUGHA2016207,
title = {Chapter 11 - Data Visualization Tools for Investigating Health Services Utilization Among Cancer Patients},
editor = {Bradford W. Hesse and David K. Ahern and Ellen Beckjord},
booktitle = {Oncology Informatics},
publisher = {Academic Press},
address = {Boston},
pages = {207-229},
year = {2016},
isbn = {978-0-12-802115-6},
doi = {https://doi.org/10.1016/B978-0-12-802115-6.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021156000112},
author = {Eberechukwu Onukwugha and Catherine Plaisant and Ben Shneiderman},
keywords = {Data visualization, big data, human-system integration, human–computer interaction},
abstract = {The era of “big data” promises more information for health practitioners, patients, researchers, and policy makers. For big data resources to be more than larger haystacks in which to find precious needles, stakeholders will have to aim higher than increasing computing power and producing faster, nimbler machines. We will have to develop tools for visualizing information; generating insight; and creating actionable, on-demand knowledge for clinical decision making. This chapter has three objectives: (1) to review the data visualization tools that are currently available and their use in oncology; (2) to discuss implications for research, practice, and decision making in oncology; and (3) to illustrate the possibilities for generating insight and actionable evidence using targeted case studies. A few innovative applications of data visualization are available from the clinical and research settings. We highlight some of these applications and discuss the implications for evidence generation and clinical practice. In addition, we develop two case studies to illustrate the possibilities for generating insight from the strategic application of data visualization tools where the interoperability problem is solved. Using linked cancer registry and Medicare claims data available from the National Cancer Institute, we illustrate how data visualization tools unlock insights from temporal event sequences represented in large, population-based data sets. We show that the information gained from the application of visualization tools such as EventFlow can define questions, refine measures, and formulate testable hypotheses for the investigation of cancer-related clinical and process outcomes.}
}
@incollection{BERANGER201697,
title = {2 - Ethical Development of the Medical Datasphere},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {97-166},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500026},
author = {Jérôme Béranger},
keywords = {Algorithmic ethics, Architecture, Complex data, Ethical data mining, Ethical issues, Ethical-technical guidance, Evaluation, Medical datasphere, Neo-Platonic modeling},
abstract = {Abstract:
As Lucy Suchmann observed in 2011, through Lévi-Strauss, “…we are our tools…” and our personal health data are an integral part of us. In these circumstances, it becomes necessary to question the value of Big Data in the health sphere.}
}
@incollection{HUANG2016297,
title = {Chapter 15 - Usage of Social Media and Cloud Computing During Natural Hazards},
editor = {Tiffany C. Vance and Nazila Merati and Chaowei Yang and May Yuan},
booktitle = {Cloud Computing in Ocean and Atmospheric Sciences},
publisher = {Academic Press},
pages = {297-324},
year = {2016},
isbn = {978-0-12-803192-6},
doi = {https://doi.org/10.1016/B978-0-12-803192-6.00015-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128031926000153},
author = {Q. Huang and G. Cervone},
keywords = {Cloud computing, Data mining, Disaster coordination, Disaster management, Disaster relief, Social media},
abstract = {Social media data have emerged as new sources for detecting and monitoring disaster events. Several recent studies have suggested that social media data streams can be used to mine actionable data for emergency response and relief operations. Such massive and rapidly changing data present several new grand challenges to archive and extract critical validated information for various disaster management activities. The volume, velocity, and variety of the data require advanced computing infrastructure technologies for big data management. Cloud computing is proposed as the ideal platform to address these computing challenges. This chapter discusses the opportunities and challenges associated with using social media to gain situational awareness during disasters and the feasibility of using cloud computing to build a resilient and real-time disaster management system. Several real case studies and applications are presented to illustrate how both social media and cloud-computing solutions are used for various natural hazards.}
}
@incollection{MCCUE201575,
title = {Chapter 5 - Data},
editor = {Colleen McCue},
booktitle = {Data Mining and Predictive Analysis (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
address = {Boston},
pages = {75-106},
year = {2015},
isbn = {978-0-12-800229-2},
doi = {https://doi.org/10.1016/B978-0-12-800229-2.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002292000055},
author = {Colleen McCue},
keywords = {Data, “Big Data”, volume, velocity, variety, “INTs”, relational data, accuracy, reliability, validity},
abstract = {It is important for the analyst to know their data, including numeric attributes and context, as well as potential limitations and constraints. Data can be obtained from a variety of sources including formal databases created specifically for the analysis of crime and intelligence data, ad hoc or user-generated databases, and nontraditional sources. The concept of “Big Data” has been introduced recently and relates to volume, velocity, and variety of data. Potential data challenges including accuracy, reliability, and validity are discussed.}
}
@incollection{TSAFOU2016245,
title = {Integrative Systems Biology},
editor = {Ralph A. Bradshaw and Philip D. Stahl},
booktitle = {Encyclopedia of Cell Biology},
publisher = {Academic Press},
address = {Waltham},
pages = {245-251},
year = {2016},
isbn = {978-0-12-394796-3},
doi = {https://doi.org/10.1016/B978-0-12-394447-4.40042-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123944474400428},
author = {K. Tsafou and L.J. Jensen},
keywords = {Controlled vocabulary, Data integration, Data standardization, Databases, Nomenclature, Ontologies, Open licenses, Prediction methods, Systems biology, Text-mining},
abstract = {The heart of integrative systems biology is to combine heterogeneous, high-dimensional data from a diverse range of resources. The aim of doing so is to make efficient use of big data to improve our understanding of biological systems. However, various barriers are in the way of doing so, which relate to the collection, standardization, quality, completeness, and licenses of the data. In this article, we discuss these challenges and the main approaches and existing efforts to overcome them.}
}
@article{AUFAURE2016100,
title = {From Business Intelligence to semantic data stream management},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {100-107},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003635},
author = {Marie-Aude Aufaure and Raja Chiky and Olivier Curé and Houda Khrouf and Gabriel Kepeklian},
keywords = {Data stream, Linked Data, Business Intelligence, Stream reasoning},
abstract = {The Semantic Web technologies are being increasingly used for exploiting relations between data. In addition, new tendencies of real-time systems, such as social networks, sensors, cameras or weather information, are continuously generating data. This implies that data and links between them are becoming extremely vast. Such huge quantity of data needs to be analyzed, processed, as well as stored if necessary. In this position paper, we will introduce recent work on Real-Time Business Intelligence combined with semantic data stream management. We will present underlying approaches such as continuous queries, data summarization and matching, and stream reasoning.}
}
@article{ENDEL2015111,
title = {Data Wrangling: Making data useful again},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {1},
pages = {111-112},
year = {2015},
note = {8th Vienna International Conferenceon Mathematical Modelling},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.05.197},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315001986},
author = {Florian Endel and Harald Piringer},
keywords = {Data acquisition, Databases, Bad data identification, Data wrangling},
abstract = {Data analysis has become an everyday business and advancements of data management routines open up new opportunities. Nevertheless, transforming and assembling newly acquired data into a suitable form remains tedious. It is often stated, that data cleaning is a critical part of the overall process, but also consumes sublime amounts of time and resources. Data Wrangling is not only about transforming and cleaning procedures. Many other aspects like data quality, merging of different sources, reproducible processes, and managing data provenance have to be considered. Although various tools designed for specific tasks are available, software solutions accompanying the whole process are still rare. In this paper, some aspects of this first phase of most data driven projects, also known as data wrangling, data munging or janitorial work are described. Beginning with an overview on the topic and current problems, concrete common tasks as well as selected software solutions and techniques are discussed.}
}
@incollection{GUDIVADA2016169,
title = {Chapter 5 - Cognitive Analytics: Going Beyond Big Data Analytics and Machine Learning},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {169-205},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169716116300517},
author = {V.N. Gudivada and M.T. Irfan and E. Fathi and D.L. Rao},
keywords = {Cognitive analytics, Text analytics, Learning analytics, Educational data mining, Cognitive systems, Cognitive computing, Personalized learning, Data science, Machine learning, Big data analytics, Business analytics},
abstract = {This chapter defines analytics and traces its evolution from its origin in 1988 to its current stage—cognitive analytics. We discuss types of learning and describe classes of machine learning algorithms. Given this backdrop, we propose a reference architecture for cognitive analytics and indicate ways to implement the architecture. A few cognitive analytics applications are briefly described. The chapter concludes by indicating current trends and future research direction.}
}
@article{KARKOUCH201657,
title = {Data quality in internet of things: A state-of-the-art survey},
journal = {Journal of Network and Computer Applications},
volume = {73},
pages = {57-81},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301564},
author = {Aimad Karkouch and Hajar Mousannif and Hassan {Al Moatassime} and Thomas Noel},
keywords = {Internet of things, Data quality, Data cleaning, Outlier detection},
abstract = {In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.}
}
@article{CHOUVARDA201522,
title = {Connected health and integrated care: Toward new models for chronic disease management},
journal = {Maturitas},
volume = {82},
number = {1},
pages = {22-27},
year = {2015},
note = {PERSONALIZED HEALTHCARE FOR MIDLIFE AND BEYOND},
issn = {0378-5122},
doi = {https://doi.org/10.1016/j.maturitas.2015.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0378512215006052},
author = {Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras},
keywords = {Connected health, Integrated care, Personal health system, Electronic health},
abstract = {The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.}
}
@article{GUETA2016139,
title = {Quantifying the value of user-level data cleaning for big data: A case study using mammal distribution models},
journal = {Ecological Informatics},
volume = {34},
pages = {139-145},
year = {2016},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1574954116300577},
author = {Tomer Gueta and Yohay Carmel},
keywords = {Biodiversity informatics, Data-cleaning, SDM performance, MaxEnt, Australian mammals, Big-data},
abstract = {The recent availability of species occurrence data from numerous sources, standardized and connected within a single portal, has the potential to answer fundamental ecological questions. These aggregated big biodiversity databases are prone to numerous data errors and biases. The data-user is responsible for identifying these errors and assessing if the data are suitable for a given purpose. Complex technical skills are increasingly required for handling and cleaning biodiversity data, while biodiversity scientists possessing these skills are rare. Here, we estimate the effect of user-level data cleaning on species distribution model (SDM) performance. We implement several simple and easy-to-execute data cleaning procedures, and evaluate the change in SDM performance. Additionally, we examine if a certain group of species is more sensitive to the use of erroneous or unsuitable data. The cleaning procedures used in this research improved SDM performance significantly, across all scales and for all performance measures. The largest improvement in distribution models following data cleaning was for small mammals (1g–100g). Data cleaning at the user level is crucial when using aggregated occurrence data, and facilitating its implementation is a key factor in order to advance data-intensive biodiversity studies. Adopting a more comprehensive approach for incorporating data cleaning as part of data analysis, will not only improve the quality of biodiversity data, but will also impose a more appropriate usage of such data.}
}
@article{DONG2015278,
title = {Traffic zone division based on big data from mobile phone base stations},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {58},
pages = {278-291},
year = {2015},
note = {Big Data in Transportation and Traffic Engineering},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2015.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15002223},
author = {Honghui Dong and Mingchao Wu and Xiaoqing Ding and Lianyu Chu and Limin Jia and Yong Qin and Xuesong Zhou},
keywords = {Mobile telephones, Call detail record (CDR) data, Traffic semantic analysis, Traffic zone division, Traffic zone attribute index, Travel patterns},
abstract = {Call detail record (CDR) data from mobile communication carriers offer an emerging and promising source of information for analysis of traffic problems. To date, research on insights and information to be gleaned from CDR data for transportation analysis has been slow, and there has been little progress on development of specific applications. This paper proposes the traffic semantic concept to extract traffic commuters’ origins and destinations information from the mobile phone CDR data and then use the extracted data for traffic zone division. A K-means clustering method was used to classify a cell-area (the area covered by a base stations) and tag a certain land use category or traffic semantic attribute (such as working, residential, or urban road) based on four feature data (including real-time user volume, inflow, outflow, and incremental flow) extracted from the CDR data. By combining the geographic information of mobile phone base stations, the roadway network within Beijing’s Sixth Ring Road was divided into a total of 73 traffic zones using another K-means clustering algorithm. Additionally, we proposed a traffic zone attribute-index to measure tendency of traffic zones to be residential or working. The calculated attribute-index values of 73 traffic zones in Beijing were consistent with the actual traffic and land-use data. The case study demonstrates that effective traffic and travel data can be obtained from mobile phones as portable sensors and base stations as fixed sensors, providing an opportunity to improve the analysis of complex travel patterns and behaviors for travel demand modeling and transportation planning.}
}
@article{STEVENS201515,
title = {Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems},
journal = {Spatial and Spatio-temporal Epidemiology},
volume = {13},
pages = {15-29},
year = {2015},
issn = {1877-5845},
doi = {https://doi.org/10.1016/j.sste.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877584515000179},
author = {Kim B. Stevens and Dirk U. Pfeiffer},
keywords = {Big data, Data warehouse, Google Earth, mHealth, Spatial data, Volunteered geographic information},
abstract = {During the last 30years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively.}
}
@article{ROALF2016903,
title = {The impact of quality assurance assessment on diffusion tensor imaging outcomes in a large-scale population-based cohort},
journal = {NeuroImage},
volume = {125},
pages = {903-919},
year = {2016},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.10.068},
url = {https://www.sciencedirect.com/science/article/pii/S1053811915009854},
author = {David R. Roalf and Megan Quarmley and Mark A. Elliott and Theodore D. Satterthwaite and Simon N. Vandekar and Kosha Ruparel and Efstathios D. Gennatas and Monica E. Calkins and Tyler M. Moore and Ryan Hopson and Karthik Prabhakaran and Chad T. Jackson and Ragini Verma and Hakon Hakonarson and Ruben C. Gur and Raquel E. Gur},
keywords = {Diffusion tensor imaging, Automated quality assurance, Motion, Adolescence, Brain maturation},
abstract = {Background
Diffusion tensor imaging (DTI) is applied in investigation of brain biomarkers for neurodevelopmental and neurodegenerative disorders. However, the quality of DTI measurements, like other neuroimaging techniques, is susceptible to several confounding factors (e.g., motion, eddy currents), which have only recently come under scrutiny. These confounds are especially relevant in adolescent samples where data quality may be compromised in ways that confound interpretation of maturation parameters. The current study aims to leverage DTI data from the Philadelphia Neurodevelopmental Cohort (PNC), a sample of 1601 youths with ages of 8–21 who underwent neuroimaging, to: 1) establish quality assurance (QA) metrics for the automatic identification of poor DTI image quality; 2) examine the performance of these QA measures in an external validation sample; 3) document the influence of data quality on developmental patterns of typical DTI metrics.
Methods
All diffusion-weighted images were acquired on the same scanner. Visual QA was performed on all subjects completing DTI; images were manually categorized as Poor, Good, or Excellent. Four image quality metrics were automatically computed and used to predict manual QA status: Mean voxel intensity outlier count (MEANVOX), Maximum voxel intensity outlier count (MAXVOX), mean relative motion (MOTION) and temporal signal-to-noise ratio (TSNR). Classification accuracy for each metric was calculated as the area under the receiver-operating characteristic curve (AUC). A threshold was generated for each measure that best differentiated visual QA status and applied in a validation sample. The effects of data quality on sensitivity to expected age effects in this developmental sample were then investigated using the traditional MRI diffusion metrics: fractional anisotropy (FA) and mean diffusivity (MD). Finally, our method of QA is compared with DTIPrep.
Results
TSNR (AUC=0.94) best differentiated Poor data from Good and Excellent data. MAXVOX (AUC=0.88) best differentiated Good from Excellent DTI data. At the optimal threshold, 88% of Poor data and 91% Good/Excellent data were correctly identified. Use of these thresholds on a validation dataset (n=374) indicated high accuracy. In the validation sample 83% of Poor data and 94% of Excellent data was identified using thresholds derived from the training sample. Both FA and MD were affected by the inclusion of poor data in an analysis of an age, sex and race matched comparison sample. In addition, we show that the inclusion of poor data results in significant attenuation of the correlation between diffusion metrics (FA and MD) and age during a critical neurodevelopmental period. We find higher correspondence between our QA method and DTIPrep for Poor data, but we find our method to be more robust for apparently high-quality images.
Conclusion
Automated QA of DTI can facilitate large-scale, high-throughput quality assurance by reliably identifying both scanner and subject induced imaging artifacts. The results present a practical example of the confounding effects of artifacts on DTI analysis in a large population-based sample, and suggest that estimates of data quality should not only be reported but also accounted for in data analysis, especially in studies of development.}
}
@article{HURTER2014207,
title = {Interactive image-based information visualization for aircraft trajectory analysis},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {47},
pages = {207-227},
year = {2014},
note = {Special Issue: Emerging Technologies Special Issue of ICTIS 2013 – Guest Editors: Liping Fu and Ming Zhong and Special Issue: Visualization & Visual Analytics in Transportation – Guest Editors: Patricia S. Hu and Michael L. Pack},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2014.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X14000710},
author = {C. Hurter and S. Conversy and D. Gianazza and A.C. Telea},
keywords = {Trajectory manipulation, Air traffic control, Image based techniques, Information visualization},
abstract = {Objectives: The objective of the presented work is to present novel methods for big data exploration in the Air Traffic Control (ATC) domain. Data is formed by sets of airplane trajectories, or trails, which in turn records the positions of an aircraft in a given airspace at several time instants, and additional information such as flight height, speed, fuel consumption, and metadata (e.g. flight ID). Analyzing and understanding this time-dependent data poses several non-trivial challenges to information visualization. Materials and methods: To address this Big Data challenge, we present a set of novel methods to analyze aircraft trajectories with interactive image-based information visualization techniques.As a result, we address the scalability challenges in terms of data manipulation and open questions by presenting a set of related visual analysis methods that focus on decision-support in the ATC domain. All methods use image-based techniques, in order to outline the advantages of such techniques in our application context, and illustrated by means of use-cases from the ATC domain. Results: For each considered use-case, we outline the type of questions posed by domain experts, data involved in addressing these questions, and describe the specific image-based techniques we used to address these questions. Further, for each of the proposed techniques, we describe the visual representation and interaction mechanisms that have been used to address the above-mentioned goals. We illustrate these use-cases with real-life datasets from the ATC domain, and show how our techniques can help end-users in the ATC domain discover new insights, and solve problems, involving the presented datasets.}
}
@article{MATE2016131,
title = {A hybrid integrated architecture for energy consumption prediction},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {131-147},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16300644},
author = {Alejandro Maté and Jesús Peral and Antonio Ferrández and David Gil and Juan Trujillo},
keywords = {Data mining, Energy consumption, Information Extraction, Big data, Decision trees, Social networks},
abstract = {Irresponsible and negligent use of natural resources in the last five decades has made it an important priority to adopt more intelligent ways of managing existing resources, especially the ones related to energy. The main objective of this paper is to explore the opportunities of integrating internal data already stored in Data Warehouses together with external Big Data to improve energy consumption predictions. This paper presents a study in which we propose an architecture that makes use of already stored energy data and external unstructured information to improve knowledge acquisition and allow managers to make better decisions. This external knowledge is represented by a torrent of information that, in many cases, is hidden across heterogeneous and unstructured data sources, which are recuperated by an Information Extraction system. Alternatively, it is present in social networks expressed as user opinions. Furthermore, our approach applies data mining techniques to exploit the already integrated data. Our approach has been applied to a real case study and shows promising results. The experiments carried out in this work are twofold: (i) using and comparing diverse Artificial Intelligence methods, and (ii) validating our approach with data sources integration.}
}
@incollection{TALBURT2015191,
title = {Chapter 11 - ISO Data Quality Standards for Master Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {191-205},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000119},
author = {John R. Talburt and Yinle Zhou},
keywords = {ISO, ANSI, ISO 8000, ISO 22745, Semantic Encoding},
abstract = {This chapter provides a discussion of the new International Organization for Standardization (ISO) standards related to the exchange of master data. It includes an in-depth look at the ISO 8000 family of standards, including ISO 8000-110, -120, -130, and -140, and their relationship to the ISO 22745-10, -30, and -40 standards. Also an explanation is given of simple versus strong ISO 8000-110 compliance, and the value proposition for ISO 8000 compliance is discussed.}
}
@incollection{20151,
title = {Chapter One - Introduction},
editor = {Olivier Curé and Guillaume Blin},
booktitle = {RDF Database Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-8},
year = {2015},
isbn = {978-0-12-799957-9},
doi = {https://doi.org/10.1016/B978-0-12-799957-9.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780127999579000018},
keywords = {RDF, Web of Data, Semantic Web, Big Data, Data management},
abstract = {This chapter motivates the importance of RDF data management through the Big Data and Web of Data/Semantic Web phenomena. It also provides some insights of existing RDF stores and presents the dimensions used in this book to compare these systems.}
}
@incollection{SHERMAN2015143,
title = {Chapter 7 - Technology & Product Architectures},
editor = {Rick Sherman},
booktitle = {Business Intelligence Guidebook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {143-169},
year = {2015},
isbn = {978-0-12-411461-6},
doi = {https://doi.org/10.1016/B978-0-12-411461-6.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114616000071},
author = {Rick Sherman},
keywords = {BI analytics, BI appliance, Big Data, BI targets, BI technology, columnar, data access API, In-database analytics, information access, in-memory analytics, integration services, massively parallel processing (MPP), OLAP, relational database},
abstract = {The technology categories that support business intelligence (BI) and data integration are technology platforms, enterprise applications, and data management. BI appliances usually combine hardware and software into one offering, although some are software only. The BI technology architecture is made up of four layers, as follows. (1) BI and analytics (analyzing information) includes reporting, ad hoc analysis, dashboards, online analytical processing (OLAP), spreadsheets, predictive analytics, performance management, data discovery, data visualization, Big Data analytics, BI search, mobile BI and cloud BI. (2) Information access and data integration (gathering, integrating, and transforming data into information) spans the sources, integration applications, integration services, data access application programming interfaces (APIs) and BI targets. (3) Data warehousing, which uses databases or files to store integrated data that is then be consumed by BI and analytics, generally uses relational databases for structured and semistructured data, and alternative databases for unstructured data. Alternatives include OLAP, columnar, in-database analytics, in-memory analytics, and massively parallel processing (MPP). (4) Data sources capture data for use in the enterprise. With Big Data, enterprises are experiencing rising data volumes, variety, and velocity. Thorough product and technology evaluations are essential.}
}
@article{KOURTESIS2014307,
title = {Semantic-based QoS management in cloud systems: Current status and future challenges},
journal = {Future Generation Computer Systems},
volume = {32},
pages = {307-323},
year = {2014},
note = {Special Section: The Management of Cloud Systems, Special Section: Cyber-Physical Society and Special Section: Special Issue on Exploiting Semantic Technologies with Particularization on Linked Data over Grid and Cloud Architectures},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1300232X},
author = {Dimitrios Kourtesis and Jose María Alvarez-Rodríguez and Iraklis Paraskakis},
keywords = {Cloud systems, Quality of service, Service oriented architectures, Semantics, Ontologies, Linked data, Sensor data, Big data},
abstract = {Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.}
}
@article{PERERA2016323,
title = {Statistical Filter based Sensor and DAQ Fault Detection for Onboard Ship Performance and Navigation Monitoring Systems},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {23},
pages = {323-328},
year = {2016},
note = {10th IFAC Conference on Control Applications in Marine SystemsCAMS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.362},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316319498},
author = {Lokukaluge P. Perera},
keywords = {Sensor Fault Detection, Principal Component Analysis, Big Data, Ship Performance, Navigation Monitoring},
abstract = {Abstract:
Statistical filter based sensor and data acquisition (DAQ) fault detection is presented in this study. The parameters of a large-scale data set of ship performance and navigation information are considered as statistical distributions and principal component analysis (PCA) is used to identify the hidden structure of the same data set. This data set relates to a specific operating region of the main engine, where ship performance and navigation conditions can be linearized. The structure derived under PCA is further investigated to identify the respective sensor and DAQ fault situations as the main contribution. That is done by projecting the same data set into the respective principal components, where a new set of ship performance and navigation parameters is derived. Then, the respective parameter variance values of the new data set are calculated and the thresholds that relate to the same variance values for detecting sensor and DAQ fault situations are derived. Finally, the data set of ship performance and navigation information is analyzed through these fault thresholds and the successful results on identifying complex fault situations are presented in this study. Hence, this approach can be used to develop advanced sensor and DAQ fault detection and isolation methodologies of ship performance and navigation monitoring systems.}
}
@article{MADHIKERMI2016145,
title = {Data quality assessment of maintenance reporting procedures},
journal = {Expert Systems with Applications},
volume = {63},
pages = {145-164},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S095741741630330X},
author = {Manik Madhikermi and Sylvain Kubler and Jérémy Robert and Andrea Buda and Kary Främling},
keywords = {Data quality, Information quality, Multi-criteria decision making, Analytic hierarchy process, Decision support systems, Maintenance},
abstract = {Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.}
}
@article{LIN2016221,
title = {Efficient quality-driven source selection from massive data sources},
journal = {Journal of Systems and Software},
volume = {118},
pages = {221-233},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216300577},
author = {Yiming Lin and Hongzhi Wang and Shuo Zhang and Jianzhong Li and Hong Gao},
keywords = {Information integration, Data source selection, Data quality},
abstract = {The query based on massive database is time-consuming and difficult. And the uneven quality of data source makes the multiple source selection more challenging. The low-quality data source can even make the result of the information unexpected. How to efficiently select quality-driven data sources on massive database remains a hard problem. In this paper, we study the efficient source selection problem on massive data set considering the quality of data sources. Our approach evaluates the quality of data source and balances the limitation of resources and the completeness of data source. For data source selection for a specific query, our method could select the data sources with the number of keywords larger than a given threshold. And the selected sources are ranked according to the values of information in data sources. Experimental results demonstrate that our method can scale to millions of data sources and perform pretty efficiently.}
}
@article{FROESLIMA20164862,
title = {Strategic modeling to improve services and operation to energy industries' customers},
journal = {Journal of Business Research},
volume = {69},
number = {11},
pages = {4862-4869},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.04.044},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316302077},
author = {Carlos Alberto {Fróes Lima} and Bernardo Marega Luz and Sílvia Tamada Takemoto and Paulo Barisson and Roberto Antônio Terencio Tezzin and Luciano E.A. Peres and Tales Neves Anarelli and Andrea Florencio {da Silva}},
keywords = {Customers' attendance, Operational improvement, Customers' demands anticipation, Relationship, Attendance strategies},
abstract = {Continuous analyses of demanded services at the energy companies are the shortest path to recognize and anticipate customers' requests, reinforce and manage the communication and operational flows. Energy utilities need to increase their operational efficiency concerning costs and agility to improve useful media and evaluate customers' expectations and requirements. Operational effectiveness must pursue the demands, considering the amount of services that the companies provide at their relationship channels, the communication facilities and the systems' infrastructure. The companies need to organize a huge amount of historical and online data to represent and forecast customers' relationship scenarios. Resources evaluation ensure regional requirements and weather conditions best attendance response, adequately addressing faults at the energy distribution grid, motivate customers to use alternative media and improve relationship channels. Reaching this scenario, big data treatment techniques provide the necessary agility to achieve the monthly/hourly volume of data (millions of registers per month) and permit communication clusters' views.}
}
@incollection{SEBASTIANCOLEMAN2013173,
title = {Chapter 13 - Directives for Data Quality Strategy},
editor = {Laura Sebastian-Coleman},
booktitle = {Measuring Data Quality for Ongoing Improvement},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {173-192},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397033-6},
doi = {https://doi.org/10.1016/B978-0-12-397033-6.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970336000146},
author = {Laura Sebastian-Coleman}
}
@article{LARSON2016700,
title = {A review and future direction of agile, business intelligence, analytics and data science},
journal = {International Journal of Information Management},
volume = {36},
number = {5},
pages = {700-710},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S026840121630233X},
author = {Deanne Larson and Victor Chang},
keywords = {Agile methodologies, Business intelligence (BI), Analytics and big data, Lifecycle for BI and Big Data},
abstract = {Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.}
}
@article{SMIRNOV2016323,
title = {Quality-based Workload Scaling for Real-time Streaming Systems},
journal = {Procedia Computer Science},
volume = {101},
pages = {323-332},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.038},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916327065},
author = {Pavel A. Smirnov and Denis Nasonov},
keywords = {scaling model, data streaming, elastic workload, quality of service, apache storm, big data},
abstract = {In this paper we propose an idea to scale workload via elastic quality of solution provided by the particular streaming applications. The contribution of this paper consists of quality-based workload scaling model, implementation details for quality assessment mechanism implemented at the top of Apache Storm and experimental evaluation of the proposed model on a synthetic and real-world (medical) examples.}
}
@incollection{TALBURT2015161,
title = {Chapter 10 - CSRUD for Big Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {161-190},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000107},
author = {John R. Talburt and Yinle Zhou},
keywords = {Big Data, Hadoop Map/Reduce, Transitive Closure, Graph Component},
abstract = {This chapter describes how a distributed processing environment such as Hadoop Map/Reduce can be used to support the CSRUD Life Cycle for Big Data. The examples shown in this chapter use the match key blocking described in Chapter 9 as a data partitioning strategy to perform ER on large datasets. The chapter includes an algorithm for finding the transitive closure of multiple match keys in a distributed processing environment using an iterative algorithm that minimizes the amount of local memory required for each processor. It also outlines a structure for an identity knowledge base in a distributed key-value data store, and describes strategies and distributed processing workflows for capture and update phases of the CSRUD life cycle using both record-based and attribute-based cluster-to-cluster structure projections.}
}
@article{KHATRI2016673,
title = {Managerial work in the realm of the digital universe: The role of the data triad},
journal = {Business Horizons},
volume = {59},
number = {6},
pages = {673-688},
year = {2016},
note = {CYBERSECURITY IN 2016: PEOPLE, TECHNOLOGY, AND PROCESSES},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681316300519},
author = {Vijay Khatri},
keywords = {Analytics, Big data, Managerial decision making, Managerial work, Digital universe},
abstract = {With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.}
}
@article{LI20161,
title = {A snail shell process model for knowledge discovery via data analytics},
journal = {Decision Support Systems},
volume = {91},
pages = {1-12},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616301233},
author = {Yan Li and Manoj A. Thomas and Kweku-Muata Osei-Bryson},
keywords = {Knowledge discovery via data analytics, Snail shell process model, KDDA, Big data analytics, Data-driven decision making},
abstract = {The rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process (KDDM) models are not adequately suited to address. We propose a snail shell process model for knowledge discovery via data analytics (KDDA) to address these challenges. We evaluate the utility of the KDDA process model using real-world analytic case studies at a global multi-media company. By comparing against traditional KDDM models, we demonstrate the need and relevance of the snail shell model, particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment.}
}
@article{WOOD20151018,
title = {Emerging uses of patient generated health data in clinical research},
journal = {Molecular Oncology},
volume = {9},
number = {5},
pages = {1018-1024},
year = {2015},
note = {Clinical trials for development of personalized cancer medicine},
issn = {1574-7891},
doi = {https://doi.org/10.1016/j.molonc.2014.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1574789114002014},
author = {William A. Wood and Antonia V. Bennett and Ethan Basch},
keywords = {Information technology, Patient reported outcomes, Quality of care, Clinical trials},
abstract = {Recent advancements in consumer directed personal computing technology have led to the generation of biomedically-relevant data streams with potential health applications. This has catalyzed international interest in Patient Generated Health Data (PGHD), defined as “health-related data – including health history, symptoms, biometric data, treatment history, lifestyle choices, and other information-created, recorded, gathered, or inferred by or from patients or their designees (i.e. care partners or those who assist them) to help address a health concern.”(Shapiro et al., 2012) PGHD offers several opportunities to improve the efficiency and output of clinical trials, particularly within oncology. These range from using PGHD to understand mechanisms of action of therapeutic strategies, to understanding and predicting treatment-related toxicity, to designing interventions to improve adherence and clinical outcomes. To facilitate the optimal use of PGHD, methodological research around considerations related to feasibility, validation, measure selection, and modeling of PGHD streams is needed. With successful integration, PGHD can catalyze the application of “big data” to cancer clinical research, creating both “n of 1” and population-level observations, and generating new insights into the nature of health and disease.}
}
@incollection{LINSTEDT20161,
title = {Chapter 1 - Introduction to Data Warehousing},
editor = {Daniel Linstedt and Michael Olschimke},
booktitle = {Building a Scalable Data Warehouse with Data Vault 2.0},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-15},
year = {2016},
isbn = {978-0-12-802510-9},
doi = {https://doi.org/10.1016/B978-0-12-802510-9.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128025109000015},
author = {Daniel Linstedt and Michael Olschimke},
keywords = {data, data warehouse, big data, decision support systems, scalability, business intelligence},
abstract = {This chapter introduces basic terminology of data warehousing, its applications, and the business context. It provides a brief description of its history and where it is heading. Basic data warehouse architectures that have been established in the industry are presented. Issues faced by data warehouse practitioners are explained, including topics such as big data, changing business requirements, performance issues, complexity, auditability, restart checkpoints, and fluctuation of team members.}
}
@article{WIBISONO201633,
title = {Traffic big data prediction and visualization using Fast Incremental Model Trees-Drift Detection (FIMT-DD)},
journal = {Knowledge-Based Systems},
volume = {93},
pages = {33-46},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004165},
author = {Ari Wibisono and Wisnu Jatmiko and Hanief Arief Wisesa and Benny Hardjono and Petrus Mursanto},
keywords = {Intelligent traffic systems, Data stream, Traffic prediction, Traffic visualization},
abstract = {Information extraction using distributed sensors has been widely used to obtain information knowledge from various regions or areas. Vehicle traffic data extraction is one of the ways to gather information in order to get the traffic condition information. This research intends to predict and visualize the traffic conditions in a particular road region. Traffic data was obtained from Department of Transport UK. These data are collected using hundreds of sensors for 24 h. Thus, the size of data is very huge. In order to get the behavior of the traffic condition, we need to analyze the huge dataset which was obtained from the sensors. The uses of conventional data mining methods are not sufficient to use, due to the process of knowledge building that should store data temporary in the memory. The fact that data is continuously becoming larger over time, therefore we need to find a method that could automatically adapt to process data in the form of streams. We use method called FIMT-DD (Fast Incremental Model Trees-Drift Detection) to analyze and predict the very large traffic dataset. Based on the prediction system that we have developed, we also visualize the prediction of traffic flow condition within generated sensor point in the real map simulation.}
}
@incollection{BOREK201423,
title = {Chapter 2 - Enterprise Information Management},
editor = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
booktitle = {Total Information Risk Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {23-38},
year = {2014},
isbn = {978-0-12-405547-6},
doi = {https://doi.org/10.1016/B978-0-12-405547-6.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405547600002X},
author = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
keywords = {EIM Strategy, EIM Governance, EIM Components, Big Data and EIM, Challenges for EIM},
abstract = {This chapter gives an introduction to concept of enterprise information management, investigates the influence of Big Data on EIM, and discusses today's key challenges and pressures for EIM.}
}
@incollection{KRISHNAN2013257,
title = {Chapter 14 - Implementing the Big Data – Data Warehouse – Real-Life Situations},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {257-265},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000143},
author = {Krish Krishnan},
keywords = {Hadoop, RDBMS, NoSQL, transformation, architecture},
abstract = {This chapter discusses the real-life implementation of the next-generation platform by three different companies and the direction they each have chosen from a technology and architecture perspective.}
}
@article{VANLOENEN2016338,
title = {Data protection legislation: A very hungry caterpillar: The case of mapping data in the European Union},
journal = {Government Information Quarterly},
volume = {33},
number = {2},
pages = {338-345},
year = {2016},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2016.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X16300326},
author = {Bastiaan {van Loenen} and Stefan Kulk and Hendrik Ploeger},
keywords = {Data protection, Privacy, Open data, Mapping data, European Union, PII2.0},
abstract = {The European Union's policy on open data aims at generating value through re-use of public sector information, such as mapping data. Open data policies should be applied in full compliance with the principles relating to the protection of personal data of the EU Data Protection Directive. Increased computer power, advancing data mining techniques and the increasing amount of publicly available big data extend the reach of the EU Data Protection Directive to much more data than currently assumed and acted upon. Especially mapping data are a key factor to identify individual data subjects and consequently subject to the EU Data Protection Directive and the recently approved EU General Data Protection Regulation. This could in effect obstruct the implementation of open data policies in the EU. The very hungry data protection legislation results in a need to rethink either the concept of personal data or the conditions for use of mapping data that are considered personal data.}
}
@article{MANTELERO2016238,
title = {Personal data for decisional purposes in the age of analytics: From an individual to a collective dimension of data protection},
journal = {Computer Law & Security Review},
volume = {32},
number = {2},
pages = {238-255},
year = {2016},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2016.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0267364916300280},
author = {Alessandro Mantelero},
keywords = {Big data, Right to privacy, Data protection, Group privacy, Collective interests, Data protection authorities, Risk assessment},
abstract = {In the big data era, new technologies and powerful analytics make it possible to collect and analyse large amounts of data in order to identify patterns in the behaviour of groups, communities and even entire countries. Existing case law and regulations are inadequate to address the potential risks and issues related to this change of paradigm in social investigation. This is due to the fact that both the right to privacy and the more recent right to data protection are protected as individual rights. The social dimension of these rights has been taken into account by courts and policymakers in various countries. Nevertheless, the rights holder has always been the data subject and the rights related to informational privacy have mainly been exercised by individuals. This atomistic approach shows its limits in the existing context of mass predictive analysis, where the larger scale of data processing and the deeper analysis of information make it necessary to consider another layer, which is different from individual rights. This new layer is represented by the collective dimension of data protection, which protects groups of persons from the potential harms of discriminatory and invasive forms of data processing. On the basis of the distinction between individual, group and collective dimensions of privacy and data protection, the author outlines the main elements that characterise the collective dimension of these rights and the representation of the underlying interests.}
}
@article{CHEN20151331,
title = {Towards Integrated Study of Data Management and Data Mining},
journal = {Procedia Computer Science},
volume = {55},
pages = {1331-1339},
year = {2015},
note = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.117},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915015926},
author = {Zhengxin Chen},
keywords = {Big Data, granular computing, granularity, databases, rough set theory, database keyword search (DBKWS)},
abstract = {From very beginning, research and practice of database management systems (DBMSs) have been cantered on handling granulation and granularities at various levels, thus sharing common interests with granular computing (GrC). Although DBMS and GrC have different focuses, the advent of Big Data has brought these two research areas closer to each other, because Big Data requires integrated study of data storage and analysis. In this paper, we explore this issue. Starting with an examination of granularities from a database perspective, we discuss new challenges of Big Data. We then turn to data management issues related to GrC. As an example of possible cross-fertilization of these two fields, we examine the recent development of database keyword search (DBKWS). Even research in DBKWS is largely independent to GrC, DBKWS has to handle various issues related to granularity handling. In particular, aggregation of DBKWS results is closely related to studies in granularities and granulation, which echoes L. Zadeh's famous formula: Granulation = Summarization. We present our proposed approach, termed as extended keyword search, which illustrates that an integrated study of data management and data mining/analysis is not restricted to GrC or rough set theory}
}
@article{SILVA2015289,
title = {Workshop Synthesis: Respondent/Survey Interaction in a World of Web and Smartphone Apps},
journal = {Transportation Research Procedia},
volume = {11},
pages = {289-296},
year = {2015},
note = {Transport Survey Methods: Embracing Behavioural and Technological Changes Selected contributions from the 10th International Conference on Transport Survey Methods 16-21 November 2014, Leura, Australia},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2015.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S2352146515003166},
author = {João de Abreu e Silva and Mark Davis},
keywords = {web based surveys, smartphone surveys, respondent interaction, respondent burden},
abstract = {Web and smartphone surveys are increasingly being used to collect travel information. This workshop explored respondent interaction with these tools, covering a range of research concerns. While smartphone surveys facilitate real-time passive collection of continuous data, thereby reducing respondent burden, their use raises many issues common with those present in web surveys. These include survey design, sample representativeness, privacy, respondent burden, data quality and validation. Workshop participants considered possible areas for future research on these issues and others such as provision of feedback to respondents, linking with big data and focusing on attitudinal and behavioural motivations.}
}
@article{LOPEZ20162128,
title = {Data Quality Control for St. Petersburg Flood Warning System},
journal = {Procedia Computer Science},
volume = {80},
pages = {2128-2140},
year = {2016},
note = {International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.532},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916310225},
author = {Jose Luis Araya Lopez and Anna V. Kalyuzhnaya and Sergey S. Kosukhin and Sergey V. Ivanov},
keywords = {outliers, quality-control, principal components, gap filling},
abstract = {This paper focuses on techniques for dealing with imperfect data in a frame of early warning system (EWS). Despite the fact that data may be technically damaged by presenting noise, outliers or missing values, met-ocean simulation systems have to deal with them to provide data transaction between models, real time data assimilation, calibration, etc. In this context data quality-control becomes one of the most important parts of EWS. St. Petersburg FWS was considered as an example of EWS. Quality control in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical control of simulated fields, statistical control and restoration of measurements and control using alternative models. Domain specific quality control was presented as two types of procedures based on theoretically proved methods were applied. The first procedure is based on probabilistic model of dynamical system, where processes are spatially interrelated and could be implemented in a form of multivariate regression (MRM). The second procedure is based on principal component analysis extended for taking into account temporal relations in data set (ePCA).}
}
@article{MOEYERSOMS201572,
title = {Including high-cardinality attributes in predictive models: A case study in churn prediction in the energy sector},
journal = {Decision Support Systems},
volume = {72},
pages = {72-81},
year = {2015},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2015.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923615000275},
author = {Julie Moeyersoms and David Martens},
keywords = {Data mining, Predictive modeling, High-cardinality attributes, Churn prediction},
abstract = {High-cardinality attributes are categorical attributes that contain a very large number of distinct values, like for example: family names, ZIP codes or bank account numbers. Within a predictive modeling setting, such features could be highly informative as it might be useful to know that people live in the same village or pay with the same bank account number. Despite this notable and intuitive advantage, high-cardinality attributes are rarely used in predictive modeling. The main reason for this is that including these attributes by using traditional transformation methods is either impossible due to anonymization of the data (when using semantic grouping of the values) or will vastly increase the dimensionality of the data set (when using dummy encoding), thereby making it difficult or even impossible for most classification techniques to build prediction models. The main contributions of this work are (1) the introduction of several possible transformation functions coming from different domains and contexts, that allow the inclusion of high-cardinality features in predictive models. (2) Using a unique data set of a large energy company with more than 1 million customers, we show that adding such features indeed improves the predictive performance of the model significantly. Moreover, (3) we empirically demonstrate that having more data leads to better prediction models, which is not observed for “traditional” data. As such, we also contribute to the area of big data analytics.}
}
@incollection{MCKNIGHT201432,
title = {Chapter Four - Data Quality: Passing the Standard},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {32-43},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080560000047},
author = {William McKnight},
keywords = {data quality, referential integrity, system of origination, data profiling},
abstract = {We might as well not do any data storage if we are not storing and passing high quality data. This chapter defines data quality and a program to maintain high standards throughout the enterprise.}
}
@article{WELCH2016393,
title = {Determinants of data sharing in U.S. city governments},
journal = {Government Information Quarterly},
volume = {33},
number = {3},
pages = {393-403},
year = {2016},
note = {Open and Smart Governments: Strategies, Tools, and Experiences},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2016.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X16301083},
author = {Eric W. Welch and Mary K. Feeney and Chul Hyun Park},
keywords = {Data sharing, Persuasion, Coercion, e-Government, Technical capacity, Local government},
abstract = {Although the rise of big data, open government, and social media imply greater data sharing, expectations currently do not match reality as many consider data exchange in government to be inadequate. Based on prior research, Additionally, the paper distinguishes technical management capacity and technical engagement capacity effects on agencies' sharing behavior. We test hypotheses predicting sharing behavior of municipal government agencies with other agencies and with non-government organizations using data from a 2012 national survey of U.S. municipal government managers. We find that data sharing with both government and non-government organizations is more strongly determined by persuasive mechanisms and technical engagement capacity, although technical management capacity is also important for sharing with other government agencies. Conclusions provide insights for future research directions and practice.}
}
@article{REIS2015238,
title = {Integrating modelling and smart sensors for environmental and human health},
journal = {Environmental Modelling & Software},
volume = {74},
pages = {238-246},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S136481521500167X},
author = {Stefan Reis and Edmund Seto and Amanda Northcross and Nigel W.T. Quinn and Matteo Convertino and Rod L. Jones and Holger R. Maier and Uwe Schlink and Susanne Steinle and Massimo Vieno and Michael C. Wimberly},
keywords = {Integrated modelling, Environmental sensors, Population health, Environmental health, Big data},
abstract = {Sensors are becoming ubiquitous in everyday life, generating data at an unprecedented rate and scale. However, models that assess impacts of human activities on environmental and human health, have typically been developed in contexts where data scarcity is the norm. Models are essential tools to understand processes, identify relationships, associations and causality, formalize stakeholder mental models, and to quantify the effects of prevention and interventions. They can help to explain data, as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results. We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing ‘Big Data’ and, more importantly, make the vital step from ‘Big Data’ to ‘Big Information’. In this paper, we illustrate current developments and identify key research needs using human and environmental health challenges as an example.}
}
@article{MATHEW201585,
title = {Big-data for building energy performance: Lessons from assembling a very large national database of building energy use},
journal = {Applied Energy},
volume = {140},
pages = {85-93},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2014.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0306261914012112},
author = {Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter},
keywords = {Buildings Performance Database, Building performance, Big data, Building data collection, Data-driven decision support},
abstract = {Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.}
}
@incollection{RAJAN2014173,
title = {Chapter 8 - Nanoinformatics: Data-Driven Materials Design for Health and Environmental Needs},
editor = {Matthew S. Hull and Diana M. Bowman},
booktitle = {Nanotechnology Environmental Health and Safety (Second Edition)},
publisher = {William Andrew Publishing},
edition = {Second Edition},
address = {Oxford},
pages = {173-198},
year = {2014},
series = {Micro and Nano Technologies},
isbn = {978-1-4557-3188-6},
doi = {https://doi.org/10.1016/B978-1-4557-3188-6.00008-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781455731886000086},
author = {Krishna Rajan},
keywords = {Environmental health and safety, data mining, nanoceria, manufactured nanoparticles, QSAR},
abstract = {This chapter explores the challenges in leveraging data-driven strategies to design engineered nanomaterials in the context of emerging nanotechnology environmental health and safety risks. A wealth of data is emerging on the potential toxicity of engineered nanomaterials, and the value of using informatics methods to unravel the complexity of that information is reviewed. This overview is followed by a discussion of case studies on how emerging data mining/informatics methods can help identify critical characteristics at the nanoscale that may have implications on the health or environmental impact of engineered nanomaterials. The chapter concludes with a discussion of how the concept of “Big Data” can help develop a data-guided decision framework to help link fundamental behaviors observed at the nanoscale to possible health and environmental effects that may become manifest at the macroscale.}
}
@incollection{MADANS2017463,
title = {Health Surveys},
editor = {Stella R. Quah},
booktitle = {International Encyclopedia of Public Health (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {463-469},
year = {2017},
isbn = {978-0-12-803708-9},
doi = {https://doi.org/10.1016/B978-0-12-803678-5.00193-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128036785001934},
author = {Jennifer H. Madans},
keywords = {Determinants of health, Functional limitation, Health, Health care, Health statistics, Measures of health, Morbidity, Population health, Survey design, Survey methodology},
abstract = {Information on health status, risk factors, health insurance, and health-care utilization is used to plan, conduct, and evaluate public health programs, to inform policies, regulations, and legislation, and to conduct research to better understand the determinants and consequences of health and health care. Health is a social as well as a biologic concept, and health surveys address all aspects of health and health care. Major topics covered include the conceptualization and measurement of health status and the determinants of health, summary measures of health, health-care utilization, and health behaviors. Survey design issues discussed include sampling, data linkage, data quality, and the protection of privacy and confidentiality.}
}
@article{FAN201575,
title = {Temporal knowledge discovery in big BAS data for building energy management},
journal = {Energy and Buildings},
volume = {109},
pages = {75-89},
year = {2015},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2015.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S0378778815302991},
author = {Cheng Fan and Fu Xiao and Henrik Madsen and Dan Wang},
keywords = {Temporal knowledge discovery, Time series data mining, Big data, Building automation system, Building energy management},
abstract = {With the advances of information technologies, today's building automation systems (BASs) are capable of managing building operational performance in an efficient and convenient way. Meanwhile, the amount of real-time monitoring and control data in BASs grows continually in the building lifecycle, which stimulates an intense demand for powerful big data analysis tools in BASs. Existing big data analytics adopted in the building automation industry focus on mining cross-sectional relationships, whereas the temporal relationships, i.e., the relationships over time, are usually overlooked. However, building operations are typically dynamic and BAS data are essentially multivariate time series data. This paper presents a time series data mining methodology for temporal knowledge discovery in big BAS data. A number of time series data mining techniques are explored and carefully assembled, including the Symbolic Aggregate approXimation (SAX), motif discovery, and temporal association rule mining. This study also develops two methods for the efficient post-processing of knowledge discovered. The methodology has been applied to analyze the BAS data retrieved from a real building. The temporal knowledge discovered is valuable to identify dynamics, patterns and anomalies in building operations, derive temporal association rules within and between subsystems, assess building system performance and spot opportunities in energy conservation.}
}
@article{CHIN201536,
title = {Cancer Genomics in Clinical Context},
journal = {Trends in Cancer},
volume = {1},
number = {1},
pages = {36-43},
year = {2015},
issn = {2405-8033},
doi = {https://doi.org/10.1016/j.trecan.2015.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S2405803315000114},
author = {Lynda Chin and Jennifer A. Wargo and Denise J. Spring and Hagop Kantarjian and P. Andrew Futreal},
keywords = {N-of-ALL, adaptive learning, patient-oriented genomic research, longitudinal genomics–phenomics profiling, data-driven science and care},
abstract = {Precision medicine requires appropriate application of genomics in clinical practice. In cancer, we have witnessed practice-changing examples of how genomic knowledge is translated into more tailored and effective therapies. The next opportunity is to embed cancer genomics in clinical context so that patient-centric longitudinal clinical, genomic, and molecular phenotypes can be compiled for adaptive learning between precision medicine research and clinical care with the goal of accelerating clinically-actionable discoveries. We describe here an adaptive learning platform, APOLLO™ (adaptive patient-oriented longitudinal learning and optimization) designed to integrate genomic research in the context of, but not in the path of, routine and investigational clinical care for purposes of enabling data-driven discovery across disciplines such that every patient can contribute to and potentially benefit from research discoveries.}
}
@article{RAHIMI2014768,
title = {Validating an ontology-based algorithm to identify patients with Type 2 Diabetes Mellitus in Electronic Health Records},
journal = {International Journal of Medical Informatics},
volume = {83},
number = {10},
pages = {768-778},
year = {2014},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2014.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505614001038},
author = {Alireza Rahimi and Siaw-Teng Liaw and Jane Taggart and Pradeep Ray and Hairong Yu},
keywords = {Ontology, SPARQL, Electronic Health Records, Diabetes Mellitus, Type 2, Validation studies},
abstract = {Background
Improving healthcare for people with chronic conditions requires clinical information systems that support integrated care and information exchange, emphasizing a semantic approach to support multiple and disparate Electronic Health Records (EHRs). Using a literature review, the Australian National Guidelines for Type 2 Diabetes Mellitus (T2DM), SNOMED-CT-AU and input from health professionals, we developed a Diabetes Mellitus Ontology (DMO) to diagnose and manage patients with diabetes. This paper describes the manual validation of the DMO-based approach using real world EHR data from a general practice (n=908 active patients) participating in the electronic Practice Based Research Network (ePBRN).
Method
The DMO-based algorithm to query, using Semantic Protocol and RDF Query Language (SPARQL), the structured fields in the ePBRN data repository were iteratively tested and refined. The accuracy of the final DMO-based algorithm was validated with a manual audit of the general practice EHR. Contingency tables were prepared and Sensitivity and Specificity (accuracy) of the algorithm to diagnose T2DM measured, using the T2DM cases found by manual EHR audit as the gold standard. Accuracy was determined with three attributes – reason for visit (RFV), medication (Rx) and pathology (path) – singly and in combination.
Results
The Sensitivity and Specificity of the algorithm were 100% and 99.88% with RFV; 96.55% and 98.97% with Rx; and 15.6% and 98.92% with Path. This suggests that Rx and Path data were not as complete or correct as the RFV for this general practice, which kept its RFV information complete and current for diabetes. However, the completeness is good enough for this purpose as confirmed by the very small relative deterioration of the accuracy (Sensitivity and Specificity of 97.67% and 99.18%) when calculated for the combination of RFV, Rx and Path. The manual EHR audit suggested that the accuracy of the algorithm was influenced by data quality such as incorrect data due to mistaken units of measurement and unavailable data due to non-documentation or documented in the wrong place or progress notes, problems with data extraction, encryption and data management errors.
Conclusion
This DMO-based algorithm is sufficiently accurate to support a semantic approach, using the RFV, Rx and Path to define patients with T2DM from EHR data. However, the accuracy can be compromised by incomplete or incorrect data. The extent of compromise requires further study, using ontology-based and other approaches.}
}
@article{STANKEVICIUS2015383,
title = {Hybrid Approach Model for Prevention of Tax Evasion and Fraud},
journal = {Procedia - Social and Behavioral Sciences},
volume = {213},
pages = {383-389},
year = {2015},
note = {20th International Scientific Conference "Economics and Management 2015 (ICEM-2015)"},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.11.555},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815059108},
author = {Evaldas Stankevicius and Linas Leonas},
keywords = {Tax, Tax evasion, Shadow economy, Determinants of tax evasion.},
abstract = {Tax evasion is an important social-economic problem in all societies of the world, regardless of the type of tax system or the country's economic development level, therefore deception using tax incentives or tax evasion should be analyzed in a wider context, as the key aspect of shadow economy. Modern economic studies, analyzing the interaction between taxpayers, tax burden, social environment and country's economic development, seek to make an integrated evaluation, not just explain individual tax avoidance motivations. The theoretical research analyzes the interaction between the taxpayer behavior and their social status, and not only individual motivations reasons to explain the level of tax evasion and as results of research - model's framework is unique incorporation of detection models and big data processing ability, combined with a psychological-social portrait of the tax evaders will allow their quick identification, even though this does not mean that they will have committed an illegal act.}
}
@article{BELLINI2014827,
title = {Km4City ontology building vs data harvesting and cleaning for smart-city services},
journal = {Journal of Visual Languages & Computing},
volume = {25},
number = {6},
pages = {827-839},
year = {2014},
note = {Distributed Multimedia Systems DMS2014 Part I},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2014.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X14001165},
author = {Pierfrancesco Bellini and Monica Benigni and Riccardo Billero and Paolo Nesi and Nadia Rauch},
keywords = {Smart city, Knowledge base construction, Reconciliation, Validation and verification of knowledge base, Smart city ontology, Linked open graph, Km4city},
abstract = {Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies and knowledge base for smart city. Smart City ontology is not yet standardized, and a lot of research work is needed to identify models that can easily support the data reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system for data ingestion and reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing a big data volume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and the big data architecture for the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection.}
}
@article{HUANG2016244,
title = {Understanding U.S. regional linguistic variation with Twitter data analysis},
journal = {Computers, Environment and Urban Systems},
volume = {59},
pages = {244-255},
year = {2016},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2015.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971515300399},
author = {Yuan Huang and Diansheng Guo and Alice Kasakoff and Jack Grieve},
keywords = {Social media, Linguistic, Twitter, American dialects, Regionalization, US regions, Spatial data mining},
abstract = {We analyze a Big Data set of geo-tagged tweets for a year (Oct. 2013–Oct. 2014) to understand the regional linguistic variation in the U.S. Prior work on regional linguistic variations usually took a long time to collect data and focused on either rural or urban areas. Geo-tagged Twitter data offers an unprecedented database with rich linguistic representation of fine spatiotemporal resolution and continuity. From the one-year Twitter corpus, we extract lexical characteristics for twitter users by summarizing the frequencies of a set of lexical alternations that each user has used. We spatially aggregate and smooth each lexical characteristic to derive county-based linguistic variables, from which orthogonal dimensions are extracted using the principal component analysis (PCA). Finally a regionalization method is used to discover hierarchical dialect regions using the PCA components. The regionalization results reveal interesting linguistic regional variations in the U.S. The discovered regions not only confirm past research findings in the literature but also provide new insights and a more detailed understanding of very recent linguistic patterns in the U.S.}
}
@article{YAISH20142168,
title = {Multi-tenant Elastic Extension Tables Data Management},
journal = {Procedia Computer Science},
volume = {29},
pages = {2168-2181},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914003792},
author = {Haitham Yaish and Madhu Goyal and George Feuerlicht},
keywords = {Cloud Computing, Software as a Service, Big Data, Elastic Extension Tables, Multi-tenant Database, Relational Tables, Virtual Relational Tables.},
abstract = {Multi-tenant database is a new database solution which is significant for Software as a service (SaaS) and Big Data applications in the context of cloud computing paradigm. This multi-tenant database has significant design challenges to develop a solution that ensures a high level of data quality, accessibility, and manageability for the tenants using this database. In this paper, we propose a multi-tenant data management service called Elastic Extension Tables Schema Handler Service (EETSHS), which is based on a multi-tenant database schema called Elastic Extension Tables (EET). This data management service satisfies tenants’ different business requirements, by creating, managing, organizing, and administratinglarge volumes of structured, semi-structured, and unstructured data. Furthermore, it combines traditional relational data with virtual relational data in a single database schema and allows tenants to manage this data by calling functions from this service. We present algorithms for frequently used functions of this service, and perform several experiments to measure the feasibility and effectiveness of managing multi-tenant data using these functions. We report experimental results of query execution timesfor managing tenants’ virtual and traditional relational data showing that EET schema is a good candidate for the management of multi-tenant data for SaaS and Big Data applications.}
}
@incollection{HOLLIN201514,
title = {Chapter 2 - Drilling into the Big Data Gold Mine: Data Fusion and High-Performance Analytics for Intelligence Professionals},
editor = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
booktitle = {Application of Big Data for National Security},
publisher = {Butterworth-Heinemann},
pages = {14-20},
year = {2015},
isbn = {978-0-12-801967-2},
doi = {https://doi.org/10.1016/B978-0-12-801967-2.00002-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019672000021},
author = {Rupert Hollin},
keywords = {Big Data, Fusion, High-performance analytics, Visualization},
abstract = {Threats to local, national, and global public security are continually evolving, and for those tasked with preventing and responding to these threats, the amount of potentially useful data can often seem overwhelming. What compounds this Big Data issue is the fact that we are living in a time of global economic austerity in which national security and law enforcement agencies need to become better at exploiting information while managing the demands of ever-shrinking budgets. This chapter looks at how, by using the latest software tools and techniques for data fusion and high-performance analytics, agencies can automate traditional labor-intensive tasks, gain a holistic view of information that originates from multiple sources, and extract valuable intelligence in a timely and more efficient manner.}
}
@article{ANG20161,
title = {Big Sensor Data Applications in Urban Environments},
journal = {Big Data Research},
volume = {4},
pages = {1-12},
year = {2016},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615300241},
author = {Li-Minn Ang and Kah Phooi Seng},
keywords = {Big data, Sensor-based systems, Survey, Application, Challenges},
abstract = {The emergence of new technologies such as Internet/Web/Network-of-Things and large scale wireless sensor systems enables the collection of data from an increasing volume and variety of networked sensors for analysis. In this review article, we summarize the latest developments of big sensor data systems (a term to conceptualize the application of the big data model towards networked sensor systems) in various representative studies for urban environments, including for air pollution monitoring, assistive living, disaster management systems, and intelligent transportation. An important focus is the inclusion of how value is extracted from the big data system. We also discuss some recent techniques for big data acquisition, cleaning, aggregation, modeling, and interpretation in large scale sensor-based systems. We conclude the paper with a discussion on future perspectives and challenges of sensor-based data systems in the big data era.}
}
@article{BAUR201538,
title = {MARK-AGE data management: Cleaning, exploration and visualization of data},
journal = {Mechanisms of Ageing and Development},
volume = {151},
pages = {38-44},
year = {2015},
note = {Biomarkers of Human Ageing},
issn = {0047-6374},
doi = {https://doi.org/10.1016/j.mad.2015.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0047637415000706},
author = {Jennifer Baur and Maria Moreno-Villanueva and Tobias Kötter and Thilo Sindlinger and Alexander Bürkle and Michael R. Berthold and Michael Junk},
keywords = {Data cleaning, Missing data, Batch effects, Outliers, Data visualization},
abstract = {Databases are an organized collection of data and necessary to investigate a wide spectrum of research questions. For data evaluation analyzers should be aware of possible data quality problems that can compromise results validity. Therefore data cleaning is an essential part of the data management process, which deals with the identification and correction of errors in order to improve data quality. In our cross-sectional study, biomarkers of ageing, analytical, anthropometric and demographic data from about 3000 volunteers have been collected in the MARK-AGE database. Although several preventive strategies were applied before data entry, errors like miscoding, missing values, batch problems etc., could not be avoided completely. Such errors can result in misleading information and affect the validity of the performed data analysis. Here we present an overview of the methods we applied for dealing with errors in the MARK-AGE database. We especially describe our strategies for the detection of missing values, outliers and batch effects and explain how they can be handled to improve data quality. Finally we report about the tools used for data exploration and data sharing between MARK-AGE collaborators.}
}
@incollection{MALCOM2014325,
title = {Chapter 14 - Analysis of Deep Sequencing Data: Insights and Challenges},
editor = {Carolina Simó and Alejandro Cifuentes and Virginia García-Cañas},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {63},
pages = {325-354},
year = {2014},
booktitle = {Fundamentals of Advanced Omics Technologies: From Genes to Metabolites},
issn = {0166-526X},
doi = {https://doi.org/10.1016/B978-0-444-62651-6.00015-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444626516000155},
author = {Jacob W. Malcom and John H. Malone},
keywords = {Deep sequencing, Seq, Big data, Statistical analysis},
abstract = {Modern biomedical research demands that investigators become familiar with deep sequencing data analysis, yet the vast nature of deep sequencing data creates a variety of roadblocks for biologists not familiar with the analysis of such large datasets. In this chapter, we provide an introduction to data analysis for biologists, review first principles, point out areas of concern, and suggest software tools that are becoming standards for analysis of deep sequencing data. Perhaps the biggest challenge in the analysis of deep sequencing data will be data management and storage and repeating complex, multitier computational analyses. The future of deep sequencing data analysis will be likely data-driven and rely on principles gleaned from “big data” analysis.}
}
@article{CHRISTOPOULOS201570,
title = {Extraction of the global absolute temperature for Northern Hemisphere using a set of 6190 meteorological stations from 1800 to 2013},
journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
volume = {128},
pages = {70-83},
year = {2015},
issn = {1364-6826},
doi = {https://doi.org/10.1016/j.jastp.2015.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364682615000577},
author = {Demetris T. Christopoulos},
keywords = {Absolute temperature, Northern Hemisphere, Valid station, Data quality, Seasonal bias, Extreme values distribution, Missing records, Big data analysis},
abstract = {Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800–2013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.}
}
@incollection{ELDER2012453,
title = {20 - Extreme scale clinical analytics with open source software},
editor = {Lee Harland and Mark Forster},
booktitle = {Open Source Software in Life Science Research},
publisher = {Woodhead Publishing},
pages = {453-480},
year = {2012},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-1-907568-97-8},
doi = {https://doi.org/10.1533/9781908818249.453},
url = {https://www.sciencedirect.com/science/article/pii/B9781907568978500205},
author = {Kirk Elder and Brian Ellenberger},
keywords = {clinical data, ICD10, HL7, electronic health records, Big Data},
abstract = {Abstract:
Knowledge is at the root of understanding all symptoms, diagnosing every ailment, and curing every disease. This knowledge comes from the deep studies performed by research organizations and diligent healthcare workers who contribute to documenting and responsibly sharing their observations. Through the American Recovery and Reinvestment Act of 2009 (ARRA [1]), the industry was incented to implement electronic medical record systems that capture more information than ever before. When billions of medical records converge within a secure network, the baton will be handed to analytics systems to make use of the data; are they ready? This chapter explores what the next-generation software infrastructure for clinical analytics looks like. We discuss integration frameworks, workflow pipelines, and 'Big Data' storage and processing solutions such as NoSQL and Hadoop, and conclude with a vision of how clinical analytics must evolve if it is to handle the recent explosion in human health data.}
}
@article{PERERA2016512,
title = {Marine Engine Operating Regions under Principal Component Analysis to evaluate Ship Performance and Navigation Behavior},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {23},
pages = {512-517},
year = {2016},
note = {10th IFAC Conference on Control Applications in Marine SystemsCAMS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.487},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316320778},
author = {Lokukaluge P. Perera and Brage Mo},
keywords = {Principal Component Analysis, Big Data, Marine Engine Operations, Ship Performance Monitoring, Structured Data},
abstract = {Abstract:
Marine engine operating regions under principal component analysis (PCA) to evaluate ship performance and navigation behavior are presented in this study. A data set with ship performance and navigation information (i.e. a selected vessel) is considered to identify its hidden structure with respect to a selected operating region of the marine engine. Firstly, the data set is classified with respect to the engine operating points (i.e. operating modes), identifying three operating regions for the main engine. Secondly, one engine operating region (i.e. a data cluster) is analyzed to calculate the respective principal components (PCs). These PCs represent various relationships among ship performance and navigation parameters of the vessel and those relationships with respect to the marine engine operating region are used to evaluate ship performance and navigation behavior. Furthermore, such knowledge (i.e. PCs and parameter behavior) can also be used for sensor fault identification and data compression/expansion types of applications as a big data solution in shipping.}
}
@article{ESTRIDGE2016311,
title = {Integration of System Modeling and Design Processes, Data and Technology for Streamlining Enterprise Integration},
journal = {Procedia Computer Science},
volume = {95},
pages = {311-318},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.340},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325133},
author = {Craig Estridge and Timothy Eveleigh and Berek Tanju},
keywords = {Data quality, data validation, data reuse, rule-driven data creation},
abstract = {Complex systems, Pattern-based Systems Engineering, and Model-based Systems Engineering are approaches to model system lifecycles and address engineered systems complexity. Each approach advocates languages, tools, and processes as supplements to traditional engineering processes; however, these approaches are not readily adopted. The challenge for these system modeling approaches is they are not linked to traditional engineering processes and require unique tools and languages independent of traditional engineering domains. Furthermore, the knowledge of the external standards and specifications used in traditional processes are not available to users of system modeling technology, and those who have knowledge of external standards and specifications do not have knowledge of system modeling technology. Moreover, embedding intelligence of external standards and specifications is available within traditional system modeling applications. The research goal is integrating the technology and processes employed by system modeling and traditional engineering to automate data creation, flow and validation through removal of barriers to rule-driven technology adoption. Rule-driven technology adoption should lead to efficiencies in creating and managing extremely large datasets through automation. The results project significant improvement to data quality and enterprise IT systems integration while reducing labor costs to; validate data, avoid and correct errors, and eliminate data omissions.}
}
@article{KAMAL2016191,
title = {A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset},
journal = {Computer Methods and Programs in Biomedicine},
volume = {131},
pages = {191-206},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715304119},
author = {Sarwar Kamal and Shamim Hasnat Ripon and Nilanjan Dey and Amira S. Ashour and V. Santhi},
keywords = {MapReduce, K-nearest neighbor, Big data, DNA (deoxyribonucleic acid), Computational biology, Imbalance data},
abstract = {Background
In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
Method
In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
Results
To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
Conclusions
The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.}
}
@incollection{KOLTAY201671,
title = {Chapter 5 - Digital Research Data: Where are we Now?},
editor = {David Baker and Wendy Evans},
booktitle = {Digital Information Strategies},
publisher = {Chandos Publishing},
pages = {71-84},
year = {2016},
isbn = {978-0-08-100251-3},
doi = {https://doi.org/10.1016/B978-0-08-100251-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002513000056},
author = {Tibor Koltay},
keywords = {data citation, data curation, data literacy, data management, data quality, data sharing, research data},
abstract = {The key topic of digital research data raises a multitude of issues: big data, data sharing, data quality, data management, data curation, data citation, data literacy. This chapter addresses questions related to the definition of these concepts, to the frameworks constructed for a better understanding and treatment of the different phenomena, as well as ethical considerations. The potential of libraries and information professionals in fulfilling data-related activities is outlined, together with the associated requirements of them.}
}
@incollection{NETTLETON2014105,
title = {Chapter 7 - Data Sampling and Partitioning},
editor = {David Nettleton},
booktitle = {Commercial Data Mining},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {105-117},
year = {2014},
isbn = {978-0-12-416602-8},
doi = {https://doi.org/10.1016/B978-0-12-416602-8.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166028000078},
author = {David Nettleton},
keywords = {sampling, data reduction, partitioning, business criteria, train, test, Big Data},
abstract = {This chapter discusses various types of sampling such as random sampling and sampling based on business criteria (age of customer, time as client, etc.). It also discusses extracting train and test datasets for specific business objectives and considers the issue of Big Data, given that it is currently a hot topic.}
}
@incollection{SHERMAN20153,
title = {Chapter 1 - The Business Demand for Data, Information, and Analytics},
editor = {Rick Sherman},
booktitle = {Business Intelligence Guidebook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {3-19},
year = {2015},
isbn = {978-0-12-411461-6},
doi = {https://doi.org/10.1016/B978-0-12-411461-6.00001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114616000010},
author = {Rick Sherman},
keywords = {Big Data, Data, Data 5 Cs, Data capture, Data variety, Data velocity, Data volume, Information, Information analysis, Operational BI},
abstract = {In the business world, knowledge is not just power. It is the lifeblood of a thriving enterprise. Knowledge comes from information, and that, in turn, comes from data. Many enterprises are overwhelmed by the deluge of data, which they are receiving from all directions. They are wondering if they can handle Big Data—with its expanding volume, variety, and velocity. There is a big difference between raw data, which by itself is not useful, and actionable information, which business people can use with confidence to make decisions. Data must to be transformed to make it clean, consistent, conformed, current, and comprehensive—the five Cs of data. It is up to a Business Intelligence (BI) team to gather and manage the data to empower the company’s business groups with the information they need to gain knowledge—knowledge that helps them make informed decisions about every step the company takes. While there are attempts to circumvent or replace BI with operational systems, there really is no good substitute for true BI. Operational systems may excel at data capture, but BI excels at information analysis.}
}
@article{GRANELL20161,
title = {Future Internet technologies for environmental applications},
journal = {Environmental Modelling & Software},
volume = {78},
pages = {1-15},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215301298},
author = {Carlos Granell and Denis Havlik and Sven Schade and Zoheir Sabeur and Conor Delaney and Jasmin Pielorz and Thomas Usländer and Paolo Mazzetti and Katharina Schleidt and Mike Kobernus and Fuada Havlik and Nils Rune Bodsberg and Arne Berre and Jose Lorenzo Mon},
keywords = {Environmental informatics, Environmental observation web, Future internet, Cloud computing, Internet of things, Big data, Environmental specific enablers, Volunteered geographic information, Crowdtasking},
abstract = {This paper investigates the usability of Future Internet technologies (aka “Generic Enablers of the Future Internet”) in the context of environmental applications. The paper incorporates the best aspects of the state-of-the-art in environmental informatics with geospatial solutions and scalable processing capabilities of Internet-based tools. It specifically targets the promotion of the “Environmental Observation Web” as an observation-centric paradigm for building the next generation of environmental applications. In the Environmental Observation Web, the great majority of data are considered as observations. These can be generated from sensors (hardware), numerical simulations (models), as well as by humans (human sensors). Independently from the observation provenance and application scope, data can be represented and processed in a standardised way in order to understand environmental processes and their interdependencies. The development of cross-domain applications is then leveraged by technologies such as Cloud Computing, Internet of Things, Big Data Processing and Analytics. For example, “the cloud” can satisfy the peak-performance needs of applications which may occasionally use large amounts of processing power at a fraction of the price of a dedicated server farm. The paper also addresses the need for Specific Enablers that connect mainstream Future Internet capabilities with sensor and geospatial technologies. Main categories of such Specific Enablers are described with an overall architectural approach for developing environmental applications and exemplar use cases.}
}
@article{WHALEN201692,
title = {Predicting protein function and other biomedical characteristics with heterogeneous ensembles},
journal = {Methods},
volume = {93},
pages = {92-102},
year = {2016},
note = {Computational protein function predictions},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2015.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1046202315300566},
author = {Sean Whalen and Om Prakash Pandey and Gaurav Pandey},
keywords = {Protein function prediction, Heterogeneous ensembles, Nested cross-validation, Diversity-performance tradeoff, Ensemble calibration, Distributed machine learning},
abstract = {Prediction problems in biomedical sciences, including protein function prediction (PFP), are generally quite difficult. This is due in part to incomplete knowledge of the cellular phenomenon of interest, the appropriateness and data quality of the variables and measurements used for prediction, as well as a lack of consensus regarding the ideal predictor for specific problems. In such scenarios, a powerful approach to improving prediction performance is to construct heterogeneous ensemble predictors that combine the output of diverse individual predictors that capture complementary aspects of the problems and/or datasets. In this paper, we demonstrate the potential of such heterogeneous ensembles, derived from stacking and ensemble selection methods, for addressing PFP and other similar biomedical prediction problems. Deeper analysis of these results shows that the superior predictive ability of these methods, especially stacking, can be attributed to their attention to the following aspects of the ensemble learning process: (i) better balance of diversity and performance, (ii) more effective calibration of outputs and (iii) more robust incorporation of additional base predictors. Finally, to make the effective application of heterogeneous ensembles to large complex datasets (big data) feasible, we present DataSink, a distributed ensemble learning framework, and demonstrate its sound scalability using the examined datasets. DataSink is publicly available from https://github.com/shwhalen/datasink.}
}
@article{HOGENBOOM201612,
title = {A Survey of event extraction methods from text for decision support systems},
journal = {Decision Support Systems},
volume = {85},
pages = {12-22},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616300173},
author = {Frederik Hogenboom and Flavius Frasincar and Uzay Kaymak and Franciska {de Jong} and Emiel Caron},
keywords = {Event extraction, Information extraction, Natural language processing (NLP), Text mining},
abstract = {Event extraction, a specialized stream of information extraction rooted back into the 1980s, has greatly gained in popularity due to the advent of big data and the developments in the related fields of text mining and natural language processing. However, up to this date, an overview of this particular field remains elusive. Therefore, we give a summarization of event extraction techniques for textual data, distinguishing between data-driven, knowledge-driven, and hybrid methods, and present a qualitative evaluation of these. Moreover, we discuss common decision support applications of event extraction from text corpora. Last, we elaborate on the evaluation of event extraction systems and identify current research issues.}
}
@incollection{SHERMAN2015375,
title = {Chapter 15 - Advanced Analytics},
editor = {Rick Sherman},
booktitle = {Business Intelligence Guidebook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {375-402},
year = {2015},
isbn = {978-0-12-411461-6},
doi = {https://doi.org/10.1016/B978-0-12-411461-6.00015-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114616000150},
author = {Rick Sherman},
keywords = {Analytical hub, Analytical sandbox, Analytics, Big Data, Data mining, Data visualization, Predictive analytics},
abstract = {Advanced analytics focuses on gauging the future and allowing what-if analysis. Predictive analytics and data mining are the processes by which one performs advanced analytics for forecasting and modeling. Data visualization is an analytics technique that presents information pictorially, often graphically, helping to communicate a huge volume of information in a more intuitive way. Enterprises need two distinct types of self-service data environments based on different business analytical requirements: analytical sandboxes, to enable business users to be able to add data and derive metrics, and analytical hubs, which accommodate more extensive and intensive data gathering and analysis. Analytics for Big Data require special attention to the scope, program, architecture, and team.}
}
@article{CHAOLONG2016709,
title = {Research on Visualization of Multi-Dimensional Real-Time Traffic Data Stream Based on Cloud Computing},
journal = {Procedia Engineering},
volume = {137},
pages = {709-718},
year = {2016},
note = {Green Intelligent Transportation System and Safety},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.01.308},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816003350},
author = {Jia Chaolong and Wang Hanning and Wei Lili},
keywords = {Smart Transportation, Visualization, Cloud Computing, Parallel Query, Big Data},
abstract = {Based on efficient continuous parallel query series algorithm supporting multi-objective optimization, by using visual graphics technology for traffic data streams for efficient real-time graphical visualization, it improve human-computer interaction, to realize real-time and visual data analysis and to improve efficiency and accuracy of the analysis. This paper employs data mining processing and statistical analysis on real-time traffic data stream, based on the parameters standards of various data mining algorithms, and by using computer graphics and image processing technology, converts graphics or images and make them displayed on the screen according to the system requirements, in order to track, forecast and maintain the operating condition of all traffic service systems effectively.}
}
@article{CHANG201656,
title = {A model to compare cloud and non-cloud storage of Big Data},
journal = {Future Generation Computer Systems},
volume = {57},
pages = {56-76},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003167},
author = {Victor Chang and Gary Wills},
keywords = {Organizational sustainability modeling (OSM), Comparison between Cloud and non-Cloud storage platforms, Real Cloud case studies, Data analysis and visualization},
abstract = {When comparing Cloud and non-Cloud Storage it can be difficult to ensure that the comparison is fair. In this paper we examine the process of setting up such a comparison and the metric used. Performance comparisons on Cloud and non-Cloud systems, deployed for biomedical scientists, have been conducted to identify improvements of efficiency and performance. Prior to the experiments, network latency, file size and job failures were identified as factors which degrade performance and experiments were conducted to understand their impacts. Organizational Sustainability Modeling (OSM) is used before, during and after the experiments to ensure fair comparisons are achieved. OSM defines the actual and expected execution time, risk control rates and is used to understand key outputs related to both Cloud and non-Cloud experiments. Forty experiments on both Cloud and non-Cloud systems were undertaken with two case studies. The first case study was focused on transferring and backing up 10,000 files of 1 GB each and the second case study was focused on transferring and backing up 1000 files 10 GB each. Results showed that first, the actual and expected execution time on the Cloud was lower than on the non-Cloud system. Second, there was more than 99% consistency between the actual and expected execution time on the Cloud while no comparable consistency was found on the non-Cloud system. Third, the improvement in efficiency was higher on the Cloud than the non-Cloud. OSM is the metric used to analyze the collected data and provided synthesis and insights to the data analysis and visualization of the two case studies.}
}
@article{CINNAMON2016253,
title = {Evidence and future potential of mobile phone data for disease disaster management},
journal = {Geoforum},
volume = {75},
pages = {253-264},
year = {2016},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2016.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0016718516301981},
author = {Jonathan Cinnamon and Sarah K. Jones and W. Neil Adger},
keywords = {Mobile phone, Call detail records, SMS, Disaster, Disease, Big data},
abstract = {Global health threats such as the recent Ebola and Zika virus outbreaks require rapid and robust responses to prevent, reduce and recover from disease dispersion. As part of broader big data and digital humanitarianism discourses, there is an emerging interest in data produced through mobile phone communications for enhancing the data environment in such circumstances. This paper assembles user perspectives and critically examines existing evidence and future potential of mobile phone data derived from call detail records (CDRs) and two-way short message service (SMS) platforms, for managing and responding to humanitarian disasters caused by communicable disease outbreaks. We undertake a scoping review of relevant literature and in-depth interviews with key informants to ascertain the: (i) information that can be gathered from CDRs or SMS data; (ii) phase(s) in the disease disaster management cycle when mobile data may be useful; (iii) value added over conventional approaches to data collection and transfer; (iv) barriers and enablers to use of mobile data in disaster contexts; and (v) the social and ethical challenges. Based on this evidence we develop a typology of mobile phone data sources, types, and end-uses, and a decision-tree for mobile data use, designed to enable effective use of mobile data for disease disaster management. We show that mobile data holds great potential for improving the quality, quantity and timing of selected information required for disaster management, but that testing and evaluation of the benefits, constraints and limitations of mobile data use in a wider range of mobile-user and disaster contexts is needed to fully understand its utility, validity, and limitations.}
}
@incollection{MOISE2015279,
title = {Chapter 12 - Terabyte-Scale Image Similarity Search},
editor = {Venu Govindaraju and Vijay V. Raghavan and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {33},
pages = {279-301},
year = {2015},
booktitle = {Big Data Analytics},
issn = {0169-7161},
doi = {https://doi.org/10.1016/B978-0-444-63492-4.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634924000125},
author = {Diana Moise and Denis Shestakov},
keywords = {Big Data, Hadoop, MapReduce, Image search, Multimedia retrieval, Smart deployment, SIFT, HDFS, Hadoop deployment, Hadoop configuration, Hadoop performance, Map waves},
abstract = {While the past decade has witnessed an unprecedented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm. Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce paradigm. Using as case study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practitioners.}
}
@article{OZDEN2016980,
title = {Using Knowledge-automation Expert Systems to Enhance the Use and Understanding of Traffic Monitoring Data in State DOTs},
journal = {Procedia Engineering},
volume = {145},
pages = {980-986},
year = {2016},
note = {ICSDEC 2016 – Integrating Data Science, Construction and Sustainability},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.04.127},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816301333},
author = {Abdulkadir Ozden and Ardeshir Faghri and Mingxin Li},
keywords = {Traffic Monitoring, Knowledge-Automation Expert Systems, Traffic Data Collection},
abstract = {Traffic monitoring is the process of observing and collecting data that describes the use and performance of the roadway systems. Strong and effective traffic monitoring program not only improves the mobility on roadways and reliability of decisions based on traffic data, but also ensures states are receiving appropriate federal funding. Rapidly evolving traffic data collection technologies; changing computing and communication technologies; receiving, processing and understanding of big data sets; economic, environmental and geographic constraints for data collection; and many other general or state specific limitations increased the complexity of traffic monitoring programs. This study aims to investigate the use of knowledge-automation expert systems in states’ traffic monitoring programs. This proposed approach is expected to increase the understanding and use of traffic data by providing situation specific advice based on user input and requests.}
}
@article{GANT2015S36,
title = {The importance of data quality to enhance the impact of omics sciences},
journal = {Toxicology Letters},
volume = {238},
number = {2, Supplement },
pages = {S36},
year = {2015},
note = {ABSTRACTS OF THE 51st Congress of the European Societies of Toxicology (EUROTOX)},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2015.08.097},
url = {https://www.sciencedirect.com/science/article/pii/S0378427415020378},
author = {T. Gant}
}
@article{STECKLER20151803,
title = {The preclinical data forum network: A new ECNP initiative to improve data quality and robustness for (preclinical) neuroscience},
journal = {European Neuropsychopharmacology},
volume = {25},
number = {10},
pages = {1803-1807},
year = {2015},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2015.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X15001674},
author = {Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov},
keywords = {Reproducibility, Robustness, Relevance, Quality assurance, Neuroscience, Pre-clinical},
abstract = {Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.}
}
@article{PRIYADARSHINI2015371,
title = {Semantic Retrieval of Relevant Sources for Large Scale Virtual Documents},
journal = {Procedia Computer Science},
volume = {54},
pages = {371-379},
year = {2015},
note = {Eleventh International Conference on Communication Networks, ICCN 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Data Mining and Warehousing, ICDMW 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Image and Signal Processing, ICISP 2015, August 21-23, 2015, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915013678},
author = {R. Priyadarshini and Latha Tamilselvan and T. Khuthbudin and S. Saravanan and S. Satish},
keywords = {Virtual documents (VD), Source document, Hadoop file System (HDFS), DW Ranking algorithm, Top  algorithm.},
abstract = {The term big data has come into use in recent years. It is used to refer to the ever-increasing amount of data that organizations are storing, processing and analyzing. An Interesting fact with bigdata is that it differ in Volume, Variety, Velocity characteristics which makes it difficult to process using the conventional Database Management System. Hence there is a need of schema less Management Systems even this will never be complete solution to bigdata analysis since the processing has no focus on the semantic information as they consider only the structural information. Content Management System like Wikipedia stores and links huge amount of documents and files. There is lack of semantic linking and analysis in such systems even though this kind of CMS uses clusters and distributed framework for storing big data. The retrieved references for a particular article are random and enormous. In order to reduce the number of references for a selected content there is a need for semantic matching. In this paper we propose framework which make use of the distributed parallel processing capability of Hadoop Distributed File System (HDFS) to perform semantic analysis over the volume of documents (bigdata) to find the best matched source document from the collection source documents for the same virtual document.}
}
@article{RAJESH201642,
title = {Forecasting supply chain resilience performance using grey prediction},
journal = {Electronic Commerce Research and Applications},
volume = {20},
pages = {42-58},
year = {2016},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2016.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1567422316300540},
author = {R. Rajesh},
keywords = {Big data, Grey theory, Grey prediction, Supply chain resilience, Resilience indicators},
abstract = {New digital technologies have empowered companies to make use of tools to produce insights for key supply chain processes. Resilience, the property of supply chains to respond better to disruptions, can be identified and evaluated using performance indicators. This research analyzes and predicts the indicators of firm resilience based on indicators from secondary data. These indicators can be measured on a regular basis based on performance in flexibility, responsiveness, quality, productivity and accessibility. Since source information for the data is often unknown, a methodology that is suited for prediction needs to be used. An improved grey prediction model is proposed in this research for forecasting the periodic indicators of resilience performance. This research shows that error measures ensure the best fit of the data to achieve strong prediction capability. A prediction model is applied to the supply chain of an Indian electronics manufacturer to forecast the measures of its resilience. Also, the results obtained were validated and have practical implications. For the supply chain that we studied, the indicators of flexibility, responsiveness and accessibility increased for the future periods, whereas the indicators for quality and productivity showed slight decreases. We argue that indicators with a negative trend should be given more attention by the firm. And managers should analyze the predicted values of resilience so they have an empirical basis for adjusting their strategies.}
}