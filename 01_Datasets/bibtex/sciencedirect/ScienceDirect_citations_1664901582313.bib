@incollection{ZEITOUNI2020159,
title = {Chapter 8 - Query Processing and Access Methods for Big Astro and Geo Databases},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {159-171},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00018-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000187},
author = {Karine Zeitouni and Mariem Brahem and Laurent Yeh and Atanas Hristov},
keywords = {Spatial databases, Spatial access methods, Query optimization, Big Data, System architecture},
abstract = {In spite of their development in different communities, either astro-informatics or geo-informatics, data management and analytics of astronomical and geospatial data share the same characteristics, and raise the same challenges when it comes to access, query, or analysis of the spatial features over Big Data. The very first challenge is to deal with the data volume, which is tremendous in many geo and astro datasets. In this chapter, we highlight their main specificity and outline the main steps of query processing in big geospatial and astronomical data servers. Through the review of the state of the art, we show the advance in the topic of Big Data management in both contexts of geospatial and sky surveying, while highlighting their similarity. This progress notwithstanding, several issues remain to deal with the variety (such as multidimensional arrays) of the data.}
}
@article{ECKARDT2020406,
title = {Opioid use disorder research and the Council for the Advancement of Nursing Science priority areas},
journal = {Nursing Outlook},
volume = {68},
number = {4},
pages = {406-416},
year = {2020},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2020.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0029655419306931},
author = {Patricia Eckardt and Donald Bailey and Holli A. DeVon and Cynthia Dougherty and Pamela Ginex and Cheryl A. Krause-Parello and Rita H. Pickler and Therese S. Richmond and Eleanor Rivera and Carol F. Roye and Nancy Redeker},
keywords = {Precision health, Big data and Data analytics, Determinants of health, Global health, Opioid use disorder research},
abstract = {Background
Chronic diseases, such as opioid use disorder (OUD) require a multifaceted scientific approach to address their evolving complexity. The Council for the Advancement of Nursing Science's (Council) four nursing science priority areas (precision health; global health, determinants of health, and big data/data analytics) were established to provide a framework to address current complex health problems.
Purpose
To examine OUD research through the nursing science priority areas and evaluate the appropriateness of the priority areas as a framework for research on complex health conditions.
Method
OUD was used as an exemplar to explore the relevance of the nursing science priorities for future research.
Findings
Research in the four priority areas is advancing knowledge in OUD identification, prevention, and treatment. Intersection of OUD research population focus and methodological approach was identified among the priority areas.
Discussion
The Council priorities provide a relevant framework for nurse scientists to address complex health problems like OUD.}
}
@article{ZHANG2020100715,
title = {Knowledge mapping of tourism demand forecasting research},
journal = {Tourism Management Perspectives},
volume = {35},
pages = {100715},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2020.100715},
url = {https://www.sciencedirect.com/science/article/pii/S2211973620300829},
author = {Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei},
keywords = {Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map, CiteSpace, Infographic},
abstract = {Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.}
}
@article{KATZ20208,
title = {A Novel Approach to Blood Smear Analysis Based on Specimen Topology: Implications for Human and Artificial Intelligence Decision Making},
journal = {Blood},
volume = {136},
pages = {8-9},
year = {2020},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood-2020-134903},
url = {https://www.sciencedirect.com/science/article/pii/S0006497118707672},
author = {Ben Zion Katz and Irit Avivi and Dan Benisty and Shahar Karni and Hadar Shimoni and Omri Grooper and Olga Pozdnyakova},
abstract = {Complete blood count (CBC) analysis is one of the most commonly ordered laboratory tests and is a critical first step in patients' clinical evaluation. However, CBC analyzers are limited in their ability to positively identify several types of white blood cells (WBC), and cells with substantial clinical significance, such as immature granulocytes or blasts, are merely marked as flags. Also, CBC analyzers fall short of recognizing informative red blood cell (RBC) morphology, such as schistocytes, and often provide inaccurate platelets count. Flags and clinically non-sufficient CBC-derived data reflex to generation of blood smear (BS), and BS review comprises a substantial portion of the workload in routine hematology laboratories. For accurate identification and classification of WBC, BS analysis (BSA) requires detailed observation of cells with high-magnification objective (60-100X), which provides a relatively narrow Field of View (FOV). This physical limitation restricts current BSA to either low resolution/wide FOV or to high resolution/narrow FOV data generation (Fig. 1A). Hence, key issues of BSA such as the effects of the smearing process on the distribution of blood components, the effects of cells distribution on their morphology and further classification, as well as many other attributes, are addressed only qualitatively or empirically, leaving the real topology of the BS obscure. The computational imaging microscopy system presented herein uses a low resolution and wide FOV objective, and records a plurality of images under different illumination conditions, of the same sample area (Fig. 1B). An algorithm reconstructs a high resolution and aberration free image of whole specimens, as can be observed in the attached link (https://tinyurl.com/Scopio-Labs-X100-ASH-2020). High resolution images are critical not only for manual BSA, but also for artificial intelligence (AI)-derived BSA, since data quality is of prime importance for deep-learning processes, and to a large extent determine their outcome. Thus, the combination of high resolution/wide FOV turns each BS into a big data analytic field, rendering the measurement of yet undetermined cell characteristics. In order to elucidate the basic topology, 60 normal BS (28 females, 32 males) were subjected to analysis utilizing this novel computational imaging microscopy. For convenience of analysis and comparison with current BSA methodology, BS were segmented into strips according to RBC density (Fig. 1C, D). The average length of smear from females (F) was higher by nearly 28% compared with smear from males (M), and the presence of acute inflammation (A) resulted in a significant 33% increase in overall smear length compared to normal (N) average (Fig. 1E). As expected, RBC density formed a linear gradient (Fig. 1C) along the axis of sample smearing, however, RBC morphology was affected by location within the BS. For example, strips 4-5 contained RBC with the appearance of spherocytes (Fig. 1F; arrows), while in strips with increased RBC density, cells aggregated resembling rouleaux formation (Fig. 1F; arrowheads). Platelets distribution was non-linear, with only a few of them reaching the feathered edge of the smear (Fig. 1G). Since the variance of both RBC/FOV and platelets/FOV concentrations drops starting with strip 4, BS-derived platelets number estimates should not be performed in strips 1-3. On average, a normal BS contains 890+399 WBC in the scanned area (strips 1-8). Similar to RBC, the location of individual WBC throughout the BS may affect their morphology, and hence their classification. WBC in the feathered edge (strips 1-3) are generally more stretched, and often squeezed between RBC, rendering their classification by AI-based tools challenging (Fig. 1H). In strips 4-7, WBC morphology is optimal for a classification task, enabling favorable outcomes for either manual or AI cell analysis (Fig. 1H). These data indicate that BSA can be taken to a sensitivity level of at least 10-3 of WBC analysis, provided that a large portion of the BS is scanned. Our system provides a novel combination of computational imaging microscopy and AI-based classification tools to unravel the complex topology of blood smears, and upgrade the data obtained in BSA. This approach enables the establishment of quantitative rules to scientifically direct the objective analysis of cellular blood components both manually, and by AI-tools. Figure
Disclosures
Katz: Scopio Labs: Consultancy.}
}
@incollection{KRISHNAN202099,
title = {5 - Pharmacy industry applications and usage},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {99-111},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000053},
author = {Krish Krishnan},
keywords = {Clinical Trials, Research, Multi-Teams, Non-Intrusive, Repeatable tests},
abstract = {One of the applications of big data applications and infrastructure is in the pharmaceutical industry. The complexity of the queries that are executed in these applications and the results they generate, make us feel the statement of torture the data and it will confess to anything. The relationships between the data in the different subject areas, the clinical trials and results, the communities in social media, the research labs and their outcomes, the clinical labs and patient results, and the financial outcomes of the pharmaceutical enterprise. Wow, think of all kinds of insights, add to this the markets, the competition, and the global industry, and we have phenomenal data to work with.}
}
@article{BENNETT2020102127,
title = {Is a pixel worth 1000 words? Critical remote sensing and China's Belt and Road Initiative},
journal = {Political Geography},
volume = {78},
pages = {102127},
year = {2020},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2019.102127},
url = {https://www.sciencedirect.com/science/article/pii/S0962629818305043},
author = {Mia M. Bennett},
keywords = {Critical remote sensing, Development, Night lights, Infrastructure, China, Belt and Road Initiative},
abstract = {As a novel means of researching China's Belt and Road Initiative (BRI), this article advances a critical remote sensing agenda that connects the view from above provided by satellite imagery with the grounded, qualitative methodologies more typical of political geography such as ethnographic fieldwork. Satellite imagery is widely used to produce empirics relating to the BRI, and the Chinese state is showing increasing interest in applying Earth observation data to governance. A more critical approach attentive to the politics of remote sensing, especially in light of China's emergence as a space and satellite power and its embrace of big data, is needed to more precisely reveal what changing pixels represent on the ground and expose the potential issues with data captured from high above the planet. This paper offers three theoretical and methodological objectives for critical remote sensing. First, I reflect on the geopolitics involved in the production and analysis of satellite imagery. Second, through analysis of night light imagery, which captures illuminated anthropogenic activities, I interrogate metanarratives of development. Third, I engage with qualitative methods by “ground-truthing” remote sensing with ethnographic observations along China's borders. I also seek to avoid the methodological nationalism often present in remote sensing research by situating these mixed-methods case studies at scales above and below the nation-state. As one of the largest development interventions in history materializes, pursuing critical remote sensing can create opportunities for social scientists to leverage quantitative and geospatial methods in support of more equitable and sustainable futures.}
}
@incollection{HOVENGA2020355,
title = {Chapter 11 - Measuring health service quality},
editor = {Evelyn J.S. Hovenga and Cherrie Lowe},
booktitle = {Measuring Capacity to Care Using Nursing Data},
publisher = {Academic Press},
pages = {355-388},
year = {2020},
isbn = {978-0-12-816977-3},
doi = {https://doi.org/10.1016/B978-0-12-816977-3.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128169773000113},
author = {Evelyn J.S. Hovenga and Cherrie Lowe},
keywords = {Performance measurement, Nursing ecosystem, Productivity, Nursing practice environments, Collegial culture, Accountability, Data quality, Data governance, Accreditation, Standards},
abstract = {Quality can be defined in multiple ways and is impacted by multiple factors. It applies to any operational process within health care and has a strong relationship with the performance of individual staff members as well as overall organizational performance outcomes. The characteristics of any nursing practice environment influence the quality of service provided. The ability to measure the quality of services provided is largely dependent upon the availability and type of data that can be accessed and processed. Meaningful measurement, trend analysis and monitoring to enable continuous improvements to be made, are only possible when governed data standards are used. This chapter has a strong focus on health and nursing, including acuity and clinical data use and provides global recommendations on health data, data standards and governance. Reference is made to other types of related standards, including accreditation standards and standards governance. The chapter concludes with an examination of various international and national outcomes research organizations, their comparative studies, and use of performance indicator data sets, clinical standards and guidelines, big data and secondary data use. Caring has been well defined yet doesn't appear to be routinely measured even though this is a major component directly impacting patient satisfaction.}
}
@article{KHAN202013,
title = {On the requirements of digital twin-driven autonomous maintenance},
journal = {Annual Reviews in Control},
volume = {50},
pages = {13-28},
year = {2020},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578820300560},
author = {Samir Khan and Michael Farnsworth and Richard McWilliam and John Erkoyuncu},
keywords = {Digital twin, Autonomous systems, Maintenance, Fault detection and isolation, Reinforcement learning},
abstract = {Autonomy has become a focal point for research and development in many industries. Whilst this was traditionally achieved by modelling self-engineering behaviours at the component-level, efforts are now being focused on the sub-system and system-level through advancements in artificial intelligence. Exploiting its benefits requires some innovative thinking to integrate overarching concepts from big data analysis, digitisation, sensing, optimisation, information technology, and systems engineering. With recent developments in Industry 4.0, machine learning and digital twin, there has been a growing interest in adapting these concepts to achieve autonomous maintenance; the automation of predictive maintenance scheduling directly from operational data and for in-built repair at the systems-level. However, there is still ambiguity whether state-of-the-art developments are truly autonomous or they simply automate a process. In light of this, it is important to present the current perspectives about where the technology stands today and indicate possible routes for the future. As a result, this effort focuses on recent trends in autonomous maintenance before moving on to discuss digital twin as a vehicle for decision making from the viewpoint of requirements, whilst the role of AI in assisting with this process is also explored. A suggested framework for integrating digital twin strategies within maintenance models is also discussed. Finally, the article looks towards future directions on the likely evolution and implications for its development as a sustainable technology.}
}
@article{LIN2020105305,
title = {The improvement of spatial-temporal resolution of PM2.5 estimation based on micro-air quality sensors by using data fusion technique},
journal = {Environment International},
volume = {134},
pages = {105305},
year = {2020},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2019.105305},
url = {https://www.sciencedirect.com/science/article/pii/S0160412018326552},
author = {Yuan-Chien Lin and Wan-Ju Chi and Yong-Qing Lin},
keywords = {PM, Micro-air quality sensors, Data fusion, Spatial-temporal estimation},
abstract = {With the rapid development of the Internet of things (IoTs) and modern industrial society, forecasting air pollution concentration, e.g., the concentration of PM2.5, is of great significance to protect human health and the environment. Accurate prediction of PM2.5 concentrations is limited by the number and the data quality of air quality monitoring stations. In Taiwan, the spatial and temporal data of PM2.5 concentrations are measured by 77 national air quality monitoring stations (built by Taiwan EPA). However, the national stations are costly and scarce because of the highly precise instrument and their size. Therefore, many places are still out of coverage of the monitoring network. Recently, under the framework of IoTs, there are hundreds of portable air quality sensors called “AirBox” developed jointly by the Taiwan local government and a private company. By virtue of its low price and portability, the AirBox can provide a higher resolution of space-time PM2.5 measurement. However, the spatiotemporal distribution is different between AirBox and EPA stations, and data quality and accuracy of AirBox is poorer than national air quality monitoring stations. Thus, to integrate the heterogeneous PM2.5 data, the data fusion technique should be used before further analysis. In this study, we propose a new data fusion method called multi-sensor space-time data fusion framework. It is based on the Optimum Linear Data Fusion theory and integrating with a multi-time step Kriging method for spatial-temporal estimation. The method is used to do heterogeneous data fusion from different sources and data qualities. It is able to improve the estimation of PM2.5 concentration in space and time. Results have shown that by combining PM2.5 concentration data from 1176 low-cost AirBoxes as additional information in our model, the estimation of spatial-temporal PM2.5 concentration becomes better and more reasonable. The r2 of the validation regression model is 0.89. Under the approach proposed in this study, we made the information of the micro-sensors more reliable and improved the higher spatial-temporal resolution of air quality monitoring. It could provide very useful information for better spatial-temporal data analysis and further environmental management, such as air pollution source localization, health risk assessment, and micro-scale air pollution analysis.}
}
@article{LI2020124178,
title = {Forecasting crude oil price with multilingual search engine data},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {551},
pages = {124178},
year = {2020},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2020.124178},
url = {https://www.sciencedirect.com/science/article/pii/S037843712030025X},
author = {Jingjing Li and Ling Tang and Shouyang Wang},
keywords = {Big data, Multilingual search engine index, Crude oil price forecasting, Google Trends, Artificial intelligence},
abstract = {In the big data era, search engine data (SED) have presented new opportunities for improving crude oil price prediction; however, the existing research were confined to single-language (mostly English) search keywords in SED collection. To address such a language bias and grasp worldwide investor attention, this study proposes a novel multilingual SED-driven forecasting methodology from a global perspective. The proposed methodology includes three main steps: (1) multilingual index construction, based on multilingual SED; (2) relationship investigation, between the multilingual index and crude oil price; and (3) oil price prediction, with the multilingual index as an informative predictor. With WTI spot price as studying samples, the empirical results indicate that SED have a powerful predictive power for crude oil price; nevertheless, multilingual SED statistically demonstrate better performance than single-language SED, in terms of enhancing prediction accuracy and model robustness.}
}
@article{JANSSEN2020101493,
title = {Data governance: Organizing data for trustworthy Artificial Intelligence},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101493},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101493},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20302719},
author = {Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski},
keywords = {Big data, Data governance, AI, Algorithmic governance, Information sharing, Artificial Intelligence, Trusted frameworks},
abstract = {The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.}
}
@article{ZHANG2020102659,
title = {Design and application of a personal credit information sharing platform based on consortium blockchain},
journal = {Journal of Information Security and Applications},
volume = {55},
pages = {102659},
year = {2020},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2020.102659},
url = {https://www.sciencedirect.com/science/article/pii/S2214212620308139},
author = {Jing Zhang and Rong Tan and Chunhua Su and Wen Si},
keywords = {Consortium blockchain, Personal credit reporting, Credit information sharing, Big data crediting},
abstract = {The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China’s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.}
}
@article{GHORBANIAN2020276,
title = {Improved land cover map of Iran using Sentinel imagery within Google Earth Engine and a novel automatic workflow for land cover classification using migrated training samples},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {167},
pages = {276-288},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620302008},
author = {Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou},
keywords = {Land cover classification, Sentinel, Google Earth Engine, Big data, Remote sensing, Iran},
abstract = {Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10 m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.}
}
@article{ROBERTSON2020214,
title = {An integrated environmental analytics system (IDEAS) based on a DGGS},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {162},
pages = {214-228},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620300502},
author = {Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts},
keywords = {DGGS, Data model, Big data, Spatial data, Analytics, Environment},
abstract = {Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS – integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.}
}
@article{REIS2020232,
title = {Assessing the drivers of machine learning business value},
journal = {Journal of Business Research},
volume = {117},
pages = {232-243},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320303581},
author = {Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro},
keywords = {Machine learning, Business value, Competitive advantage, Dynamic capabilities theory},
abstract = {Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.}
}
@article{AMOON2020107861,
title = {Internet of things sensor assisted security and quality analysis for health care data sets using artificial intelligent based heuristic health management system},
journal = {Measurement},
volume = {161},
pages = {107861},
year = {2020},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.107861},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120303997},
author = {Mohammed Amoon and Torki Altameem and Ayman Altameem},
keywords = {Artificial intelligent, IoT sensors, Health care data, Security and privacy},
abstract = {The developments in the medical systems, especially in health care management systems, play a vital role in patients. The effective management of health records leads to an increase in the importance of the healthcare management system all over the world. A real-time health monitoring system is a key zone for the Internet of Things (IoT) sensor technology in human services using Big Data Analytics. The major challenge that has to do with the health care data sets is security and privacy. In this paper, an artificial intelligence-based heuristic health management system has been designed and developed. This system is exceptionally close to improve the security and privacy of the live datasets of patients and the association of medicinal services over its different viewpoints. These services include the capacity for specialists, experts, attendants, and staff to settle on better decisions faster. Moreover, security and quality of data by configuration should be a part of any IoT use case, task or arrangement. Utilizing IoT assisted artificial intelligent based heuristic health management system intends to improve and minimize the security risk on health care data sets with assisted IoT sensors. The experimental results show promising outcomes in terms of various performance factors. The system attains precision as 99.75%, error rate as 0.0646 and predicted positive condition rate as 98.46%, Informedness as 98.6% and accuracy as 99.66%. The system is implemented using the MATLAB program.}
}
@article{ALTURJMAN2020357,
title = {Intelligence and security in big 5G-oriented IoNT: An overview},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {357-368},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301074},
author = {Fadi Al-Turjman},
keywords = {IoNT, Security, Big data, Design factors},
abstract = {Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.}
}
@article{AGANY20201704,
title = {Assessment of vector-host-pathogen relationships using data mining and machine learning},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {1704-1721},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S2001037020303202},
author = {Diing D.M. Agany and Jose E. Pietri and Etienne Z. Gnimpieba},
keywords = {Systems Bioscience, OMICs, Pathogenicity, Transmission, Adaptation, Data Mining, Big Data, Machine Learning, Association Mining, Host-Pathogen, Interaction, Infectious Disease, Vector-Borne Disease},
abstract = {Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.}
}
@incollection{KRISHNAN202085,
title = {4 - Scientific research applications and usage},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {85-97},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000041},
author = {Krish Krishnan},
keywords = {CERN, God particle, Large electron–positron collider, Large hadron collider, Quarks, Scientific research, Standard model},
abstract = {Scientific research is one area of applications and usage of big data where we can generate lots of data in a single experiment and perform complex analytics on the same in the outcome of that experiment. The most famous example that we can talk about is the usage of all infrastructure technologies in the discovery of the “God particle” or “Higgs boson particle” which is leading us to uncover more exploration around the universe.}
}
@incollection{LEE2020213,
title = {Chapter 8 - Industrial AI and predictive analytics for smart manufacturing systems},
editor = {Masoud Soroush and Michael Baldea and Thomas F. Edgar},
booktitle = {Smart Manufacturing},
publisher = {Elsevier},
pages = {213-244},
year = {2020},
isbn = {978-0-12-820027-8},
doi = {https://doi.org/10.1016/B978-0-12-820027-8.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200278000083},
author = {Jay Lee and Jaskaran Singh and Moslem Azamfar and Vibhor Pandhare},
keywords = {Industrial AI, Smart manufacturing, Industry 4.0, Cyberphysical systems, Enabling technologies},
abstract = {Today, the manufacturing industry is aiming to improve competitiveness by integrating capabilities such as sensing, communication, big data analytics, and cloud computing technologies in order to secure a new growth engine. This integration is colloquially known as a “smart industry.” To enable the realization of smart manufacturing systems, a clear roadmap for systematic development and implementation of artificial intelligence (AI) in industrial systems is necessary and for that, the concept of Industrial AI would play an important role. This chapter presents a comprehensive overview of the important role of key enabling technologies of Industrial AI in the manufacturing industry in general and how their systematic adoption will aid in producing new value-creation opportunities and avoidance of problems that haven’t even occurred yet.}
}
@article{QI202052,
title = {FRIEND: Feature selection on inconsistent data},
journal = {Neurocomputing},
volume = {391},
pages = {52-64},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.01.094},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220301521},
author = {Zhixin Qi and Hongzhi Wang and Tao He and Jianzhong Li and Hong Gao},
keywords = {Feature selection, Inconsistent data, Mutual information, Data quality, Approximation},
abstract = {With the explosive growth of information, inconsistent data are increasingly common. However, traditional feature selection methods are lack of efficiency due to inconsistent data repairing beforehand. Therefore, it is necessary to take inconsistencies into consideration during feature selection to not only reduce time costs but also guarantee accuracy of machine learning models. To achieve this goal, we present FRIEND, a feature selection approach on inconsistent data. Since features in consistency rules have higher correlation with each other, we aim to select a specific amount of features from these. We prove that the specific feature selection problem is NP-hard and develop an approximation algorithm for this problem. Extensive experimental results demonstrate the efficiency and effectiveness of our proposed approach.}
}
@article{GE20201883,
title = {Developing the Quality Model for Collaborative Open Data},
journal = {Procedia Computer Science},
volume = {176},
pages = {1883-1892},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.228},
url = {https://www.sciencedirect.com/science/article/pii/S187705092032130X},
author = {Mouzhi Ge and Włodzimierz Lewoniewski},
keywords = {Data Quality, Quality Assessment, Collaborative Open Data, Wikipedia, Quality Model},
abstract = {Nowadays, the development of data sharing technologies allows to involve more people to collaboratively contribute knowledge on the Web. The shared knowledge is usually represented as Collaborative Open Data (COD), for example, Wikipedia is one of the well-known sources for COD. The Wikipedia articles can be written in different languages, updated in real time, and originated from a vast variety of editors. However, COD also bring different data quality problems such as data inconsistency and low data objectiveness due to the crowd-based and dynamic nature. These data quality problems such as biased information may lead to sentimental changes or social impacts. This paper therefore proposes a new measurement model to assess the quality of COD. In order to evaluate the proposed model, A preliminary experiment is conducted with a large scale of Wikipedia articles to validate the applicability and efficiency of this proposed quality model in the real-world scenario.}
}
@article{GARCIA2020113526,
title = {Automatic alarm prioritization by data mining for fault management in cellular networks},
journal = {Expert Systems with Applications},
volume = {158},
pages = {113526},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113526},
url = {https://www.sciencedirect.com/science/article/pii/S095741742030350X},
author = {Antonio J. García and Matías Toril and Pablo Oliver and Salvador Luna-Ramírez and Manuel Ortiz},
keywords = {Network management, Automation, Fault management, Data mining, Machine learning},
abstract = {Network management systems play an important role to deal with the large size and complexity of current cellular networks. Thus, operators and vendors focus much of their efforts on developing new techniques and tools for network management. One of the most critical processes in network management is fault management, since a failure in a network element might have a strong impact on user satisfaction due to service degradation. Unfortunately, cellular networks generate thousands of alarms daily, which have to be checked manually by operator personnel. With the latest advances in big data analytics, different methods for reducing the number of alarms to be monitored have been proposed in the literature. In this work, an automatic method for prioritizing alarms based on the need for specialized personnel is presented. The core of the method is an ensemble model built with supervised learning that estimates the probability that an alarm generates a trouble ticket. The model is trained with trouble ticket data from the network operation center. A performance comparison of four classical base classifiers (naïve Bayes, random forest, artificial neural network and support vector machine) for the ensemble is presented. The model is implemented in IBM SPSS Modeler and tested with a real alarm and trouble ticket dataset taken from a live cellular network. Results show that the proposed model correctly flags those alarms that need further analysis by the operator and filter out those alarms that do not have impact on network performance. The main contribution of this work is unveiling a new application (the automatic prioritization of alarms in a cellular network based on the need for specialized personnel) and presenting for the first time a performance comparison of base classifiers used for this purpose (since the required dataset is extremely difficult to find for privacy reasons).}
}
@article{LEE2020157,
title = {Machine learning for enterprises: Applications, algorithm selection, and challenges},
journal = {Business Horizons},
volume = {63},
number = {2},
pages = {157-170},
year = {2020},
note = {ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2019.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0007681319301521},
author = {In Lee and Yong Jae Shin},
keywords = {Machine learning, Artificial intelligence, Deep learning, Big data, Neural networks, Chatbot, Innovation capability, Resources and capabilities},
abstract = {Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.}
}
@article{JUNG2020112,
title = {Ten-year patient journey of stage III non-small cell lung cancer patients: A single-center, observational, retrospective study in Korea (Realtime autOmatically updated data warehOuse in healTh care; UNIVERSE-ROOT study)},
journal = {Lung Cancer},
volume = {146},
pages = {112-119},
year = {2020},
issn = {0169-5002},
doi = {https://doi.org/10.1016/j.lungcan.2020.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0169500220304670},
author = {Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park},
keywords = {Real-time updated system, Big data, Real-world data, NSCLC, Treatment},
abstract = {Introduction
Until the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.
Methods
This non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients’ characteristics, and assessment of treatment outcomes.
Results
We analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N = 4138, 18.2 %) were diagnosed as IIIA (N = 2,547, 11.2 %) or IIIB (N = 1,591. 7.0 %). Treated stage III patients (N = 2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N = 1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N = 648, 25.6 %); palliative CT (N = 270, 10.7 %), or thoracic RT (N = 170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0–56.5), 52.5 (43.1–61.9), 30.3 (26.6–34.0), 14.7 (13.0–16.4), 8.8 (6.2–11.3), and 2.0 (1.0–3.0) months, respectively.
Conclusions
This unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.}
}
@article{LIU2020263,
title = {Super Resolution Perception for Smart Meter Data},
journal = {Information Sciences},
volume = {526},
pages = {263-273},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.03.088},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520302681},
author = {Guolong Liu and Jinjin Gu and Junhua Zhao and Fushuan Wen and Gaoqi Liang},
keywords = {Super resolution perception, Smart meter data, High-frequency data, Big data analysis},
abstract = {In this paper, we present the problem formulation and methodology framework of Super Resolution Perception (SRP) on smart meter data. With the widespread use of smart meters, a massive amount of electricity consumption data can be obtained. Smart meter data is the basis of automated billing and pricing, appliance identification, demand response, etc. However, the provision of high-quality data may be expensive in many cases. In this paper, we propose a novel problem - the SRP problem as reconstructing high-quality data from unsatisfactory data in smart grids. Advanced generative models are then proposed to solve the problem. This technology makes it possible for empowering existing facilities without upgrading existing meters or deploying additional meters. We first mathematically formulate the SRP problem under the Maximum a Posteriori (MAP) estimation framework. The dataset namely Super Resolution Perception Dataset (SRPD) is designed for this problem and released. A case study is then presented, which performs SRP on smart meter data. A network namely Super Resolution Perception Convolutional Neural Network (SRPCNN) is proposed to generate high-frequency load data from low-frequency data. Experiments demonstrate that our SRP models can reconstruct high-frequency data effectively. Moreover, the reconstructed high-frequency data can lead to better appliance identification results.}
}
@article{HAMILTON2020103926,
title = {Fast and automated sensory analysis: Using natural language processing for descriptive lexicon development},
journal = {Food Quality and Preference},
volume = {83},
pages = {103926},
year = {2020},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2020.103926},
url = {https://www.sciencedirect.com/science/article/pii/S0950329319308304},
author = {Leah M. Hamilton and Jacob Lahne},
keywords = {Natural language processing, Rapid descriptive methods, Big data, Whisky, Research methodology, Machine learning},
abstract = {As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.}
}
@article{LU2020101837,
title = {Digital Twin-driven smart manufacturing: Connotation, reference model, applications and research issues},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {61},
pages = {101837},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101837},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519302480},
author = {Yuqian Lu and Chao Liu and Kevin I-Kai Wang and Huiyue Huang and Xun Xu},
keywords = {Smart manufacturing, Digital Twin, Industry 4.0, Cyber-physical System, Big Data, Standard},
abstract = {This paper reviews the recent development of Digital Twin technologies in manufacturing systems and processes, to analyze the connotation, application scenarios, and research issues of Digital Twin-driven smart manufacturing in the context of Industry 4.0. To understand Digital Twin and its future potential in manufacturing, we summarized the definition and state-of-the-art development outcomes of Digital Twin. Existing technologies for developing a Digital Twin for smart manufacturing are reviewed under a Digital Twin reference model to systematize the development methodology for Digital Twin. Representative applications are reviewed with a focus on the alignment with the proposed reference model. Outstanding research issues of developing Digital Twins for smart manufacturing are identified at the end of the paper.}
}
@article{JIN2020112412,
title = {Artificial intelligence biosensors: Challenges and prospects},
journal = {Biosensors and Bioelectronics},
volume = {165},
pages = {112412},
year = {2020},
issn = {0956-5663},
doi = {https://doi.org/10.1016/j.bios.2020.112412},
url = {https://www.sciencedirect.com/science/article/pii/S0956566320304061},
author = {Xiaofeng Jin and Conghui Liu and Tailin Xu and Lei Su and Xueji Zhang},
keywords = {Wearable biosensor, Artificial intelligence, Biomarker, Wireless communication, Machine learning, Healthcare},
abstract = {Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.}
}
@article{QIU2020115,
title = {Research on Cost Management Optimization of Financial Sharing Center Based on RPA},
journal = {Procedia Computer Science},
volume = {166},
pages = {115-119},
year = {2020},
note = {Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920301538},
author = {Yu Lian Qiu and Guo Fang Xiao},
keywords = {RPA, Financial shared service center, Cost management, Process optimization, Big data},
abstract = {With the development of artificial intelligence technology, the widespread application of robot process automation (RPA) in the future financial field has become an inevitable trend. Through the review of the current situation of cost management of A Group’s financial shared service center, the article deeply expounds the problems that the current cross-system data cannot be automatically collected, the cost accounting is not timely, and the cost analysis report mode is too fixed. Based on Robot Process Automation (RPA), cost management process optimization and improvement were made on the cross-system data acquisition, "Cloud Purchasing Platform" construction, and comprehensive multi-dimensional cost analysis. It is expected to provide reference for the robot process automation application of the financial shared service center.}
}
@article{WANG2020107259,
title = {Use of AIS data for performance evaluation of ship traffic with speed control},
journal = {Ocean Engineering},
volume = {204},
pages = {107259},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.107259},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820303085},
author = {Likun Wang and Yang Li and Zheng Wan and Zaili Yang and Tong Wang and Keping Guan and Lei Fu},
keywords = {Yangtze river (Shanghai section) strait, Speed limit regulation, Normal distribution fitting, AIS data, Maritime safety},
abstract = {Speed control in inland water systems needs to achieve effective balance between ship operational efficiency and transport safety. However, speed limit regulations are largely formulated through expert judgment rather than objective evidence-based evaluation, which sometimes leads to inefficiency due to subjective bias. In this study, a new method is proposed to evaluate the performance of shipping traffic under current speed limits by using the automatic identification system (AIS) big data of 4923 ships in the Shanghai section of the Yangtze River in China. The key elements of this method include data acquisition, error elimination, combination of ship AIS and waterway geocoded data to model traffic flow characteristics, and estimation of the correlation between ship speed and congestion level. Shipping traffic performance in different segments is analyzed. Results reveal that the overall compliance to the speed limit is high, and only a few over-speeding cases are noted in certain segments. Furthermore, we use a normal distribution to model the correlation between ship speed and traffic volume. The findings indicate that the current speed limit in the Shanghai section of Yangtze River is rational. This work provides useful insights into testing the rationality of speed limits in other waterways or shipping channels.}
}
@article{KEBISEK202011168,
title = {Artificial Intelligence Platform Proposal for Paint Structure Quality Prediction within the Industry 4.0 Concept},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11168-11174},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.299},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320305796},
author = {M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec},
keywords = {artificial intelligence, automotive, big data analytics, industry 4.0, knowledge discovery, neural networks, prediction, principal component analysis},
abstract = {This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.}
}
@article{SAVOLAINEN202095,
title = {Organisational Constraints in Data-driven Maintenance: a case study in the automotive industry},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {95-100},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301592},
author = {P Savolainen and J Magnusson and M. Gopalakrishnan and E. {Turanoglu Bekar} and A. Skoogh},
keywords = {Maintenance Management, Decision Support, Data Quality, Data-driven Decisions, Organisational Factors, Smart Maintenance},
abstract = {Technological development and innovations has been the focus of research in the field of smart maintenance, whereas there is less research regarding how maintenance organisations adapt the development. This case study focuses to understand what constraints maintenance organisations in the transition into applying more data-driven decisions in maintenance. This paper aims to emphasize the organisational challenges in data-driven maintenance, such as trustworthiness of data-driven decisions, data quality, management and competences. Through a case study at a global company in the automotive industry these challenges are highlighted and discussed through a questionnaire survey participated by 72 people and interviews with 7 people from the maintenance organisation.}
}
@article{SUGDEN2020100014,
title = {Patterns of Reliability: Assessing the Reproducibility and Integrity of DNA Methylation Measurement},
journal = {Patterns},
volume = {1},
number = {2},
pages = {100014},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100014},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920300143},
author = {Karen Sugden and Eilis J. Hannon and Louise Arseneault and Daniel W. Belsky and David L. Corcoran and Helen L. Fisher and Renate M. Houts and Radhika Kandaswamy and Terrie E. Moffitt and Richie Poulton and Joseph A. Prinz and Line J.H. Rasmussen and Benjamin S. Williams and Chloe C.Y. Wong and Jonathan Mill and Avshalom Caspi},
keywords = {DSML 3:  Data science output has been rolled out/validated across multiple domains/problems},
abstract = {Summary
DNA methylation plays an important role in both normal human development and risk of disease. The most utilized method of assessing DNA methylation uses BeadChips, generating an epigenome-wide “snapshot” of >450,000 observations (probe measurements) per assay. However, the reliability of each of these measurements is not equal, and little consideration is paid to consequences for research. We correlated repeat measurements of the same DNA samples using the Illumina HumanMethylation450K and the Infinium MethylationEPIC BeadChips in 350 blood DNA samples. Probes that were reliably measured were more heritable and showed consistent associations with environmental exposures, gene expression, and greater cross-tissue concordance. Unreliable probes were less replicable and generated an unknown volume of false negatives. This serves as a lesson for working with DNA methylation data, but the lessons are equally applicable to working with other data: as we advance toward generating increasingly greater volumes of data, failure to document reliability risks harming reproducibility.}
}
@article{AZAR2020222,
title = {Robust IoT time series classification with data compression and deep learning},
journal = {Neurocomputing},
volume = {398},
pages = {222-234},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.02.097},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220302939},
author = {Joseph Azar and Abdallah Makhoul and Raphaël Couturier and Jacques Demerjian},
keywords = {IoT applications, Energy efficiency, Lossy compression, Data reduction, Time series classification, Deep neural networks, Discrete wavelet transform, lifting scheme},
abstract = {Internet of Things (IoT) and wearable systems are very resource limited in terms of power, memory, bandwidth and processor performance. Sensor time series compression can be regarded as a direct way to use memory and bandwidth resources efficiently. On the other hand, the time series classification has recently attracted great attention and has found numerous potential uses in areas such as finance, industry and healthcare. This paper investigates the effect of lossy compression techniques on the time series classification task using deep neural networks. Furthermore, this paper proposes an efficient compression approach for univariate and multivariate time series that combines the lifting implementation of the discrete wavelet transform with an error-bound compressor, namely Squeeze (SZ), to attain an optimal trade-off between data compression and data quality.}
}
@article{ABDULAZIZ202095,
title = {The effective seismic attributes in porosity prediction for different rock types: Some implications from four case studies},
journal = {Egyptian Journal of Petroleum},
volume = {29},
number = {1},
pages = {95-104},
year = {2020},
issn = {1110-0621},
doi = {https://doi.org/10.1016/j.ejpe.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S111006211930412X},
author = {Abdulaziz M. Abdulaziz},
keywords = {Porosity prediction, Seismic attributes, PNN, Al Ghani field, Sooner field, Kifl field, Baltim field},
abstract = {The present study evaluates the performance of PNN models for porosity prediction using seismic attributes. Four seismic datasets and more than 20 wells from different sedimentary basins located in Libya, Iraq, Egypt and USA are employed to characterize the effective attributes for porosity prediction. Verification and testing error analysis is adopted for evaluating the prediction performance. Results indicated that the porosity prediction models are primarily dependent to the propagation related attributes with frequency attributes as the most effective parameters in porosity prediction. In addition, the data quality and processing history strongly control the prediction model performance with relatively limited effects to dataset dimensionality (2D versus 3D) and the number of wells utilized in model construction. Such results are important to better understand and evaluate the performance of PNN porosity prediction models using various seismic attributes.}
}
@article{CAMPOS2020103959,
title = {Data preprocessing for multiblock modelling – A systematization with new methods},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {199},
pages = {103959},
year = {2020},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2020.103959},
url = {https://www.sciencedirect.com/science/article/pii/S0169743919305350},
author = {Maria P. Campos and Marco S. Reis},
keywords = {Multiblock methods, Data preprocessing, Partial Least Squares, Data science, Big data, Industry 4.0},
abstract = {With the advance of Industry 4.0, new data collectors are appearing at different points of the process generating blocks of data whose integrity should be preserved during data analysis. This is the scope of multiblock methods, whose potential has been recognized in several areas of application where they are becoming increasingly popular. Multiblock methods can be applied to a wide range of data-driven problems that practitioners face nowadays such as plant-wide process monitoring and diagnosis, process optimization and quality prediction of key product properties. These methods have the ability to find associations and interpretative connections between different data blocks from different sources and carrying complementary or overlapping information, as well as assessing the blocks’ relative contributions to the final outcome. A critical stage in the application of multiblock methods is the selection of the appropriate preprocessing to apply to each block, before proceeding to the modelling. The preprocessing strategy can exponentiate the information extracted from the blocks and their mutual interactions or hide/mask/distort them if inappropriately done. In this article, we present a systematic workflow where both the intra-block and inter-block variation components are considered during preprocessing. We illustrate the application of the framework using two real case studies where a critical comparison is presented for the different preprocessing alternatives.}
}
@article{ESCOBAR2020103378,
title = {Adding value to Linked Open Data using a multidimensional model approach based on the RDF Data Cube vocabulary},
journal = {Computer Standards & Interfaces},
volume = {68},
pages = {103378},
year = {2020},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.103378},
url = {https://www.sciencedirect.com/science/article/pii/S0920548919300480},
author = {Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such and Jesús Peral},
keywords = {Linked Open Data, Multidimensional modelling, Conceptual modelling, RDF Data Cube vocabulary, Semantic web, Big data},
abstract = {Most organisations using Open Data currently focus on data processing and analysis. However, although Open Data may be available online, these data are generally of poor quality, thus discouraging others from contributing to and reusing them. This paper describes an approach to publish statistical data from public repositories by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in order to facilitate the analysis of multidimensional models. We have defined a framework based on the entire lifecycle of data publication including a novel step of Linked Open Data assessment and the use of external repositories as knowledge base for data enrichment. As a result, users are able to interact with the data generated according to the RDF Data Cube vocabulary, which makes it possible for general users to avoid the complexity of SPARQL when analysing data. The use case was applied to the Barcelona Open Data platform and revealed the benefits of the application of our approach, such as helping in the decision-making process.}
}
@article{CHUNG2020101837,
title = {Data science and analytics in aviation},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {134},
pages = {101837},
year = {2020},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.101837},
url = {https://www.sciencedirect.com/science/article/pii/S1366554520300077},
author = {Sai-Ho Chung and Hoi-Lam Ma and Mark Hansen and Tsan-Ming Choi},
keywords = {Data science, Aviation, Analytics, Flight, Air logistics},
abstract = {Data science and analytics are attracting more and more attention from researchers and practitioners in recent years. Due to the rapid development of advanced technologies nowadays, a massive amount of real time data regarding flight information, flight performance, airport conditions, air traffic conditions, weather, ticket prices, passengers comments, crew comments, etc., are all available from a diverse set of sources, including flight performance monitoring systems, operational systems of airlines and airports, and social media platforms. Development of data analytics in aviation and related applications is also growing rapidly. This paper concisely examines data science and analytics in aviation studies in several critical areas, namely big data analysis, air transport network management, forecasting, and machine learning. The papers featured in this special issue are also introduced and reviewed, and future directions for data science and analytics in aviation are discussed.}
}
@article{FAGUNDES202063,
title = {Decision-making models and support systems for supply chain risk: literature mapping and future research agenda},
journal = {European Research on Management and Business Economics},
volume = {26},
number = {2},
pages = {63-70},
year = {2020},
issn = {2444-8834},
doi = {https://doi.org/10.1016/j.iedeen.2020.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2444883418302602},
author = {Marcus Vinicius Carvalho Fagundes and Eduardo Oliveira Teles and Silvio A.B. {Vieira de Melo} and Francisco Gaudêncio Mendonça Freires},
keywords = {risk model, multicriteria decision, stochastic and computational model, bibliometrics},
abstract = {Supply chain disruptions have serious consequences for society and this has made supply chain risk management (SCRM) an attractive area for researchers and managers. In this paper, we use an objective literature mapping approach to identify, classify, and analyze decision-making models and support systems for SCRM, providing an agenda for future research. Through bibliometric networks of articles published in the Scopus database, we analyze the most influential decision-making models and support systems for SCRM, evaluate the main areas of current research, and provide insights for future research in this field. The main results are the following: we found that the identity of the area is structured in three groups of risk decision support models: (i) quantitative multicriteria decision models, (ii) stochastic decision-making models, and (iii) computational simulation/optimization models. We mapped six current research clusters: (i) conceptual and qualitative risk models, (ii) upstream supply chain risk models, (iii) downstream supply chain risk models, (iv) supply chain sustainability risk models, (v) stochastic and multicriteria decision risk models, and (vi) emerging techniques risk models. We identified seven future research clusters, with insights from further studies for: (i) tools to operate SCRM data, (ii) validation of risk models, (iii) computational improvement for data analysis, (iv) multi-level and multi-period supply chains, (v) agrifood risks, (vi) energy risks and (vii) sustainability risks. Finally, the future research agenda should prioritize SCRM's holistic vision, the relationship between Big Data, Industry 4.0 and SCRM, as well as emerging social and environmental risks.}
}
@article{PIRI2020113339,
title = {Missing care: A framework to address the issue of frequent missing values;The case of a clinical decision support system for Parkinson's disease},
journal = {Decision Support Systems},
volume = {136},
pages = {113339},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113339},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620300944},
author = {Saeed Piri},
keywords = {Electronic health records, Data missing values, Clinical decision support systems, Predictive healthcare analytics, Imbalanced data learning, Parkinson's disease},
abstract = {In recent decades, the implementation of electronic health record (EHR) systems has been evolving worldwide, leading to the creation of immense data volume in healthcare. Moreover, there has been a call for research studies to enhance personalized medicine and develop clinical decision support systems (CDSS) by analyzing the available EHR data. In EHR data, usually, there are millions of patients records with hundreds of features collected over a long period of time. This enormity of EHR data poses significant challenges, one of which is dealing with many variables with very high degrees of missing values. In this study, the data quality issue of incompleteness in EHR data is discussed, and a framework called ‘Missing Care’ is introduced to address this issue. Using Missing Care, researchers will be able to select the most important variables at an acceptable missing values degree to develop predictive models with high predictive power. Moreover, Missing Care is applied to analyze a unique, large EHR data to develop a CDSS for detecting Parkinson's disease. Parkinson is a complex disease, and even a specialist's diagnosis is not without error. Besides, there is a lack of access to specialists in more remote areas, and as a result, about half of the patients with Parkinson's disease in the US remain undiagnosed. The developed CDSS can be integrated into EHR systems or utilized as an independent tool by healthcare practitioners who are not necessarily specialists; therefore, making up for the limited access to specialized care in remote areas.}
}
@article{WORTHENCHAUDHARI2020109726,
title = {In-home neurogaming: Demonstrating the impact of valid gesture recognition method on high volume kinematic outcomes},
journal = {Journal of Biomechanics},
volume = {104},
pages = {109726},
year = {2020},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2020.109726},
url = {https://www.sciencedirect.com/science/article/pii/S0021929020301421},
author = {Lise C. Worthen-Chaudhari and Michael P. McNally and Akshay Deshpande and Vivek Bakaraju},
abstract = {The process of cleaning motion capture data of aberrant points has been described as “the bane of motion capture operators”. Yet, managing the high volume kinematic data generated through in-home neurogames requires data quality control that, executed insufficiently, jeopardizes accuracy of outcomes. To begin to address this issue at the intersection of biomechanics and “big data”, we performed a secondary analysis of a neurogame, evaluating gesture count as well as shoulder and elbow joint angle outcomes calculated from kinematic data in which valid gestures were identified through 3 methods: visual review of regions of interest by an expert (BP); manufacturer-recommended data smoothing (MS); and automated methods (AI). We hypothesized that upper extremity kinematic outcomes from BP would be matched by AI but not MS methods. From one person with post-stroke hemiparesis, upper-extremity kinematic data were collected for 6 days over 2 weeks using a Microsoft Kinect™-based neurogame. We calculated gesture count, shoulder angle, and elbow angle outcomes from data managed using BP, MS, and AI methods. BP identified 1929 valid gestures total over 6 days which was different than the other two methods (p = 0.0015). In contrast, the AI algorithm with best precision identified 4372 and MS identified 4459 valid gestures. Furthermore, angle outcomes calculated from AI and MS methods resulted in different values than BP (p < 0.001 for 5 of 6 variables). More research is needed to automate treatment of high volume, low quality motion data to support investigation of motion associated with in-home rehabilitation neurogames.}
}
@article{MA2020109941,
title = {A bi-directional missing data imputation scheme based on LSTM and transfer learning for building energy data},
journal = {Energy and Buildings},
volume = {216},
pages = {109941},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.109941},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819333717},
author = {Jun Ma and Jack C.P. Cheng and Feifeng Jiang and Weiwei Chen and Mingzhu Wang and Chong Zhai},
keywords = {Bi-directional estimation, Building energy, Deep learning, Electric power, Missing data, Transfer learning},
abstract = {Improving the energy efficiency of the buildings is a worldwide hot topic nowadays. To assist comprehensive analysis and smart management, high-quality historical data records of the energy consumption is one of the key bases. However, the energy data records in the real world always contain different kinds of problems. The most common problem is missing data. It is also one of the most frequently reported data quality problems in big data/machine learning/deep learning related literature in energy management. However, limited studied have been conducted to comprehensively discuss different kinds of missing data situations, including random missing, continuous missing, and large proportionally missing. Also, the methods used in previous literature often rely on linear statistical methods or traditional machine learning methods. Limited study has explored the feasibility of advanced deep learning and transfer learning techniques in this problem. To this end, this study proposed a methodology, namely the hybrid Long Short Term Memory model with Bi-directional Imputation and Transfer Learning (LSTM-BIT). It integrates the powerful modeling ability of deep learning networks and flexible transferability of transfer learning. A case study on the electric consumption data of a campus lab building was utilized to test the method. Results show that LSTM-BIT outperforms other methods with 4.24% to 47.15% lower RMSE under different missing rates.}
}
@article{LUO2020103945,
title = {Cross-scale characterization of the elasticity of shales: Statistical nanoindentation and data analytics},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {140},
pages = {103945},
year = {2020},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2020.103945},
url = {https://www.sciencedirect.com/science/article/pii/S0022509620301812},
author = {Shengmin Luo and Yunhu Lu and Yongkang Wu and Jinliang Song and Don J. DeGroot and Yan Jin and Guoping Zhang},
keywords = {Data analytics, Nanoindentation, Shale, Surround effect, Young's modulus},
abstract = {Shales are a class of multiscale, multiphase, hybrid inorganic-organic composite materials exhibiting both frictional and cohesive behavior, and it is very challenging to characterize and interpret their complex mechanical properties. A statistical nanoindentation approach with pertinent viable data analytics was developed to probe the mechanical properties of shales across different length scales. Grid nanoindentation experiments with continuous stiffness measurement performed on shales to relatively large depths of 6–8 µm obtained massive data, which were processed by the new data analytics: segmentation at selected depths of a great number (e.g., >500) of continuous Young's modulus versus indentation depth curves obtained from unknown constituent phases yielded multiple discretized sub-datasets that were processed to extract individual phases’ elastic moduli at respective segmentation depths via probability density function (PDF)-based deconvolution; these depth-dependent Young's moduli of each phase were then fitted by a newly proposed surround effect model, leading to determination of the properties of both individual phases at the nano/micro-scales (i.e., virtually infinitesimal depths) and the bulk rock at the macroscale (i.e., ~10–100 µm depths). A significant advantage of this massive data-based indentation approach is that the mechanical properties of composite materials such as shales can be probed across different scales by a single measurement technique. In addition, a new criterion, termed Bin Size Index, was formulated for selecting depth-dependent, rational, optimized bin sizes for PDF construction. For the studied shales, results show that five mechanically-distinct phases are discerned, including a virtual interface phase between hard and soft constituents accounting for a majority of indents. Coincidently, the Young's modulus of the bulk rock is nearly the same as that of the interface phase, suggesting that the macroscopic properties of similar composites may be estimated from measurements on the interface of two phases with contrasting mechanical properties. Finally, this approach can guide the selection of appropriate indentation depths to probe the mechanical properties of both highly heterogeneous bulk materials at the macroscale and their individual constituent phases at the nano/micro-scale.}
}
@article{ZHAO2020264,
title = {The Value of the Surgeon Informatician},
journal = {Journal of Surgical Research},
volume = {252},
pages = {264-271},
year = {2020},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022480420302079},
author = {Jane Zhao and Raquel Forsythe and Alexander Langerman and Genevieve B. Melton and David F. Schneider and Gretchen Purcell Jackson},
keywords = {Clinical informatics, Surgery, Health information technology, Interoperability, Telemedicine, Clinical decision support},
abstract = {Clinical informatics is an interdisciplinary specialty that leverages big data, health information technologies, and the science of biomedical informatics within clinical environments to improve quality and outcomes in the increasingly complex and often siloed health care systems. Core competencies of clinical informatics primarily focus on clinical decision making and care process improvement, health information systems, and leadership and change management. Although the broad relevance of clinical informatics is apparent, this review focuses on its application and pertinence to the discipline of surgery, which is less well defined. In doing so, we hope to highlight the importance of the surgeon informatician. Topics covered include electronic health records, clinical decision support systems, computerized order entry, data analytics, clinical documentation, information architectures, implementation science, quality improvement, simulation, education, and telemedicine. The formal pathway for surgeons to become clinical informaticians is also discussed.}
}
@article{RETTIG2020e03829,
title = {The Female Empowerment Index (FEMI): spatial and temporal variation in women's empowerment in Nigeria},
journal = {Heliyon},
volume = {6},
number = {5},
pages = {e03829},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2020.e03829},
url = {https://www.sciencedirect.com/science/article/pii/S2405844020306745},
author = {Erica M. Rettig and Stephen E. Fick and Robert J. Hijmans},
keywords = {Africa, Nigeria, Women's empowerment, Demographic and health surveys, Gender inequality, Machine learning, Data analytics, Data visualization, Big data, Data mining, Human geography, Social inequality, Human rights, Geography, Sociology, Information science},
abstract = {Improving female empowerment is an important human rights and development goal that needs better monitoring. A number of indices have been developed to track female empowerment at the national level, but these are incomplete and may obscure important sub-national variation. We developed the Female Empowerment Index (FEMI) to track multiple domains of women's empowerment at the sub-national level. The index is based on six categories of empowerment: violence against women, employment, education, reproductive healthcare, decision making, and access to contraceptives. The FEMI has a range of zero to one (low to high empowerment), and it is calculated as the mean proportion of positive outcomes in the six categories. To provide a proof of concept, we computed the FEMI for Nigeria and its 36 states from five Demographic and Health Surveys between the years of 1990 and 2013, using questions asked to 98,542 women between 15 and 49 years old. At the national level, the FEMI increased from 0.34 to 0.48. However, there was substantial sub-national variation, with state-level values ranging from 0.16-0.60 in 1990 to 0.19–0.73 in 2013. Our findings thus illustrate the importance of considering sub-national variation in female empowerment. The FEMI can be readily computed for other countries, and its ability to track spatial and temporal variation in woman's empowerment across a broad set of categories may make it more useful than existing approaches.}
}
@article{CHEN2020104344,
title = {Robust Bayesian networks for low-quality data modeling and process monitoring applications},
journal = {Control Engineering Practice},
volume = {97},
pages = {104344},
year = {2020},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2020.104344},
url = {https://www.sciencedirect.com/science/article/pii/S0967066120300289},
author = {Guangjie Chen and Zhiqiang Ge},
keywords = {Robust Bayesian network, Data quality feature, Process monitoring, Fault diagnosis},
abstract = {In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.}
}
@incollection{KRISHNAN2020157,
title = {9 - Governance},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {157-174},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000090},
author = {Krish Krishnan},
keywords = {Big data application, Data management, Data-driven architecture, Governance, Machine learning, Master data, Metadata},
abstract = {Building the big data application is very interesting and can provide multiple users with multiple perspectives, data discovery to end state analytics and beyond is very much what everybody wants to achieve. Enterprises are ready to spend millions of dollars to get a share of your wallet, they want to be a part of your life and be present at every event that gets your attention. They want to leverage their partnerships and influence you, how do they make this all happen? The most successful companies will tell you their story is built on governance. The aspect of governance is very critical to the success of this journey whether internal or external. What governance are we talking about? How do we implement the same? This chapter will focus on those aspects.}
}
@article{DALZOCHIO2020103298,
title = {Machine learning and reasoning for predictive maintenance in Industry 4.0: Current status and challenges},
journal = {Computers in Industry},
volume = {123},
pages = {103298},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103298},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305327},
author = {Jovani Dalzochio and Rafael Kunst and Edison Pignaton and Alecio Binotto and Srijnan Sanyal and Jose Favilla and Jorge Barbosa},
keywords = {Industry 4.0, Internet of Things, Artificial intelligence, Systematic literature review, Predictive maintenance, Ontology},
abstract = {In recent years, the fourth industrial revolution has attracted attention worldwide. Several concepts were born in conjunction with this new revolution, such as predictive maintenance. This study aims to investigate academic advances in failure prediction. The prediction of failures takes into account concepts as a predictive maintenance decision support system and a design support system. We focus on frameworks that use machine learning and reasoning for predictive maintenance in Industry 4.0. More specifically, we consider the challenges in the application of machine learning techniques and ontologies in the context of predictive maintenance. We conduct a systematic review of the literature (SLR) to analyze academic articles that were published online from 2015 until the beginning of June 2020. The screening process resulted in a final population of 38 studies of a total of 562 analyzed. We removed papers not directly related to predictive maintenance, machine learning, as well as researches classified as surveys or reviews. We discuss the proposals and results of these papers, considering three research questions. This article contributes to the field of predictive maintenance to highlight the challenges faced in the area, both for implementation and use-case. We conclude by pointing out that predictive maintenance is a hot topic in the context of Industry 4.0 but with several challenges to be better investigated in the area of machine learning and the application of reasoning.}
}
@article{R2020235,
title = {Weibull Cumulative Distribution based real-time response and performance capacity modeling of Cyber–Physical Systems through software defined networking},
journal = {Computer Communications},
volume = {150},
pages = {235-244},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419311673},
author = {Gifty R. and Bharathi R.},
keywords = {Cyber–Physical Systems (CPS), Weibull Cumulative Distribution, Big data, Response time},
abstract = {Huge volumes of data are generated at rates faster than the speed of computing resources and executing processors available in market place. This anticipates a draft of information challenges associated with the performance capacity and the ability of big data processing systems to retort in real-time. Moreover, the elapsed time between probabilistic failures drops as the scale of information increases. An error occurred at a specific cluster node of a large Cyber–Physical System influences the overall computation requires to unfold big data transactions. Numerous failure characteristics, statistical response time and lifetime evaluation can be modeled through Weibull Distribution. In this paper, to scrutinize the latency for a data infrastructure, the three-parameter Weibull Cumulative Distribution is used through software defined networking in cyber–physical system. This speculation predicts that the shape of the response time distribution confide in the shape of the learning curve and depicts its parameters to the criterion of the input distribution.}
}
@incollection{CINNAMON202057,
title = {Geographic Information Systems; Ethics},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {57-62},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10554-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105542},
author = {Jonathan Cinnamon},
keywords = {Access, Big data, Cartography, Digital divide, Geodemographics, Mapping, Morals, Privacy, Profiling, Representation, Responsibilities, Spatial data, Surveillance, Values},
abstract = {The development and use of geographic information system (GIS) within particular sociopolitical contexts means that ethical issues can arise both from how GIS is used and also due to the affordances and constraints of the software, hardware, and data. Ethics is a longstanding concern in the field of geographic information science (GIScience), arising amid the critical cartography and GIS and Society debates beginning in the late 1980s, in which human geographers and GIS scholars began to call for more attention to the implications of maps, GIS, and spatial data. Ethics in GIS draws on normative frameworks including deontological (duties and obligations) and teleological (consequences and outcomes) ethical perspectives, as well as nonnormative critical ethics to understand concerns that arise with GIS and map-based representation, uneven access to spatial data and technologies, and the use of GIS in geodemographic profiling, location analytics, and war. Attention to ethics in GIS has led to the development of ethics education, guidance, and codes of conduct for GIS users. Recent advancements in the availability of geolocated personal data, wider societal use of geospatial technologies, and data analytics have pulled GIS ethics to the forefront of the larger domain of information ethics, as location becomes increasingly central to wider ethical debates in the era of big data, automation, and artificial intelligence.}
}
@article{HARRISON2020102672,
title = {New and emerging data forms in transportation planning and policy: Opportunities and challenges for “Track and Trace” data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {117},
pages = {102672},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102672},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20305878},
author = {Gillian Harrison and Susan M. Grant-Muller and Frances C. Hodgson},
keywords = {Transport policy, Track and Trace, Mobile phone data, Mobility profile, Big Data},
abstract = {High quality, reliable data and robust models are central to the development and appraisal of transportation planning and policy. Although conventional data may offer good ‘content’, it is widely observed that it lacks context i.e. who and why people are travelling. Transportation modelling has developed within these boundaries, with implications for the planning, design and management of transportation systems and policy-making. This paper establishes the potential of passively collected GPS-based “Track & Trace” (T&T) datasets of individual mobility profiles towards enhancing transportation modelling and policy-making. T&T is a type of New and Emerging Data Form (NEDF), lying within the broader ‘Big Data’ paradigm, and is typically collected using mobile phone sensors and related technologies. These capture highly grained mobility content and can be linked to the phone owner/user behavioural choices and other individual context. Our meta-analysis of existing literature related to spatio-temporal mobile phone data demonstrates that NEDF’s, and in particular T&T data, have had little mention to date within an applied transportation planning and policy context. We thus establish there is an opportunity for policy-makers, transportation modellers, researchers and a wide range of stakeholders to collaborate in developing new analytic approaches, revise existing models and build the skills and related capacity needed to lever greatest value from the data, as well as to adopt new business models that could revolutionise citizen participation in policy-making. This is of particular importance due to the growing awareness in many countries for a need to develop and monitor efficient cross-sectoral policies to deliver sustainable communities.}
}
@article{SQUITIERI2020231,
title = {Deriving Evidence from Secondary Data in Hand Surgery: Strengths, Limitations, and Future Directions},
journal = {Hand Clinics},
volume = {36},
number = {2},
pages = {231-243},
year = {2020},
note = {Health Policy and Advocacy in Hand Surgery},
issn = {0749-0712},
doi = {https://doi.org/10.1016/j.hcl.2020.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0749071220300111},
author = {Lee Squitieri and Kevin C. Chung},
keywords = {Registry, Hand surgery, Administrative, Claims, Electronic health records, Big data, Secondary data analysis}
}
@article{LIU2020393,
title = {Wind speed forecasting using deep neural network with feature selection},
journal = {Neurocomputing},
volume = {397},
pages = {393-403},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.108},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220304148},
author = {Xiangjie Liu and Hao Zhang and Xiaobing Kong and Kwang Y. Lee},
keywords = {Wind speed forecasting, Deep neural network, Mutual information, Stacked auto-encoder, Denoising, Long short-term memory network},
abstract = {With the rapid growth of wind power penetration into modern power grids, wind speed forecasting (WSF) becomes an increasing important task in the planning and operation of electric power and energy systems. However, WSF is quite challengeable due to its highly varying and complex features. In this paper, a novel hybrid deep neural network forecasting method is constituted. A feature selection method based on mutual information is developed in the WSF problem. With the real-time big data from the wind farm running log, the deep neural network model for WSF is established using a stacked denoising auto-encoder and long short-term memory network. The effectiveness of the deep neural network is evaluated by 10-minutes-ahead WSF. Comparing with the traditional multi-layer perceptron network, conventional long short-term memory network and stacked auto-encoder, the resulting deep neural network significantly improves the forecasting accuracy.}
}
@article{ZHANG2020104512,
title = {Blockchain-based life cycle assessment: An implementation framework and system architecture},
journal = {Resources, Conservation and Recycling},
volume = {152},
pages = {104512},
year = {2020},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2019.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0921344919304185},
author = {Abraham Zhang and Ray Y Zhong and Muhammad Farooque and Kai Kang and V G Venkatesh},
keywords = {Blockchain, Life cycle assessment, Supply chain sustainability, Environmental sustainability, Operational excellence},
abstract = {Life cycle assessment (LCA) is widely used for assessing the environmental impacts of a product or service. Collecting reliable data is a major challenge in LCA due to the complexities involved in the tracking and quantifying inputs and outputs at multiple supply chain stages. Blockchain technology offers an ideal solution to overcome the challenge in sustainable supply chain management. Its use in combination with internet-of-things (IoT) and big data analytics and visualization can help organizations achieve operational excellence in conducting LCA for improving supply chain sustainability. This research develops a framework to guide the implementation of Blockchain-based LCA. It proposes a system architecture that integrates the use of Blockchain, IoT, and big data analytics and visualization. The proposed implementation framework and system architecture were validated by practitioners who were experienced with Blockchain applications. The research also analyzes system implementation costs and discusses potential issues and solutions, as well as managerial and policy implications.}
}
@article{ZHOU2020e667,
title = {Artificial intelligence in COVID-19 drug repurposing},
journal = {The Lancet Digital Health},
volume = {2},
number = {12},
pages = {e667-e676},
year = {2020},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(20)30192-8},
url = {https://www.sciencedirect.com/science/article/pii/S2589750020301928},
author = {Yadi Zhou and Fei Wang and Jian Tang and Ruth Nussinov and Feixiong Cheng},
abstract = {Summary
Drug repurposing or repositioning is a technique whereby existing drugs are used to treat emerging and challenging diseases, including COVID-19. Drug repurposing has become a promising approach because of the opportunity for reduced development timelines and overall costs. In the big data era, artificial intelligence (AI) and network medicine offer cutting-edge application of information science to defining disease, medicine, therapeutics, and identifying targets with the least error. In this Review, we introduce guidelines on how to use AI for accelerating drug repurposing or repositioning, for which AI approaches are not just formidable but are also necessary. We discuss how to use AI models in precision medicine, and as an example, how AI models can accelerate COVID-19 drug repurposing. Rapidly developing, powerful, and innovative AI and network medicine technologies can expedite therapeutic development. This Review provides a strong rationale for using AI-based assistive tools for drug repurposing medications for human disease, including during the COVID-19 pandemic.}
}
@article{VILLAHENRIKSEN202060,
title = {Internet of Things in arable farming: Implementation, applications, challenges and potential},
journal = {Biosystems Engineering},
volume = {191},
pages = {60-84},
year = {2020},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2019.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1537511020300039},
author = {Andrés Villa-Henriksen and Gareth T.C. Edwards and Liisa A. Pesonen and Ole Green and Claus Aage Grøn Sørensen},
keywords = {Smart farming, Internet of things, Wireless sensor network, Farm management information system, Big data, Machine learning},
abstract = {The Internet of Things is allowing agriculture, here specifically arable farming, to become data-driven, leading to more timely and cost-effective production and management of farms, and at the same time reducing their environmental impact. This review is addressing an analytical survey of the current and potential application of Internet of Things in arable farming, where spatial data, highly varying environments, task diversity and mobile devices pose unique challenges to be overcome compared to other agricultural systems. The review contributes an overview of the state of the art of technologies deployed. It provides an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Lastly, it presents some future directions for the Internet of Things in arable farming. Current issues such as smart phones, intelligent management of Wireless Sensor Networks, middleware platforms, integrated Farm Management Information Systems across the supply chain, or autonomous vehicles and robotics stand out because of their potential to lead arable farming to smart arable farming. During the implementation, different challenges are encountered, and here interoperability is a key major hurdle throughout all the layers in the architecture of an Internet of Things system, which can be addressed by shared standards and protocols. Challenges such as affordability, device power consumption, network latency, Big Data analysis, data privacy and security, among others, have been identified by the articles reviewed and are discussed in detail. Different solutions to all identified challenges are presented addressing technologies such as machine learning, middleware platforms, or intelligent data management.}
}
@article{DUIN2020102544,
title = {The Current State of Analytics: Implications for Learning Management System (LMS) Use in Writing Pedagogy},
journal = {Computers and Composition},
volume = {55},
pages = {102544},
year = {2020},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2020.102544},
url = {https://www.sciencedirect.com/science/article/pii/S8755461520300050},
author = {Ann Hill Duin and Jason Tham},
keywords = {Learning management systems, Academic and learning analytics, Writing pedagogy, Student privacy, Access},
abstract = {Amid the burgeoning interest in and use of academic and learning analytics through learning management systems (LMS), the implications of big data and their uses should be central to computers and writing scholarship. In this case study we describe the UMN Canvas LMS experience in such as way so that writing instructors might become more familiar with levels of access to academic and learning analytics, more acquainted with the analytical capabilities in LMSs, and more mindful of implications of learning analytics stemming from LMS use in writing pedagogy. We provide a historical account on the development and infusion of LMS in writing pedagogy and demonstrate how these systems are affecting the way computers and composition scholars consider writing instruction and assessment. We then respond critically to the collection of data drawn from the authors’ use of these systems in on-campus and online teaching. We conclude with implications for writing pedagogy along with a matrix for addressing ethical concerns.}
}
@article{OMRI202023,
title = {Industrial data management strategy towards an SME-oriented PHM},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {23-36},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300467},
author = {N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni},
keywords = {Small and medium-sized enterprises, Data-driven PHM, Industrial data management, Data quality metrics, PHM implementation strategy},
abstract = {The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.}
}
@incollection{CHANIOTAKIS202077,
title = {Chapter 5 - Data aspects of the evaluation of demand for emerging transportation systems},
editor = {Constantinos Antoniou and Dimitrios Efthymiou and Emmanouil Chaniotakis},
booktitle = {Demand for Emerging Transportation Systems},
publisher = {Elsevier},
pages = {77-99},
year = {2020},
isbn = {978-0-12-815018-4},
doi = {https://doi.org/10.1016/B978-0-12-815018-4.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012815018400005X},
author = {Emmanouil Chaniotakis and Dimitrios Efthymiou and Constantinos Antoniou},
keywords = {Data aspects, Diverse data sources, Emerging data, Trasport survey},
abstract = {Of major importance for the evaluation of the demand for emerging transportation systems is the data used. Given the evolution of data availability the last few years, the understanding of the data properties and their characteristics is of major importance. An overview of the data sources commonly used or lately emerging is presented, followed by a classification, based on the component of the transportation system that they represent, aiming at exploring the different benefits and shortcomings of them. A meta-analysis is also performed for the exploration of data quality aspects concerning transport-related datasets.}
}
@article{ZHU202011283,
title = {Supervised Block-Aware Factorization Machine for Multi-Block Quality-Relevant Monitoring},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11283-11288},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.370},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320306546},
author = {Qinqin Zhu},
keywords = {Quality-relevant monitoring, block-aware factorization machine, supervised learning, multi-block processes},
abstract = {Multi-block multivariate statistical methods have been developed to extract useful information from process and quality data in the era of big data, where process variables are partitioned into several meaningful blocks. However, most of these methods did not consider cross-correlations among divided blocks, which leads to inferior monitoring performance. In this article, a block-aware factorization machine (BAFM) algorithm is proposed to exploit information from process and quality data. In BAFM, quality data are first classified into normal and abnormal labels with principal component analysis based quality monitoring framework. Afterwards, a block number is attached to each process variable, and the interactions among different variables (both within and cross blocks) are learned through latent variables, which is supervised by the classified quality labels. Apart from the variable relation within the same block, BAFM also incorporates the block information; thus, both inner and cross correlations are constructed. The monitoring framework based on BAFM is developed, and its effectiveness and superiority are demonstrated through the Tennessee Eastman process.}
}
@incollection{PEZOULAS202067,
title = {Chapter 3 - Medical data sharing},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {67-104},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000037},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Data curation, Data quality management, Data sharing, Data sharing frameworks, Standardization},
abstract = {This chapter introduces the primary step toward the realization of a strategically federated platform, namely data sharing. The rationale behind medical data sharing is related to the interlinking of medical cohorts with respect to data protection regulations and dealing with the unmet needs in various clinical domains. Emphasis is given on data quality management and especially on the existence of a data curation mechanism toward an effective data quality assessment procedure. Data curation and standardization methods are presented along with the latest advances in data sharing assessment and case studies on real data. Existing data sharing frameworks and global initiatives are extensively discussed. Crucial barriers toward data sharing are finally stated along with solutions and guidelines against the misuse of shared data.}
}
@article{FARROKHI2020257,
title = {Using artificial intelligence to detect crisis related to events: Decision making in B2B by artificial intelligence},
journal = {Industrial Marketing Management},
volume = {91},
pages = {257-273},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308464},
author = {Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi},
keywords = {Big data, Artificial intelligence, Machine learning, Data mining, Sentiment analytics},
abstract = {Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.}
}
@article{GOEL2020104316,
title = {Integration of data analytics with cloud services for safer process systems, application examples and implementation challenges},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {68},
pages = {104316},
year = {2020},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2020.104316},
url = {https://www.sciencedirect.com/science/article/pii/S0950423020306033},
author = {Pankaj Goel and Prerna Jain and Hans J. Pasman and E.N. Pistikopoulos and Aniruddha Datta},
keywords = {Data analytics, Maintenance, Deep learning, Sustainability indicators, Natural language processing, Process safety},
abstract = {Emerging sensors, computers, network technologies, and connected platforms result potentially in an immeasurable collection of data within plant operations. This creates the possibility of solving problems innovatively. Because most of the data appear to be unstructured or semi-structured, organizations shall design and adopt new strategies. Further, workflow architectures with data analytics are needed including machine learning tools and artificial intelligence techniques before proto-type solutions can be developed. We shall discuss several prospects of using (big) data analytics integrated with cloud services to produce solutions for improving plant operations. The paper outlines the vision and a systematic framework highlighting the data analytics lifecycle in the area of plant operation, process safety, and environmental protection. Four rather diverse example case studies are demonstrated including (1) deep learning-based predictive maintenance monitoring modeling, (2) Natural Language Processing (NLP) for mining text, (3) barrier assessment for dynamic risk mapping (DRA), and (4) correlation development for sustainability indicators. It further discusses the challenges in both research and implementation of proposed solutions in the industry. It is concluded that a well-balanced integrated approach including machine supporting decisions integrated with expert knowledge and available information from various key resources is required to enable more informed policy, strategic, and operational risk decision-making leading to safer, reliable and more efficient operations.}
}
@article{OKAWA2020107227,
title = {Online signature verification using single-template matching with time-series averaging and gradient boosting},
journal = {Pattern Recognition},
volume = {102},
pages = {107227},
year = {2020},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107227},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320300339},
author = {Manabu Okawa},
keywords = {Biometrics, Forensics, Signature verification, Template matching, Variable importance, Gradient boosting, Dynamic time warping (DTW), Euclidean barycenter-based DTW barycenter averaging (EB-DBA)},
abstract = {In keeping with recent developments in artificial intelligence in the era of big data, there is a demand for online signature verification systems that operate at high speeds, provide a high level of security, and allow high tolerances while achieving sufficient performance. In response to these needs, the present study proposes a novel, single-template strategy using a mean template set and weighted multiple dynamic time warping (DTW) distances for a function-based approach to online signature verification. Specifically, to obtain an effective mean template for each feature while reflecting intra-user variability between all the reference samples, we adopt a novel time-series averaging method based on Euclidean barycenter-based DTW barycenter averaging. Then, by using the mean template set, we calculate multiple DTW distances from multivariate time series based on dependent and independent warping. Finally, to boost the discriminative power, we apply a weighting scheme using a gradient boosting model to efficiently combine the multiple DTW distances. Experimental results using the common SVC2004 Task1/Task2 and MCYT-100 signature datasets confirm that the proposed method is effective for online signature verification.}
}
@article{WU2020116388,
title = {Impact factors of the real-world fuel consumption rate of light duty vehicles in China},
journal = {Energy},
volume = {190},
pages = {116388},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.116388},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219320833},
author = {Tian Wu and Xiao Han and M. Mocarlo Zheng and Xunmin Ou and Hongbo Sun and Xiong Zhang},
keywords = {Real-world fuel consumption rate, Energy consumption, Private passenger vehicles, Big data, China},
abstract = {Measuring real-world fuel consumption of light duty vehicles can be challenging due to the limited collection of actual data. In this paper, we use big data retrieved from the record of real-world fuel consumptions of different brands of vehicles in different areas (n = 106,809 samples from 201 brands of vehicles and 34 cities) in China to build up a real-world fuel consumption rate (RFCR) model to estimate the fuel consumption given the driving conditions and figure out the main factors that affect actual fuel consumption in the real world. We find the average deviation of actual fuel consumptions and the fitting results of RFCR model is 4.22% , which does not significantly differ from zero, and the fuel consumptions calculated by RFCR model tend to be 1.40 L/100 km (about 25%) higher than the official reported data. Furthermore, we find that annual average temperature and altitude factors significantly influence the fuel consumption rate. The results indicate that there is a real world performance discrepancy between the theoretical fuel consumption released by authorities and that in the real world, and some green behaviors (choose light duty vehicles, reduce the use of air conditioning and change to manual transmission type) can reduce energy consumption of vehicles.}
}
@article{BETTS202035,
title = {Predicting postpartum psychiatric admission using a machine learning approach},
journal = {Journal of Psychiatric Research},
volume = {130},
pages = {35-40},
year = {2020},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0022395620308669},
author = {Kim S. Betts and Steve Kisely and Rosa Alati},
keywords = {Administrative data linkage, Postpartum psychiatric admissions, Predictive models, Machine learning},
abstract = {Aims
The accurate identification of mothers at risk of postpartum psychiatric admission would allow for preventive intervention or more timely admission. We developed a prediction model to identify women at risk of postpartum psychiatric admission.
Methods
Data included administrative health data of all inpatient live births in the Australian state of Queensland between January 2009 and October 2014. Analyses were restricted to mothers with one or more indicator of mental health problems during pregnancy (n = 75,054 births). The predictors included all maternal data up to and including the delivery, and neonatal data recorded at delivery. We used multiple machine learning methods to predict hospital admission in the 12 months following delivery in which the primary diagnosis was recorded as an ICD-10 psychotic, bipolar or depressive disorders.
Results
The boosted trees algorithm produced the best performing model, predicting postpartum psychiatric admission in the validation data with good discrimination [AUC = 0.80; 95% CI = (0.76, 0.83)] and achieving good calibration. This model outperformed benchmark logistic regression model and an elastic net model. In addition to indicators of maternal metal health history, maternal and neonatal anthropometric measures and social/lifestyle factors were strong predictors.
Conclusion
Our results indicate the potential of a big data approach when aiming to identify mothers at risk of postpartum psychiatric admission. Mothers at risk could be followed-up and supported after neonatal discharge to either remove the need for admission or facilitate more timely admission.}
}
@article{LEZOCHE2020103187,
title = {Agri-food 4.0: A survey of the supply chains and technologies for the future agriculture},
journal = {Computers in Industry},
volume = {117},
pages = {103187},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103187},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519307584},
author = {Mario Lezoche and Jorge E. Hernandez and Maria del Mar Eva {Alemany Díaz} and Hervé Panetto and Janusz Kacprzyk},
keywords = {Agri-Food 4.0, Agriculture 4.0, Supply chains, Internet of things, Big data, Blockchain, Artificial intelligence},
abstract = {The term “Agri-Food 4.0” is an analogy to the term "Industry 4.0", coming from the concept “agriculture 4.0”. Since the origins of the industrial revolution, where the steam engines started the concept of Industry 1.0 and later the use of electricity upgraded the concept to Industry 2.0, the use of technologies generated a milestone in the industry revolution by addressing the Industry 3.0 concept. Hence, Industry 4.0, it is about including and integrating the latest developments based on digital technologies as well as the interoperability process across them. This allows enterprises to transmit real-time information in terms behaviour and performance. Therefore, the challenge is to maintain these complex networked structures efficiently linked and organised within the use of such technologies, especially to identify and satisfy supply chain stakeholders dynamic requirements. In this context, the agriculture domain is not an exception although it possesses some specialities depending from the domain. In fact, all agricultural machinery incorporates electronic controls and has entered to the digital age, enhancing their current performance. In addition, electronics, using sensors and drones, support the data collection of several agriculture key aspects, such as weather, geographical spatialization, animals and crops behaviours, as well as the entire farm life cycle. However, the use of the right methods and methodologies for enhancing agriculture supply chains performance is still a challenge, thus the concept of Industry 4.0 has evolved and adapted to agriculture 4.0 in order analyse the behaviours and performance in this specific domain. Thus, the question mark on how agriculture 4.0 support a better supply chain decision-making process, or how can help to save time to farmer to make effective decision based on objective data, remains open. Therefore, in this survey, a review of more than hundred papers on new technologies and the new available supply chains methods are analysed and contrasted to understand the future paths of the Agri-Food domain.}
}
@article{LEE2020101426,
title = {Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science},
journal = {The Leadership Quarterly},
pages = {101426},
year = {2020},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2020.101426},
url = {https://www.sciencedirect.com/science/article/pii/S1048984320300539},
author = {Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene},
keywords = {Leadership effectiveness, Leadership processes, Machine Learning, Artificial intelligence, Causality, Experiments, Big Data, Heterogeneous treatment effects},
abstract = {Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.}
}
@article{WANG2020119852,
title = {Safety informatics as a new, promising and sustainable area of safety science in the information age},
journal = {Journal of Cleaner Production},
volume = {252},
pages = {119852},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119852},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619347225},
author = {Bing Wang and Chao Wu},
keywords = {Safety science, Information science, Safety information, Safety informatics, Safety 4.0},
abstract = {Safety is a central dimension in contemporary debates on human health, loss prevention, environmental protection, sustainability, and cleaner production. In the information age, especially in the era of big data, safety information is an essential strategy for safety, and safety informatics has become a major research interest and a popular issue in the field of safety science. In recent years, safety informatics—a new area of safety science—has received increasing attention, developing greatly with successful research on the subject. The three key purposes of this paper are: (i) to analyze the historical development of safety informatics, (ii) to review the research progress of safety informatics, and (iii) to review limitations and propose future directions in the field of safety informatics. First, the development process of safety informatics is divided into four typical stages: (i) the embryonic stage (1940–1980), (ii) the initial stage (1980–1990), (iii) the formation stage (1990–2010), and (iv) the deepening stage (2010–present). Then, a review of safety informatics research is provided from seven aspects, including: (i) the discipline construction of safety informatics, (ii) theoretical safety information model, (iii) accident causation model from a safety information perspective, (iv) safety management based on safety information, (v) safety big data, (vi) safety intelligence, and (vii) safety information technology. Finally, limitations and future research directions in the safety informatics area are briefly discussed.}
}
@article{SALIM2020106964,
title = {Modelling urban-scale occupant behaviour, mobility, and energy in buildings: A survey},
journal = {Building and Environment},
volume = {183},
pages = {106964},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106964},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320303231},
author = {Flora D. Salim and Bing Dong and Mohamed Ouf and Qi Wang and Ilaria Pigliautile and Xuyuan Kang and Tianzhen Hong and Wenbo Wu and Yapan Liu and Shakila Khan Rumi and Mohammad Saiedur Rahaman and Jingjing An and Hengfang Deng and Wei Shao and Jakub Dziedzic and Fisayo Caleb Sangogboye and Mikkel Baun Kjærgaard and Meng Kong and Claudia Fabiani and Anna Laura Pisello and Da Yan},
keywords = {Big data, Occupant behaviour, Energy modelling, Mobility, Urban data, Sensors, Machine learning, Energy in buildings, Energy in cities},
abstract = {The proliferation of urban sensing, IoT, and big data in cities provides unprecedented opportunities for a deeper understanding of occupant behaviour and energy usage patterns at the urban scale. This enables data-driven building and energy models to capture the urban dynamics, specifically the intrinsic occupant and energy use behavioural profiles that are not usually considered in traditional models. Although there are related reviews, none investigated urban data for use in modelling occupant behaviour and energy use at multiple scales, from buildings to neighbourhood to city. This survey paper aims to fill this gap by providing a critical summary and analysis of the works reported in the literature. We present the different sources of occupant-centric urban data that are useful for data-driven modelling and categorise the range of applications and recent data-driven modelling techniques for urban behaviour and energy modelling, along with the traditional stochastic and simulation-based approaches. Finally, we present a set of recommendations for future directions in data-driven modelling of occupant behaviour and energy in buildings at the urban scale.}
}
@article{DECAIGNY2020113232,
title = {Leveraging fine-grained transaction data for customer life event predictions},
journal = {Decision Support Systems},
volume = {130},
pages = {113232},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113232},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619302611},
author = {Arno {De Caigny} and Kristof Coussement and Koen W. {De Bock}},
keywords = {Life event prediction, Predictive modeling, Pseudo-social networks, Customer relationship management (CRM), Big data, Data science},
abstract = {This real-world study with a large European financial services provider combines aggregated customer data including customer demographics, behavior and contact with the firm, with fine-grained transaction data to predict four different customer life events: moving, birth of a child, new relationship, and end of a relationship. The fine-grained transaction data—approximately 60 million debit transactions involving around 132,000 customers to >1.5 million different counterparties over a one-year period—reveal a pseudo-social network that supports the derivation of behavioral similarity measures. To advance decision support systems literature, this study validates the proposed customer life event prediction model in a real-world setting in the financial services industry; compares models that rely on aggregated data, fine-grained transaction data, and their combination; and extends existing methods to incorporate fine-grained data that preserve recency, frequency, and monetary value information of the transactions. The results show that the proposed model predicts life events significantly better than random guessing, especially with the combination of fine-grained transaction and aggregated data. Incorporating recency, frequency, and monetary value information of fine-grained transaction data also significantly improves performance compared with models based on binary logs. Fine-grained transaction data accounts for the largest part of the total variable importance, for all but one of the life events.}
}
@article{ARIMURA2020100212,
title = {Changes in urban mobility in Sapporo city, Japan due to the Covid-19 emergency declarations},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {7},
pages = {100212},
year = {2020},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220301238},
author = {Mikiharu Arimura and Tran Vinh Ha and Kota Okumura and Takumi Asada},
keywords = {Covid-19, Moving pattern, Mobile spatial statistics, Population concentration, Big data},
abstract = {At the time of writing, the world is facing the new coronavirus pandemic, which has been declared one of the most dangerous disasters of the 21st century. All nations and communities have applied many countermeasures to control the spread of the epidemic. In terms of countermeasures, lockdowns and reductions of social activities are meant to flatten the curve of infection. Nevertheless, to date, there has been no evaluation of the effectiveness of these methods. Thus, the present study aims to interpret the change in the population density of Sapporo city in the emergency's period declaration using big data obtained from mobile spatial statistics. The results indicate that, in the time of refraining from traveling, the city's residents have been more likely to stay home and less likely to travel to the center area. This has led to a decrease of up to 90% of the population density in crowded areas. The study's outcomes partly explain the statement of reducing 70%–80% of contact between people in line with the purpose of the emergency declaration. Moreover, these findings establish the primary step for further analysis of estimating the efficiency of policy in controlling the epidemic.}
}
@article{JIANG2020101505,
title = {Ignorance is bliss? An empirical analysis of the determinants of PSS usefulness in practice},
journal = {Computers, Environment and Urban Systems},
volume = {83},
pages = {101505},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101505},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302386},
author = {Huaxiong Jiang and Stan Geertman and Patrick Witte},
keywords = {Smart city, Implementation gap, Success and failure factors, Utility, Usability, Context},
abstract = {Planning support systems (PSS) enabled by smart city technologies (big data and information and communication technologies (ICTs)) are becoming more widespread in their availability, but have not yet been fully recognized as being useful in planning practice. Thus, a better understanding of the determinants of PSS usefulness in practice helps to improve the functional support of PSS for smart cities. This study is based on a recent international questionnaire (268 respondents) designed to evaluate the perceptions of scholars and practitioners in the smart city planning field. Based on the empirical evidence, this paper recommends that it is imperative for PSS developers and users to be more responsive to the fit for task-technology and user-technology (i.e., utility and usability, respectively) since they positively contribute to PSS usefulness in practice and to be more sensitive to the potential negative effects of contextual factors on PSS usefulness in smart cities. The empirical analyses further suggest that rather than merely striving for integrating smart city technologies into advancing PSS, the way that innovative PSS are integrated into the planning framework (i.e., how well PSS can satisfy the needs of planning tasks and users by considering context-specificities) is of great significance in promoting PSS's actual usefulness.}
}
@article{AMARATUNGA2020100027,
title = {Uses and opportunities for machine learning in hypertension research},
journal = {International Journal of Cardiology Hypertension},
volume = {5},
pages = {100027},
year = {2020},
issn = {2590-0862},
doi = {https://doi.org/10.1016/j.ijchy.2020.100027},
url = {https://www.sciencedirect.com/science/article/pii/S2590086220300045},
author = {Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis},
keywords = {Machine learning, Deep neural networks, Hypertension, Disease management, Personalized disease network},
abstract = {Background
Artificial intelligence (AI) promises to provide useful information to clinicians specializing in hypertension. Already, there are some significant AI applications on large validated data sets.
Methods and results
This review presents the use of AI to predict clinical outcomes in big data i.e. data with high volume, variety, veracity, velocity and value. Four examples are included in this review. In the first example, deep learning and support vector machine (SVM) predicted the occurrence of cardiovascular events with 56%–57% accuracy. In the second example, in a data base of 378,256 patients, a neural network algorithm predicted the occurrence of cardiovascular events during 10 year follow up with sensitivity (68%) and specificity (71%). In the third example, a machine learning algorithm classified 1,504,437 patients on the presence or absence of hypertension with 51% sensitivity, 99% specificity and area under the curve 87%. In example four, wearable biosensors and portable devices were used in assessing a person's risk of developing hypertension using photoplethysmography to separate persons who were at risk of developing hypertension with sensitivity higher than 80% and positive predictive value higher than 90%. The results of the above studies were adjusted for demographics and the traditional risk factors for atherosclerotic disease.
Conclusion
These examples describe the use of artificial intelligence methods in the field of hypertension.}
}
@incollection{FILCHEV2020103,
title = {Chapter 6 - Surveys, Catalogues, Databases/Archives, and State-of-the-Art Methods for Geoscience Data Processing},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {103-136},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00016-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000163},
author = {Lachezar Filchev and Lyubka Pashova and Vasil Kolev and Stuart Frye},
keywords = {Geosensor networks, Remote sensing, Earth observations, Geoinformation, Big Data, Databases/archives, Satellite image processing, Geographic information systems, Web geoportals, ICT, Decision analysis and technologies, Spectral imaging, Fourier analysis, Principal component analysis, Karhunen–Loève transform, Continuous and discrete wavelet, Multiwavelet transforms, Hyperspectral images, Classification methods, Image denoising},
abstract = {Recent years are marked with rapid growth in sources and availability of geospatial data and information providing new opportunities and challenges for scientific knowledge and technology solutions on time. This chapter represents a general overview of modern ICT tools and methods for acquiring Earth observation (EO) data storage, processing, analysis, and interpretation for many research and applied purposes. The main contribution to Big Data developments in EO is the space activities of the space and governmental agencies, such as CNES, CSA, CSIRO, DLR, ESA, INPE, ISRO, JAXA, NASA, RADI, and Roscosmos. Special attention is devoted to the international archives, catalogues, and databases of satellite EO, which already become an indispensable and crucial source of information in support of many sectors of social-economic activities and resolving environmental issues. Main technological and information products, geoportals, and services to deal with Big EO datasets are shortly discussed. Some advanced contemporary approaches for processing big EO data, compressing, clustering, and denoising, and hyperspectral images in the geoinformation science are outlined.}
}
@article{DEMOULIN2020103120,
title = {Acceptance of text-mining systems: The signaling role of information quality},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103120},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308765},
author = {Nathalie T.M. Demoulin and Kristof Coussement},
keywords = {Technology acceptance model (TAM), Text mining, Big data, Information quality, Top management support},
abstract = {The popularity of the big data domain has boosted corporate interest in collecting and storing tremendous amounts of consumers’ textual information. However, decision makers are often overwhelmed by the abundance of information, and the usage of text mining (TM) tools is still at its infancy. This study validates an extended technology acceptance model integrating information quality (IQ) and top management support. Results confirm that IQ influences behavioral intentions and TM tools usage, through perceptions of external control, perceived ease of use, and perceived usefulness; top management support also has a key role in determining the usage of TM tools.}
}
@article{NASCIMENTO202097,
title = {Estimating record linkage costs in distributed environments},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {97-106},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520302756},
author = {Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre},
keywords = {Record linkage, Theoretical model, Data quality, Cloud computing},
abstract = {Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.}
}
@incollection{CANNARD2020207,
title = {Chapter 16 - Self-health monitoring and wearable neurotechnologies},
editor = {Nick F. Ramsey and José del R. Millán},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {168},
pages = {207-232},
year = {2020},
booktitle = {Brain-Computer Interfaces},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-444-63934-9.00016-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444639349000160},
author = {Cedric Cannard and Tracy Brandmeyer and Helané Wahbeh and Arnaud Delorme},
keywords = {Wearable neurotechnologies, EEG, Neurofeedback, BCI, Mobility, Real world, Big data, Home use},
abstract = {Brain-computer interfaces and wearable neurotechnologies are now used to measure real-time neural and physiologic signals from the human body and hold immense potential for advancements in medical diagnostics, prevention, and intervention. Given the future role that wearable neurotechnologies will likely serve in the health sector, a critical state-of-the-art assessment is necessary to gain a better understanding of their current strengths and limitations. In this chapter we present wearable electroencephalography systems that reflect groundbreaking innovations and improvements in real-time data collection and health monitoring. We focus on specifications reflecting technical advantages and disadvantages, discuss their use in fundamental and clinical research, their current applications, limitations, and future directions. While many methodological and ethical challenges remain, these systems host the potential to facilitate large-scale data collection far beyond the reach of traditional research laboratory settings.}
}
@article{MCCLURE2020100109,
title = {Artificial Intelligence Meets Citizen Science to Supercharge Ecological Monitoring},
journal = {Patterns},
volume = {1},
number = {7},
pages = {100109},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100109},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920301434},
author = {Eva C. McClure and Michael Sievers and Christopher J. Brown and Christina A. Buelow and Ellen M. Ditria and Matthew A. Hayes and Ryan M. Pearson and Vivitskaia J.D. Tulloch and Richard K.F. Unsworth and Rod M. Connolly},
keywords = {CS, AI, biological conservation, automation, machine learning, deep learning, data processing, big data},
abstract = {Summary
The development and uptake of citizen science and artificial intelligence (AI) techniques for ecological monitoring is increasing rapidly. Citizen science and AI allow scientists to create and process larger volumes of data than possible with conventional methods. However, managers of large ecological monitoring projects have little guidance on whether citizen science, AI, or both, best suit their resource capacity and objectives. To highlight the benefits of integrating the two techniques and guide future implementation by managers, we explore the opportunities, challenges, and complementarities of using citizen science and AI for ecological monitoring. We identify project attributes to consider when implementing these techniques and suggest that financial resources, engagement, participant training, technical expertise, and subject charisma and identification are important project considerations. Ultimately, we highlight that integration can supercharge outcomes for ecological monitoring, enhancing cost-efficiency, accuracy, and multi-sector engagement.}
}
@article{JI2020103459,
title = {Converting clinical document architecture documents to the common data model for incorporating health information exchange data in observational health studies: CDA to CDM},
journal = {Journal of Biomedical Informatics},
volume = {107},
pages = {103459},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103459},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420300873},
author = {Hyerim Ji and Seok Kim and Soyoung Yi and Hee Hwang and Jeong-Whun Kim and Sooyoung Yoo},
keywords = {Clinical document architecture, Common data model, Observational Medical Outcomes Partnership, Referral documents},
abstract = {Background
Utilization of standard health information exchange (HIE) data is growing due to the high adoption rate and interoperability of electronic health record (EHR) systems. However, integration of HIE data into an EHR system is not yet fully adopted in clinical research. In addition, data quality should be verified for the secondary use of these data. Thus, the aims of this study were to convert referral documents in a Health Level 7 (HL7) clinical document architecture (CDA) to the common data model (CDM) to facilitate HIE data availability for longitudinal data analysis, and to identify data quality levels for application in future clinical studies.
Methods
A total of 21,492 referral CDA documents accumulated for over 10 years in a tertiary general hospital in South Korea were analyzed. To convert CDA documents to the Observational Medical Outcomes Partnership (OMOP) CDM, processes such as CDA parsing, data cleaning, standard vocabulary mapping, CDA-to-CDM mapping, and CDM conversion were performed. The quality of CDM data was then evaluated using the Achilles Heel and visualized with the Achilles tool.
Results
Mapping five CDA elements (document header, problem, medication, laboratory, and procedure) into an OMOP CDM table resulted in population of 9 CDM tables (person, visit_occurrence, condition_occurrence, drug_exposure, measurement, observation, procedure_occurrence, care_site, and provider). Three CDM tables (drug_era, condition_era, and observation_period) were derived from the converted table. From vocabulary mapping codes in CDA documents according to domain, 98.6% of conditions, 68.8% of drugs, 35.7% of measurements, 100% of observation, and 56.4% of procedures were mapped as standard concepts. The conversion rates of the CDA to the OMOP CDM were 96.3% for conditions, 97.2% for drug exposure, 98.1% for procedure occurrence, 55.1% for measurements, and 100% for observation.
Conclusions
We examined the possibility of CDM conversion by defining mapping rules for CDA-to-CDM conversion using the referral CDA documents collected from clinics in actual medical practice. Although mapping standard vocabulary for CDM conversion requires further improvement, the conversion could facilitate further research on the usage patterns of medical resources and referral patterns.}
}
@article{BUCKINGHAM202092,
title = {The untapped potential of mining news media events for understanding environmental change},
journal = {Current Opinion in Environmental Sustainability},
volume = {45},
pages = {92-99},
year = {2020},
note = {Open Issue 2020 Part A: Technology Innovations and Environmental Sustainability in the Anthropocene},
issn = {1877-3435},
doi = {https://doi.org/10.1016/j.cosust.2020.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S1877343520300701},
author = {Kathleen Buckingham and John Brandt and Will Anderson and Luiz Fernando do Amaral and Ruchika Singh},
abstract = {Climate change affects the lives of millions of people. While much attention has been paid to the biophysical impacts of climate change, researchers have little empirical information on the impacts on human society. Climate related social challenges are difficult to accurately measure. One recent data source, the Global Database of Events, Language, and Tone (GDELT) Project, could close that gap. It monitors the world’s broadcast, print, and web news in over 100 languages and identifies the people, locations, organizations, themes, and events driving our global society. Increasingly, big data sources like GDELT are being used to understand how changing actors, events and sentiment in the news media can help understand social change. By analyzing GDELT’s data, applications, and methods, this review identifies the potential of this new data source for the increasingly important role that computational social science can play alongside established biophysical data in monitoring largescale environmental change.}
}
@article{AMAYRI2020109578,
title = {Database quality assessment for interactive learning: Application to occupancy estimation},
journal = {Energy and Buildings},
volume = {209},
pages = {109578},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.109578},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819325575},
author = {Manar Amayri and Stephane Ploix and Nizar Bouguila and Frederic Wurtz},
keywords = {Data quality, Machine leaning, Data mining, Human behavior, Building performance, Activities recognition, Office buildings},
abstract = {Data quality assesment is a key component for many real applications, since it can drive better modelling. In this work a methodology to asses data quality (Qscore) is proposed and discussed. The validation of Qscore is performed via an interactive learning experiment related to occupancy estimation. Interactive learning has been shown to be crucial to consider and integrate occupant behavior in smart buildings. Indeed, valuable feedback and information can be collected from the occupants by involving them and by improving their consciousness about energy management systems. Users should feel involved to keep developing highly energy-efficient buildings. To reach this goal, occupants should be aware of the building features to feel more in control. This paper proposes a framework to interact with occupants to estimate building occupancy. This framework is based on an enhanced supervised learning approach that involves interaction with occupants, when necessary, to keep collecting training data. The training data consist of the measurements (i.e. features) collected from common sensors, for instance, motion detection, power consumption, and CO2 concentration, and the label (i.e. number of occupants) provided by the occupants during interactions. The considered learning machine in our experiments is the Multi-layer Perceptron regressor (MLP), although other approaches could be easily integrated within the proposed framework. In order to avoid useless interaction with users a new concept is introduced, called spread rate, to measure the quality of the data to decide if an interaction with the user is necessary or not. Extensive simulations have shown the merits of the proposed approach.}
}
@article{BOLISLIS2020926,
title = {Use of Real-world Data for New Drug Applications and Line Extensions},
journal = {Clinical Therapeutics},
volume = {42},
number = {5},
pages = {926-938},
year = {2020},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2020.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0149291820301338},
author = {Winona R. Bolislis and Myriam Fay and Thomas C. Kühler},
keywords = {big data, drug applications, rare diseases, real-world data, regulatory},
abstract = {Purpose
For this article, the authors compiled, summarized, and analyzed data from 27 cases in which real-world data (RWD) were applied in regulatory approval. The aims were to provide an overview of RWD, based on classifications per therapeutic area, age group, drivers of acceptability, utility, data sources, and timelines, and to present insights on how it has been applied in regulatory decision making to date.
Methods
Clarivate Analytics was commissioned to collect data from cases in which RWD was used for new drug applications and line extensions submitted to the European Medicines Agency (EMA), the US Food and Drug Administration (FDA), Health Canada, and Japan's Pharmaceuticals and Medical Devices Agency. The query resulted in 27 cases in which regulatory approval was associated with RWD. The data were then categorized and elaborated with supporting information gathered from public databases and company websites.
Findings
There were 17 identified cases in which RWD were used for new drug applications, and 10 for line extensions, between the years 1998 and 2019. Approvals were spread across regulatory bodies: the EMA alone (6 cases), the FDA alone (4 cases), or jointly between the EMA and FDA or other regulatory bodies. The applications were also distributed across age groups and therapeutic areas but were mostly applied in oncology and metabolism. The new drug applications of all 17 products were approved, with drugs from new drug applications initially marketed as orphan drugs. In most cases, RWD were used either as primary data, when noncomparative data were available to demonstrate tolerability and efficacy, or as supportive data when validating findings. Common sources of RWD have been health or medical records (16 cases) and registries (8 cases). Review timelines in which RWD were applied were than 1 year for new drug applications and between 3 and 10 months for line extensions.
Implications
The analysis of this study was limited in that the data were gathered from the commissioned query and may therefore have been nonexhaustive. Nonetheless, we recognize that the use of RWD has been gaining attention across the community and is expected to expand as a result of the various initiatives and efforts carried out in the sector. While the current application of RWD has been limited to specific cases, there is a potential to further explore and develop its application. Further refinements in the analytical processes, methodologies, and techniques would need to be established to achieve similar effects observed in randomized controlled trials}
}
@incollection{CINNAMON2020121,
title = {Humanitarian Mapping},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {121-128},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10559-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105591},
author = {Jonathan Cinnamon},
keywords = {Big data, Cartography, Crisis, Crowdsourcing, Disaster, Emergency, Geospatial web, Relief, Spatial data, Web mapping},
abstract = {Humanitarian mapping refers to the production of spatial data and cartographic products to improve situational awareness and decision-making around humanitarian issues from acute events such as natural disasters and public health emergencies to longer term events such as refugee crises and political unrest. Mapping is a key part of the broader area of humanitarian information management, which has traditionally been undertaken by governments and international humanitarian organizations. As a core aspect of the field of digital humanitarianism, mapping activities are now widely undertaken by smaller organizations and networks of volunteers who produce spatial data and maps on the ground and remotely via the use of Web mapping and mobile phone technologies. Big data based on location and behavioral attributes produced online and through interaction with digital systems and networks can also be exploited to enhance information environments. Together, these new developments signal new possibilities for improved risk and crisis management, based on up-to-date high resolution spatial and temporal evidence. Research in human geography, geographic information science, and related disciplines focuses on tracing benefits such as increased speed and low costs, as well as the risks of relying on distributed volunteers and new sources of data of questionable accuracy and validity.}
}
@incollection{VAVILOVA202057,
title = {Chapter 5 - Surveys, Catalogues, Databases, and Archives of Astronomical Data},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {57-102},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000151},
author = {Irina Vavilova and Ludmila Pakuliak and Iurii Babyk and Andrii Elyiv and Daria Dobrycheva and Olga Melnyk},
keywords = {Astronomical surveys, Catalogues, Databases, Archives, Astroplates, Multiwavelength astronomy},
abstract = {This chapter traces the development of astronomical observational methods from visual, for decades tracking the behavior of individual objects, to modern space missions that produce data simultaneously for several hundred thousand objects in different spectral ranges. Thanks to this, astronomy has moved from the study of individual objects to the study of the universe as a whole and has become the science of Big Data. The chapter describes briefly visual, photographic, and CCD surveys of stars, galaxies and the intergalactic medium, spectral photographic and spectral CCD surveys, and multiwavelength ground-based and space-born databases and archives, which made it possible to create a high-precision coordinate system, to discover new properties of celestial bodies, and, as a result, to construct three-dimensional models of the visible parts of the universe. We mention also the “conserved” data of photographic astroplates accumulated over the centuries, which have been actively digitized during the last decades and provide a new knowledge from the comparative analysis of old and new observational material. Astronomy research is changing from being hypothesis-driven to being data-driven to being data-intensive. To cope with the various challenges and opportunities offered by the exponential growth of astronomical data volumes, rates, and complexity, the new disciplines of astrostatistics and astroinformatics have emerged.}
}
@article{POURHABIBI2020113303,
title = {Fraud detection: A systematic literature review of graph-based anomaly detection approaches},
journal = {Decision Support Systems},
volume = {133},
pages = {113303},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113303},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620300580},
author = {Tahereh Pourhabibi and Kok-Leong Ong and Booi H. Kam and Yee Ling Boo},
keywords = {Fraud detection, Graph-based anomaly detection, Graph data, Systematic literature review, Social network, Big data analytics},
abstract = {Graph-based anomaly detection (GBAD) approaches are among the most popular techniques used to analyze connectivity patterns in communication networks and identify suspicious behaviors. Given the different GBAD approaches proposed for fraud detection, in this study, we develop a framework to synthesize the existing literature on the application of GBAD methods in fraud detection published between 2007 and 2018. This study aims to investigate the present trends and identify the key challenges that require significant research efforts to increase the credibility of the technique. Additionally, we provide some recommendations to deal with these challenges.}
}
@article{CAI2020107500,
title = {Incorporating structured assumptions with probabilistic graphical models in fMRI data analysis},
journal = {Neuropsychologia},
volume = {144},
pages = {107500},
year = {2020},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2020.107500},
url = {https://www.sciencedirect.com/science/article/pii/S0028393220301706},
author = {Ming Bo Cai and Michael Shvartsman and Anqi Wu and Hejia Zhang and Xia Zhu},
keywords = {Probabilistic graphical model, Bayesian, fMRI, Cognitive neuroscience, Big data, Factor model, Matrix normal},
abstract = {With the wide adoption of functional magnetic resonance imaging (fMRI) by cognitive neuroscience researchers, large volumes of brain imaging data have been accumulated in recent years. Aggregating these data to derive scientific insights often faces the challenge that fMRI data are high-dimensional, heterogeneous across people, and noisy. These challenges demand the development of computational tools that are tailored both for the neuroscience questions and for the properties of the data. We review a few recently developed algorithms in various domains of fMRI research: fMRI in naturalistic tasks, analyzing full-brain functional connectivity, pattern classification, inferring representational similarity and modeling structured residuals. These algorithms all tackle the challenges in fMRI similarly: they start by making clear statements of assumptions about neural data and existing domain knowledge, incorporate those assumptions and domain knowledge into probabilistic graphical models, and use those models to estimate properties of interest or latent structures in the data. Such approaches can avoid erroneous findings, reduce the impact of noise, better utilize known properties of the data, and better aggregate data across groups of subjects. With these successful cases, we advocate wider adoption of explicit model construction in cognitive neuroscience. Although we focus on fMRI, the principle illustrated here is generally applicable to brain data of other modalities.}
}
@article{LI2020121458,
title = {Research trend of the application of information technologies in construction and demolition waste management},
journal = {Journal of Cleaner Production},
volume = {263},
pages = {121458},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.121458},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620315055},
author = {Clyde Zhengdao Li and Yiyu Zhao and Bing Xiao and Bo Yu and Vivian W.Y. Tam and Zhe Chen and Yingyi Ya},
keywords = {Construction and demolition waste, Information technology, Waste management, Literature review},
abstract = {Information technologies are increasingly adopted across the world to promote the efficiency of construction and demolition (C&D) waste management and alleviate the environmental and social effects of waste disposal. In addressing management issues of C&D waste by information technologies, several studies have been published by international peer-reviewing journals related to construction management over the past decades. However, a systematic summary on the research development in information technologies used in C&D waste management discipline is lacking. Therefore, this study examines the latest research situation in this discipline by analysing published construction management research in peer-reviewing journals in the period of 2000–2019, and 57 related papers are collected by filtering. The characteristics of the collected papers are classified according to the following criteria: application scenarios in C&D waste management and different technologies. Eight categories of present advanced technologies used in C&D waste management are identified, namely, building information modelling, geographic information system, big data, radio frequency identification, image recognition technology, image analysis, global positioning system and barcode technology. A critical analysis of the identified technologies is conducted in accordance with their characteristics and applications. This study also performs the scientometric analysis of the collected papers by VOSviewer. The limitations of information technologies and the recommendations for potential future research directions are presented on the basis of the analytical review.}
}
@article{VANASSEN2020109083,
title = {Artificial intelligence from A to Z: From neural network to legal framework},
journal = {European Journal of Radiology},
volume = {129},
pages = {109083},
year = {2020},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2020.109083},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X20302722},
author = {Marly {van Assen} and Scott J. Lee and Carlo N. {De Cecco}},
keywords = {Artificial intelligence, Cardiac, Chest},
abstract = {Artificial intelligence (AI) will continue to cause substantial changes within the field of radiology, and it will become increasingly important for clinicians to be familiar with several concepts behind AI algorithms in order to effectively guide their clinical implementation. This review aims to give medical professionals the basic information needed to understand AI development and research. The general concepts behind several AI algorithms, including their data requirements, training, and evaluation methods are explained. The potential legal implications of using AI algorithms in clinical practice are also discussed.}
}
@incollection{MILLER2020205,
title = {Chapter 10 - AI, autonomous machines and human awareness: Towards shared machine-human contexts in medicine},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {205-220},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000109},
author = {D. Douglas Miller and Elena A. Wood},
keywords = {Medicine, Health care, Artificial intelligence, Medical education, Applications, Challenges},
abstract = {Medical curricula trend to integrate clinical skills training and to create efficiencies in preclinical medical sciences, but the rapid emergence big data-intensive health care has led to initiating collaborations among data scientists, computer engineers, and medical educators that might generate novel educational high-technology platforms and innovative AI practice applications. The preprocessing of big data improves neural network feature recognition, improving the speed and accuracy of AI diagnostics and permitting chronic disease predictions. Applications of generative adversarial networks to create virtual patient phenotypes and image sets exposes medical learners to endless illness presentations, improving system-1 critical thinking for differential diagnosis development. AI offers great potential for education data managers working in support of medical educators and learners. These opportunities to build a shared context, in keeping with these themes of this book, include emerging data-driven AI applications for medical education and provider training include individual aptitude-based career advising, early identification of learners with academic difficulties, highly focused e-tutoring interventions, and natural language processing of standardized exam questions.}
}
@article{ENGIN2020140,
title = {Data-driven urban management: Mapping the landscape},
journal = {Journal of Urban Management},
volume = {9},
number = {2},
pages = {140-150},
year = {2020},
issn = {2226-5856},
doi = {https://doi.org/10.1016/j.jum.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2226585619301153},
author = {Zeynep Engin and Justin {van Dijk} and Tian Lan and Paul A. Longley and Philip Treleaven and Michael Batty and Alan Penn},
keywords = {Data-driven society, Urban management and applications, Evidence-based decision making},
abstract = {Big data analytics and artificial intelligence, paired with blockchain technology, the Internet of Things, and other emerging technologies, are poised to revolutionise urban management. With massive amounts of data collected from citizens, devices, and traditional sources such as routine and well-established censuses, urban areas across the world have – for the first time in history – the opportunity to monitor and manage their urban infrastructure in real-time. This simultaneously provides previously unimaginable opportunities to shape the future of cities, but also gives rise to new ethical challenges. This paper provides a transdisciplinary synthesis of the developments, opportunities, and challenges for urban management and planning under this ongoing ‘digital revolution’ to provide a reference point for the largely fragmented research efforts and policy practice in this area. We consider both top-down systems engineering approaches and the bottom-up emergent approaches to coordination of different systems and functions, their implications for the existing physical and institutional constraints on the built environment and various planning practices, as well as the social and ethical considerations associated with this transformation from non-digital urban management to data-driven urban management.}
}
@article{RIM2020793,
title = {Deep Learning for Automated Sorting of Retinal Photographs},
journal = {Ophthalmology Retina},
volume = {4},
number = {8},
pages = {793-800},
year = {2020},
issn = {2468-6530},
doi = {https://doi.org/10.1016/j.oret.2020.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S2468653020301007},
author = {Tyler Hyungtaek Rim and Zhi Da Soh and Yih-Chung Tham and Henrik Hee Seung Yang and Geunyoung Lee and Youngnam Kim and Simon Nusinovici and Daniel Shu Wei Ting and Tien Yin Wong and Ching-Yu Cheng},
abstract = {Purpose
Though the domain of big data and artificial intelligence in health care continues to evolve, there is a lack of systemic methods to improve data quality and streamline the preparation process. To address this, we aimed to develop an automated sorting system (RetiSort) that accurately labels the type and laterality of retinal photographs.
Design
Cross-sectional study.
Participants
RetiSort was developed with retinal photographs from the Singapore Epidemiology of Eye Diseases (SEED) study.
Methods
The development of RetiSort was composed of 3 steps: 2 deep-learning (DL) algorithms and 1 rule-based classifier. For step 1, a DL algorithm was developed to locate the optic disc, the “landmark feature.” For step 2, based on the location of the optic disc derived from step 1, a rule-based classifier was developed to sort retinal photographs into 3 types: macular-centered, optic disc–centered, or related to other fields. Step 2 concurrently distinguished laterality (i.e., the left or right eye) of macular-centered photographs. For step 3, an additional DL algorithm was developed to differentiate the laterality of disc-centered photographs. Via the 3 steps, RetiSort sorted and labeled retinal images into (1) right macular–centered, (2) left macular–centered, (3) right optic disc–centered, (4) left optic disc–centered, and (5) images relating to other fields. Subsequently, the accuracy of RetiSort was evaluated on 5000 randomly selected retinal images from SEED as well as on 3 publicly available image databases (DIARETDB0, HEI-MED, and Drishti-GS). The main outcome measure was the accuracy for sorting of retinal photographs.
Results
RetiSort mislabeled 48 out of 5000 retinal images from SEED, representing an overall accuracy of 99.0% (95% confidence interval [CI], 98.7–99.3). In external tests, RetiSort mislabeled 1, 0, and 2 images, respectively, from DIARETDB0, HEI-MED, and Drishti-GS, representing an accuracy of 99.2% (95% CI, 95.8–99.9), 100%, and 98.0% (95% CI, 93.1–99.8), respectively. Saliency maps consistently showed that the DL algorithm in step 3 required pixels in the central left lateral border and optic disc of optic disc–centered retinal photographs to differentiate the laterality.
Conclusions
RetiSort is a highly accurate automated sorting system. It can aid in data preparation and has practical applications in DL research that uses retinal photographs.}
}
@article{YANG202027,
title = {CHAIN: Cyber Hierarchy and Interactional Network Enabling Digital Solution for Battery Full-Lifespan Management},
journal = {Matter},
volume = {3},
number = {1},
pages = {27-41},
year = {2020},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2020.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S2590238520301867},
author = {Shichun Yang and Rong He and Zhengjie Zhang and Yaoguang Cao and Xinlei Gao and Xinhua Liu},
keywords = {battery, cloud management, multi-scale, modeling, manufacturing},
abstract = {Summary
With the ever-growing need from the electric transportation industry, lithium-ion batteries are the systems of choice, offering high energy density, flexible and lightweight design, and longer lifespan than comparable battery technologies. Here, an effective management strategy, namely CHAIN, is presented for a multi-scale design and manufacturing process regarding material synthesis, characterization, electrochemical performance, and safety. The physical and electrochemical parameters of the materials are uploaded and shared in the cloud to perform real-time model calculation, achieving traceability from raw materials to products. Based on the cloud platform, a closed-loop design-and-optimization system is established, which can predict battery performance and provide an optimized management scheme by adjusting parameters simultaneously. As a multi-disciplinary system, the framework of CHAIN has profound theoretical and applicable value in full-lifespan management for battery systems, electric vehicles, and other emerging engineering systems yet to be addressed.}
}
@article{LIU2020101495,
title = {Discovering and merging related analytic datasets},
journal = {Information Systems},
volume = {91},
pages = {101495},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101495},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300065},
author = {Rutian Liu and Eric Simon and Bernd Amann and Stéphane Gançarski},
keywords = {Schema augmentation, Schema complement, Data quality, SAP HANA},
abstract = {The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues.}
}
@article{TIAN2020116335,
title = {Impact of water source mixture and population changes on the Al residue in megalopolitan drinking water},
journal = {Water Research},
volume = {186},
pages = {116335},
year = {2020},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2020.116335},
url = {https://www.sciencedirect.com/science/article/pii/S004313542030871X},
author = {Chenhao Tian and Chenghong Feng and Lei Chen and Qixuan Wang},
keywords = {Al residue, Mixed water sources, Big data analysis, Megalopolitan, Drinking water},
abstract = {This study establishes a new understanding of the contributions of Al residue in a megalopolitan drinking water supply system with mixed water sources. The different influences and contributions of foreign water source, resident migration and season changing to Al residue in drinking water were investigated. Especially, the role of Southern water transferred over 1200 km via the South-to-North Water Diversion Project in the Al residue of drinking water supply system of a northern megalopolitan were revealed for the first time. Comparisons of big data on Al residue in the water supply system with sole and mixed water sources showed that the introduction of Southern water enhanced the Al residue in drinking water by over 35%. The world's largest annual residents’ migration during Chinese Lunar New Year and the changes of season affect the water pipework hydrodynamics, which were embodied as the periodic changes of particulate aluminium and the relations with resident's temporal-spatial distribution in the megalopolitan. Because of the differences in water quality, Southern water promotes the release of historically deposited Al and facilitates the cleaning of old pipes.}
}
@article{CAO2020107850,
title = {Online investigation of vibration serviceability limitations using smartphones},
journal = {Measurement},
volume = {162},
pages = {107850},
year = {2020},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.107850},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120303882},
author = {Lei Cao and Jun Chen},
keywords = {Vibration serviceability, Online sampling, Big data, Data cleaning},
abstract = {Vibration serviceability issue has attracted increasing attentions recently. Many studies on vibration serviceability limitations have been performed in labs using simulation. The proposed limits were incompatible and lacked details about the physiological and environmental factors because of small sample sizes and unrealistic environments. This study proposes a novel online big data approach for investigating vibration serviceability limits in real environment. A smartphone-based application (App) was designed and spread to volunteers to collect multi-source heterogeneous data including questionnaires of personal judgement on vibration level, vibration signals, environmental and biological factors in their daily life. So far, 8521 records have been received. Data cleaning was performed and a qualified database with large volume and various types of factor information was produced. Analysis of the database showed that vibration limits given by the new method were compatible with previous results, but with more abundant details that were ignored in previous studies.}
}
@article{MATHEUS2020101284,
title = {Data science empowering the public: Data-driven dashboards for transparent and accountable decision-making in smart cities},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101284},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18300303},
author = {Ricardo Matheus and Marijn Janssen and Devender Maheshwari},
keywords = {Data science, Dashboards, E-government, Open government, Open data, Big data, Smart City, Design principles, Transparency, Accountability, Trust, Policy-making, Decision-making},
abstract = {Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.}
}
@incollection{PEZOULAS202019,
title = {Chapter 2 - Types and sources of medical and other related data},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {19-65},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000025},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Big data, Cohorts, Medical data acquisition, Sources of medical data, Types of medical data},
abstract = {This chapter presents the origin of medical data that comprises the core of any federated cloud platform that deals with medical data sharing and analytics. The different types and sources of medical data are extensively described along with applications and standard data acquisition protocols. Emphasis is given on the definition and the impact of the cohorts in clinical research, as well as the importance of the big data in medicine. The impact of the big data in medicine is also discussed along with emerging opportunities and challenges. The need to develop data standardization protocols across heterogeneous and dispersed data sources is finally highlighted, to enable the analysis of different types of medical big data.}
}
@article{JIANG2020161,
title = {Clicking position and user posting behavior in online review systems: A data-driven agent-based modeling approach},
journal = {Information Sciences},
volume = {512},
pages = {161-174},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309089},
author = {Guoyin Jiang and Xiaodong Feng and Wenping Liu and Xingjun Liu},
keywords = {Agent-based modeling, Big data, Online review systems, Clicking position, Posting behavior},
abstract = {In online review systems, a participant's level of knowledge impacts his/her posting behaviors, and an increase in knowledge occurs when the participant reads the reviews posted on the systems. To capture the collective dynamics of posting reviews, we used real-world big data collected over 153 months to drive an agent-based model for replicating the operation process of online review systems. The model explains the effects of clicking position (e.g., on a review webpage's serial list) and the number of items per webpage on posting contributions. Reading reviews from the last webpage only, or from the first webpage and last webpage simultaneously, can promote a greater review volume than reading reviews in other positions. This illustrates that representing primacy (first items) and recency (recent items) within one page simultaneously, or displaying recent items in reverse chronological order, are relatively better strategies for the webpage display of online reviews. The number of items plays a nonlinear moderating role in bridging the clicking position and posting behavior, and we determine the optimal number of items. To effectively establish strategies for webpage design in online review systems, business managers must switch from reliance on experience to reliance on an agent-based model as a decision support system for the formalized webpage design of online review systems.}
}