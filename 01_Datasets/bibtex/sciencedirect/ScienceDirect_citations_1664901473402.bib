@article{VYDRA2019101383,
title = {Techno-optimism and policy-pessimism in the public sector big data debate},
journal = {Government Information Quarterly},
volume = {36},
number = {4},
pages = {101383},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18302326},
author = {Simon Vydra and Bram Klievink},
keywords = {Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance},
abstract = {Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.}
}
@article{GHASEMAGHAEI201969,
title = {Does big data enhance firm innovation competency? The mediating role of data-driven insights},
journal = {Journal of Business Research},
volume = {104},
pages = {69-84},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304138},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data characteristics, Descriptive insight, Predictive insight, Prescriptive insight, Innovation competency},
abstract = {Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.}
}
@article{LAU2019209,
title = {Pitfalls in big data analysis: next-generation technologies, last-generation data},
journal = {Diagnostic Microbiology and Infectious Disease},
volume = {94},
number = {2},
pages = {209-210},
year = {2019},
issn = {0732-8893},
doi = {https://doi.org/10.1016/j.diagmicrobio.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732889318306710},
author = {Susanna K.P. Lau and Patrick C.Y. Woo}
}
@incollection{EBBELS2019329,
title = {Chapter 11 - Big Data and Databases for Metabolic Phenotyping},
editor = {John C. Lindon and Jeremy K. Nicholson and Elaine Holmes},
booktitle = {The Handbook of Metabolic Phenotyping},
publisher = {Elsevier},
pages = {329-367},
year = {2019},
isbn = {978-0-12-812293-8},
doi = {https://doi.org/10.1016/B978-0-12-812293-8.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122938000116},
author = {Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang Gao and Robert C. Glen},
keywords = {Metabolomics, Metabonomics, Metabolic phenotyping, Big data, Cloud computing, High-performance computing, Software tools, Databases, PhenoMeNal, Ethical, Legal, Social implications, ELSI},
abstract = {Metabolic phenotyping is entering the era of Big Data, leading to new opportunities and challenges. Cloud computing has been proposed as a novel paradigm, but as yet is not widely understood or used. In this chapter we introduce the concepts of Big Data and cloud computing, and discuss how they might change the landscape of metabolic phenotyping and analysis. We highlight some of the reasons for the increase in data size and explain advantages and disadvantages of large-scale computing in this context. We illustrate the area with a survey of software tools and databases currently available, and describe the newly developed cloud infrastructure “PhenoMeNal,” which will enable widespread use of these approaches. We conclude the chapter with a discussion of the important ethical, legal, and social implications (ELSI) of large-scale computing in this rapidly developing field.}
}
@article{TAI2019101704,
title = {Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry},
journal = {Artificial Intelligence in Medicine},
volume = {99},
pages = {101704},
year = {2019},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.101704},
url = {https://www.sciencedirect.com/science/article/pii/S0933365717301781},
author = {Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre},
keywords = {Big data, Machine learning, Precision medicine, AI, Mental health, Mental disease, Psychiatry, Data mining, RDoC, Research domain criteria, DSM-5. Schizophrenia, ADHD, Alzheimer, Depression, fMRI, MRI, Algorithms, IBM Watson, Neuro networking, Random forests, Decision trees, Support vector machines},
abstract = {Introduction
Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.
Methods
Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.
Results
Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.
Conclusions
Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.}
}
@incollection{SHARMA2019189,
title = {Chapter 8 - Why Big Data and What Is It? Basic to Advanced Big Data Journey for the Medical Industry},
editor = {Valentina E. Balas and Le Hoang Son and Sudan Jha and Manju Khari and Raghvendra Kumar},
booktitle = {Internet of Things in Biomedical Engineering},
publisher = {Academic Press},
pages = {189-212},
year = {2019},
isbn = {978-0-12-817356-5},
doi = {https://doi.org/10.1016/B978-0-12-817356-5.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173565000103},
author = {Neha Sharma and Malini M. Patil and Madhavi Shamkuwar},
keywords = {Big data, Medical big data, Healthcare data, Medical big data analytics, Healthcare data analytics, Data analytics, Pharmacology data analytics},
abstract = {The idea of big data is mainly reflected in its dimensions, which are popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses, to focus on data analysis, hypothesis generation, and ascertaining the progressive strength of association. Preliminary study reveals that big data analytics adopts many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive analytics. This evolving technology has tremendous application in healthcare, such as surveillance of safety or disease, predictive modeling, public health, pharma data analytics, clinical data analytics, healthcare analytics, and research. Moreover, the journey of big data in the medical domain is proving to be one of the important research thrusts of recent times. Study reveals that medical data is very specific and heterogeneous due to varied data sources such as scanned images, CT scan reports, doctor prescriptions, electronic health records (EHRs), etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions, bias, and limitations of the study of patients through observation. Therefore, special big data techniques are required to handle them. Besides, many ethical, legal, social, clinical, and utility challenges are also a part of the data-handling process, which makes the role of big data in the medical field very challenging. Nevertheless, big data analytics is a fuel to the healthcare system that will provide a healthier life to patients; the issues and bottlenecks when removed from the system will be a boon for the entire human race. The chapter focuses on understanding the big data characteristics in medical big data, medical big data analytics, and its various applications in the interest of society.}
}
@article{DEFREITASVISCONDI201954,
title = {A Systematic Literature Review on big data for solar photovoltaic electricity generation forecasting},
journal = {Sustainable Energy Technologies and Assessments},
volume = {31},
pages = {54-63},
year = {2019},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S2213138818301036},
author = {Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza},
keywords = {Systematic Literature Review, Solar energy forecasting, Machine learning, Data mining},
abstract = {Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.}
}
@article{BIKAKIS2019100123,
title = {Big Data Exploration, Visualization and Analytics},
journal = {Big Data Research},
volume = {18},
pages = {100123},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2214579619302254},
author = {Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil}
}
@article{RIVERA20191,
title = {Is Big Data About to Retire Expert Knowledge? A Predictive Maintenance Study},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {24},
pages = {1-6},
year = {2019},
note = {5th IFAC Symposium on Telematics Applications TA 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.364},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319322645},
author = {Domingo Llorente Rivera and Markus R. Scholz and Christoph Bühl and Markus Krauss and Klaus Schilling},
keywords = {industrial analytics, anomaly detection, predictive maintenance, hydraulic pump, stochastic modeling},
abstract = {In this contribution, a data-driven approach towards the prediction of maintenance for the critical component of an injection molding machine is presented. We present our path from exploring and cleaning the data towards the implementation of a prediction algorithm based on kernel density estimation. We give first analytical evidence of the algorithms potential. Moreover, we compare the approach described here with our previous work where we went a model-based approach and present advantages and disadvantages of the two approaches. We try to contribute to a non-comprehensive guide on the implementation of predictive maintenance systems for industrial mass production facilities.}
}
@incollection{CARNICERO2019121,
title = {Chapter 8 - Healthcare Decision-Making Support Based on the Application of Big Data to Electronic Medical Records: A Knowledge Management Cycle},
editor = {Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang},
booktitle = {Leveraging Biomedical and Healthcare Data},
publisher = {Academic Press},
pages = {121-131},
year = {2019},
isbn = {978-0-12-809556-0},
doi = {https://doi.org/10.1016/B978-0-12-809556-0.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095560000083},
author = {Javier Carnicero and David Rojas},
keywords = {Big Data, Electronic medical record, Practice-based medicine, Learning health system, Semantic interoperability},
abstract = {Any given health system needs to increase efficiency and effectiveness up to the point of requiring a transformation of their current model to ensure their sustainability and continuity. The electronic medical record (EMR) is the main source of knowledge to improve the quality of healthcare, clinical research, epidemiological surveillance, patient empowerment, personalized medicine, and clinical decision-making support systems. There is also a huge amount of available information related to diseases and other medical conditions, such as drugs and therapies, omics data (genetic and proteomic), social networks, and wearable devices. Big Data technologies allow the processing of this data to reach the final goal, which is a learning health system. The great diversity of data, sources, structures, and uses requires a data linkage procedure to integrate and harmonize these data. This generation of knowledge allows the transition from evidence-based medicine, which still prevails, to practice-based medicine. The key points for any Big Data project based on EMRs and other medical information sources are semantic interoperability, data structure and granularity, information quality, patient privacy, legal framework, and bioethics.}
}
@article{GHASEMAGHAEI201938,
title = {Can big data improve firm decision quality? The role of data quality and data diagnosticity},
journal = {Decision Support Systems},
volume = {120},
pages = {38-49},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300466},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data utilization, Data quality, Decision quality, Data diagnosticity},
abstract = {Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.}
}
@article{SAFHI201930,
title = {Assessing reliability of Big Data Knowledge Discovery process},
journal = {Procedia Computer Science},
volume = {148},
pages = {30-36},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919300055},
author = {Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi},
keywords = {Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining},
abstract = {Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V’s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.}
}
@article{SHUKLA20191015,
title = {Next generation smart sustainable auditing systems using Big Data Analytics: Understanding the interaction of critical barriers},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1015-1026},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301992},
author = {Manish Shukla and Lana Mattar},
keywords = {Big Data Analytics, Sustainable auditing systems, Barriers, RSPO, Interpretive Structural Modelling},
abstract = {In the current scenario, sustainable auditing, for example roundtable of sustainable palm oil (RSPO), requires a huge amount of data to be manually collected and entered into paper forms by farmers. Such systems are inherently inefficient, time-consuming, and, prone to errors. Researchers have proposed Big Data Analytics (BDA) based framework for next-generation smart sustainable auditing systems. Though theoretically feasible, real-life implementation of such frameworks is extremely difficult. Thus, this paper aims to identify the critical barriers that hinder the application of BDA based smart sustainable auditing system. It also aims to explore the dynamic interrelations among the barriers. We applied Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates BDA adoption barriers and their relationships. The proposed model illustrates how barriers are spread over various levels and how specific barriers impact other barriers through direct and/or transitive links. This study provides practitioners with a roadmap to prioritise the interventions to facilitate the adoption of BDA in the sustainable auditing systems. Insights of this study could be used by academics to enhance understanding of the barriers to BDA applications.}
}
@article{TALHA2019916,
title = {Big Data: Trade-off between Data Quality and Data Security},
journal = {Procedia Computer Science},
volume = {151},
pages = {916-922},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.127},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305915},
author = {M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI},
keywords = {Big Data, Data Quality, Data Security, Trade-off between Quality, Security},
abstract = {The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.}
}
@article{KUN2019556,
title = {Application of Big Data Technology in Scientific Research Data Management of Military Enterprises},
journal = {Procedia Computer Science},
volume = {147},
pages = {556-561},
year = {2019},
note = {2018 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.221},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919302406},
author = {Wang Kun and Liu Tong and Xie Xiaodan},
keywords = {big data technology, scientific research data, data analysis, decision},
abstract = {Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.}
}
@article{MOKTADIR20191063,
title = {Barriers to big data analytics in manufacturing supply chains: A case study from Bangladesh},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1063-1075},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301505},
author = {Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh Shukla},
keywords = {AHP, Big data analytics, Barriers to BDA, Delphi, Information and communication technology (ICT), Manufacturing supply chains},
abstract = {Recently, big data (BD) has attracted researchers and practitioners due to its potential usefulness in decision-making processes. Big data analytics (BDA) is becoming increasingly popular among manufacturing companies as it helps gain insights and make decisions based on BD. However, there many barriers to the adoption of BDA in manufacturing supply chains. It is therefore necessary for manufacturing companies to identify and examine the nature of each barrier. Previous studies have mostly built conceptual frameworks for BDA in a given situation and have ignored examining the nature of the barriers to BDA. Due to the significance of both BD and BDA, this research aims to identify and examine the critical barriers to the adoption of BDA in manufacturing supply chains in the context of Bangladesh. This research explores the existing body of knowledge by examining these barriers using a Delphi-based analytic hierarchy process (AHP). Data were obtained from five Bangladeshi manufacturing companies. The findings of this research are as follows: (i) data-related barriers are most important, (ii) technology-related barriers are second, and (iii) the five most important components of these barriers are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy, (d) lack of availability of BDA tools and (e) high cost of investment. The findings can assist industrial managers to understand the actual nature of the barriers and potential benefits of using BDA and to make policy regarding BDA adoption in manufacturing supply chains. A sensitivity analysis was carried out to justify the robustness of the barrier rankings.}
}
@article{SEO201969,
title = {A pilot infrastructure for searching rainfall metadata and generating rainfall product using the big data of NEXRAD},
journal = {Environmental Modelling & Software},
volume = {117},
pages = {69-75},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218307667},
author = {Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and Witold F. Krajewski},
keywords = {NEXRAD, Rainfall, Cloud computing, Level II data, Hydrology},
abstract = {The Iowa Flood Center (IFC) developed a pilot infrastructure to explore rainfall metadata (descriptive statistics) and generate rainfall products over the Iowa domain based on the NEXRAD Level II data directly accessible through cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD radar data. Taking advantage of the cloud storage benefits (unlimited storage and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data exploration systems, which often lead to massive data acquisition/ingestion and rapid filling of limited system storage. Its map-based interface allows researchers to select a space-time domain of interest, retrieve and visualize pre-calculated rainfall metadata, and generate radar-derived rainfall products. Because the system provides generalized approaches to compute metadata and process data for rainfall estimation, the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications.}
}
@article{ALVAREZSANCHEZ2019104824,
title = {TAQIH, a tool for tabular data quality assessment and improvement in the context of health data},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104824},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718304188},
author = {Roberto {Álvarez Sánchez} and Andoni {Beristain Iraola} and Gorka {Epelde Unanue} and Paul Carlin},
keywords = {Data quality, Exploratory data analysis, Data pre-processing},
abstract = {Background and Objectives
Data curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.
Methods
A web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.
Results
It has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with > 75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (∼ + 1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (> 70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.
Conclusions
TAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.}
}
@article{GUPTA2019466,
title = {Circular economy and big data analytics: A stakeholder perspective},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {466-474},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517314488},
author = {Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santibañez Gonzalez}},
keywords = {Circular economy, Big data, Stakeholder theory, Relational view, Supply chain management, Sustainability},
abstract = {The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.}
}
@article{YANG2019755,
title = {How big data enriches maritime research – a critical review of Automatic Identification System (AIS) data applications},
journal = {Transport Reviews},
volume = {39},
number = {6},
pages = {755-773},
year = {2019},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2019.1649315},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722001568},
author = {Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li},
keywords = {AIS data, data mining, navigation safety, ship behaviour analysis, environmental evaluation, advanced applications of AIS data},
abstract = {ABSTRACT
The information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.}
}
@article{SCHULER2019191,
title = {Big Data Readiness in Radiation Oncology: An Efficient Approach for Relabeling Radiation Therapy Structures With Their TG-263 Standard Name in Real-World Data Sets},
journal = {Advances in Radiation Oncology},
volume = {4},
number = {1},
pages = {191-200},
year = {2019},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S2452109418302240},
author = {Thilo Schuler and John Kipritidis and Thomas Eade and George Hruby and Andrew Kneebone and Mario Perez and Kylie Grimberg and Kylie Richardson and Sally Evill and Brooke Evans and Blanca Gallego},
abstract = {Purpose
To prepare for big data analyses on radiation therapy data, we developed Stature, a tool-supported approach for standardization of structure names in existing radiation therapy plans. We applied the widely endorsed nomenclature standard TG-263 as the mapping target and quantified the structure name inconsistency in 2 real-world data sets.
Methods and Materials
The clinically relevant structures in the radiation therapy plans were identified by reference to randomized controlled trials. The Stature approach was used by clinicians to identify the synonyms for each relevant structure, which was then mapped to the corresponding TG-263 name. We applied Stature to standardize the structure names for 654 patients with prostate cancer (PCa) and 224 patients with head and neck squamous cell carcinoma (HNSCC) who received curative radiation therapy at our institution between 2007 and 2017. The accuracy of the Stature process was manually validated in a random sample from each cohort. For the HNSCC cohort we measured the resource requirements for Stature, and for the PCa cohort we demonstrated its impact on an example clinical analytics scenario.
Results
All but 1 synonym group (“Hydrogel”) was mapped to the corresponding TG-263 name, resulting in a TG-263 relabel rate of 99% (8837 of 8925 structures). For the PCa cohort, Stature matched a total of 5969 structures. Of these, 5682 structures were exact matches (ie, following local naming convention), 284 were matched via a synonym, and 3 required manual matching. This original radiation therapy structure names therefore had a naming inconsistency rate of 4.81%. For the HNSCC cohort, Stature mapped a total of 2956 structures (2638 exact, 304 synonym, 14 manual; 10.76% inconsistency rate) and required 7.5 clinician hours. The clinician hours required were one-fifth of those that would be required for manual relabeling. The accuracy of Stature was 99.97% (PCa) and 99.61% (HNSCC).
Conclusions
The Stature approach was highly accurate and had significant resource efficiencies compared with manual curation.}
}
@article{LI20191259,
title = {Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm and data pre-processing technology},
journal = {Applied Energy},
volume = {242},
pages = {1259-1273},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.03.154},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919305495},
author = {Shuangqi Li and Hongwen He and Jianwei Li},
keywords = {Electric vehicles, Battery energy storage, Temperature-dependent model, Battery management system, Big data, Deep learning},
abstract = {As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.}
}
@article{CORTEREAL2019160,
title = {Unlocking the drivers of big data analytics value in firms},
journal = {Journal of Business Research},
volume = {97},
pages = {160-173},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.12.072},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318306908},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira and Aleš Popovič},
keywords = {IT business value, Big data analytics (BDA), Delphi method, Mixed methodology, Competitive advantage},
abstract = {Although big data analytics (BDA) is considered the next “frontier” in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.}
}
@article{PEZOULAS2019270,
title = {Medical data quality assessment: On the development of an automated framework for medical data curation},
journal = {Computers in Biology and Medicine},
volume = {107},
pages = {270-283},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519300733},
author = {Vasileios C. Pezoulas and Konstantina D. Kourou and Fanis Kalatzis and Themis P. Exarchos and Aliki Venetsanopoulou and Evi Zampeli and Saviana Gandolfo and Fotini Skopouli and Salvatore {De Vita} and Athanasios G. Tzioufas and Dimitrios I. Fotiadis},
keywords = {Big data, Data quality, Data quality assessment, Data curation, Data standardization},
abstract = {Data quality assessment has gained attention in the recent years since more and more companies and medical centers are highlighting the importance of an automated framework to effectively manage the quality of their big data. Data cleaning, also known as data curation, lies in the heart of the data quality assessment and is a key aspect prior to the development of any data analytics services. In this work, we present the objectives, functionalities and methodological advances of an automated framework for data curation from a medical perspective. The steps towards the development of a system for data quality assessment are first described along with multidisciplinary data quality measures. A three-layer architecture which realizes these steps is then presented. Emphasis is given on the detection and tracking of inconsistencies, missing values, outliers, and similarities, as well as, on data standardization to finally enable data harmonization. A case study is conducted in order to demonstrate the applicability and reliability of the proposed framework on two well-established cohorts with clinical data related to the primary Sjögren's Syndrome (pSS). Our results confirm the validity of the proposed framework towards the automated and fast identification of outliers, inconsistencies, and highly-correlated and duplicated terms, as well as, the successful matching of more than 85% of the pSS-related medical terms in both cohorts, yielding more accurate, relevant, and consistent clinical data.}
}
@article{JIA20191652,
title = {Opportunities and challenges of using big data for global health},
journal = {Science Bulletin},
volume = {64},
number = {22},
pages = {1652-1654},
year = {2019},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2019.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S2095927319305523},
author = {Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang}
}
@article{HADJSASSI2019534,
title = {A New Architecture for Cognitive Internet of Things and Big Data},
journal = {Procedia Computer Science},
volume = {159},
pages = {534-543},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.208},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313924},
author = {Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati},
keywords = {Internet of Things, Big-Data, Architecture, Cognitive, Data-flow},
abstract = {Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.}
}
@article{REN20191343,
title = {A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions},
journal = {Journal of Cleaner Production},
volume = {210},
pages = {1343-1365},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618334255},
author = {Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida},
keywords = {Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle},
abstract = {Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.}
}
@article{SIEMSANDERSON2019100071,
title = {An adaptive big data weather system for surface transportation},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {3},
pages = {100071},
year = {2019},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2019.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2590198219300703},
author = {Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William P. Mahoney and Sue Ellen Haupt},
keywords = {Big data, Pikalert, Road weather, Surface transportation, Pavement condition, Weather forecasts},
abstract = {Operating modern multi-modal surface transportation systems are becoming increasingly automated and driven by decision support systems. One aspect necessary for successful, safe, reliable, and efficient operation of any transportation network is real-time and forecasted weather and pavement condition information. Providing such information requires an adaptive system capable of blending large amounts of observational and model data that arrives quickly, in disparate formats and times, and blends and optimizes their use via expert systems and machine-learning algorithms. Quality control of the data is also essential, and historical data is required to both develop expert-based empirical algorithms and train machine learning models. This paper reports on the open-source Pikalert® system that brings together weather information and real-time data from connected vehicles to provide crucial information to enhance the safety and efficiency of surface transportation systems. This robust framework can be applied to a diverse array of user community specifications and is designed to rapidly ingest more, unique data sets as they become available. Ultimately, the developmental framework of this system will provide critical environmental information necessary to promote the development, growth, refinement, and expanded adoption of automated and connected multi-modal vehicular systems globally.}
}
@article{HUI2019S90,
title = {PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS},
journal = {Value in Health},
volume = {22},
pages = {S90},
year = {2019},
note = {ISPOR 2019: Rapid. Disruptive. Innovative: A New Era in HEOR},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2019.04.303},
url = {https://www.sciencedirect.com/science/article/pii/S1098301519304954},
author = {Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li}
}
@article{HONG20191387,
title = {Energy forecasting in the big data world},
journal = {International Journal of Forecasting},
volume = {35},
number = {4},
pages = {1387-1388},
year = {2019},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301062},
author = {Tao Hong and Pierre Pinson}
}
@article{SONG2019288,
title = {Dynamic assessment of PM2.5 exposure and health risk using remote sensing and geo-spatial big data},
journal = {Environmental Pollution},
volume = {253},
pages = {288-296},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.06.057},
url = {https://www.sciencedirect.com/science/article/pii/S026974911930418X},
author = {Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood},
keywords = {Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data, Environmental health},
abstract = {In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5 μm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80 μg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.}
}
@article{LIN2019197,
title = {Data source selection for information integration in big data era},
journal = {Information Sciences},
volume = {479},
pages = {197-213},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309162},
author = {Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao},
keywords = {Source selection, Data integration, Data cleaning},
abstract = {In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.}
}
@article{ALAOUI2019803,
title = {The Impact of Big Data Quality on Sentiment Analysis Approaches},
journal = {Procedia Computer Science},
volume = {160},
pages = {803-810},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317077},
author = {Imane El Alaoui and Youssef Gahi},
keywords = {Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining},
abstract = {Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.}
}
@article{RAJAN2019193,
title = {Towards a content agnostic computable knowledge repository for data quality assessment},
journal = {Computer Methods and Programs in Biomedicine},
volume = {177},
pages = {193-201},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718306254},
author = {Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen and Julio C. Facelli},
keywords = {Data Quality Metadata Repository, Knowledge representation, Data quality assessment, Data quality dimensions, Data quality framework},
abstract = {Background and objective
In recent years, several data quality conceptual frameworks have been proposed across the Data Quality and Information Quality domains towards assessment of quality of data. These frameworks are diverse, varying from simple lists of concepts to complex ontological and taxonomical representations of data quality concepts. The goal of this study is to design, develop and implement a platform agnostic computable data quality knowledge repository for data quality assessments.
Methods
We identified computable data quality concepts by performing a comprehensive literature review of articles indexed in three major bibliographic data sources. From this corpus, we extracted data quality concepts, their definitions, applicable measures, their computability and identified conceptual relationships. We used these relationships to design and develop a data quality meta-model and implemented it in a quality knowledge repository.
Results
We identified three primitives for programmatically performing data quality assessments: data quality concept, its definition, its measure or rule for data quality assessment, and their associations. We modeled a computable data quality meta-data repository and extended this framework to adapt, store, retrieve and automate assessment of other existing data quality assessment models.
Conclusion
We identified research gaps in data quality literature towards automating data quality assessments methods. In this process, we designed, developed and implemented a computable data quality knowledge repository for assessing quality and characterizing data in health data repositories. We leverage this knowledge repository in a service-oriented architecture to perform scalable and reproducible framework for data quality assessments in disparate biomedical data sources.}
}
@article{VANDERVOORT201927,
title = {Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {27-38},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17304951},
author = {H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer},
abstract = {Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.}
}
@article{ANEJIONU2019456,
title = {Spatial urban data system: A cloud-enabled big data infrastructure for social and economic urban analytics},
journal = {Future Generation Computer Systems},
volume = {98},
pages = {456-473},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.03.052},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18319046},
author = {Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh and Yeran Sun and David McArthur and Phil Mason and Rod Walpole},
keywords = {Urban big data infrastructure, Urban analytics, Spatial urban indicators, Small area assessment, Spatial big data},
abstract = {The Spatial Urban Data System (SUDS) is a spatial big data infrastructure to support UK-wide analytics of the social and economic aspects of cities and city-regions. It utilises data generated from traditional as well as new and emerging sources of urban data. The SUDS deploys geospatial technology, synthetic small area urban metrics, and cloud computing to enable urban analytics, and geovisualization with the goal of deriving actionable knowledge for better urban management and data-driven urban decision making. At the core of the system is a programme of urban indicators generated by using novel forms of data and urban modelling and simulation programme. SUDS differs from other similar systems by its emphasis on the generation and use of regularly updated spatially-activated urban area metrics from real or near-real time data sources, to enhance understanding of intra-city interactions and dynamics. By deploying public transport, labour market accessibility and housing advertisement data in the system, we were able to identify spatial variations of key urban services at intra-city levels as well as social and economically-marginalised output areas in major cities across the UK. This paper discusses the design and implementation of SUDS, the challenges and limitations encountered, and considerations made during its development. The innovative approach adopted in the design of SUDS will enable it to support research and analysis of urban areas, policy and city administration, business decision-making, private sector innovation, and public engagement. Having been tested with housing, transport and employment metrics, efforts are ongoing to integrate information from other sources such as IoT, and User Generated Content into the system to enable urban predictive analytics.}
}
@article{PALANISAMY2019415,
title = {Implications of big data analytics in developing healthcare frameworks – A review},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {31},
number = {4},
pages = {415-425},
year = {2019},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817302938},
author = {Venketesh Palanisamy and Ramkumar Thirunavukarasu},
keywords = {Big data, Healthcare, Framework, Infrastructure, Analytics, Patterns, Tools},
abstract = {The domain of healthcare acquired its influence by the impact of big data since the data sources involved in the healthcare organizations are well-known for their volume, heterogeneous complexity and high dynamism. Though the role of big data analytical techniques, platforms, tools are realized among various domains, their impact on healthcare organization for implementing and delivering novel use-cases for potential healthcare applications shows promising research directions. In the context of big data, the success of healthcare applications solely depends on the underlying architecture and utilization of appropriate tools as evidenced in pioneering research attempts. Novel research works have been carried out for deriving application specific healthcare frameworks that offer diversified data analytical capabilities for handling sources of data ranging from electronic health records to medical images. In this paper, we have presented various analytical avenues that exist in the patient-centric healthcare system from the perspective of various stakeholders. We have also reviewed various big data frameworks with respect to underlying data sources, analytical capability and application areas. In addition, the implication of big data tools in developing healthcare eco system is also presented.}
}
@article{MOHARM2019100945,
title = {State of the art in big data applications in microgrid: A review},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100945},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100945},
url = {https://www.sciencedirect.com/science/article/pii/S147403461830702X},
author = {Karim Moharm},
keywords = {Big data, Microgrid},
abstract = {The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.}
}
@article{RAJABION2019271,
title = {Healthcare big data processing mechanisms: The role of cloud computing},
journal = {International Journal of Information Management},
volume = {49},
pages = {271-289},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217304917},
author = {Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar},
keywords = {Cloud computing, Processing, Healthcare, Big data, Review},
abstract = {Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.}
}
@article{UEDA2019150,
title = {Delineation of Nitrogen Signaling Networks: Computational Approaches in the Big Data Era},
journal = {Molecular Plant},
volume = {12},
number = {2},
pages = {150-152},
year = {2019},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1674205219300139},
author = {Yoshiaki Ueda and Shuichi Yanagisawa}
}
@article{RICHTER201929,
title = {Efficient learning from big data for cancer risk modeling: A case study with melanoma},
journal = {Computers in Biology and Medicine},
volume = {110},
pages = {29-39},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519301477},
author = {Aaron N. Richter and Taghi M. Khoshgoftaar},
keywords = {Big data, Cloud computing, Machine learning, Electronic health records, Early detection of cancer},
abstract = {Background
Building cancer risk models from real-world data requires overcoming challenges in data preprocessing, efficient representation, and computational performance. We present a case study of a cloud-based approach to learning from de-identified electronic health record data and demonstrate its effectiveness for melanoma risk prediction.
Methods
We used a hybrid distributed and non-distributed approach to computing in the cloud: distributed processing with Apache Spark for data preprocessing and labeling, and non-distributed processing for machine learning model training with scikit-learn. Moreover, we explored the effects of sampling the training dataset to improve computational performance. Risk factors were evaluated using regression weights as well as tree SHAP values.
Results
Among 4,061,172 patients who did not have melanoma through the 2016 calendar year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted classifier achieved the best predictive performance with cross-validation (AUC = 0.799, Sensitivity = 0.753, Specificity = 0.688). Compared to a model built on the original data, a dataset two orders of magnitude smaller could achieve statistically similar or better performance with less than 1% of the training time and cost.
Conclusions
We produced a model that can effectively predict melanoma risk for a diverse dermatology population in the U.S. by using hybrid computing infrastructure and data sampling. For this de-identified clinical dataset, sampling approaches significantly shortened the time for model building while retaining predictive accuracy, allowing for more rapid machine learning model experimentation on familiar computing machinery. A large number of risk factors (>300) were required to produce the best model.}
}
@article{LIANG2019290,
title = {A survey on big data-driven digital phenotyping of mental health},
journal = {Information Fusion},
volume = {52},
pages = {290-307},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253518305244},
author = {Yunji Liang and Xiaolong Zheng and Daniel D. Zeng},
keywords = {Digital phenotyping, Big data, Mental health, Data mining, Information fusion},
abstract = {The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.}
}
@article{GALETSI2019112533,
title = {Values, challenges and future directions of big data analytics in healthcare: A systematic review},
journal = {Social Science & Medicine},
volume = {241},
pages = {112533},
year = {2019},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2019.112533},
url = {https://www.sciencedirect.com/science/article/pii/S0277953619305271},
author = {P. Galetsi and K. Katsaliaki and S. Kumar},
keywords = {Systematic review, Big data analytics, Health-medicine, Decision-making, Organizational and societal values, Preferred reporting items for systematic reviews and meta-analyses},
abstract = {The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.}
}
@article{NEILSON201935,
title = {Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications},
journal = {Big Data Research},
volume = {17},
pages = {35-44},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579617303866},
author = {Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra},
keywords = {Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero},
abstract = {Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.}
}
@article{CHOI2019139,
title = {Data quality challenges for sustainable fashion supply chain operations in emerging markets: Roles of blockchain, government sponsors and environment taxes},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {131},
pages = {139-152},
year = {2019},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2019.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S1366554519311494},
author = {Tsan-Ming Choi and Suyuan Luo},
keywords = {Fashion business operations, Supply chain centralization, Emerging markets, Sustainable operations, Social welfare},
abstract = {In emerging markets, there are data quality problems. In this paper, we establish theoretical models to explore how data quality problems affect sustainable fashion supply chain operations. We start with the decentralized supply chain and find that poor data quality lowers supply chain profit and social welfare. We consider the implementation of blockchain to help and identify the situation in which blockchain helps enhance social welfare but brings harm to supply chain profitability. We propose a government sponsor scheme as well as an environment taxation waiving scheme to help. We further extend the study to the centralized supply chain setting.}
}
@incollection{AGOSTON201953,
title = {Chapter 4 - Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma},
editor = {Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang},
booktitle = {Leveraging Biomedical and Healthcare Data},
publisher = {Academic Press},
pages = {53-75},
year = {2019},
isbn = {978-0-12-809556-0},
doi = {https://doi.org/10.1016/B978-0-12-809556-0.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095560000046},
author = {Denes V. Agoston},
keywords = {Big Data, Artificial intelligence and machine learning in neurotrauma},
abstract = {Rapid advances in the collection, storage, and analysis of large volumes of data—Big Data—offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the “most complex disease of the most complex organ.” While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches—text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.}
}
@article{URREHMAN2019247,
title = {The role of big data analytics in industrial Internet of Things},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {247-259},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18313645},
author = {Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad Imran and Prem Prakash Jayaraman and Charith Perera},
keywords = {Internet of Things, Cyber-physical systems, Cloud computing, Analytics, Big data},
abstract = {Big data production in industrial Internet of Things (IIoT) is evident due to the massive deployment of sensors and Internet of Things (IoT) devices. However, big data processing is challenging due to limited computational, networking and storage resources at IoT device-end. Big data analytics (BDA) is expected to provide operational- and customer-level intelligence in IIoT systems. Although numerous studies on IIoT and BDA exist, only a few studies have explored the convergence of the two paradigms. In this study, we investigate the recent BDA technologies, algorithms and techniques that can lead to the development of intelligent IIoT systems. We devise a taxonomy by classifying and categorising the literature on the basis of important parameters (e.g. data sources, analytics tools, analytics techniques, requirements, industrial analytics applications and analytics types). We present the frameworks and case studies of the various enterprises that have benefited from BDA. We also enumerate the considerable opportunities introduced by BDA in IIoT. We identify and discuss the indispensable challenges that remain to be addressed, serving as future research directions.}
}
@article{TSAI2019306,
title = {Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {306-310},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300347},
author = {Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez},
abstract = {In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.}
}
@article{ZHU2019229,
title = {The application of big data and the development of nursing science: A discussion paper},
journal = {International Journal of Nursing Sciences},
volume = {6},
number = {2},
pages = {229-234},
year = {2019},
issn = {2352-0132},
doi = {https://doi.org/10.1016/j.ijnss.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352013218305507},
author = {Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan},
keywords = {Artificial intelligence, Data mining, Knowledge bases, Nursing},
abstract = {Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.}
}
@article{JAN2019275,
title = {Deep learning in big data Analytics: A comparative study},
journal = {Computers & Electrical Engineering},
volume = {75},
pages = {275-287},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315835},
author = {Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon},
keywords = {Big data, Deep learning, Deep belief networks, Convolutional Neural Networks},
abstract = {Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.}
}
@article{SHAMIM2019103135,
title = {Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view},
journal = {Information & Management},
volume = {56},
number = {6},
pages = {103135},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618302854},
author = {Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan},
keywords = {Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China},
abstract = {This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.}
}
@article{BENASSULI20197,
title = {Bringing big data analytics closer to practice: A methodological explanation and demonstration of classification algorithms},
journal = {Health Policy and Technology},
volume = {8},
number = {1},
pages = {7-13},
year = {2019},
issn = {2211-8837},
doi = {https://doi.org/10.1016/j.hlpt.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211883718301631},
author = {Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner},
keywords = {Congestive heart failure, Machine learning, Logistic regression, Boosted decision tree, Support vector machine, Neural network},
abstract = {Background
Big data analytics are becoming more prevalent due to the recent availability of health data. Yet in spite of evidence supporting the potential contribution of big data analytics to health policy makers and care providers, these tools are still too complex to be routinely used. Further, access to comprehensive datasets required for more accurate results is complex and costly. Consequently, big data analytics are mostly used by researchers and experts who are far removed from actual clinical practice. Hence, policy makers should allocate resources to encourage studies that clarify and simplify big data analytics so it can be used by non-experts (e.g., clinicians, practitioners and decision-makers who may not have advanced computer skills). It is also important to fund data collection and integration from various health IT, a pre-condition for any big data analytics project.
Objectives
To methodologically clarify the rationale and logic behind several analytics algorithms to help non-expert users employ big data analytics by understanding how to implement relatively easy to use platforms as Azure ML.
Methods
We demonstrate the predictive power of four known algorithms and compare their accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.
Results
The results of our models outperform those reported in the literature, attesting to the strength of some of the models, and the utility of comprehensive data.
Conclusions
The results support our call to policy makers to allocate resources to establishing comprehensive, integrated health IT systems, and to projects aimed at simplifying ML analytics.}
}
@article{RAUT201910,
title = {Linking big data analytics and operational sustainability practices for sustainable business management},
journal = {Journal of Cleaner Production},
volume = {224},
pages = {10-24},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.03.181},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619308753},
author = {Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede},
keywords = {Big-data analytics, Ecological-economic-social sustainability, Green practices, SustainableOperations management, Structural equation modelling-artificial neural network, Emerging economies},
abstract = {Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.}
}
@article{GHANI2019417,
title = {Social media big data analytics: A survey},
journal = {Computers in Human Behavior},
volume = {101},
pages = {417-428},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.08.039},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830414X},
author = {Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed},
keywords = {Big data, Social media, Machine learning, Analytics},
abstract = {Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.}
}
@article{LIN201949,
title = {Strategic orientations, developmental culture, and big data capability},
journal = {Journal of Business Research},
volume = {105},
pages = {49-60},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304333},
author = {Canchu Lin and Anand Kunnathur},
keywords = {Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture},
abstract = {Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.}
}
@article{NUNEZREIZ201952,
title = {Big data and machine learning in critical care: Opportunities for collaborative research},
journal = {Medicina Intensiva (English Edition)},
volume = {43},
number = {1},
pages = {52-57},
year = {2019},
issn = {2173-5727},
doi = {https://doi.org/10.1016/j.medine.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S217357271930013X},
author = {A. {Núñez Reiz}},
keywords = {Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo},
abstract = {The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.}
}
@article{GARCIAGIL2019135,
title = {Enabling Smart Data: Noise filtering in Big Data classification},
journal = {Information Sciences},
volume = {479},
pages = {135-152},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309460},
author = {Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera},
keywords = {Big Data, Smart Data, Classification, Class noise, Label noise.},
abstract = {In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.}
}
@article{BEN2019403,
title = {A spatio-temporally weighted hybrid model to improve estimates of personal PM2.5 exposure: Incorporating big data from multiple data sources},
journal = {Environmental Pollution},
volume = {253},
pages = {403-411},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S026974911930795X},
author = {YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong},
keywords = {Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai},
abstract = {An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5 μm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3 ± 27.1 μg/m3 (n = 365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1 ± 13.9 μg/m3 (n = 365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.}
}
@article{YASSINE2019563,
title = {IoT big data analytics for smart homes with fog and cloud computing},
journal = {Future Generation Computer Systems},
volume = {91},
pages = {563-573},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18311099},
author = {Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam Muhammad},
keywords = {Internet of Things (IoT), Cloud computing, Fog computing, Big data analytics, Energy management, Smart homes},
abstract = {Internet of Things (IoT) analytics is an essential mean to derive knowledge and support applications for smart homes. Connected appliances and devices inside the smart home produce a significant amount of data about consumers and how they go about their daily activities. IoT analytics can aid in personalizing applications that benefit both homeowners and the ever growing industries that need to tap into consumers profiles. This article presents a new platform that enables innovative analytics on IoT captured data from smart homes. We propose the use of fog nodes and cloud system to allow data-driven services and address the challenges of complexities and resource demands for online and offline data processing, storage, and classification analysis. We discuss in this paper the requirements and the design components of the system. To validate the platform and present meaningful results, we present a case study using a dataset acquired from real smart home in Vancouver, Canada. The results of the experiments show clearly the benefit and practicality of the proposed platform.}
}
@article{CORIZZO201918,
title = {Anomaly Detection and Repair for Accurate Predictions in Geo-distributed Big Data},
journal = {Big Data Research},
volume = {16},
pages = {18-35},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579618302119},
author = {Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz},
keywords = {Anomaly detection, Data repair, Geo-distributed big data, Spatial autocorrelation, Neural networks, Gradient-boosting},
abstract = {The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.}
}
@article{MILNE2019235,
title = {Big data and understanding change in the context of planning transport systems},
journal = {Journal of Transport Geography},
volume = {76},
pages = {235-244},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2017.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300984},
author = {Dave Milne and David Watling},
abstract = {This paper considers the implications of so-called ‘big data’ for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly ‘special’ about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.}
}
@incollection{SAHOO2019227,
title = {Chapter 9 - Intelligence-Based Health Recommendation System Using Big Data Analytics},
editor = {Nilanjan Dey and Himansu Das and Bighnaraj Naik and Himansu Sekhar Behera},
booktitle = {Big Data Analytics for Intelligent Healthcare Management},
publisher = {Academic Press},
pages = {227-246},
year = {2019},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-818146-1},
doi = {https://doi.org/10.1016/B978-0-12-818146-1.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818146100009X},
author = {Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das},
keywords = {Big data analytics, Classification, Healthcare, Privacy preservation, Recommendation system},
abstract = {In today's digital world, healthcare is one of the core areas in the medical domain. A healthcare system is required to analyze a large amount of patient data, which helps to derive insights and predictions of disease. This system should be intelligent and able to predict the patient's health condition by analyzing the patient's lifestyle, physical health records, and social activities. The health recommendation system (HRS) is becoming an important platform for healthcare services. In this context, health intelligent systems have become indispensable tools in decision-making processes in the healthcare sector. The main objective is to ensure the availability of valuable information at the right time by ensuring information quality, trustworthiness, authentication, and privacy. As people use social networks to learn about their health condition, so the HRS is very important to derive outcomes such as recommending diagnosis, health insurance, clinical pathway-based treatment methods, and alternative medicines based on the patient's health profile. In this chapter, we discuss recent research that targeted utilization of large volumes of medical data while combining multimodal data from disparate sources, which reduces the workload and cost in healthcare. In the healthcare sector, big data analytics using a recommendation system has an important role in terms of decision-making processes regarding the patient's health. This chapter presents a proposed intelligent HRS that provides an insight into how to use big data analytics for implementing an effective health recommendation engine and shows how to transform the healthcare industry from the traditional scenario to more personalized paradigm in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE value when compared to existing approaches.}
}
@article{YE201965,
title = {A hybrid IT framework for identifying high-quality physicians using big data analytics},
journal = {International Journal of Information Management},
volume = {47},
pages = {65-75},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830834X},
author = {Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang},
keywords = {Online healthcare communities, Physician identifying, Signaling theory, Machine learning, Topic modeling, Multi-criterion analysis},
abstract = {Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.}
}
@article{BAIG2019102095,
title = {Big data adoption: State of the art and research challenges},
journal = {Information Processing & Management},
volume = {56},
number = {6},
pages = {102095},
year = {2019},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102095},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319301773},
author = {Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi},
keywords = {Big data adoption, Technology–Organization–Environment, Diffusion of Innovations},
abstract = {Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology–Organization–Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.}
}
@article{ALIC2019243,
title = {BIGSEA: A Big Data analytics platform for public transportation information},
journal = {Future Generation Computer Systems},
volume = {96},
pages = {243-269},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304448},
author = {Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and Nádia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira},
abstract = {Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe–Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/).}
}
@article{BELHADI2019106099,
title = {Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies},
journal = {Computers & Industrial Engineering},
volume = {137},
pages = {106099},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.106099},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219305686},
author = {Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and Said {El fezazi}},
keywords = {Big Data Analytics, Manufacturing process, Big Data Analytics capabilities, Business intelligence, Literature review, Multiple case study},
abstract = {Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.}
}
@article{HAMALAINEN2019100105,
title = {Industrial applications of big data in disruptive innovations supporting environmental reporting},
journal = {Journal of Industrial Information Integration},
volume = {16},
pages = {100105},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.100105},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300044},
author = {Esa Hämäläinen and Tommi Inkinen},
keywords = {Big data, Disruption, Responsible, Process industry, Economic efficiency, Economic geography},
abstract = {Disruptive innovations are usually identified as ideas that are created ‘outside the box’. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.}
}
@article{ZHANG2019814,
title = {Big data driven decision-making for batch-based production systems},
journal = {Procedia CIRP},
volume = {83},
pages = {814-818},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119310169},
author = {Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li},
keywords = {Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan},
abstract = {The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).}
}
@article{PURANIK2019838,
title = {The perils and pitfalls of big data analysis in medicine},
journal = {The Ocular Surface},
volume = {17},
number = {4},
pages = {838-839},
year = {2019},
issn = {1542-0124},
doi = {https://doi.org/10.1016/j.jtos.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1542012419301740},
author = {C.J. Puranik and Sreenivasa Rao and S. Chennamaneni}
}
@article{ANDREASEN201926,
title = {Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices},
journal = {Journal of Econometrics},
volume = {212},
number = {1},
pages = {26-46},
year = {2019},
note = {Big Data in Dynamic Predictive Econometric Modeling},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2019.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0304407619300740},
author = {Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch},
keywords = {Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson–Siegel model},
abstract = {Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.}
}
@article{LOZADA2019e02541,
title = {Big data analytics capability and co-innovation: An empirical study},
journal = {Heliyon},
volume = {5},
number = {10},
pages = {e02541},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02541},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019362012},
author = {Nelson Lozada and Jose Arias-Pérez and Geovanny Perdomo-Charry},
keywords = {Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation},
abstract = {There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.}
}
@article{NUNEZREIZ201952,
title = {Big data and machine learning in critical care: Opportunities for collaborative research},
journal = {Medicina Intensiva},
volume = {43},
number = {1},
pages = {52-57},
year = {2019},
issn = {0210-5691},
doi = {https://doi.org/10.1016/j.medin.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0210569118301827},
author = {Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García}},
keywords = {Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo},
abstract = {The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.}
}
@article{DUNCAN2019127,
title = {Big data sharing and analysis to advance research in post-traumatic epilepsy},
journal = {Neurobiology of Disease},
volume = {123},
pages = {127-136},
year = {2019},
note = {Antiepileptogenesis following Traumatic Brain Injury},
issn = {0969-9961},
doi = {https://doi.org/10.1016/j.nbd.2018.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0969996118301700},
author = {Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga},
keywords = {Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI},
abstract = {We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.}
}
@article{SYED2019136,
title = {Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {136-151},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18321071},
author = {Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi},
keywords = {Ambient Assisted Living (AAL), Big data analytics, Internet of Medical Things (IoMT), Machine learning techniques, Physical activities, Wearable sensors},
abstract = {In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.}
}
@incollection{KABALCI2019309,
title = {Chapter 8 - Big data, privacy and security in smart grids},
editor = {Ersan Kabalci and Yasin Kabalci},
booktitle = {From Smart Grid to Internet of Energy},
publisher = {Academic Press},
pages = {309-333},
year = {2019},
isbn = {978-0-12-819710-3},
doi = {https://doi.org/10.1016/B978-0-12-819710-3.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197103000089},
author = {Ersan Kabalci and Yasin Kabalci},
keywords = {Internet of things (IoT), Machine-to-machine communication (M2M), Human to machine (H2M), Machine learning, Smart grid security, Hadoop, Data mining, Security, Privacy},
abstract = {The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.}
}
@article{SOUIBGUI2019676,
title = {Data quality in ETL process: A preliminary study},
journal = {Procedia Computer Science},
volume = {159},
pages = {676-687},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314097},
author = {Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and Sadok Ben Yahia},
keywords = {Business Intelligence & Analytics, ETL quality, Data, process quality, Talend Data Integration, Talend Data Quality},
abstract = {The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics. We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research.}
}
@article{SILVA2019532,
title = {Risk Analysis of Using Big Data in Computer Sciences},
journal = {Procedia Computer Science},
volume = {160},
pages = {532-537},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317521},
author = {Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fernández},
keywords = {Data management, data quality, decision making, data analysis},
abstract = {Today, as technologies mature and people are encouraged to contribute data to organizations’ databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.}
}
@article{RANA2019807,
title = {How Big Data Science Can Improve Linkage and Retention in Care},
journal = {Infectious Disease Clinics of North America},
volume = {33},
number = {3},
pages = {807-815},
year = {2019},
note = {HIV},
issn = {0891-5520},
doi = {https://doi.org/10.1016/j.idc.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0891552019300455},
author = {Aadia I. Rana and Michael J. Mugavero},
keywords = {HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance}
}
@article{NIMMAGADDA20191155,
title = {On Modelling Big Data Guided Supply Chains in Knowledge-Base Geographic Information Systems},
journal = {Procedia Computer Science},
volume = {159},
pages = {1155-1164},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.284},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314814},
author = {Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood},
keywords = {Supply Chain Management, Project Management, Laws of Geography, Domain Ontologies, Data Mining},
abstract = {We examine the existing goals of business- and geographic - information systems and their influence on logistics and supply chain management systems. Modelling supply chain management systems is held back because of lack of consistent and poorly aligned data with supply chain elements and processes. The issues constraining the decision-making process limit the connectivity between supply chains and geographically controlled database systems. The heterogeneous and unstructured data are added challenges to connectivity and integration processes. The research focus is on analysing the data heterogeneity and multidimensionality relevant to supply chain systems and geographically controlled databases. In pursuance of the challenges, a unified methodological framework is designed with data structuring, data warehousing and mining, visualization and interpretation artefacts to support connectivity and integration process. Multidimensional ontologies, ecosystem conceptualization and Big Data novelty are added motivations, facilitating the relationships between events of supply chain operations. The models construed for optimizing the resources are analysed in terms of effectiveness of the integrated framework articulations in global supply chains that obey laws of geography. The integrated articulations analysed with laws of geography can affect the operational costs, sure for better with reduced lead times and enhanced stock management.}
}
@article{PEDRO20193,
title = {Capabilities and Readiness for Big Data Analytics},
journal = {Procedia Computer Science},
volume = {164},
pages = {3-10},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.147},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321866},
author = {Jenifer Pedro and Irwin Brown and Mike Hart},
keywords = {Big data analytics, organisational readiness, organisational capabilities, frameworks, business analytics, thematic analysis},
abstract = {Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added.}
}
@article{LEEPOST2019113135,
title = {Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development},
journal = {Decision Support Systems},
volume = {126},
pages = {113135},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113135},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301642},
author = {Anita Lee-Post and Ram Pakath},
keywords = {Data quality, Big data, Secondary data, Numerical data, Quality threshold},
abstract = {An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.}
}
@article{PIRRACCHIO2019377,
title = {Big data and targeted machine learning in action to assist medical decision in the ICU},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {38},
number = {4},
pages = {377-384},
year = {2019},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352556818302169},
author = {Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon and Alan Hubbard},
abstract = {Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.}
}
@article{WONG201944,
title = {Artificial Intelligence for infectious disease Big Data Analytics},
journal = {Infection, Disease & Health},
volume = {24},
number = {1},
pages = {44-48},
year = {2019},
issn = {2468-0451},
doi = {https://doi.org/10.1016/j.idh.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468045118301445},
author = {Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang},
keywords = {Infectious diseases modelling, Emergency response, Artificial Intelligence, Machine learning},
abstract = {Background
Since the beginning of the 21st century, the amount of data obtained from public health surveillance has increased dramatically due to the advancement of information and communications technology and the data collection systems now in place.
Methods
This paper aims to highlight the opportunities gained through the use of Artificial Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection in this information age.
Results and Conclusion
It is foreseeable that together with reliable data management platforms AI methods will enable analysis of massive infectious disease and surveillance data effectively to support government agencies, healthcare service providers, and medical professionals to response to disease in the future.}
}
@article{RIDZUAN2019731,
title = {A Review on Data Cleansing Methods for Big Data},
journal = {Procedia Computer Science},
volume = {161},
pages = {731-738},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919318885},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {data cleansing, big data, data quality},
abstract = {Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.}
}
@article{MIKALEF2019261,
title = {Big data analytics and firm performance: Findings from a mixed-method approach},
journal = {Journal of Business Research},
volume = {98},
pages = {261-276},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S014829631930061X},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty},
abstract = {Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.}
}
@article{ARIYALURANHABEEB2019289,
title = {Real-time big data processing for anomaly detection: A Survey},
journal = {International Journal of Information Management},
volume = {45},
pages = {289-307},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218301658},
author = {Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran},
keywords = {Real-time, Big data processing, Anomaly detection and machine learning algorithms},
abstract = {The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.}
}
@article{LI2019168,
title = {Lithium-ion battery modeling based on Big Data},
journal = {Energy Procedia},
volume = {159},
pages = {168-173},
year = {2019},
note = {Renewable Energy Integration with Mini/Microgrid},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2018.12.046},
url = {https://www.sciencedirect.com/science/article/pii/S1876610218313419},
author = {Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang},
keywords = {electric vehicle, lithium-ion power battery, modeling, battery management, bigdata, deeplearning},
abstract = {Battery is the bottleneck technology of electric vehicles. The complex chemical reactions inside the battery are difficult to monitor directly. The establishment of a precise mathematical model for the battery is of great significance in ensuring the secure and stable operation of the battery management system. First of all, a data cleaning method based on machine learning is put forward, which is applicable to the characteristics of big data from batteries in electric vehicles. Secondly, this paper establishes a lithium-ion battery model based on deep learning algorithm and the error of model based on different algorithms is compared. The data of electric buses are used for validating the effectiveness of the model. The result shows that the data cleaning method achieves good results, in the case of the terminal voltage missing, the mean absolute percentage error of filling is within 4%, and the battery modeling method in this paper is able to simulate the battery characteristics accurately, and the mean absolute percentage error of the terminal voltage estimation is within 2.5%.}
}
@article{JONES20193,
title = {What we talk about when we talk about (big) data},
journal = {The Journal of Strategic Information Systems},
volume = {28},
number = {1},
pages = {3-16},
year = {2019},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0963868718302622},
author = {Matthew Jones},
abstract = {In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between “data in principle” as they are recorded, and the “data in practice” as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.}
}
@article{LI2019103149,
title = {Experience and reflection from China’s Xiangya medical big data project},
journal = {Journal of Biomedical Informatics},
volume = {93},
pages = {103149},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103149},
url = {https://www.sciencedirect.com/science/article/pii/S153204641930067X},
author = {Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan},
keywords = {Medical big data, Data sharing, Information security, Cooperation mechanism, Medical data acquisition, Medical data centre},
abstract = {The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.}
}
@article{HUANG2019592,
title = {Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {592-599},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619317810},
author = {Lang Huang and Chao Wu and Bing Wang},
keywords = {Big data, Production safety management, Big-data-driven, Challenges, Opportunities},
abstract = {Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.}
}
@article{LI2019991,
title = {Prospects for energy economy modelling with big data: Hype, eliminating blind spots, or revolutionising the state of the art?},
journal = {Applied Energy},
volume = {239},
pages = {991-1002},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919302922},
author = {Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan},
keywords = {Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy data, Big data},
abstract = {Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.}
}
@article{SCHUH201943,
title = {Data quality program management for digital shadows of products},
journal = {Procedia CIRP},
volume = {86},
pages = {43-48},
year = {2019},
note = {7th CIRP Global Web Conference – Towards shifted production value stream patterns through inference of data, models, and technology (CIRPe 2019)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120300366},
author = {Günther Schuh and Eric Rebentisch and Michael Riesener and Thorben Ipers and Christian Tönnes and Merle-Hendrikje Jank},
keywords = {data quality program, digital shadow, data quality management},
abstract = {Nowadays, companies are facing challenges due to increasingly dynamic market environments, a growing internal and external complexity, as well as globally intensifying competition. To keep pace, companies need to establish extensive knowledge about their business and its surroundings based on insights generated through the analysis of data. The digital shadow is a novel information system concept that integrates data of heterogeneous sources to provide product-related information to stakeholders across the company. The concept aims at improving the results of decision making, enabling advanced data analyses, and increasing information handling efficiency. As insufficient information quality has immediate effects on the utility of the information and induces significant costs, managing the quality of the digital shadow data basis is crucial. However, there are currently no comprehensive methodologies for the assessment and improvement of the data quality of digital shadows. Therefore, this paper introduces a methodology that supports the derivation of data quality projects aimed at optimizing the digital shadow data basis. The proposed methodology comprises four steps: First, digital shadow use cases along the product lifecycle are described. Next, the use cases are prioritized with regard to the expected benefits of applying the digital shadow. Third, quality deficiencies in the digital shadow data basis are assessed with respect to use case specific requirements. Finally, the prioritized use cases in relation with the identified quality deficits allow deriving needs for action, which are addressed by data quality projects. Together, the data quality projects constitute a data quality program. The methodology is applied in an industry case to prove the practical effectivity and efficiency.}
}
@article{VERBRUGGHE2019298,
title = {The electronic medical record: Big data, little information?},
journal = {Journal of Critical Care},
volume = {54},
pages = {298-299},
year = {2019},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2019.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0883944119313723},
author = {Walter Verbrugghe and Kirsten Colpaert}
}
@article{WIBISONO201929,
title = {Average Restrain Divider of Evaluation Value (ARDEV) in data stream algorithm for big data prediction},
journal = {Knowledge-Based Systems},
volume = {176},
pages = {29-39},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119301443},
author = {Ari Wibisono and Devvi Sarwinda},
keywords = {ARDEV, Big data prediction, FIMT-DD, Tree regression},
abstract = {Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.}
}
@article{LIU2019242,
title = {How Physical Exercise Level Affects Sleep Quality? Analyzing Big Data Collected from Wearables},
journal = {Procedia Computer Science},
volume = {155},
pages = {242-249},
year = {2019},
note = {The 16th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2019),The 14th International Conference on Future Networks and Communications (FNC-2019),The 9th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919309494},
author = {Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha Röning},
keywords = {Data analytics, wearables, sleep quality, statistical methods},
abstract = {Physical exercise and sleep have independent, yet synergistic, impacts on the health. However, the effects of acute exercise level on sleep quality have not been well investigated. We utilize statistical methods to investigate the differences of exercise level between the good and bad sleep nights. Our results present a complex interrelation between physical exercise and sleep quality with analyzing large personal data sets collected from wearables. As far as we know, this is the first study to investigate insights of interrelation of physical exercise and sleep quality based on a big volume of data collected from wearable devices of real users.}
}
@article{LIOUTAS2019100297,
title = {Key questions on the use of big data in farming: An activity theory approach},
journal = {NJAS - Wageningen Journal of Life Sciences},
volume = {90-91},
pages = {100297},
year = {2019},
issn = {1573-5214},
doi = {https://doi.org/10.1016/j.njas.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1573521418302197},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}},
keywords = {Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory},
abstract = {Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers’ limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.}
}
@article{LI2019393,
title = {Functional Neuroimaging in the New Era of Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {393-401},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301603},
author = {Xiang Li and Ning Guo and Quanzheng Li},
keywords = {Big data, Neuroimaging, Machine learning, Health informatics, fMRI},
abstract = {The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.}
}
@article{ULLAH201981,
title = {Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review},
journal = {Journal of Systems and Software},
volume = {151},
pages = {81-118},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.051},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300172},
author = {Faheem Ullah and Muhammad {Ali Babar}},
keywords = {Big data, Cybersecurity, Quality attribute, Architectural tactic},
abstract = {Context
Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.
Objective
We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.
Method
We used Systematic Literature Review (SLR) method for reviewing 74 papers.
Result
Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.
Conclusion
Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.}
}
@article{MCNUTT2019326,
title = {Use of Big Data for Quality Assurance in Radiation Therapy},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {326-332},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300384},
author = {Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright},
abstract = {The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.}
}
@article{LOPEZROBLES2019729,
title = {The last five years of Big Data Research in Economics, Econometrics and Finance: Identification and conceptual analysis},
journal = {Procedia Computer Science},
volume = {162},
pages = {729-736},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320551},
author = {José Ricardo López-Robles and Marisela Rodríguez-Salvador and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jesús Cobo},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Today, the Big Data term has a multidimensional approach where five main characteristics stand out: volume, velocity, veracity, value and variety. It has changed from being an emerging theme to a growing research area. In this respect, this study analyses the literature on Big Data in the Economics, Econometrics and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated using SciMAT as a bibliometric and network analysis software. SciMAT offers a complete approach of the field and evaluates the most cited and productive authors, countries and subject areas related to Big Data. Lastly, a science map is performed to understand the intellectual structure and the main research lines (themes).}
}