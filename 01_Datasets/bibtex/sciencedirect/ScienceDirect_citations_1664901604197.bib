@article{YANG2020454,
title = {A Data-Driven Approach for Quality Analytics of Screwing Processes in a Global Learning Factory},
journal = {Procedia Manufacturing},
volume = {45},
pages = {454-459},
year = {2020},
note = {Learning Factories across the value chain – from innovation to service – The 10th Conference on Learning Factories 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.052},
url = {https://www.sciencedirect.com/science/article/pii/S235197892031091X},
author = {Shun Yang and Hui Liu and Yue Zhang and Tobias Arndt and Constantin Hofmann and Benjamin Häfner and Gisela Lanza},
keywords = {learning factory, quality analytics, data-driven, edge, cloud analytics},
abstract = {Quality problems of screwing processes in assembly systems, which are an important issue for operation excellence, needs to be quickly analyzed and solved. A network can be very beneficial for root cause analysis due to different data from various factories. Nevertheless, it is difficult to obtain reliable and consistent data. In this context, this paper aims to develop a method for data-driven oriented quality analytics of screwing processes considering a global production network. Firstly, the overview of data structure is introduced. Further, the data transformation is modelled for edge- and cloud-based analytics across the global production network. Lastly, the rules for analyzing are identified. A joint case study based on Learning Factory Global Production (LF) in Germany and I4.0 Innovation Centre and Artificial Intelligence Innovation Factory (IC&AIIF) in China is used to validate the proposed approach, which is also a new teaching method for quality analysis in the framework of learning factory.}
}
@article{XIAO2020709,
title = {Deep interaction: Wearable robot-assisted emotion communication for enhancing perception and expression ability of children with Autism Spectrum Disorders},
journal = {Future Generation Computer Systems},
volume = {108},
pages = {709-716},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19328389},
author = {Wenjing Xiao and Miao Li and Min Chen and Ahmed Barnawi},
keywords = {Emotion recognition, Deep learning, Social communication},
abstract = {Recent changes in both the social economy and people’s living conditions and their habits have had an influence on the incidence of Autism Spectrum Disorder (ASD), which has brought huge economic and mental burden to the society , and has become an urgent public health problem. The main symptom of autism in children is the presence of a social barrier, and one of the main reasons for that is a lack of emotional cognitive ability. However, the existing autism-treatment systems intended for children pay little attention to the emotion cognition disorder. Besides, too much importance has been given to the interaction with children, while limiting the timeliness and movability of these systems. With the aim to address this shortcoming, in this work, we focus on the emotion cognition disorder and design a feasible solution for enhancing perception and expression ability of children with ASD. First, the first-view emotional care system for children with ASD (First-ECS) is developed using a wearable robot as a system carrier and realizing the deep emotional interaction with children with autism from the first-view perspective. Emotion communication is used to meet high timeliness requirements for emotion transmission in the First-ECS. Next, the emotional interaction mechanism that is applicable to the line of sight and non-line of sight communication scenarios is introduced. Also, the emotion perception engine and emotion expression engine are designed. Subsequently, multimodal data collection and processing by a wearable affective robot are discussed. In addition, this paper introduces a multimodal data fusion method from the angle of emotion relevance and emotion computing model based on audio-visual data. Finally, a demo platform is built to verify the feasibility of the proposed system.}
}
@article{BUNKER2020102201,
title = {Who do you trust? The digital destruction of shared situational awareness and the COVID-19 infodemic},
journal = {International Journal of Information Management},
volume = {55},
pages = {102201},
year = {2020},
note = {Impact of COVID-19 Pandemic on Information Management Research and Practice: Editorial Perspectives},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102201},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220311555},
author = {Deborah Bunker},
keywords = {COVID-19, Trust, Digital disruption, Digital destruction, Shared situational awareness, Mental model, Infodemic},
abstract = {Developments in centrally managed communications (e.g. Twitter, Facebook) and service (e.g. Uber, airbnb) platforms, search engines and data aggregation (e.g. Google) as well as data analytics and artificial intelligence, have created an era of digital disruption during the last decade. Individual user profiles are produced by platform providers to make money from tracking, predicting, exploiting and influencing their users’ decision preferences and behavior, while product and service providers transform their business models by targeting potential customers with more accuracy. There have been many social and economic benefits to this digital disruption, but it has also largely contributed to the digital destruction of mental model alignment and shared situational awareness through the propagation of mis-information i.e. reinforcement of dissonant mental models by recommender algorithms, bots and trusted individual platform users (influencers). To mitigate this process of digital destruction, new methods and approaches to the centralized management of these platforms are needed to build on and encourage trust in the actors that use them (and by association trust in their mental models). The global ‘infodemic’ resulting from the COVID-19 pandemic of 2020, highlights the current problem confronting the information system discipline and the urgency of finding workable solutions.}
}
@article{LIN20201348,
title = {Image-based high-content screening in drug discovery},
journal = {Drug Discovery Today},
volume = {25},
number = {8},
pages = {1348-1361},
year = {2020},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620302269},
author = {Sean Lin and Kenji Schorpp and Ina Rothenaigner and Kamyar Hadian},
abstract = {While target-based drug discovery strategies rely on the precise knowledge of the identity and function of the drug targets, phenotypic drug discovery (PDD) approaches allow the identification of novel drugs based on knowledge of a distinct phenotype. Image-based high-content screening (HCS) is a potent PDD strategy that characterizes small-molecule effects through the quantification of features that depict cellular changes among or within cell populations, thereby generating valuable data sets for subsequent data analysis. However, these data can be complex, making image analysis from large HCS campaigns challenging. Technological advances in image acquisition, processing, and analysis as well as machine-learning (ML) approaches for the analysis of multidimensional data sets have rendered HCS as a viable technology for small-molecule drug discovery. Here, we discuss HCS concepts, current workflows as well as opportunities and challenges of image-based phenotypic screening and data analysis.}
}
@article{LIN2020116313,
title = {Modeling repeated coseismic slip to identify and characterize individual earthquakes from geomorphic offsets on strike-slip faults},
journal = {Earth and Planetary Science Letters},
volume = {545},
pages = {116313},
year = {2020},
issn = {0012-821X},
doi = {https://doi.org/10.1016/j.epsl.2020.116313},
url = {https://www.sciencedirect.com/science/article/pii/S0012821X20302570},
author = {Zhou Lin and Jing Liu-Zeng and Ray J. Weldon and Jing Tian and Chao Ding and Yu Du},
keywords = {geomorphic offset,  (coefficient of offset variation), COPD (cumulative offset probability distribution), global historical coseismic surface ruptures, strike-slip fault, earthquake recurrence models},
abstract = {Coseismic slip distributions of repeated earthquakes are important data used to forecast magnitude and recurrence characteristics of future earthquakes. Many studies infer coseismic slip of past individual paleoearthquakes from frequency histograms of geomorphic offsets. Such histograms, often expressed as a COPD (cumulative offset probability distribution), commonly show multiple peaks with similar displacements, which are interpreted to be incremental slip associated with successive paleoearthquakes. However, assumptions in linking COPD peaks to individual earthquakes have not been fully explored. Here, we combine statistical modeling with global historical coseismic surface rupture data to evaluate the conditions required for the approach. The results show that only coseismic slip with low variability allow more than one event identification; for example, even low CoVs (coefficient of offset variation) of 0.25 and 0.35 permit identification of only 2 to 3 events. In contrast, the mean CoV value for all historical ruptures is 0.58 to 0.66. Furthermore, interpreting the first COPD peak as a single-event slip distribution could result in artificially low variation and flatter distribution of offset than observed in historical earthquakes. This study cautions that robust COPD interpretation on large strike-slip faults requires low offset variability, a sufficient number of offset measurements, and additional constraints like historical coseismic slip data or paleoseismic event sequence data.}
}
@article{ZHONG2020111416,
title = {Satellite-ground integrated destriping network: A new perspective for EO-1 Hyperion and Chinese hyperspectral satellite datasets},
journal = {Remote Sensing of Environment},
volume = {237},
pages = {111416},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2019.111416},
url = {https://www.sciencedirect.com/science/article/pii/S0034425719304353},
author = {Yanfei Zhong and Wenqing Li and Xinyu Wang and Shuying Jin and Liangpei Zhang},
keywords = {Hyperspectral image destriping, Satellite-ground integration, Spectral-spatial feature extraction, Convolutional neural network},
abstract = {From the EO-1 Hyperion imaging spectrometer to the newly launched Chinese satellite hyperspectral imagers, stripe noise is a ubiquitous phenomenon that seriously degrades the data quality and usability. Although previous efforts have achieved inspiring results, hyperspectral image (HSI) destriping remains a challenging task, as the stripe degradations are sometimes more complicated than the predefined assumptions, i.e., the preselected reference, filter, or handcrafted priors. With the rapid advances in deep learning technologies, convolutional neural networks (CNNs) provide a new potential to learn essential priors in an automatic manner. However, the training of CNNs is highly reliant on a large high-quality standard dataset, which is difficult to acquire for hyperspectral spaceborne sensors. In this paper, an innovative approach termed the satellite-ground integrated destriping network (SGIDN) is proposed for HSIs. Rather than using self-training, a satellite-ground integrated strategy is proposed, for the first time, to mitigate the data dependency, so that a large set of striped-clean pairs is generated from the ground-based HSIs. Considering the varied stripes among different bands, a unique CNN architecture design, including the combination of 3D convolution and 2D convolution, residual learning, and supplementary gradient channels, is integrated to capture the intrinsic spectral-spatial features in the HSIs and the unidirectional property of stripe noise. Compared with the traditional methods, SGIDN can be flexibly extended to specific HSI destriping tasks, e.g., coexisting horizontal and vertical stripes, and generalizes well to different hyperspectral satellite sensors. Given the same study area (Shanghai, China), three HSIs acquired by the EO-1 Hyperion imaging spectrometer, the Chinese HJ-1A HSI sensor, and the wide-range hyperspectral imager onboard the Chinese SPARK spectral micro-nano satellite are adopted to assess the proposed SGIDN model. Both simulated and real-data experiments confirm that SGIDN can consistently outperform the benchmark methods, with a higher degree of efficiency. Moreover, the land-cover mapping results further demonstrate the necessity of destriping and the suitability of the destriped results for use in further applications.}
}
@article{LI2020138243,
title = {Source apportionment of hourly-resolved ambient volatile organic compounds: Influence of temporal resolution},
journal = {Science of The Total Environment},
volume = {725},
pages = {138243},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.138243},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720317563},
author = {Zhiyuan Li and Kin-Fai Ho and Steve Hung Lam Yim},
keywords = {Hourly measurements, PMF, VOC sources, Diurnal variation, Temporal resolution},
abstract = {High temporal-resolution VOC concentration data can provide detailed and important temporal variations of VOC species and emission sources, which is not possible when using coarse temporal-resolution data. In this study, we utilized the positive matrix factorization (PMF) model to conduct source apportionment of hourly concentrations of nineteen VOC species and CO measured at the Mong Kok air quality monitoring station, operated by the Hong Kong Environmental Protection Department, from January 2013 to December 2014. The PMF analysis of the hourly dataset (PMF_Hourly) identified five sources, including liquefied petroleum gas (LPG) (contribution of 45%), gasoline exhaust (21%), combustion (20%), biogenic emission (9%), and paint solvents (6%). The diurnal patterns of VOC emissions from identified sources are likely to be affected by the strength of emissions, variation of the planetary boundary layer height, and photochemical reactions. In addition, the PMF analyses of hourly and 24-hour averaged data of the hourly-resolved data (PMF_Hourly and PMF_Daily) were generally comparable, but the time series of VOC emissions from PMF_Hourly could not be well captured by PMF_Daily for two local VOC sources of gasoline exhaust and LPG. This study highlights the benefit of high temporal-resolution measurement data in apportioning VOC sources, hence providing critical information on VOC emission sources (e.g., diurnal variations) for controlling VOC emissions effectively.}
}
@article{ZHAO2020204,
title = {Condition monitoring of power transmission and transformation equipment based on industrial internet of things technology},
journal = {Computer Communications},
volume = {157},
pages = {204-212},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420300232},
author = {Jindong Zhao and Xingzuo Yue},
keywords = {IoT(IoT) technology, Power transmission and transformation equipment, Status monitoring, Panoramic data},
abstract = {Nowadays, Internet of Things (IoT) technology is an advanced and emerging information based technology and it possesses avast development potential. The application of IoT technology in power system is beneficial for the intelligent transformation of the power system. The power system provides a vast application space for the IoT technology. The purpose of this paper is to strongly promote the application and deployment process of the IoT in various industries and play a long-term important role in promoting the informatization transformation of China’s industry. This paper studies the state monitoring system of power transmission and transformation equipment based on panoramic data and introduces the information model into the IoT of power transmission and transformation equipment. According to the equipment panorama information required by the life cycle management of power transmission a comprehensive panorama information modeling scheme is proposed for smart substation equipment, and the network performance is monitored through simulation analysis. In particular, the end-to-end delay of the application layer changed greatly, with a decrease of 0.015 s. This study provides a reference for the development of state monitoring system of power transmission and transformation equipment in China.}
}
@article{CHECASTALDO2020108913,
title = {Comments to “Persistent problems in the construction of matrix population models”},
journal = {Ecological Modelling},
volume = {416},
pages = {108913},
year = {2020},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2019.108913},
url = {https://www.sciencedirect.com/science/article/pii/S0304380019304211},
author = {Judy Che-Castaldo and Owen R. Jones and Bruce E. Kendall and Jean H. Burns and Dylan Z. Childs and Thomas H.G. Ezard and Haydee Hernandez-Yanez and David J. Hodgson and Eelke Jongejans and Tiffany Knight and Cory Merow and Satu Ramula and Iain Stott and Yngvild Vindenes and Hiroyuki Yokomizo and Roberto Salguero-Gómez}
}
@article{FAROOQUE2020106684,
title = {Fuzzy DEMATEL analysis of barriers to Blockchain-based life cycle assessment in China},
journal = {Computers & Industrial Engineering},
volume = {147},
pages = {106684},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106684},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220304186},
author = {Muhammad Farooque and Vipul Jain and Abraham Zhang and Zhi Li},
keywords = {Blockchain, Life cycle assessment, Supply chain sustainability, Barrier, Fuzzy DEMATEL},
abstract = {The emerging Blockchain technology can drastically improve the effectiveness and efficiency of life cycle assessment, which is widely used for assessing the environmental impact of products and processes. However, Blockchain adoption is impeded by various barriers including systems-related, external, intra-organizational and inter-organizational barriers. So far, no research has analyzed how these barriers interact with each other for better decision-making in life cycle assessment. This research narrows the knowledge gap by prioritizing the important barriers using a fuzzy Decision-Making Trial and Evaluation of Laboratory (DEMATEL) method. Pairwise comparison data were collected from three representative organizations in China, which all have Blockchain implementation experiences. The results show that the key cause barriers are immaturity of technology, and technical challenges for collecting supply chain data in real time. The prominent barriers include lack of new organizational policies for using technology, and lack of government policy/regulation guidance and support, among others. Managerial implications are discussed based on the results and findings.}
}
@article{2020109257,
title = {Transitioning Machine Learning from Theory to Practice in Natural Resources Management},
journal = {Ecological Modelling},
volume = {435},
pages = {109257},
year = {2020},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2020.109257},
url = {https://www.sciencedirect.com/science/article/pii/S0304380020303276}
}
@article{ANSARI2020295,
title = {Insurability of Cyber Physical Production Systems: How Does Digital Twin Improve Predictability of Failure Risk?},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {295-300},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.063},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320302147},
author = {Fazel Ansari and Steffen Nixdorf and Wilfried Sihn},
keywords = {Cyber-Physical Production Systems, Industry 4.0, Intelligent maintenance systems, Knowledge discovery, AI for business, economy, Data driven decision making},
abstract = {Original equipment manufacturers (OEMs), machine operators and suppliers are three main stakeholders in today’s industrial asset management lifecycle. Considering high rate of investment and prices to purchase and operationalize secured IoT-based production systems, insurance companies explore contributing into industrial asset management lifecycle by providing new products to transparently monitor online condition and operation status of industrial machines, and accordingly adjust the insurance prices and offers. Hence, joint efforts and cross-disciplinary research is vital to introduce a new insurance product, i.e. failure-based insurance, gaining benefits from AI technologies and knowledge-based maintenance approaches. This paper provides insights into the aforementioned problem, and presents the work-in-progress research on failure-based insurance model aided by Digital Twin technology. Finally, the practical implications and future research directions are outlined.}
}
@incollection{LADLEY202033,
title = {Chapter 4 - Overview: A day in the life of a data governance program and its capabilities},
editor = {John Ladley},
booktitle = {Data Governance (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {33-49},
year = {2020},
isbn = {978-0-12-815831-9},
doi = {https://doi.org/10.1016/B978-0-12-815831-9.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158319000047},
author = {John Ladley},
keywords = {Federation, Scope, Business model, Operating model, Principles, CSFs},
abstract = {This chapter addresses the most common question—“What does it look like?” It continues with a detailed examination of who should do the governing, what activities they need to perform, what is actually governed, and what data governance looks like when it occurs.}
}
@article{ZHANG2020100145,
title = {Data-Driven Computational Social Science: A Survey},
journal = {Big Data Research},
volume = {21},
pages = {100145},
year = {2020},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100145},
url = {https://www.sciencedirect.com/science/article/pii/S2214579620300137},
author = {Jun Zhang and Wei Wang and Feng Xia and Yu-Ru Lin and Hanghang Tong},
keywords = {Computational social science, Human dynamics, Individual, Collective, Relationship, Machine learning},
abstract = {Social science concerns issues on individuals, relationships, and the whole society. The complexity of research topics in social science makes it the amalgamation of multiple disciplines, such as economics, political science, and sociology, etc. For centuries, scientists have conducted many studies to understand the mechanisms of the society. However, due to the limitations of traditional research methods, there exist many critical social issues to be explored. To solve those issues, computational social science emerges due to the rapid advancements of computation technologies and the profound studies on social science. With the aids of the advanced research techniques, various kinds of data from diverse areas can be acquired nowadays, and they can help us look into social problems with a new eye. As a result, utilizing various data to reveal issues derived from computational social science area has attracted more and more attentions. In this paper, to the best of our knowledge, we present a survey on data-driven computational social science for the first time which primarily focuses on reviewing application domains involving human dynamics. The state-of-the-art research on human dynamics is reviewed from three aspects: individuals, relationships, and collectives. Specifically, the research methodologies used to address research challenges in aforementioned application domains are summarized. In addition, some important open challenges with respect to both emerging research topics and research methods are discussed.}
}
@article{SOLOVEV2020111192,
title = {Multi-omics approaches to human biological age estimation},
journal = {Mechanisms of Ageing and Development},
volume = {185},
pages = {111192},
year = {2020},
issn = {0047-6374},
doi = {https://doi.org/10.1016/j.mad.2019.111192},
url = {https://www.sciencedirect.com/science/article/pii/S0047637419301976},
author = {Ilya Solovev and Mikhail Shaposhnikov and Alexey Moskalev},
keywords = {Aging, Aging biomarkers, Biological age, Multi-omics approach},
abstract = {Multi-omics approach nowadays increasingly applied to molecular research in many fields of life sciences. Biogerontology is not an exception; multi-omics gives possibility to evaluate complex biomarkers (or panels) which consist of quantitative as well as phenotypic ones. It is especially important because of weak understanding of the nature of aging. The difficulty now is distinguishing between causes and effects of aging. The application of the whole set of metabolome, methylome, transcriptome, proteome or metagenome data in aging biomarker design becomes the only way to create a holistic view of aging landscape without missing undiscovered mechanisms and levels of organization. We found patents, up-to-date multi-omics datasets and studies, which include bioinformatics innovations to predict biological age in humans. We hope that the review will be also useful for clinicians, because it follows majorly translational purposes.}
}
@incollection{FAULKNER2020211,
title = {12 - Live Management and Control},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {211-227},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00027-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000279},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Data-centric robustness, Data-centric resilience, Data-centric competency},
abstract = {At the point of implementation, a system addresses the safety of an ‘average’ operational scenario. Furthermore, it needs to address variability around the average point, and a number of misuse scenarios. It has a degree of robustness. It also has a degree of resilience because should the system fall outside the robustness boundary, this will be identified and corrective action will be taken. This chapter considers these two related properties for data-centric systems and organisations. The chapter also considers the integration of data from multiple sources, the management of data change and data obsolescence. All of these are reliant on authentication of the identity of system (data) elements to ensure only appropriate activities are undertaken. Competency of personnel is a key operational safety management requirement. The nature of the roles and knowledge, skills and behaviours will change as organisations become more data-centric.}
}
@article{MANI2020850,
title = {An IoT Guided Healthcare Monitoring System for Managing Real-Time Notifications by Fog Computing Services},
journal = {Procedia Computer Science},
volume = {167},
pages = {850-859},
year = {2020},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.424},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920308905},
author = {Neel Mani and Akhil Singh and Shastri L Nimmagadda},
keywords = {IOT, Healthcare, Fog computing, Smart devices, Noticification services},
abstract = {Fog Computing is a new computing paradigm which is grown ever since it is being used. It is aimed at bringing the computations close to data sources from healthcare centers. IoT driven Fog Computing is developed in the healthcare industry that can expedite facilities and services among the mass population and help in saving billions of lives. The new computing platform, founded as fog computing paradigm may help to ease latency while transmitting and communicating signals with remote servers, which can accelerate medical services in spatial-temporal dimensions. The latency reduction is one of the necessary features of computing platforms which can enable completing the healthcare operations, especially in large-size medical projects and in relation to providing sensitive and intensive services. Reducing the cost of delivering data to the cloud is one of the research objectives.}
}
@article{POCH2020140980,
title = {When the fourth water and digital revolution encountered COVID-19},
journal = {Science of The Total Environment},
volume = {744},
pages = {140980},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.140980},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720345095},
author = {Manel Poch and Manel Garrido-Baserba and Lluís Corominas and Antoni Perelló-Moragues and Hector Monclús and Manuel Cermerón-Romero and Nikos Melitas and Sunny C. Jiang and Diego Rosso},
keywords = {SARS-CoV-2, COVID-19, Coronavirus, Infrastructure, Data, Digitalisation},
abstract = {The ongoing COVID-19 pandemic is, undeniably, a substantial shock to our civilization which has revealed the value of public services that relate to public health. Ensuring a safe and reliable water supply and maintaining water sanitation has become ever more critical during the pandemic. For this reason, researchers and practitioners have promptly investigated the impact associated with the spread of SARS-CoV-2 on water treatment processes, focusing specifically on water disinfection. However, the COVID-19 pandemic impacts multiple aspects of the urban water sector besides those related to the engineering processes, including sanitary, economic, and social consequences which can have significant effects in the near future. Furthermore, this outbreak appears at a time when the water sector was already experiencing a fourth revolution, transitioning toward the digitalisation of the sector, which redefines the Water-Human-Data Nexus. In this contribution, a product of collaboration between academics and practitioners from water utilities, we delve into the multiple impacts that the pandemic is currently causing and their possible consequences in the future. We show how the digitalisation of the water sector can provide useful approaches and tools to help address the impact of the pandemic. We expect this discussion to contribute not only to current challenges, but also to the conceptualization of new projects and the broader task of ameliorating climate change.}
}
@article{AGGESTAM2020152,
title = {Is sharing truly caring? Environmental data value chains and policymaking in Europe and Central Asia},
journal = {Environmental Science & Policy},
volume = {114},
pages = {152-161},
year = {2020},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2020.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1462901118307779},
author = {Filip Aggestam and Diana Mangalagiu},
keywords = {Shared environmental information system, Data integration, Environmental indicators, Data use},
abstract = {Access to high-quality, timely and comparable data is a prerequisite for any effective decision-making process and having such data available for the environment is absolutely fundamental to efficient and evidence-based policymaking. This article reviews the establishment of a Shared Environmental Information System (SEIS) in Europe and Central Asia to improve our understanding of how environmental data value chains are being employed to produce, share and use reliable data on the environment and whether this data is used in policymaking. Three sources of data are utilised to analyse whether relevant environmental data and information are being drawn upon by policymakers, namely, the 2019 SEIS mid-term review, SEIS Factsheets and SEIS Gap Analysis Reports. The results reveal that the pan-European region still faces significant data harmonisation problems, owing in part to differences in types and methods of data collection, data definitions and legislation. Whilst problems in some individual country’s participation have persisted since the launch of the SEIS initiative in the pan-European region, the development and successful piloting of the SEIS self-assessment framework is considered as evidence of positive progress. However, it remains difficult to adequately assess to what extent the data flows covered by this study impact on policymaking, nevertheless, the analysis serves to highlight the inter-linkages between environmental data flows, policymakers and environmental governance. In practical terms, the paper demonstrate a disconnect between data production and data use in policymaking and emphasises the need to both improve our understanding of the political determinants of data use and to further investigate how the uptake of environmental data and information can be facilitated in policymaking.}
}
@article{TAO2020102131,
title = {Network planning and operation of sustainable closed-loop supply chains in emerging markets: Retail market configurations and carbon policies},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {144},
pages = {102131},
year = {2020},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.102131},
url = {https://www.sciencedirect.com/science/article/pii/S136655452030778X},
author = {Yi Tao and Jianhuang Wu and Xiaofan Lai and Fan Wang},
keywords = {Sustainable supply chain, Retail network configurations, Carbon policies, Emerging markets, Stochastic program, L-shaped algorithm},
abstract = {This paper studies an integrated network planning and operation problem of sustainable closed-loop supply chain (CLSC) under demand uncertainty with particularly interest in the configuration of retail network. The problem is modeled as a two-stage stochastic program which is further extended to evaluate the impact of various carbon polices on planning and operation decisions of the CLSC. A branch and bound based integer L-shaped algorithm has been proposed to solve these models. Extensive numerical experiments are conducted and impacts of carbon policies are analyzed. Sensitivity analysis is carried out and several managerial insights are drawn.}
}
@article{YIN2020101960,
title = {A distributed sensing data anomaly detection scheme},
journal = {Computers & Security},
volume = {97},
pages = {101960},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101960},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820302364},
author = {Chunyong Yin and Bo Li and Zhichao Yin},
keywords = {Internet of things, Time series, Distributed anomaly detection, Confidence interval, Data stream},
abstract = {With the continuous development of Internet of Things, the information society has gradually entered a new era of the Internet of everything. Sensor nodes are important sources of data in the Internet of Things. The abnormal and failure of sensing data in the Internet of Things will affect the connectivity of the network. If the accuracy and reliability of the corresponding perception data can be effectively improved, we can timely and accurately find out the emergency and monitor the working status of the network. Therefore, it is of great significance to detect the abnormal data of data streams in the sensor network nodes and confirm its source. For the low quality of sensor data collected in real time in IoT, this paper proposes an anomaly detection method for sensing data streams based on edge computing. In this algorithm, the sensor data is expressed in the form of time series. On the edge computing based sensor data anomaly detection model, the improved confidence interval is used to detect whether the data is abnormal. The concept of interval difference is proposed as the judgment of the source of the anomaly. The accuracy and effectiveness of the algorithm are verified by experiments. The results show that the detection rate of abnormal data is above 98%, which indicates that the algorithm has certain practicability.}
}
@article{HSU2020106358,
title = {Similarity matching of wafer bin maps for manufacturing intelligence to empower Industry 3.5 for semiconductor manufacturing},
journal = {Computers & Industrial Engineering},
volume = {142},
pages = {106358},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106358},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220300929},
author = {Chia-Yu Hsu and Wei-Ju Chen and Ju-Chien Chien},
keywords = {Wafer bin map, Yield enhancement, Defect diagnosis, Similarity, Manufacturing intelligence, Industry 3.5},
abstract = {Yield improvement is increasingly important as advanced fabrication technologies are complicated and interrelated for semiconductor manufacturing. Wafer bin maps (WBM) present specific failure patterns which provide crucial information to track the process excursions to empower intelligent manufacturing for wafer fabrication. In practice, WBM identification is still subjective relied on domain knowledge and human-eye. As the semiconductor industry continuously migrates for advanced nano technologies, many rare defect patterns are also generated by different pattern, pattern size, noise degree, pattern density, pattern shift, and wafer rotation. Existing studies regarding WBM focus on classification and lack of capability to detect a rare pattern. In order to overcome the shortage of WBM classification, the similar WBMs provide useful information of WBM identification. Following Industry 3.5 as a hybrid strategy between Industry 3.0 and to-be Industry 4.0, this study aims to develop a novel approach to measure the similarity of defect patterns of WBMs to enhance decision quality for fault detection and defect diagnosis effectively and efficiently. In particular, the proposed approach applied a mountain clustering algorithm to enhance the defect features depending on clustering density. Then, Weighted Modified Hausdorff Distance (WMHD) is employed to measure the similarity level. Furthermore, a decision support system embedded the developed algorithms is constructed. An empirical study of WBM clustering was conducted in a fab for validation. The results have shown practical viability of the proposed approach.}
}
@article{JIA2020103115,
title = {Status and application of advanced airborne hyperspectral imaging technology: A review},
journal = {Infrared Physics & Technology},
volume = {104},
pages = {103115},
year = {2020},
issn = {1350-4495},
doi = {https://doi.org/10.1016/j.infrared.2019.103115},
url = {https://www.sciencedirect.com/science/article/pii/S1350449519307649},
author = {Jianxin Jia and Yueming Wang and Jinsong Chen and Ran Guo and Rong Shu and Jianyu Wang},
keywords = {Airborne, Hyperspectral, Key technology, Surface reflectance, Application},
abstract = {Hyperspectral imaging technology has evolved for over thirty years and is widely used for geologic mapping, environmental monitoring, vegetation analysis, atmospheric characterization, biological and chemical detection, etc. With advances in technology, hyperspectral imagery not only determines the presence of materials and objects, but more importantly, also quantifies the variability and abundance of the identified materials or objects. Airborne hyperspectral imagers still perform a vital role in remote sensing fields due to advantages of spatial resolution, performance capabilities in a cloudy atmosphere, and onboard maintenance as compared to similar imagers aboard spaceborne platforms. To date, hundreds of airborne hyperspectral systems have been designed, built, and operated. Here, a review of key technologies for airborne hyperspectral imaging technology during past three decades is presented. First discussed will be high throughput imaging modes, high quality spectroscopic subsystems, and high sensitivity detector technology used on current airborne hyperspectral imagers. Particularly, the importance of data-processing such as calibration, geometric rectification, and atmospheric correction are discussed. Next, several new and novel applications are presented on the basis of state-of-the-art airborne hyperspectral technology. Finally, an outlook of challenges and future technology directions is presented along with general advice for designing and realizing novel high-performance airborne hyperspectral systems in this rapidly evolving field. By illustrating the status and prospects of typical airborne hyperspectral imagers, this overview provides a comparison of the technologies employed in previous hyperspectral imaging systems, current imaging technology research programs and prospects for innovative technology in future airborne hyperspectral imaging platforms.}
}
@article{WANG2020102035,
title = {Examining customer engagement and brand intimacy in social media context},
journal = {Journal of Retailing and Consumer Services},
volume = {54},
pages = {102035},
year = {2020},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2020.102035},
url = {https://www.sciencedirect.com/science/article/pii/S0969698918311445},
author = {Tien Wang and Fu-Yu Lee},
keywords = {Customer engagement, Social media, Brand intimacy, Advice seeking, Self-image expression, Fashion involvement},
abstract = {Customer engagement (CE) is critical for firms to cultivate and improve customer brand experience in the customer journey. However, few studies are available on the effects of customer-based driving forces on CE in a defined brand experience context. Given the multidimensional nature of CE, the interrelationships among CE dimensions and various dimensional effects on the customer–brand relationship, represented by brand intimacy, have not been thoroughly explored. To address these research gaps, this study explores three customer- and context-based forces that drive CE in social media contexts from a consumer’s perspective. CE is operationalized as a second-order construct consisting of three dimensions (i.e., consumption, contribution, and creation) to reflect its multifaceted nature. An online survey was used to collect data. The results suggest that customer-based forces, advice seeking, and self-image expression exert positive influences on behavioral CE dimensions. The effect of a context-based factor, that is, fashion involvement, is salient only when gender difference is integrated. Moreover, the three facets of behavioral CE affect brand intimacy to different extents. Brand intimacy is the most affected by creation followed by consumption. The research findings contribute to the literature on CE and brand intimacy and also offer practical insights on marketing communications and segmentation.}
}
@article{KHALID2020102275,
title = {A survey on hyperparameters optimization algorithms of forecasting models in smart grid},
journal = {Sustainable Cities and Society},
volume = {61},
pages = {102275},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102275},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720304960},
author = {Rabiya Khalid and Nadeem Javaid},
keywords = {Forecasting, Hyperparameters, Parameter tuning, Data preprocessing, Training algorithms, Outliers in data, Processing time},
abstract = {Forecasting in the smart grid (SG) plays a vital role in maintaining the balance between demand and supply of electricity, efficient energy management, better planning of energy generation units and renewable energy sources and their dispatching and scheduling. Existing forecasting models are being used and new models are developed for a wide range of SG applications. These algorithms have hyperparameters which need to be optimized carefully before forecasting. The optimized values of these algorithms increase the forecasting accuracy up to a significant level. In this paper, we present a brief literature review of forecasting models and the optimization methods used to tune their hyperparameters. In addition, we have also discussed the data preprocessing methods. A comparative analysis of these forecasting models, according to their hyperparameter optimization, error methods and preprocessing methods, is also presented. Besides, we have critically analyzed the existing optimization and data preprocessing models and highlighted the important findings. A survey of existing survey papers is also presented and their recency score is computed based on the number of recent papers reviewed in them. By recent, we mean that the year in which a survey paper is published and its previous three years. Finally, future research directions are discussed in detail.}
}
@article{HARRIS2020151089,
title = {How to Develop Statistical Predictive Risk Models in Oncology Nursing to Enhance Psychosocial and Supportive Care},
journal = {Seminars in Oncology Nursing},
volume = {36},
number = {6},
pages = {151089},
year = {2020},
note = {Digital Platforms in Cancer Care},
issn = {0749-2081},
doi = {https://doi.org/10.1016/j.soncn.2020.151089},
url = {https://www.sciencedirect.com/science/article/pii/S0749208120301042},
author = {Jenny Harris and Edward Purssell and Emma Ream and Anne Jones and Jo Armes and Victoria Cornelius},
keywords = {Predictive risk models, Regression models, Cancer, Psychosocial, Supportive care, Psychological, Distress},
abstract = {ABSTRACT
Objectives
Predictive risk models are advocated in psychosocial oncology practice to provide timely and appropriate support to those likely to experience the emotional and psychological consequences of cancer and its treatments. New digital technologies mean that large scale and routine data collection are becoming part of everyday clinical practice. Using these data to try to identify those at greatest risk for late psychosocial effects of cancer is an attractive proposition in a climate of unmet need and limited resource. In this paper, we present a framework to support the development of high-quality predictive risk models in psychosocial and supportive oncology. The aim is to provide awareness and increase accessibility of best practice literature to support researchers in psychosocial and supportive care to undertake a structured evidence-based approach.
Data Sources
Statistical prediction risk model publications.
Conclusion
In statistical modeling and data science different approaches are needed if the goal is to predict rather than explain. The deployment of a poorly developed and tested predictive risk model has the potential to do great harm. Recommendations for best practice to develop predictive risk models have been developed but there appears to be little application within psychosocial and supportive oncology care.
Implications for Nursing Practice
Use of best practice evidence will ensure the development and validation of predictive models that are robust as these are currently lacking. These models have the potential to enhance supportive oncology care through harnessing routine digital collection of patient-reported outcomes and the targeting of interventions according to risk characteristics.}
}
@article{JEET2020,
title = {Investigating the progress of human e-healthcare systems with understanding the necessity of using emerging blockchain technology},
journal = {Materials Today: Proceedings},
year = {2020},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.10.083},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320376501},
author = {Rubal Jeet and Sandeep {Singh Kang}},
keywords = {E-health, E-healthcare system, Blockchain technology, Applications, IoT},
abstract = {Healthcare is an area that is supposed to have significant impacts on the blockchain (BC). Nevertheless, researchers and experts in health informatics are researching and developments in the field. Work is very new in the field but is progressively increasing. It can be referred as uprising in the digital world because of its effective performance concerning security, efficacy, and productivity of different frameworks. It operates as a distributed database, exchanged within a decentralized computer network. It stores data of the transactions performed using crypto-currency and information is managed using various computers linked to a peer-to-peer network. Besides, no trusted third parties are required in a distributed e-Health environment and support document removal. Health care is one of the most important aspects that require efficacious technologies to stay up to date and analyze the population's health status so that medical data can be maintained. Thus, in this paper, a broad review is presented that delineates the usage of conventional healthcare systems and also the use of technology to make it e-healthcare. Further, the role of the blockchain is presented in this paper. Although, a plethora of studies have been proposed that included various methodologies of blockchain in the healthcare industry, yet these studies are not enough to portray the effectiveness of these proposed approaches. In the review, different applications of blockchain that are being utilized at the present time are demonstrated along with the benefits and limitations of blockchain. It is observed that there is a scope of research in this field to know to enhance the utility of blockchain in healthcare. Moreover, an ontology-based Adverse Drug event system can be taken into consideration to use it as a BC application.}
}
@incollection{MCCAFFREY2020279,
title = {Chapter 19 - Analysis best practices},
editor = {Peter Mccaffrey},
booktitle = {An Introduction to Healthcare Informatics},
publisher = {Academic Press},
pages = {279-289},
year = {2020},
isbn = {978-0-12-814915-7},
doi = {https://doi.org/10.1016/B978-0-12-814915-7.00019-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128149157000193},
author = {Peter Mccaffrey},
keywords = {Design patterns, Antipattern, Organization, Cleanliness, Transparency},
abstract = {In this last section of the book, we discuss several important technical concept necessary to accomplish many data engineering and analysis tasks at scale. Whether dealing with large data sets, complex aggregations among heterogeneous sources, or computationally demanding analyses, there are many reasons why an analytical project would need to scale beyond an individual analyst, a personal workstation, or a single database. In previous chapters, we have discussed several of issues around scoping a project and setting goals and timelines. In this chapter, this discussion will be of a more technical variety, covering specific design decisions and processes, which will enhance or confound analytical projects. Understanding these concepts is what will allow you to makes informed decisions about when and how to incorporate many of the technologies mentioned through this book and, more importantly, will allow you to consistently accomplish repeatable analytical work, which is understandable both to external consumers and to members of your own analytical team.}
}
@article{LIU2020647,
title = {Evaluating smart grid renewable energy accommodation capability with uncertain generation using deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {647-657},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19323039},
author = {Yongnan Liu and Xin Guan and Jun Li and Di Sun and Tomoaki Ohtsuki and Mohammad Mehedi Hassan and Abdulhameed Alelaiwi},
keywords = {Accommodation capability, Deep reinforcement learning, Uncertain renewable energy description},
abstract = {Due to environment-friendliness, renewable energy like solar power and wind power is more and more introduced to energy systems all over the world. Simultaneously, high penetrations of wind and solar generation also have brought severe curtailment of wind and solar. How to alleviate curtailment of wind and solar is a crucial problem in evaluating accommodation capability of renewable energy, which reflects the extent of utilization of renewable energy and economic benefits. The uncertainty of renewable energy brings challenges to precisely describe renewable generation, which leads to difficulty in designing effective mechanisms for accommodation capability of renewable energy. Existing work suffers from high computation overhead from frequently updated data, and low precision of describing renewable energy, which leads to less effective policies for renewable energy accommodation and underestimated accommodation capability. To make the most of renewable energy, an algorithm AccCap-DRL based on deep reinforcement learning is proposed. AccCap-DRL partitions a distribution into segments by time intervals, employs WGAN to describe distributions of renewable energy data, and employs DDPG to obtain approximate policies for renewable energy accommodation in different scenarios. Simulation results from real power generation and users’ demand data show high effectiveness of the proposed algorithm, and high efficiency of evaluating accommodation capability.}
}
@article{XUE2020418,
title = {From LiDAR point cloud towards digital twin city: Clustering city objects based on Gestalt principles},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {167},
pages = {418-431},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620302070},
author = {Fan Xue and Weisheng Lu and Zhe Chen and Christopher J. Webster},
keywords = {LiDAR, Digital Twin City (DTC), Gestalt principles, Hierarchical clustering, Symmetry, Cross-section},
abstract = {Recent advancement of remote sensing technologies has brought in accurate, dense, and inexpensive city-scale Light Detection And Ranging (LiDAR) point clouds, which can be utilized to model city objects (e.g., buildings, roads, and automobiles) for creating Digital Twin Cities (DTCs). However, processing such unstructured point clouds is very challenging, epitomized by high cost, movable objects, limited object classes, and high information inadequacy/redundancy. We noticed that many city objects are not in random shapes; rather, they have invariant cross-sections following the Gestalt design principles, including proximity, connectivity, symmetry, and similarity. In this paper, we present a novel unsupervised method, called Clustering Of Symmetric Cross-sections of Objects (COSCO), to process urban LiDAR point clouds to a hierarchy of objects based on their characteristic cross-sections. First, city objects are segmented as connected patches of proximate 3D points. Then, symmetric cross-sections are detected for symmetric city objects. Finally, the taxonomy and groups of city objects are recognized from a hierarchical clustering analysis of the dissimilarity matrix. Experimental results showed that COSCO detected the correct taxonomy and types of 12 cars from 24,126 LiDAR points in 8.28 s. Based on the cross-sections and taxonomy, a digital twin was created by registering online free 3D car models in 29.58 s. The contribution of this paper is twofold. First, it presents an effective unsupervised method for understanding and developing DTC objects in LiDAR point clouds by harnessing innate Gestalt design principles. Secondly, COSCO can be an efficient LiDAR pre-processing tool for recognizing symmetric city objects’ cross-sections, positions, heading directions, dimensions, and possible types for smart city applications in GIScience, Architecture, Engineering, Construction and Operation (AECO), and autonomous vehicles.}
}
@article{BAZAZ20201125,
title = {Availability of Manufacturing data resources in Digital Twin},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1125-1131},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.158},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320151},
author = {Sara Moghadaszadeh Bazaz and Mika Lohtander and Juha Varis},
keywords = {Data ownership, digital twin, manufacturing data, information, turning process},
abstract = {Data and information are the basis of the integrated manufacturing environment. The latest, synonym of the integrated manufacturing environment is a digital twin. The concept of a digital twin has grown profoundly in recent years. Several models and frameworks are suggested to create a digital twin in manufacturing. Various information and data flows are involved in any manufacturing case. Diagnosis and gather required data and information is the first and basis of creating digital twin. Data ownership is a significant issue because of the large amount of isolated data islands and due to different data ownership relationships of the whole production chain. This study focuses on the ownership paradigm in manufacturing and digital twin. The reliable data resources in manufacturing the metal-based products are recognized. Available and unavailable data in the turning process are studied and classified in general form to cover every condition as a case study.}
}
@article{XUAN2020106587,
title = {An incentive mechanism for data sharing based on blockchain with smart contracts},
journal = {Computers & Electrical Engineering},
volume = {83},
pages = {106587},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106587},
url = {https://www.sciencedirect.com/science/article/pii/S0045790619314855},
author = {Shichang Xuan and Li Zheng and Ilyong Chung and Wei Wang and Dapeng Man and Xiaojiang Du and Wu Yang and Mohsen Guizani},
keywords = {Blockchain, Data sharing, Game theory, Smart contract, Trust, Automation transaction, Incentive mechanism},
abstract = {Data sharing techniques have progressively drawn increasing attention as a means of significantly reducing repetitive work. However, in the process of data sharing, the challenges regarding formation of mutual-trust relationships and increasing the level of user participation are yet to be solved. The existing solution is to use a third party as a trust organization for data sharing, but there is no dynamic incentive mechanism for data sharing with a large number of users. Blockchain 2.0 with smart contract has the natural advantage of being able to enable trust and automated transactions between a large number of users. This paper proposes a data sharing incentive model based on evolutionary game theory using blockchain with smart contract. The smart contract mechanism can dynamically control the excitation parameters and continuously encourages users to participate in data sharing.}
}
@article{FERNANDEZ202010,
title = {The Future Is Now: The 2020 Evolution of The Society of Thoracic Surgeons National Database},
journal = {The Annals of Thoracic Surgery},
volume = {109},
number = {1},
pages = {10-13},
year = {2020},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2019.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0003497519317096},
author = {Felix G. Fernandez}
}
@article{LEE2020100095,
title = {Emerging data for pedestrian and bicycle monitoring: Sources and applications},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {4},
pages = {100095},
year = {2020},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100095},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220300063},
author = {Kyuhyun Lee and Ipek N. Sener},
keywords = {Emerging data, Pedestrian, Bicycle, Smartphone, Data collection, Crowdsource},
abstract = {Growing attention on the benefits of non-motorized travel has increased the demand for accurate and timely pedestrian and bicycle travel data. Advancements in technologies and the proliferation of smartphones have created new data sources that can help eliminate limitations related to small sample size and infrequent updates due to limited resources. This study reviews the emerging data sources and their current use, focusing on non-motorized travel monitoring. In this study, the emerging data are categorized into mode-unspecified and mode-specified data based on whether the mode used can be detected with no or little effort. While mode-unspecified data are collected without sorting out non-motorized travelers, mode-specified data at least know who (which mode) is being monitored. So far, commercial vendors provide a vast volume of mode-unspecified data, but their products have been mainly used for motorized trips or are in initial stages of development. Meanwhile, readily available data sources and their applications are more concentrated on mode-specified data, which have enabled varying non-motorized travel studies—including travel pattern identification, route-choice modeling, crash/air pollution exposure estimation, and new facility provision evaluation—but are mostly focused on bicycling. Despite the potential of emerging data, their use also has several challenges, such as limited mode inference, sample bias, and lack of detailed trip/traveler information due to privacy issues. More efforts are needed, such as improving data accuracy and developing robust data fusion techniques, to be able to fully utilize the emerging data sources.}
}
@article{INGEMARSDOTTER2020105047,
title = {Opportunities and challenges in IoT-enabled circular business model implementation – A case study},
journal = {Resources, Conservation and Recycling},
volume = {162},
pages = {105047},
year = {2020},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2020.105047},
url = {https://www.sciencedirect.com/science/article/pii/S0921344920303645},
author = {Emilia Ingemarsdotter and Ella Jamsin and Ruud Balkenende},
keywords = {Digitalization, Circular economy, Product service systems, Predictive maintenance, Smart lighting},
abstract = {While the enabling capabilities of the Internet of Things (IoT) in the Circular Economy (CE) have been highlighted in a number of publications, knowledge about how to leverage IoT in actual implementation of circular strategies is still lacking. This paper aims to elucidate reasons for the apparent mismatch between the ‘theoretical opportunities’ of IoT for CE as described in literature, and current implementation in practice. To this end, we present a case study in the field of LED lighting, within a company with previous experience and knowledge in both IoT and CE. The primary data source is twelve semi-structured interviews with stakeholders from the company. We identify opportunities for using IoT to support circular strategies in this specific case: IoT can support servitized business models; improve tracking and record keeping of in-use and post-use products; enable conditions monitoring and predictive maintenance; improve estimations of remaining lifetime of used products; and inform design decisions to improve durability of products. Related to these opportunities, we identify implementation challenges faced by the company. The main IoT-specific implementation challenges in the case are (1) a lack of structured data management processes to ensure high quality data collection and analysis, and (2) the difficulty of designing IoT-enabled products for interoperability, adaptability, and upgradability, especially considering that IoT technologies develop at a high pace. By elucidating these challenges, this paper contributes with IoT-specific insights to the available literature about challenges in circular business model implementation. Moreover, this paper adds an important emphasis on real-world implementation challenges to the literature about digitally-enabled circular strategies.}
}
@incollection{MILLER2020181,
title = {Chapter eight - Data science and the exposome},
editor = {Gary W. Miller},
booktitle = {The Exposome (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {181-209},
year = {2020},
isbn = {978-0-12-814079-6},
doi = {https://doi.org/10.1016/B978-0-12-814079-6.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128140796000080},
author = {Gary W. Miller},
keywords = {Bioinformatics, systems biology, models, computational biology, machine learning, Bayesian methods, artificial intelligence, causal inference},
abstract = {Data science is focused on extracting meaningful value from complex datasets. Exposome-related data are certainly complex with information coming from a myriad of sources. The huge amounts of data must be organized in some manner that allows appropriate interpretations and associations to be drawn. It is unlikely that unsupervised approaches will allow for causal associations to be made, but with proper study design and appropriate statistical and computational models, it should be possible to derive meaningful connections between complex exposures and specific health outcomes. The complex types of data will undoubtedly require sophistical mathematical approaches, including bioinformatics, computational, machine learning, and systems biology-based techniques. This chapter reviews some of the possible strategies that can be used to keep track of the diverse and massive datasets that will result from exposome research.}
}
@article{OSULLIVAN20201523,
title = {A case-study in the introduction of a digital twin in a large-scale smart manufacturing facility},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1523-1530},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.212},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320837},
author = {Jamie O’Sullivan and Dominic O’Sullivan and Ken Bruton},
keywords = {Digital Twin, Predicitve Maintenance},
abstract = {In the field of industrial engineering the knowledge produced by newly obtained data is driving business forward. Automating the process of capturing data from industrial machines, analyzing it and using the knowledge gained to make better decisions for the machines is the crux of the digital twin. Sensor technology, Internet of Things platforms, information and communication technology and smart analytics allow the digital twin to transform a physical asset into a connected smart item that is now part of a cyber physical system and that is far more valuable than when it existed in isolation. The digital twin can be adopted by the maintenance engineering industry to aid in the prediction of issues before they occur thus creating value for the business. In this paper the authors look to introduce a maintenance digital twin to a large-scale manufacturing facility. Issues that hamper such work are discovered and categorized to highlight the difficulty of the practical installation of this concept. To aid in the installation process a digital twin framework is presented that simplifies the digital twin development process into steps that can be completed independently. With the framework in place the authors commence the task of completing these steps.}
}
@article{DADASHOVA2020102368,
title = {Random parameter models for estimating statewide daily bicycle counts using crowdsourced data},
journal = {Transportation Research Part D: Transport and Environment},
volume = {84},
pages = {102368},
year = {2020},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2020.102368},
url = {https://www.sciencedirect.com/science/article/pii/S1361920920305551},
author = {Bahar Dadashova and Greg P. Griffin},
keywords = {Bicycle Counts, crowdsourced data, Strava, Travel demand, Time series data, Random parameter models},
abstract = {Persistent lack of non-motorized traffic counts can affect the evidence-based decisions of transportation planning and safety-concerned agencies in making reliable investments in bikeway and other non-motorized facilities. Researchers have used various approaches to estimate bicycles counts, such as scaling, direct-demand modeling, time series, and others. In recent years, an increasing number of studies have tried to use crowdsourced data for estimating the bicycle counts. Crowdsourced data only represents a small percentage of cyclists. This percentage, on the other hand, can change based on the location, facility type, meteorological, and other factors. Moreover, the autocorrelation observed in bicycle counts may be different from the autocorrelation structure observed among crowdsourced platform users, such as Strava. Strava users are more consistent; hence, the time series count data may be stationary, while bicycle demand may vary based on seasonal factors. In addition to seasonal variation, several time-invariant contributing factors (e.g., facility type, roadway characteristics, household income) affect bicycle demand, which needs to be accounted for when developing direct demand models. In this paper, we use a mixed-effects model with autocorrelated errors to predict daily bicycle counts from crowdsourced data across the state of Texas. Additionally, we supplement crowdsourced data with other spatial and temporal factors such as roadway facility, household income, population demographics, population density and weather conditions to predict bicycle counts. The results show that using a robust methodology, we can predict bicycle demand with a 29% margin of error, which is significantly lower than merely scaling the crowdsourced data (41%).}
}
@article{SIPIOR2020102170,
title = {Considerations for development and use of AI in response to COVID-19},
journal = {International Journal of Information Management},
volume = {55},
pages = {102170},
year = {2020},
note = {Impact of COVID-19 Pandemic on Information Management Research and Practice: Editorial Perspectives},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102170},
url = {https://www.sciencedirect.com/science/article/pii/S026840122030949X},
author = {Janice C. Sipior},
keywords = {Artificial intelligence, AI, Machine learning, COVID-19, Coronavirus, AI applications, Strategy, Bias, Repurposed AI, Data, Team diversity},
abstract = {Artificial intelligence (AI) is playing a key supporting role in the fight against COVID-19 and perhaps will contribute to solutions quicker than we would otherwise achieve in many fields and applications. Since the outbreak of the pandemic, there has been an upsurge in the exploration and use of AI, and other data analytic tools, in a multitude of areas. This paper addresses some of the many considerations for managing the development and deployment of AI applications, including planning; unpredictable, unexpected, or biased results; repurposing; the importance of data; and diversity in AI team membership. We provide implications for research and for practice, according to each of the considerations. Finally we conclude that we need to plan and carefully consider the issues associated with the development and use of AI as we look for quick solutions.}
}
@article{WANG2020107357,
title = {Data fusion-based algorithm for predicting miRNA–Disease associations},
journal = {Computational Biology and Chemistry},
volume = {88},
pages = {107357},
year = {2020},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2020.107357},
url = {https://www.sciencedirect.com/science/article/pii/S1476927120307660},
author = {Chunyu Wang and Kai Sun and Juexin Wang and Maozu Guo},
keywords = {Network fusion, Random walk, miRNA, Disease},
abstract = {Technological progress and the development of laboratory techniques and bioinformatics tools have led to the availability of ever-increasing amounts of biological data including genomic, proteomic, and transcriptomic sequences and related information. These data have helped in understanding some of the complicated life process from a systematic level. Many diseases are generated by abnormalities in multiple regulating processes. In this study, we constructed a novel miRNA–gene–disease fusion (MGDF) algorithm by integrating three genome-wide networks, namely microRNA (miRNA), gene function, and disease similarity networks. The data fusion method was applied to construct a miRNA–gene–disease association network model from these networks to explore miRNA–disease associations mediated by genes with similar functions. mmiRNAs bind to their target genes and regulate their expression, so the miRNA–gene and gene–disease regulatory relationships were included in the network model to more accurately predict miRNA–disease associations. The proposed MGDF was used to predict miRNA–cancer associations and the results show that most of the predicted associations had evidence in existing databases.}
}
@incollection{ALLAM202089,
title = {Chapter 6 - The Rise of Machine Intelligence in the COVID-19 Pandemic and Its Impact on Health Policy},
editor = {Zaheer Allam},
booktitle = {Surveying the Covid-19 Pandemic and its Implications},
publisher = {Elsevier},
pages = {89-96},
year = {2020},
isbn = {978-0-12-824313-8},
doi = {https://doi.org/10.1016/B978-0-12-824313-8.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243138000061},
author = {Zaheer Allam},
keywords = {Artificial intelligence, China, Coronavirus, COVID-19, Detection, Disease surveillance, Endemic, Epidemic, Global health, Healthcare, Infectious disease, Outbreak, Pandemic},
abstract = {The use of advanced technologies, especially predictive computing in the health sector, is on the rise in this era, and they have successfully transformed the sector with quality insights, better decision-making, and quality policies. Even though notable benefits have been achieved through the uptake of the technologies, adoption is still slow, as most of them are still new, hence facing some hurdles in their applications especially in national and international policy levels. But the recent case of COVID-19 outbreak has given an opportunity to showcase that these technologies, especially artificial intelligence (AI), have the capacity to produce accurate, real-time, and reliable predictions on issues as serious as pandemic outbreak. A case in point is how companies such as BlueDot and Metabiota managed to correctly predict the spread route of the virus days before such events happened and officially announced by the World Health Organization. In this chapter, an increase in the use of AI-based technologies to detect infectious diseases is underlined and how such uses have led to early detections of infectious diseases. Nevertheless, there is evidence that there is need to enhance data sharing activities, especially by rethinking how to improve the efficiency of data protocols. The chapter further proposes the need for enhanced use of technologies and data sharing to ensure that future outbreaks are detected even earlier, thus accelerating early preventive measures.}
}
@article{WANG2020140361,
title = {Spatiotemporal changes of surface solar radiation: Implication for air pollution and rice yield in East China},
journal = {Science of The Total Environment},
volume = {739},
pages = {140361},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.140361},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720338833},
author = {Yanyu Wang and Ze Meng and Rui Lyu and Guan Huang and Qianshan He and Tiantao Cheng},
keywords = {Surface solar radiation, Aerosol pollution, Rice yield, WRF, SBDART},
abstract = {The changes of surface solar radiation (SSR) have significant implication for air pollution and rice yield. In this study, gridded SSR data, derived from multi-platform datasets and radiation model, were used to analyze its spatiotemporal changes over East China during 2000–2016. The results show SSR experiences dimming during 2000–2005, then turns into brightening till 2016. Both aerosol optical depth (AOD) and single scattering albedo (SSA) contribute to SSR trend. AOD dominates the spatiotemporal changes of SSR in East China, and this impact is higher in the North than the South. SSA has little impact on SSR with low AOD, but its contribution to SSR becomes important as AOD increases. Moreover, gridded planet boundary layer (PBL) was simulated by the Weather Research and Forecasting Model (WRF) and SSR-PBL relationship was also explored. Long-term evidence indicates PBL has a regulatory effect on SSR in the air pollution. Additionally, aerosol-induced radiation reduction can influence rice yield in East China, and it can result in about mean 6.74% reduction in rice yield over East China. Province-level changes of aerosol-induced reduction in rice production were also evaluated and it suggests the impact of aerosols on rice production is non-negligible, especially in Jiangsu and Anhui Province. Our study underscores the importance of aerosol pollution on surface radiation and the mitigation of aerosols is beneficial for crop production under climate change.}
}
@article{MISHRA2020101828,
title = {CyanoTRACKER: A cloud-based integrated multi-platform architecture for global observation of cyanobacterial harmful algal blooms},
journal = {Harmful Algae},
volume = {96},
pages = {101828},
year = {2020},
issn = {1568-9883},
doi = {https://doi.org/10.1016/j.hal.2020.101828},
url = {https://www.sciencedirect.com/science/article/pii/S1568988320301074},
author = {Deepak R. Mishra and Abhishek Kumar and Lakshmish Ramaswamy and Vinay K. Boddula and Moumita C. Das and Benjamin P. Page and Samuel J. Weber},
keywords = {CyanoHABs, Remote sensing, Wireless sensors, Social media, Satellite data, Cyberinfrastructure},
abstract = {Over the past decade, the global proliferation of cyanobacterial harmful algal blooms (CyanoHABs) have presented a major risk to the public and wildlife, and ecosystem and economic services provided by inland water resources. As a consequence, water resources, environmental, and healthcare agencies are in need of early information about the development of these blooms to mitigate or minimize their impact. Results from various components of a novel multi-cloud cyber-infrastructure referred to as “CyanoTRACKER” for initial detection and continuous monitoring of spatio-temporal growth of CyanoHABs is highlighted in this study. The novelty of the CyanoTRACKER framework is the collection and integration of combined community reports (social cloud), remote sensing data (sensor cloud) and digital image analytics (computation cloud) to detect and differentiate between regular algal blooms and CyanoHABs. Individual components of CyanoTRACKER include a reporting website, mobile application (App), remotely deployable solar powered automated hyperspectral sensor (CyanoSense), and a cloud-based satellite data processing and integration tool. All components of CyanoTRACKER provided important data related to CyanoHABs assessments for regional and global water bodies. Reports and data received via social cloud including the mobile App, Twitter, Facebook, and CyanoTRACKER website, helped in identifying the geographic locations of CyanoHABs affected water bodies. A significant increase (124.92%) in tweet numbers related to CyanoHABs was observed between 2011 (total relevant tweets = 2925) and 2015 (total relevant tweets = 6579) that reflected an increasing trend of the harmful phenomena across the globe as well as an increased awareness about CyanoHABs among Twitter users. The CyanoHABs affected water bodies extracted via the social cloud were categorized, and smaller water bodies were selected for the deployment of CyanoSense, and satellite data analysis was performed for larger water bodies. CyanoSense was able to differentiate between ordinary algae and CyanoHABs through the use of their characteristic absorption feature at 620 nm. The results and products from this infrastructure can be rapidly disseminated via the CyanoTRACKER website, social media, and direct communication with appropriate management agencies for issuing warnings and alerting lake managers, stakeholders and ordinary citizens to the dangers posed by these environmentally harmful phenomena.}
}
@article{YU2020100203,
title = {Service node selection optimization for mobile crowd sensing in a road network environment},
journal = {Vehicular Communications},
volume = {22},
pages = {100203},
year = {2020},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2019.100203},
url = {https://www.sciencedirect.com/science/article/pii/S2214209619302505},
author = {Haiyang Yu and Chenyang Liu and Yilong Ren and Nan Ji and Can Yang},
keywords = {Mobile crowd sensing, Urban road network, Quality of service (QoS), Service node selection},
abstract = {Mobile crowd sensing (MCS) can be an effective method for urban traffic sensing applications by collecting data in urban road networks through ubiquitous sensor-mounted vehicles. However, due to the limited network resources and the randomness of automobiles, the quality of service (QoS) of MCS cannot be effectively guaranteed. Some related works noted that optimizing the selection of service nodes can effectively improve the QoS of MCS. However, existing node selection methods are unsuitable for MCS in an urban road network (MCS-URN). An MCS-URN is a unique MCS environment in which the service nodes are vehicles, and the sensing area is road segments. In this paper, we focus on improving the QoS in an MCS-URN by optimizing the selection of service nodes with limited network resources. First, the utility function of the QoS in MCS-URN is proposed based on the coverage and the data score. Then the service node optimization model in the MCS-URN is presented, by selecting an appropriate set of service nodes within the maximum proportion of the total network resources to maximize the utility value of the QoS. Also, an innovative service node selection method which considering the mobility of automobiles and the topological structure of urban road networks is introduced. In the end, a simulation study is carried out to evaluate the service node optimization model in the MCS-URN. The simulation results show that our service node optimization model can effectively improve the QoS of the MCS-URN.}
}
@article{SONG2020105572,
title = {SACPC: A framework based on probabilistic linguistic terms for short text sentiment analysis},
journal = {Knowledge-Based Systems},
volume = {194},
pages = {105572},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.105572},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120300599},
author = {Chao Song and Xiao-Kang Wang and Peng-fei Cheng and Jian-qiang Wang and Lin Li},
keywords = {Semantic change, Sentiment analysis, Probabilistic linguistic terms, Polarity classification},
abstract = {Short text sentiment analysis is challenging because short texts are limited in length and lack context. Short texts are usually rather ambiguous because of polysemy and the typos these texts contain. Polysemy is the coexistence of multiple word meanings and commonly appears in every language. Various uses of a word may assign the word both positive and negative meanings. In previous studies, the variability of words is often ignored, which may cause analysis errors. In this study, to resolve this problem, we proposed a novel text representation model named Word2PLTS for short text sentiment analysis by introducing probabilistic linguistic terms sets (PLTSs) and the relevant theory. In this model, every word is represented as a PLTS that fully describes the possibilities for the sentiment polarity of the word. Then, by using support vector machines (SVM), a novel sentiment analysis and polarity classification framework named SACPC is obtained. This framework is a technique that combines supervised learning and unsupervised learning. We compare SAPCP to lexicon-based approaches and machine learning approaches on three benchmark datasets. A noticeable improvement in performance is achieved. To further verify the superiority of SAPCP, state of the art performance comparisons are conducted, and the results are impressive.}
}
@incollection{CHRISMAN202043,
title = {Geographic Information Systems, History of},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {43-47},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10555-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105554},
author = {Nicholas R. Chrisman},
keywords = {Computing, Conceptions of space, Database, GIS, Social implications of technology},
abstract = {Geographic Information Systems (GIS) have emerged from diverse origins in a number of disciplines and professions to become an element of basic infrastructure supporting many applications. The GIS technical assemblage engages a diversity of actors (human and nonhuman) in various distinct settings around the world, making it difficult to frame a coherent narrative for the history of this enterprise. The term itself can be traced to the mid-1960s, when it was adopted by a number of persons who later claimed first use. Some components of GIS were already in use at that time, but the term became a catalyst for coherent action by the 1980s, particularly in North America. This history combines some broad themes related to technological changes in the larger society, as well as how geographers and cartographers have come to terms with the specific nature of GIS innovations.}
}
@article{YOU2020103261,
title = {Tibetan Plateau amplification of climate extremes under global warming of 1.5 °C, 2 °C and 3 °C},
journal = {Global and Planetary Change},
volume = {192},
pages = {103261},
year = {2020},
issn = {0921-8181},
doi = {https://doi.org/10.1016/j.gloplacha.2020.103261},
url = {https://www.sciencedirect.com/science/article/pii/S0921818120301521},
author = {Qinglong You and Fangying Wu and Liucheng Shen and Nick Pepin and Zhihong Jiang and Shichang Kang},
keywords = {Tibetan Plateau, CMIP5, 1.5 °C, 2 °C and 3 °C, Linearity analysis},
abstract = {Global warming may increase the frequency of climate extremes, but systematic examinations at different temperature thresholds are unknown over the Tibetan Plateau (TP). Changes in surface temperature and precipitation extreme indices derived from a multi-model ensemble mean (MMEM) of the Coupled Model Inter-comparison Project Phase 5 (CMIP5) models are examined under global warming of 1.5 °C (RCP2.6), 2 °C (RCP4.5) and 3 °C (RCP8.5) above pre-industrial levels. The TP amplification of future temperature and precipitation changes is evident for all three scenarios, with greater trend magnitudes in extreme indices than those for the whole China, regions between 25°N and 40°N, Northern Hemisphere (land only), Northern Hemisphere and the global mean. The TP amplification is also projected to intensify in each scenario, resulting in faster changes in intensity, duration and frequency of climate extremes. There appears to be greater difference for precipitation-based indices between 2 °C and 3 °C than for temperature, and the differences between 1.5 °C and 2 °C are less dramatic. Overall changes in climate extremes at 2 °C are greater than at 1.5 °C, but differences are less discernible between 3 °C and 2 °C. The Kolmogorov-Smirnov test between simulated and scaled temperature distributions shows that accelerated warming over the TP from 1.5 °C to 2 °C follows a broadly linear response, but the nonlinearity occurs between 2 °C and 3 °C. This suggests that the rate of warming might make a large difference to the future TP amplification at different thresholds.}
}
@article{BATAINEH2020435,
title = {Toward monetizing personal data: A two-sided market analysis},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {435-459},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301414},
author = {Ahmed Saleh Bataineh and Rabeb Mizouni and Jamal Bentahar and May {El Barachi}},
keywords = {Personal data monetization, Two-sided market, Demand curve, Supply curve, Externalities, Subsidy technique},
abstract = {With the increasing popularity of social mobile applications and mobile crowd sensing, holders of smart devices are generating a huge amount of personal data. Nowadays, a wide variety of domains ranging from health-care applications to pollution monitoring are benefiting from collected data. In fact, these personal data may have a monetary value and currently, secondary data owners (such as clinics, Facebook and Twitter) are getting benefit from them either by reselling these data to third entities or by generating statistical analysis. Unfortunately, the primary data owners, the users themselves, are not getting benefit from these transactions. Today, there is no platform to help users monetize their own personal data. In this paper, we propose a two-sided market-based platform for monetizing personal data. Given the intrinsic properties of data as economic good, we prove formally that two-sided market is a realistic solution as it can offer the service of collecting the required data amount and within the quality range required by the buyers. More precisely, (1) we study the two-sided platform equilibrium under non-linear externalities and extract mathematically the condition that states which side will be subsidized by the platform; (2) we study formally the impact of the direct sale mechanism on the platform payoff and show that the platform payoff is given by a logarithmic function of end users stability in the platform; and finally (3) using a real dataset from Amazon, we construct an empirical comparison between the two-sided platform model and the classic merchant model. In addition, we simulate the efficiency of the two-sided market model in presence of direct sale. Simulation results show that our two-sided market platform can play a critical role in motivating users to share their personal data and can be a practical solution for monetizing data generated from mobile crowd sensing.}
}
@incollection{PEZOULAS2020185,
title = {Chapter 6 - Cloud infrastructures for data sharing},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {185-226},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000062},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Cloud architectures, Cloud computing, Cloud computing services, Cloud security, Healthcare},
abstract = {This chapter offers the basis for understanding the origins of cloud computing and its importance toward medical data sharing. The concept of cloud computing is described through the Internet of Things (IoT) along with standard cloud models for enabling this concept. Popular cloud architectures are presented, including the Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS), as the three fundamental layers for establishing a cloud platform. Methods for developing web services are further presented along with tools and popular cloud vendors in healthcare. Cloud security issues, encryption protocols, and international security guidelines and standards are discussed. The most important technical challenges are also highlighted toward a secure, on-demand, federated cloud platform.}
}
@article{YANG2020440,
title = {Study on a soil erosion sampling survey in the Pan-Third Pole region based on higher-resolution images},
journal = {International Soil and Water Conservation Research},
volume = {8},
number = {4},
pages = {440-451},
year = {2020},
note = {Soil erosion assessment tools and data; creation, consolidation, and harmonization – A special issue from the Global Symposium on Soil Erosion 2019 (Rome, FAO HQ).},
issn = {2095-6339},
doi = {https://doi.org/10.1016/j.iswcr.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095633920300502},
author = {Qinke Yang and Mengyang Zhu and Chunmei Wang and Xiaoping Zhang and Baoyuan Liu and Xin Wei and Guowei Pang and Chaozhen Du and Lihua Yang},
keywords = {Pan -third pole area, Land use, Soil conservation measures, Remote sensing, Variable probability sampling},
abstract = {Soil erosion is one of the most severe global environmental problems, and soil erosion surveys are the scientific basis for planning soil conservation and ecological development. To improve soil erosion sampling survey methods and accurately and rapidly estimate the actual rates of soil erosion, a Pan-Third Pole region was taken as an example to study a methodology of soil erosion sampling survey based on high-spatial-resolution remote sensing images. The sampling units were designed using a stratified variable probability systematic sampling method. The spatiotemporal characteristics of soil erosion and conservation were taken into account, and finer-resolution freely available and accessible images in Google Earth were used. Through the visual interpretation of the free high-resolution remote sensing images, detailed information on land use and soil conservation measures was obtained. Then, combined with the regional soil erosion factor data products, such as rainfall-runoff erosivity factor (R), soil erodibility factor (K), and slope length and steepness factor (LS), the soil loss rates of some sampling units were calculated. The results show that, based on these high-resolution remote sensing images, the land use and soil conservation measures of the sampling units can be quickly and accurately extracted. The interpretation accuracy in 4 typical cross sections was more than 80%, and sampling accuracy, described by histogram similarity in 11 large sampling sites, show that the landuse of sampling uints can represent the structural characteristics of regional land use. Based on the interpretation of data from the sample survey and the regional soil erosion factor data products, the calculation of the soil erosion rate can be completed quickly. The calculation results can reflect the actual conditions of soil erosion better than the potential soil erosion rates calculated by using the coarse-resolution remote sensing method.}
}
@article{FAUNDEZUGALDE2020105441,
title = {Use of artificial intelligence by tax administrations: An analysis regarding taxpayers’ rights in Latin American countries},
journal = {Computer Law & Security Review},
volume = {38},
pages = {105441},
year = {2020},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105441},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920300467},
author = {Antonio Faúndez-Ugalde and Rafael Mellado-Silva and Eduardo Aldunate-Lizana},
keywords = {Artificial intelligence, Tax law, Right to defense, Tax administration},
abstract = {In this paper, we analyze taxpayers’ rights to have access to artificial intelligence algorithms and formulas that have been used by tax administrations in Latin America. We consider two applications of artificial intelligence: in the characterization of taxpayers’ risk and the robotization of tax audit actions. Very little has been described in the literature on how these technologies coexist with taxpayers’ rights, especially in the exercise of their right to defense in administrative and contentious proceedings. The evidence reflects that, although in the countries under study the access to these techniques is not clearly regulated, general principles derived from the fundamental rights declared by each country make it possible to safeguard taxpayers’ right to access this information.}
}
@article{TONG2020135323,
title = {Prediction model for air particulate matter levels in the households of elderly individuals in Hong Kong},
journal = {Science of The Total Environment},
volume = {717},
pages = {135323},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.135323},
url = {https://www.sciencedirect.com/science/article/pii/S004896971935315X},
author = {Xinning Tong and Jason Man Wai Ho and Zhiyuan Li and Ka-Hei Lui and Timothy C.Y. Kwok and Kelvin K.F. Tsoi and K.F. Ho},
keywords = {Prediction model, Linear mixed-effects regression, Indoor air, PM},
abstract = {Air pollution has shown to cause adverse health effects on mankind. Aging causes functional decline and leaves elderly people more susceptible to health threats associated with air pollution exposure. Elderly spend approximately 80% of their lifetime at home every day. To understand air pollution exposure, indoor air pollutants are the targets for consideration especially for the elderly population. However, indoor air monitoring for epidemiological studies requires a large population, is labor intensive and time consuming. As a result, a prediction model is necessary. For 3 consecutive days in summer and winter, 24-h average of mass concentrations of fine particulate matter (aerodynamic diameter <2.5 μm: PM2.5) were measured in indoors for 116 households. A PM2.5 prediction model for elderly households in Hong Kong has been developed by combining ambient PM2.5 concentrations obtained from land use regression model and questionnaire-elicited information related to the indoor PM2.5 sources. The fitted linear mixed-effects model is moderately predictive for the observed indoor PM2.5, with R2 = 0.67 (or R2 = 0.61 by cross-validation). The model shows indoor PM2.5 was positively influenced by outdoor PM2.5 levels. Meteorological factors (e.g. temperature and relative humidity) were related to the indoor PM2.5 in a relatively complex manner. Congested living areas, opening windows for extended periods for ventilation and use of liquefied petroleum gas for cooking were the factors determining the ultimate indoor air quality. This study aims to provide information about controlling household air quality and can be used for future epidemiological studies associated with indoor air pollution in large population.}
}
@incollection{COUCLELIS2020357,
title = {Computational Human Geography},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {357-363},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10619-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955106195},
author = {Helen Couclelis},
keywords = {Agent-based models, Ambient computing, Cellular automata, Computer modeling, Data revolution, GIS, Geocomputation, Simulation, Urban informatics, Visualization},
abstract = {Computational human geography refers to the use of computational methods and techniques to solve problems in human geography research and applications. The approach goes back to the beginnings of the quantitative revolution in geography and is philosophically related though methodologically distinct from it. Geographic information systems (GIS) and science are a big part of computational human geography, but the latter notion is considerably broader, encompassing spatial process modeling and simulation, the modeling of spatial decision and behavior, visualization techniques, spatial analysis, and an increasing number of new research areas and methods enabled by the most recent technological developments. The latter are discussed under the rubrics of The Data Revolution, Urban (Spatial) Informatics, and Ambient Computing. Two major thrusts have persisted throughout the years: the use of numerical techniques to solve large, complex quantitative problems; the development of models of complex spatial processes expressed directly in computational terms. Both have evolved with the times and continue to be central to computational human geography. Critiques originate from both within the field and from the humanities and social theory perspectives. These address epistemological and methodological problems as well as issues of ontology and representation.}
}
@article{ULUSAR2020106780,
title = {Cognitive RF-based localization for mission-critical applications in smart cities: An overview},
journal = {Computers & Electrical Engineering},
volume = {87},
pages = {106780},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106780},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620306352},
author = {Umit Deniz Ulusar and Gurkan Celik and Fadi Al-Turjman},
keywords = {Localization, Mission-critical applications, Positioning technologies, Smart cities, IoT},
abstract = {The accessibility of accurate location information for operators in mission-critical scenarios would considerably increase their mission success. In order to obtain precise location information, numerous algorithms and technologies have been suggested. These methods and systems show varying performances under different conditions, and with the help of machine learning techniques, their reliability can be enhanced dramatically. In this paper, we overview the state-of-the-art in emerging algorithms and technologies employing cognitive solutions in mission critical localization applications. We compare these algorithms in terms of different localization parameters such as scalability, power consumption, availability, service quality and accuracy. Consequently, this survey will assist researchers who are working in the area of RF-based localization to achieve better performance in mission critical scenarios that can be experienced in smart city applications.}
}
@incollection{FAULKNER202093,
title = {5 - Data-Centric Systems},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {93-103},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00018-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000188},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Classification data-centric systems, Uncertainty, Adaptive systems},
abstract = {This chapter delves more deeply into the nature of data-centric systems. It classifies systems that employ data into three categories. Increasingly machine learning facilitates the ability to allow systems to use data to make choices (decisions) that are more strategic with minimal direct human control. Decision models based on collected data and processing of that data are explored. There is a high degree of uncertainty inherent in this shift. Data veracity becomes a core challenge. Safety management of adaptive systems is important because of their rapidly changing nature.}
}
@article{ZENG2020101054,
title = {Prediction of building electricity usage using Gaussian Process Regression},
journal = {Journal of Building Engineering},
volume = {28},
pages = {101054},
year = {2020},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2019.101054},
url = {https://www.sciencedirect.com/science/article/pii/S235271021930662X},
author = {Aaron Zeng and Hodde Ho and Yao Yu},
keywords = {Energy use prediction, Machine learning, Electricity consumption, Gaussian process regression, Online building energy},
abstract = {The prediction of building energy use is the basis for smart building operation, which optimizes building performance through control and low-energy strategy. For reducing computation complexity and improving calculation accuracy, a comparative study of online electricity data predictions for different types of buildings was conducted. This study is also intended to assess the capability and accuracy of the supervised machine learning methods, with which the kernel algorithms of predictions were developed. Specifically, in this study, large-scale real data collected from the building energy management system were used in the online energy consumption forecasting, which is specially designed for optimized control, real-time fault detection, diagnosis and abnormality alarms. Firstly, the characteristics of building energy profiles and data reliability were addressed. Mathematical algorithms were introduced and their previous applications in building energy usage prediction were summarized, including the evaluation criteria that are effective for energy use predictions in buildings. The reliability and efficiency of the proposed algorithms were then demonstrated through the comparison between the monitored actual data and the predicted results. It is found that Gaussian Process Regression (GPR) can give acceptable predictions on the energy consumption of office buildings with an equilibrium of data prediction accuracy with the average deviations of below 15% and low computation time. Additionally, the statistical evaluation criteria proposed by ASHRAE can also be satisfied. For hotels and shopping malls where complex functions were applied in these buildings, their accuracy are not better or even the same as those of simplified models, due to the significant effects of the factors involving occupant's activities and schedules as well as data reliability on building energy usage. Our result revealed that GPR is a reliable method and can still generate highly accurate predictions when a large data set with a small time interval and complex energy use patterns obtained from real building measurements rather than simulated data are involved.}
}
@article{DU2020105274,
title = {An intelligent recognition model for dynamic air traffic decision-making},
journal = {Knowledge-Based Systems},
volume = {199},
pages = {105274},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105274},
url = {https://www.sciencedirect.com/science/article/pii/S095070511930574X},
author = {Xinru Du and Zi Lu and Dianshuang Wu},
keywords = {Intelligent management systems, Air traffic flow, Dynamic decision-making, Intelligent rules},
abstract = {Air traffic flow management system (ATFMS) is becoming increasingly important due to the rapid growth of air traffic and serious flight delay nowadays. To aware the air traffic flow density and identify the heat airspace in terminal areas of large hub airports is essential for an ATFMS. Due to numerous parameters in air traffic flow, traditional methods based on one single parameter fail to reflect the true complexity relationship between these parameters. This study aims to develop an intelligent air traffic flow heat airspace recognition model using advanced data science technique for establishing a real-time cloud map in the terminal airspace of airports, which attempts to use machine learning models to represent the complex relationship among these parameters. In the proposed intelligent recognition model, high dimensions of parameters (basic parameters, additional parameters and time parameters) are processed to achieve a comprehensive and accurate situation awareness for support dynamic air traffic decision-making. An aircraft trajectories points clustering method is developed to generate a 4D heat airspace map. The basic parameters and time parameters are used to identify the heat airspaces; the changes of additional parameters which influence the heat airspaces are identified and analyzed by use of grid graphs of flight trajectories; probability fitting graphs are used to verify accuracy of 4D results in order to support air traffic decision-making. A case study on Beijing International Airport (PEK) is conducted to test our model and has obtained two main research findings: there are two areas of PEK that have the high density and there are hot peaks at two different heights; flight trajectories and speed of trajectories also effect on the heat airspace. The study realizes that the proposed 4D heat airspace model is better for detailed and accurate information construction, expression of spatial changes, and visualization of multiple parameters of temporal and spatial density and range. It can assist the decisions on airspace allocation, and also have a definite reference meaning on alleviating the contradiction between the current air traffic demand and airspace resource constraints.}
}
@article{GUTIERREZOSORIO2020432,
title = {Modern data sources and techniques for analysis and forecast of road accidents: A review},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {7},
number = {4},
pages = {432-446},
year = {2020},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S209575642030101X},
author = {Camilo Gutierrez-Osorio and César Pedraza},
keywords = {Traffic engineering, Data analysis, Machine learning, Road accident forecasting, Traffic accident prediction},
abstract = {Road accidents are one of the most relevant causes of injuries and death worldwide, and therefore, they constitute a significant field of research on the use of advanced algorithms and techniques to analyze and predict traffic accidents and determine the most relevant elements that contribute to road accidents. The research of road accident prediction aims to respond to the challenge of offer tools to generate a more secure mobility environment, and ultimately, save lives. This paper aims to provide an overview of the state of the art in the prediction of road accidents through machine learning algorithms and advanced techniques for analyzing information, such as convolutional neural networks and long short-term memory networks, among other deep learning architectures. Furthermore, in this article, a compendium and study of the most used data sources for the road accident forecast is made. And a classification is proposed according to its origin and characteristics, such as open data, measurement technologies, onboard equipment and social media data. For the analysis of the information, the different algorithms employed to make predictions about road accidents are listed and compared, as well as their applicability depending on the types of data being analyzed, along with the results obtained and their ease of interpretation and analysis. The best results reported by the authors are obtained when two or more analytic techniques are combined, in such a way that analysis of the obtained results is strengthened. Among the future challenges in road traffic forecasting lies the enhancement of the scope of the proposed models and predictions by the incorporation of heterogeneous data sources, that include geo spatial data, information from traffic volume, traffic statistics, video, sound, text and sentiment from social media, that many authors concur that can improve the precision and accuracy of the analysis and predictions.}
}
@article{GHORBEL2020101864,
title = {Handling data imperfection—False data inputs in applications for Alzheimer’s patients},
journal = {Data & Knowledge Engineering},
volume = {130},
pages = {101864},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101864},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2030094X},
author = {Fatma Ghorbel and Fayçal Hamdi and Nassira Achich and Elisabeth Metais},
keywords = {Applications for Alzheimer’s patients, Imperfection types, False data inputs, Believability},
abstract = {Handling data imperfection is a crucial issue in many application domains. This is particularly true when handling imperfect data inputs in applications for Alzheimer’s patients. In this paper we first propose a typology of imperfection for data entered by Alzheimer’s patients or their caregivers in the context of these applications (mainly due to the memory discordance caused by the disease). This topology includes nine direct and three indirect imperfection types. The direct ones are deduced from the data inputs e.g. uncertainty and uselessness. The indirect imperfection types are deduced from the direct ones, e.g. the redundancy. We then propose an approach, called DBE_ALZ, that handles false data entry by estimating the believability of each data input. Based on the proposed typology, the falsity of these data is related to five imperfection types: uncertainty, confusion, typing error, wrong knowledge and inconsistency. DBE_ALZ includes a believability model that defines a set of dimensions and sub-dimensions allowing a qualitative estimation of the believability of a given data input. It is estimated based on its reasonableness and the reliability of its author. Compared to related work, the data input reasonableness is measured not only based on common-sense standard, but also based on a set of personalized assertions. The reliability of the patient is estimated based on the progression of the disease and the state of his memory at the moment of entry. However, the reliability of the caregiver is estimated based on his age and his knowledge about the data input’s field. Based on the believability model, we estimate quantitatively the believability of the data input by defining a set of metrics associated to the proposed dimensions and sub-dimensions. The measurement methods rely on probability and fuzzy set theories to reason about uncertain and imprecise knowledge (Bayesian networks and Mamdani fuzzy inference systems). Three languages are supported: English, French and Arabic. Based on the generated believability degrees, a set of decisive actions are proposed to guarantee the quality of the data inputs e.g., inferring or not based on a given data. We illustrate the usefulness of our approach in the context of the Captain Memo memory prosthesis. Finally, we discuss the encouraging results derived from the evaluation step.}
}
@article{RICHTER2020119970,
title = {Towards an integrated urban development considering novel intelligent transportation systems: Urban Development Considering Novel Transport},
journal = {Technological Forecasting and Social Change},
volume = {155},
pages = {119970},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.119970},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518319498},
author = {Andreas Richter and Marc-O. Löwner and Rüdiger Ebendt and Michael Scholz},
keywords = {Intelligent Transportation Systems, Urban development, Geo-databases, Toolchains, Data formats},
abstract = {Urban areas are currently facing new and enormous challenges: urbanization, connected and automated land and air transport with new demand for transport and logistic services, maintenance of more complex traffic and supply infrastructure as well as mandatory digitalization of cadastral information under the constraints of limited space and resources. Different stakeholders are interested in using detailed, precise and up-to-date data about the urban environment. These stakeholders are not only governmental bodies, road operators and (public) fleet managers but also companies that are interested in testing and operating new intelligent transportation systems and connected and automated vehicles in realistic and complex urban simulation environments. This article proposes a concept of how to tackle this complex task based on approaches already conducted in the domain of the development and test of automated driving and city modeling. Core elements of this thesis are an all-embracing geo-database, a toolchain to import, validate, process and fuse the necessary data as well as interfaces and data formats for automated data exchange. The feasibility and challenges as well as the potential and synergies of implementing of this concept are discussed by analyzing similar solutions in the key domains. The article concludes with a proposal to realize such a concept.}
}
@article{CAMMIN2020102479,
title = {Monitoring of air emissions in maritime ports},
journal = {Transportation Research Part D: Transport and Environment},
volume = {87},
pages = {102479},
year = {2020},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2020.102479},
url = {https://www.sciencedirect.com/science/article/pii/S1361920920306660},
author = {Philip Cammin and Jingjing Yu and Leonard Heilig and Stefan Voß},
keywords = {Monitoring, Air emissions, Inventory, Green ports, Information systems},
abstract = {Maritime ports play a crucial role in the development of domestic and international trade and economies. Although near-port communities profit from economic benefits, there exist significant concerns regarding exposure to air emissions, which affect human health and climate change. To tackle this issue, a port authority can develop emission reduction plans and projects, whose performance is tracked through an air emissions inventory (EI). Despite the attention on EI methodologies in the past, little research has focused on the implementation of methodologies in information systems. Therefore, a case study is conducted in this paper to investigate the motivation for creating EIs and the obstacles in the EI-creation process from an information systems perspective. The results indicate that data confidentiality and weak information systems are major obstacles, which hinder the creation of high-quality EIs and generate additional costs. Our findings enable port stakeholders and decision makers to understand the current obstacles and facilitate the development of adequate information systems that support the creation of high-quality EIs.}
}
@incollection{HOFFMANN2020237,
title = {Roadless Areas as Key Approach to Conservation of Functional Forest Ecosystems},
editor = {Michael I. Goldstein and Dominick A. DellaSala},
booktitle = {Encyclopedia of the World's Biomes},
publisher = {Elsevier},
address = {Oxford},
pages = {237-248},
year = {2020},
isbn = {978-0-12-816097-8},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.11896-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489118962},
author = {Monika T. Hoffmann and Stefan Kreft and Vassiliki Kati and Pierre L. Ibisch},
keywords = {Cartography, Conservation policy, Road impacts, Roadless areas, Roadless forests},
abstract = {Roadless areas are free from any kind of road(-like) infrastructure and their direct or indirect impacts on the ecosystems. The largest tracts of ecologically most valuable roadless areas refer to large unfragmented forests regions, both in the tropics and the boreal zone (Amazon, Congo basin, East and Southeast Asia). Among all terrestrial ecosystems, roadless forests are the single most important strongholds of regulating ecosystem services: among others, soil protection, water retention, buffering of the local and regional climate and mitigation of global climate change via capturing of atmospheric carbon. But roadless areas also comprise much demanded natural resource assets, such as timber, often also minerals and space for agricultural development. There is a substantial conflict between diverse short-term economic interests and the long-term conservation of roadless areas. Large roadless areas can serve as a measurable surrogate for the most pristine and functional ecosystems. Roadlessness is a property of areas, which are not impacted by roads; it can be used as a proxy for assessing ecosystem integrity and the absence of many anthropogenic disturbances. We recommend that policy-makers give roadless areas conservation priority over areas that have already been fragmented. It is essential to establish roadlessness as a criterion for the planning of ecosystem-based, cost-effective sustainable development. In parallel with measures to protect roadlessness, we recommend alternative approaches to mobility that can work under roadless conditions, e.g., related to railroads, blimps and other low-energy technologies with low infrastructure requirements. Even if “climate-friendly” renewable energy was available for road transport on a large scale, the construction, existence and operation of roads would continue to severely impair ecosystem functionality.}
}
@article{BAO2020100057,
title = {Explaining the Genetic Causality for Complex Phenotype via Deep Association Kernel Learning},
journal = {Patterns},
volume = {1},
number = {6},
pages = {100057},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920300684},
author = {Feng Bao and Yue Deng and Mulong Du and Zhiquan Ren and Sen Wan and Kenny Ye Liang and Shaohua Liu and Bo Wang and Junyi Xin and Feng Chen and David C. Christiani and Meilin Wang and Qionghai Dai},
keywords = {kernel learning, association analysis, deep learning, genome-wide association studies, disease causality},
abstract = {Summary
The genetic effect explains the causality from genetic mutations to the development of complex diseases. Existing genome-wide association study (GWAS) approaches are always built under a linear assumption, restricting their generalization in dissecting complicated causality such as the recessive genetic effect. Therefore, a sophisticated and general GWAS model that can work with different types of genetic effects is highly desired. Here, we introduce a deep association kernel learning (DAK) model to enable automatic causal genotype encoding for GWAS at pathway level. DAK can detect both common and rare variants with complicated genetic effects where existing approaches fail. When applied to four real-world GWAS datasets including cancers and schizophrenia, our DAK discovered potential casual pathways, including the association between dilated cardiomyopathy pathway and schizophrenia.}
}
@article{LWAKATARE2020106368,
title = {Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions},
journal = {Information and Software Technology},
volume = {127},
pages = {106368},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106368},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301373},
author = {Lucy Ellen Lwakatare and Aiswarya Raj and Ivica Crnkovic and Jan Bosch and Helena Holmström Olsson},
keywords = {Machine learning systems, Software engineering, Industrial settings, Challenges, Solutions, SLR},
abstract = {Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.}
}
@article{CORDIER20202,
title = {Comprehensive care documentation: A first step not to be missed},
journal = {Australian Critical Care},
volume = {33},
number = {1},
pages = {2},
year = {2020},
issn = {1036-7314},
doi = {https://doi.org/10.1016/j.aucc.2019.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1036731419302206},
author = {Pierre-Yves Cordier and Eliott Gaudray and Edouard Martin and Raphaël Paris and Salah Boussen and Philippe Goutorbe}
}
@article{HRIBERNIK2020191,
title = {Unified Predictive Maintenance System - Findings Based on its Initial Deployment in Three Use Case},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {191-196},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.031},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301774},
author = {K. Hribernik and M. {von Stietencron} and D. Ntalaperas and K.-D. Thoben},
keywords = {maintenance, predictive maintenance, system, platform},
abstract = {With the maintenance offering significant savings potential for manufacturing companies, the relevance of advanced maintenance strategies like predictive maintenance is becoming increasingly important. The H2020 action UPTIME is developing a Unified Predictive Maintenance System, which is currently being applied to three industrial use cases. This paper presents the current state of the developments and its implementation in the use cases.}
}
@article{WANG2020101785,
title = {Privacy-preserving high-dimensional data publishing for classification},
journal = {Computers & Security},
volume = {93},
pages = {101785},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101785},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820300705},
author = {Rong Wang and Yan Zhu and Chin-Chen Chang and Qiang Peng},
keywords = {Privacy-preserving data publishing, High-dimensional data, Classification analysis, -Anonymity},
abstract = {With increasing amounts of personal information being collected by various organizations, many privacy models have been proposed for masking the collected data so that the data can be published without releasing individual privacy. However, most existing privacy models are not applicable to high-dimensional data, because of the sparseness of high-dimensional search space. In this paper, we present our solution to release high-dimensional data for privacy preservation and classification analysis. The challenge facing us is how to reduce high dimensions from the perspective of privacy models while preserving as much information as possible for classification. Our proposed approach tackles it by using an idea of vertical partition, which is to vertically divide the raw data into different disjointed subsets of smaller dimensionality. Specifically, our partition metric considers both the correlation between attributes and the proportion of attributes in each subset. Then a generalization method based on local recoding is employed to each subset separately for achieving k-anonymity. Considering the hardness of the optimal implementation of k-anonymity, the local recoding method finds a near-optimal solution with the goal of improving efficiency. The proposed approach was evaluated using two datasets, and the experimental results showed that it outperformed two related approaches in data utility at the same privacy level.}
}
@article{MANIVASAGAM2020105554,
title = {Practices for upscaling crop simulation models from field scale to large regions},
journal = {Computers and Electronics in Agriculture},
volume = {175},
pages = {105554},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105554},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920304622},
author = {V.S. Manivasagam and Offer Rozenstein},
keywords = {Remote sensing, Regional yield assessment, Process-based model, Data assimilation, Scalable yield modeling},
abstract = {Most crop models were developed and tested in homogeneous field conditions. However, these crop models are increasingly applied beyond the field scale for larger regions. Inadequate representation of the spatial variability at a larger scale introduces significant errors in the models’ predictions, yet attention to this topic is lacking. The selection of optimal crop models and their inputs when moving from the field to a regional scale must be performed carefully using strict guidelines while considering uncertainty propagation. This paper reviews crop modeling applications and their constraints in large-scale studies. The discussion focuses on the core issues that arise when applying crop models to a range of spatial scales: (i) parameterization and calibration of model inputs beyond field scale; (ii) constraints in the selection of model inputs at various scales; and (iii) retrieval and integration of remotely sensed crop variables into the crop model. Further, this review highlights cutting-edge approaches, namely scalable yield modeling, semi-empirical crop models, and global modeling initiatives, which can be used in a multi-scale assessment of agricultural systems.}
}
@article{KATAPALLY2020102453,
title = {A systematic review of the evolution of GPS use in active living research: A state of the evidence for research, policy, and practice},
journal = {Health & Place},
volume = {66},
pages = {102453},
year = {2020},
issn = {1353-8292},
doi = {https://doi.org/10.1016/j.healthplace.2020.102453},
url = {https://www.sciencedirect.com/science/article/pii/S1353829220303956},
author = {Tarun R. Katapally and Jasmin Bhawra and Pinal Patel},
keywords = {Global positioning systems, Active living, Physical activity, Sedentary behavior, Physical contexts, Social environment},
abstract = {This is the first systematic review to comprehensively capture Global Positioning Systems’ (GPS) utilization in active living research by investigating the influence of physical contexts and social environment on all intensities of physical activity and sedentary behavior among all age groups. An extensive search of peer-reviewed literature was conducted using six databases. Out of 2026 articles identified, 129 studies met the inclusion criteria. After describing the evolution of GPS use across four themes (study designs and methods, physical contexts and social environment, active transportation, and behaviors), evidence-based recommendations for active living research, policy, and practice were generated.}
}
@article{WU2020121011,
title = {Impacts of agricultural industrial agglomeration on China’s agricultural energy efficiency: A spatial econometrics analysis},
journal = {Journal of Cleaner Production},
volume = {260},
pages = {121011},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.121011},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620310581},
author = {Jianzhai Wu and Zhangming Ge and Shuqing Han and Liwei Xing and Mengshuai Zhu and Jing Zhang and Jifang Liu},
keywords = {Industrial agglomeration, Energy efficiency, Spatial econometrics, Agriculture in China},
abstract = {The rapid development of traditional agriculture in China was achieved at the expense of high energy consumption and investments. However, the global green development trend made it necessary for the country to transform its agricultural energy utilization. Energy efficiency changes are affected by many factors, particularly industrial agglomeration. In recent years, the Chinese government has introduced a series of policies, including setting major producing regions for grains and advantageous regions for characteristic agricultural product. These have caused significant changes to the spatial layout of the agriculture industry. However, there is still a lack of research on the impact of these changes on agricultural energy efficiency (AEE). In this study, panel data of 30 Chinese provinces from 2000 to 2016 were entered into stochastic frontier models to measure the country’s AEE at the provincial level. A series of spatial econometric models were also used to analyze the impact of agricultural industrial agglomeration on China’s AEE. The results indicated that the country’s AEE exhibited obvious spatial gradients and correlations. After controlling the impacts of spatial correlation and other factors in the models, agricultural industrial agglomeration was found to have an overall positive impact on China’s AEE. In the future, policies should be formulated to increase AEE by establishing agricultural functional areas, strengthening the innovation and sharing of green development technologies at the farm level, and promoting the optimization of energy structures in agricultural and rural areas.}
}
@article{GAO2020107636,
title = {Ship encounter azimuth map division based on automatic identification system data and support vector classification},
journal = {Ocean Engineering},
volume = {213},
pages = {107636},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.107636},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820306363},
author = {Miao Gao and Guo-You Shi and Jiao Liu},
keywords = {Encounter azimuth map, AIS data, SVC, Classification},
abstract = {Currently, the division of encounter situations and collision avoidance decisions both depend on the individual subjective judgment of officers under conditions of extraordinary complexity and randomness. Ambiguities and contradictions are present among the existing quantifications of azimuth division from the International Regulations for Preventing Collisions at Sea (COLREGS), radar collision avoidance diagrams, and expert questionnaire results. At present, there is no unified and practical division model for the variety of azimuth divisions encountered by ships. With the development of intelligent ship technology, the realization of maritime autonomous surface ships is possible. However, more obscure problems must be accurately defined. Moreover, the requirements for an accurate division of the ship encounter situation in maritime accident analysis are becoming more intense. Additional requirements have been imposed on the division of azimuth, and ship encounters have been quantified into multiple features for machine learning. In this study, automatic identification system data near Zhoushan Port were used to reproduce the relative motion process of ships, and extract the meeting position of the ship and the corresponding actual avoidance behavior. By combining the requirements for the light range in COLREGS and support vector classification to supervise and learn the actual meeting data, a map of the ship encounter azimuth division was constructed. The map can serve as an accurate numerical basis for the division of marine encounter situations, maritime accident responsibility division, and intelligent ship collision avoidance decisions.}
}
@article{LIN202010280,
title = {The application of hydrogen and photovoltaic for reactive power optimization},
journal = {International Journal of Hydrogen Energy},
volume = {45},
number = {17},
pages = {10280-10291},
year = {2020},
note = {AEM 2018 - Smart Materials for Hydrogen Energy},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2019.08.078},
url = {https://www.sciencedirect.com/science/article/pii/S0360319919330514},
author = {Rongheng Lin and Zezhou Ye and Budan Wu},
keywords = {Hydrogen, Photovoltaic, Distributed generation, Reactive power optimization},
abstract = {Hydrogen and photovoltaic (PV) are two typical new energies, which are important to sustainable development. Introducing hydrogen or PV into smart grid as distributed generation (DG) becomes a promising approach. These kinds of power generations will help the grid gather more energy and introduce new chances of grid management. In this paper, we will introduce an application of hydrogen and PV in reactive power control. PV is used for hydrogen harvest, and PV is variable and dependent on weather conditions compared with a conventional generator that produces a stable output. Photovoltaic hydrogen fuel cell (PV-H2-FC) is introduced as DG, which connects to the grid. Adding hydrogen-based DG would help improve the quality of supply power. A genetic algorithm for DG site selection supporting DG cost optimization is proposed. Reactive power optimization (RPO) is an important function in planning for the future and daily operations of the smart grid system. Implementation of reactive power optimization based on the historical solution matching is also proposed, it considers the PV-H2-FC features and grid historical data, which uses Cosine distance for similarity measurement. The proposed RPO algorithm has a great advantage in calculation speed compared with traditional algorithms. The historical load data with the highest similarity are extracted, and its historical RPO scheme is applied to simulate the current RPO scheme. Results show that this method could help to find out an RPO solution effectively. The proposed solution would provide processing purposes for power company information data and further explore the supporting role of information resources in grid operations, which has broad social benefits.}
}
@article{CHANG2020216,
title = {Global road traffic injury statistics: Challenges, mechanisms and solutions},
journal = {Chinese Journal of Traumatology},
volume = {23},
number = {4},
pages = {216-218},
year = {2020},
issn = {1008-1275},
doi = {https://doi.org/10.1016/j.cjtee.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1008127520301437},
author = {Fang-Rong Chang and He-Lai Huang and David C. Schwebel and Alan H.S. Chan and Guo-Qing Hu},
keywords = {Traffic injury data, Reported problems, Mechanisms behind the data},
abstract = {High-quality data are the foundation to monitor the progress and evaluate the effects of road traffic injury prevention measures. Unfortunately, official road traffic injury statistics delivered by governments worldwide, are often believed somewhat unreliable and invalid. We summarized the reported problems concerning the road traffic injury statistics through systematically searching and reviewing the literature. The problems include absence of regular data, under-reporting, low specificity, distorted cause spectrum of road traffic injury, inconsistency, inaccessibility, and delay of data release. We also explored the mechanisms behind the problematic data and proposed the solutions to the addressed challenges for road traffic statistics.}
}
@article{MAKRINOU2020110611,
title = {Genome-wide methylation profiling in granulosa lutein cells of women with polycystic ovary syndrome (PCOS)},
journal = {Molecular and Cellular Endocrinology},
volume = {500},
pages = {110611},
year = {2020},
issn = {0303-7207},
doi = {https://doi.org/10.1016/j.mce.2019.110611},
url = {https://www.sciencedirect.com/science/article/pii/S0303720719303132},
author = {E. Makrinou and A.W. Drong and G. Christopoulos and A. Lerner and I. Chapa-Chorda and T. Karaderi and S. Lavery and K. Hardy and C.M. Lindgren and S. Franks},
keywords = {PCOS, EWAS, DNA methylation, Metabolic syndrome, Reproduction},
abstract = {Polycystic Ovary Syndrome (PCOS) is the most common endocrine disorder amongst women of reproductive age, whose aetiology remains unclear. To improve our understanding of the molecular mechanisms underlying the disease, we conducted a genome-wide DNA methylation profiling in granulosa lutein cells collected from 16 women suffering from PCOS, in comparison to 16 healthy controls. Samples were collected by follicular aspiration during routine egg collection for IVF treatment. Study groups were matched for age and BMI, did not suffer from other disease and were not taking confounding medication. Comparing women with polycystic versus normal ovarian morphology, after correcting for multiple comparisons, we identified 106 differentially methylated CpG sites with p-values <5.8 × 10−8 that were associated with 88 genes, several of which are known to relate either to PCOS or to ovarian function. Replication and validation of the experiment was done using pyrosequencing to analyse six of the identified differentially methylated sites. Pathway analysis indicated potential disruption in canonical pathways and gene networks that are, amongst other, associated with cancer, cardiogenesis, Hedgehog signalling and immune response. In conclusion, these novel findings indicate that women with PCOS display epigenetic changes in ovarian granulosa cells that may be associated with the heterogeneity of the disorder.}
}
@incollection{KRISHNAN2020199,
title = {11 - Data discovery and connectivity},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {199-212},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000119},
author = {Krish Krishnan},
keywords = {AI lifecycle, Artificial intelligence, Basel III, CCPA, Cloud computing, Data management, Future World, GDPR, Machine learning, New Communications, Prevalent Data},
abstract = {In the world today, data is produced every second and can drive insights that are beyond the imagination of the human threshold. We have tipped the threshold of data and analytics processing today with the integration of tools and technologies. The rate of accessing data as soon as it arrives or even as it is produced to provide insights and impact has evolved into an everyday expectation. The impact of cloud as a platform is very large and to realize the overall business value, speed is very important, and this platform capability will deliver that requirement.}
}
@article{ZIEGLER2020496,
title = {A Graph-based Approach to Manage CAE Data in a Data Lake},
journal = {Procedia CIRP},
volume = {93},
pages = {496-501},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.155},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120310349},
author = {Julian Ziegler and Peter Reimann and Florian Keller and Bernhard Mitschang},
keywords = {data lake, data management, metadata, graph, graph database, computer-aided engineering, simulation data},
abstract = {Computer-aided engineering (CAE) applications generate vast quantities of heterogeneous data. Domain experts often fail to explore and analyze these data, because they are not integrated across different applications. Existing data management solutions are rather tailored to scientific applications. In our approach, we tackle this issue by combining a data lake solution with graph-based metadata management. This provides a holistic view of all CAE data and of the data-generating applications in one interconnected structure. Based on a prototypical implementation, we discuss how this eases the task of domain experts to explore and extract data for further analyses.}
}
@article{DAWIDOWICZ2020100362,
title = {System architecture of an INSPIRE-compliant green cadastre system for the EU Member State of Poland},
journal = {Remote Sensing Applications: Society and Environment},
volume = {20},
pages = {100362},
year = {2020},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2020.100362},
url = {https://www.sciencedirect.com/science/article/pii/S2352938520301427},
author = {Agnieszka Dawidowicz and Marcin Kulawiak and Elżbieta Zysk and Katarzyna Kocur-Bera},
keywords = {Green cadastre, Agriculture, Spatial data infrastructure, Land administration system, GIS, INSPIRE},
abstract = {In response to the need for a sustainable agricultural policy, which would support activities such as decision making in precise agriculture and mitigation of crop threats, a concept agricultural information system was developed for the area of Poland. This innovative concept, called Green Cadastre (GC), proposes to create a uniform system designed for use on a national scale by both state administration as well as local farmers. This article presents the concept system architecture of the proposed GC solution. The system architecture takes into account the current state of the INSPIRE Spatial Data Infrastructure, the previously established requirements for a GC system, as well as an analysis of current trends in geospatial IT. The proposed solution combines open data exchange protocols with Open Source technologies and current international SDI standards in order to provide a flexible and cost-effective solution.}
}
@article{ERRANDONEA2020103316,
title = {Digital Twin for maintenance: A literature review},
journal = {Computers in Industry},
volume = {123},
pages = {103316},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103316},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305509},
author = {Itxaro Errandonea and Sergio Beltrán and Saioa Arrizabalaga},
keywords = {Decision support systems, Digital twin, Information system, Knowledge support system, Smart maintenance},
abstract = {In recent years, Digital Twins (DT) have been implemented in different industrial sectors, in several applications areas such as design, production, manufacturing, and maintenance. In particular, maintenance is one of the most researched applications, as the impact of the execution of maintenance task may have a great impact in the business of the companies. For example, in sector such as energy or manufacturing, a maintenance activity can cause the shutdown of an entire production line, or in the case of a wind turbine inspection, may face the safety of an operator to measure a simple indicator. Hence, the application of more intelligent maintenance strategies can offer huge benefits. In this context, this paper focuses on the review of DT applications for maintenance, as no previous work has been found with this aim. For instance, both “Digital Twin” and “maintenance” concepts and strategies are described in detail, and then a literature review is carried out where these two concepts are involved. In addition to identifying and analyzing how DTs are currently being applied for maintenance, this paper also highlights future research lines and open issues.}
}
@article{XHAFA2020730,
title = {Evaluation of IoT stream processing at edge computing layer for semantic data enrichment},
journal = {Future Generation Computer Systems},
volume = {105},
pages = {730-736},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19321296},
author = {Fatos Xhafa and Burak Kilic and Paul Krause},
keywords = {IoT computing, Edge computing, Data stream processing, Anomaly detection, Data stream rate},
abstract = {The fast development of Internet of Things (IoT) computing and technologies has prompted a decentralization of Cloud-based systems. Indeed, sending all the information from IoT devices directly to the Cloud is not a feasible option for many applications with demanding requirements on real-time response, low latency, energy-aware processing and security. Such decentralization has led in a few years to the proliferation of new computing layers between Cloud and IoT, known as Edge computing layer, which comprises of small computing devices (e.g. Raspberry Pi) to larger computing nodes such as Gateways, Road Side Units, Mini Clouds, MEC Servers, Fog nodes, etc. In this paper, we study the challenges of processing an IoT data stream in an Edge computing layer. By using a real life data stream set arising from a car data stream as well as a real infrastructure using Raspberry Pi and Node-Red server, we highlight the complexities of achieving real time requirements of applications based on IoT stream processing.}
}
@article{PENG2020402,
title = {High concurrency massive data collection algorithm for IoMT applications},
journal = {Computer Communications},
volume = {157},
pages = {402-409},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420304217},
author = {Jianhua Peng and Ken Cai and Xiaojing Jin},
keywords = {Sensor network, Internet of Things, Internet of Medical Things, Data collect, Concurrency},
abstract = {The symbiotic development of machine learning (ML) and artificial intelligence (AI) is amplifying the value of the Internet of Medical Things (IoMT). Doctors are able to reach actionable conclusions faster and more reliably when dealing with large volumes of streaming data from networked medical devices. However the Internet of Things (IoT) or sensor network has a large number of base stations transmitting data to the data center server, the data center server will face challenges in collecting, parsing, and processing data. Based on the existing technical solutions, when the number of wireless sensor network base stations is large, the data collection of the IoMT system will have a concurrent bottleneck, which will cause the data collection failure and have a catastrophic impact on the application of the IoMT. This paper proposes a highly concurrent and massive data collect algorithm for IoMT applications. This algorithm uses the principle of separation of reception and processing, distributed parallel processing and multi-threading technology, and combines the highly concurrent data transmission channel provided by TCP/IP to provide a set of independent data receiving components. This component receives the data of the IoT base station, and then simply processes the data and puts it into the distributed message system to complete the sensor data receiving function. It also provides a data processing cluster. Each node of the cluster starts multiple data processing unit, each data processing unit separately obtains sensor data from the distributed message system, processes the data, and delivers the processing results to the application. The experimental results show that the algorithm proposed in this paper has a high ability of parallel collection of IoMT data.}
}
@incollection{LADLEY2020141,
title = {Chapter 9 - Architecture and design},
editor = {John Ladley},
booktitle = {Data Governance (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {141-185},
year = {2020},
isbn = {978-0-12-815831-9},
doi = {https://doi.org/10.1016/B978-0-12-815831-9.00009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158319000096},
author = {John Ladley},
keywords = {Technology, Glossary, Lineage, Work flow, Collaboration, Operating model, Engagement model, Stewards, Stewardship, Accountability},
abstract = {This chapter covers the activities for defining and designing the operational, staffing, and technology resources. The chapter on data governance technology is also built into this chapter from the prior edition.}
}
@article{GUO2020125094,
title = {Mining commuting behavior of urban rail transit network by using association rules},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {559},
pages = {125094},
year = {2020},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2020.125094},
url = {https://www.sciencedirect.com/science/article/pii/S0378437120305732},
author = {Xin Guo and David Z.W. Wang and Jianjun Wu and Huijun Sun and Li Zhou},
keywords = {Urban rail transit networks, Commuting behavior, Association rules, Transfer coordination, Data mining},
abstract = {Automated Fare Collection (AFC) systems in rail transit services collect enormous amounts of detailed data on on-board transactions. A better understanding of travelers’ commuting and transfer behavior based on those massive volumes of AFC data would enable the rail service operators to evaluate their service quality and optimize operation strategies. This paper proposes an efficient and effective data mining procedure to figure out the association rules, aiming to extract connectivity and correlation of passenger flow among different services lines in urban rail transit networks. A case study based on the Beijing Subway network is conducted to demonstrate the applicability of the proposed method. Using up to 28 million AFC smart card transaction data, we match and analyze travelers’ trip chains to investigate the commuting trip patterns in terms of spatio-temporal distribution characteristics. An innovational non-nigh-to-five commuting behavior and traditional nine-to-five commuting behavior are divided by the obtained associated rules to ensure a more nuanced description of commuting behaviors. Further, the results indicated by stronger association rules (2-frequent itemset and 3-frequent itemset) also provide a better understanding of transfer behaviors, like the frequent transfers among different service lines, and potentially vulnerable stations in the network. The research outcomes can be used to assist rail transit service operators in developing optimal operation strategies like timetabling design to enhance the transfer performance between different rail lines.}
}
@article{LEONG2020122870,
title = {Enhancing the adaptability: Lean and green strategy towards the Industry Revolution 4.0},
journal = {Journal of Cleaner Production},
volume = {273},
pages = {122870},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122870},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620329152},
author = {Wei Dong Leong and Sin Yong Teng and Bing Shen How and Sue Lin Ngan and Anas Abd Rahman and Chee Pin Tan and S.G. Ponnambalam and Hon Loong Lam},
keywords = {Lean and green manufacturing, Lean and green index, Machine learning, Process optimization, Adaptive model, Industry revolution 4.0},
abstract = {Industry 4.0 has brought forth many advantages and challenges for the industry players. Many organizations are strategizing to take advantage of this industrial paradigm shift, thus improving the sustainability of the enterprise. However, there are many factors such as talent development, machinery advancement and infrastructure development which involve huge investment that need to be considered. This paper presents an enhanced adaptive model for the implementation of the lean and green (L&G) strategy in processing sectors to solve dynamic industry problems associated with Industry 4.0. A feature of this enhanced adaptive model is that it combines experts’ experience and operational data as input in dealing with real industry application. A lean and green index is coupled in the model to serve as a benchmark and process improvement tracking indicator. This allows the industrialists to set a lean and green index (LGI) target for effective process improvement. From this integrated model, an ensemble of backpropagation optimizers is then used to identify the best-optimized strategy. This ensemble optimizer is formulated to perform operation improvement and update the targeted LGI automatically when a higher index is achieved for continuous improvement. A case study on a combined heat and power plant is performed and reflects an improvement of 18.25% on the LGI. This work serves as a practical transition strategy for the industrialist desiring to improve the sustainability of the facility with Industry 4.0 elements at minimum investment cost.}
}
@article{BARBOSAPOVOA2020106606,
title = {Process supply chains: Perspectives from academia and industry},
journal = {Computers & Chemical Engineering},
volume = {132},
pages = {106606},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.106606},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419301899},
author = {Ana Paula Barbosa-Povoa and José Mauricio Pinto},
keywords = {Process supply chains, Industrial gases, Optimization, Sustainability, Uncertainty, Multiscale, Challenges, Perspectives},
abstract = {Process systems engineering (PSE) has been an active research area for nearly seventy years and addresses multiple systems from the process industry. Among these are Process Supply Chains that can be described as interconnected sets of entities responsible for the sourcing, production and distribution of a large set of chemical and/or bio- based products. Due to the high diversity of materials, processes and information flows such networks result in highly complex systems that are very difficult to manage. The PSE community has a critical role to support the design and management of such systems through the development of tools that are able to address such complexity. Focusing initially on a real-world process supply chain, the industrial gas supply chain, this paper identifies and discusses current contributions, challenges and perspectives in process supply chains that can guide research professionals to address such challenges. In general, such challenges encompass supply chain scope representations, modeling approaches, data management and implementation. Examples include supply chain risk and uncertainty, multiscale decisions, sustainability and resiliency.}
}
@article{COFFEY2020836,
title = {One center’s experience developing a burn outpatient registry},
journal = {Burns},
volume = {46},
number = {4},
pages = {836-841},
year = {2020},
issn = {0305-4179},
doi = {https://doi.org/10.1016/j.burns.2019.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S0305417919303535},
author = {Rebecca Coffey and Rachel Penny and Larry Jones and J. Kevin Bailey},
keywords = {Outpatient registry, Database outcomes},
abstract = {Introduction
Recent advances in burn care have resulted in the transition of care from inpatient to outpatient. There is a growing appreciation that with improved survival, meaningful markers of quality need to include recovery of form, function, and reconstruction. Capture of the data describing care delivered in the outpatient setting is being missed.
Methods
Development of our outpatient database included providers, registrar, program manager, and outpatient nursing staff. Data points were included if they described the population, and epidemiology of our patients, were useful for programmatic changes and improvements as well as anticipated research focus areas.
Results
The database platform chosen was Midas+™ because it was in use by hospital quality and integrated with the electronic medical record. Fields were customized based on changing program needs and are updated for new programs or outcomes measures. Reports can be easily built and both outpatients and inpatients are included. This allows for longitudinal tracking of burn patients. Ongoing additions to original data points include variables to track outcomes related to laser therapy for scar management, time to custom garment donning, and to track functional outcomes. Epidemiologic data collected is used to target high-risk populations for prevention and outreach efforts. Outcome data is used for evaluation of programs and care.
Conclusions
High quality databases serve to measure effectiveness of care and offer insight for areas of improvement. There is a clear need for inclusion of outpatient activity in the National Burn Registry (NBR).}
}
@article{MILLER2020470,
title = {Machine Intelligence for Management of Acute Coronary Syndromes: Neural or Nervous Times?},
journal = {Canadian Journal of Cardiology},
volume = {36},
number = {4},
pages = {470-473},
year = {2020},
issn = {0828-282X},
doi = {https://doi.org/10.1016/j.cjca.2019.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0828282X19312632},
author = {D. Douglas Miller}
}
@incollection{MCNITT2020261,
title = {Chapter 22 - GOES-R Series Data Access and Dissemination},
editor = {Steven J. Goodman and Timothy J. Schmit and Jaime Daniels and Robert J. Redmon},
booktitle = {The GOES-R Series},
publisher = {Elsevier},
pages = {261-271},
year = {2020},
isbn = {978-0-12-814327-8},
doi = {https://doi.org/10.1016/B978-0-12-814327-8.00022-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128143278000226},
author = {James McNitt and Kathryn Mozer and Donna McNamara and Graeme Martin},
keywords = {GOES-R Series, Direct broadcast, GOES Rebroadcast, High Rate Information Transmission, Emergency Managers Weather Information Network, Product Distribution and Access, Environmental Satellite Processing Center, Comprehensive Large Array-data Stewardship System, GEONETCast Americas, Satellite},
abstract = {The National Oceanic and Atmospheric Administration (NOAA) Geostationary Operational Environmental Satellites (GOES)-R Series systems, data centers, broadcasts, and data access portals serve a large and diverse user community. The GOES-R Series direct broadcast (DB) services are the GOES Rebroadcast (GRB) and High Rate Information Transmission (HRIT)/Emergency Managers Weather Information Network (EMWIN). The Product Distribution and Access (PDA) capability of the Environmental Satellite Processing Center (ESPC) distributes Level 1b products, Level 2+ products, and associated mission data to authorized operational customers. Non real-time users can access products through NOAA’s Comprehensive Large Array-data Stewardship System (CLASS). NOAA also disseminates GOES-R Series products through a commercial rebroadcast, called GEONETCast Americas (GNC-A). GNC-A users can install a receive station at a relatively low cost. Users can also download imagery from websites maintained by NOAA, National Aeronautics and Space Administration (NASA), NOAA Cooperative Institutes, universities, meteorological services, and other organizations.}
}
@article{SPENCE2020269,
title = {The Future Directions of Research in Cardiac Anesthesiology},
journal = {Advances in Anesthesia},
volume = {38},
pages = {269-282},
year = {2020},
note = {Advances in Anesthesia},
issn = {0737-6146},
doi = {https://doi.org/10.1016/j.aan.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0737614620300137},
author = {Jessica Spence and C. David Mazer},
keywords = {Cardiothoracic anesthesia, Research, Trial design, Future}
}
@article{KIM2020113302,
title = {Transparency and accountability in AI decision support: Explaining and visualizing convolutional neural networks for text information},
journal = {Decision Support Systems},
volume = {134},
pages = {113302},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113302},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620300579},
author = {Buomsoo Kim and Jinsoo Park and Jihae Suh},
keywords = {Convolutional neural network, Machine learning interpretability, Class activation mapping, Explainable artificial intelligence},
abstract = {Proliferating applications of deep learning, along with the prevalence of large-scale text datasets, have revolutionized the natural language processing (NLP) field, thereby driving the recent explosive growth. Nevertheless, it is argued that state-of-the-art studies focus excessively on producing quantitative performances superior to existing models, by playing “the Kaggle game.” Hence, the field requires more effort in solving new problems and proposing novel approaches and architectures. We claim that one of the promising and constructive efforts would be to design transparent and accountable artificial intelligence (AI) systems for text analytics. By doing so, we can enhance the applicability and problem-solving capacity of the system for real-world decision support. It is widely accepted that deep learning models demonstrate remarkable performances compared to existing algorithms. However, they are often criticized for being less interpretable, i.e., the “black box.” In such cases, users tend to hesitate to utilize them for decision-making, especially in crucial tasks. Such complexity obstructs transparency and accountability of the overall system, potentially debilitating the deployment of decision support systems powered by AI. Furthermore, recent regulations are emphasizing fairness and transparency in algorithms to a greater extent, turning explanations more compulsory than voluntary. Thus, to enhance the transparency and accountability of the decision support system and preserve the capacity to model complex text data at the same time, we propose the Explaining and Visualizing Convolutional neural networks for Text information (EVCT) framework. By adopting and ameliorating cutting-edge methods in NLP and image processing, the EVCT framework provides a human-interpretable solution to the problem of text classification while minimizing information loss. Experimental results with large-scale, real-world datasets show that EVCT performs comparably to benchmark models, including widely used deep learning models. In addition, we provide instances of human-interpretable and relevant visualized explanations obtained from applying EVCT to the dataset and possible applications for real-world decision support.}
}
@article{XU2020591,
title = {Searching for Genomic Biomarkers for Major Depressive Disorder in Peripheral Immune Cells},
journal = {Biological Psychiatry},
volume = {88},
number = {8},
pages = {591-593},
year = {2020},
note = {Depression: Novel Mechanisms and Personalized Treatments},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2020.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0006322320318072},
author = {Ke Xu and Bradley E. Aouizerat}
}
@article{RIESENER2020127,
title = {Applying Supervised and Reinforcement Learning to Design Product Portfolios in Accordance with Corporate Goals},
journal = {Procedia CIRP},
volume = {91},
pages = {127-133},
year = {2020},
note = {Enhancing design through the 4th Industrial Revolution Thinking},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.02.157},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307952},
author = {Michael Riesener and Christian Dölle and Christopher Dierkes and Merle-Hendrikje Jank},
keywords = {Product portfolio management, Neural networks, Corporate goals, Reinforcement learning},
abstract = {Faced with rapidly changing technologies, diminishing product life cycles and heightened global competition, product portfolio managers across all industries encounter increasing challenges within portfolio design processes. Aligning the product portfolio with corporate strategies is central to sustain long-term company success. However, regarding volatile environments, this is becoming increasingly challenging. In the last decades, product portfolio decisions were based on subjective experience, but this is no longer sufficient. Nowadays, as portfolio complexity grows constantly, data-based decision support procedures are needed to enable effective decisions in product portfolio management. Regarding the field of portfolio management, only little research has been conducted on the usage of data-based analytical methods. Additionally, the alignment of product portfolios with corporate strategies is still largely unexplored and, in this context, the application of analytical methods has been largely omitted until now. This paper proposes a methodology that uses neural networks with supervised learning to model correlations among product portfolio control parameters and corporate goal indicators. Based on this, reinforcement learning is applied to derive goal-conform recommendations for product portfolio managers. For both supervised and reinforcement learning, the presented methodology includes generic steps for implementation. Moreover, for both machine learning methods, requirements regarding necessary product portfolio data are elaborated. The methodology is validated using a case study.}
}
@article{DOBSON2020455,
title = {Making Messy Data Work for Conservation},
journal = {One Earth},
volume = {2},
number = {5},
pages = {455-465},
year = {2020},
issn = {2590-3322},
doi = {https://doi.org/10.1016/j.oneear.2020.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2590332220301998},
author = {A.D.M. Dobson and E.J. Milner-Gulland and Nicholas J. Aebischer and Colin M. Beale and Robert Brozovic and Peter Coals and Rob Critchlow and Anthony Dancer and Michelle Greve and Amy Hinsley and Harriet Ibbett and Alison Johnston and Timothy Kuiper and Steven {Le Comber} and Simon P. Mahood and Jennifer F. Moore and Erlend B. Nilsen and Michael J.O. Pocock and Anthony Quinn and Henry Travers and Paulo Wilfred and Joss Wright and Aidan Keane},
keywords = {unstructured observational data, bias, citizen science, observation process, volunteer data, crowd sensing},
abstract = {Conservationists increasingly use unstructured observational data, such as citizen science records or ranger patrol observations, to guide decision making. These datasets are often large and relatively cheap to collect, and they have enormous potential. However, the resulting data are generally “messy,” and their use can incur considerable costs, some of which are hidden. We present an overview of the opportunities and limitations associated with messy data by explaining how the preferences, skills, and incentives of data collectors affect the quality of the information they contain and the investment required to unlock their potential. Drawing widely from across the sciences, we break down elements of the observation process in order to highlight likely sources of bias and error while emphasizing the importance of cross-disciplinary collaboration. We propose a framework for appraising messy data to guide those engaging with these types of dataset and make them work for conservation and broader sustainability applications.}
}
@incollection{LADLEY20207,
title = {Chapter 2 - Introduction},
editor = {John Ladley},
booktitle = {Data Governance (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {7-13},
year = {2020},
isbn = {978-0-12-815831-9},
doi = {https://doi.org/10.1016/B978-0-12-815831-9.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158319000023},
author = {John Ladley},
keywords = {Concepts, Data literacy, Terminology},
abstract = {This chapter introduces the book; what is new from the first edition, what new ideas are presented and how the reader should approach the material.}
}
@article{WANG2020123365,
title = {An active preventive maintenance approach of complex equipment based on a novel product-service system operation mode},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123365},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123365},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620334107},
author = {Ning Wang and Shan Ren and Yang Liu and Miying Yang and Jin Wang and Donald Huisingh},
keywords = {Product-service system, Ethical, Sustainable, Preventive maintenance, Sharing, Remaining effective life},
abstract = {The product-service system (PSS) business model has received increasing attention in equipment maintenance studies, as it has the potential to provide high value-added services for equipment users and construct ethical principles for equipment providers to support the implementation of circular economy. However, the PSS providers in equipment industry are facing many challenges when implementing Industry 4.0 technologies. One important challenge is how to fully collect and analyse the operational data of different equipment and diverse users in widely varied conditions to make the PSS providers create innovative equipment management services for their customers. To address this challenge, an active preventive maintenance approach for complex equipment is proposed. Firstly, a novel PSS operation mode was developed, where complex equipment is offered as a part of PSS and under exclusive control by the providers. Then, a solution of equipment preventive maintenance based on the operation mode was designed. A deep neural network was trained to predict the remaining effective life of the key components and thereby, it can pre-emptively assess the health status of equipment. Finally, a real-world industrial case of a leading CNC machine provider was developed to illustrate the feasibility and effectiveness of the proposed approach. Higher accuracy for predicting the remaining effective life was achieved, which resulted in predictive identification of the fault features, proactive implementation of the preventive maintenance, and reduction of the PSS providers’ maintenance costs and resource consumption. Consequently, the result shows that it can help PSS providers move towards more ethical and sustainable directions.}
}
@article{XU2020104,
title = {Real-time road traffic state prediction based on kernel-KNN},
journal = {Transportmetrica A Transport Science},
volume = {16},
number = {1},
pages = {104-118},
year = {2020},
issn = {2324-9935},
doi = {https://doi.org/10.1080/23249935.2018.1491073},
url = {https://www.sciencedirect.com/science/article/pii/S2324993522001737},
author = {Dongwei Xu and Yongdong Wang and Peng Peng and Shen Beilun and Zhang Deng and Haifeng Guo},
keywords = {Road traffic, kernel-KNN, kernel function, state prediction},
abstract = {ABSTRACT
The real-time and accurate prediction of road traffic states is the basis of information service for road traffic participants. An algorithm based on kernel K-nearest neighbors (kernel-KNN) is presented to predict road traffic states in time series in this paper. First, representative road traffic state data are extracted to build the road traffic running characteristics reference sequences. Then, kernel function of the road traffic state data sequence in time series is constructed. The current and referenced road traffic state data sequences are matched, based on which k nearest referenced road traffic states are selected and the road traffic states are predicted. Several typical road links in Beijing are considered for a series of case studies. The final experiments results prove that the road traffic states prediction approach based on kernel-KNN presented herein is feasible and can achieve a high level of accuracy.}
}
@incollection{RAJU2020141,
title = {Chapter 10 - Review of Intellectual Video Surveillance Through Internet of Things},
editor = {Dinesh Peter and Amir H. Alavi and Bahman Javadi and Steven L. Fernandes},
booktitle = {The Cognitive Approach in Cloud Computing and Internet of Things Technologies for Surveillance Tracking Systems},
publisher = {Academic Press},
pages = {141-155},
year = {2020},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-816385-6},
doi = {https://doi.org/10.1016/B978-0-12-816385-6.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163856000106},
author = {Preethi Sambandam Raju and Murugan Mahalingam and Revathi Arumugam Rajendran},
keywords = {Video surveillance, Internet of Things, front-end intelligence, edge computing},
abstract = {Surveillance is a vital utility in all places to enable the provision of a secure environment for the well-being of living folks and can be effectively achieved with the help of a video. Industries, along with researchers, are endeavoring hard to overcome the difficulties seen in the implementation of a mature Internet of Things (IoT) framework for a video surveillance application. Video surveillance and the IoT are converged for ensuring compactness for the nodes of video surveillance, ease of time, and high efficiency, and for providing video surveillance over Internet of Things (VS-IoT). This chapter highlights the analysis of VS-IoT and articulates the important stages of enactment. Added to that, it also depicts the contemporary technologies and challenges present in each stage, so that it could lead the research of successful intelligent video surveillance enactment.}
}
@article{HULLS2020109971,
title = {DNA methylation signature of passive smoke exposure is less pronounced than active smoking: The Understanding Society study},
journal = {Environmental Research},
volume = {190},
pages = {109971},
year = {2020},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2020.109971},
url = {https://www.sciencedirect.com/science/article/pii/S0013935120308665},
author = {Paige M. Hulls and Frank {de Vocht} and Yanchun Bao and Caroline L. Relton and Richard M. Martin and Rebecca C. Richmond},
keywords = {DNA methylation, EWAS, Understanding society, Passive smoke exposure},
abstract = {Introduction
The extent of the biological impact of passive smoke exposure is unclear. We sought to investigate the association between passive smoke exposure and DNA methylation, which could serve as a biomarker of health risk.
Materials and methods
We derived passive smoke exposure from self-reported questionnaire data among smoking and non-smoking partners of participants enrolled in the UK Household Longitudinal Study ‘Understanding Society’ (n=769). We performed an epigenome-wide association study (EWAS) of passive smoke exposure with DNA methylation in peripheral blood measured using the Illumina Infinium Methylation EPIC array.
Results
No CpG sites surpassed the epigenome-wide significance threshold of p<5.97 × 10−8 in relation to partner smoking, compared with 10 CpG sites identified in relation to own smoking. However, 10 CpG sites surpassed a less stringent threshold of p<1 × 10−5 in a model of partner smoking adjusted for own smoking (model 1), 7 CpG sites in a model of partner smoking restricted to non-smokers (model 2) and 16 CpGs in a model restricted to regular smokers (model 3). In addition, there was evidence for an interaction between own smoking status and partners’ smoking status on DNA methylation levels at the majority of CpG sites identified in models 2 and 3. There was a clear lack of enrichment for previously identified smoking signals in the EWAS of passive smoke exposure compared with the EWAS of own smoking.
Conclusion
The DNA methylation signature associated with passive smoke exposure is much less pronounced than that of own smoking, with no positive findings for ‘expected’ signals. It is unlikely that changes to DNA methylation serve as an important mechanism underlying the health risks of passive smoke exposure.}
}
@article{SCHAFER20201311,
title = {Smart Use Case Picking with DUCAR: A Hands-On Approach for a Successful Integration of Machine Learning in Production Processes},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1311-1318},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.183},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320400},
author = {Franziska Schäfer and Andreas Mayr and Erik Schwulera and Jörg Franke},
keywords = {machine learning, use case, process improvement, production, manufacturing},
abstract = {The increasing data availability in production processes combined with open source software tools in the field of machine learning (ML) lead to a new level of process improvement possibilities. For many companies, the challenge is now to identify potential use cases with considerable business impact by integrating ML techniques in production processes. Especially hands-on approaches, which provide a structured guideline through the process with concrete tools, are missing. Therefore, this paper presents the smart use case picking approach DUCAR, which consists of five phases. The first phase is the definition of the ML use case domain followed by the understanding of potential processes and process steps in the second phase. In the third phase, use case ideas are collected based on the gained knowledge. After the analysis and selection of the most promising use cases in the fourth phase, the final fifth phase focuses the realization of these use cases to successfully integrate ML in production processes. To validate the DUCAR approach, the field of electronics production is used as an example.}
}
@article{ZHAO2020226,
title = {Intelligent city intelligent medical sharing technology based on internet of things technology},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {226-233},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20300170},
author = {Xin Zhao and Wei Xiao and Lu Wu and Zhigang Zhao and Jidong Huo and Shi Wang and Zhenhua Guo and Dianmin Sun},
keywords = {Internet of things, Intelligent city, Intelligent medical sharing, Data encryption, Information security},
abstract = {Because of the imperfect domestic public medical management system, high medical costs, less channels and other problems troubling people’s livelihood. In particular, medical problems represented by ”inefficient medical system, poor quality of medical services, medical difficulties and high costs” are the focus of social concern. Therefore, the purpose of this paper is to establish a set of intelligent medical information network platform system, so that patients can enjoy safe, convenient and high-quality diagnosis and treatment services on the basis of short waiting time and basic medical expenses. Fundamentally solve the problem of ”difficult and expensive medical treatment”, and truly achieve ”everyone’s health, everyone’s health”. This paper designs and develops a medical distributed data sharing and integration system based on distributed heterogeneous data sets of medical data, and applies it to some local first-class hospitals. Finally, after questionnaire and interview survey, it is concluded that compared with the previous medical treatment, the sharing of intelligent medicine among the general public has increased by about 79%, and the medical environment has been greatly improved.}
}
@article{DUSHKOVA2020101096,
title = {Methodology for development of a data and knowledge base for learning from existing nature-based solutions in Europe: The CONNECTING Nature project},
journal = {MethodsX},
volume = {7},
pages = {101096},
year = {2020},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2020.101096},
url = {https://www.sciencedirect.com/science/article/pii/S2215016120303162},
author = {Diana Dushkova and Dagmar Haase},
keywords = {Nature-based solutions (NBS), Data- and knowledge base, Climate change, Societal challenges, Sustainability, Resilience, Urban Europe},
abstract = {Within CONNECTING Nature, we are dealing with developing innovative nature-based solutions (NBS) for climate change adaptation, health and well-being, social cohesion and sustainable economic development in European cities. In order to enable “learning by comparing” and “generating new knowledge” from multiple NBS related studies, a novel data and knowledge base is needed which requires a specified methodological approach for its development. This paper provides conceptual and methodological context and techniques for constructing such a data and knowledge base that will systematically support the process of NBS monitoring and assessment:•A methodology presents the comprehensive, multi-step approach to the NBS data and knowledge development that helps to guide work and influence the quality of an information included.•The paper describes the methodology and main steps/phases for developing a large data and knowledge base of NBS that will allow further systematic review.•The suggested methodology explains how to build NBS related databases from the conceptualization and requirements phases through to implementation and maintenance. In this regard, such a methodology is iterative, with extensive NBS stakeholders’ and end-user's involvement that are packaged with reusable templates or deliverables offering a good opportunity for success when used by practitioners and other end-users.•The NBS data and knowledge base gathers information about different NBS models and generations into one easy-to-find, easy-to-use place and provides detailed descriptions of each of the 1490 NBS cases from urban centers in Europe.•The data and knowledge base thus helps users identify the best and most appropriated NBS model/type for addressing the particular goals and, at the same time, considers the local context and potential.•The data obtained can be used for the further meta-analysis by applying statistics or searching for specific sample cases and thus enables to generate and expand the knowledge from multiple NBS related studies, in both qualitative and quantitative ways.}
}