@article{CHEN201798,
title = {Data quality of electricity consumption data in a smart grid environment},
journal = {Renewable and Sustainable Energy Reviews},
volume = {75},
pages = {98-105},
year = {2017},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2016.10.054},
url = {https://www.sciencedirect.com/science/article/pii/S1364032116307109},
author = {Wen Chen and Kaile Zhou and Shanlin Yang and Cheng Wu},
keywords = {Electricity consumption data, Data quality, Outlier detection, Outlier data, Smart grid},
abstract = {With the increasing penetration of traditional and emerging information technologies in the electric power industry, together with the rapid development of electricity market reform, the electric power industry has accumulated a large amount of data. Data quality issues have become increasingly prominent, which affect the accuracy and effectiveness of electricity data mining and energy big data analytics. It is also closely related to the safety and reliability of the power system operation and management based on data-driven decision support. In this paper, we study the data quality of electricity consumption data in a smart grid environment. First, we analyze the significance of data quality. Also, the definition and classification of data quality issues are explained. Then we analyze the data quality of electricity consumption data and introduce the characteristics of electricity consumption data in a smart grid environment. The data quality issues of electricity consumption data are divided into three types, namely noise data, incomplete data and outlier data. We make a detailed discussion on these three types of data quality issues. In view of that outlier data is one of the most prominent issues in electricity consumption data, so we mainly focus on the outlier detection of electricity consumption data. This paper introduces the causes of electricity consumption outlier data and illustrates the significance of the electricity consumption outlier data from the negative and positive aspects respectively. Finally, the focus of this paper is to provide a review on the detection methods of electricity consumption outlier data. The methods are mainly divided into two categories, namely the data mining-based and the state estimation-based methods.}
}
@article{GARCIABERNARDO2018164,
title = {The effects of data quality on the analysis of corporate board interlock networks},
journal = {Information Systems},
volume = {78},
pages = {164-172},
year = {2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917302272},
author = {Javier Garcia-Bernardo and Frank W. Takes},
abstract = {Nowadays, social network data of ever increasing size is gathered, stored and analyzed by researchers from a range of disciplines. This data is often automatically gathered from API’s, websites or existing databases. As a result, the quality of this data is typically not manually validated, and the resulting social networks may be based on false, biased or incomplete data. In this paper, we investigate the effect of data quality issues on the analysis of large networks. We focus on the global board interlock network, in which nodes represent firms across the globe, and edges model social ties between firms – shared board members holding a position at both firms. First, we demonstrate how we can automatically assess the completeness of a large dataset of 160 million firms, in which data is missing not at random. Second, we present a novel method to increase the accuracy of the entries in our data. By comparing the expected and empirical characteristics of the resulting network topology, we develop a technique that automatically prunes and merges duplicate nodes and edges. Third, we use a case study of the board interlock network of Sweden to show how poor quality data results in distorted network topologies, incorrect community division, biased centrality values and abnormal influence spread under a well-known diffusion model. Finally, we demonstrate how the proposed data quality assessment methods help restore the network structure, ultimately allowing us to derive meaningful and correct results from the analysis of the network.}
}
@article{ZHAO20171085,
title = {An optimization model for green supply chain management by using a big data analytic approach},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {1085-1097},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616300579},
author = {Rui Zhao and Yiyun Liu and Ning Zhang and Tao Huang},
keywords = {Hazardous materials, Inherent risk, Carbon emissions, Multi-objective optimization, Green supply chain management, Big data analysis},
abstract = {This paper presents a multi-objective optimization model for a green supply chain management scheme that minimizes the inherent risk occurred by hazardous materials, associated carbon emission and economic cost. The model related parameters are capitalized on a big data analysis. Three scenarios are proposed to improve green supply chain management. The first scenario divides optimization into three options: the first involves minimizing risk and then dealing with carbon emissions (and thus economic cost); the second minimizes both risk and carbon emissions first, with the ultimate goal of minimizing overall cost; and the third option attempts to minimize risk, carbon emissions, and economic cost simultaneously. This paper provides a case study to verify the optimization model. Finally, the limitations of this research and approach are discussed to lay a foundation for further improvement.}
}
@article{SIVARAJAH2017263,
title = {Critical analysis of Big Data challenges and analytical methods},
journal = {Journal of Business Research},
volume = {70},
pages = {263-286},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S014829631630488X},
author = {Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody},
keywords = {Big Data, Big Data Analytics, Challenges, Methods, Systematic literature review},
abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.}
}
@incollection{BERMAN20181,
title = {1 - Introduction},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {1-13},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000017},
author = {Jules J. Berman},
keywords = {Big data definition, Small data, Data filtering, Data reduction},
abstract = {Big Data is not synonymous with lots and lots of data. Useful Big Data resources adhere to a set of data management principles that are fundamentally different from the traditional practices followed for small data projects. The areas of difference include: data collection; data annotation (including metadata and identifiers); location and distribution of stored data; classification of data; data access rules; data curation; data immutability; data permanence; verification and validity methods for the contained data; analytic methods; costs; and incumbent legal, social, and ethical issues. Skilled professionals who are adept in the design and management of small data resources may be unprepared for the unique challenges posed by Big Data. This chapter is an introduction to topics that will be fully explained in later chapters.}
}
@article{ZHANG2018149,
title = {Product features characterization and customers’ preferences prediction based on purchasing data},
journal = {CIRP Annals},
volume = {67},
number = {1},
pages = {149-152},
year = {2018},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2018.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0007850618300441},
author = {Jian Zhang and Alessandro Simeone and Peihua Gu and Bo Hong},
keywords = {Design, Product development, Big data},
abstract = {Big data of online product purchases is an emerging source for obtaining customers’ preferences of product features for new product development. This paper proposes a framework and associated method for product features characterization and customers’ preference prediction based on online product purchase data. Specifications and components of products are firstly analyzed and the relationships between product specifications and components are then established for features characterization. The customers preferred specifications, features and their combinations are predicted for development of new products. The features characterization and customers’ preferences prediction of toy cars were used as an example of illustrating the proposed method.}
}
@article{MOON2018304,
title = {Evaluating fidelity of lossy compression on spatiotemporal data from an IoT enabled smart farm},
journal = {Computers and Electronics in Agriculture},
volume = {154},
pages = {304-313},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918303697},
author = {Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son},
keywords = {Smart farm, Lossy compression, IoT, Signal processing, Data fidelity},
abstract = {As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.}
}
@article{KAMPKER2018120,
title = {Enabling Data Analytics in Large Scale Manufacturing},
journal = {Procedia Manufacturing},
volume = {24},
pages = {120-127},
year = {2018},
note = {4th International Conference on System-Integrated Intelligence: Intelligent, Flexible and Connected Systems in Products and Production},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918305341},
author = {Achim Kampker and Heiner Heimes and Ulrich Bührer and Christoph Lienemann and Stefan Krotil},
keywords = {Automotive, Manufacturing, Data Analytics, Big Data, Optimization},
abstract = {Companies of the manufacturing industry face increasing process complexity. To remain competitive, increasing the knowledge concerning innovative manufacturing processes is necessary. In other areas, data analytics methods have been successfully applied for this purpose. Currently, their application in large scale manufacturing is hampered by insufficient data availability. Therefore, this study presents a solution approach that enables adaptive data availability by establishing a data-use-case-matrix (DUCM), which allows use case prioritization to support dimensioning of control systems and IT infrastructures. In order to support technology development, further proposed is a scalable implementation of the prioritized use cases starting in early prototyping phases.}
}
@article{PUTHAL201722,
title = {A dynamic prime number based efficient security mechanism for big sensing data streams},
journal = {Journal of Computer and System Sciences},
volume = {83},
number = {1},
pages = {22-42},
year = {2017},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022000016000209},
author = {Deepak Puthal and Surya Nepal and Rajiv Ranjan and Jinjun Chen},
keywords = {Security, Sensor networks, Big data stream, Key exchange, Security verification},
abstract = {Big data streaming has become an important paradigm for real-time processing of massive continuous data flows in large scale sensing networks. While dealing with big sensing data streams, a Data Stream Manager (DSM) must always verify the security (i.e. authenticity, integrity, and confidentiality) to ensure end-to-end security and maintain data quality. Existing technologies are not suitable, because real time introduces delay in data stream. In this paper, we propose a Dynamic Prime Number Based Security Verification (DPBSV) scheme for big data streams. Our scheme is based on a common shared key that updated dynamically by generating synchronized prime numbers. The common shared key updates at both ends, i.e., source sensing devices and DSM, without further communication after handshaking. Theoretical analyses and experimental results of our DPBSV scheme show that it can significantly improve the efficiency of verification process by reducing the time and utilizing a smaller buffer size in DSM.}
}
@article{DETRE2018541,
title = {Handling veracity in multi-criteria decision-making: A multi-dimensional approach},
journal = {Information Sciences},
volume = {460-461},
pages = {541-554},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517309337},
author = {Guy {De Tré} and Robin {De Mol} and Antoon Bronselaer},
keywords = {Multi-criteria decision-making, Veracity in ‘big’ data, Data quality assessment, Data quality handling},
abstract = {Decision support systems aim to help a decision maker with selecting the option from a set of available options that best meets her or his needs. In multi-criteria based decision support approaches, a suitability degree is computed for each option, reflecting how suitable that option is considering the preferences of the decision maker. Nowadays, it becomes more and more common that data of different quality, originating from different data sets and different data providers have to be integrated and processed in order to compute the suitability degrees. Also, data sets can be very large such that their data become commonly prone to incompleteness, imprecision and uncertainty. Hence, not all data used for decision making can be trusted to the same extent and consequently, neither the results of computations with such data can be trusted to the same extent. For this reason, data quality assessment becomes an important aspect of a decision making process. To correctly inform the users, it is essential to communicate not only the computed suitability degrees of the available options, but also the confidence about these suitability degrees as can be derived from data quality assessment. In this paper a novel multi-dimensional approach for data quality assessment in multi-criteria decision making, supporting the computation of associated confidence degrees, is presented. Providing confidence information adds an extra dimension to the decision making process and leads to more soundly decisions. The added value of the approach is illustrated with aspects of a geographic decision making process.}
}
@incollection{FIESCHI2018197,
title = {16 - Data for Epidemiology and Public Health, and Big Data11The questions posed by data processing for epidemiology and public health are often similar to those discussed in the chapters on clinical research (Chapter 18) and bioinformatics data (Chapter 17). For the sake of clarity, we address these questions in different chapters, although the problems are of the same nature and the solutions are isomorphic. In order to avoid too much repetition, the issue of big data is discussed here without going into the content of the other chapters.},
editor = {Marius Fieschi},
booktitle = {Health Data Processing},
publisher = {Elsevier},
pages = {197-212},
year = {2018},
isbn = {978-1-78548-287-8},
doi = {https://doi.org/10.1016/B978-1-78548-287-8.50016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978178548287850016X},
author = {Marius Fieschi},
keywords = {Data processing, Data-sharing, e-health, Epidemiology, Health security, Monitoring systems, Preventive action, Public health, SurSaUD system},
abstract = {Abstract:
The approaches used by epidemiologists are diverse: they range from “field studies” for modeling and healthcare monitoring, to methods developed for researching and combating the emergence of diseases. Their analytical tools focus on the bio-statistics used as a tool to objectify phenomena studied in well-defined populations.}
}
@incollection{DEMCHENKO201721,
title = {Chapter 2 - Cloud Computing Infrastructure for Data Intensive Applications},
editor = {Hui-Huang Hsu and Chuan-Yu Chang and Ching-Hsien Hsu},
booktitle = {Big Data Analytics for Sensor-Network Collected Intelligence},
publisher = {Academic Press},
pages = {21-62},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-809393-1},
doi = {https://doi.org/10.1016/B978-0-12-809393-1.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093931000027},
author = {Yuri Demchenko and Fatih Turkmen and Cees {de Laat} and Ching-Hsien Hsu and Christophe Blanchet and Charles Loomis},
keywords = {Big Data, Big Data Architecture Framework (BDAF), Big data infrastructure, NIST Big Data Architecture (BDRA), Cloud computing, Intercloud Architecture Framework (ICAF), Cloud powered design, SlipStream cloud automation platform},
abstract = {This chapter describes the general architecture and functional components of the cloud-based big data infrastructure (BDI). The chapter starts with the analysis of emerging Big Data and data intensive technologies and provides the general definition of the Big Data Architecture Framework (BDAF) that includes the following components: Big Data definition, Data Management including data lifecycle and data structures, generically cloud based BDI, Data Analytics technologies and platforms, and Big Data security, compliance, and privacy. The chapter refers to NIST Big Data Reference Architecture (BDRA) and summarizes general requirements to Big Data systems described in NIST documents. The proposed BDI and its cloud-based components are defined in accordance with the NIST BDRA and BDAF. This chapter provides detailed analysis of the two bioinformatics use cases as typical example of the Big Data applications that have being developed by the authors in the framework of the CYCLONE project. The effective use of cloud for bioinformatics applications requires maximum automation of the applications deployment and management that may include resources from multiple clouds and providers. The proposed CYCLONE platform for multicloud multiprovider applications deployment and management is based on the SlipStream cloud automation platform and includes all necessary components to build and operate complex scientific applications. The chapter discusses existing platforms for cloud powered applications development and deployment automation, in particularly referring to the SlipStream cloud automation platform, which allows multicloud applications deployment and management. The chapter also includes a short overview of the existing Big Data platforms and services provided by the major cloud services providers which can be used for fast deployment of customer Big Data applications using the benefits of cloud technologies and global cloud infrastructure.}
}
@article{LIN2018220,
title = {An on-demand coverage based self-deployment algorithm for big data perception in mobile sensing networks},
journal = {Future Generation Computer Systems},
volume = {82},
pages = {220-234},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17313262},
author = {Yaguang Lin and Xiaoming Wang and Fei Hao and Liang Wang and Lichen Zhang and Ruonan Zhao},
keywords = {Mobile sensing network, High performance sensing, Big data perception, Node self-deployment, On-demand coverage, Mobile cellular learning automata},
abstract = {Mobile Sensing Networks have been widely applied to many fields for big data perception such as intelligent transportation, medical health and environment sensing. However, in some complex environments and unreachable regions of inconvenience for human, the establishment of the mobile sensing networks, the layout of the nodes and the control of the network topology to achieve high performance sensing of big data are increasingly becoming a main issue in the applications of the mobile sensing networks. To deal with this problem, we propose a novel on-demand coverage based self-deployment algorithm for big data perception based on mobile sensing networks in this paper. Firstly, by considering characteristics of mobile sensing nodes, we extend the cellular automata model and propose a new mobile cellular automata model for effectively characterizing the spatial–temporal evolutionary process of nodes. Secondly, based on the learning automata theory and the historical information of node movement, we further explore a new mobile cellular learning automata model, in which nodes can self-adaptively and intelligently decide the best direction of movement with low energy consumption. Finally, we propose a new optimization algorithm which can quickly solve the node self-adaptive deployment problem, thus, we derive the best deployment scheme of nodes in a short time. The extensive simulation results show that the proposed algorithm in this paper outperforms the existing algorithms by as much as 40% in terms of the degree of satisfaction of network coverage, the iterations of the algorithm, the average moving steps of nodes and the energy consumption of nodes. Hence, we believe that our work will make contributions to large-scale adaptive deployment and high performance sensing scenarios of the mobile sensing networks.}
}
@article{KARAFILI201852,
title = {An argumentation reasoning approach for data processing},
journal = {Computers in Industry},
volume = {94},
pages = {52-61},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S016636151730338X},
author = {Erisa Karafili and Konstantina Spanaki and Emil C. Lupu},
keywords = {Data processing, Data quality, Usage control, Argumentation reasoning, Data manufacturing, Case scenarios},
abstract = {Data-intensive environments enable us to capture information and knowledge about the physical surroundings, to optimise our resources, enjoy personalised services and gain unprecedented insights into our lives. However, to obtain these endeavours extracted from the data, this data should be generated, collected and the insight should be exploited. Following an argumentation reasoning approach for data processing and building on the theoretical background of data management, we highlight the importance of data sharing agreements (DSAs) and quality attributes for the proposed data processing mechanism. The proposed approach is taking into account the DSAs and usage policies as well as the quality attributes of the data, which were previously neglected compared to existing methods in the data processing and management field. Previous research provided techniques towards this direction; however, a more intensive research approach for processing techniques should be introduced for the future to enhance the value creation from the data and new strategies should be formed around this data generated daily from various devices and sources.}
}
@article{BJORNSDOTTIR20181195,
title = {Exhibiting caution with use of big data: The case of amphetamine in Iceland's prescription registry},
journal = {Research in Social and Administrative Pharmacy},
volume = {14},
number = {12},
pages = {1195-1202},
year = {2018},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S155174111830127X},
author = {Ingunn Björnsdottir and Guri Birgitte Verne},
abstract = {Background
Data from large electronic databases are increasingly used in epidemiological research, but golden standards for database validation remain elusive. The Prescription Registry (IPR) and the National Health Service (NHS) databases in Iceland have not undergone formal validation, and gross errors have repeatedly been found in Icelandic statistics on pharmaceuticals. In 2015, new amphetamine tablets entered the Icelandic market, but were withdrawn half a year later due to being substandard. Return of unused stocks provided knowledge of the exact number of tablets used and hence a case where quality of the data could be assessed.
Objective
A case study of the quality of statistics in a national database on pharmaceuticals.
Methods
Data on the sales of the substandard amphetamine were obtained from the Prescription Registry and the pharmaceuticals statistics database. Upon the revelation of discrepancies, explanations were sought from the respective institutions, the producer, and dose dispensing companies.
Results
The substandard amphetamine was available from 1.9.2015 until 15.3.2016. According to NHS, 73990 tablets were sold to consumers in that period, whereas IPR initially stated 82860 tablets to have been sold, correcting to 74796 upon being notified about errors. The producer stated 72811 tablets to have been sold, and agreed with the dose dispensing companies on sales to those. The producer’s numbers were confirmed by the Medicines Agency.
Conclusion
Over-registration in the IPR was 13.8% before correction, 2.7% after correction, and 1.6% in the NHS. This case provided a unique opportunity for external validation of sales data for pharmaceuticals in Iceland, revealing enormous quality problems. The case has implications regarding database integrity beyond Iceland.}
}
@article{TAGLIAFERRI201873,
title = {A new standardized data collection system for interdisciplinary thyroid cancer management: Thyroid COBRA},
journal = {European Journal of Internal Medicine},
volume = {53},
pages = {73-78},
year = {2018},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0953620518300621},
author = {Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni},
keywords = {Big data, Data pooling, Personalized medicine, Radiotherapy, Thyroid, Cancer management},
abstract = {The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow “Cloud storage way” or “distributed learning” approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.}
}
@article{MAHDAVINEJAD2018161,
title = {Machine learning for internet of things data analysis: a survey},
journal = {Digital Communications and Networks},
volume = {4},
number = {3},
pages = {161-175},
year = {2018},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S235286481730247X},
author = {Mohammad Saeid Mahdavinejad and Mohammadreza Rezvan and Mohammadamin Barekatain and Peyman Adibi and Payam Barnaghi and Amit P. Sheth},
keywords = {Machine learning, Internet of Things, Smart data, Smart City},
abstract = {Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.}
}
@article{MAIER201797,
title = {Big data in large-scale systemic mouse phenotyping},
journal = {Current Opinion in Systems Biology},
volume = {4},
pages = {97-104},
year = {2017},
note = {Big data acquisition and analysis • Pharmacology and drug discovery},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S2452310017300525},
author = {Holger Maier and Stefanie Leuchtenberger and Helmut Fuchs and Valerie Gailus-Durner and Martin {Hrabe de Angelis}},
abstract = {Systemic phenotyping of mutant mice has been established at large scale in the last decade as a new tool to uncover the relations between genotype, phenotype and environment. Recent advances in that field led to the generation of a valuable open access data resource that can be used to better understanding the underlying causes for human diseases. From an ethical perspective, systemic phenotyping significantly contributes to the reduction of experimental animals and the refinement of animal experiments by enforcing standardisation efforts. There are particular logistical, experimental and analytical challenges of systemic large-scale mouse phenotyping. On all levels, IT solutions are critical to implement and efficiently support breeding, phenotyping and data analysis processes that lead to the generation of high-quality systemic phenotyping data accessible for the scientific community.}
}
@article{ABBASIAN201829,
title = {Improving early OSV design robustness by applying ‘Multivariate Big Data Analytics’ on a ship's life cycle},
journal = {Journal of Industrial Information Integration},
volume = {10},
pages = {29-38},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300869},
author = {Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett},
keywords = {External data, Internal data, Abnormality, Missing data, Outliers, Randomness, Multivariate analysis, Data integration, Clustering},
abstract = {Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.}
}
@article{ZUO2018839,
title = {Using big data from air quality monitors to evaluate indoor PM2.5 exposure in buildings: Case study in Beijing},
journal = {Environmental Pollution},
volume = {240},
pages = {839-847},
year = {2018},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2018.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0269749118307681},
author = {JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong},
keywords = {Indoor PM, Infiltration factor, Indoor/outdoor ratio, Beijing},
abstract = {Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5 μm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6 ± 18.4 μg/m3. Specifically, the concentration in non-heating season was 34.9 ± 15.8 μg/m3, which was 24% lower than that in heating season (46.1 ± 21.2 μg/m3). A significant correlation between indoor and ambient PM2.5 (p < 0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor/outdoor (I/O) ratio was estimated to be 0.73 ± 0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8 ± 27.4 μg/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.}
}
@article{LEE201820,
title = {Industrial Artificial Intelligence for industry 4.0-based manufacturing systems},
journal = {Manufacturing Letters},
volume = {18},
pages = {20-23},
year = {2018},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213846318301081},
author = {Jay Lee and Hossein Davari and Jaskaran Singh and Vibhor Pandhare},
keywords = {Industrial AI, Industry 4.0, Big data, Smart manufacturing, Cyber physical systems},
abstract = {The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.}
}
@article{RIVAS201794,
title = {Towards a service architecture for master data exchange based on ISO 8000 with support to process large datasets},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {94-104},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916301192},
author = {Bibiano Rivas and Jorge Merino and Ismael Caballero and Manuel Serrano and Mario Piattini},
keywords = {Master data, Data quality, ISO 8000, Big data},
abstract = {During the execution of business processes involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data in order to prevent problems in the business processes. Organizations can be benefitted from having information about the level of quality of master data along with the master data to support decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO 8000-1x0 specifies how to add this information to the master data messages. From the clauses stated in the various part of standard we developed a reference architecture, enhanced with big data technologies to better support the management of large datasets The main contribution of this paper is a service architecture for Master Data Exchange supporting the requirements stated by the different parts of the standard like the development of a data dictionary with master data terms; a communication protocol; an API to manage the master data messages; and the algorithms in MapReduce to measure the data quality.}
}
@article{HEZEL20181,
title = {What we know about elemental bulk chondrule and matrix compositions: Presenting the ChondriteDB Database},
journal = {Geochemistry},
volume = {78},
number = {1},
pages = {1-14},
year = {2018},
issn = {0009-2819},
doi = {https://doi.org/10.1016/j.chemer.2017.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0009281916302896},
author = {Dominik C. Hezel and Markus Harak and Guy Libourel},
keywords = {Chondrules, Matrix, Elemental composition, ChondritedDB, Database},
abstract = {Chondrules and matrix are the major components of chondritic meteorites and represent a significant evolutionary step in planet formation. The formation and evolution of chondrules and matrix and, in particular, the mechanics of chondrule formation remain the biggest unsolved challenge in meteoritics. A large number of studies of these major components not only helped to understand these in ever greater detail, but also produced a remarkably large body of data. Studying all available data has become known as ‹big data› analyses and promises deep insights – in this case – to chondrule and matrix formation and relationships. Looking at all data may also allow one to better understand the mechanism of chondrule formation or, equally important, what information we might be missing to identify this process. A database of all available chondrule and matrix data further provides an overview and quick visualisation, which will not only help to solve actual problems, but also enable students and future researchers to quickly access and understand all we know about these components. We collected all available data on elemental bulk chondrule and matrix compositions in a database that we call ChondriteDB. The database also contains petrographic and petrologic information on chondrules. Currently, ChondriteDB contains about 2388 chondrule and 1064 matrix data from 70 different publications and 161 different chondrites. Future iterations of ChondriteDB will include isotope data and information on other chondrite components. Data quality is of critical importance. However, as we discuss, quality is not an objective category, but a subjective judgement. Quantifiable data acquisition categories are required that allow selecting the appropriate data from a database in the context of a given research problem. We provide a comprehensive overview on the contents of ChondriteDB. The database is available as an Excel file upon request from the senior author of this paper, or can be accessed through MetBase.}
}
@article{LIU2018191,
title = {Steering data quality with visual analytics: The complexity challenge},
journal = {Visual Informatics},
volume = {2},
number = {4},
pages = {191-197},
year = {2018},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X18300573},
author = {Shixia Liu and Gennady Andrienko and Yingcai Wu and Nan Cao and Liu Jiang and Conglei Shi and Yu-Shuen Wang and Seokhee Hong},
keywords = {Data quality management, Visual analytics, Data cleansing},
abstract = {Data quality management, especially data cleansing, has been extensively studied for many years in the areas of data management and visual analytics. In the paper, we first review and explore the relevant work from the research areas of data management, visual analytics and human-computer interaction. Then for different types of data such as multimedia data, textual data, trajectory data, and graph data, we summarize the common methods for improving data quality by leveraging data cleansing techniques at different analysis stages. Based on a thorough analysis, we propose a general visual analytics framework for interactively cleansing data. Finally, the challenges and opportunities are analyzed and discussed in the context of data and humans.}
}
@article{KNEPPER201792,
title = {Forward Observer system for radar data workflows: Big data management in the field},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {92-97},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17310567},
author = {Richard Knepper and Matthew Standish},
keywords = {Microcomputers, Information storage, Physical sciences and engineering},
abstract = {There are unique challenges in managing data collection and management from instruments in the field in general. These issues become extreme when “in the field” means “in a plane over the Antarctic”. In this paper we present the design and function of the Forward Observer a computer cluster and data analysis system that flies in a plane in the Arctic and Antarctic to collect, analyze in real time, and store Synthetic Aperture Radar (SAR) data. SAR is used to analyze the thickness and structure of polar ice sheets. We also discuss the processing of data once it is returned to the continental US and made available via data grids. The needs for in-flight data analysis and storage in the Antarctic and Arctic are highly unusual, and we have developed a novel system to meet those needs. We describe the constraints and requirements that led to the creation of this system and the general functionality which it applies to any instrument. We discuss the main means for handling replication and creating checksum information to ensure that data collected in polar regions are returned safely to mainland US for analysis. So far, not a single byte of data collected in the field has failed to make it home to the US for analysis (although many particular data storage devices have failed or been damaged due to the challenges of the extreme environments in which this system is used). While the Forward Observer system is developed for the extreme situation of data management in the field in the Antarctic, the technology and solutions we have developed are applicable and potentially usable in many situations where researchers wish to do real time data management in the field in areas that are constrained in terms of electrical supply.}
}
@article{DREWER2017298,
title = {The BIG DATA Challenge: Impact and opportunity of large quantities of information under the Europol Regulation},
journal = {Computer Law & Security Review},
volume = {33},
number = {3},
pages = {298-308},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917300699},
author = {Daniel Drewer and Vesela Miladinova},
keywords = {Europol, Big data, Privacy, Data protection, Data protection impact assessment, Risk assessment, Privacy by design, Advanced technologies, Europol Regulation, Integrated Data Management Concept (IDMC)},
abstract = {In the digital age, the interaction between privacy, data protection and advanced technological developments such as big data analytics has become pertinent to Europol's effectiveness in providing accurate crime analyses. For the purposes of preventing and combating crime falling within the scope of its objectives, it is imperative for Europol to employ the fullest and most up-to-date information and technical capabilities possible whilst respecting fundamental human rights. The present article addresses precisely the “paradox” of on one side protecting fundamental human rights against external terrorist and/or cybercrime intrusions, and on the other providing a privacy-conscious approach to data collection and analytics, so that Europol can even more effectively support and strengthen action in protecting society against internal threats in a proportionate, responsible and legitimate manner. The advantage proposed in this very context of large quantities of data informing strategic analysis at Europol is a purpose-oriented data protection impact assessment. Namely, the evolution from traditional instruments in the fight against organised crime and terrorism to more technologically advanced ones equally requires an alteration of the conventional notions of privacy and investigative and information-sharing methods.}
}
@incollection{KRESS201911,
title = {Big Data for Ecological Models},
editor = {Brian Fath},
booktitle = {Encyclopedia of Ecology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {11-20},
year = {2019},
isbn = {978-0-444-64130-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10557-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489105573},
author = {Marin M. Kress},
keywords = {Big data, Crowdsourcing or crowdsourced, Data discovery, Data discovery, Data science, Database, Dryad, Environmental health, Interdisciplinary, Machine readable, Metadata, Remote sensing, Social media},
abstract = {The use of data repositories for parameterizing ecological models and storing model runs is becoming more common, yet often these data archives do not contain the appropriate metadata, nor are they maintained for others to use. Data archiving and sharing are additional steps in the scientific process that add value to a researcher׳s work, and more importantly, facilitate transparency and repeatability of a researcher׳s work. Historically, peer-reviewed publications did not allow for the full presentation of underlying datasets, which were only shared through personal contact with a scientist. However, with the expanding use of “supporting online material” (SOM) files that accompany digital publication there is an increased expectation that even large datasets can be made accessible to readers. Thus, researchers are faced with the additional task of becoming their own archivist and depositing data in a repository where it can be used by others. This article introduces basic concepts in data archiving and sharing, including major digital repositories for life science data, commonly used digital file formats, and why metadata is an essential element to successful data sharing when machine-readable data is increasingly used in large-scale studies.}
}
@article{HE201714946,
title = {Statistical Process Monitoring for IoT-Enabled Cybermanufacturing: Opportunities and Challenges},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {14946-14951},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2546},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317334717},
author = {Q. Peter He and Jin Wang and Devarshi Shah and Nader Vahdat},
keywords = {Cybermanufacturing, Internet of Things, sensors, statistical process monitoring, fault detection, fault diagnosis, statistics pattern analysis},
abstract = {Initiated from services and consumer products industries, there is a growing interest in using Internet of Things (IoT) technologies in various industries. In particular, IoT-enabled cybermanufacturing starts to draw increasing attention. Because IoT devices such as IoT sensors are usually much cheaper and smaller than the traditional sensors, there is a potential for instrumenting manufacturing systems with massive number of sensors. The premise is that the big data subsequently collected from IoT sensors can be utilized to advance manufacturing. Therefore, data-driven statistical process monitoring (SPM) is expected to contribute significantly to the advancement of cybermanufacturing. In this work, the state-of-the-art in cybermanufacturing is reviewed; an IoT-enabled manufacturing technology testbed (MTT) was built to explore the potential of IoT sensors for manufacturing, as well as to understand the characteristics of data produced by the IoT sensors; finally, the potentials and challenges associated with big data analytics presented by cybermanufacturing systems is discussed; and we propose statistics pattern analysis (SPA) as a promising SPM tool for cybermanufacturing.}
}
@article{KOZJEK2018209,
title = {Big data analytics for operations management in engineer-to-order manufacturing},
journal = {Procedia CIRP},
volume = {72},
pages = {209-214},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.098},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118302531},
author = {Dominik Kozjek and Rok Vrabič and Borut Rihtaršič and Peter Butala},
keywords = {Engineer-to-order manufacturing, Operations management, Data analytics, Industrial data, Data mining, Big data},
abstract = {Manufacturing data offers big potential for improving management of manufacturing operations. The paper addresses an approach to data analytics in engineer-to-order (ETO) manufacturing systems where the product quality and due-date reliability play a key role in management decisionmaking. The objective of the research is to investigate manufacturing data which are collected by a manufacturing execution system (MES) during operations in an ETO enterprise and to develop tools for supporting scheduling of operations. The developed tools can be used for simulation of production and forecasting of potential resource overloads.}
}
@article{DAMIANI20181,
title = {Large databases (Big Data) and evidence-based medicine},
journal = {European Journal of Internal Medicine},
volume = {53},
pages = {1-2},
year = {2018},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2018.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0953620518301961},
author = {Andrea Damiani and Graziano Onder and Vincenzo Valentini}
}
@incollection{PHAN2017253,
title = {9 - Big Data and Monitoring the Grid},
editor = {Brian W. D’Andrade},
booktitle = {The Power Grid},
publisher = {Academic Press},
pages = {253-285},
year = {2017},
isbn = {978-0-12-805321-8},
doi = {https://doi.org/10.1016/B978-0-12-805321-8.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053218000094},
author = {Sonal K. Phan and Cathy Chen},
keywords = {Big data, power quality disturbance detection, intrusion detection, islanding detection, feature extraction, classification, data analytics, forecasting, visualization, smart meters, demand response},
abstract = {A traditional power grid, also known as the legacy grid, collects data at a few locations on the grid to monitor grid performance and forecast energy requirements on a macro level. A smart grid is the next generation of the electric power grid; it includes technologies for real-time data acquisition from various sections of the grid and provides a means for two-way communication between energy suppliers and consumers. Compared to a legacy grid, the smart grid generates large volumes of data that can be exploited for power quality event monitoring, intrusion detection, islanding detection, price forecasting, and energy forecasting at a much more granular level. These large volumes of data have to be analyzed in real-time and with high accuracy in order assist in decision making for power system operations and optimal power flow. This poses a big data challenge, which to be implemented successfully requires changes in infrastructure and data analysis methods. This chapter describes the smart grid and its associated big data and discusses methods for informative feature extraction from raw data, event monitoring, and energy consumption forecasting using these features and visualization methods to assist with data interpretation and decision making.}
}
@article{KOBUSINSKA20181321,
title = {Big Data fingerprinting information analytics for sustainability},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1321-1337},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.12.061},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329965},
author = {Anna Kobusińska and Kamil Pawluczuk and Jerzy Brzeziński},
keywords = {Big Data, Fingerprinting, Web tracking, Security, Analytics},
abstract = {Web-based device fingerprinting is the process of collecting security information through the browser to perform stateless device identification. Fingerprints may then be used to identify and track computing devices in the web. There are various reasons why device-related information may be needed. Among the others, this technique could help to efficiently analyze security information for sustainability. In this paper we introduce a fingerprinting analytics tool that discovers the most appropriate device fingerprints and their corresponding optimal implementations. The fingerprints selected in the result of the performed analysis are used to enrich and improve an open-source fingerprinting analytics tool Fingerprintjs2, daily consumed by hundreds of websites. As a result, the paper provides a noticeable progress in analytics of dozens of values of device fingerprints, and enhances analysis of fingerprints security information.}
}
@article{SZYMANSKA20181,
title = {Modern data science for analytical chemical data – A comprehensive review},
journal = {Analytica Chimica Acta},
volume = {1028},
pages = {1-10},
year = {2018},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2018.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0003267018306421},
author = {Ewa Szymańska},
keywords = {Chemometrics, Data science, Big data, Chemical analytical data, Methodology},
abstract = {Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.}
}
@article{BABAR2018155,
title = {Energy-harvesting based on internet of things and big data analytics for smart health monitoring},
journal = {Sustainable Computing: Informatics and Systems},
volume = {20},
pages = {155-164},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2210537917302238},
author = {Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon},
keywords = {Big data analytics, IoT, Energy harvesting},
abstract = {Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture is a three-layered architecture that comprises: (1) energy harvesting and data generation; (2) data pre-processing; and (3) data processing and application. The proposed scheme highlights the effectiveness of energy-harvesting based IoT in healthcare. In addition, it also proposes a solution for smart health monitoring and planning. We also utilized consistent datasets on the Hadoop server to validate the proposed architecture based on threshold limit values (TLVs). The study demonstrates that the proposed architecture offers substantial and immediate value to the field of smart health.}
}
@article{SARAN2017713,
title = {The China Kidney Disease Network (CK-NET): “Big Data—Big Dreams”},
journal = {American Journal of Kidney Diseases},
volume = {69},
number = {6},
pages = {713-716},
year = {2017},
issn = {0272-6386},
doi = {https://doi.org/10.1053/j.ajkd.2017.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0272638617306340},
author = {Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham}
}
@article{LIANG201887,
title = {Application and research of global grid database design based on geographic information},
journal = {Global Energy Interconnection},
volume = {1},
number = {1},
pages = {87-95},
year = {2018},
issn = {2096-5117},
doi = {https://doi.org/10.14171/j.2096-5117.gei.2018.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S2096511718300112},
author = {Xuming Liang},
keywords = {Big data collection, Geographic information, Grid database, Data mining},
abstract = {Energy crisis and climate change have become two seriously concerned issues universally. As a feasible solution, Global Energy Interconnection (GEI) has been highly praised and positively responded by the international community once proposed by China. From strategic conception to implementation, GEI development has entered a new phase of joint action now. Gathering and building a global grid database is a prerequisite for conducting research on GEI. Based on the requirement of global grid data management and application, combining with big data and geographic information technology, this paper studies the global grid data acquisition and analysis process, sorts out and designs the global grid database structure supporting GEI research, and builds a global grid database system.}
}
@incollection{WANG2018247,
title = {Chapter 8 - Real-Time Monitoring and Early Warning of a Train’s Running State and Operation Behavior},
editor = {Junfeng Wang},
booktitle = {Safety Theory and Control Technology of High-Speed Train Operation},
publisher = {Academic Press},
pages = {247-266},
year = {2018},
isbn = {978-0-12-813304-0},
doi = {https://doi.org/10.1016/B978-0-12-813304-0.00008-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128133040000086},
author = {Junfeng Wang},
keywords = {High-speed railway, big data, train running status, monitoring, early warning},
abstract = {In the view of the whole system this chapter introduces the train state monitoring and early warning, which is based on big data and combines big data theory with human and signaling systems, comprehensively considering the coordination among the TCC, CBI, CTC, and other subsystems. We analyze the factors that can affect the train state of operation systematically, including the operation action of the signaling system and the operator, realizing the real-time and online monitoring and early warning of train running state then to ensure the safety of train operation.}
}
@article{EVANGELISTA2018112,
title = {Topological support and data quality can only be assessed through multiple tests in reviewing Blattodea phylogeny},
journal = {Molecular Phylogenetics and Evolution},
volume = {128},
pages = {112-122},
year = {2018},
issn = {1055-7903},
doi = {https://doi.org/10.1016/j.ympev.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1055790318300186},
author = {Dominic Evangelista and France Thouzé and Manpreet Kaur Kohli and Philippe Lopez and Frédéric Legendre},
keywords = {Phylogenetic signal, mtDNA, Termite, Dictyoptera, SAMS, Rogue taxa, Long branch attraction, Signal analysis},
abstract = {Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree’s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.}
}
@article{RISLING201789,
title = {Educating the nurses of 2025: Technology trends of the next decade},
journal = {Nurse Education in Practice},
volume = {22},
pages = {89-92},
year = {2017},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1471595316302748},
author = {Tracie Risling},
keywords = {Informatics, Curriculum development, Technology},
abstract = {The pace of technological evolution in healthcare is advancing. In this article key technology trends are identified that are likely to influence nursing practice and education over the next decade. The complexity of curricular revision can create challenges in the face of rapid practice change. Nurse educators are encouraged to consider the role of electronic health records (EHRs), wearable technologies, big data and data analytics, and increased patient engagement as key areas for curriculum development. Student nurses, and those already in practice, should be offered ongoing educational opportunities to enhance a wide spectrum of professional informatics skills. The nurses of 2025 will most certainly inhabit a very different practice environment than what exists today and technology will be key in this transformation. Nurse educators must prepare now to lead these practitioners into the future.}
}
@article{DEBAUCHE2018112,
title = {Cloud Platform using Big Data and HPC Technologies for Distributed and Parallels Treatments},
journal = {Procedia Computer Science},
volume = {141},
pages = {112-118},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318064},
author = {Olivier Debauche and Sidi Ahmed Mahmoudi and Saïd Mahmoudi and Pierre Manneback},
keywords = {GPU, FPGA, MIC, CPU, TPU, Cloud, Big Data, parallel, distributed processing, heterogeneous cloud architecture},
abstract = {Smart agriculture is one of the most diverse research. In addition, the quantity of data to be stored and the choice of the most efficient algorithms to process are significant elements in this field. The storage of collecting data from Internet of Things (IoT), existing on distributed, local databases and open data need a particular infrastructure to federate all these data to make complex treatments. The storage of this wide range of data that comes at high frequency and variable throughput is particularly difficult. In this paper, we propose the use of distributed databases and high-performance computing architecture in order to exploit multiple re-configurable computing and application specific processing such as CPUs, GPUs, TPUs and FPGAs efficiently. This exploitation allows an accurate training for an application to machine learning, deep learning and unsupervised modeling algorithms. The last ones are used for training supervised algorithms on images when it labels a set of images and unsupervised algorithms on IoT data which are unlabeled with variable qualities. The processing of data is based on Hadoop 3.1 MapReduce to achieve parallel processing and use containerization technologies to distribute treatments on Multi GPU, MIC and FPGA. This architecture allows efficient treatments of data coming from several sources with a cloud high-performance heterogeneous architecture. The proposed 4 layers infrastructure can also implement FPGA and MIC which are now natively supported by recent version of Hadoop. Moreover, with the advent of new technologies like Intel® MovidiusTM; it is now possible to deploy CNN at the Fog level in the IoT network and to make inference with the cloud and therefore limit significantly the network traffic that result in reducing the move of large amounts of data to the cloud.}
}
@article{KOLOSSA2018775,
title = {Data quality over data quantity in computational cognitive neuroscience},
journal = {NeuroImage},
volume = {172},
pages = {775-785},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918300053},
author = {Antonio Kolossa and Bruno Kopp},
keywords = {Computational modeling, Functional brain imaging, Signal-to-noise ratio, Reliability, Replicability},
abstract = {We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.}
}
@article{DAVIS2017224,
title = {Residential land values in the Washington, DC metro area: New insights from big data},
journal = {Regional Science and Urban Economics},
volume = {66},
pages = {224-246},
year = {2017},
issn = {0166-0462},
doi = {https://doi.org/10.1016/j.regsciurbeco.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0166046216301508},
author = {Morris A. Davis and Stephen D. Oliner and Edward J. Pinto and Sankar Bokka},
keywords = {Land, Housing, House prices, Housing boom and bust, Financial crisis},
abstract = {We use a new property-level data set and an innovative methodology to estimate the price of land from 2000 to 2013 for nearly the universe of detached single-family homes in the Washington, DC metro area and to characterize the boom-bust cycle in land and house prices at a fine geography. The results show that land prices were more volatile than house prices everywhere, but especially so in the areas where land was inexpensive in 2000. We demonstrate that the change in the land share of house value during the boom was a significant predictor of the decline in house prices during the bust, highlighting the value of focusing on land in assessing house-price risk.}
}
@article{SOHRABI2018280,
title = {Systematic method for finding emergence research areas as data quality},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {280-287},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517318140},
author = {Babak Sohrabi and Ahmad Khalilijafarabad},
keywords = {Data quality, Text mining, Science mapping, Data mining, Trend analysis},
abstract = {The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.}
}
@article{KIM201818,
title = {Exploring Determinants of Semantic Web Technology Adoption from IT Professionals' Perspective: Industry Competition, Organization Innovativeness, and Data Management Capability},
journal = {Computers in Human Behavior},
volume = {86},
pages = {18-33},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830178X},
author = {Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis},
keywords = {Semantic web, IT professionals' perspective technology adoption, Technology-organization-environment framework, Innovation diffusion theory},
abstract = {The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology–organization–environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.}
}
@article{HEINRICH201895,
title = {Assessing data quality – A probability-based metric for semantic consistency},
journal = {Decision Support Systems},
volume = {110},
pages = {95-106},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300599},
author = {Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner},
keywords = {Data quality, Data quality assessment, Data quality metric, Data consistency},
abstract = {We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.}
}
@article{SCHLEICH20183,
title = {Geometrical Variations Management 4.0: towards next Generation Geometry Assurance},
journal = {Procedia CIRP},
volume = {75},
pages = {3-10},
year = {2018},
note = {The 15th CIRP Conference on Computer Aided Tolerancing, CIRP CAT 2018, 11-13 June 2018, Milan, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.078},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305948},
author = {Benjamin Schleich and Kristina Wärmefjord and Rikard Söderberg and Sandro Wartzack},
keywords = {Industry 4.0, Digital Twin, Geometry Assurance},
abstract = {Product realization processes are undergoing radical change considering the increasing digitalization of manufacturing fostered by cyber-physical production systems, the internet of things, big data, cloud computing, and the advancing use of digital twins. These trends are subsumed under the term “industry 4.0” describing the vision of a digitally connected manufacturing environment. The contribution gives an overview of future challenges and potentials for next generation geometry assurance and geometrical variations management in the context of industry 4.0. Particularly, the focus is set on potentials and risks of increasingly available manufacturing data and the use of digital twins in geometrical variations management.}
}
@article{BIBRI2017449,
title = {ICT of the new wave of computing for sustainable urban forms: Their big data and context-aware augmented typologies and design concepts},
journal = {Sustainable Cities and Society},
volume = {32},
pages = {449-474},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716302475},
author = {Simon Elias Bibri and John Krogstie},
keywords = {Sustainable urban forms, Smart sustainable cities, Big data analytics, Context-aware computing, Typologies and design concepts, Technologies and applications, ICT of the new wave of computing},
abstract = {Undoubtedly, sustainable development has inspired a generation of scholars and practitioners in different disciplines into a quest for the immense opportunities created by the development of sustainable urban forms for human settlements that will enable built environments to function in a more constructive and efficient way. However, there are still significant challenges that need to be addressed and overcome. The issue of such forms has been problematic and difficult to deal with, particularly in relation to the evaluation and improvement of their contribution to the goals of sustainable development. As it is an urban world where the informational and physical landscapes are increasingly being merged, sustainable urban forms need to embrace and leverage what current and future ICT has to offer as innovative solutions and sophisticated methods so as to thrive—i.e. advance their contribution to sustainability. The need for ICT of the new wave of computing to be embedded in such forms is underpinned by the recognition that urban sustainability applications are deemed of high relevance to the contemporary research agenda of computing and ICT. To unlock and exploit the underlying potential, the field of sustainable urban planning is required to extend its boundaries and broaden its horizons beyond the ambit of the built form of cities to include technological innovation opportunities. This paper explores and substantiates the real potential of ICT of the new wave of computing to evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. This entails merging big data and context-aware technologies and their applications with the typologies and design concepts of sustainable urban forms to achieve multiple hitherto unrealized goals. In doing so, this paper identifies models of smart sustainable city and their technologies and applications and models of sustainable urban form and their design concepts and typologies. In addition, it addresses the question of how these technologies and applications can be amalgamated with these design concepts and typologies in ways that ultimately evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. The overall aim of this paper suits a mix of three methodologies: literature review, thematic analysis, and secondary (qualitative) data analysis to achieve different but related objectives. The study identifies four technologies and two classes of applications pertaining to models of smart sustainable city as well as three design concepts and four typologies related to models of sustainable urban form. Finally, this paper proposes a Matrix to help scholars and planners in understanding and analyzing how and to what extent the contribution of sustainable urban forms to sustainability can be improved through ICT of the new wave of computing as to the underlying novel technologies and their applications, as well as a data-centric approach into investigating and evaluating this contribution and a simulation method for strategically optimizing it.}
}
@incollection{GUDIVADA201731,
title = {Chapter 2 - Data Analytics: Fundamentals},
editor = {Mashrur Chowdhury and Amy Apon and Kakan Dey},
booktitle = {Data Analytics for Intelligent Transportation Systems},
publisher = {Elsevier},
pages = {31-67},
year = {2017},
isbn = {978-0-12-809715-1},
doi = {https://doi.org/10.1016/B978-0-12-809715-1.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809715100002X},
author = {Venkat N. Gudivada},
keywords = {Data analytics, data science, data mining, clustering, classification, model building},
abstract = {This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, big data analytics, to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open source tools and resources for developing data analytics systems are listed. Future directions in data analytics are indicated. The chapter concludes by providing a summary. To reinforce and enhance the reader’s data analytics knowledge and tools, questions and exercise problems are provided at the end of the chapter.}
}
@article{LI20189,
title = {Big enterprise registration data imputation: Supporting spatiotemporal analysis of industries in China},
journal = {Computers, Environment and Urban Systems},
volume = {70},
pages = {9-23},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517301916},
author = {Fa Li and Zhipeng Gui and Huayi Wu and Jianya Gong and Yuan Wang and Siyu Tian and Jiawen Zhang},
keywords = {Geocoding, Missing values imputation, High Performance Computing, Industrial spatial distribution, Urban spatial structure, Short text classification},
abstract = {Big, fine-grained enterprise registration data that includes time and location information enables us to quantitatively analyze, visualize, and understand the patterns of industries at multiple scales across time and space. However, data quality issues like incompleteness and ambiguity, hinder such analysis and application. These issues become more challenging when the volume of data is immense and constantly growing. High Performance Computing (HPC) frameworks can tackle big data computational issues, but few studies have systematically investigated imputation methods for enterprise registration data in this type of computing environment. In this paper, we propose a big data imputation workflow based on Apache Spark as well as a bare-metal computing cluster, to impute enterprise registration data. We integrated external data sources, employed Natural Language Processing (NLP), and compared several machine-learning methods to address incompleteness and ambiguity problems found in enterprise registration data. Experimental results illustrate the feasibility, efficiency, and scalability of the proposed HPC-based imputation framework, which also provides a reference for other big georeferenced text data processing. Using these imputation results, we visualize and briefly discuss the spatiotemporal distribution of industries in China, demonstrating the potential applications of such data when quality issues are resolved.}
}
@article{SINGH2018652,
title = {Real world big data for clinical research and drug development},
journal = {Drug Discovery Today},
volume = {23},
number = {3},
pages = {652-660},
year = {2018},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359644617305950},
author = {Gurparkash Singh and Duane Schulthess and Nigel Hughes and Bart Vannieuwenhuyse and Dipak Kalra},
abstract = {The objective of this paper is to identify the extent to which real world data (RWD) is being utilized, or could be utilized, at scale in drug development. Through screening peer-reviewed literature, we have cited specific examples where RWD can be used for biomarker discovery or validation, gaining a new understanding of a disease or disease associations, discovering new markers for patient stratification and targeted therapies, new markers for identifying persons with a disease, and pharmacovigilance. None of the papers meeting our criteria was specifically geared toward novel targets or indications in the biopharmaceutical sector; the majority were focused on the area of public health, often sponsored by universities, insurance providers or in combination with public health bodies such as national insurers. The field is still in an early phase of practical application, and is being harnessed broadly where it serves the most direct need in public health applications in early, rare and novel disease incidents. However, these exemplars provide a valuable contribution to insights on the use of RWD to create novel, faster and less invasive approaches to advance disease understanding and biomarker discovery. We believe that pharma needs to invest in making better use of Electronic Health Records and the need for more precompetitive collaboration to grow the scale of this ‘big denominator’ capability, especially given the needs of precision medicine research.}
}
@article{YUAN201786,
title = {Exploring inter-country connection in mass media: A case study of China},
journal = {Computers, Environment and Urban Systems},
volume = {62},
pages = {86-96},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2016.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516303313},
author = {Yihong Yuan and Yu Liu and Guixing Wei},
keywords = {Time series, Inter-country relations, Spatio-temporal data mining, Mass media events, GDELT},
abstract = {The development of theories and techniques for big data analytics offers tremendous possibility for investigating large-scale events and patterns that emerge over space and time. In this research, we utilize a unique open dataset “The Global Data on Events, Location and Tone” (GDELT) to model the image of China in mass media, specifically, how China has related to the rest of the world and how this connection has evolved upon time. The results of this research contribute to both the methodological and the empirical perspectives: We examined the effectiveness of the dynamic time warping (DTW) distances in measuring the differences between long-term mass media data. We identified four types of connection strength patterns between China and its top 15 related countries. With that, the distance decay effect in mass media is also examined and compared with social media and public transportation data. While using multiple datasets and focusing on mass media, this study generates valuable input regarding the interpretation of the diplomatic and regional correlation for the nation of China. It also provides methodological references for investigating international relations in other countries and regions in the big data era.}
}
@article{SEMANJSKI201738,
title = {Spatial context mining approach for transport mode recognition from mobile sensed big data},
journal = {Computers, Environment and Urban Systems},
volume = {66},
pages = {38-52},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516304367},
author = {Ivana Semanjski and Sidharta Gautama and Rein Ahas and Frank Witlox},
keywords = {Transport mode recognition, Mobile sensed big data, Spatial awareness, Geographic information systems, Smart city, Support vector machines, Context mining, Urban data},
abstract = {Knowledge about what transport mode people use is important information of any mobility or travel behaviour research. With ubiquitous presence of smartphones, and its sensing possibilities, new opportunities to infer transport mode from movement data are appearing. In this paper we investigate the role of spatial context of human movements in inferring transport mode from mobile sensed data. For this we use data collected from more than 8000 participants over a period of four months, in combination with freely available geographical information. We develop a support vectors machines-based model to infer five transport modes and achieve success rate of 94%. The developed model is applicable across different mobile sensed data, as it is independent on the integration of additional sensors in the device itself. Furthermore, suggested approach is robust, as it strongly relies on pre-processed data, which makes it applicable for big data implementations in (smart) cities and other data-driven mobility platforms.}
}
@article{FAIEQ2017151,
title = {C2IoT: A framework for Cloud-based Context-aware Internet of Things services for smart cities},
journal = {Procedia Computer Science},
volume = {110},
pages = {151-158},
year = {2017},
note = {14th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2017) / 12th International Conference on Future Networks and Communications (FNC 2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.06.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917312486},
author = {Soufiane Faieq and Rajaa Saidi and Hamid Elghazi and Moulay Driss Rahmani},
keywords = {Internet of Things, Cloud Computing, Context-Awareness, Big Data, Service Composition, Smart City},
abstract = {The smart city vision was born by the integration of ICT in the day to day city management operations and citizens lives, owing to the need for novel and smart ways to manage the cities resources; making them more efficient, sustainable and transparent. However, the understanding of the crucial elements to this integration and how they can benefit from each other proves difficult and unclear. In this article, we investigate the intricate synergies between different technologies and paradigms involved in the smart city vision, to help design a robust framework, capable of handling the challenges impeding its successful implementation. To this end, we propose a context-aware centered approach to present a holistic view of a smart city as viewed from the different angles (Cloud, IoT, Big Data). We also propose a framework encompassing elements from the different enablers, leveraging their strengths to build and develop smart-x applications and services.}
}
@article{VIDAURRE2018646,
title = {Discovering dynamic brain networks from big data in rest and task},
journal = {NeuroImage},
volume = {180},
pages = {646-656},
year = {2018},
note = {Brain Connectivity Dynamics},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.06.077},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917305487},
author = {Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich},
abstract = {Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.}
}
@article{VIDGEN2017626,
title = {Management challenges in creating value from business analytics},
journal = {European Journal of Operational Research},
volume = {261},
number = {2},
pages = {626-639},
year = {2017},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717301455},
author = {Richard Vidgen and Sarah Shaw and David B. Grant},
keywords = {Analytics, Delphi, Management challenges, Value creation, Ecosystem},
abstract = {The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.}
}
@article{SADIQ2017150,
title = {Open data: Quality over quantity},
journal = {International Journal of Information Management},
volume = {37},
number = {3},
pages = {150-154},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216309021},
author = {Shazia Sadiq and Marta Indulska},
keywords = {Open data, Data quality},
abstract = {Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.}
}
@article{NABATI2017160,
title = {Data Driven Decision Making in Planning the Maintenance Activities of Off-shore Wind Energy},
journal = {Procedia CIRP},
volume = {59},
pages = {160-165},
year = {2017},
note = {Proceedings of the 5th International Conference in Through-life Engineering Services Cranfield University, 1st and 2nd November 2016},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S221282711630960X},
author = {Elaheh Gholamzadeh Nabati and Klaus-Dieter Thoben},
keywords = {Maintenance, (Big) data analysis, Off-shore wind turbines, Decision making},
abstract = {Planning and scheduling for wind farms play a critical role in the costs of maintenance. The use and analysis of field data or so-called Product Use Information (PUI) to improve maintenance activities and to reduce the costs has gained attention in the recent years. The product use data consist of sources such as measure of sensors on the turbines, the alarms information or signals from the condition monitoring, Supervisory Control and Data Acquisition (SCADA) systems, which are currently used in maintenance activities. However, those data have the potential to offer alternative solutions to improve processes and provide better decisions, by transforming them into actionable knowledge. In order to make the right decision it is important to understand, which PUI data source and which data analysis methods, are suitable for what kind of decision making task. The aim of this study is to discover, how analysis of PUI can help in the maintenance processes of off-shore wind power. The techniques from the field of big data analytics for analyzing the PUI are here addressed. The results of this study contain suggestions on the basis of algorithms of data analytics, suitable for each decision type.}
}
@article{KUMAR2018428,
title = {A big data driven sustainable manufacturing framework for condition-based maintenance prediction},
journal = {Journal of Computational Science},
volume = {27},
pages = {428-439},
year = {2018},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877750316305129},
author = {Ajay Kumar and Ravi Shankar and Lakshman S. Thakur},
keywords = {data driven sustainable enterprise, fuzzy unordered induction algo, big data analytics, condition-based maintenance, machine learning techniques, backward feature elimination},
abstract = {Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics.}
}
@article{BUFFAT2017277,
title = {Big data GIS analysis for novel approaches in building stock modelling},
journal = {Applied Energy},
volume = {208},
pages = {277-290},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S030626191731454X},
author = {René Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg},
keywords = {Building heat demand, Big data, Large scale modelling, Bottom-up modelling, GIS, Climate data, Spatio-temporal modelling},
abstract = {Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.}
}
@article{BRONSELAER201895,
title = {An incremental approach for data quality measurement with insufficient information},
journal = {International Journal of Approximate Reasoning},
volume = {96},
pages = {95-111},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X17307478},
author = {A. Bronselaer and J. Nielandt and G. {De Tré}},
keywords = {Data quality measurement, Uncertainty modelling, Insufficient information, Possibility theory},
abstract = {Recently, a fundamental study on measurement of data quality introduced an ordinal-scaled procedure of measurement. Besides the pure ordinal information about the level of quality, numerical information is induced when considering uncertainty involved during measurement. In the case where uncertainty is modelled as probability, this numerical information is ratio-scaled. An essential property of the mentioned approach is that the application of a measure on a large collection of data can be represented efficiently in the sense that (i) the representation has a low storage complexity and (ii) it can be updated incrementally when new data are observed. However, this property only holds when the evaluation of predicates is clear and does not deal with uncertainty. For some dimensions of quality, this assumption is far too strong and uncertainty comes into play almost naturally. In this paper, we investigate how the presence of uncertainty influences the efficiency of a measurement procedure. Hereby, we focus specifically on the case where uncertainty is caused by insufficient information and is thus modelled by means of possibility theory. It is shown that the amount of data that reaches a certain level of quality, can be summarized as a possibility distribution over the set of natural numbers. We investigate an approximation of this distribution that has a controllable loss of information, allows for incremental updates and exhibits a low space complexity.}
}
@article{BUTTERWORTH2018257,
title = {The ICO and artificial intelligence: The role of fairness in the GDPR framework},
journal = {Computer Law & Security Review},
volume = {34},
number = {2},
pages = {257-268},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S026736491830044X},
author = {Michael Butterworth},
keywords = {Artificial intelligence (AI), Big data analytics, General Data Protection Regulation (GDPR), Fairness, Regulations, Collective rights, Data ethics},
abstract = {The year 2017 has seen many EU and UK legislative initiatives and proposals to consider and address the impact of artificial intelligence on society, covering questions of liability, legal personality and other ethical and legal issues, including in the context of data processing. In March 2017, the Information Commissioner's Office (UK) updated its big data guidance to address the development of artificial intelligence and machine learning, and to provide (GDPR), which will apply from 25 May 2018. This paper situates the ICO's guidance in the context of wider legal and ethical considerations and provides a critique of the position adopted by the ICO. On the ICO's analysis, the key challenge for artificial intelligence processing personal data is in establishing that such processing is fair. This shift reflects the potential for artificial intelligence to have negative social consequences (whether intended or unintended) that are not otherwise addressed by the GDPR. The question of ‘fairness’ is an important one, to address the imbalance between big data organisations and individual data subjects, with a number of ethical and social impacts that need to be evaluated.}
}
@article{SU201722,
title = {A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning},
journal = {Habitat International},
volume = {64},
pages = {22-40},
year = {2017},
issn = {0197-3975},
doi = {https://doi.org/10.1016/j.habitatint.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0197397517300498},
author = {Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng},
keywords = {Food geography, Healthy food access, Accessibility, Social inequalities, Transport mode, Multilevel regression},
abstract = {Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30–20:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.}
}
@article{APPELBAUM201729,
title = {Impact of business analytics and enterprise systems on managerial accounting},
journal = {International Journal of Accounting Information Systems},
volume = {25},
pages = {29-44},
year = {2017},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1467089517300490},
author = {Deniz Appelbaum and Alexander Kogan and Miklos Vasarhelyi and Zhaokai Yan},
keywords = {Managerial accounting, Business analytics, Big data, Enterprise systems, Business intelligence},
abstract = {The nature of management accountants' responsibility is evolving from merely reporting aggregated historical value to also including organizational performance measurement and providing management with decision related information. Corporate information systems such as enterprise resource planning (ERP) systems have provided management accountants with both expanded data storage power and enhanced computational power. With big data extracted from both internal and external data sources, management accountants now could utilize data analytics techniques to answer the questions including: what has happened (descriptive analytics), what will happen (predictive analytics), and what is the optimized solution (prescriptive analytics). However, research shows that the nature and scope of managerial accounting has barely changed and that management accountants employ mostly descriptive analytics, some predictive analytics, and a bare minimum of prescriptive analytics. This paper proposes a Managerial Accounting Data Analytics (MADA) framework based on the balanced scorecard theory in a business intelligence context. MADA provides management accountants the ability to utilize comprehensive business analytics to conduct performance measurement and provide decision related information. With MADA, three types of business analytics (descriptive, predictive, and prescriptive) are implemented into four corporate performance measurement perspectives (financial, customer, internal process, and learning and growth) in an enterprise system environment. Other related issues that affect the successful utilization of business analytics within a corporate-wide business intelligence (BI) system, such as data quality and data integrity, are also discussed. This paper contributes to the literature by discussing the impact of business analytics on managerial accounting from an enterprise systems and BI perspective and by providing the Managerial Accounting Data Analytics (MADA) framework that incorporates balanced scorecard methodology.}
}
@article{KHALOUFI2018294,
title = {Security model for Big Healthcare Data Lifecycle},
journal = {Procedia Computer Science},
volume = {141},
pages = {294-301},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.199},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318520},
author = {Hayat Khaloufi and Karim Abouelmehdi and Abderrahim Beni-hssane and Mostafa Saadi},
keywords = {Big data Security, Big data in healthcare, Big data lifecycle, Security threat model},
abstract = {Big data is a concept that aimed at collecting, storing, processing and transforming large amount of data into value using new combination of strategies and technologies. Big data is characterized by data that have a large volume, massive velocity, numerous variety, useful value, and veracity. Big Data Analytics offers tremendous insights to different organizations especially in healthcare. Currently, Big healthcare data has the highest potential for improving patient outcomes, gaining valuable insights, predicting outbreaks of epidemics, avoiding preventable diseases and effectively minimizing the cost of healthcare delivery. However, the dynamic nature of health data presents various conceptual, technical, legal and ethical challenges associated with the data processing and analysis activities. The big data security and privacy concepts are some of the most pertinent issues and have become increasingly significant associated with big healthcare data in the modern world. In this paper, we give an overview of big data characteristics and challenges in healthcare and present big healthcare data lifecycle integrated with security threats and attacks to provide encompass policies and mechanisms that aim at solving the various security challenges in each step of big data lifecycle. The focus is also placed on the description of the recently proposed techniques related to authentication, encryption, anonymization, access control, and privacy. We finally propose an approach to secure threat model for big healthcare data lifecycle as a main contribution of this paper.}
}
@article{SHARMA2018103,
title = {Big GIS analytics framework for agriculture supply chains: A literature review identifying the current trends and future perspectives},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {103-120},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918311311},
author = {Rohit Sharma and Sachin S. Kamble and Angappa Gunasekaran},
keywords = {Agriculture supply chain, GIS analytics, Big data analytics, Internet of things, Drones, Smart farming},
abstract = {The world population is estimated to reach nine billion by 2050. Many challenges are adding pressure on the current agriculture supply chains that include shrinking land sizes, ever increasing demand for natural resources and environmental issues. The agriculture systems need a major transformation from the traditional practices to precision agriculture or smart farming practices to overcome these challenges. Geographic information system (GIS) is one such technology that pushes the current methods to precision agriculture. In this paper, we present a systematic literature review (SLR) of 120 research papers on various applications of big GIS analytics (BGA) in agriculture. The selected papers are classified into two broad categories; the level of analytics and GIS applications in agriculture. The GIS applications viz., land suitability, site search and selection, resource allocation, impact assessment, land allocation, and knowledge-based systems are considered in this study. The outcome of this study is a proposed BGA framework for agriculture supply chain. This framework identifies big data analytics to play a significant role in improving the quality of GIS application in agriculture and provides the researchers, practitioners, and policymakers with guidelines on the successful management of big GIS data for improved agricultural productivity.}
}
@article{YAHIA20181,
title = {Preface: Special Issue on Big Data},
journal = {Fuzzy Sets and Systems},
volume = {348},
pages = {1-3},
year = {2018},
note = {SI: Fuzzy Approaches to Big Data},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2018.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0165011418302987},
author = {Sadok Ben Yahia and Anne Laurent and Gabriella Pasi}
}
@article{DUVIER2018358,
title = {Data quality and governance in a UK social housing initiative: Implications for smart sustainable cities},
journal = {Sustainable Cities and Society},
volume = {39},
pages = {358-365},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717312520},
author = {Caroline Duvier and P.B. Anand and Crina Oltean-Dumbrava},
keywords = {Data quality, Data interoperability, Social housing, Smart sustainable cities, Business intelligence},
abstract = {Smart Sustainable Cities (SSC) consist of multiple stakeholders, who must cooperate in order for SSCs to be successful. Housing is an important challenge and in many cities, therefore, a key stakeholder are social housing organisations. This paper introduces a qualitative case study of a social housing provider in the UK who implemented a business intelligence project (a method to assess data networks within an organisation) to increase data quality and data interoperability. Our analysis suggests that creating pathways for different information systems within an organisation to ‘talk to’ each other is the first step. Some of the issues during the project implementation include the lack of training and development, organisational reluctance to change, and the lack of a project plan. The challenges faced by the organisation during this project can be helpful for those implementing SSCs. Currently, many SSC frameworks and models exist, yet most seem to neglect localised challenges faced by the different stakeholders. This paper hopes to help bridge this gap in the SSC research agenda.}
}
@incollection{BROWN2018277,
title = {Chapter Five - Big Data in Drug Discovery},
editor = {David R. Witty and Brian Cox},
series = {Progress in Medicinal Chemistry},
publisher = {Elsevier},
volume = {57},
pages = {277-356},
year = {2018},
issn = {0079-6468},
doi = {https://doi.org/10.1016/bs.pmch.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079646817300243},
author = {Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard},
keywords = {Big Data, Artificial intelligence, Drug discovery, Biology, Chemistry, Clinical trials},
abstract = {Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.}
}
@article{ANJUM2018326,
title = {Privacy preserving data by conceptualizing smart cities using MIDR-Angelization},
journal = {Sustainable Cities and Society},
volume = {40},
pages = {326-334},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717314646},
author = {Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq},
keywords = {Big data, IoT data management, Disclosure risk, HIPAA, Patient privacy, Re-identification risk, Smart city},
abstract = {Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as “MIDR-Angelization” to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.}
}
@incollection{DHAESE2018137,
title = {Chapter 13 - Big Data and Deep Brain Stimulation},
editor = {Elliot S. Krames and P. Hunter Peckham and Ali R. Rezai},
booktitle = {Neuromodulation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {137-145},
year = {2018},
isbn = {978-0-12-805353-9},
doi = {https://doi.org/10.1016/B978-0-12-805353-9.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053539000139},
author = {Pierre-Francois D’Haese and Peter E. Konrad and Benoit M. Dawant},
keywords = {Atlas, Big data, Collaborative, CranialCloud, DBS, Normalization},
abstract = {Surgeons, neurologists, researchers, and patients have lacked the technology-based tools to facilitate sharing the tremendously valuable data about patients’ treatment and research in regard to what is working and what is not. Today, only 9% of patients who could benefit from complex therapies to address neurologic conditions actually receive them, and the medical information for each patient who does is hidden away in disconnected databases. To optimize and accelerate our understanding of the brain, we need to gather intelligence around every case, every research subject, every study while connecting that information through a unified, Health Insurance Portability and Accountability Act of 1996 (HIPAA)-compliant network that leverages technology and harnesses the Internet to drive advancements and better connect patients to their care teams. In this chapter, we highlight the key aspects needed to fulfill the requirements of a robust, HIPAA-compliant archive for brain data and highlight the impact of normalization on the accuracy of statistical analyses.}
}
@article{SONG201734,
title = {Data quality management for service-oriented manufacturing cyber-physical systems},
journal = {Computers & Electrical Engineering},
volume = {64},
pages = {34-44},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302099},
author = {Zhiting Song and Yanming Sun and Jiafu Wan and Peipei Liang},
keywords = {Data quality, Cyber-physical systems, Service-oriented manufacturing, Workflow nets},
abstract = {Service-oriented manufacturing (SOM) is a new worldwide manufacturing paradigm, and a cyber-physical system (CPS) is accepted as a strategic choice of SOM enterprises looking to provide bundles of satisfying products and services to customers. The issue of data quality is common in any CPS and poses great challenges to its efficient operation. This paper focuses on defective data generated by the improper operation of physical and cyber components of a service-oriented manufacturing CPS (SMCPS), and develops effective managerial policies to deal with such data. First, formal semantics of workflow nets (WF-nets) are employed to construct process-oriented ontology for the SMCPS. Second, a two-stage optimization model together with algorithms is designed to find optimal policies that balance local and global management objectives. Finally, our model is illustrated through a case. Results show that the proposed control strategy outperforms one-stage control and random control in guaranteeing data quality and saving control costs.}
}
@article{RANJAN2017495,
title = {A note on exploration of IoT generated big data using semantics},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {495-498},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17313912},
author = {Rajiv Ranjan and Dhavalkumar Thakker and Armin Haller and Rajkumar Buyya},
abstract = {Welcome to this special issue of the Future Generation Computer Systems (FGCS) journal. The special issue compiles seven technical contributions that significantly advance the state-of-the-art in exploration of Internet of Things (IoT) generated big data using semantic web techniques and technologies.}
}
@article{ENRIQUEZ201714,
title = {Entity reconciliation in big data sources: A systematic mapping study},
journal = {Expert Systems with Applications},
volume = {80},
pages = {14-27},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417301550},
author = {J.G. Enríquez and F.J. Domínguez-Mayo and M.J. Escalona and M. Ross and G. Staples},
keywords = {Systematic mapping study, Entity reconciliation, Heterogeneous databases, Big data},
abstract = {The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.}
}
@incollection{GROOT2017127,
title = {Chapter 5 - Data Management Tools and Techniques},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {127-177},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000053},
author = {Martijn Groot},
keywords = {data management technology, databases, big data, financial analytics, IT management},
abstract = {In this chapter we look at the technology and tooling available for data management. We start with a discussion on the different components of an IT infrastructure and how to describe them. We provide a short taxonomy of tools from analytics and data distribution to data governance and end-user tools. This is followed by a discussion on data models and storage models, from traditional relational databases to different challenger technologies including NoSQL databases. We discuss data quality management and curation processes and data storage at different scales from data marts and warehouses to data lakes. We then discuss data analytics and big data technologies and what some of the use cases and implications for financial services firms are. After discussing privacy and security aspects, and the promising application areas of blockchain technology in master data, we discuss cloud storage models and what the cloud trend means for banks and asset managers. We end with a discussion on IT sourcing options, IT management, and IT maturity models before concluding with a look ahead.}
}
@article{CLARKE2018467,
title = {Guidelines for the responsible application of data analytics},
journal = {Computer Law & Security Review},
volume = {34},
number = {3},
pages = {467-476},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917303643},
author = {Roger Clarke},
keywords = {Big data, Data science, Data quality, Decision quality, Regulation},
abstract = {The vague but vogue notion of ‘big data’ is enjoying a prolonged honeymoon. Well-funded, ambitious projects are reaching fruition, and inferences are being drawn from inadequate data processed by inadequately understood and often inappropriate data analytic techniques. As decisions are made and actions taken on the basis of those inferences, harm will arise to external stakeholders, and, over time, to internal stakeholders as well. A set of Guidelines is presented, whose purpose is to intercept ill-advised uses of data and analytical tools, prevent harm to important values, and assist organisations to extract the achievable benefits from data, rather than dreaming dangerous dreams.}
}
@article{NIMMAGADDA20171871,
title = {Big Data Guided Design Science Information System (DSIS) Development for Sustainability Management and Accounting},
journal = {Procedia Computer Science},
volume = {112},
pages = {1871-1880},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.233},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316381},
author = {Shastri L Nimmagadda and Torsten Reiners and and {Gary Burke}},
keywords = {Design Science, Digital Ecosystem, Sustainability, Multidimensional Artefacts, Data Interpretation},
abstract = {Sustainability is a dynamic, complex and composite data relationship among geographically distributed human and environment ecosystems. The ecosystems may have strong interactions among their elements and processes, but with dynamic implicit boundaries. Multi-scalable and multidimensional ecosystems have significance based on a commonality of basic structural units and domains. We intend to develop a holistic information system for managing different ecosystems within a sustainability framework/context, using an empirical qualitative and quantitative interpretation and analysis of the measured observations. Design Science Research (DSR) approach is aimed at developing an information system using the volumes of unstructured Big Data observations. Collaborating multiple domains, interpreting and evaluating the commonality, uncovering the connectivity among multiple systems are key aspects of the study. The Design Science Information System (DSIS), evolved from DSR approach is used in solving the ecosystem issues associated with multiple domains, in which the sustainability challenges manifest. In this context, we propose a human-environment-economic ecosystem (HEES) framework consisting of human, environment and economic elements and processes. In broad terms, human, environment and economic domains are conceptualized as different players/agents that operate within a range of sustainability scenarios. This approach recognizes the existing constraints of the systems as well as the emerging knowledge of the boundaries of ecosystems and their connectivity. The connectivity and interaction among the systems are analyzed by data mining, visualization and interpretation artefacts within a sustainability policy framework.}
}
@article{WANG2018440,
title = {Research on the Theory and Method of Grid Data Asset Management},
journal = {Procedia Computer Science},
volume = {139},
pages = {440-447},
year = {2018},
note = {6th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.258},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319288},
author = {Jun Wang and Yun-si Li and Wei Song and Ai-hua Li},
keywords = {big data, grid data asset, asset management, data governance},
abstract = {In the era of Big Data, data assets have become a strategic resource which cannot be overlooked by both society and enterprises. However, data is not equal to the assets. This paper first introduces the necessary conditions of data assetization and discriminates the concepts of data governance, data management and data asset management. Then it focuses on the unique connotation and characteristics of grid data assets. With reference to the mainstream theory of data management, the framework for the grid data asset management is set up in the combination of the characteristics of data assets, business needs and the actual situation in the power supply enterprises. Finally, this paper puts forward higher system requirements and technical requirements for China’s power supply enterprises to conduct data asset management.}
}
@article{DUVIER2018196,
title = {Data quality challenges in the UK social housing sector},
journal = {International Journal of Information Management},
volume = {38},
number = {1},
pages = {196-200},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216308222},
author = {Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens},
keywords = {Social housing, Data quality},
abstract = {The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.}
}
@incollection{OLANIYAN2023275,
title = {Chapter 17 - New trends in deep learning for neuroimaging analysis and disease prediction},
editor = {Ajith Abraham and Sujata Dash and Subhendu Kumar Pani and Laura García-Hernández},
booktitle = {Artificial Intelligence for Neurological Disorders},
publisher = {Academic Press},
pages = {275-287},
year = {2023},
isbn = {978-0-323-90277-9},
doi = {https://doi.org/10.1016/B978-0-323-90277-9.00012-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323902779000122},
author = {Olugbemi T. Olaniyan and Charles O. Adetunji and Ayobami Dare and Olorunsola Adeyomoye and Mayowa J. Adeniyi and Alex Enoch},
keywords = {Deep learning, Diagnosis, Neuroimaging, Disease prediction, Cognitive functions},
abstract = {In the last few decades, deep learning techniques for diagnosing and predicting disease conditions from neuroimaging have attracted much attention and interest from the scientific community. Big data and artificial intelligence approaches and innovations are currently being utilized to generate large datasets from images, text, sounds, graphs, and signals. New trends in the utilization of deep learning for disease prediction in neurology, oncology, cardiology, and other areas entail converting patient electronic health records, biological system information, physiological signals, biomarkers, and biomedical images to cognitive functions. The current trends in deep learning techniques focus on utilizing neuroimaging analysis to evaluate alterations in local morphological topographies of different brain sub-regions and then predict novel disorder-linked brain patterns. Hence, this chapter presents a detailed overview of different approaches in deep learning for the prediction of major brain diseases such as mild cognitive impairment, Alzheimer's disease, brain tumors, depressive disorders, traumatic brain injury, schizophrenia, Parkinson's disease, autism spectrum disease, attention-deficit hyperactivity disorder, epilepsy, stroke, multiple sclerosis, and more. The chapter also discusses the current challenges of utilizing deep learning in assessing brain disorders in neuroimaging data.}
}
@article{MINET2017126,
title = {Crowdsourcing for agricultural applications: A review of uses and opportunities for a farmsourcing approach},
journal = {Computers and Electronics in Agriculture},
volume = {142},
pages = {126-138},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917300479},
author = {Julien Minet and Yannick Curnel and Anne Gobin and Jean-Pierre Goffart and François Mélard and Bernard Tychon and Joost Wellens and Pierre Defourny},
keywords = {Crowdsourcing, Citizen science, Smart farming, Participatory approaches, Big data, ICT, Data collection},
abstract = {Crowdsourcing, understood as outsourcing tasks or data collection by a large group of non-professionals, is increasingly used in scientific research and operational applications. In this paper, we reviewed crowdsourcing initiatives in agricultural science and farming activities and further discussed the particular characteristics of this approach in the field of agriculture. On-going crowdsourcing initiatives in agriculture were analysed and categorised according to their crowdsourcing component. We identified eight types of agricultural data and information that can be generated from crowdsourcing initiatives. Subsequently we described existing methods of quality control of the crowdsourced data. We analysed the profiles of potential contributors in crowdsourcing initiatives in agriculture, suggested ways for increasing farmers’ participation, and discussed the on-going initiatives in the light of their target beneficiaries. While crowdsourcing is reported to be an efficient way of collecting observations relevant to environmental monitoring and contributing to science in general, we pointed out that crowdsourcing applications in agriculture may be hampered by privacy issues and other barriers to participation. Close connections with the farming sector, including extension services and farm advisory companies, could leverage the potential of crowdsourcing for both agricultural research and farming applications. This paper coins the term of farmsourcing asa professional crowdsourcing strategy in farming activities and provides a source of recommendations and inspirations for future collaborative actions in agricultural crowdsourcing.}
}
@article{NG2017939,
title = {A Master Data Management Solution to Unlock the Value of Big Infrastructure Data for Smart, Sustainable and Resilient City Planning},
journal = {Procedia Engineering},
volume = {196},
pages = {939-947},
year = {2017},
note = {Creative Construction Conference 2017, CCC 2017, 19-22 June 2017, Primosten, Croatia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.034},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817331569},
author = {S. Thomas Ng and Frank J. Xu and Yifan Yang and Mengxue Lu},
keywords = {Big data, integrated infrastructure asset management, master data management, smart city, smart infrastructure},
abstract = {In recent years, many governments have launched various smart city or smart infrastructure initiatives to improve the quality of citizens’ life and help city managers / planners optimize the operation and management of urban infrastructures. By deploying internet of things (IoT) to infrastructure systems, high-volume and high-variety of data pertinent to the condition and performance of infrastructure systems along with the behaviors of citizens can be gathered, processed, integrated and analyzed through cloud-based infrastructure asset management systems, ubiquitous mobile applications and big data analytics platforms. Nonetheless, how to fully exploit the value of ‘big infrastructure data’ is still a key challenge facing most stakeholders. Unless data is shared by different infrastructure systems in an interoperable and consistent manner, it is difficult to realize the smart infrastructure concept for efficient smart city planning, not to mention about developing appropriate resilience and sustainable programs. To unlock the value of big infrastructure data for smart, sustainable and resilient city planning, a master data management (MDM) solution is proposed in this paper. MDM has been adopted in the business sector to orchestrate operational and analytical big data applications. In order to derive a suitable MDM solution for smart, sustainable and resilient city planning, commercial and open source MDM systems, smart city standards, smart city concept models, smart community infrastructure frameworks, semantic web technologies will be critically reviewed, and feedback and requirements will be gathered from experts who are responsible for developing smart, sustainable and resilient city programs. A case study which focuses on the building and transportation infrastructures of a selected community in Hong Kong will be conducted to pilot the proposed MDM solution.}
}
@incollection{BERMAN2018395,
title = {19 - Legalities},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {395-417},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00019-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000194},
author = {Jules J. Berman},
keywords = {Data Quality Act, Freedom of Information Act, FOIA, Limited Data Use Agreements, and Madey v. Duke, Tort, Patents, Intellectual property, Informed consent, Data ownership, Copyright, Infringement, Fair use},
abstract = {Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly: copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of a Big Data resource. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) obtaining the rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four issues will never go away.}
}
@article{WANG201714,
title = {GSA: Genome Sequence Archive*},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {15},
number = {1},
pages = {14-18},
year = {2017},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1672022917300025},
author = {Yanqing Wang and Fuhai Song and Junwei Zhu and Sisi Zhang and Yadong Yang and Tingting Chen and Bixia Tang and Lili Dong and Nan Ding and Qian Zhang and Zhouxian Bai and Xunong Dong and Huanxin Chen and Mingyuan Sun and Shuang Zhai and Yubin Sun and Lei Yu and Li Lan and Jingfa Xiao and Xiangdong Fang and Hongxing Lei and Zhang Zhang and Wenming Zhao},
keywords = {Genome Sequence Archive, GSA, Big data, Raw sequence data, INSDC},
abstract = {With the rapid development of sequencing technologies towards higher throughput and lower cost, sequence data are generated at an unprecedentedly explosive rate. To provide an efficient and easy-to-use platform for managing huge sequence data, here we present Genome Sequence Archive (GSA; http://bigd.big.ac.cn/gsa or http://gsa.big.ac.cn), a data repository for archiving raw sequence data. In compliance with data standards and structures of the International Nucleotide Sequence Database Collaboration (INSDC), GSA adopts four data objects (BioProject, BioSample, Experiment, and Run) for data organization, accepts raw sequence reads produced by a variety of sequencing platforms, stores both sequence reads and metadata submitted from all over the world, and makes all these data publicly available to worldwide scientific communities. In the era of big data, GSA is not only an important complement to existing INSDC members by alleviating the increasing burdens of handling sequence data deluge, but also takes the significant responsibility for global big data archive and provides free unrestricted access to all publicly available data in support of research activities throughout the world.}
}
@article{AZEROUAL201850,
title = {Analyzing data quality issues in research information systems via data profiling},
journal = {International Journal of Information Management},
volume = {41},
pages = {50-56},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218300975},
author = {Otmane Azeroual and Gunter Saake and Eike Schallehn},
keywords = {Current research information systems, CRIS, Research information systems, RIS, Research information, Data sources, Data quality, Extraction transformation load, ETL, Data analysis, Data profiling, Science system, Standardization},
abstract = {The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.}
}
@article{LIN2018293,
title = {DTRM: A new reputation mechanism to enhance data trustworthiness for high-performance cloud computing},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {293-302},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17317247},
author = {Hui Lin and Jia Hu and Chuanfeng Xu and Jianfeng Ma and Mengyang Yu},
keywords = {Cloud computing, Reputation mechanism, Trustworthiness, Data veracity},
abstract = {Cloud computing and the mobile Internet have been the two most influential information technology revolutions, which intersect in mobile cloud computing (MCC). The burgeoning MCC enables the large-scale collection and processing of big data, which demand trusted, authentic, and accurate data to ensure an important but often overlooked aspect of big data — data veracity. Troublesome internal attacks launched by internal malicious users is one key problem that reduces data veracity and remains difficult to handle. To enhance data veracity and thus improve the performance of big data computing in MCC, this paper proposes a Data Trustworthiness enhanced Reputation Mechanism (DTRM) which can be used to defend against internal attacks. In the DTRM, the sensitivity-level based data category, Metagraph theory based user group division, and reputation transferring methods are integrated into the reputation query and evaluation process. The extensive simulation results based on real datasets show that the DTRM outperforms existing classic reputation mechanisms under bad mouthing attacks and mobile attacks.}
}
@article{CHE2023513,
title = {Impacts of pollution heterogeneity on population exposure in dense urban areas using ultra-fine resolution air quality data},
journal = {Journal of Environmental Sciences},
volume = {125},
pages = {513-523},
year = {2023},
issn = {1001-0742},
doi = {https://doi.org/10.1016/j.jes.2022.02.041},
url = {https://www.sciencedirect.com/science/article/pii/S1001074222001061},
author = {Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau},
keywords = {Particulate matter, Nitrogen dioxide, Ozone, Pollution heterogeneity, Urban area},
abstract = {Traditional air quality data have a spatial resolution of 1 km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1 km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.}
}
@article{HUANG20181413,
title = {Improving Quality of Experience in multimedia Internet of Things leveraging machine learning on big data},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1413-1423},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.02.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17324500},
author = {Xiaohong Huang and Kun Xie and Supeng Leng and Tingting Yuan and Maode Ma},
keywords = {Data fusion, Multimedia Internet of Things, Big data, Quality of Experience, Machine learning, Neural network},
abstract = {With rapid evolution of the Internet of Things (IoT) applications on multimedia, there is an urgent need to enhance the satisfaction level of Multimedia IoT (MIoT) network users. An important and unsolved problem is automatic optimization of Quality of Experience (QoE) through collecting/managing/processing various data from MIoT network. In this paper, we propose an MIoT QoE optimization mechanism leveraging data fusion technology, called QoE optimization via Data Fusion (QoEDF). QoEDF consists of two steps. Firstly, a multimodal data fusion approach is proposed to build a QoE mapping between the uncontrollable user data with the controllable network-related system data. Secondly, an automatic QoE optimization model is built taking fused results, which is different from the traditional way. QoEDF is able to adjust network-related system data automatically so as to achieve optimized user satisfaction. Simulation results show that QoEDF will lead to significant improvements in QoE level as well as be adaptable to dynamic network changes.}
}
@article{SIRGO2018166,
title = {Validation of the ICU-DaMa tool for automatically extracting variables for minimum dataset and quality indicators: The importance of data quality assessment},
journal = {International Journal of Medical Informatics},
volume = {112},
pages = {166-172},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618300443},
author = {Gonzalo Sirgo and Federico Esteban and Josep Gómez and Gerard Moreno and Alejandro Rodríguez and Lluis Blanch and Juan José Guardiola and Rafael Gracia and Lluis {De Haro} and María Bodí},
keywords = {Electronic medical record, Quality indicators, Critical care, Information processing, Data quality, Verification},
abstract = {Background
Big data analytics promise insights into healthcare processes and management, improving outcomes while reducing costs. However, data quality is a major challenge for reliable results. Business process discovery techniques and an associated data model were used to develop data management tool, ICU-DaMa, for extracting variables essential for overseeing the quality of care in the intensive care unit (ICU).
Objective
To determine the feasibility of using ICU-DaMa to automatically extract variables for the minimum dataset and ICU quality indicators from the clinical information system (CIS).
Methods
The Wilcoxon signed-rank test and Fisher’s exact test were used to compare the values extracted from the CIS with ICU-DaMa for 25 variables from all patients attended in a polyvalent ICU during a two-month period against the gold standard of values manually extracted by two trained physicians. Discrepancies with the gold standard were classified into plausibility, conformance, and completeness errors.
Results
Data from 149 patients were included. Although there were no significant differences between the automatic method and the manual method, we detected differences in values for five variables, including one plausibility error and two conformance and completeness errors. Plausibility: 1) Sex, ICU-DaMa incorrectly classified one male patient as female (error generated by the Hospital’s Admissions Department). Conformance: 2) Reason for isolation, ICU-DaMa failed to detect a human error in which a professional misclassified a patient’s isolation. 3) Brain death, ICU-DaMa failed to detect another human error in which a professional likely entered two mutually exclusive values related to the death of the patient (brain death and controlled donation after circulatory death). Completeness: 4) Destination at ICU discharge, ICU-DaMa incorrectly classified two patients due to a professional failing to fill out the patient discharge form when thepatients died. 5) Length of continuous renal replacement therapy, data were missing for one patient because the CRRT device was not connected to the CIS.
Conclusions
Automatic generation of minimum dataset and ICU quality indicators using ICU-DaMa is feasible. The discrepancies were identified and can be corrected by improving CIS ergonomics, training healthcare professionals in the culture of the quality of information, and using tools for detecting and correcting data errors.}
}
@article{YU20171,
title = {Data pricing strategy based on data quality},
journal = {Computers & Industrial Engineering},
volume = {112},
pages = {1-10},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217303509},
author = {Haifei Yu and Mengxiao Zhang},
keywords = {Big data, Data marketplace, Data pricing, Production management, Bi-level programming model},
abstract = {This paper presents a bi-level mathematical programming model for the data-pricing problem that considers both data quality and data versioning strategies. Data products and data-related services differ from information products or services in terms of quality assessment methods. For this problem, we consider two aspects of data quality: (1) its multidimensionality and (2) the interaction between the dimensions. We designed a multi-version data strategy and propose a data-pricing bi-level programming model based on the data quality to maximize the profit by the owner of the data platform and the utility to consumers. A genetic algorithm was used to solve the model. The numerical solutions for the data-pricing model indicate that the multi-version strategy achieves a better market segmentation and is more profitable and feasible when the multiple dimensions of data quality are considered. These results also provide managerial guidance on data provision and data pricing for platform owners.}
}
@article{GIL201761,
title = {Big Data. New approaches of modelling and management},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {61-63},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917301022},
author = {David Gil and Il-Yeol Song and José F. Aldana and Juan Trujillo},
abstract = {Nowadays, there are a huge number of autonomous and diverse information sources providing heterogeneous data. Sensors, social media data, data on the Web, open data, just to name a few, resulting in a major confluence of Big Data. In this survey, we discuss these diverse data sources and detail the way in which data are acquired, stored, processed and analysed. Although some of the opportunities in this new state are mentioned, the main objective of this analysis is to present the challenges for Big Data. To accomplish this goal, we examine the new proposals and approaches presented in this special issue with the aim of establishing new models for improving the management of the volume, velocity, and variety, of Big Data. Some of these schemes establish the use of Ontologies, Semantic Processing, Cloud Computing and Data Management and could be seen as intelligent services integrated as context-aware services.}
}
@article{JEFFREYKUO2018120,
title = {Analyze the energy consumption characteristics and affecting factors of Taiwan's convenience stores-using the big data mining approach},
journal = {Energy and Buildings},
volume = {168},
pages = {120-136},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2018.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817334345},
author = {Chung-Feng {Jeffrey Kuo} and Chieh-Hung Lin and Ming-Hao Lee},
keywords = {Convenience store, Data mining, Machine learning, Energy consumption characteristics, Energy consumption affecting factor},
abstract = {This study applies big data mining, machine learning analysis technique and uses the Waikato Environment for Knowledge Analysis (WEKA) as a tool to discuss the convenience stores energy consumption performance in Taiwan which consists of (a). Influential factors of architectural space environment and geographical conditions; (b). Influential factors of management type; (c). Influential factors of business equipment; (d). Influential factors of local climatic conditions; (e). Influential factors of service area socioeconomic conditions. The survey data of 1,052 chain convenience stores belong to 7-Eleven, Family Mart and Hi-Life groups by Taiwan Architecture and Building Center (TABC) in 2014. The implicit knowledge will be explored in order to improve the traditional analysis technique which is unlikely to build a model for complex, inexact and uncertain dynamic energy consumption system for convenience stores. The analysis process comprises of (a). Problem definition and objective setting; (b). Data source selection; (c). Data collection; (d). Data preprocessing/preparation; (e). Data attributes selection; (f). Data mining and model construction; (g). Results analysis and evaluation; (h). Knowledge discovery and dissemination. The key factors influencing the convenience stores energy consumption and the influence intensity order can be explored by data attributes selection. The numerical prediction model for energy consumption is built by applying regression analysis and classification techniques. The optimization thresholds of various influential factors are obtained. The different cluster data are compared by using clustering analysis to verify the correlation between the factors influencing the convenience stores energy consumption characteristic. The implicit knowledge of energy consumption characteristic obtained by the aforesaid analysis can be used to (a). Provide the owners with accurate predicted energy consumption performance to optimize architectural space, business equipment and operations management mode; (b). The design planners can obtain the optimum design proposal of Cost Performance Ratio (C/P) by planning the thresholds of various key factors and the validation of prediction model; (c). Provide decision support for government energy and environment departments, to make energy saving and carbon emission reduction policies, in order to estimate and set the energy consumption scenarios of convenience store industry.}
}
@article{CHOW2017455,
title = {Internet-based computer technology on radiotherapy},
journal = {Reports of Practical Oncology & Radiotherapy},
volume = {22},
number = {6},
pages = {455-462},
year = {2017},
issn = {1507-1367},
doi = {https://doi.org/10.1016/j.rpor.2017.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1507136716301602},
author = {James C.L. Chow},
keywords = {Radiotherapy, Computer technology, Cloud computing, Machine learning, Big data},
abstract = {Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.}
}
@article{SUN20171,
title = {Special Issue on Scalable Computing Systems for Big Data Applications},
journal = {Journal of Parallel and Distributed Computing},
volume = {108},
pages = {1-2},
year = {2017},
note = {Special Issue on Scalable Computing Systems for Big Data Applications},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517301776},
author = {Xian-He Sun and Marc Frincu and Charalampos Chelmis}
}
@article{MARSDEN2018A1,
title = {Numerical data quality in IS research and the implications for replication},
journal = {Decision Support Systems},
volume = {115},
pages = {A1-A7},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618301647},
author = {James R. Marsden and David E. Pingry},
abstract = {We argue that there are major, persistent numerical data quality issues in IS academic research. These issues undermine the ability to replicate our research – a critical element of scientific investigation and analysis. In IS empirical and analytics research articles, the amount of space devoted to the details of data collection, validation, and/or quality pales in comparison to the space devoted to the evaluation and selection of relatively sophisticated model form(s) and estimation technique(s). Yet erudite modeling and estimation can yield no immediate value or be meaningfully replicated without high quality data inputs. The purpose of this paper is: 1) to detail potential quality issues with data types currently used in IS research, and 2) to start a wider and deeper discussion of data quality in IS research. No data type is inherently of low quality and no data type guarantees high quality. As researchers, our empirical research must always address data quality issues and provide the information necessary to determine What, When, Where, How, Who, and Which.}
}
@article{HAMMER2017715,
title = {Profit Per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure},
journal = {Procedia CIRP},
volume = {63},
pages = {715-720},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.094},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117302408},
author = {Markus Hammer and Ken Somers and Hugo Karre and Christian Ramsauer},
keywords = {Operations Management, Manufacturing Systems 4.0, Profit per Hour, Advanced Process Control, Big Data Analytics, Agile Manufacturing},
abstract = {The rise of Industry 4.0 and in particular Big Data analytics of production parameters offers exciting new ways for optimization. The majority of factories in process industries currently aim for example, either for output maximization, yield increase, or cost reduction. The availability of real-time data and online processing capability with advanced algorithms enables a profit per hour operational management approach. Profit per hour as a target control metric allows running factories at the optimal available operating point taking all revenue and cost drivers into account. This paper describes the suitability of profit per hour as a target process control parameter for production in process industries. The authors explain how this management approach helps to make better operational decisions, trading off yield, energy, throughput, among other factors, and the resulting cumulative benefits. They also lay out how Big Data and advanced algorithms are the key enabler to this new approach, as well as a standardized methodology for implementation. With profit per hour an agile control approach is presented which aims to optimize the performance of industrial manufacturing systems in a world of ever increasing volatility.}
}
@incollection{CHESSELL201733,
title = {Chapter 3 - Architecting to Deliver Value From a Big Data and Hybrid Cloud Architecture},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {33-48},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805467300003X},
author = {Mandy Chessell and Dan Wolfson and Tim Vincent},
keywords = {Enterprise architecture, Self-service data, Systems of insight, Data-driven security, Business-driven governance, Trust and confidence, Hybrid-cloud, Information supply chains},
abstract = {Big data and analytics, particularly when combined with the use of cloud-based deployments, can transform the operation of an organization – increasing innovation, improving time to value and decision-making. However, an organization only derives value from data and analytics when (1) the collection of big data is organized, systematic and automated and (2) the use of data and analytic insight is embedded in the organization's day-to-day operation. Often the ambition of a big data and analytics solution requires data to flow freely across an organization. This can be in direct conflict with the organization's political and process silos that exist to partition the work of the organization into manageable chunks of function and responsibility. Thus the architecture of a big data solution must accommodate the realities within the organization to ensure sufficient value is realized by all of the stakeholders that are needed to enable this data interchange. Through examples of architectures for big data and analytics solutions, we explain how the scope of a big data solution can affect its architecture and the additional components necessary when a big data solution needs to span multiple organization silos.}
}
@article{WANG2018139,
title = {Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data},
journal = {Computers & Geosciences},
volume = {111},
pages = {139-147},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S009830041730609X},
author = {Ruo-Qian Wang and Huina Mao and Yuan Wang and Chris Rae and Wesley Shaw},
abstract = {Hyper-resolution datasets for urban flooding are rare. This problem prevents detailed flooding risk analysis, urban flooding control, and the validation of hyper-resolution numerical models. We employed social media and crowdsourcing data to address this issue. Natural Language Processing and Computer Vision techniques are applied to the data collected from Twitter and MyCoast (a crowdsourcing app). We found these big data based flood monitoring approaches can complement the existing means of flood data collection. The extracted information is validated against precipitation data and road closure reports to examine the data quality. The two data collection approaches are compared and the two data mining methods are discussed. A series of suggestions is given to improve the data collection strategy.}
}
@article{LLAVE2018516,
title = {Data lakes in business intelligence: reporting from the trenches},
journal = {Procedia Computer Science},
volume = {138},
pages = {516-524},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918317046},
author = {Marilex Rea Llave},
keywords = {Business intelligence, big data, data lake, BI architecture},
abstract = {The data lake approach has emerged as a promising way to handle large volumes of structured and unstructured data. This big data technology enables enterprises to profoundly improve their Business Intelligence. However, there is a lack of empirical studies on the use of the data lake approach in enterprises. This paper provides the results of an exploratory study designed to improve the understanding of the use of the data lake approach in enterprises. I interviewed 12 experts who had implemented this approach in various enterprises and identified three important purposes of implementing data lakes: (1) as staging areas or sources for data warehouses, (2) as a platform for experimentation for data scientists and analysts, and (3) as a direct source for self-service business intelligence. The study also identifies several perceived benefits and challenges of the data lake approach. The results may be beneficial for both academics and practitioners. Further, suggestions for future research is presented.}
}
@article{NGUYEN2018254,
title = {Big data analytics in supply chain management: A state-of-the-art literature review},
journal = {Computers & Operations Research},
volume = {98},
pages = {254-264},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0305054817301685},
author = {Truong Nguyen and Li ZHOU and Virginia Spiegler and Petros Ieromonachou and Yong Lin},
keywords = {Literature review, Big data, Big data analytics, Supply chain management, Research directions},
abstract = {The rapidly growing interest from both academics and practitioners in the application of big data analytics (BDA) in supply chain management (SCM) has urged the need for review of up-to-date research development in order to develop a new agenda. This review responds to the call by proposing a novel classification framework that provides a full picture of current literature on where and how BDA has been applied within the SCM context. The classification framework is structurally based on the content analysis method of Mayring (2008), addressing four research questions: (1) in what areas of SCM is BDA being applied? (2) At what level of analytics is BDA used in these SCM areas? (3) What types of BDA models are used in SCM? (4) What BDA techniques are employed to develop these models? The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.}
}
@article{DEKHTIAR2018227,
title = {Deep learning for big data applications in CAD and PLM – Research review, opportunities and case study},
journal = {Computers in Industry},
volume = {100},
pages = {227-243},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517305560},
author = {Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis},
keywords = {Deep learning, Machine learning, Computer vision, Product Lifecycle Management, Digital mock-up, Shape retrieval},
abstract = {With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.}
}