@article{FOSSOWAMBA2015234,
title = {How ‘big data’ can make big impact: Findings from a systematic review and a longitudinal case study},
journal = {International Journal of Production Economics},
volume = {165},
pages = {234-246},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004253},
author = {Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou},
keywords = {‘Big data’, Analytics, Business value, Issues, Case study, Emergency services, Literature review},
abstract = {Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.}
}
@incollection{SIMON201577,
title = {Chapter 8 - Considerations for the Big Data Era},
editor = {Alan Simon},
booktitle = {Enterprise Business Intelligence and Data Warehousing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {77-82},
year = {2015},
isbn = {978-0-12-801540-7},
doi = {https://doi.org/10.1016/B978-0-12-801540-7.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128015407000081},
author = {Alan Simon},
keywords = {Big Data, business intelligence, BI, analytics, predictive analytics, program manager, program management, project manager, project management, challenges, data warehouse, data warehousing, enterprise data warehouse, EDW},
abstract = {The Big Data era is creating seismic shifts in how we approach enterprise business intelligence and data warehousing. This final chapter discusses considerations related to technology and architecture, analytics-oriented requirements collection, and organizational structure.}
}
@article{MERINO2016123,
title = {A Data Quality in Use model for Big Data},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {123-130},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003817},
author = {Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini},
keywords = {Data Quality, Big Data, Measurement, Quality-in-Use, Model},
abstract = {Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution–business value–of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the “3As Data Quality-in-Use model”, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.}
}
@article{WANG2016747,
title = {Towards felicitous decision making: An overview on challenges and trends of Big Data},
journal = {Information Sciences},
volume = {367-368},
pages = {747-765},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516304868},
author = {Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu},
keywords = {Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science},
abstract = {The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.}
}
@article{ADDOTENKORANG2016528,
title = {Big data applications in operations/supply-chain management: A literature review},
journal = {Computers & Industrial Engineering},
volume = {101},
pages = {528-543},
year = {2016},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2016.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S0360835216303631},
author = {Richard Addo-Tenkorang and Petri T. Helo},
keywords = {Big data – applications and analysis, Internet of Things (IoT), Cloud computing, Master database management, Operations/supply-chain management},
abstract = {Purpose
Big data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of “big data.” Therefore, this paper attempts to thoroughly investigate “big data,” its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of “big data” and its extension into “big data II”/IoT–value-adding perspectives by proposing a value-adding framework.
Methodology/research approach
The research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of “big data” applications between 2010 and 2015.
Findings/results
The four main attributes or factors identified with “big data” include – big data development sources (Variety – V1), big data acquisition (Velocity – V2), big data storage (Volume – V3), and finally big data analysis (Veracity – V4). However, the study of “big data” has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding – V5) – “Big Data cloud computing perspective/Internet of Things (IoT)”. Hence, the four Vs of “big data” is now expanded into five Vs.
Originality/value of research
This paper presents original literature review research discussing “big data” issues, trends and perspectives in operations/supply-chain management in order to propose “Big data II” (IoT – Value-adding) framework. This proposed framework is supposed or assumed to be an extension of “big data” in a value-adding perspective, thus proposing that “big data” be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.}
}
@article{SHAHA2016725,
title = {Big Data, Big Problems: Incorporating Mission, Values, and Culture in Provider Affiliations},
journal = {Orthopedic Clinics of North America},
volume = {47},
number = {4},
pages = {725-732},
year = {2016},
note = {Sports-Related Injuries},
issn = {0030-5898},
doi = {https://doi.org/10.1016/j.ocl.2016.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0030589816300554},
author = {Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh},
keywords = {Big data, Comparative effectiveness, Orthopedics, Total joint arthroplasty, Administrative database, Clinical database}
}
@article{HASHEM201598,
title = {The rise of “big data” on cloud computing: Review and open research issues},
journal = {Information Systems},
volume = {47},
pages = {98-115},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001288},
author = {Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Nor Badrul Anuar and Salimah Mokhtar and Abdullah Gani and Samee {Ullah Khan}},
keywords = {Big data, Cloud computing, Hadoop},
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.}
}
@article{WANG2015782,
title = {Privacy trust crisis of personal data in China in the era of Big Data: The survey and countermeasures},
journal = {Computer Law & Security Review},
volume = {31},
number = {6},
pages = {782-792},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267364915001296},
author = {Zhong Wang and Qian Yu},
keywords = {Personal data, Privacy trust, Questionnaires, Interview, Big data},
abstract = {Privacy trust directly affects the personal willingness to share data and thus influences the quality and size of the data, thus affecting the development of big data technology and industry. As China is probably the largest personal data pool and vastest application market of big data, the situation of Chinese privacy trust plays a significant role. Based on the 17 most common data collection scenarios, the following aspects have been observed through 508 questionnaires and interviews of 20 samples. To start with, there is a severe privacy trust crisis in China, both in the field of enterprise services such as online shopping and social networks, etc. and in some public services like medical care and education, etc. Besides, there are also doubts about data collected by the government since individuals refuse to offer personal information or give false information as much as possible. Some people even buy two phone numbers, one is in use, while the other is not carried around or used by them, which is only bought to be offered to data collectors. Secondly, in terms of gender, females have lower trust in enterprises and social associations than males, especially in the fields of social networks and personal consumption. However, there is no obvious difference in fields of government and public services. Females possess stronger awareness but less skilled in precautions than males. Thirdly, people between the ages of 18 and 50 are more suspicious of data collected by enterprises, while age exerts little obvious influence on the credibility of data collected by the government, social associations and public services. Older people are less aware of precautions than people at other ages. In addition, from the perspective of education background, people with higher degrees possess stronger awareness of precautions and thus lower degree of trust. Therefore, it is suggested that more education on privacy consciousness should be given, and relative laws as well as regulations need improving. Besides, innovation in privacy protection technologies should be encouraged. What is more, we need to reinforce the management of the internet industry and strictly regulate personal data collection of the government.}
}
@article{KNEPPER20151504,
title = {Big Data on Ice: The Forward Observer System for In-flight Synthetic Aperture Radar Processing},
journal = {Procedia Computer Science},
volume = {51},
pages = {1504-1513},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.340},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915011485},
author = {Richard Knepper and Matthew Standish and Matthew Link},
keywords = {Big Data, Network Filesystems, Synthetic Aperture Radar, Ice Sheet Data},
abstract = {We introduce the Forward Observer system, which is designed to provide data assurance in field data acquisition while receiving significant amounts (several terabytes per flight) of Synthetic Aperture Radar data during flights over the polar regions, which provide unique requirements for developing data collection and processing systems. Under polar conditions in the field and given the difficulty and expense of collecting data, data retention is absolutely critical. Our system provides a storage and analysis cluster with software that connects to field instruments via standard protocols, replicates data to multiple stores automatically as soon as it is written, and provides pre-processing of data so that initial visualizations are available immediately after collection, where they can provide feedback to researchers in the aircraft during the flight.}
}
@incollection{KRISHNAN2013199,
title = {Chapter 10 - Integration of Big Data and Data Warehousing},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {199-217},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000106},
author = {Krish Krishnan},
keywords = {Big Data, Big Data appliances, Hadoop, NoSQL, RDBMS, data virtualization, semantic framework},
abstract = {The focus of this chapter is to discuss the integration of Big Data and the data warehouse, the possible techniques and pitfalls, and where we leverage a technology. How do we deal with complexity and heterogeneity of technologies? What are the performance and scalabilities of each technology, and how can we sustain performance for the new environment?}
}
@article{HAZEN2016592,
title = {Big data and predictive analytics for supply chain sustainability: A theory-driven research agenda},
journal = {Computers & Industrial Engineering},
volume = {101},
pages = {592-598},
year = {2016},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2016.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S036083521630225X},
author = {Benjamin T. Hazen and Joseph B. Skipper and Jeremy D. Ezell and Christopher A. Boone},
keywords = {Big data, Predictive analytics, Supply chain management},
abstract = {Big data and predictive analytics (BDPA) tools and methodologies are leveraged by businesses in many ways to improve operational and strategic capabilities, and ultimately, to positively impact corporate financial performance. BDPA has become crucial for managing supply chain functions, where data intensive processes can be vastly improved through its effective use. BDPA has also become a competitive necessity for the management of supply chains, with practitioners and scholars focused almost entirely on how BDPA is used to increase economic measures of performance. There is limited understanding, however, as to how BDPA can impact other aspects of the triple bottom-line, namely environmental and social sustainability outcomes. Indeed, this area is in immediate need of attention from scholars in many fields including industrial engineering, supply chain management, information systems, business analytics, as well as other business and engineering disciplines. The purpose of this article is to motivate such research by proposing an agenda based in well-established theory. This article reviews eight theories that can be used by researchers to examine and clarify the nature of BDPA’s impact on supply chain sustainability, and presents research questions based upon this review. Scholars can leverage this article as the basis for future research activity, and practitioners can use this article as a means to understand how company-wide BDPA initiatives might impact measures of supply chain sustainability.}
}
@article{PHILIPCHEN2014314,
title = {Data-intensive applications, challenges, techniques and technologies: A survey on Big Data},
journal = {Information Sciences},
volume = {275},
pages = {314-347},
year = {2014},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514000346},
author = {C.L. {Philip Chen} and Chun-Yang Zhang},
keywords = {Big Data, Data-intensive computing, e-Science, Parallel and distributed computing, Cloud computing},
abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore’s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.}
}
@article{YANG20141563,
title = {A spatiotemporal compression based approach for efficient big data processing on Cloud},
journal = {Journal of Computer and System Sciences},
volume = {80},
number = {8},
pages = {1563-1583},
year = {2014},
note = {Special Issue on Theory and Applications in Parallel and Distributed Computing Systems},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2014.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S002200001400066X},
author = {Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen},
keywords = {Big data, Graph data, Spatiotemporal compression, Cloud computing, Scheduling},
abstract = {It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.}
}
@article{BARASH201510,
title = {Harnessing big data for precision medicine: A panel of experts elucidates the data challenges and proposes key strategic decisions points},
journal = {Applied & Translational Genomics},
volume = {4},
pages = {10-13},
year = {2015},
issn = {2212-0661},
doi = {https://doi.org/10.1016/j.atg.2015.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212066115000046},
author = {Carol Isaacson Barash and Keith O. Elliston and W. {Andrew Faucett} and Jonathan Hirsch and Gauri Naik and Alice Rathjen and Grant Wood},
abstract = {A group of disparate translational bioinformatics experts convened at the 6th Annual Precision Medicine Partnership Meeting, October 29–30, 2014 to discuss big data challenges and key strategic decisions needed to advance precision medicine, emerging solutions, and the anticipated path to success. This article reports the panel discussion.}
}
@article{ROMERO2015336,
title = {Tuning small analytics on Big Data: Data partitioning and secondary indexes in the Hadoop ecosystem},
journal = {Information Systems},
volume = {54},
pages = {336-356},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001458},
author = {Oscar Romero and Victor Herrero and Alberto Abelló and Jaume Ferrarons},
keywords = {Big Data, OLAP, Multidimensional model, Indexes, Partitioning, Cost estimation},
abstract = {In the recent years the problems of using generic storage (i.e., relational) techniques for very specific applications have been detected and outlined and, as a consequence, some alternatives to Relational DBMSs (e.g., HBase) have bloomed. Most of these alternatives sit on the cloud and benefit from cloud computing, which is nowadays a reality that helps us to save money by eliminating the hardware as well as software fixed costs and just pay per use. On top of this, specific querying frameworks to exploit the brute force in the cloud (e.g., MapReduce) have also been devised. The question arising next tries to clear out if this (rather naive) exploitation of the cloud is an alternative to tuning DBMSs or it still makes sense to consider other options when retrieving data from these settings. In this paper, we study the feasibility of solving OLAP queries with Hadoop (the Apache project implementing MapReduce) while benefiting from secondary indexes and partitioning in HBase. Our main contribution is the comparison of different access plans and the definition of criteria (i.e., cost estimation) to choose among them in terms of consumed resources (namely CPU, bandwidth and I/O).}
}
@article{ELRAGAL2014242,
title = {ERP and Big Data: The Inept Couple},
journal = {Procedia Technology},
volume = {16},
pages = {242-249},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.089},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314003168},
author = {Ahmed Elragal},
keywords = {ERP, Big Data, Research Agenda},
abstract = {The world is witnessing an unprecedented interest in big data. Big data is data that is big in size (volume), big in variety (structured; semi-structured; unstructured), and big in speed of change (velocity). It was reported that almost 90% of the data worldwide was just created in the past 2 years. Therefore, this paper is an attempt to align ERP systems with big data. The objective is to suggest a future research agenda to bring together big data and ERP. While almost everyone is talking about big data at the product or tool level, relationship with social media, relationship with Internet of things, etc. no one has tried to integrate big data and ERP. A research agenda is discussed and introduced in this paper.}
}
@article{THADURI2015457,
title = {Railway Assets: A Potential Domain for Big Data Analytics},
journal = {Procedia Computer Science},
volume = {53},
pages = {457-467},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018268},
author = {Adithya Thaduri and Diego Galar and Uday Kumar},
keywords = {Big Data, Railways, Maintenance, Transportation},
abstract = {Two concepts currently at the leading edge of todays information technology revolution are Analytics and Big Data. The public transportation industry has been at the forefront in utilizing and implementing Analytics and Big Data, from ridership forecasting to transit operations Rail transit systems have been especially involved with these IT concepts, and tend to be especially amenable to the advantages of Analytics and Big Data because they are generally closed systems that involve sophisticated processing of large volumes of data. The more that public transportation professionals and decision makers understand the role of Analytics and Big Data in their industry in perspective, the more effectively they will be able to utilize its promise. This paper gives an overview of Big Data technologies in context of transportation with specific to Railways. This paper also gives an insight on how the existing data modules from the transport authority combines Big Data and how can be incorporated in providing maintenance decision making.}
}
@incollection{KRISHNAN2013219,
title = {Chapter 11 - Data-Driven Architecture for Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {219-240},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00011-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000118},
author = {Krish Krishnan},
keywords = {metadata, master data, machine learning, algorithms, semantic libraries, data governance},
abstract = {The goal of this chapter is to provide readers with data governance in the age of Big Data. We will discuss the goals of what managing data means with respect to the next generation of data warehousing and the role of metadata and master data in integrating Big Data into the data warehouse.}
}
@incollection{TALBURT20151,
title = {Chapter 1 - The Value Proposition for MDM and Big Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-16},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000016},
author = {John R. Talburt and Yinle Zhou},
keywords = {Master data, master data management, MDM, Big Data, reference data management, RDM},
abstract = {This chapter gives a definition of master data management (MDM) and describes how it generates value for organizations. It also provides an overview of Big Data and the challenges it brings to MDM.}
}
@article{BENDLE2016115,
title = {Uncovering the message from the mess of big data},
journal = {Business Horizons},
volume = {59},
number = {1},
pages = {115-124},
year = {2016},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681315001408},
author = {Neil T. Bendle and Xin (Shane) Wang},
keywords = {Big data, User-Generated content, Latent Dirichlet Allocation, Topic modeling, Market research, Qualitative data},
abstract = {User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses’ strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers’ minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.}
}
@article{SIDDIQA2016151,
title = {A survey of big data management: Taxonomy and state-of-the-art},
journal = {Journal of Network and Computer Applications},
volume = {71},
pages = {151-166},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516300583},
author = {Aisha Siddiqa and Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Mohsen Marjani and Shahabuddin Shamshirband and Abdullah Gani and Fariza Nasaruddin},
keywords = {Big data management, Storage, Big data, Processing, Security},
abstract = {The rapid growth of emerging applications and the evolution of cloud computing technologies have significantly enhanced the capability to generate vast amounts of data. Thus, it has become a great challenge in this big data era to manage such voluminous amount of data. The recent advancements in big data techniques and technologies have enabled many enterprises to handle big data efficiently. However, these advances in techniques and technologies have not yet been studied in detail and a comprehensive survey of this domain is still lacking. With focus on big data management, this survey aims to investigate feasible techniques of managing big data by emphasizing on storage, pre-processing, processing and security. Moreover, the critical aspects of these techniques are analyzed by devising a taxonomy in order to identify the problems and proposals made to alleviate these problems. Furthermore, big data management techniques are also summarized. Finally, several future research directions are presented.}
}
@article{KUM2015127,
title = {Using big data for evidence based governance in child welfare},
journal = {Children and Youth Services Review},
volume = {58},
pages = {127-136},
year = {2015},
issn = {0190-7409},
doi = {https://doi.org/10.1016/j.childyouth.2015.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0190740915300591},
author = {Hye-Chung Kum and C. {Joy Stewart} and Roderick A. Rose and Dean F. Duncan},
keywords = {Big data, Evidence based governance, Knowledge discovery and data mining (KDD), Data science, Population informatics, Policy informatics, Academic government partnership, Administrative data},
abstract = {Numerous approaches are available for improving governance of the child welfare system, all of which require longitudinal data reporting on child welfare clients. A substantial amount of agency administrative information – big data – can be transformed into knowledge for policy and management actions through a rigorous information generation process. Important properties of the information generation process are that it must generate accurate, timely information while protecting the confidentiality of the clients. In addition, it must be extensible to serve an ever-changing policy and technology environment. Knowledge discovery and data mining (KDD), aka data science, is a method developed in the private sector to mine consumer data and can be used in public settings to support evidence based governance. KDD consists of a rigorous 5-step process that includes a Web-based end-user interface. The relationship between KDD and governance is a continuous feedback cycle that enables ongoing development of new information and knowledge as stakeholders identify emerging needs. In this paper, we synthesis the different frameworks for utilizing big data for public governance, introduce the KDD process, describe the nature of big data in child welfare, and then present an updated KDD architecture that can support these frameworks to utilize big data for governance. We also demonstrate the role KDD plays in child welfare management through 2 case studies. We conclude with a discussion on implications for agency–university partnerships and research-to-practice.}
}
@article{PERRONS2015117,
title = {Data as an asset: What the oil and gas sector can learn from other industries about “Big Data”},
journal = {Energy Policy},
volume = {81},
pages = {117-121},
year = {2015},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2015.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S0301421515000932},
author = {Robert K. Perrons and Jesse W. Jensen},
keywords = {Big data, Oil and gas, Information technologies, Data},
abstract = {The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but “Big Data” is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset—but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.}
}
@article{SHIN2015311,
title = {Ecological views of big data: Perspectives and issues},
journal = {Telematics and Informatics},
volume = {32},
number = {2},
pages = {311-320},
year = {2015},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2014.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0736585314000665},
author = {Dong-Hee Shin and Min Jae Choi},
keywords = {Big data, Data ecosystem, Ecology, South Korea, Socio-technical perspective, Big data policy, Big data for development},
abstract = {From the viewpoint of big data as a socio-technical phenomenon, this study examines the associated assumptions and biases critically and contextually. The research analyzes the big data phenomenon from a socio-technical systems theory perspective: cultural, technological, and scholarly phenomena that rest on the interplay of technology, analysis, and mythology provoking extensive utopian and dystopian rhetoric. It examines the development of big data by reviewing this theory, identifying key components of the big data ecosystem, and explaining how these components are likely to evolve over time. Despite extensive investment and proactive drive, uncertainty exists concerning the evolution of big data and the impact on the new information milieu. Significant concerns recently addressed are in the areas of privacy, data quality, access, curation, preservation, and use. This study provides insight into these challenges and opportunities through the lens of a socio-technical analysis of big data development, which includes social dynamics, political discourse, and technological choices inherent in the design and development of next-generation ICT ecology. The policy implications of big data are addressed using Korean information initiatives to highlight key considerations as the country progresses in this new ecology era.}
}
@incollection{MAYER201667,
title = {Chapter 5 - Big Data For Health Through Social Media},
editor = {Shabbir Syed-Abdul and Elia Gabarron and Annie Y.S. Lau},
booktitle = {Participatory Health Through Social Media},
publisher = {Academic Press},
pages = {67-82},
year = {2016},
isbn = {978-0-12-809269-9},
doi = {https://doi.org/10.1016/B978-0-12-809269-9.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092699000050},
author = {M.A. Mayer and L. Fernández-Luque and A. Leis},
keywords = {Big Data, social media, data analysis, public health, Internet},
abstract = {Social Media (SM) can be a complementary channel of information to other official means for the health data collection such as the epidemiological surveillance activities and control carried out by health authorities. For this reason, more and more organizations, professionals, and scientific institutions are seeing the need to make the most of resources of health information based on SM platforms through the use of Big Data tools and analytics. Although there is a consensus on the potential benefits and opportunities that SM may provide when used for healthcare purposes, its use has brought unsuspected drawbacks and challenges related to the protection of personal data, it is essential to promote a wide reflection and that the authorities and governments establish, in collaboration with patients associations and professional institutions, specific ethical, legal guidelines, and use policies to the benefit of the current and future healthcare professional–patient relationship and general public.}
}
@article{MEHMOOD20151107,
title = {Big Data Logistics: A health-care Transport Capacity Sharing Model},
journal = {Procedia Computer Science},
volume = {64},
pages = {1107-1114},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.566},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915027015},
author = {Rashid Mehmood and Gary Graham},
keywords = {future city, Big data, transport operation management, healthcare informationsystems, integrated systems, shared resources},
abstract = {The growth of cities in the 21st century has put more pressure on resources and conditions of urban life. There are several reasons why the health-care industry is the focus of this investigation. For instance, in the UK various studies point to the lack of failure of basic quality control procedures and misalignment between customer needs and provider services and duplication of logistics practices. The development of smart cities and big data present unprecedented challenges and opportunities for operations managers; they need to develop new tools and techniques for network planning and control. Our paper aims to make a contribution to big data and city operations theory by exploring how big data can lead to improvements in transport capacity sharing. We explore using Markov models the integration of big data with future city (health-care) transport sharing. A mathematical model was designed to illustrate how sharing transport load (and capacity) in a smart city can improve efficiencies in meeting demand for city services. The results from our analysis of 13 different sharing/demand scenarios are presented. A key finding is that the probability for system failure and performance variance tends to be highest in a scenario of high demand/zero sharing.}
}
@article{SHIN2016837,
title = {Demystifying big data: Anatomy of big data developmental process},
journal = {Telecommunications Policy},
volume = {40},
number = {9},
pages = {837-854},
year = {2016},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2015.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0308596115000567},
author = {Dong-Hee Shin},
keywords = {Big data, Data ecosystem, Normalization, Normalization process theory, Big data user, Big data user experience},
abstract = {This study seeks to understand big data ecology, how it is perceived by different stakeholders, the potential value and challenges, and the implications for the private sector and public organizations, as well as for policy makers. With Normalization Process Theory in place, this study conducts socio-technical evaluation on the big data phenomenon to understand the developmental processes through which new practices of thinking and enacting are implemented, embedded, and integrated in South Korea. It also undertakes empirical analyses of user modeling to explore the factors influencing users׳ adoption of big data by integrating cognitive motivations as well as user values as the primary determining factors. Based on the qualitative and quantitative findings, this study concludes that big data should be developed with user-centered ideas and that users should be the focus of big data design.}
}
@article{RAIKOV2016147,
title = {Big Data Refining on the Base of Cognitive Modeling},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {32},
pages = {147-152},
year = {2016},
note = {Cyber-Physical & Human-Systems CPHS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.12.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316328774},
author = {Alexander N. Raikov and Z. Avdeeva and A. Ermakov},
keywords = {data refining, cognitive modeling, Big Data, intellectual agents, networked expertise},
abstract = {Abstract:
In conditions of rapid external changes the requirement to quality of control of purposeful development of complex system (states, regions, corporations etc.) dramatically increases. Automation support of the key stages of decision making process is one of the ways to cope with the challenges. This paper focuses on the approach based on the Big Data Refining during cognitive modeling that proves the correctness of modeling and decision-making. The approach uses the requests to the Big Data for cognitive model components verification. The requests are created by intelligent agents with feedback from decision makers. Some practical results confirm the adequacy of the proposed approach.}
}
@article{AHMAD2016439,
title = {An efficient divide-and-conquer approach for big data analytics in machine-to-machine communication},
journal = {Neurocomputing},
volume = {174},
pages = {439-453},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.04.109},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215012369},
author = {Awais Ahmad and Anand Paul and M. Mazhar Rathore},
keywords = {M2M, Big Data, Divide-and-conquer, Data fusion domain},
abstract = {Machine-to-Machine (M2M) communication relies on the physical objects (e.g., satellites, sensors, and so forth) interconnected with each other, creating mesh of machines producing massive volume of data about large geographical area (e.g., living and non-living environment). Thus, the M2M is an ideal example of Big Data. On the contrary, the M2M platforms that handle Big Data might perform poorly or not according to the goals of their operator (in term of cost, database utilization, data quality, processing and computational efficiency, analysis and feature extraction applications). Therefore, to address the aforementioned needs, we propose a new effective, memory and processing efficient system architecture for Big Data in M2M, which, unlike other previous proposals, does not require whole set of data to be processed (including raw data sets), and to be kept in the main memory. Our designed system architecture exploits divide-and-conquer approach and data block-wise vertical representation of the database follows a particular petitionary strategy, which formalizes the problem of feature extraction applications. The architecture goes from physical objects to the processing servers, where Big Data set is first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using fusion algorithm. Finally, the results are stored in a server that helps the users in making decision. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup on UBUNTU 14.04 LTS core™i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.}
}
@article{RICHTER201537,
title = {Medicinal chemistry in the era of big data},
journal = {Drug Discovery Today: Technologies},
volume = {14},
pages = {37-41},
year = {2015},
note = {From Chemistry to Biology Database Curation},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2015.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1740674915000141},
author = {Lars Richter and Gerhard F. Ecker},
abstract = {In the era of big data medicinal chemists are exposed to an enormous amount of bioactivity data. Numerous public data sources allow for querying across medium to large data sets mostly compiled from literature. However, the data available are still quite incomplete and of mixed quality. This mini review will focus on how medicinal chemists might use such resources and how valuable the current data sources are for guiding drug discovery.}
}
@article{CHEN2016184,
title = {On Big Data and Hydroinformatics},
journal = {Procedia Engineering},
volume = {154},
pages = {184-191},
year = {2016},
note = {12th International Conference on Hydroinformatics (HIC 2016) - Smart Water for the Future},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.443},
url = {https://www.sciencedirect.com/science/article/pii/S187770581631832X},
author = {Yiheng Chen and Dawei Han},
keywords = {Big data, Hydroinformatics},
abstract = {Big data is an increasingly hot concept in the past five years in the area of computer science, e-commence, and bioinformatics, because more and more data has been collected by the internet, remote sensing network, wearable devices and the Internet of Things. The big data technology provides techniques and analytical tools to handle large datasets, so that creative ideas and new values can be extracted from them. However, the hydroinformatics research community are not so familiar with big data. This paper provides readers who are embracing the data-rich era with a timely review on big data and its relevant technology, and then points out the relevance with hydroinformatics in three aspects.}
}
@article{REHMAN2016917,
title = {Big data reduction framework for value creation in sustainable enterprises},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part A},
pages = {917-928},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216303097},
author = {Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah},
keywords = {Sustainable enterprises, Value creation, Big data analytics, Data reduction, Business model},
abstract = {Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.}
}
@article{BIBAULT2016110,
title = {Big Data and machine learning in radiation oncology: State of the art and future prospects},
journal = {Cancer Letters},
volume = {382},
number = {1},
pages = {110-117},
year = {2016},
issn = {0304-3835},
doi = {https://doi.org/10.1016/j.canlet.2016.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0304383516303469},
author = {Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun},
keywords = {Radiation oncology, Big Data, Predictive model, Machine learning},
abstract = {Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.}
}
@article{OLMEDILLA201679,
title = {Harvesting Big Data in social science: A methodological approach for collecting online user-generated content},
journal = {Computer Standards & Interfaces},
volume = {46},
pages = {79-87},
year = {2016},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916300034},
author = {M. Olmedilla and M.R. Martínez-Torres and S.L. Toral},
keywords = {Big Data, User-generated content, e-Social science, Computing, Data gathering},
abstract = {Online user-generated content is playing a progressively important role as information source for social scientists seeking for digging out value. Advances procedures and technologies to enable the capture, storage, management, and analysis of the data make possible to exploit increasing amounts of data generated directly by users. In that regard, Big Data is gaining meaning into social science from quantitative datasets side, which differs from traditional social science where collecting data has always been hard, time consuming, and resource intensive. Hence, the emergent field of computational social science is broadening researchers' perspectives. However, it also requires a multidisciplinary approach involving several and different knowledge areas. This paper outlines an architectural framework and methodology to collect Big Data from an electronic Word-of-Mouth (eWOM) website containing user-generated content. Although the paper is written from the social science perspective, it must be also considered together with other complementary disciplines such as data accessing and computing.}
}
@article{JANKE2016227,
title = {Exploring the Potential of Predictive Analytics and Big Data in Emergency Care},
journal = {Annals of Emergency Medicine},
volume = {67},
number = {2},
pages = {227-236},
year = {2016},
issn = {0196-0644},
doi = {https://doi.org/10.1016/j.annemergmed.2015.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0196064415005302},
author = {Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy},
abstract = {Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients’ risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.}
}
@article{SANCHEZMARTINEZ2016236,
title = {Workshop 5 report: Harnessing big data},
journal = {Research in Transportation Economics},
volume = {59},
pages = {236-241},
year = {2016},
note = {Competition and Ownership in Land Passenger Transport (selected papers from the Thredbo 14 conference)},
issn = {0739-8859},
doi = {https://doi.org/10.1016/j.retrec.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0739885916301494},
author = {Gabriel E. Sánchez-Martínez and Marcela Munizaga},
keywords = {Big data, Measurement, Implementation challenges, Analysis tools, Transit best practices},
abstract = {A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled “Harnessing Big Data”, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.}
}
@article{CALYAM20163,
title = {Synchronous Big Data analytics for personalized and remote physical therapy},
journal = {Pervasive and Mobile Computing},
volume = {28},
pages = {3-20},
year = {2016},
note = {Special Issue on Big Data for Healthcare; Guest Editors: Sriram Chellappan, Nirmalya Roy, Sajal K. Das and Special Issue on Security and Privacy in Mobile Clouds Guest; Editors: Sherman S.M. Chow, Urs Hengartner, Joseph K. Liu, Kui Ren},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2015.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1574119215001704},
author = {Prasad Calyam and Anup Mishra and Ronny Bazan Antequera and Dmitrii Chemodanov and Alex Berryman and Kunpeng Zhu and Carmen Abbott and Marjorie Skubic},
keywords = {Smart health care, Personalized remote physical therapy, Synchronous Big Data, Gigabit networking app},
abstract = {With gigabit networking becoming economically feasible and widely installed at homes, there are new opportunities to revisit in-home, personalized telehealth services. In this paper, we describe a novel telehealth eldercare service that we developed viz., “PhysicalTherapy-as-a-Service” (PTaaS) that connects a remote physical therapist at a clinic to a senior at home. The service leverages a high-speed, low-latency network connection through an interactive interface built on top of Microsoft Kinect motion sensing capabilities. The interface that is built using user-centered design principles for wellness coaching exercises is essentially a ‘Synchronous Big Data’ application due to its: (i) high data-in-motion velocity (i.e., peak data rate is ≈400 Mbps), (ii) considerable variety (i.e., measurements include 3D sensing, network health, user opinion surveys and video clips of RGB, skeletal and depth data), and (iii) large volume (i.e., several GB of measurement data for a simple exercise activity). The successful PTaaS delivery through this interface is dependent on the veracity analytics needed for correlation of the real-time Big Data streams within a session, in order to assess exercise balance of the senior without any bias due to network quality effects. Our experiments with PTaaS in an actual testbed involving senior homes in Kansas City with Google Fiber connections and our university clinic demonstrate the network configuration and time synchronization related challenges in order to perform online analytics. Our findings provide insights on how to: (a) enable suitable resource calibration and perform network troubleshooting for high user experience for both the therapist and the senior, and (b) realize a Big Data architecture for PTaaS and other similar personalized healthcare services to be remotely delivered at a large-scale in a reliable, secure and cost-effective manner.}
}
@incollection{ANYA201699,
title = {Chapter 5 - Leveraging Big Data Analytics for Personalized Elderly Care: Opportunities and Challenges},
editor = {Dhiya Al-Jumeily and Abir Hussain and Conor Mallucci and Carol Oliver},
booktitle = {Applied Computing in Medicine and Health},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {99-124},
year = {2016},
series = {Emerging Topics in Computer Science and Applied Computing},
isbn = {978-0-12-803468-2},
doi = {https://doi.org/10.1016/B978-0-12-803468-2.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034682000059},
author = {Obinna Anya and Hissam Tawfik},
keywords = {Elderly care, personalized care, independent living, ACTVAGE, Big Data analytics, CAPIM, lifestyle-oriented, context-awareness, framework},
abstract = {Owing to the growing increase in the world’s ageing population, research has focused on developing information and communication technology (ICT)–based services for personalized care, improved health, and quality social life for the elderly. Recent efforts explore Big Data in order to build mathematical models of personal behavior and lifestyle for analytics. Leveraging Big Data analytics holds enormous potential for solving some of the biggest and most intractable challenges in personalized elderly care through quantified modeling of a person’s lifestyle in a way that takes cognizance of their beliefs, values, and preferences, and connects to a history of events, things, and places around which they have progressively built their lives. However, the idea of discovering patterns to personalize care and inform critical health care decisions for the elderly is challenged as data grow exponentially in volume, become faster and increasingly unstructured, and are generated from sociodigital engagements that often may not accurately reflect the real-world entities and contexts they represent. As a result, the idea raises issues along several dimensions, including social, technical, and context-aware challenges. In this chapter, we present an overview of the state of the art in personalized elderly care, and explore the opportunities and inherent sociotechnical challenges in leveraging Big Data analytics to support elderly care and independent living. Based on this discussion, and arguing that analytics need to take account of the contexts that shape the generation and use of data, ACTVAGE, a context-aware lifestyle-oriented framework for personalized elderly care and independent living is proposed.}
}
@incollection{KRISHNAN2013101,
title = {Chapter 5 - Big Data Driving Business Value},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {101-123},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000052},
author = {Krish Krishnan},
keywords = {sensor data, machine data, social media, compliance, safety},
abstract = {The first four chapters provided you an introduction to Big Data, the complexities associated with Big Data, and the processing techniques and technologies for Big Data. This chapter will focus on use cases of Big Data and how real-world companies are implementing Big Data.}
}
@article{OPRESNIK2015174,
title = {The value of Big Data in servitization},
journal = {International Journal of Production Economics},
volume = {165},
pages = {174-184},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004307},
author = {David Opresnik and Marco Taisch},
keywords = {Servitization, Big Data, Manufacturing, Competitive advantage, Value, Information},
abstract = {Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product–services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five “Vs” in Big Data–Value, in addition to the other four “Vs”—Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value— “information”, beside the two existing ones: product and service. The results have strategic implications for managers.}
}
@article{PFEIFFER2015213,
title = {Spatial and temporal epidemiological analysis in the Big Data era},
journal = {Preventive Veterinary Medicine},
volume = {122},
number = {1},
pages = {213-220},
year = {2015},
issn = {0167-5877},
doi = {https://doi.org/10.1016/j.prevetmed.2015.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167587715002111},
author = {Dirk U. Pfeiffer and Kim B. Stevens},
keywords = {Data science, Exploratory analysis, Internet of Things, Modelling, Multi-criteria decision analysis, Spatial analysis, Visualisation},
abstract = {Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.}
}
@article{SIMPAO2015350,
title = {Big data and visual analytics in anaesthesia and health care†},
journal = {British Journal of Anaesthesia},
volume = {115},
number = {3},
pages = {350-356},
year = {2015},
issn = {0007-0912},
doi = {https://doi.org/10.1093/bja/aeu552},
url = {https://www.sciencedirect.com/science/article/pii/S0007091217311479},
author = {A.F. Simpao and L.M. Ahumada and M.A. Rehman},
keywords = {decision support systems, clinical, electronic health records, integrated advanced information management systems, medical informatics},
abstract = {Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics—the systematic use of data combined with quantitative and qualitative analysis to make decisions—can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.}
}
@article{KACFAHEMANI201570,
title = {Understandable Big Data: A survey},
journal = {Computer Science Review},
volume = {17},
pages = {70-81},
year = {2015},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2015.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1574013715000064},
author = {Cheikh {Kacfah Emani} and Nadine Cullot and Christophe Nicolle},
keywords = {Big data, Hadoop, Reasoning, Coreference resolution, Entity linking, Information extraction, Ontology alignment},
abstract = {This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.}
}
@article{YAQOOB20161231,
title = {Big data: From beginning to future},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part B},
pages = {1231-1247},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216304753},
author = {Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos},
keywords = {Big data, Parallel and distributed computing, Cloud computing, Internet of things, Social media, Analytics},
abstract = {Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.}
}
@article{BILAL2016500,
title = {Big Data in the construction industry: A review of present status, opportunities, and future trends},
journal = {Advanced Engineering Informatics},
volume = {30},
number = {3},
pages = {500-521},
year = {2016},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616301938},
author = {Muhammad Bilal and Lukumon O. Oyedele and Junaid Qadir and Kamran Munir and Saheed O. Ajayi and Olugbenga O. Akinade and Hakeem A. Owolabi and Hafiz A. Alaka and Maruf Pasha},
keywords = {Big Data Engineering, Big Data Analytics, Construction industry, Machine learning},
abstract = {The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon—dubbed as Big Data—has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.}
}
@article{ENGLEBRIGHT2016280,
title = {The Role of the Chief Nurse Executive in the Big Data Revolution},
journal = {Nurse Leader},
volume = {14},
number = {4},
pages = {280-284},
year = {2016},
issn = {1541-4612},
doi = {https://doi.org/10.1016/j.mnl.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1541461215300094},
author = {Jane Englebright and Barbara Caspers}
}
@article{BELLOORGAZ201645,
title = {Social big data: Recent achievements and new challenges},
journal = {Information Fusion},
volume = {28},
pages = {45-59},
year = {2016},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253515000780},
author = {Gema Bello-Orgaz and Jason J. Jung and David Camacho},
keywords = {Big data, Data mining, Social media, Social networks, Social-based frameworks and applications},
abstract = {Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the “umbrella” of the social networks, social media and big data paradigms.}
}
@article{FUMEO2015437,
title = {Condition Based Maintenance in Railway Transportation Systems Based on Big Data Streaming Analysis},
journal = {Procedia Computer Science},
volume = {53},
pages = {437-446},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.321},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018244},
author = {Emanuele Fumeo and Luca Oneto and Davide Anguita},
keywords = {Big Data Streams, Data Analytics, Condition Based Maintenance, Intelligent Transporta- tion Systems, Online Learning, Model Selection},
abstract = {Streaming Data Analysis (SDA) of Big Data Streams (BDS) for Condition Based Maintenance (CBM) in the context of Rail Transportation Systems (RTS) is a state-of-the-art field of re- search. SDA of BDS is the problem of analyzing, modeling and extracting information from huge amounts of data that continuously come from several sources in real time through com- putational aware solutions. Among others, CBM for Rail Transportation is one of the most challenging SDA problems, consisting of the implementation of a predictive maintenance system for evaluating the future status of the monitored assets in order to reduce risks related to failures and to avoid service disruptions. The challenge is to collect and analyze all the data streams that come from the numerous on-board sensors monitoring the assets. This paper deals with the problem of CBM applied to the condition monitoring and predictive maintenance of train axle bearings based on sensors data collection, with the purpose of maximizing their Remaining Useful Life (RUL). In particular we propose a novel algorithm for CBM based on SDA that takes advantage of the Online Support Vector Regression (OL-SVR) for predicting the RUL. The novelty of our proposal is the heuristic approach for optimizing the trade-off between the accuracy of the OL-SVR models and the computational time and resources needed in order to build them. Results from tests on a real-world dataset show the actual benefits brought by the proposed methodology.}
}
@article{TAN2015223,
title = {Harvesting big data to enhance supply chain innovation capabilities: An analytic infrastructure based on deduction graph},
journal = {International Journal of Production Economics},
volume = {165},
pages = {223-233},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004289},
author = {Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang},
keywords = {Big data, Analytic infrastructure, Competence set, Deduction graph, Supply chain innovation},
abstract = {Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm׳s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.}
}
@article{REKHA2015295,
title = {Survey on Software Project Risks and Big Data Analytics},
journal = {Procedia Computer Science},
volume = {50},
pages = {295-300},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915005463},
author = {J.H. Rekha and R. Parvathi},
keywords = {software project, big data analytics, anlytics tools.},
abstract = {Software project is collaborative enterprise of making a desired software for the client. Each software is unique and is delivered by following the process. The process includes understanding the requirement, planning, designing the software and implementation. Risk occurs in the software project which need attention by the managers and workers to make the project efficient. Big data analytics is commonly used in all fields. Big data deals with huge data which are unstructured. Using analytics tools, it can be chunked down and analyzed to provide valuable solutions. In this paper, a review of risk in software project and big data analytics are briefed out.}
}
@article{COLEMAN20151091,
title = {How Big Data Informs Us About Cataract Surgery: The LXXII Edward Jackson Memorial Lecture},
journal = {American Journal of Ophthalmology},
volume = {160},
number = {6},
pages = {1091-1103.e3},
year = {2015},
issn = {0002-9394},
doi = {https://doi.org/10.1016/j.ajo.2015.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S000293941500598X},
author = {Anne Louise Coleman},
abstract = {Purpose
To characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.
Design
Review of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.
Methods
Statistical analysis of observational data.
Results
The overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P = .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1–7 days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20/40.
Conclusions
Big Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.}
}
@article{WESTRA2016286,
title = {Big Data and Perioperative Nursing},
journal = {AORN Journal},
volume = {104},
number = {4},
pages = {286-292},
year = {2016},
note = {Special Focus Issue: Technology},
issn = {0001-2092},
doi = {https://doi.org/10.1016/j.aorn.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0001209216304410},
author = {Bonnie L. Westra and Jessica J. Peterson},
keywords = {big data, perioperative nursing, quality care, nursing knowledge, nursing informatics},
abstract = {Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five “Vs”: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.}
}
@article{AKTER2016113,
title = {How to improve firm performance using big data analytics capability and business strategy alignment?},
journal = {International Journal of Production Economics},
volume = {182},
pages = {113-131},
year = {2016},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2016.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527316302110},
author = {Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe},
keywords = {Capabilities, Entanglement view, Big data analytics, Hierarchical modeling},
abstract = {The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability–business strategy alignment on the BDAC–FPER relationship.}
}
@incollection{LOSHIN2013105,
title = {Chapter 11 - Developing the Big Data Roadmap},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {105-120},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000119},
author = {David Loshin},
keywords = {Need for big data, organizational buy-in, team building, big data evangelist, application architect, data integration, platform architect, data scientist, proof of concept, big data pilot, technology evaluation, technology selection, data management, appliance, application development, YARN, MapReduce, SDLC, training, project scoping, platform scoping, problem data size, computational complexity, storage configuration, integration plan, maintenance, management, assessment},
abstract = {This final chapter reviews best practices for incrementally adopting big data into the enterprise. The chapter revisits assessing the need and value of big data, organizational buy-in, building the big data team, scoping and piloting a proof of concept, technology evaluation and selection, application development, testing, and implementation, platform and project scoping, the big data integration plan, management and maintenance, assessment of success criteria, and overall summary and considerations.}
}
@article{ZHOU2016215,
title = {Big data driven smart energy management: From big data to big insights},
journal = {Renewable and Sustainable Energy Reviews},
volume = {56},
pages = {215-225},
year = {2016},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2015.11.050},
url = {https://www.sciencedirect.com/science/article/pii/S1364032115013179},
author = {Kaile Zhou and Chao Fu and Shanlin Yang},
keywords = {Energy big data, Smart energy management, Big data analytics, Smart grid, Demand side management (DSM)},
abstract = {Large amounts of data are increasingly accumulated in the energy sector with the continuous application of sensors, wireless transmission, network communication, and cloud computing technologies. To fulfill the potential of energy big data and obtain insights to achieve smart energy management, we present a comprehensive study of big data driven smart energy management. We first discuss the sources and characteristics of energy big data. Also, a process model of big data driven smart energy management is proposed. Then taking smart grid as the research background, we provide a systematic review of big data analytics for smart energy management. It is discussed from four major aspects, namely power generation side management, microgrid and renewable energy management, asset management and collaborative operation, as well as demand side management (DSM). Afterwards, the industrial development of big data-driven smart energy management is analyzed and discussed. Finally, we point out the challenges of big data-driven smart energy management in IT infrastructure, data collection and governance, data integration and sharing, processing and analysis, security and privacy, and professionals.}
}
@article{MAYO2016260,
title = {The big data effort in radiation oncology: Data mining or data farming?},
journal = {Advances in Radiation Oncology},
volume = {1},
number = {4},
pages = {260-271},
year = {2016},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452109416300550},
author = {Charles S. Mayo and Marc L. Kessler and Avraham Eisbruch and Grant Weyburne and Mary Feng and James A. Hayman and Shruti Jolly and Issam {El Naqa} and Jean M. Moran and Martha M. Matuszak and Carlos J. Anderson and Lynn P. Holevinski and Daniel L. McShan and Sue M. Merkel and Sherry L. Machnak and Theodore S. Lawrence and Randall K. {Ten Haken}},
abstract = {Although large volumes of information are entered into our electronic health care records, radiation oncology information systems and treatment planning systems on a daily basis, the goal of extracting and using this big data has been slow to emerge. Development of strategies to meet this goal is aided by examining issues with a data farming instead of a data mining conceptualization. Using this model, a vision of key data elements, clinical process changes, technology issues and solutions, and role for professional societies is presented. With a better view of technology, process and standardization factors, definition and prioritization of efforts can be more effectively directed.}
}
@article{KWON2014387,
title = {Data quality management, data usage experience and acquisition intention of big data analytics},
journal = {International Journal of Information Management},
volume = {34},
number = {3},
pages = {387-394},
year = {2014},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401214000127},
author = {Ohbyung Kwon and Namyeon Lee and Bongsik Shin},
keywords = {Big data analytics, Resource-based view, Data quality management, IT capability, Data usage},
abstract = {Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.}
}
@article{RAMOS20151031,
title = {Primary Education Evaluation in Brazil Using Big Data and Cluster Analysis},
journal = {Procedia Computer Science},
volume = {55},
pages = {1031-1039},
year = {2015},
note = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915015367},
author = {Thiago Graca Ramos and Jean Cristian Ferreira Machado and Bruna Principe Vieira Cordeiro},
keywords = {Big Data, Data Warehouse, Cluster, Education, IDEB},
abstract = {This study aims to understand the assessment of basic education in the perspective of the State Reviewer as a mechanism that generates information regarding the positivity and weaknesses of a school or an educational system to provide improvements. For this reason, a Data Warehouse was created and later some analysis of the indicators were performed through clustering.}
}
@article{LIU2016134,
title = {Rethinking big data: A review on the data quality and usage issues},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {134-142},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002567},
author = {Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu},
keywords = {Big data, Data quality and error, Data ethnics, Spatial information sciences},
abstract = {The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.}
}
@incollection{LOSHIN201339,
title = {Chapter 5 - Data Governance for Big Data Analytics: Considerations for Data Policies and Processes},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {39-48},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000053},
author = {David Loshin},
keywords = {Data governance, accuracy, completeness, consistency, currency, data requirements, consumer expectations, data quality dimensions, metadata, reference data, repurposing, enrichment, enhancement},
abstract = {In this chapter we look at the need for oversight and governance for the data, especially when those developing big data applications often bypass traditional IT and data management channels. Some of the key issues involve the fact that for big data applications that consume massive amounts of data streamed from external sources, there is little or no control that can be exerted to ensure data quality and usability. We consider five key concepts, namely managing data consumer expectations, defining critical data quality dimensions, monitoring consistency of metadata, data repurposing and reinterpretation, and data enrichment when possible.}
}
@incollection{SAMPSON2015229,
title = {Chapter 15 - The Legal Challenges of Big Data Application in Law Enforcement},
editor = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
booktitle = {Application of Big Data for National Security},
publisher = {Butterworth-Heinemann},
pages = {229-237},
year = {2015},
isbn = {978-0-12-801967-2},
doi = {https://doi.org/10.1016/B978-0-12-801967-2.00015-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801967200015X},
author = {Fraser Sampson},
keywords = {Dilemma, Human rights, Jurisdiction, Law enforcement, Privacy, Purpose limitation},
abstract = {This chapter considers the specific issues that Big Data presents for law enforcement agencies (LEAs). In particular, it looks at the dilemmas created for LEAs seeking to use the advantages Big Data gives them while remaining compliant with the developing legal framework governing privacy and the protection of personal data, and how those very advantages can present challenges in law enforcement.}
}
@article{EREVELLES2016897,
title = {Big Data consumer analytics and the transformation of marketing},
journal = {Journal of Business Research},
volume = {69},
number = {2},
pages = {897-904},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2015.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0148296315002842},
author = {Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne},
keywords = {Big Data, Consumer analytics, Consumer insights, Resource-based theory, Induction, Ignorance},
abstract = {Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources—physical, human, and organizational capital—moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.}
}
@article{ZHANG2015606,
title = {A System for Tender Price Evaluation of Construction Project Based on Big Data},
journal = {Procedia Engineering},
volume = {123},
pages = {606-614},
year = {2015},
note = {Selected papers from Creative Construction Conference 2015},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.10.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815032154},
author = {Yongcheng Zhang and Hanbin Luo and Yi He},
keywords = {System, Bid price evaluation, Construction project, Big data},
abstract = {Tender price evaluation of construction project is one of the most important works for the clients to control project cost in the bidding stage. However,the previously underutilization of project cost data made the tender price evaluation of new projects lack of effective evaluation criterion, which brings challenge to cost control. With the improvement of companies’ information technology application and the advent of big data era, the project cost-related data can be completely and systematically recorded in real time, as well as fully utilized to support decision-making for construction project cost management. In this paper, a system for tender price evaluation of construction project based on big data is presented, aiming to use related technique of big data to analysis project cost data to give a reasonable cost range, which contributes to obtaining the evaluation criterion to support the tender price controls. The paper introduced the data sources, data extraction, data storage and data analysis of the system respectively. A case study is conducted in a metro station project to evaluate the system. The results show that the system based on big data is significant for tender price evaluation in construction project.}
}
@article{HAZEN201472,
title = {Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications},
journal = {International Journal of Production Economics},
volume = {154},
pages = {72-80},
year = {2014},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314001339},
author = {Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer},
keywords = {Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory},
abstract = {Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.}
}
@article{NATIVI20151,
title = {Big Data challenges in building the Global Earth Observation System of Systems},
journal = {Environmental Modelling & Software},
volume = {68},
pages = {1-26},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215000481},
author = {Stefano Nativi and Paolo Mazzetti and Mattia Santoro and Fabrizio Papeschi and Max Craglia and Osamu Ochiai},
keywords = {GEOSS, Big Data, Multidisciplinary systems, Earth System Science, Research infrastructures, Interoperability, Cloud systems},
abstract = {There are many expectations and concerns about Big Data in the sector of Earth Observation. It is necessary to understand whether Big Data is a radical shift or an incremental change for the existing digital infrastructures. This manuscript explores the impact of Big Data dimensionalities (commonly known as ‘V’ axes: volume, variety, velocity, veracity, visualization) on the Global Earth Observation System of Systems (GEOSS) and particularly its common digital infrastructure (i.e. the GEOSS Common Infrastructure). GEOSS is a global and flexible network of content providers allowing decision makers to access an extraordinary range of data and information. GEOSS is a pioneering framework for global and multidisciplinary data sharing in the EO realm. The manuscript introduces and discusses the general GEOSS strategies to address Big Data challenges, focusing on the cloud-based discovery and access solutions. A final section reports the results of the scalability and flexibility performance tests.}
}
@incollection{REEVE2013141,
title = {Chapter 21 - Big Data Integration},
editor = {April Reeve},
booktitle = {Managing Data in Motion},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {141-156},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397167-8},
doi = {https://doi.org/10.1016/B978-0-12-397167-8.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971678000212},
author = {April Reeve}
}
@article{JESSE2016275,
title = {Internet of Things and Big Data – The Disruption of the Value Chain and the Rise of New Software Ecosystems},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {29},
pages = {275-282},
year = {2016},
note = {17th IFAC Conference on International Stability, Technology and Culture TECIS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.079},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316325174},
author = {Norbert Jesse},
keywords = {Internet of Things, Smart Factories, Big Data, Software Platforms, Data Science},
abstract = {Abstract:
IoT connects devices, humans, places, and even abstract items like events. Driven by smart sensors, powerful embedded microelectronics, high-speed connectivity and the standards of the internet, IoT is on the brink of disrupting today's value chains. Big Data, characterized by high volume, high velocity and a high variety of formats, is a result of and also a driving force for IoT. The datafication of business presents completely new opportunities and risks. To hedge the technical risks posed by the interaction between “everything”, IoT requires comprehensive modelling tools. Furthermore, new IT platforms and architectures are necessary to process and store the unprecedented flow of structured and unstructured, repetitive and non-repetitive data in real-time. In the end, only powerful analytics tools are able to extract “sense” from the exponentially growing amount of data and, as a consequence, data science becomes a strategic asset. The era of IoT relies heavily on standards for technologies which guarantee the interoperability of everything. This paper outlines some fundamental standardization activities. Big Data approaches for real-time processing are outlined and tools for analytics are addressed. As consequence, IoT is a (fast) evolutionary process whose success in penetrating all dimensions of life heavily depends on close cooperation between standardization organizations, open source communities and IT experts.}
}
@article{GAO2016952,
title = {A review of control loop monitoring and diagnosis: Prospects of controller maintenance in big data era},
journal = {Chinese Journal of Chemical Engineering},
volume = {24},
number = {8},
pages = {952-962},
year = {2016},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2016.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S1004954116305134},
author = {Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang},
keywords = {Control loop performance assessment, Industrial alarm system, Process knowledge, Root cause diagnosis, Big data},
abstract = {Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.}
}
@article{LI2016119,
title = {Geospatial big data handling theory and methods: A review and research challenges},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {119-133},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002439},
author = {Songnian Li and Suzana Dragicevic and Francesc Antón Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng},
keywords = {Big data, Geospatial, Data handling, Analytics, Spatial modeling, Review},
abstract = {Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.}
}
@article{GARG2016940,
title = {Challenges and Techniques for Testing of Big Data},
journal = {Procedia Computer Science},
volume = {85},
pages = {940-948},
year = {2016},
note = {International Conference on Computational Modelling and Security (CMS 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.285},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916306354},
author = {Naveen Garg and Sanjay Singla and Surender Jangra},
keywords = {Big Data, Testing, Verasity, Hadoop},
abstract = {Big Data, the new buzz word in the industry, is data that exceeds the processing and analytic capacity of conventional database systems within the time necessary to make them useful. With multiple data stores in abundant formats, billions of rows of data with hundreds of millions of data combinations and the urgent need of making best possible decisions, the challenge is big and the solution bigger, Big Data. Comes with it, new advances in computing technology together with its high performance analytics for simpler and faster processing of only relevant data to enable timely and accurate insights using data mining and predictive analytics, text mining, forecasting and optimization on complex data to continuously drive innovation and make the best possible decisions. While Big Data provides solutions to complex business problems like analyzing larger volumes of data than was previously possible to drive more precise answers, analyzing data in motion to capture opportunities that were previously lost, it poses bigger challenges in testing these scenarios. Testing such highly volatile data, which is more often than not unstructured generated from myriad sources such as web logs, radio frequency Id (RFID), sensors embedded in devices, GPS systems etc. and mostly clustered data for its accuracy, high availability, security requires specialization. One of the most challenging things for a tester is to keep pace with changing dynamics of the industry. While on most aspects of testing, the tester need not know the technical details behind the scene however this is where testing Big Data Technology is so different. A tester not only needs to be strong on testing fundamentals but also has to be equally aware of minute details in the architecture of the database designs to analyze several performance bottlenecks and other issues. Like in the example quoted above on In-Memory databases, a tester would need to know how the operating systems allocate and de-allocate memory and understand how much memory is being used at any given time. So, concluding, as the data- analytics Industry evolves further we would see the IT Testing Services getting closely aligned with the Database Engineering and the industry would need more skilled testing professional in this domain to grab the new opportunities.}
}
@article{TAGLANG201617,
title = {Use of “big data” in drug discovery and clinical trials},
journal = {Gynecologic Oncology},
volume = {141},
number = {1},
pages = {17-23},
year = {2016},
issn = {0090-8258},
doi = {https://doi.org/10.1016/j.ygyno.2016.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S0090825816300464},
author = {Guillaume Taglang and David B. Jackson},
keywords = {Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers},
abstract = {Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as “big data”, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.}
}
@article{DABEK2015265,
title = {Leveraging Big Data to Model the Likelihood of Developing Psychological Conditions After a Concussion},
journal = {Procedia Computer Science},
volume = {53},
pages = {265-273},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018062},
author = {Filip Dabek and Jesus J. Caban},
keywords = {Big Data, Machine Learning, Concussion, Informatics mild Traumatic Brain Injury},
abstract = {A concussion is an invisible and poorly understood mild traumatic brain injury (mTBI) that can alter the way the brain functions. Patients who have screened positive for mTBI are at an increased risk of depression, post-traumatic stress disorder (PTSD), headaches, sleep disorders, and other neurological and psychological problems. Early detection of psychological conditions such as PTSD following a concussion might improve the overall outcome of a patient and could potentially reduce the cost associated with intense interventions often required when conditions go untreated for a long time. Statistical and predictive models that leverage large-scale clinical repositories and use pre-existing conditions to determine the probability of a patient developing psychological conditions following a concussion have not been widely studied. This paper presents an SVM-based model that has been trained with a longitudinal dataset of over 5.3 million clinical encounters of 89,840 service members that have sustained a concussion. The model has been tested and validated with over 16,045 patients that developed PTSD and it has shown an accuracy of over 85% (AUC of 86.52%) at predicting the condition within the first year following the injury.}
}
@incollection{LEVIN2016317,
title = {Chapter 11 - From Databases to Big Data},
editor = {Elaine Holmes and Jeremy K. Nicholson and Ara W. Darzi and John C. Lindon},
booktitle = {Metabolic Phenotyping in Personalized and Public Healthcare},
publisher = {Academic Press},
address = {Boston},
pages = {317-331},
year = {2016},
isbn = {978-0-12-800344-2},
doi = {https://doi.org/10.1016/B978-0-12-800344-2.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128003442000112},
author = {Nadine Levin and Reza M. Salek and Christoph Steinbeck},
keywords = {Databases, big data, metabolomics, metabolic phenotyping, data exchange, data warehousing},
abstract = {This chapter explains the concept of big data and shows that the analytical data and related meta-data of metabolic phenotyping experiments fall into this category. The various databases that can be used to aid interpretation of such data, comprising general chemical and biochemical data, biochemical pathway specific data, analytical chemistry data to aid metabolite identification, and finally metabolic results, are discussed. The concept of data warehousing is explored in the context of metabolic data sets. Finally, the challenges for successful data storage, data exchange, and data interpretation are discussed.}
}
@article{KAMBATLA20142561,
title = {Trends in big data analytics},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {7},
pages = {2561-2573},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514000057},
author = {Karthik Kambatla and Giorgos Kollias and Vipin Kumar and Ananth Grama},
keywords = {Big-data, Analytics, Data centers, Distributed systems},
abstract = {One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications’ considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.}
}
@article{ANDREOLINI201567,
title = {Adaptive, scalable and reliable monitoring of big data on clouds},
journal = {Journal of Parallel and Distributed Computing},
volume = {79-80},
pages = {67-79},
year = {2015},
note = {Special Issue on Scalable Systems for Big Data Management and Analytics},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S074373151400149X},
author = {Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi},
keywords = {Adaptivity, Monitoring, Cloud computing, Big data, Scalability},
abstract = {Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.}
}
@article{MARKOWETZ2014405,
title = {Psycho-Informatics: Big Data shaping modern psychometrics},
journal = {Medical Hypotheses},
volume = {82},
number = {4},
pages = {405-411},
year = {2014},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2013.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0306987713005598},
author = {Alexander Markowetz and Konrad Błaszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer},
abstract = {For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.}
}
@article{MENDESSAMPAIO20158304,
title = {DQ2S – A framework for data quality-aware information management},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8304-8326},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.050},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004522},
author = {Sandra de F. {Mendes Sampaio} and Chao Dong and Pedro Sampaio},
keywords = {Information management, Data quality, Query language extensions, Data profiling, Decision support systems, Big data},
abstract = {This paper describes the design and implementation of the Data Quality Query System (DQ2S), a query processing framework and tool incorporating data quality profiling functionality in the processing of queries involving quality-aware query language extensions. DQ2S supports the combination of performance and quality-oriented query optimizations, and a query processing platform that enables advanced data profiling queries to be formulated based on well established query language constructs, often used to interact with relational database management systems. DQ2S encompasses a declarative query language and a data model that provides users with the capability to express constraints on the quality of query results as well as query quality-related information; a set of algebraic operators for manipulating data quality-related information, and optimization heuristics. The proposed query language and algebra represent seamless extensions to SQL and relational database engines, respectively. The constructs of the proposed data model are implemented at the user’s view level and are internally mapped into relational model constructs. The quality-aware extensions and features are extremely useful when users need to assess the quality of relational data sets and define quality constraints for acceptable data prior to using candidate data sources in decision support systems and conducting big data analytical tasks.}
}
@article{MACKIE2015189,
title = {Big data! Big deal?},
journal = {Public Health},
volume = {129},
number = {3},
pages = {189-190},
year = {2015},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2015.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033350615000621},
author = {P. Mackie and F. Sim and C. Johnman}
}
@article{LIN2014532,
title = {A New Idea of Study on the Influence Factors of Companies’ Debt Costs in the Big Data Era},
journal = {Procedia Computer Science},
volume = {31},
pages = {532-541},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.299},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004761},
author = {Li Lin and Wang Shuang and Liu Yifang and Wang Shouyang},
keywords = {debt cost, big data, quality of accounting information, corporate governance, LASSO method},
abstract = {Under the background of big data era today, once been widely used method – multiple linear regressions can not satisfy people's need to handle big data any more because of its bad characteristics such as multicollinearity, instability, subjectivity in model chosen etc. Contrary to MLR, LASSO method has many good natures. it is stable and can handle multicollinearity and successfully select the best model and do estimation in the same time. LASSO method is an effective improvement of multiple linear regressions. It is a natural change and innovation to introduce LASSO method into the accounting field and use it to deal with the debt costs problems. It helps us join the statistic field and accounting field together step by step. What's more, in order to proof the applicability of LASSO method in dealing with debt costs problems, we take 2301 companies’ data from Shanghai and Shenzhen A-share market in 2012 as samples, and chose 18 indexes to verify that the results of LASSO method is scientific, reasonable and accurate. In the end, we compare LASSO method with traditional multiple linear regressions and ridge regression, finding out that LASSO method can not only offer the most accurate prediction but also simplify the model.}
}
@article{ALLES201644,
title = {Incorporating big data in audits: Identifying inhibitors and a research agenda to address those inhibitors},
journal = {International Journal of Accounting Information Systems},
volume = {22},
pages = {44-59},
year = {2016},
note = {2015 Research Symposium on Information Integrity & Information Systems Assurance},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300811},
author = {Michael Alles and Glen L. Gray},
keywords = {Big Data, Auditing, Accounting information systems},
abstract = {With corporate investment in Big Data of $34 billion in 2013 growing to $232 billion through 2016 (Gartner 2012), the Big 4 accounting firms are aiming to be at the forefront of Big Data implementations. Notably, they see Big Data as an increasingly essential part of their assurance practice. We argue that while there is a place for Big Data in auditing, its application to auditing is less clear than it is in the other fields, such as marketing and medical research. The objectives of this paper are to: (1) provide a discussion of both the inhibitors of incorporating Big Data into financial statement audits; and (3) present a research agenda to identify approaches to ameliorate those inhibitors.}
}
@article{SARALADEVI2015596,
title = {Big Data and Hadoop-a Study in Security Perspective},
journal = {Procedia Computer Science},
volume = {50},
pages = {596-601},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.091},
url = {https://www.sciencedirect.com/science/article/pii/S187705091500592X},
author = {B. Saraladevi and N. Pazhaniraja and P. Victer Paul and M.S. Saleem Basha and P. Dhavachelvan},
keywords = {Big data ;Hadoop ;HDFS ;Security},
abstract = {Big data is the collection and analysis of large set of data which holds many intelligence and raw information based on user data, Sensor data, Medical and Enterprise data. The Hadoop platform is used to Store, Manage, and Distribute Big data across several server nodes. This paper shows the Big data issues and focused more on security issue arises in Hadoop Architecture base layer called Hadoop Distributed File System (HDFS). The HDFS security is enhanced by using three approaches like Kerberos, Algorithm and Name node.}
}
@article{YU201621,
title = {Single-cell Transcriptome Study as Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {14},
number = {1},
pages = {21-30},
year = {2016},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022916000437},
author = {Pingjian Yu and Wei Lin},
keywords = {Single cell, RNA-seq, Big data, Transcriptional heterogeneity, Signal normalization},
abstract = {The rapid growth of single-cell RNA-seq studies (scRNA-seq) demands efficient data storage, processing, and analysis. Big-data technology provides a framework that facilitates the comprehensive discovery of biological signals from inter-institutional scRNA-seq datasets. The strategies to solve the stochastic and heterogeneous single-cell transcriptome signal are discussed in this article. After extensively reviewing the available big-data applications of next-generation sequencing (NGS)-based studies, we propose a workflow that accounts for the unique characteristics of scRNA-seq data and primary objectives of single-cell studies.}
}
@incollection{SHEIKH2013185,
title = {Chapter 11 - Big Data, Hadoop, and Cloud Computing},
editor = {Nauman Sheikh},
booktitle = {Implementing Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {185-197},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-401696-5},
doi = {https://doi.org/10.1016/B978-0-12-401696-5.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124016965000116},
author = {Nauman Sheikh},
keywords = {Hadoop, Big Data, cloud computing},
abstract = {When the idea for this book was originally conceived, Big Data and Hadoop were not the most popular themes on the tech circuit, although cloud computing was somewhat more prominent. Some of the reviewer feedback suggested that these topics should be addressed in the context of the conceptual layout of analytics solutions. In this chapter their use in an overall analytics solution will be explained using the previous chapters as a foundation. Big Data, Hadoop, and cloud computing are presented as standalone material, each tying back into the overall analytics solution implementations presented in preceding chapters.}
}
@article{201616,
title = {Clinical research and big data},
journal = {Dental Abstracts},
volume = {61},
number = {1},
pages = {16-17},
year = {2016},
issn = {0011-8486},
doi = {https://doi.org/10.1016/j.denabs.2015.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0011848615010043}
}
@article{ASSUNCAO20153,
title = {Big Data computing and clouds: Trends and future directions},
journal = {Journal of Parallel and Distributed Computing},
volume = {79-80},
pages = {3-15},
year = {2015},
note = {Special Issue on Scalable Systems for Big Data Management and Analytics},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514001452},
author = {Marcos D. Assunção and Rodrigo N. Calheiros and Silvia Bianchi and Marco A.S. Netto and Rajkumar Buyya},
keywords = {Big Data, Cloud computing, Analytics, Data management},
abstract = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.}
}
@article{HASHEM2016748,
title = {The role of big data in smart city},
journal = {International Journal of Information Management},
volume = {36},
number = {5},
pages = {748-758},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216302778},
author = {Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma},
keywords = {Smart city, Big data, Internet of things, Smart environments, Cloud computing, Distributed computing},
abstract = {The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{MCDERMOTT2015303,
title = {What are the implications of the big data paradigm shift for disability and health?},
journal = {Disability and Health Journal},
volume = {8},
number = {3},
pages = {303-304},
year = {2015},
issn = {1936-6574},
doi = {https://doi.org/10.1016/j.dhjo.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1936657415000515},
author = {Suzanne McDermott and Margaret A. Turk}
}
@article{BUDHIRAJA2016241,
title = {The Role of Big Data in the Management of Sleep-Disordered Breathing},
journal = {Sleep Medicine Clinics},
volume = {11},
number = {2},
pages = {241-255},
year = {2016},
note = {Novel Approaches to the Management of Sleep-Disordered Breathing},
issn = {1556-407X},
doi = {https://doi.org/10.1016/j.jsmc.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1556407X1630008X},
author = {Rohit Budhiraja and Robert Thomas and Matthew Kim and Susan Redline},
keywords = {Sleep-disordered breathing, Big data, Management, Sleep apnea}
}
@article{VITOLO2015185,
title = {Web technologies for environmental Big Data},
journal = {Environmental Modelling & Software},
volume = {63},
pages = {185-198},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002965},
author = {Claudia Vitolo and Yehia Elkhatib and Dominik Reusser and Christopher J.A. Macleod and Wouter Buytaert},
keywords = {Web-based modelling, Big Data, Web services, OGC standards},
abstract = {Recent evolutions in computing science and web technology provide the environmental community with continuously expanding resources for data collection and analysis that pose unprecedented challenges to the design of analysis methods, workflows, and interaction with data sets. In the light of the recent UK Research Council funded Environmental Virtual Observatory pilot project, this paper gives an overview of currently available implementations related to web-based technologies for processing large and heterogeneous datasets and discuss their relevance within the context of environmental data processing, simulation and prediction. We found that, the processing of the simple datasets used in the pilot proved to be relatively straightforward using a combination of R, RPy2, PyWPS and PostgreSQL. However, the use of NoSQL databases and more versatile frameworks such as OGC standard based implementations may provide a wider and more flexible set of features that particularly facilitate working with larger volumes and more heterogeneous data sources.}
}
@article{GIL201696,
title = {Modeling and Management of Big Data: Challenges and opportunities},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {96-99},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15002514},
author = {David Gil and Il-Yeol Song},
keywords = {Conceptual modeling Big Data, Ecosystem, Integrate & analyze & visualize},
abstract = {The term Big Data denotes huge-volume, complex, rapid growing datasets with numerous, autonomous and independent sources. In these new circumstances Big Data bring many attractive opportunities; however, good opportunities are always followed by challenges, such as modelling, new paradigms, novel architectures that require original approaches to address data complexities. The purpose of this special issue on Modeling and Management of Big Data is to discuss research and experience in modelling and to develop as well as deploy systems and techniques to deal with Big Data. A summary of the selected papers is presented, followed by a conceptual modelling proposal for Big Data. Big Data creates new requirements based on complexities in data capture, data storage, data analysis and data visualization. These concerns are discussed in detail in this study and proposals are recommended for specific areas of future research.}
}
@incollection{KRISHNAN2013241,
title = {Chapter 12 - Information Management and Life Cycle for Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {241-250},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405891000012X},
author = {Krish Krishnan},
keywords = {information life-cycle management, governance, program governance, data governance, data quality},
abstract = {This chapter deals with how to implement information life-cycle management principles to Big Data and create a sustainable process that will ensure that business continuity is not interrupted and data is available on demand.}
}
@incollection{KRISHNAN20133,
title = {Chapter 1 - Introduction to Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {3-14},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000015},
author = {Krish Krishnan},
keywords = {Big Data, data warehousing, sentiments, social media, machine data},
abstract = {Why this book? Why now? The goal of this book is to provide readers with a concise perspective into the biggest buzz in the industry—Big Data—and, more importantly, its impact on data processing, management, decision support, and data warehousing. At the time of this writing, there is a lot of interest to adopt a Big Data solution, but the profound confusion is what is the future of data warehousing and many investments that have been made over the years into building the decision support platform. This book addresses those areas of concern and provides readers an introduction to the next-generation of data management and data warehousing. This chapter provides you a concise and example driven introduction to what is Big Data, and how any organization needs to understand the value of Big Data.}
}
@article{OBRIEN2015442,
title = {‘Accounting’ for Data Quality in Enterprise Systems},
journal = {Procedia Computer Science},
volume = {64},
pages = {442-449},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.539},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915026745},
author = {Tony O’Brien},
keywords = {Data Quality, Enterprise Systems, Accounting Information Systems, ERP, SCM, CRM, Big Data},
abstract = {Organisations are facing ever more diverse challenges in managing their enterprise systems as emerging technologies bring both added complexities as well as opportunities to the way they conduct their business. Underpinning this ever-increasing volatility is the importance of having quality data to provide information to make those important enterprise-wide decisions. Numerous studies suggest that many organisations are not paying enough attention to their data and that a major cause of this is their failure to measure its quality and value and/or evaluate the costs of having poor data. This study proposes an integrated framework that organisations can adopt as part of their financial and management control processes to provide a mechanism for quantifying data problems, costing potential solutions and monitoring the on-going costs and benefits, to assist them in improving and then sustaining the quality of their data.}
}
@article{DEMIRKAN2013412,
title = {Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud},
journal = {Decision Support Systems},
volume = {55},
number = {1},
pages = {412-421},
year = {2013},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2012.05.048},
url = {https://www.sciencedirect.com/science/article/pii/S0167923612001595},
author = {Haluk Demirkan and Dursun Delen},
keywords = {Cloud computing, Service orientation, Service science, Data-as-a-service, Information-as-a-service, Analytics-as-a-service, Big data},
abstract = {Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.}
}
@article{CONNELLY20161,
title = {The role of administrative data in the big data revolution in social science research},
journal = {Social Science Research},
volume = {59},
pages = {1-12},
year = {2016},
note = {Special issue on Big Data in the Social Sciences},
issn = {0049-089X},
doi = {https://doi.org/10.1016/j.ssresearch.2016.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0049089X1630206X},
author = {Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben},
keywords = {Big data, Administrative data, Data management, Data quality, Data access},
abstract = {The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.}
}
@article{JIN201559,
title = {Significance and Challenges of Big Data Research},
journal = {Big Data Research},
volume = {2},
number = {2},
pages = {59-64},
year = {2015},
note = {Visions on Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000076},
author = {Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang},
keywords = {Big data, Data complexity, Computational complexity, System complexity},
abstract = {In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.}
}
@article{XIAO2014594,
title = {Knowledge diffusion path analysis of data quality literature: A main path analysis},
journal = {Journal of Informetrics},
volume = {8},
number = {3},
pages = {594-605},
year = {2014},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2014.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1751157714000492},
author = {Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou},
keywords = {Data quality, Main path analysis, Knowledge diffusion, Citation analysis, Social network analysis, Big data},
abstract = {This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.}
}
@incollection{LOSHIN201329,
title = {Chapter 4 - Developing a Strategy for Integrating Big Data Analytics into the Enterprise},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {29-37},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000041},
author = {David Loshin},
keywords = {Strategic plan, business requirements, technology adoption, massive scalability, data reuse, data repurposing, oversight, governance, mainstreaming technology, enterprise integration},
abstract = {This chapter expands on the previous one by looking at some key issues that often plague new technology adoption and show that the key issues are not new ones, and that there is likely to be organizational knowledge that can help in fleshing out a reasonable strategic plan. We look at the typical hype cycle, and how its flaws can be mitigated by instituting good practices for defining expectations and continuing to measure performance. We help define the acceptability criteria for evaluating the result of a big data pilot that can be used to make a go/no-go decision. We then pose some thoughts about preparing the organization for massive scalability, data reuse, and the need for oversight and governance. The objective is to provide a pathway for mainstreaming big data into the technology infrastructure that is integrated with the existing investment.}
}
@article{CLARK2016443,
title = {Big data and ophthalmic research},
journal = {Survey of Ophthalmology},
volume = {61},
number = {4},
pages = {443-465},
year = {2016},
issn = {0039-6257},
doi = {https://doi.org/10.1016/j.survophthal.2016.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0039625716000023},
author = {Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens},
keywords = {data linkage, clinical registry, health services research, ophthalmic epidemiology, big data},
abstract = {Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research—including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.}
}
@incollection{CELKO2014119,
title = {Chapter 9 - Big Data and Cloud Computing},
editor = {Joe Celko},
booktitle = {Joe Celko’s Complete Guide to NoSQL},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {119-128},
year = {2014},
isbn = {978-0-12-407192-6},
doi = {https://doi.org/10.1016/B978-0-12-407192-6.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124071926000091},
author = {Joe Celko},
keywords = {Forrester Research, V-list, cloud computing, Big Data, data mining},
abstract = {Big Data is largely a buzzword in IT right now. It was coined by Forrester Research to put a wrapper around existing database mining, data management, and other extensions of existing technology to the current hardware. The goal is to use mixed tools with larger volumes of several different forms of data being brought together under one roof. Along with this approach to data, we are also concerned with cloud computing, which is a public or private Internet network that replaces the tradition hardwired landlines within a company.}
}