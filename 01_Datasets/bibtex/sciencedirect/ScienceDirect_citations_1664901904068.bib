@article{PAVEL20224837,
title = {The potential of a data centred approach & knowledge graph data representation in chemical safety and drug design},
journal = {Computational and Structural Biotechnology Journal},
volume = {20},
pages = {4837-4849},
year = {2022},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2022.08.061},
url = {https://www.sciencedirect.com/science/article/pii/S2001037022003956},
author = {Alisa Pavel and Laura A. Saarimäki and Lena Möbus and Antonio Federico and Angela Serra and Dario Greco},
keywords = {Knowledge graph, Big data, Data integration, Toxicology, Drug design, Chemical safety},
abstract = {Big Data pervades nearly all areas of life sciences, yet the analysis of large integrated data sets remains a major challenge. Moreover, the field of life sciences is highly fragmented and, consequently, so is its data, knowledge, and standards. This, in turn, makes integrated data analysis and knowledge gathering across sub-fields a demanding task. At the same time, the integration of various research angles and data types is crucial for modelling the complexity of organisms and biological processes in a holistic manner. This is especially valid in the context of drug development and chemical safety assessment where computational methods can provide solutions for the urgent need of fast, effective, and sustainable approaches. At the same time, such computational methods require the development of methodologies suitable for an integrated and data centred Big Data view. Here we discuss Knowledge Graphs (KG) as a solution to a data centred analysis approach for drug and chemical development and safety assessment. KGs are knowledge bases, data analysis engines, and knowledge discovery systems all in one, allowing them to be used from simple data retrieval, over meta-analysis to complex predictive and knowledge discovery systems. Therefore, KGs have immense potential to advance the data centred approach, the re-usability, and informativity of data. Furthermore, they can improve the power of analysis, and the complexity of modelled processes, all while providing knowledge in a natively human understandable network data model.}
}
@article{LEE20221728,
title = {Quality assurance of integrative big data for medical research within a multihospital system},
journal = {Journal of the Formosan Medical Association},
volume = {121},
number = {9},
pages = {1728-1738},
year = {2022},
issn = {0929-6646},
doi = {https://doi.org/10.1016/j.jfma.2021.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S092966462100591X},
author = {Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang},
keywords = {Big data, Electronic health record, Evidence based healthcare management, Validation study},
abstract = {Background
The need is growing to create medical big data based on the electronic health records collected from different hospitals. Errors for sure occur and how to correct them should be explored.
Methods
Electronic health records of 9,197,817 patients and 53,081,148 visits, totaling about 500 million records for 2006–2016, were transmitted from eight hospitals into an integrated database. We randomly selected 10% of patients, accumulated the primary keys for their tabulated data, and compared the key numbers in the transmitted data with those of the raw data. Errors were identified based on statistical testing and clinical reasoning.
Results
Data were recorded in 1573 tables. Among these, 58 (3.7%) had different key numbers, with the maximum of 16.34/1000. Statistical differences (P < 0.05) were found in 34 (58.6%), of which 15 were caused by changes in diagnostic codes, wrong accounts, or modified orders. For the rest, the differences were related to accumulation of hospital visits over time. In the remaining 24 tables (41.4%) without significant differences, three were revised because of incorrect computer programming or wrong accounts. For the rest, the programming was correct and absolute differences were negligible. The applicability was confirmed using the data of 2,730,883 patients and 15,647,468 patient-visits transmitted during 2017–2018, in which 10 (3.5%) tables were corrected.
Conclusion
Significant magnitude of inconsistent data does exist during the transmission of big data from diverse sources. Systematic validation is essential. Comparing the number of data tabulated using the primary keys allow us to rapidly identify and correct these scattered errors.}
}
@article{CREMIN2022138,
title = {Big data: Historic advances and emerging trends in biomedical research},
journal = {Current Research in Biotechnology},
volume = {4},
pages = {138-151},
year = {2022},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2590262822000090},
author = {Conor John Cremin and Sabyasachi Dash and Xiaofeng Huang},
keywords = {Big data, Big data in biomedicine, Data analytics in biomedical research, Multiomics big data, Biomedical data management, Big data in personalized medicine},
abstract = {Big data is transforming biomedical research by integrating massive amounts of data from laboratory experiments, clinical investigations, healthcare records, and the internet of things. Specifically, the increasing rate at which information is obtained from omics technologies (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and pharmacogenomics) is providing an opportunity for future advances in personalized medicine that are paving the way to improved patient care. The recent advances in omics technologies are profoundly contributing to big data in biomedicine and are anticipated to aid in disease diagnosis and patient care management. Herein, we critically review the major computational techniques, algorithms, and their outcomes that have contributed to recent advances in big data generated from biomedical research in various complex human diseases, such as cancer and infectious diseases. Finally, we discuss trends in the field and the future directions that must be considered to advance the influence of big data on biomedical research and its translation in the healthcare industry.}
}
@article{JACOBBRASSARD2022,
title = {Big Data: Using Databases and Registries},
journal = {Seminars in Vascular Surgery},
year = {2022},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2022.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S089579672200062X},
author = {Jean Jacob-Brassard and Charles {de Mestral}},
abstract = {ABSTRACT
The field of vascular surgery is in constant evolution. Administrative data and registries can provide important contemporary evidence to inform clinical decision making and the delivery of health services. The following review outlines some important considerations for retrospective studies using administrative health databases and registries. First, these data sources have advantages (e.g. real-world applicability, timely data access, relatively lower research cost) and disadvantages (e.g. potential missing data, selection bias, confounding bias) that may be more or less relevant to different administrative databases or registries. Second, we discuss a framework to guide data source selection and provide a summary of frequently used data sources in vascular surgery research. Third, a retrospective study design warrants pre-planned exposure, outcome and covariate definitions and, when studying an exposure-outcome association, careful consideration of confounders through direct acyclic graphs. Finally, investigators must plan the most appropriate analytic approach and we distinguish descriptive, explanatory, and predictive analyses.}
}
@article{KEDDY2022e130,
title = {Using big data and mobile health to manage diarrhoeal disease in children in low-income and middle-income countries: societal barriers and ethical implications},
journal = {The Lancet Infectious Diseases},
volume = {22},
number = {5},
pages = {e130-e142},
year = {2022},
issn = {1473-3099},
doi = {https://doi.org/10.1016/S1473-3099(21)00585-5},
url = {https://www.sciencedirect.com/science/article/pii/S1473309921005855},
author = {Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and Farah Naz Qamar and Zoya Haq and Iruka N Okeke},
abstract = {Summary
Diarrhoea is an important cause of morbidity and mortality in children from low-income and middle-income countries (LMICs), despite advances in the management of this condition. Understanding of the causes of diarrhoea in children in LMICs has advanced owing to large multinational studies and big data analytics computing the disease burden, identifying the important variables that have contributed to reducing this burden. The advent of the mobile phone has further enabled the management of childhood diarrhoea by providing both clinical support to health-care workers (such as diagnosis and management) and communicating preventive measures to carers (such as breastfeeding and vaccination reminders) in some settings. There are still challenges in addressing the burden of diarrhoeal diseases, such as incomplete patient information, underrepresented geographical areas, concerns about patient confidentiality, unequal partnerships between study investigators, and the reactive approach to outbreaks. A transparent approach to promote the inclusion of researchers in LMICs could address partnership imbalances. A big data umbrella encompassing cloud-based centralised databases to analyse interlinked human, animal, agricultural, social, and climate data would provide an informative solution to the development of appropriate management protocols in LMICs.}
}
@article{BRAVE2022481,
title = {The perils of working with big data, and a SMALL checklist you can use to recognize them},
journal = {Business Horizons},
volume = {65},
number = {4},
pages = {481-492},
year = {2022},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681321001178},
author = {Scott A. Brave and R. Andrew Butters and Michael Fogarty},
keywords = {Big data, Data analysis, Economic forecasting, Selection bias, Reporting lags, High-frequency data, Real-time forecasts, Leading indicator},
abstract = {The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.}
}
@article{TANG2022e198,
title = {The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review},
journal = {World Neurosurgery},
volume = {162},
pages = {e198-e217},
year = {2022},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2022.02.113},
url = {https://www.sciencedirect.com/science/article/pii/S1878875022002601},
author = {Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms},
keywords = {Big data, Disparities, Health care costs, Health policy, Hospital volume, Machine learning, National Inpatient Sample, Nationwide Inpatient Sample, NIS},
abstract = {Objective
The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.
Methods
We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.
Results
We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.
Conclusions
We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.}
}
@article{CHO2022102477,
title = {What's driving the diffusion of next-generation digital technologies?},
journal = {Technovation},
pages = {102477},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102477},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000244},
author = {Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik},
abstract = {The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.}
}
@article{MOSTEFAOUI2022374,
title = {Big data architecture for connected vehicles: Feedback and application examples from an automotive group},
journal = {Future Generation Computer Systems},
volume = {134},
pages = {374-387},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001492},
author = {Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables},
keywords = {Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA},
abstract = {Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).}
}
@article{YANG202223,
title = {Big data and reference intervals},
journal = {Clinica Chimica Acta},
volume = {527},
pages = {23-32},
year = {2022},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cca.2022.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0009898122000018},
author = {Dan Yang and Zihan Su and Min Zhao},
keywords = {Indirect method, Reference interval, Data pre-processing, Verification, Big data},
abstract = {Although reference intervals (RIs) play an important role in clinical diagnosis, there remain significant differences with respect to race, gender, age and geographic location. Accordingly, the Clinical Laboratory Standards Institute (CLSI) EP28-A3c has recommended that clinical laboratories establish RIs appropriate to their subject population. Unfortunately, the traditional and direct approach to establish RIs relies on the recruitment of a sufficient number of healthy individuals of various age groups, collection and testing of large numbers of specimens and accurate data interpretation. The advent of the big data era has, however, created a unique opportunity to “mine” laboratory information. Unfortunately, this indirect method lacks standardization, consensus support and CLSI guidance. In this review we provide a historical perspective, comprehensively assess data processing and statistical methods, and post-verification analysis to validate this big data approach in establishing laboratory specific RIs.}
}
@article{LI2022119292,
title = {Data cleaning and restoring method for vehicle battery big data platform},
journal = {Applied Energy},
volume = {320},
pages = {119292},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119292},
url = {https://www.sciencedirect.com/science/article/pii/S030626192200647X},
author = {Shuangqi Li and Hongwen He and Pengfei Zhao and Shuang Cheng},
keywords = {Big data, Internet of vehicle, Electric vehicles, Data cleaning, Battery management system, Battery state estimation},
abstract = {Battery is one of the most important and costly devices in electric vehicles (EVs). Developing an efficient battery management method is of great significance to enhancing vehicle safety and economy. Recently developed big-data and cloud platform computing technologies bring a bright perspective for efficient utilization and protection of vehicle batteries. However, a reliable data transmission network and a high-quality cloud battery dataset are indispensable to enable this benefit. This paper makes the first effort to systematically solve data quality problems in cloud-based vehicle battery monitoring and management by developing a novel integrated battery data cleaning framework. In the first stage, the outlier samples are detected by analyzing the temporal features in the battery data time series. The outlier data in the dataset can be accurately detected to avoid their impacts on battery monitoring and management. Then, the abnormal samples, including the noise polluted data and missing value, are restored by a novel future fusion data restoring model. The real electric bus operation data collected by a cloud-based battery monitoring and management platform are used to verify the performance of the developed data cleaning method. More than 93.3% of outlier samples can be detected, and the data restoring error can be limited to 2.11%, which validates the effectiveness of the developed methods. The proposed data cleaning method provides an effective data quality assessment tool in cloud-based vehicle battery management, which can further boost the practical application of the vehicle big data platform and Internet of vehicle.}
}
@article{MOSTAFA2022100363,
title = {Renewable energy management in smart grids by using big data analytics and machine learning},
journal = {Machine Learning with Applications},
volume = {9},
pages = {100363},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000597},
author = {Noha Mostafa and Haitham Saad Mohamed Ramadan and Omar Elfarouk},
keywords = {Energy internet, Renewable energy, Smart grid, Big data analytics, Machine learning, Predictive models},
abstract = {The application of big data in the energy sector is considered as one of the main elements of Energy Internet. Crucial and promising challenges exist especially with the integration of renewable energy sources and smart grids. The ability to collect data and to properly use it for better decision-making is a key feature; in this work, the benefits and challenges of implementing big data analytics for renewable energy power stations are addressed. A framework was developed for the potential implementation of big data analytics for smart grids and renewable energy power utilities. A five-step approach is proposed for predicting the smart grid stability by using five different machine learning methods. Data from a decentralized smart grid data system consisting of 60,000 instances and 12 attributes was used to predict the stability of the system through three different machine learning methods. The results of fitting the penalized linear regression model show an accuracy of 96% for the model implemented using 70% of the data as a training set. Using the random forest tree model has shown 84% accuracy, and the decision tree model has shown 78% accuracy. Both the convolutional neural network model and the gradient boosted decision tree model yielded 87% for the classification model. The main limitation of this work is that the amount of data available in the dataset is considered relatively small for big data analytics; however the cloud computing and real-time event analysis provided was suitable for big data analytics framework. Future research should include bigger datasets with variety of renewable energy sources and demand across more countries.}
}
@article{OESTERREICH2022103685,
title = {What translates big data into business value? A meta-analysis of the impacts of business analytics on firm performance},
journal = {Information & Management},
volume = {59},
number = {6},
pages = {103685},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103685},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000945},
author = {Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg},
keywords = {Business analytics, Business intelligence, IT business value, Firm performance, Meta-analysis, Moderator analysis},
abstract = {The main purpose of this study is to examine the factors that are critical to create business value from business analytics (BA). Therefore, we conduct a meta-analysis of 125 firm-level studies spanning ten years of research from across 26 countries. We found evidence that the social factors of BA, such as human resources, management capabilities, and organizational culture show a greater impact on business value, whereas technical aspects play a minor role in enhancing firm performance. Through these findings, we contribute to the ongoing debate concerning BA business value by synthesizing and validating the findings of the body of knowledge.}
}
@article{WEERASINGHE2022121222,
title = {Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121222},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121222},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006557},
author = {Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin},
keywords = {Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory},
abstract = {The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.}
}
@article{LI202245,
title = {Spatially gap free analysis of aerosol type grids in China: First retrieval via satellite remote sensing and big data analytics},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {193},
pages = {45-59},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622002374},
author = {Ke Li and Kaixu Bai and Mingliang Ma and Jianping Guo and Zhengqiang Li and Gehui Wang and Ni-Bin Chang},
keywords = {Aerosol types, Fine mode fraction, Haze reduction, Satellite remote sensing, Big data analytics},
abstract = {Spatially contiguous aerosol type grids were rarely available for air quality management in the past. To bridge the gap, we developed an integrated remote sensing and big data analytics framework to generate spatially gap-free aerosol type grids between 2000 and 2020 in China. The effect of emission control via environmental management on haze reduction was fully realized for the first time with the aid of satellite-based gap-free aerosol type data. Daily gap-free aerosol fine mode fraction (FMF) data were first derived via a data-driven regression model based on remote sensing big data. According to empirically determined FMF probability distributions over regions with typical emission sources, aerosols in China were classified into eight major types, including typical/atypical/mixed anthropogenic aerosols, typical/atypical/mixed dust, and typical mixed and multiple modes. The results indicated that the gridded FMF estimates derived in this study agreed well with FMF retrievals from AERONET, with correlation coefficient of 0.81 and root mean square error of 0.13. The long-term variations in major aerosol types showed that in China the territory covered by typical anthropogenic aerosols was reduced from 21.38% to 11.76% over the past two decades, while dust aerosols decreased from 6.99% to 2.15%. The declining trend in anthropogenic aerosols could be attributed to reduced coal consumption and/or favorable dispersion conditions, whereas decreasing dust aerosols were largely associated with increased vegetation cover and/or weakened wind speed in the west. Overall, such advancements provide fresh evidence to improve our understanding of the emission control effect on haze pollution variations in China.}
}
@article{PENG2022102674,
title = {The relationship between soil microbial diversity and angelica planting based on network big data},
journal = {Sustainable Energy Technologies and Assessments},
pages = {102674},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102674},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822007238},
author = {Yinan Peng and Ze Ye and Peng Xi and Hongshan Qi and Bin Ji and Zhiye Wang},
keywords = {Big data, Soil microorganisms, Biodiversity analysis, Angelica cultivation, Hadoop systems, Metagenome research},
abstract = {Angelica sinensis is a kind of traditional Chinese medicine with very good blood nourishing effect, and it is cultivated in many regions of China. But with the increasingly severe climate, angelica cultivation has become a big problem. Therefore, this paper starts from the soil microorganisms of angelica planting, and studies the influence of soil biodiversity and angelica planting in the context of big data. This paper proposes a Hadoop system for big data analysis, combining the biocommunity characteristics and metagenomes of soil microorganisms. It then calculates the distance between samples and generates a dissimilarity matrix. Finally, this paper proposes a soil optimization method for angelica planting based on big data analysis of soil microorganisms. In order to optimize the Angelica planting soil designed in this paper, a soil microbial genome comparison experiment and a big data concurrent control test experiment were designed in this paper. It then analyzes the data obtained from the experiment, and the results of the analysis are used to optimize the soil for angelica planting. It finally compares the soil method of Angelica planting designed in this paper with the traditional Angelica planting method. The experimental results show that the survival rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms has increased by 16.09% compared with the traditional Angelica sinensis planting site. The growth rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms increased by 9.64% compared with the traditional Angelica sinensis planting area.}
}
@incollection{DOMINGUEZ2022,
title = {Big Data applications in power systems},
booktitle = {Reference Module in Materials Science and Materials Engineering},
publisher = {Elsevier},
year = {2022},
isbn = {978-0-12-803581-8},
doi = {https://doi.org/10.1016/B978-0-12-821204-2.00073-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212042000738},
author = {Xavier Dominguez and Alvaro Prado and Pablo Arboleya},
keywords = {Artificial Intelligence, Big Data, Big Data Analytics, Data mining, Machine learning, Power systems},
abstract = {Abstract
In the last years, the term Big Data (BD) is becoming ubiquitous in almost every scientific field given the information era we are living. In this respect, power systems is not the exception. This overflood of information can be overwhelming for the different energy stakeholders, nonetheless, numerous opportunities can also be derived from this BD. To do so, it is necessary to perform the corresponding knowledge extraction to this large data. This has been referred as Big Data Analytics (BDA) in the recent literature. In this respect, this work provides a comprehensive understanding of the attributes of BD in the power sector, along with the most representative tools, techniques, applications and challenges related to the incorporation of BDA in this field.}
}
@article{YIN2022104285,
title = {Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning},
journal = {Tunnelling and Underground Space Technology},
volume = {120},
pages = {104285},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.104285},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821004764},
author = {Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan},
keywords = {TBM, Surrounding rock class, Perception model, Unsupervised learning, Supervised learning},
abstract = {The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.}
}
@article{WANG2022106310,
title = {Are the official national data credible? Empirical evidence from statistics quality evaluation of China's coal and its downstream industries},
journal = {Energy Economics},
pages = {106310},
year = {2022},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2022.106310},
url = {https://www.sciencedirect.com/science/article/pii/S014098832200439X},
author = {Delu Wang and Fan Chen and Jingqi Mao and Nannan Liu and Fangyu Rong},
keywords = {Industrial statistics, Data quality, Comprehensive evaluation, Coal-related industry},
abstract = {The authenticity and quality of industrial statistical data directly affects all types of systematic research based on it. Considering the limitations of extant data quality evaluation literature on research objects and evaluation methods, we constructed a new data quality comprehensive inspection and evaluation model based on Benford's Law (BL) and the technique for order of preference by similarity to ideal solution (TOPSIS), selected coal-related industries as the research object, and conducted an empirical test along the research path of “Industry→Province→Indicator”. The results showed that, at industry level, the quality of statistical data for China's coal-related industries from 2001 to 2016 was generally poor. Among the eight sample industries selected, the data quality for five industries (including coal, electricity, and steel) was assessed as poor or slightly poor. Furthermore, at the provincial level, there is significant spatial heterogeneity in the quality of statistical data for various industries affected by factors such as economic structure, marketization level, and industrial diversity. Compared with other types of statistical indicators, industry financial indicators are more prone to data quality problems at the indicator level, and the suspicious indicators of different industries show certain common characteristics and some industry differences. To improve the quality of industrial statistical data and reduce the possible adverse impacts of data quality problems, based on the research findings, we propose targeted countermeasures and suggestions on how to prevent data fraud and effectively identify and rationally use suspicious data.}
}
@article{ULLAH2022103294,
title = {On the scalability of Big Data Cyber Security Analytics systems},
journal = {Journal of Network and Computer Applications},
volume = {198},
pages = {103294},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103294},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002897},
author = {Faheem Ullah and M. Ali Babar},
keywords = {Big data, Cyber security, Adaptation, Scalability, Configuration parameter, Spark},
abstract = {Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.}
}
@article{CHEN2022157581,
title = {Quantifying on-road vehicle emissions during traffic congestion using updated emission factors of light-duty gasoline vehicles and real-world traffic monitoring big data},
journal = {Science of The Total Environment},
volume = {847},
pages = {157581},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.157581},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722046794},
author = {Xue Chen and Linhui Jiang and Yan Xia and Lu Wang and Jianjie Ye and Tangyan Hou and Yibo Zhang and Mengying Li and Zhen Li and Zhe Song and Jiali Li and Yaping Jiang and Pengfei Li and Xiaoye Zhang and Yang Zhang and Daniel Rosenfeld and John H. Seinfeld and Shaocai Yu},
keywords = {On-road vehicle emissions, Traffic congestion, Light-duty gasoline vehicles, Real-world, Big data, Emission factors},
abstract = {Light-duty gasoline vehicles (LDGVs) have made up >90 % of vehicle fleets in China since 2019, moreover, with a high annual growth rate (> 10 %) since 2017. Hence, accurate estimates of air pollutant emissions of these fast-changing LDGVs are vital for air quality management, human healthcare, and ecological protection. However, this issue is poorly quantified due to insufficient reserves of timely updated LDGV emission factors, which are dependent on real-world activity levels. Here we constructed a big dataset of explicit emission profiles (e.g., emission factors and accumulated mileages) for 159,051 LDGVs based on an official I/M database by matching real-time traffic dynamics via real-world traffic monitoring (e.g., traffic volumes and speeds). Consequently, we provide robust evidence that the emission factors of these LDGVs follow a clear heavy-tailed distribution. The top 10 % emitters contributed >60 % to the total fleet emissions, while the bottom 50 % contributed <10 %. Such emission factors were effectively reduced by 75.7–86.2 % as official emission standards upgraded gradually (i.e., from China 2 to China 5) within 13 years from 2004 to 2017. Nevertheless, such achievements would be offset once traffic congestion occurred. In the real world, the typical traffic congestions (i.e., vehicle speed <5 km/h) can lead to emissions 5– 9 times higher than those on non-congested roads (i.e., vehicle speed >50 km/h). These empirical analyses enabled us to propose future traffic scenarios that could harmonize emission standards and traffic congestion. Practical approaches on vehicle emission controls under realistic conditions are proposed, which would provide new insights for future urban vehicle emission management.}
}
@article{HERRMANN2022194,
title = {An ERP Data Quality Assessment Framework for the Implementation of an APS system using Bayesian Networks},
journal = {Procedia Computer Science},
volume = {200},
pages = {194-204},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002277},
author = {Jan-Phillip Herrmann and Sven Tackenberg and Elio Padoano and Jörg Hartlief and Jens Rautenstengel and Christine Loeser and Jörg Böhme},
keywords = {Data Quality Assessment, Advanced Planning, Scheduling, Bayesian Network, Enterprise Resource Planning},
abstract = {In today’s manufacturing industry, enterprise-resource-planning (ERP) systems reach their limit when planning and scheduling production subject to multiple objectives and constraints. Advanced planning and scheduling (APS) systems provide these capabilities and are an extension for ERP systems. However, when integrating an APS and ERP system, the ERP data frequently lacks quality, hindering the APS system from working as required. This paper introduces a data quality (DQ) assessment framework that employs a Bayesian Network (BN) to perform quick DQ assessments based on expert interviews and DQ measurements with actual ERP data. We explain the BN’s functionality, design, and validation and show how using the perceived DQ of experts and a semi-supervised learning algorithm improves the BN’s predictions over time. We discuss applying our framework in an APS system implementation project involving an APS system provider and a medium-sized manufacturer of hydraulic cylinders. Despite considering the DQ assessment framework in such a specific context, it is not restricted to a particular domain. We close by discussing the framework’s limits, particularly the BN as a DQ assessment methodology and future works to improve its performance.}
}
@article{MORANFERNANDEZ2022365,
title = {How important is data quality? Best classifiers vs best features},
journal = {Neurocomputing},
volume = {470},
pages = {365-375},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.05.107},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221011127},
author = {Laura Morán-Fernández and Verónica Bólon-Canedo and Amparo Alonso-Betanzos},
keywords = {Feature selection, Filters, Preprocessing, High dimensionality, Classification, Data analysis},
abstract = {The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.}
}
@article{MUHEIDAT202215,
title = {Emerging Concepts Using Blockchain and Big Data},
journal = {Procedia Computer Science},
volume = {198},
pages = {15-22},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024455},
author = {Fadi Muheidat and Dhaval Patel and Spandana Tammisetty and Lo’ai A. Tawalbeh and Mais Tawalbeh},
keywords = {Blockchain, Bigdata, Data analytics, Smart Cities applications, Healthcare},
abstract = {Blockchain and big data are the emerging technologies that are on the highest agendas of the firms. These are significantly expected to transform the ways in which the business as well as the firm run. These are on the verge of increasing the expectation of the distributed ledgers, which would keep the firms away from struggling challenges. The concept of big data and Blockchain has been used up by various other concepts that would help secure and interpret the information. The ideal solutions offered by these technologies shall address the challenges of big data management as well as for analytics. In addition to that, Blockchain provides its own consensus method, which is the primary means to create an audit trail. This enables users to verify all transactions. The audit trail is a means of verifying the correctness and integrity of every transaction, regardless of who owns the asset. The Blockchain can also verify that different parties of a transaction are following an agreement and not breaking the agreement. Moreover, there have been continuous arguments in the concept of Blockchain at which the bitcoin is fundamental and there are several popular blockchain approaches developed which would deliver performance, security as well as privacy. Apart from this, the use of Blockchain plays a major role in adding an extra data layer for the big data analytics process. Big data is considered secure which cannot be further forged with the network architecture. The current paper shall be discussing the emerging concepts that are using Blockchain as well as big data.}
}
@article{FERNANDES2022817,
title = {Big Data Analytics for Vehicle Multisensory Anomalies Detection},
journal = {Procedia Computer Science},
volume = {204},
pages = {817-824},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008377},
author = {Ana Xavier Fernandes and Pedro Guimarães and Maribel Yasmina Santos},
keywords = {Big Data Warehouse, Big Data, ETL},
abstract = {Autonomous driving is assisted by different sensors, each providing information about certain parameters. What we are looking for is an integrated perspective of all these parameters to drive us into better decisions. To achieve this goal, a system that can handle these Big Data issues regarding volume, velocity and variety is needed. This paper aims to design and develop a real-time Big Data Warehouse repository, integrating the data generated by the multiple sensors developed in the context of IVS (In-Vehicle Sensing) systems; the data to be stored in this repository should be merged, which will imply its processing, consolidation and preparation for the analytical mechanisms that will be required. This multisensory fusion is important because it allows the integration of different perspectives in terms of sensor data, since they complement each other. Therefore, it can enrich the entire analysis process at the decision-making level, for instance, understanding what is going on inside the cockpit.}
}
@article{TANG2022100289,
title = {Big Data in Forecasting Research: A Literature Review},
journal = {Big Data Research},
volume = {27},
pages = {100289},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100289},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621001064},
author = {Ling Tang and Jieyi Li and Hongchuan Du and Ling Li and Jun Wu and Shouyang Wang},
keywords = {Big data, Forecasting, Literature review, Prediction models, Information},
abstract = {With the boom in Internet techniques and computer science, a variety of big data have been introduced into forecasting research, bringing new knowledge and improving prediction models. This paper is the first attempt to conduct a literature review on full-scale big data in forecasting research. By source, big data in forecasting research fell into user-generated content data (from the users on social media in texts, photos, etc.), device-monitored data (by meteorological monitors, smart meters, GPS, etc.) and activity log data (for web searching/visiting, online/offline marketing, clinical treatments, laboratory experiments, etc.). Different data types, bearing distinctive information and characteristics, dominated different forecasting tasks, required different analysis technologies and improved different forecasting models. This survey provides an overall review of big data-based forecasting research, details what (regarding data types and sources), where (forecasting hotspots) and how (analysis and forecasting methods used) big data improved prediction, and offers insights into future prospects.}
}
@article{WU2022129487,
title = {Machine learning in the identification, prediction and exploration of environmental toxicology: Challenges and perspectives},
journal = {Journal of Hazardous Materials},
volume = {438},
pages = {129487},
year = {2022},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2022.129487},
url = {https://www.sciencedirect.com/science/article/pii/S0304389422012808},
author = {Xiaotong Wu and Qixing Zhou and Li Mu and Xiangang Hu},
keywords = {Machine learning, Chemical, Toxicity, Environmental health, Big data},
abstract = {Over the past few decades, data-driven machine learning (ML) has distinguished itself from hypothesis-driven studies and has recently received much attention in environmental toxicology. However, the use of ML in environmental toxicology remains in the early stages, with knowledge gaps, technical bottlenecks in data quality, high-dimensional/heterogeneous/small-sample data analysis and model interpretability, and a lack of an in-depth understanding of environmental toxicology. Given the above problems, we review the recent progress in the literature and highlight state-of-the-art toxicological studies using ML (such as learning and predicting toxicity in complicated biosystems and multiple-factor environmental scenarios of long-term and large-scale pollution). Beyond predicting simple biological endpoints by integrating untargeted omics and adverse outcome pathways, ML development should focus on revealing toxicological mechanisms. The integration of data-driven ML with other methods (e.g., omics analysis and adverse outcome pathway frameworks) endows ML with widely promising application in revealing toxicological mechanisms. High-quality databases and interpretable algorithms are urgently needed for toxicology and environmental science. Addressing the core issues and future challenges for ML in this review may narrow the knowledge gap between environmental toxicity and computational science and facilitate the control of environmental risk in the future.}
}
@article{SALEM20225552,
title = {LODQuMa: A Free-ontology process for Linked (Open) Data quality management},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {5552-5563},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001348},
author = {Samah Salem and Fouzia Benchikha},
keywords = {Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia},
abstract = {For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.}
}
@article{RADONJIC2022,
title = {Artificial intelligence and HRM: HR managers’ perspective on decisiveness and challenges},
journal = {European Management Journal},
year = {2022},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0263237322000883},
author = {Aleksandar Radonjić and Henrique Duarte and Nádia Pereira},
keywords = {Human resources, HR, HRM, e-HRM, Decision-making, Big data (BD), Big data maturity models (BDMM), Artificial intelligence (AI)},
abstract = {Focus
The transformative power of today's big data (BD) has allowed many companies, i.e., decision-makers, to evolve at an unprecedented pace. With regard to decision-making, artificial intelligence (AI) takes task delegation to a new level, and by employing AI-assisted tools, companies can provide their HR departments with the means to manage the existing data and HR altogether.
Objectives
To determine how HR managers assess whether BD management is facilitated by AI, and how they frame the changes necessary to meet the trends related to AI and its implementation, namely their willingness to master its implementation and to meet the possible challenges.
Methodology
Content analysis was conducted on interviews held with a sample of 16 HR practitioners from a spectrum of areas, and the findings were analysed using the big data maturity model (BDMM) framework. Domains covered by this model allow the study of decision-making trends, in terms of preparedness and willingness to tackle disruptive technology with the aim of improving and gaining the competitive edge in decision-making.
Findings
The central potential of AI lies in faster data storage and processing power, thereby leading to more insightful and effective decision-making. This article contains closer insights into the challenges underlying the implementation of AI in decision-making processes, specifically in terms of strategic alignment, governance, and implementation. The results reflect the notions regarding the nature of AI – in assisting HR – and lay out the path that precedes the extraction of BD, through the delivery of advantageous intelligence, to augment decision-making in HR.}
}
@article{KESKAR2022532,
title = {Perspective of anomaly detection in big data for data quality improvement},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {532-537},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.597},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321042243},
author = {Vinaya Keskar and Jyoti Yadav and Ajay Kumar},
keywords = {Credit card, Validation, LUHN, Big data, Bank},
abstract = {The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.}
}
@article{CABALLERO2022102058,
title = {BR4DQ: A methodology for grouping business rules for data quality evaluation},
journal = {Information Systems},
volume = {109},
pages = {102058},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2022.102058},
url = {https://www.sciencedirect.com/science/article/pii/S0306437922000485},
author = {Ismael Caballero and Fernando Gualo and Moisés Rodríguez and Mario Piattini},
keywords = {Business rules, Data quality, Data quality evaluation, Data quality measurement, Data quality characteristics, Data quality properties, ISO/IEC 25012, ISO/IEC 25024},
abstract = {Data quality evaluation is built upon data quality measurement results. “Data quality evaluation” uses the “data quality rules” representing the risk appetite of the organization to decide on the usability of the data; “data quality measurement” uses the business rules describing the “data requirements” or “data specifications” to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO/IEC 25012 and ISO/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.}
}
@article{TRIVEDI2022,
title = {A Practical Guide to Use of Publicly Available Data Sets for Observational Research in Interventional Radiology},
journal = {Journal of Vascular and Interventional Radiology},
year = {2022},
issn = {1051-0443},
doi = {https://doi.org/10.1016/j.jvir.2022.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1051044322011241},
author = {Premal S. Trivedi and Vincent M. Timpone and Rustain L. Morgan and Alexandria M. Jensen and Margaret Reid and P. Michael Ho and Osman Ahmed},
abstract = {Observational data research studying access, utilization, cost, and outcomes of image-guided interventions using publicly available “big data” sets is growing in the interventional radiology (IR) literature. Publicly available data sets offer insight into real-world care and represent an important pillar of IR research moving forward. They offer insights into how IR procedures are being used nationally and whether they are working as intended. On the other hand, large data sources are aggregated using complex sampling frames, and their strengths and weaknesses only become apparent after extensive use. Unintentional misuse of large data sets can result in misleading or sometimes erroneous conclusions. This review introduces the most commonly used databases relevant to IR research, highlights their strengths and limitations, and provides recommendations for use. In addition, it summarizes methodologic best practices pertinent to all data sets for planning and executing scientifically rigorous and clinically relevant observational research.}
}
@article{BAVARESCO2022112197,
title = {Are years-long field studies about window operation efficient? a data-driven approach based on information theory and deep learning},
journal = {Energy and Buildings},
volume = {268},
pages = {112197},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112197},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822003681},
author = {Mateus Bavaresco and Ioannis Kousis and Ilaria Pigliautile and Anna {Laura Pisello} and Cristina Piselli and Enedir Ghisi},
keywords = {Information theory, Occupant behaviour, Energy efficiency, Machine learning, Data length, Big data, Indoor environmental quality, Continuous monitoring},
abstract = {Scientific literature about building occupants’ behaviour and the related energy performance analyses document about several strategies to monitor window operation, including different sensors and data series lengths. In this framework, the primary goal of this study is to propose effective guidelines for minimum experiment durations and their reliability. A six-year-long database from a living laboratory was used as a benchmark; and a recursive strategy enabled to split it into more than 2,500 subsets, supporting two main steps. First, information theory concepts were used to calculate uncertainty and subsets’ divergence were compared to the full database. Second, the subsets were used to train deep neural networks and evaluate the influence of monitoring lengths combined with different kinds of environmental data (i.e. indoor or outdoor). From the information-theoretic metrics, the results support that indoor-related variables can reduce most of the uncertainty related to window operation. Besides, subsets influenced by autumn and winter diverge the most compared to the full database. Considering the modelling approach, the results demonstrated that by including indoor-related variables, higher shares of reliably-performing models were achieved, and smaller subsets were needed. Seasonality has also played a major role along these lines. As a consequence, the conclusions supported the feasibility of nine-month-long field studies, starting in summer or spring, when indoor and outdoor variables are monitored.}
}
@article{GEORGIADIS2022105640,
title = {Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review},
journal = {Computer Law & Security Review},
volume = {44},
pages = {105640},
year = {2022},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105640},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921001138},
author = {Georgios Georgiadis and Geert Poels},
keywords = {Big data analytics, Data protection, Data protection directive, General data protection regulation, Governance, Information security, Privacy, Privacy impact assessment, Systematic literature review},
abstract = {Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics.}
}
@article{LIU2022105219,
title = {Investigation of VRF system cooling operation and performance in residential buildings based on large-scale dataset},
journal = {Journal of Building Engineering},
volume = {61},
pages = {105219},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.105219},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222012256},
author = {Hua Liu and Yi Wu and Da Yan and Shan Hu and Mingyang Qian},
keywords = {Big data, VRF systems, Operational performance, Statistical analysis},
abstract = {With the increase in the energy consumption of air-conditioning (AC) systems in Chinese residential buildings, the realization of energy savings in AC systems has attracted increasing attention. The variable refrigerant flow (VRF) system is a common AC system for residential buildings in China. In most previous studies on VRF systems, onsite measurements or surveys were conducted to collect operational data. These traditional methods may face various data issues, such as limited sample sizes and invalid data, making them unable to capture the spatial and temporal performance features of VRF systems in residential buildings on a large scale. To fill this gap, with advances in data storage and transmission technology, Big Data methods have been widely used for data collection. In the present study, researchers adopted 16,985 sets of VRF system operation data from China as the database and conducted data analysis for both the spatial and temporal dimensions. Several key indicators were proposed from the two perspectives (spatial and temporal), including the part-space index (PSI), load ratio (LR), use duration (UD), and cooling energy consumption. The main findings were as follows: (1) The “part-time part-space” operation mode of residential VRF systems can be analyzed according to the statistical results of the UD and PSI. (2) An LR of <30% is the main operating condition for VRF systems in residential buildings. (3) Extracted typical LR patterns can reflect different user behavior. The statistical results obtained in this study provide a basis for VRF engineering projects.}
}
@article{OESTERREICH2022128,
title = {The role of the social and technical factors in creating business value from big data analytics: A meta-analysis},
journal = {Journal of Business Research},
volume = {153},
pages = {128-149},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.08.028},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322007111},
author = {Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg and Yogesh K Dwivedi},
keywords = {Big data analytics, IT business value, Firm performance, Meta-analysis, Moderator analysis, Sociotechnical theory},
abstract = {Big data analytics (BDA) has recently gained importance as an emerging technology for handling big data. The use of advanced techniques with differing levels of intelligence, such as descriptive, predictive, prescriptive, and autonomous analytics, is expected to create value for firms. By viewing BDA as a sociotechnical system, we conduct a meta-analysis of 107 individual studies to integrate prior evidence on the role of the technical and social factors of BDA in creating BDA business value. The findings underline the predominant role of the social components in enhancing firm performance, such as the BDA system’s human factors and a nurturing organizational structure, in contrast to the minor role of the technological factors. However, both the technical and social factors are found to be strong determinants of BDA business value. Through the combined lens of sociotechnical theory and the IS business value framework, we contribute to research and practice by enhancing the understanding of the main technical and social determinants of BDA business value at the firm level.}
}
@article{ALWIS2022103624,
title = {A survey on smart farming data, applications and techniques},
journal = {Computers in Industry},
volume = {138},
pages = {103624},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103624},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000197},
author = {Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar},
keywords = {Smart farming, Data analysis, Big data, Machine learning, Digital farming, Predictive farming, Farming industry},
abstract = {The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.}
}
@article{CHENG2022134380,
title = {Dirty skies lower subjective well-being},
journal = {Journal of Cleaner Production},
pages = {134380},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134380},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203952X},
author = {Lu Cheng and Zhifu Mi and Yi-Ming Wei and Shidong Wang and Klaus Hubacek},
keywords = {Subjective well-being, Air pollution, Big data, Sentiment analysis},
abstract = {Self-reported life satisfaction of China's population has not improved as much as expected during the economic boom, which was accompanied by a significant decline in environmental performance. Is environmental pollution the culprit for the lagging subjective well-being? To explore this issue, this paper adopts the sentiment analysis method to construct a real-time daily subjective well-being metric at the city level based on the big data of online search traces. Using daily data from 13 Chinese cities centred on Beijing between August 2014 and December 2019, we look at the corelation between subjective well-being and air pollution and the heterogeneity in this relationship based on two separate identification strategies. We find that air pollutants are negatively correlated with subjective well-being, and well-being tends to decline more from pollution during hot seasons. In addition, residents in wealthier regions tend to be more sensitive to air pollution. This result may be explained by the differences in the subjective perception of air pollution and personal preferences at different levels of income. These findings provide information about the concerns of the public to the central government, thereby helping it take appropriate actions to respond to the dynamics of subjective well-being.}
}
@article{NARAYANAN20223121,
title = {A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part B},
pages = {3121-3135},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820303700},
author = {Uma Narayanan and Varghese Paul and Shelbi Joseph},
keywords = {Big data outsourcing, Big data sharing, Big data management, SALSA encryption with MapReduce, Fractal index tree, SHA-3},
abstract = {With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.}
}
@article{GOLDSTEIN2022100215,
title = {Visual Acuity – Assessment of Data Quality and Usability in an Electronic Health Record System},
journal = {Ophthalmology Science},
pages = {100215},
year = {2022},
issn = {2666-9145},
doi = {https://doi.org/10.1016/j.xops.2022.100215},
url = {https://www.sciencedirect.com/science/article/pii/S266691452200104X},
author = {Judith E. Goldstein and Xinxing Guo and Michael V. Boland and Kerry E. Smith},
abstract = {ABSTRACT
Objective
To examine the data quality and usability of visual acuity (VA) data extracted from an electronic health record (EHR) system during ophthalmology encounters and provide recommendations for consideration of relevant VA endpoints in retrospective analyses.
Design
Retrospective, EHR data analysis
Participants
All patients with eyecare office encounters at any one of the nine locations of a large academic medical center between August 1st 2013 and December 31st 2015.
Methods
Data from 13 of the 21 VA fields (accounting for 93% VA data) in EHR encounters were extracted, categorized, recoded, and assessed for conformance and plausibility using an internal data dictionary, a 38-item listing of VA line measurements and observations including 28 line-measurements (e.g., 20/30, 20/400) and 10 observations (e.g., NLP [no light perception]). Entries were classified into usable and unusable data. Usable data were further categorized based on conformance to internal data dictionary: (1) exact match; (2) conditional conformance, letter count (e.g., 20/30+2-3); (3) convertible conformance (e.g., 5/200 to 20/800); (4) plausible but cannot be conformed (e.g., 5/400). Data were deemed unusable when they were not plausible.
Main Outcome Measures
Proportions of usable and unusable VA entries at the overall and subspecialty levels.
Results
All VA data from 513,036 encounters representing 166,212 patients were included. Of the 1,573,643 VA entries, 1,438,661 (91.4%) contained usable data. There were 1,196,720 (76.1%) exact match (category 1), 185,692 (11.8%) conditional conformance (category 2), 40,270 (2.6%) convertible conformance (category 3), and 15,979 (1.1%) plausible but not conformed entries. VA entries during visits with providers from retina (17.5%), glaucoma (14.0%), neuro-ophthalmology (8.9%), and low vision (8.8%) had the highest rates of unusable data. Documented VA entries with providers from comprehensive eye care (86.7%), oculoplastics (81.5%), and pediatrics/strabismus (78.6%) yielded the highest proportions of exact match with the data dictionary.
Conclusions
EHR VA data quality and usability vary across documented VA measures, observations, and eyecare subspecialty. We proposed a checklist of considerations and recommendations for planning, extracting, analyzing, and reporting retrospective study outcomes using EHR VA data. These are important first steps to standardize analyses enabling comparative research.}
}
@article{WANG2022e10312,
title = {Impact of big data resources on clinicians’ activation of prior medical knowledge},
journal = {Heliyon},
volume = {8},
number = {9},
pages = {e10312},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e10312},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022016000},
author = {Sufen Wang and Junyi Yuan and Changqing Pan},
keywords = {Big data resources, Activation of prior medical knowledge, Shared big data resources, Private big data resources},
abstract = {Background
Activating prior medical knowledge in diagnosis and treatment is an important basis for clinicians to improve their care ability. However, it has not been systematically explained whether and how various big data resources affect the activation of prior knowledge in the big data environment faced by clinicians.
Objective
The aim of this study is to contribute to a better understanding on how the activation of prior knowledge of clinicians is affected by a wide range of shared and private big data resources, to reveal the impact of big data resources on clinical competence and professional development of clinicians.
Method
Through the comprehensive analysis of extant research results, big data resources are classified as big data itself, big data technology and big data services at the public and institutional levels. A survey was conducted on clinicians and IT personnel in Chinese hospitals. A total of 616 surveys are completed, involving 308 medical institutions. Each medical institution includes a clinician and an IT personnel. SmartPLS version 2.0 software package was used to test the direct impact of big data resources on the activation of prior knowledge. We further analyze their indirect impact of those big data resources without direct impact.
Results
(1) Big data quality environment at the institutional level and the big data sharing environment at the public level directly affect activation of prior medical knowledge; (2) Big data service environment at the institutional level directly affects activation of prior medical knowledge; (3) Big data deployment environment at the institutional level and big data service environment at the public level have no direct impact on activation of prior knowledge of clinicians, but they have an indirect impact through big data quality environment and service environment at the institutional level and the big data sharing environment at the public level.
Conclusions
Big data technology, big data itself and big data service at the public level and institutional level interact and influence each other to activate prior medical knowledge. This study highlights the implications of big data resources on improvement of clinicians’ diagnosis and treatment ability.}
}
@article{WANG2022e97,
title = {Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care},
journal = {Clinical Oncology},
volume = {34},
number = {2},
pages = {e97-e103},
year = {2022},
note = {Artificial Intelligence in Radiation Therapy},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2021.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0936655521004593},
author = {J.W. Wang and M. Williams},
keywords = {Artificial intelligence, Big Data, database, deep learning, registries, repository},
abstract = {Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.}
}
@article{SUN2022,
title = {A blockchain-based audit approach for encrypted data in federated learning},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000979},
author = {Zhe Sun and Junping Wan and Lihua Yin and Zhiqiang Cao and Tianjie Luo and Bin Wang},
keywords = {Audit, Data quality, Blockchain, Secure aggregation, Federated learning},
abstract = {The development of data-driven artificial intelligence technology has given birth to a variety of big data applications. Data has become an essential factor to improve these applications. Federated learning, a privacy-preserving machine learning method, is proposed to leverage data from different data owners. It is typically used in conjunction with cryptographic methods, in which data owners train the global model by sharing encrypted model updates. However, data encryption makes it difficult to identify the quality of these model updates. Malicious data owners may launch attacks such as data poisoning and free-riding. To defend against such attacks, it is necessary to find an approach to audit encrypted model updates. In this paper, we propose a blockchain-based audit approach for encrypted gradients. It uses a behavior chain to record the encrypted gradients from data owners, and an audit chain to evaluate the gradients’ quality. Specifically, we propose a privacy-preserving homomorphic noise mechanism in which the noise of each gradient sums to zero after aggregation, ensuring the availability of aggregated gradient. In addition, we design a joint audit algorithm that can locate malicious data owners without decrypting individual gradients. Through security analysis and experimental evaluation, we demonstrate that our approach can defend against malicious gradient attacks in federated learning.}
}
@article{JAGATHEESAPERUMAL2022107691,
title = {A holistic survey on the use of emerging technologies to provision secure healthcare solutions},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107691},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107691},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000131},
author = {Senthil Kumar Jagatheesaperumal and Preeti Mishra and Nour Moustafa and Rahul Chauhan},
keywords = {Healthcare, Security, Internet of Things, Artificial intelligence, Machine learning, Deep learning, 5G networks},
abstract = {Healthcare applications demand systematic approaches to eradicate inevitable human errors to design a framework that systematically eliminates cyber-threats. The key focus of this paper is to provide a comprehensive survey on the use of modern enabling technologies, such as the Internet of Things (IoT), 5G networks, artificial intelligence (AI), and big data analytics, for providing secure and resilient healthcare solutions. A detailed taxonomy of existing technologies has been demonstrated for tackling various healthcare problems, along with their security-related issues in handling healthcare data. The application areas of each of the emerging technologies, along with their security aspects, are explained. Furthermore, an IoT-enabled smart pill bottle prototype is designed and illustrated as a case study for providing better understanding of the subject. Finally, various key research challenges are summarized with future research directions.}
}
@article{HORNG202222,
title = {Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view},
journal = {Journal of Hospitality and Tourism Management},
volume = {51},
pages = {22-38},
year = {2022},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2022.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S1447677022000389},
author = {Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu},
keywords = {Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy},
abstract = {To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.}
}
@article{LI2022129190,
title = {Big data nanoindentation characterization of cross-scale mechanical properties of oilwell cement-elastomer composites},
journal = {Construction and Building Materials},
volume = {354},
pages = {129190},
year = {2022},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2022.129190},
url = {https://www.sciencedirect.com/science/article/pii/S095006182202846X},
author = {Yucheng Li and Yunhu Lu and Li Liu and Shengmin Luo and Li He and Yongfeng Deng and Guoping Zhang},
keywords = {Big data nanoindentation, Cross-scale characterization, Elastomer, Microstructure, Oilwell cement, Young’s modulus},
abstract = {Intensive big data nanoindentation (BDNi) characterization was performed to reveal the cross-scale mechanical properties of, and hence distinguish the different phases in, inorganic–organic hybrid oilwell cement-elastomer composites, hydrothermally cured at 160 °C and 20 MPa for 28 days. Totally-three emulsified and particulate elastomers, including styrene-butadiene latex (SBL) emulsion (6, 12, and 14 wt.%), polypropylene (PP) powder (12 wt.%), and nitrile rubber (NR) powder (6 wt.%), and a weighting agent, hematite (50 wt.%), were used as additives to finely adjust the mechanical properties and microstructure of the hybrid composites, which were respectively examined by the BDNi and mercury intrusion porosimetry and scanning electron microscopy. BDNi data were statistically deconvoluted by the Gaussian mixture modeling (GMM) to discern mechanically distinct phases and their Young’s moduli and hardness at the micro/nano scale and the bulk composites’ properties at the macro scale. Results show that the SBL emulsion can be more homogeneously dispersed into the cement matrix, due to its emulsified soft consistency and hydrophilicity, resulting in the formation of soft coatings on, and softer infills intermixed with, the cement hydration products (CHPs). In contrast, the two hydrophobic, inert, particulate elastomers, PP and NR powders, only act as isolated soft inclusions embedded in the hydrated cement matrix. The NR melts at high temperatures and permeates into the pores of the cement matrix, leading to the formation of complex intervened micromorphology and hence functions better than the PP. All elastomers can effectively reduce the composites’ Young’s moduli: with increasing the elastomer contents, while the modulus of a BDNi-identified major CHP phase decreases from 20.9 to 11.3 GPa, the bulk composites’ counterpart from 17.3 to 10.7 GPa. The BDNi enables the identification of multiple mechanically distinct phases in the hybrid composites and quantification of the property changes of these phases.}
}
@article{COZZINI2022133422,
title = {Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family},
journal = {Chemosphere},
volume = {292},
pages = {133422},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2021.133422},
url = {https://www.sciencedirect.com/science/article/pii/S0045653521038960},
author = {Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani},
keywords = {Computational chemistry, Consensus prediction, Database, Nuclear receptors, Toxicology},
abstract = {According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.}
}
@article{PENG2022104,
title = {Industrial big data-driven mechanical performance prediction for hot-rolling steel using lower upper bound estimation method},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {104-114},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S027861252200142X},
author = {Gongzhuang Peng and Yinliang Cheng and Yufei Zhang and Jian Shao and Hongwei Wang and Weiming Shen},
keywords = {Industrial big data, Mechanical performances prediction, Lower upper bound estimation, Broad learning system, Hot-rolling},
abstract = {Industrial big data technology has become one of the important driving forces to intelligent manufacturing in the steel industry. In this study, the characteristics of data in steel production are analyzed and an industrial big data platform for steeling process is developed to extract the quality-related parameters. A data-driven approach to construct prediction intervals (PIs) of mechanical performances for hot-rolling strips is proposed to represent the uncertainty and reliability of the prediction results. The proposed method employs a new manifold visualization method, SLISEMAP, to reduce the feature dimensions with interpretability, utilizes lower upper bound estimation (LUBE) method to obtain the PIs, in which the broad learning system (BLS) is used as the basic training network model and the artificial bee colony (ABC) algorithm is applied to optimize the weighting parameters of BLS under the LUBE framework. A hot-rolling steel coil dataset consisting of 39 variables and 1335 coil samples is used to validate the proposed method. Two Delta-based approaches, namely back propagation neural network (BPNN) and extreme learning machine (ELM); and three LUBE-based approaches, namely ABC-BPNN, ABC-ELM, and ABC-support vector regression (SVR) are compared with the proposed method. Results show that the proposed ABC-BLS in LUBE is effective and efficient in constructing the PIs with a higher coverage probability and a narrower interval width.}
}
@article{OUAFIQ2022102093,
title = {AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities},
journal = {Sustainable Energy Technologies and Assessments},
volume = {52},
pages = {102093},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102093},
url = {https://www.sciencedirect.com/science/article/pii/S221313882200145X},
author = {El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon},
keywords = {Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability},
abstract = {The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.}
}
@article{XU2022,
title = {Smart breeding driven by big data, artificial intelligence and integrated genomic-enviromic prediction},
journal = {Molecular Plant},
year = {2022},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1674205222002957},
author = {Yunbi Xu and Xingping Zhang and Huihui Li and Hongjian Zheng and Jianan Zhang and Michael S. Olsen and Rajeev K. Varshney and Boddupalli M. Prasanna and Qian Qian},
keywords = {smart breeding, genomic selection, integrated genomic-enviromic selection, spatiotemporal omics, crop design, machine and deep learning, big data, artificial intelligence},
abstract = {The first paradigm of plant breeding involves direct selection based phenotypic observation, followed by predictive breeding using statistical models constructed for quantitative traits based on genetic experimental design and more recently by incorporating molecular marker genotypes. However, plant performance or phenotype (P) is determined by the combining effects of genotype (G), envirotype (E) and genotype by environment interaction (GEI). Phenotypes can be predicted more precisely by training a model using data collected from multiple sources, including spatiotemporal omics (genomics, phenomics and enviromics across time and space). Integration of 3D information profiles (G-P-E), each with multidimensionality, provides predictive breeding with both tremendous opportunities and great challenges. Here, we first review innovative technologies for predictive breeding. We then evaluate multidimensional information profiles that can be integrated with a predictive breeding strategy, particularly envirotypic data, which have largely been neglected in data collection and nearly untouched in model construction. We propose a smart breeding scheme, integrated genomic-enviromic prediction (iGEP), as an extension of genomic prediction, using integrated multiomics information, big data technology and artificial intelligence (mainly focus on machine and deep learning). How to implement iGEP was discussed, including spatiotemporal models, environmental indices, factorial and spatiotemporal structure of plant breeding data, and cross-species prediction. A strategy is then proposed for prediction-based crop redesign at both the macro (individual, population and species) and micro (gene, metabolism and network) scales. Finally, we provide perspectives on translating the smart breeding into genetic gain through integrative breeding platforms and open-source breeding initiatives. We call for coordinated efforts in smart breeding through iGEP, institutional partnerships, and innovative technological support.}
}
@article{DEVI20224980,
title = {Traffic management in smart cities using support vector machine for predicting the accuracy during peak traffic conditions},
journal = {Materials Today: Proceedings},
volume = {62},
pages = {4980-4984},
year = {2022},
note = {International Conference on Innovative Technology for Sustainable Development},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.03.722},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322022131},
author = {T. Devi and K. Alice and N. Deepa},
keywords = {Big data, Mobility, Linear and logistic regression, Support vector machine (SVM), Traffic management},
abstract = {Mobility is the main key for smart living, where navigation and automatic suggestions are also a strategy for a successful life in smart cities. Big Data analytics are behind urban changes in the mobility of smart cities to bring sustainable life. By the year 2025 all over Indian states can reach the expected lifestyle by providing high security and mobility which can grow the opportunities also high. As the population is rapidly increasing, the needs of people are also increasing such that necessitating real-time apps for daily needs, communication devices, and so on. We focus our idea on the benefits of traffic and safety measures which are becoming a huge challenge nowadays. Many are preferred with sophistication when traveling for short distances. In such a way the big data analytics tools R studio and weka are used on the dataset smart city from the Kaggle website for traffic patterns during the high traffic duration. Using the dataset, the data are classified using a Support vector machine (SVM) and applied with regression such as linear, logistic regression to find the accuracy of traffic peak situations. The proposed work aims to compare the efficiency of big data technologies which can be applied using various classification and regression that can be shown on various tools such as R, Weka, map-reduce which can produce accurate results to visualize the smart cities and their traffic analysis.}
}
@article{GOKALP2022103585,
title = {A process assessment model for big data analytics},
journal = {Computer Standards & Interfaces},
volume = {80},
pages = {103585},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103585},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000805},
author = {Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Selin Gökalp and Altan Koçyiğit and P. Erhan Eren},
keywords = {Big data, Data analytics, Software development, Software process improvement, Software process assessment},
abstract = {Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs’ potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way.}
}
@article{GUO20221792,
title = {Measuring and evaluating SDG indicators with Big Earth Data},
journal = {Science Bulletin},
volume = {67},
number = {17},
pages = {1792-1801},
year = {2022},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2022.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S2095927322002997},
author = {Huadong Guo and Dong Liang and Zhongchang Sun and Fang Chen and Xinyuan Wang and Junsheng Li and Li Zhu and Jinhu Bian and Yanqiang Wei and Lei Huang and Yu Chen and Dailiang Peng and Xiaosong Li and Shanlong Lu and Jie Liu and Zeeshan Shirazi},
keywords = {Big Earth Data, Big data, Sustainable Development Goals (SDGs), Decision support, CASEarth, Digital Earth},
abstract = {The United Nations 2030 Agenda for Sustainable Development provides an important framework for economic, social, and environmental action. A comprehensive indicator system to aid in the systematic implementation and monitoring of progress toward the Sustainable Development Goals (SDGs) is unfortunately limited in many countries due to lack of data. The availability of a growing amount of multi-source data and rapid advancements in big data methods and infrastructure provide unique opportunities to mitigate these data shortages and develop innovative methodologies for comparatively monitoring SDGs. Big Earth Data, a special class of big data with spatial attributes, holds tremendous potential to facilitate science, technology, and innovation toward implementing SDGs around the world. Several programs and initiatives in China have invested in Big Earth Data infrastructure and capabilities, and have successfully carried out case studies to demonstrate their utility in sustainability science. This paper presents implementations of Big Earth Data in evaluating SDG indicators, including the development of new algorithms, indicator expansion (for SDG 11.4.1) and indicator extension (for SDG 11.3.1), introduction of a biodiversity risk index as a more effective analysis method for SDG 15.5.1, and several new high-quality data products, such as global net ecosystem productivity, high-resolution global mountain green cover index, and endangered species richness. These innovations are used to present a comprehensive analysis of SDGs 2, 6, 11, 13, 14, and 15 from 2010 to 2020 in China utilizing Big Earth Data, concluding that all six SDGs are on schedule to be achieved by 2030.}
}
@article{ENCINAS2022109904,
title = {Downhole data correction for data-driven rate of penetration prediction modeling},
journal = {Journal of Petroleum Science and Engineering},
volume = {210},
pages = {109904},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109904},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521015217},
author = {Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui},
keywords = {Drilling, Machine learning, Rate of penetration, Drilling data quality improvement, Recurrent neural networks},
abstract = {In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.}
}
@article{ALWAN2022101951,
title = {Data quality challenges in large-scale cyber-physical systems: A systematic review},
journal = {Information Systems},
volume = {105},
pages = {101951},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101951},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001484},
author = {Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin},
keywords = {Cyber-physical systems (CPS), Wireless Sensor Networks(WSN), Data quality management, Data quality dimensions, Smart cities, Quality of observations},
abstract = {Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes’ measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes’ observations.}
}
@article{KIM2022100256,
title = {Organizational process maturity model for IoT data quality management},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100256},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100256},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000480},
author = {Sunho Kim and Ricardo Pérez-Castillo and Ismael Caballero and Downgwoo Lee},
keywords = {Data quality, Data quality management, IoT, ISO 8000, Process-centric, Process reference model, Maturity, Process maturity, process attribute},
abstract = {Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.}
}
@article{ROOS202218,
title = {Record linkage and big data—enhancing information and improving design},
journal = {Journal of Clinical Epidemiology},
volume = {150},
pages = {18-24},
year = {2022},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2022.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0895435622001524},
author = {Leslie L. Roos and Elizabeth Wall-Wieler and Charles Burchill and Naomi C. Hamm and Amani F. Hamad and Lisa M. Lix},
keywords = {Research possibilities, Covariates, Family data, Registries, Observational studies, Design},
abstract = {Background and Objectives
To highlight the potential of multiple file record linkage. Linkage increases the value of existing information by supplying missing data or correcting errors in existing data, through generating important covariates, and by using family information to control for unmeasured variables and expand research opportunities.
Methods
Recent Manitoba papers highlight the use of linkage to produce better studies. Specific ways in which linkage helps deal with different substantive issues are described.
Results
Wide data files—files containing considerable amounts of information on each individual—generated by linkage improve research by facilitating better design. Nonexperimental work in particular benefits from such linkages. Population registries are especially valuable in supplying family data to facilitate work across different substantive fields.
Conclusion
Several examples show how record linkage magnifies the value of information from individual projects. The results of observational studies become more defensible through the better designs facilitated by such linkage.}
}
@article{ZHANG2022101626,
title = {Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development},
journal = {Government Information Quarterly},
volume = {39},
number = {1},
pages = {101626},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101626},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000629},
author = {Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui},
keywords = {Smart city, Big data, Digital sustainability, Resource orchestration, Socio-technical issues},
abstract = {Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.}
}
@article{LIU2022103622,
title = {scenario modeling for government big data governance decision-making: Chinese experience with public safety services},
journal = {Information & Management},
volume = {59},
number = {3},
pages = {103622},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103622},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000349},
author = {Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU},
keywords = {Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services},
abstract = {In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.}
}
@article{DIVAIO2022121201,
title = {Data intelligence and analytics: A bibliometric analysis of human–Artificial intelligence in public sector decision-making effectiveness},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121201},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121201},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100634X},
author = {Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine},
keywords = {Ambidexterity, Industry 4.0, Business intelligence, Big data, Intellectual capital, Human intellect, Accountability and performance},
abstract = {This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.}
}
@article{YOUSSEF2022102827,
title = {Cross-national differences in big data analytics adoption in the retail industry},
journal = {Journal of Retailing and Consumer Services},
volume = {64},
pages = {102827},
year = {2022},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2021.102827},
url = {https://www.sciencedirect.com/science/article/pii/S0969698921003933},
author = {Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag},
keywords = {Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry},
abstract = {Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.}
}
@article{HE2022112372,
title = {A rule-based data preprocessing framework for chiller rooms inspired by the analysis of engineering big data},
journal = {Energy and Buildings},
volume = {273},
pages = {112372},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112372},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822005436},
author = {Ruikai He and Tong Xiao and Shunian Qiu and Jiefan Gu and Minchen Wei and Peng Xu},
keywords = {Data pre-processing, Big engineering data, Building energy management},
abstract = {The rapid development of building energy consumption monitoring platforms makes engineering data more diverse, which facilitates the goal of reducing emissions. It is increasingly acknowledged that data preprocessing deserves the same attention as intelligent algorithms. In this work, the data quality issue of the engineering big data from non-demonstration complexes in China are analyzed thoroughly, and the analysis is based on clustering-based algorithms. We can conclude that the data of the hourly power of equipment groups are quality and stable, which is suitable for the benchmark to check other data. The quality of the data about pipes is acceptable. The number of data types about cooling towers is less, and the quality is worse. Regarding other data, the quality is unstable, so researchers should deal with those case-by-case. According to the above analysis, we proposed a convenient, rule-based data preprocessing framework that utilizes the law of physics, ensuring the strong coupling of multi-variants. After the data preprocessing, these engineering data are more reliable and can be used to improve performance or train models. Additionally, the proposed framework is more suitable for preprocessing multi-variant engineering data.}
}
@article{HARRISON2022103795,
title = {At the limit? Using operational data to estimate train driver human reliability},
journal = {Applied Ergonomics},
volume = {104},
pages = {103795},
year = {2022},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103795},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022001181},
author = {Chris Harrison and Julian Stow and Xiaocheng Ge and Jonathan Gregory and Huw Gibson and Alice Monk},
abstract = {Human reliability analysis plays an important role in the safety assessment and management of rail operations. This paper discusses how the increasing availability of operational data can be used to develop an understanding of train driver reliability. The paper derives human reliability data for two driving tasks, stopping at red signals and controlling speed on approach to buffer stops. In the first of these cases, a tool has been developed that can estimate the number of times a signal is approached at red by trains on the Great Britain (GB) rail network. The tool has been developed using big data techniques and ideas, recording and analysing millions of pieces of data from live operational feeds to update and summarise statistics from thousands of signal locations in GB on a daily basis. The resulting driver reliability data are compared to similar analyses of other train driving tasks. This shows human reliability approaching the currently accepted limits of human performance. It also shows higher error rates amongst freight train drivers than passenger train drivers for these tasks. The paper highlights the importance of understanding the task specific performance limits if further improvements in human reliability are sought. It also provides a practical example of how big data could play an increasingly important role in system error management, whether from the perspective of understanding normal performance and the limits of performance for specific tasks or as the basis for dynamic safety indicators which, if not leading, could at least become closer to real time.}
}
@article{DEEPA2022209,
title = {A survey on blockchain for big data: Approaches, opportunities, and future directions},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {209-226},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000243},
author = {N. Deepa and Quoc-Viet Pham and Dinh C. Nguyen and Sweta Bhattacharya and B. Prabadevi and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta and Fang Fang and Pubudu N. Pathirana},
keywords = {Blockchain, Big data, Vertical applications, Smart city, Smart healthcare, Smart transportation, Security},
abstract = {Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.}
}
@article{KASTOUNI20222758,
title = {Big data analytics in telecommunications: Governance, architecture and use cases},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part A},
pages = {2758-2770},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S131915782030553X},
author = {Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}},
keywords = {Big data analytics, Big data project’s governance methodology, Big data architecture, Data governance methodology, Big data project’s team, Big data telecommunications use cases},
abstract = {With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project’s Governance, Architecture, Data Governance and the BDA Project’s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.}
}
@article{GHORBANI2022101089,
title = {Framework components for data-centric dry laboratories in the minerals industry: A path to science-and-technology-led innovation},
journal = {The Extractive Industries and Society},
volume = {10},
pages = {101089},
year = {2022},
issn = {2214-790X},
doi = {https://doi.org/10.1016/j.exis.2022.101089},
url = {https://www.sciencedirect.com/science/article/pii/S2214790X22000508},
author = {Yousef Ghorbani and Steven E. Zhang and Glen T. Nwaila and Julie E. Bourdeau},
keywords = {Dry laboratory, Data analytics, Process simulation, Mining industry, Data-centric, Data-driven},
abstract = {ABSTRACT
The world continues to experience a surge in data generation and digital transformation. Historic data is increasingly being replaced by modernized data, such as big data, which is regarded as data that exhibits the 5Vs: volume, variety, velocity, veracity and value. The capacity to optimally use and comprehend value from big data has become an indispensable aptitude for modern companies. In contrast to commercial and technology firms, usage, management and governance of data, including big data is a novel and evolving trend for mining and mineral industries. Although the mining industry can be unenthusiastic to change, embracing modernized data and big data is evolutionarily unavoidable, given many industry-wide challenges (i.e., fluctuation in commodity prices, geotechnical and harsh ground conditions, and ore grade), which corrode revenues and increase business risks, including the possibility of regulatory non-compliance. The minerals industry holds a genuine gold mine of data that were collected for scientific, engineering, operational and other purposes. Data and data-centric workspaces that are targeted towards innovation and experimentation, which if combined with in-discipline expertise are two harmonious ingredients that can provide many practical solutions for the mining and mineral industries. In this paper, the concept, the opportunity and the necessity for a move towards a technology- and innovation-based, data-centric ‘dry laboratories’ (common workspaces that facilitates data-centric experimentation and innovation) in the minerals industry are assessed. We contend that the dry laboratory environment maximizes the value of data for the minerals industry. Toward the establishment of dry laboratories, we propose several essential components of a framework that would enable the functionality of dry laboratories in the minerals industry, while concomitantly examining the components from both academia and industry perspectives.}
}
@article{PICCAROZZI20221746,
title = {The role of Big Data in the business challenge of Covid-19: a systematic literature review in managerial studies},
journal = {Procedia Computer Science},
volume = {200},
pages = {1746-1755},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.375},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003842},
author = {Michela Piccarozzi and Barbara Aquilani},
keywords = {Big Data, Covid-19, systematic literature review, management},
abstract = {2020 was globally greatly affected by the Covid-19 pandemic caused by SARS-CoV-2, which is still today impacting and profoundly changing life globally for people but also for firms. In this context, the need for timely and accurate information has become vital in every area of business management. The spread of the Covid-19 global pandemic has generated an exponential increase and extraordinary volume of data. In this domain, Big Data is one of the digital innovation technologies that can support business organizations during these complex times. Based on these considerations, the aim of this paper is to analyze the managerial literature concerning the issue of Big Data in the management of the Covid-19 pandemic through a systematic literature review. The results show a fundamental role of Big Data in pandemic management for businesses. The paper also provides managerial and theoretical implications.}
}
@article{DHAND2022101010,
title = {Deep enriched salp swarm optimization based bidirectional -long short term memory model for healthcare monitoring system in big data},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101010},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101010},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001538},
author = {Geetika Dhand and Kavita Sheoran and Parul Agarwal and Siddhartha Sankar Biswas},
keywords = {Big data, Enriched sea salp optimization, Wearable sensors, Health care monitoring systems},
abstract = {The rapid development of communication technologies and expert systems have resulted in a large volume of medical data. Big data such as clinical data, omics data, and electronic health data are difficult to manage in real-time due to noise, large size, different formats, missing values and large features. Hence, it is more difficult for the health monitoring system to extract the correct information. Low quality and noisy data can lead to unnecessary treatment. To overcome these issues, we proposed Enriched Salp Swarm Optimization based Bidirectional Long Short Term Memory (ESSOBiLSTM) to monitor health. This method consists of four layers, such as the data collection layer, data storage layer, data analytics, and presentation layer. The initial layer handles a variety of information from main sources: wearable sensor devices (WSD), social network data, and medical records (MR). The second layer stores all the collected data from WSD, MR, and social network data to the cloud server through the wireless network. The proposed framework for performing big data analytics steps like preprocessing, filtering, dimensionality reduction, and classification is performed in the third layer. In the final layer, the doctor analyzes the patient's condition based on the classification results of the enriched SSO-BiLSTM. Based on the evaluation report, the proposed ESSOBiLSTM gives an accuracy of 85%, precision of 80%, RMSE of 0.6, MAE of 0.58, recall of 85% and F-measure of 79%. As a result, ESSOBiLSTM has proven to be more effective in monitoring health in large datasets.}
}
@article{RIDZUAN2022685,
title = {Diagnostic analysis for outlier detection in big data analytics},
journal = {Procedia Computer Science},
volume = {197},
pages = {685-692},
year = {2022},
note = {Sixth Information Systems International Conference (ISICO 2021)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024133},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {Big data, data quality, outlier, Sustainable Development Goals},
abstract = {Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.}
}
@article{EZERINS2022105569,
title = {Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health},
journal = {Safety Science},
volume = {146},
pages = {105569},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105569},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521004112},
author = {Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz},
keywords = {Safety analytics, Data analytics, Readiness assessment, Occupational health},
abstract = {Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.}
}
@article{GOH20221029,
title = {Are batch effects still relevant in the age of big data?},
journal = {Trends in Biotechnology},
volume = {40},
number = {9},
pages = {1029-1040},
year = {2022},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2022.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167779922000361},
author = {Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong},
keywords = {artificial intelligence, batch effect, machine learning, RNA sequencing, single cell},
abstract = {Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.}
}
@article{CAISSIE2022100925,
title = {Head and Neck Radiotherapy Patterns of Practice Variability Identified as a Challenge to Real-World Big Data: results from the Learning from Analysis of Multicentre Big Data Aggregation (LAMBDA) Consortium},
journal = {Advances in Radiation Oncology},
pages = {100925},
year = {2022},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2022.100925},
url = {https://www.sciencedirect.com/science/article/pii/S245210942200032X},
author = {Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo},
abstract = {Purpose/Objective
Outside of randomized clinical trials, it is difficult to develop clinically relevant evidence-based recommendations for radiotherapy (RT) practice guidelines due to lack of comprehensive real-world data. To address this knowledge gap, we formed the Learning and Analytics from Multicenter Big Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization, develop software solutions for data analysis and recommend clinical practice change based on real-world data analyzed. The first phase of this “Big Data” study aimed at characterizing variability in clinical practice patterns of dosimetric data for organs at risk (OAR), that would undermine subsequent use of large scale, electronically aggregated data to characterize associations with outcomes. Evidence from this study was used as the basis for practical recommendations to improve data quality.
Materials/Methods
Dosimetric details of patients with H&N cancer treated with RT between 2014 and 2019 were analyzed. Institutional patterns of practice were characterized including structure nomenclature, volumes and frequency of contouring. Dose volume histogram (DVH) distributions were characterized and compared to institutional constraints and literature values.
Results
Plans for 4664 patients treated to a mean plan dose of 64.4 ± 13.2 Gy in 32 ± 4 fractions were aggregated. Prior to implementation of TG263 guidelines in each institution, there was variability in OAR nomenclature across institutions and structures. With evidence from this study, we identified a targeted and practical set of recommendations aimed at improving the quality of real-world data.
Conclusion
Quantifying similarities and differences among institutions for OAR structures and DVH metrics is the launching point for next steps to investigate potential relationships between DVH parameters and patient outcomes.}
}
@article{KLEES202214,
title = {Building a smart database for predictive maintenance in already implemented manufacturing systems},
journal = {Procedia Computer Science},
volume = {204},
pages = {14-21},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007402},
author = {Marina Klees and Safa Evirgen},
keywords = {predictive maintenance, smart maintenance, big data analytics, Sensor Data Analytics},
abstract = {Predictive analytics methods have become increasingly important in Manufacturing Organization in the context of Smart Maintenance. Standardized process models for data mining already known to search existing data stocks for patterns, trends and correlations. Sensors are progressively implemented in production machines to create a database for data mining processes. But the risk of Big Data, thus the risk of low quality data is probably high. For an economic consideration, the amount of investment in new measurement technology and infrastructure should be assessed. Organizations are confronted with the challenge of how much they have to invest to obtain a meaningful database. For this reason, it is important to research which existing approaches support the development of a sufficient database for predictive maintenance in manufacturing systems and provide a methodical framework.}
}
@article{BACHECHI2022100292,
title = {Big Data Analytics and Visualization in Traffic Monitoring},
journal = {Big Data Research},
volume = {27},
pages = {100292},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100292},
url = {https://www.sciencedirect.com/science/article/pii/S221457962100109X},
author = {Chiara Bachechi and Laura Po and Federica Rollo},
keywords = {Traffic, Time series, Air quality maps, Real-time data, Spatio-temporal data, Smart cities},
abstract = {This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.}
}
@article{XIN2022100197,
title = {Review on A big data-based innovative knowledge teaching evaluation system in universities},
journal = {Journal of Innovation & Knowledge},
volume = {7},
number = {3},
pages = {100197},
year = {2022},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2022.100197},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X22000373},
author = {Xu Xin and Yu Shu-Jiang and Pang Nan and Dou ChenXu and Li Dan},
keywords = {Big data, Knowledge teaching evaluation, Performance management},
abstract = {With the widespread use of digital technologies such as big data, cloud computing and artificial intelligence in higher education, how to establish a scientific and systematic evaluation system to turn the traditional classroom with the one-way transmission of knowledge into an interactive space for exchanging ideas and inspiring wisdom has become an essential task for human resource management in universities, and a key to improving teaching quality. However, due to the debate between scientism and humanism in teaching evaluation, studies related to teaching performance have been isolated from human resource management, resulting in the lack of a systematic vision and framework for such studies. Relevant studies are still limited to the evaluation contents of different evaluation subjects. Evaluations also tend to focus only on the teaching process, ignoring the objectives of talent training, making it difficult for evaluations to play a goal-oriented role and hindering the further development of relevant studies. Therefore, this paper draws on human resource management methodologies and analyzes knowledge teaching evaluation system characteristics in colleges and universities in a big data context to construct a “multiple evaluations, trinity and four-step closed-loop” big data-based knowledge teaching evaluation system. “Trinity” represents evaluation from three performance dimensions: teaching effect, teaching behavior and teaching ability. “Multiple evaluations” represents the design of teaching performance indicators based on teaching data, breaking the barriers between different evaluation subjects. “Four-step closed-loop” draws on performance management theory to standardize the teaching performance management process from four aspects: planning, implementation, evaluation, and feedback. This evaluation system provides a systematic methodology for unifying the theory and practice of innovative knowledge teaching evaluation system in universities in a big data context.}
}
@article{SUN2022112331,
title = {Understanding building energy efficiency with administrative and emerging urban big data by deep learning in Glasgow},
journal = {Energy and Buildings},
volume = {273},
pages = {112331},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112331},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822005023},
author = {Maoran Sun and Changyu Han and Quan Nie and Jingying Xu and Fan Zhang and Qunshan Zhao},
keywords = {Building energy efficiency, Energy performance certificate, Deep learning, Google street view, SHapley additive explanations},
abstract = {With buildings consuming nearly 40% of energy in developed countries, it is important to accurately estimate and understand the building energy efficiency in a city. A better understanding of building energy efficiency is beneficial for reducing overall household energy use and providing guidance for future housing improvement and retrofit. In this research, we propose a deep learning-based multi-source data fusion framework to estimate building energy efficiency. We consider the traditional factors associated with the building energy efficiency from the Energy Performance Certificate (EPC) for 160,000 properties (30,000 buildings) in Glasgow, UK (e.g., property structural attributes and morphological attributes), as well as the Google Street View (GSV) building façade images as a complement. We compare the performance improvements between our data-fusion framework with traditional morphological attributes and image-only models. The results show that including the building façade images from GSV, the overall model accuracy increases from 79.7% to 86.8%. A further investigation and explanation of the deep learning model are conducted to understand the relationships between building features and building energy efficiency by using SHapley Additive exPlanations (SHAP). Our research demonstrates the potential of using multi-source data in building energy efficiency prediction with high accuracy and short inference time. Our paper also helps understand building energy efficiency at the city level to help achieve the net-zero target by 2050.}
}
@article{WANG2022111488,
title = {An empirical study on the challenges that developers encounter when developing Apache Spark applications},
journal = {Journal of Systems and Software},
volume = {194},
pages = {111488},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111488},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001674},
author = {Zehao Wang and Tse-Hsun (Peter) Chen and Haoxiang Zhang and Shaowei Wang},
keywords = {Big data system, Empirical study, Stack Overflow},
abstract = {Apache Spark is one of the most popular big data frameworks that abstract the underlying distributed computation details. However, even though Spark provides various abstractions, developers may still encounter challenges related to the peculiarity of distributed computation and environment. To understand the challenges that developers encounter, and provide insight for future studies, in this paper, we conduct an empirical study on the questions that developers encounter. We manually analyze 1,000 randomly selected questions that we collected from Stack Overflow. We find that: 1) questions related to data processing (e.g., transforming data format) are the most common among the 11 types of questions that we uncovered. 2) Even though data processing questions are the most common ones, they require the least amount of time to receive an answer. Questions related to configuration and performance require the most time to receive an answer. 3) Most of the issues are caused by developers’ insufficient knowledge in API usages, data conversation across frameworks, and environment-related configurations. We also discuss the implication of our findings for researchers and practitioners. In summary, our work provides insights for future research directions and highlight the need for more software engineering research in this area.}
}
@article{WANG2022643,
title = {Does city construction improve life quality?-evidence from POI data of China},
journal = {International Review of Economics & Finance},
volume = {80},
pages = {643-653},
year = {2022},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1059056022000041},
author = {Yang Wang and Hong Zhang and Libing Liu},
keywords = {Quality of life, Point of interest, Happiness},
abstract = {To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the "clogging point" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for "meeting the people's increasing needs for a better life".}
}
@article{KOTECHA2022e757,
title = {CODE-EHR best-practice framework for the use of structured electronic health-care records in clinical research},
journal = {The Lancet Digital Health},
volume = {4},
number = {10},
pages = {e757-e764},
year = {2022},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(22)00151-0},
url = {https://www.sciencedirect.com/science/article/pii/S2589750022001510},
author = {Dipak Kotecha and Folkert W Asselbergs and Stephan Achenbach and Stefan D Anker and Dan Atar and Colin Baigent and Amitava Banerjee and Birgit Beger and Gunnar Brobert and Barbara Casadei and Cinzia Ceccarelli and Martin R Cowie and Filippo Crea and Maureen Cronin and Spiros Denaxas and Andrea Derix and Donna Fitzsimons and Martin Fredriksson and Chris P Gale and Georgios V Gkoutos and Wim Goettsch and Harry Hemingway and Martin Ingvar and Adrian Jonas and Robert Kazmierski and Susanne Løgstrup and R Thomas Lumbers and Thomas F Lüscher and Paul McGreavy and Ileana L Piña and Lothar Roessig and Carl Steinbeisser and Mats Sundgren and Benoît Tyl and Ghislaine van Thiel and Kees van Bochove and Panos E Vardas and Tiago Villanueva and Marilena Vrana and Wim Weber and Franz Weidinger and Stephan Windecker and Angela Wood and Diederick E Grobbee},
abstract = {Summary
Big data is important to new developments in global clinical science that aim to improve the lives of patients. Technological advances have led to the regular use of structured electronic health-care records with the potential to address key deficits in clinical evidence that could improve patient care. The COVID-19 pandemic has shown this potential in big data and related analytics but has also revealed important limitations. Data verification, data validation, data privacy, and a mandate from the public to conduct research are important challenges to effective use of routine health-care data. The European Society of Cardiology and the BigData@Heart consortium have brought together a range of international stakeholders, including representation from patients, clinicians, scientists, regulators, journal editors, and industry members. In this Review, we propose the CODE-EHR minimum standards framework to be used by researchers and clinicians to improve the design of studies and enhance transparency of study methods. The CODE-EHR framework aims to develop robust and effective utilisation of health-care data for research purposes.}
}
@article{LIN2022120320,
title = {Enhanced commercial cooking inventories from the city scale through normalized emission factor dataset and big data},
journal = {Environmental Pollution},
pages = {120320},
year = {2022},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2022.120320},
url = {https://www.sciencedirect.com/science/article/pii/S0269749122015342},
author = {Pengchuan Lin and Jian Gao and Yisheng Xu and James J. Schauer and Jiaqi Wang and Wanqing He and Lei Nie},
keywords = {Point of interest data, Emission inventory, Cooking emission factors, Dataset, Beijing},
abstract = {Cooking emission inventories always have poor spatial resolutions when applying with traditional methods, making their impacts on ambient air and human health remain obscure. In this study, we created a systematic dataset of cooking emission factors (CEFs) and applied it with a new data source, cooking-related point of interest (POI) data, to build up highly spatial resolved cooking emission inventories from the city scale. Averaged CEFs of six particulate and gaseous species (PM, OC, EC, NMHC, OVOCs, VOCs) were 5.92 ± 6.28, 4.10 ± 5.50, 0.05 ± 0.05, 22.54 ± 20.48, 1.56 ± 1.44, and 7.94 ± 6.27 g/h normalized in every cook stove, respectively. A three-field CEF index containing activity and emission factor species was created to identify and further build a connection with cooking-related POI data. A total of 95,034 cooking point sources were extracted from Beijing, as a study city. In downtown areas, four POI types were overlapped in the central part of the city and radiated into eight distinct directions from south to north. Estimated PM/VOC emissions caused by cooking activities in Beijing were 4.81/9.85 t per day. A 3D emission map showed an extremely unbalanced emission density in the Beijing region. Emission hotspots were seen in Central Business District (CBD), Sanlitun, and Wangjing in Chaoyang District and Willow and Zhongguancun in Haidian District. PM/VOC emissions could be as high as 16.6/42.0 kg/d in the searching radius of 2 km. For PM, the total emissions were 417.4, 389.0, 466.9, and 443.0 t between Q1 and Q4 2019 in Beijing, respectively. The proposed methodology is transferrable to other Chinese cities for deriving enhanced commercial cooking inventories and potentially highlighting the further importance of cooking emissions on air quality and human health.}
}
@article{FANG2022104070,
title = {BIM-integrated portfolio-based strategic asset data quality management},
journal = {Automation in Construction},
volume = {134},
pages = {104070},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104070},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005215},
author = {Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian},
keywords = {Strategic asset management (SAM), Building information modeling (BIM), Portfolio management, Data quality management},
abstract = {A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.}
}
@article{ZHANG2022103231,
title = {Orchestrating big data analytics capability for sustainability: A study of air pollution management in China},
journal = {Information & Management},
volume = {59},
number = {5},
pages = {103231},
year = {2022},
note = {Big Data Analytics for Sustainability},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103231},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619302010},
author = {Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu},
keywords = {Big data, Big data analytics, Sustainability, Air pollution, Resource orchestration},
abstract = {Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.}
}
@article{DU2022,
title = {Intelligent Monitoring System Based on Spatio–Temporal Data for Underground Space Infrastructure},
journal = {Engineering},
year = {2022},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S209580992200635X},
author = {Bowen Du and Junchen Ye and Hehua Zhu and Leilei Sun and Yanliang Du},
keywords = {Structure health monitoring, Underground space infrastructure, Machine learning, Spatio–temporal data},
abstract = {Intelligent sensing, mechanism understanding, and the deterioration forecasting based on spatio–temporal big data not only promote the safety of the infrastructure but also indicate the basic theory and key technology for the infrastructure construction to turn to intelligentization. The advancement of underground space utilization has led to the development of three characteristics (deep, big, and clustered) that help shape a tridimensional urban layout. However, compared to buildings and bridges overground, the diseases and degradation that occur underground are more insidious and difficult to identify. Numerous challenges during the construction and service periods remain. To address this gap, this paper summarizes the existing methods and evaluates their strong points and weak points based on real-world space safety management. The key scientific issues, as well as solutions, are discussed in a unified intelligent monitoring system.}
}
@article{JIA202255,
title = {Data Quality and Usability Assessment Methodology for Prognostics and Health Management: A Systematic Framework},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {55-60},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.183},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322013969},
author = {Xiaodong Jia and Da-Yan Ji and Takanobu Minami and Jay Lee},
keywords = {Intelligence Maintenance, Data Quality, Artificial Intelligence, Industry 4.0},
abstract = {Collecting useful and informative data play an essential role in ensuring the performance of data-driven solutions for intelligent maintenance. However, there is still a lack of methodology to systematically assess the data usefulness (or data suitability) for modeling. This lack of data suitability assessment becomes a more pressing issue in the big data environment where a large volume of machine data is generated at a high velocity. Therefore, there are imperative needs for standardized procedures and systematic solutions that can scan through a large amount of data to quantify the data suitability and locate the useful datasets for model development. To fill in this gap, this paper proposes a novel methodology to evaluate the data suitability for PHM modeling from the aspects of detectability assessment, diagnosability assessment, and prognosability assessment. In the discussion, new assessment procedures and algorithms are proposed by using a series of similarity metrics between data vectors or data distribution. Also, the proposed methods provide both visualization tools and quantitative metrics to assess the data suitability. The effectiveness of the methodology is demonstrated by using real-world examples about the ball screw degradation and boring tool degradation. The results successfully demonstrate the effectiveness and practicality of the proposed methodology and analytics.}
}
@article{LI2022101021,
title = {A review of industrial big data for decision making in intelligent manufacturing},
journal = {Engineering Science and Technology, an International Journal},
volume = {29},
pages = {101021},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621001336},
author = {Chunquan Li and Yaqiong Chen and Yuling Shang},
keywords = {Intelligent manufacturing, Artificial intelligence, Industrial big data, Big data-driven technology, Decision-making},
abstract = {Under the trend of economic globalization, intelligent manufacturing has attracted a lot of attention from academic and industry. Related enabling technologies make manufacturing industry more intelligent. As one of the key technologies in artificial intelligence, big data driven analysis improves the market competitiveness of manufacturing industry by mining the hidden knowledge value and potential ability of industrial big data, and helps enterprise leaders make wise decisions in various complex manufacturing environments. This paper provides a theoretical analysis basis for big data-driven technology to guide decision-making in intelligent manufacturing, fully demonstrating the practicability of big data-driven technology in the intelligent manufacturing industry, including key advantages and internal motivation. A conceptual framework of intelligent decision-making based on industrial big data-driven technology is proposed in this study, which provides valuable insights and thoughts for the severe challenges and future research directions in this field.}
}
@article{GRIGG2022100619,
title = {DUG Insight: A software package for big-data analysis and visualisation, and its demonstration for passive radar space situational awareness using radio telescopes},
journal = {Astronomy and Computing},
volume = {40},
pages = {100619},
year = {2022},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2022.100619},
url = {https://www.sciencedirect.com/science/article/pii/S2213133722000452},
author = {D. Grigg and S.J. Tingay and M. Sokolowski and R.B. Wayth},
keywords = {Space situational awareness, , Radio astronomy, Interactive workflow creation, High performance computing},
abstract = {As the demand for software to support the processing and analysis of massive radio astronomy datasets increases in the era of the SKA, we demonstrate the interactive workflow building, data mining, processing, and visualisation capabilities of DUG Insight. We test the performance and flexibility of DUG Insight by processing almost 68,000 full sky radio images produced from the Engineering Development Array (EDA2) over the course of a three day period. The goal of the processing was to passively detect and identify known Resident Space Objects (RSOs: satellites and debris in orbit) and investigate how radio interferometry could be used to passively monitor aircraft traffic. These signals are observable due to both terrestrial FM radio signals reflected back to Earth and out-of-band transmission from RSOs. This surveillance of the low Earth orbit and airspace environment is useful as a contribution to space situational awareness and aircraft tracking technology. From the observations, we made 40 detections of 19 unique RSOs within a range of 1,500 km from the EDA2. This is a significant improvement on a previously published study of the same dataset and showcases the flexible features of DUG Insight that allow the processing of complex datasets at scale. Future enhancements of our DUG Insight workflow will aim to realise real-time acquisition, detect unknown RSOs, and continue to process data from SKA-relevant facilities.}
}
@article{MACHADO2022263,
title = {Data Mesh: Concepts and Principles of a Paradigm Shift in Data Architectures},
journal = {Procedia Computer Science},
volume = {196},
pages = {263-271},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022365},
author = {Inês Araújo Machado and Carlos Costa and Maribel Yasmina Santos},
keywords = {Big Data, Data Mesh, Data Architectures, Data Lake},
abstract = {Inherent to the growing use of the most varied forms of software (e.g., social applications), there is the creation and storage of data that, due to its characteristics (volume, variety, and velocity), make the concept of Big Data emerge. Big Data Warehouses and Data Lakes are concepts already well established and implemented by several organizations, to serve their decision-making needs. After analyzing the various problems demonstrated by those monolithic architectures, it is possible to conclude about the need for a paradigm shift that will make organizations truly data-oriented. In this new paradigm, data is seen as the main concern of the organization, and the pipelining tools and the Data Lake itself are seen as a secondary concern. Thus, the Data Mesh consists in the implementation of an architecture where data is intentionally distributed among several Mesh nodes, in such a way that there is no chaos or data silos, since there are centralized governance strategies and the guarantee that the core principles are shared throughout the Mesh nodes. This paper presents the motivation for the appearance of the Data Mesh paradigm, its features, and approaches for its implementation.}
}
@article{ZORRILLA2022103595,
title = {A reference framework for the implementation of data governance systems for industry 4.0},
journal = {Computer Standards & Interfaces},
volume = {81},
pages = {103595},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000908},
author = {Marta Zorrilla and Juan Yebenes},
keywords = {Data governance, Data-Centric architecture, Industry 4.0, Big data, IoT},
abstract = {The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.}
}
@article{LIN2022103680,
title = {How big data analytics enables the alliance relationship stability of contract farming in the age of digital transformation},
journal = {Information & Management},
volume = {59},
number = {6},
pages = {103680},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103680},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000891},
author = {Shunzhi Lin and Jiabao Lin and Feiyun Han and Xin (Robert) Luo},
keywords = {Big data analytics, Risk management capability, Data quality, Alliance relationship stability},
abstract = {Notwithstanding the potential of big data analytics technology for alliance management, there is a lack of understanding of how such digital technology influences alliance relationship stability (ARS). Drawing on the information technology-enabled organizational capabilities (IT-enabled OCs) perspective, this study empirically verifies that big data analytics promotes ARS and risk management capability. Moreover, market risk management capability (MRM) enhances ARS, and data quality moderates the relationship between big data analytics usage (BDU) and MRM. This research reveals the impact mechanism of BDU on the ARS. Implications for management and future research are presented as well.}
}
@article{RUSSELL2022108709,
title = {Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring},
journal = {Mechanical Systems and Signal Processing},
volume = {168},
pages = {108709},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108709},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021010293},
author = {Matthew Russell and Peng Wang},
keywords = {Physics-informed deep learning, Prognostics and health management, Data compression, Big data},
abstract = {The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.}
}
@article{ZHANG20221,
title = {Application of machine learning, deep learning and optimization algorithms in geoengineering and geoscience: Comprehensive review and future challenge},
journal = {Gondwana Research},
volume = {109},
pages = {1-17},
year = {2022},
issn = {1342-937X},
doi = {https://doi.org/10.1016/j.gr.2022.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S1342937X2200123X},
author = {Wengang Zhang and Xin Gu and Libin Tang and Yueping Yin and Dongsheng Liu and Yanmei Zhang},
keywords = {Machine learning, Deep learning, Optimization algorithms, Geoengineering and geoscience, VOSviewer},
abstract = {The so-called Fourth Paradigm has witnessed a boom during the past two decades, with large volumes of observational data becoming available to scientists and engineers. Big data is characterized by the rule of the five Vs: Volume, Variety, Value, Velocity and Veracity. The concept of big data naturally matches well with the features of geoengineering and geoscience. Large-scale, comprehensive, multidirectional and multifield geotechnical data analysis is becoming a trend. On the other hand, Machine learning (ML), Deep Learning (DL) and Optimization Algorithm (OA) provide the ability to learn from data and deliver in-depth insight into geotechnical problems. Researchers use different ML, DL and OA models to solve various problems associated with geoengineering and geoscience. Consequently, there is a need to extend its research with big data research through integrating the use of ML, DL and OA techniques. This work focuses on a systematic review on the state-of-the-art application of ML, DL and OA algorithms in geoengineering and geoscience. Various ML, DL, and OA approaches are firstly concisely introduced, concerning mainly the supervised learning, unsupervised learning, deep learning and optimization algorithms. Then their representative applications in the geoengineering and geoscience are summarized via VOSviewer demonstration. The authors also provided their own thoughts learnt from these applications as well as work ongoing and future recommendations. This review paper aims to make a comprehensive summary and provide fundamental guidelines for researchers and engineers in the discipline of geoengineering and geoscience or similar research areas on how to integrate and apply ML, DL and OA methods.}
}
@article{LI2022121355,
title = {Evaluating the impact of big data analytics usage on the decision-making quality of organizations},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121355},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121355},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521007861},
author = {Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo},
keywords = {Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms},
abstract = {Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.}
}
@article{NARASIMHULU2022104690,
title = {HIGH PERFORMANCE SOCIAL DATA COMPUTING WITH DEVELOPMENT OF INTELLIGENT TOPIC MODELS FOR HEALTHCARE},
journal = {Microprocessors and Microsystems},
pages = {104690},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104690},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122002204},
author = {K Narasimhulu and K.T. {Meena Abarna}},
keywords = {Topic Model, Ailments Aspects, ATAM, Healthcare Topic Model, Social Recommended Healthcare Results},
abstract = {Data mining and big data computing are the emerging domains in the current era of predictions for societal applications. Millions of people are interested in sharing their views through tweets. Healthcare predictions are one of the attractive researches in big data social mining. Healthcare predictions are derived by implementing topic models by the ailments data. An ailment refers to either illness or sign of a particular health problem. Millions of tweets are collected based on conditions and assessed with ailment topic aspect models. The existing topic model, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing, Probabilistic LSI (PLSI), limits the healthcare results assessment concerning any one of the ailments aspects. Recent ailments topic aspect model (ATAM) overcome the problems of these topic models and delivers the healthcare assessment results concerning the fundamental aspects of ailments data except side-effects analysis of treatments. The scalability performance of ATAM is degraded in showing healthcare results over the massive amounts of health data. A high-performance computing model of ATAM has been developed in the distributed environment to address scalability. Its intelligent model is designed in the cloud and multi-node Hadoop environment to deliver high-performance social computing results for healthcare. Experiments are conducted on many comparative studies is demonstrated between the existing and proposed high-performance models using the massive amount of health-related tweets concerning the ailments aspects.}
}
@article{CORALLO2022102331,
title = {Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {76},
pages = {102331},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102331},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000205},
author = {Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi},
keywords = {Big Data Analytics, BDA, MBDAaaS framework, Smart manufacturing, Industry 4.0, Anomaly detection},
abstract = {Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.}
}
@article{SCHMUCKER2022100061,
title = {Measuring tourism with big data? Empirical insights from comparing passive GPS data and passive mobile data},
journal = {Annals of Tourism Research Empirical Insights},
volume = {3},
number = {2},
pages = {100061},
year = {2022},
issn = {2666-9579},
doi = {https://doi.org/10.1016/j.annale.2022.100061},
url = {https://www.sciencedirect.com/science/article/pii/S2666957922000295},
author = {Dirk Schmücker and Julian Reif},
keywords = {Big Data, Mobile Network Data, Passive GPS Data, Spatio-temporal behaviour, Tourist classification},
abstract = {In this paper we aim to classify digital data sources for the measurement of tourist mobility, to establish a set of assessment indicators, and to compare two Big Data sources to gain empirical insights into how we can measure tourism with Big Data. For three holiday destinations in Germany, passive mobile data and passive global positioning systems (GPS) data are compared with reference data from the destinations for twelve weeks in the summer of 2019. Results show that mobile network data are on a plausible level compared to the local reference data and are able to predict the temporal pattern to a very high degree. GPS app-based data also perform well, but are less plausible and precise than mobile network data.}
}
@article{LIU2022103138,
title = {Effects of governmental data governance on urban fire risk: A city-wide analysis in China},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103138},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103138},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003570},
author = {Zhao-Ge Liu and Xiang-Yang Li and Grunde Jomaas},
keywords = {Urban fire risk, Fire risk management, Big data technologies, Data governance, Socio-economic factors, City-wide analysis},
abstract = {The effects of data governance (as a means to maximize big data value creation in fire risk management) performance on fire risk was analyzed based on multi-source statistical data of 105 cities in China from 2016 to 2018. Specifically, data governance was first quantified with ten detailed indicators, which were then selected for explaining urban fire risk through correlation analysis. Next, the sample cities were clustered in terms of major socio-economic characteristics, and then the effects of data governance were examined by constructing multivariate regression models for each city cluster with ordinary least squares (OLS). The results showed that the constructed regression models produced good interpretation of fire risk in different types of cities, with coefficient of determination (R2) in each model exceeding 0.65. Among the indicators, the development of infrastructures (e.g. data collection devices and data analysis platforms), the level of data use, and the updating of fire risk related data were proved to produce significant effects on the reduction of fire frequency and fire consequence. Moreover, the organizational maturity of data governance was proved to be helpful in reducing fire frequency. For the cities with large population, the cross-department sharing of high-value data was found to be another important determinant of urban fire frequency. In comparison with existing statistical models which interpreted fire risk with general social factors (with the highest R2 = 0.60), these new regression models presented a better statistical performance (with the average R2 = 0.72). These findings are expected to provide decision support for the local governments of China and other jurisdictions to facilitate big data projects in improving fire risk management.}
}
@article{SHEN2022102529,
title = {Personal big data pricing method based on differential privacy},
journal = {Computers & Security},
volume = {113},
pages = {102529},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102529},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003539},
author = {Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang},
keywords = {Personal big data, Data privacy, Privacy protection, Differential privacy, Positive pricing, Reverse pricing, Privacy budget, Privacy compensation},
abstract = {Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.}
}
@article{YANG2022108322,
title = {Data quality assessment and analysis for pest identification in smart agriculture},
journal = {Computers and Electrical Engineering},
volume = {103},
pages = {108322},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108322},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622005444},
author = {Jiachen Yang and Guipeng Lan and Yang Li and Yicheng Gong and Zhuo Zhang and Sezai Ercisli},
keywords = {Smart agriculture, Pest identification, Information entropy, Data-centric},
abstract = {Deep learning has played a crucial role in the field of smart agriculture and been widely used in various applications. However, the deep learning models are constrained by data quality, which means poor data quality and unreliable data annotation will seriously restrict the performance of smart applications. In this paper, we proposed two methods to assess data quality, named Bound-DE and Multi-Branch. In experiments, the IP06 dataset and the ResNet-18 backbone network were adopted. The results show that the redundancy of the used public dataset is so large that about 50% of the data can achieve the similar test accuracy. Furthermore, we also analyzed the high contributive samples and summarized the rules of those selected informative samples, which is significant for the design of high-efficiency datasets. In summary, this study guides and promotes the following data-centric research in the field of smart agriculture.}
}
@article{JOSE2022100081,
title = {Integrating big data and blockchain to manage energy smart grids—TOTEM framework},
journal = {Blockchain: Research and Applications},
volume = {3},
number = {3},
pages = {100081},
year = {2022},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2022.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2096720922000227},
author = {Dhanya Therese Jose and Jørgen Holme and Antorweep Chakravorty and Chunming Rong},
keywords = {Blockchain, Big data analytic, Hyperledger fabric, Hadoop, MapReduce, Docker, PIVT},
abstract = {The demand for electricity is increasing exponentially day by day, especially with the arrival of electric vehicles. In the smart community neighborhood project, electricity should be produced at the household or community level and sold or bought according to the demands. Since the actors can produce, sell, and buy according to the demands, thus the name prosumers. ICT solutions can contribute to this in several ways, such as machine learning for analyzing the household data for customer demand and peak hours for the usage of electricity, blockchain as a trustworthy platform for selling or buying, data hub, and ensuring data security and privacy of prosumers. TOTEM: Token for controlled computation is a framework that allows users to analyze the data without moving the data from the data owner's environment. It also ensures the data security and privacy of the data. Here, in this article, we will show the importance of the TOTEM architecture in the EnergiX project and how the extended version of TOTEM can be efficiently merged with the demands of the current and similar projects.}
}
@article{BALTI20221,
title = {Multidimensional architecture using a massive and heterogeneous data: Application to drought monitoring},
journal = {Future Generation Computer Systems},
volume = {136},
pages = {1-14},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001753},
author = {Hanen Balti and Ali Ben Abbes and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle},
keywords = {Big data, Data storage, Spatio-temporal querying, Decision-making, Earth observation, Disaster management},
abstract = {The rapid increase in the number of Earth Observation (EO) systems generates a massive amount of heterogeneous data. It has raised big issues in collecting, preprocessing, storing, and the visualization these data. However, traditional techniques are facing serious challenges when dealing with big EO data dimensions (i.e., Volume, Veracity, Variety, and Velocity), especially in natural hazards management. Therefore, big data techniques and tools attract more attention. In this paper we propose a multidimensional model framework for Big EO data warehousing. This framework includes 3 parts: (1) Data collection and preprocessing, being responsible for collecting data and improving their quality; (2) Data loading and storage, performing the ingestion task which consists of transferring the data from external resources to the Big data platform for storage; and (3) Visualization and interpretation, aiming to provide spatio-temporal analysis. This framework could be useful for decision-makers in monitoring the effects of drought disasters and, consequently, planning the mitigation and remediation measures. Experiments are carried out on drought monitoring in China along the period 2000–2020. The input data include remote sensing data, biophysical data, and climatological data. The results reveal that the proposed framework has a higher retrieval speed and a greater elasticity with different kinds (i.e. spatial, temporal, or spatiotemporal) of requests compared to traditional frameworks, indicating its superiority.}
}