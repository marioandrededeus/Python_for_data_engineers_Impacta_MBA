@article{BRIGHT2020109706,
title = {Bright-Sun: A globally applicable 1-min irradiance clear-sky detection model},
journal = {Renewable and Sustainable Energy Reviews},
volume = {121},
pages = {109706},
year = {2020},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.109706},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120300046},
author = {Jamie M. Bright and Xixi Sun and Christian A. Gueymard and Brendan Acord and Peng Wang and Nicholas A. Engerer},
keywords = {Clear-sky detection, Irradiance, Cloudless sky, Clear-sky, Bright-Sun CSD},
abstract = {Clear-sky detection (CSD) is a crucial process in numerous solar energy applications. Many CSD models have been proposed over the years, though model performance is generally found unsatisfactory for worldwide use. We demonstrate this qualitatively on 22 CSD models at five climatologically-diverse radiometric stations; all exhibit one or more limitations: (1) unreliability at high zenith; (2) unrealistic “clear” periods immediately before or after cloudy conditions; (3) relaxed (short-term false positives); (4) over-relaxed during clear conditions (longer-term false positives); (5) conservative (short-term false negatives); and (6) over-conservative during clear conditions (longer-term false negatives). A new globally applicable CSD methodology is proposed for a posteriori detection of apparent “cloudless sky” conditions on 1-min irradiance time series, named the Bright-Sun model. This new tool requires measured global horizontal irradiance (GHI) and diffuse horizontal irradiance (DIF), and consists of three stages: (1) clear-sky irradiance optimisation, (2) tri-component CSD analysis with the newly derived Modified-Reno method, and (3) a cascading durational filters to determine periods of apparent cloudless sky. Through qualitative evaluation and exploring sensitivity to clear-sky model selection, the Bright-Sun model does not suffer any of the aforementioned limitations at any of the five stations, despite their distinctive climates. Due to the significant influence of bright or dark clouds on DIF, which have much lower impact on GHI, the new model also exhibits extra discretionary power by including analysis on DIF and can thus identify apparently clear periods with zero or near-zero cloudiness. The Bright-Sun CSD model is coded in Matlab®and freely available (future releases in R and Python are anticipated). A script is attached as supplementary material in the original form. For a supported and version controlled release of the Bright-Sun model, as well as other CSD models mentioned within this document, the reader can refer to the CSD Library at https://jamiembright.github.io/csd-library/.}
}
@incollection{LADLEY202015,
title = {Chapter 3 - Data literacy and concepts},
editor = {John Ladley},
booktitle = {Data Governance (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {15-31},
year = {2020},
isbn = {978-0-12-815831-9},
doi = {https://doi.org/10.1016/B978-0-12-815831-9.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158319000035},
author = {John Ladley},
keywords = {Definitions, Capabilities, Data literacy, Common terms, Asset, Data supply chain},
abstract = {We will present the core managerial and business concepts required for building and operating a data governance (DG) program. The key takeaway from this chapter is that DG is not part of information technology’s job description.}
}
@incollection{RICKER2020187,
title = {Volunteered Geographic Information},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {187-195},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10615-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955106158},
author = {Britta Ricker},
keywords = {Accuracy, Citizen science, Crowd-sourced, Ethics, Geographic information systems, Location-based services, Maps, Participation, Precision, Volunteered geographic information},
abstract = {Volunteered geographic information (VGI) is spatial data that are gathered and shared by individuals to derive information about the world. VGI has largely been embraced due to the pervasiveness of mobile devices and affordances offered by the locational sensors that are housed within them. VGI is deeply valuable to scientists because now seemingly anyone, heterogeneous geographically dispersed populations, with a mobile device can make observations about the world and share them on the Internet. These contributions may be qualitative or quantitative observations and include multimedia content such as videos and photos. With this new data source, new spatial patterns maybe revealed that previously went unnoticed. Specific issues associated to VGI that are particularly interesting to human geographers relate to volunteer's motivation, accuracy, precision, and reliability of the data, questions about whether the data are contributed by a local expert or a remote user, ethical considerations related to data ownership and views that may be privileged over others on new maps generated with VGI.}
}
@article{ALI2020115834,
title = {A data-driven approach for multi-scale GIS-based building energy modeling for analysis, planning and support decision making},
journal = {Applied Energy},
volume = {279},
pages = {115834},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115834},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920313106},
author = {Usman Ali and Mohammad Haris Shamsi and Mark Bohacek and Karl Purcell and Cathal Hoare and Eleni Mangina and James O’Donnell},
keywords = {GIS modeling, Machine learning, Urban planning, Data-driven approaches, Building energy performance, Urban building energy modeling, Energy performance certificate},
abstract = {Urban planners, local authorities, and energy policymakers often develop strategic sustainable energy plans for the urban building stock in order to minimize overall energy consumption and emissions. Planning at such scales could be informed by building stock modeling using existing building data and Geographic Information System-based mapping. However, implementing these processes involves several issues, namely, data availability, data inconsistency, data scalability, data integration, geocoding, and data privacy. This research addresses the aforementioned information challenges by proposing a generalized integrated methodology that implements bottom-up, data-driven, and spatial modeling approaches for multi-scale Geographic Information System mapping of building energy modeling. This study uses the Irish building stock to map building energy performance at multiple scales. The generalized data-driven methodology uses approximately 650,000 Irish Energy Performance Certificates buildings data to predict more than 2 million buildings’ energy performance. In this case, the approach delivers a prediction accuracy of 88% using deep learning algorithms. These prediction results are then used for spatial modeling at multiple scales from the individual building level to a national level. Furthermore, these maps are coupled with available spatial resources (social, economic, or environmental data) for energy planning, analysis, and support decision-making. The modeling results identify clusters of buildings that have a significant potential for energy savings within any specific region. Geographic Information System-based modeling aids stakeholders in identifying priority areas for implementing energy efficiency measures. Furthermore, the stakeholders could target local communities for retrofit campaigns, which would enhance the implementation of sustainable energy policy decisions.}
}
@incollection{2020233,
title = {Index},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Data Democracy},
publisher = {Academic Press},
pages = {233-241},
year = {2020},
isbn = {978-0-12-818366-3},
doi = {https://doi.org/10.1016/B978-0-12-818366-3.20001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128183663200014}
}
@incollection{CHANG2020397,
title = {Chapter 9 - Implementation of Artificial Intelligence in Medicine},
editor = {Anthony C. Chang},
booktitle = {Intelligence-Based Medicine},
publisher = {Academic Press},
pages = {397-412},
year = {2020},
isbn = {978-0-12-823337-5},
doi = {https://doi.org/10.1016/B978-0-12-823337-5.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128233375000093},
author = {Anthony C. Chang},
keywords = {Electronic health records, artificial intelligence adoption, artificial intelligence deployment, black box},
abstract = {The challenge of artificial intelligence adoption and deployment in a health-care setting is presented. Two authors with day-to-day experiences of artificial intelligence deployment discuss their perspectives. A table on the assessment of artificial intelligence readiness in a health-care organization as well as two lists on implementation of artificial intelligence in health care (strategies for success and obstacles to overcome) are presented at the end of this section.}
}
@incollection{SUBASI20201,
title = {Chapter 1 - Introduction},
editor = {Abdulhamit Subasi},
booktitle = {Practical Machine Learning for Data Analysis Using Python},
publisher = {Academic Press},
pages = {1-26},
year = {2020},
isbn = {978-0-12-821379-7},
doi = {https://doi.org/10.1016/B978-0-12-821379-7.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213797000011},
author = {Abdulhamit Subasi},
keywords = {machine learning techniques, artificial intelligence, data mining, business intelligence, decision support},
abstract = {Recently many achievements in artificial intelligence, machine learning, and data mining have been realized. Data collected from several sources contain valuable information that cannot be observed directly but is hidden in the data structure. This hidden information can be transformed into useful information by using various machine learning techniques. Moreover, data analysis using diverse machine learning algorithms can become a vital instrument in extracting significant information hidden in the data. Data analysis is used in many areas for decision support. Typically, collected data is evaluated by experts in the field, but this might result in unreliable or inefficient decision support. Consequently the aim of automated data analysis is to reduce the subjectivity of the expert assessment in business intelligence, or decision support. The artificial intelligence–based data analysis used for assessing different data characteristics helps to make objective decisions by improving accuracy.}
}
@incollection{2020339,
title = {Index},
editor = {Adam Bohr and Kaveh Memarzadeh},
booktitle = {Artificial Intelligence in Healthcare},
publisher = {Academic Press},
pages = {339-355},
year = {2020},
isbn = {978-0-12-818438-7},
doi = {https://doi.org/10.1016/B978-0-12-818438-7.00019-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128184387000198}
}
@article{FANTA2020105044,
title = {How old are the towns and villages in Central Europe? Archaeological data reveal the size of bias in dating obtained from traditional historical sources},
journal = {Journal of Archaeological Science},
volume = {113},
pages = {105044},
year = {2020},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2019.105044},
url = {https://www.sciencedirect.com/science/article/pii/S0305440319301311},
author = {Václav Fanta and Jan Zouhar and Jaromír Beneš and Jiří Bumerl and Petr Sklenicka},
keywords = {Time lag, Dating, Archaeology, Written sources, Middle ages, Historical settlement, Central Europe},
abstract = {In various research fields, from archaeology to landscape history and ecology, it is important to know the date of the origin of historical settlements (i.e. towns, villages, hamlets, isolated farms) as precisely as possible. In Central Europe, there are two primary ways to obtain the date when a settlement was founded: “historical dating” (based on historical written sources) and “archaeological dating” (based on archaeological findings). Historical dating usually does not reflect the real time of origin, since the first reference to a settlement in written sources can be recorded many years after the real origin of the settlement. However, the time lag is unknown. Until now, no study has attempted to show exactly how the time lag differs in different centuries, or whether the time lag has been affected by any geographical factors. This paper compares the dates of origin from archaeological data and from written sources of medieval and early modern settlements (n = 527, AD 850–1600) in the present-day Czech Republic. We also tested the influence of local environmental conditions on the time lag. Our comparison shows that the time lag has been decreasing with the passing of calendar years (from a time lag of 250 years for AD 1000 to approx. 80 years for AD 1400). Towns and places close to major towns also have a shorter time lag in their historical dating (the difference is almost 100 years). These results make an interpretation of the historical dating of medieval towns and villages more complicated. The length of the time lag and its dispersion means that, for the purposes of settlement dating, historical dating needs to be combined with other dating methods (especially in the medieval period). Our results also identify a possible bias in the chronology of landscape transformation.}
}
@article{BAXTER2020364,
title = {Digital Health Primer for Cardiothoracic Surgeons},
journal = {The Annals of Thoracic Surgery},
volume = {110},
number = {2},
pages = {364-372},
year = {2020},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2020.02.072},
url = {https://www.sciencedirect.com/science/article/pii/S000349752030504X},
author = {Ronald D. Baxter and James I. Fann and J. Michael DiMaio and Kevin Lobdell},
abstract = {The burgeoning demands for quality, safety, and value in cardiothoracic surgery, in combination with the advancement and acceleration of digital health solutions and information technology, provide a unique opportunity to improve efficiency and effectiveness simultaneously in cardiothoracic surgery. This primer on digital health explores and reviews data integration, data processing, complex modeling, telehealth with remote monitoring, and cybersecurity as they shape the future of cardiothoracic surgery.}
}
@article{KONDYLAKIS2020103440,
title = {Personally Managed Health Data: Barriers, Approaches, and a Roadmap for the Future},
journal = {Journal of Biomedical Informatics},
volume = {106},
pages = {103440},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103440},
url = {https://www.sciencedirect.com/science/article/pii/S153204642030068X},
author = {Haridimos Kondylakis and Lefteris Koumakis and Manolis Tsiknakis and Stephan Kiefer}
}
@article{HAN2020962,
title = {Short-Term travel speed prediction for urban expressways using convolutional neural network and tensor decomposition},
journal = {Transportation Research Procedia},
volume = {48},
pages = {962-974},
year = {2020},
note = {Recent Advances and Emerging Issues in Transport Research – An Editorial Note for the Selected Proceedings of WCTR 2019 Mumbai},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.08.125},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520305421},
author = {Tianyang Han and Keshuang Tang and Takashi Oguchi},
keywords = {Traffic Speed Prediction, Convolutional Neural Network, Tensor Decomposition, Fusion Algorithm, Expressway, Loop Detector},
abstract = {Prediction is an important part of the traffic management system (TMS) which supports route planning, dynamic traffic control, and information provision. We developed a multi-dimensional learning machine for predicting the traffic speed. Proposed methodology considered both historical experience and near past observation of traffic data by combining a convolutional neural network (CNN) with tensor decomposition (TD) predictor. TD based method is treated as an effective traffic predictor considering temporal-spatial neighbourhood data. However, limited by learning mechanism, such a method cannot extract historical traffic pattern from large datasets. Our main contribution is converting the traffic prediction into a tensor imputation problem, which considers the historical pattern information from CNN by constructing input tensor. Besides, multiple low-rank choosing and weighted optimization are introduced to improve the accuracy of TD-based prediction. We validated our methodology using empirical detector data of urban expressway in Shanghai. Compared with single algorithms, our method has smaller absolute and relative error.}
}
@article{SEGURA2020105550,
title = {Visual computing technologies to support the Operator 4.0},
journal = {Computers & Industrial Engineering},
volume = {139},
pages = {105550},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.11.060},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218306004},
author = {Álvaro Segura and Helen V. Diez and Iñigo Barandiaran and Ander Arbelaiz and Hugo Álvarez and Bruno Simões and Jorge Posada and Alejandro García-Alonso and Ramón Ugarte},
keywords = {Industry 4.0, Augmented Operator, Visual computing, Digital twin, Operator 4.0},
abstract = {Nowadays there is a clear trend for improving productivity and efficiency in the Industrial sector by integrating new advanced ICT technologies that are re-shaping the industrial production paradigms, as in the Industry 4.0 initiative. This new trend does not only affect production lines and machines but also operators. Markets demanding efficiency and flexibility would not be possible excluding the human-factor. Putting the operators in the centre of this new paradigm is mandatory for its success. The operators need to be empowered by giving them new tools and solutions for improving their decision-making processes. In this paper we show how Visual Computing technologies can play a key role in this empowering process, being therefore essential in the realization of the Operator 4.0 vision.}
}
@article{YU202063,
title = {Yeast systems biology in understanding principles of physiology underlying complex human diseases},
journal = {Current Opinion in Biotechnology},
volume = {63},
pages = {63-69},
year = {2020},
note = {Nanobiotechnology ● Systems Biology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2019.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0958166919301375},
author = {Rosemary Yu and Jens Nielsen},
abstract = {Complex human diseases commonly arise from deregulation of cell growth, metabolism, and/or gene expression. Yeast is a eukaryal model organism that is widely used to study these processes. Yeast systems biology benefits from the ability to exert fine experimental control over the cell growth rate and nutrient composition, which allows orthogonal experimental design and generation of multi-omics data at high resolution. This has led to several insights on the principles of cellular physiology, including many cellular processes associated with complex human diseases. Here we review these biological insights together with experimental and modeling approaches developed in yeast to study systems biology. The role of yeast systems biology to further advance systems and personalized therapies for complex diseases is discussed.}
}
@article{BARBIERI2020107,
title = {A Case Study for Problem-based Learning Education in Fault Diagnosis Assessment},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {107-112},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301610},
author = {Giacomo Barbieri and David Sanchez-Londoño and Laura Cattaneo and Luca Fumagalli and David Romero},
keywords = {Engineering Education, Maintenance Engineering, Fault Diagnosis Assessment, Condition-based Maintenance, Predictive Maintenance, Problem-based Learning, Industry 4.0},
abstract = {The use of Condition-based Predictive Maintenance (CBPdM) has grown significantly due to the Industry 4.0 movement and to the advancements in data acquisition, gathering, storing and analytics. In modern maintenance engineering education, there is the need to include CBPdM alongside with traditional maintenance approaches. Within this paper, a case study is proposed for education in Fault Diagnosis Assessment (FDA) using the Problem-based Learning (PBL) approach. Following a PBL approach, the proposed case study consists of a ‘close-to-real-life’ problem and allows the implementation of most of the steps of FDA, and the assessment of the answer through objective metrics. We hope that this work may impulse the production of more educational case studies within the topic of CBPdM.}
}
@article{HO2020497,
title = {Enabling Technologies for Personalized and Precision Medicine},
journal = {Trends in Biotechnology},
volume = {38},
number = {5},
pages = {497-518},
year = {2020},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2019.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167779919303166},
author = {Dean Ho and Stephen R. Quake and Edward R.B. McCabe and Wee Joo Chng and Edward K. Chow and Xianting Ding and Bruce D. Gelb and Geoffrey S. Ginsburg and Jason Hassenstab and Chih-Ming Ho and William C. Mobley and Garry P. Nolan and Steven T. Rosen and Patrick Tan and Yun Yen and Ali Zarrinpar},
keywords = {personalized medicine, precision medicine, therapeutics, diagnostics, artificial intelligence, clinical trials},
abstract = {Individualizing patient treatment is a core objective of the medical field. Reaching this objective has been elusive owing to the complex set of factors contributing to both disease and health; many factors, from genes to proteins, remain unknown in their role in human physiology. Accurately diagnosing, monitoring, and treating disorders requires advances in biomarker discovery, the subsequent development of accurate signatures that correspond with dynamic disease states, as well as therapeutic interventions that can be continuously optimized and modulated for dose and drug selection. This work highlights key breakthroughs in the development of enabling technologies that further the goal of personalized and precision medicine, and remaining challenges that, when addressed, may forge unprecedented capabilities in realizing truly individualized patient care.}
}
@incollection{CHANG2020267,
title = {Chapter 8 - Artificial Intelligence in Subspecialties},
editor = {Anthony C. Chang},
booktitle = {Intelligence-Based Medicine},
publisher = {Academic Press},
pages = {267-396},
year = {2020},
isbn = {978-0-12-823337-5},
doi = {https://doi.org/10.1016/B978-0-12-823337-5.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128233375000081},
author = {Anthony C. Chang},
keywords = {Machine and deep learning, natural language processing, robotic process automation, robotic technology, medical imaging, decision supportn, virtual assistants, altered reality},
abstract = {The present state of artificial intelligence in subspecialties is summarized to provide a backdrop for this chapter on the state of artificial intelligence in each subspecialty. First, a broad discussion of artificial intelligence application for all subspecialties reviews the main areas of focus such as use of machine and deep learning for medical image interpretation and decision support, use of artificial intelligence tools for administrative support, use of natural language processing for communication, use of artificial intelligence for data mining, and use of machine and deep learning for risk assessment and intervention. For each of the subspecialties or health care sector (such as medical education and training or health-care administration), the current published reviews and selected works are first discussed, followed by present assessment and future strategy for that subspecialty. For each subspecialty, a table of clinical relevance versus current artificial intelligence availability is presented to visualize potential application in the future.}
}
@article{FAHMIDEH2020101409,
title = {An exploration of IoT platform development},
journal = {Information Systems},
volume = {87},
pages = {101409},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437919302728},
author = {Mahdi Fahmideh and Didar Zowghi},
keywords = {IoT platform, Smart city, Development process lifecycle, Evaluation framework},
abstract = {IoT (Internet of Things) platforms are key enablers for smart city initiatives, targeting the improvement of citizens’ quality of life and economic growth. As IoT platforms are dynamic, proactive, and heterogeneous socio-technical artefacts, systematic approaches are required for their development. Limited surveys have exclusively explored how IoT platforms are developed and maintained from the perspective of information system development process lifecycle. In this paper, we present a detailed analysis of 63 approaches. This is accomplished by proposing an evaluation framework as a cornerstone to highlight the characteristics, strengths, and weaknesses of these approaches. The survey results not only provide insights of empirical findings, recommendations, and mechanisms for the development of quality aware IoT platforms, but also identify important issues and gaps that need to be addressed.}
}
@article{MA2020121,
title = {Blockchain-based mechanism for fine-grained authorization in data crowdsourcing},
journal = {Future Generation Computer Systems},
volume = {106},
pages = {121-134},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19300822},
author = {Haiying Ma and Elmo X. Huang and Kwok-Yan Lam},
keywords = {Data crowdsourcing, Blockchain, Smart contracts, Fine-grained authorization, Data trade},
abstract = {Data crowdsourcing is a distributed data acquisition method to efficiently collect a sizeable amount of high-quality data from a large network of contributors who participate in data trading activities. However, traditional data crowdsourcing platforms are almost invariably based on a centralized architecture, which tends to give unfair advantages to the platform operator; besides, centralized platforms are obvious targets for cybersecurity attacks and become a single point of failure. Furthermore, a centralized approach with stringent security control also suffers from serious scalability issue. For example, if data owners manage a database with large amounts of valuable data, they have to retrieve data from their database in accordance with certain access policies and encrypt retrieved data for each requester; hence they become bottlenecks in the data trading process when the number of requesters is very large. To address the above issues, we propose a blockchain-based mechanism for fine-grained authorization in data crowdsourcing (BC-FGA-DCrowd). In the BC-FGA-DCrowd scheme, we use a public blockchain to implement cryptocurrencies and payment services as incentive schemes for data trading platform users. With this approach, data owners can employ Ciphertext-Policy Attribute-Based Encryption (CP-ABE) to pre-process the complex encryption workload, and generate the attribute private key for data requester to achieve the fine-grained authorization. In this paper, we also prove that the BC-FGA-DCrowd scheme satisfies the correctness and fairness requirements of data trading, and can effectively withstand malicious activities of internal users and external DDos and Sybil attackers. The approach was tested on a private Ethereum network using Ganache with a local host.}
}
@incollection{WARHEIT2020397,
title = {Chapter 29 - Selected aspects of nanotoxicology},
editor = {Carey N. Pope and Jing Liu},
booktitle = {An Introduction to Interdisciplinary Toxicology},
publisher = {Academic Press},
pages = {397-409},
year = {2020},
isbn = {978-0-12-813602-7},
doi = {https://doi.org/10.1016/B978-0-12-813602-7.00029-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128136027000296},
author = {D.B. Warheit and S.C. Brown},
keywords = {Exposure, hazard versus risk, NanoRisk Framework, particle toxicology, subchronic inhalation toxicity, epidermis},
abstract = {Nanotechnology is a multidisciplinary science and technology that involves the integration of chemistry and physics to develop and apply materials and structures from 1 to 100nm. When some materials are produced at the nanoscale their behaviors can change resulting in unique and novel properties. For example, many metallic particles adsorb light differently as particle size decreases resulting in changes in colors, other materials that typically serve as insulators can become conductive. However, these unique and novel behaviors that allow for new innovations also have drawn questions regarding the potential health impacts different from those anticipated from their chemical composition alone. It remains to be determined whether human exposures to nanoparticles through the usual pathways (oral, dermal, and inhalation) result in different outcomes when compared to those encountered by exposures to larger, nonnano “fine-sized” particles. This chapter contributes a brief overview of selected fundamental aspects of nanotoxicology including (1) relevant routes of exposure; (2) the basic but important concepts of hazard versus risk; (3) an overview of physicochemical properties of nanomaterials relevant to particle toxicology; (4) emerging definitions and regulatory relevant considerations for nanomaterials; (5) a brief discussion of the NanoRisk Framework; and (6) an example of a subchronic inhalation toxicity study with carbon nanofibers in rats.}
}
@incollection{2020321,
title = {Index},
editor = {John Ladley},
booktitle = {Data Governance (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {321-327},
year = {2020},
isbn = {978-0-12-815831-9},
doi = {https://doi.org/10.1016/B978-0-12-815831-9.09993-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158319099938}
}
@article{HU2020126733,
title = {Using Wi-Fi probe and location data to analyze the human distribution characteristics of green spaces: A case study of the Yanfu Greenland Park, China},
journal = {Urban Forestry & Urban Greening},
volume = {54},
pages = {126733},
year = {2020},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2020.126733},
url = {https://www.sciencedirect.com/science/article/pii/S161886671930651X},
author = {Xinyu Hu and Peiyu Shen and Yi Shi and Zhonghu Zhang},
keywords = {Human distribution characteristics, Public open space, Urban green space, Wi-Fi probe, Spatio-temporal data},
abstract = {Public open space constitutes a significant part of the sustainable urban environment. Urban green space—which includes parks, sports fields, and wetlands—is a crucial type of public open space in a healthy urban ecosystem. Most studies on urban green spaces comprise large-scale objective evaluations, with relatively few related theoretical studies from the perspective of human behavior. This is predominantly due to the technical constraints involved in obtaining small-scale public open space population activity data. To address this issue, this study examines the crowd activities of an urban green space by combining Wi-Fi probe and location data to discern population distribution characteristics. This study uses Yanfu Greenland Park, a green space in a central area of Shanghai—a typical high-density city—as a case study. This study proposes a technical method for measuring population distribution in a small-scale public space, demonstrating its applicability through an analysis of the population density distribution and activity trajectory in an urban green space. The methodology demonstrated in this study can help urban planning designers and managers create more successful public open spaces. This technical method may also assist urban planning and related research on different scales.}
}
@article{JINGMIN2020334,
title = {A New Stellar Spectral Feature Extraction Method Based on Two-dimensional Fourier Spectrum Image and Its Application in the Stellar Spectral Classification Based on Deep Network},
journal = {Chinese Astronomy and Astrophysics},
volume = {44},
number = {3},
pages = {334-344},
year = {2020},
issn = {0275-1062},
doi = {https://doi.org/10.1016/j.chinastron.2020.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0275106220300771},
author = {ZHANG Jing-min and MA Chen-ye and Wang Lu and Du Li-ting and XU Ting-ting and Ai Lin-pin and ZHOU Wei-hong},
keywords = {Stars: fundamental parameters, Methods: data analysis, techniques: spectral analysis},
abstract = {The classification of celestial spectra is one of the important contents of astronomical research. The key is to select and extract the most effective feature for classification from spectra data. In this paper, we propose a new feature extraction method for astronomical spectra based on two-dimensional Fourier spectrum image, and apply the method to the classification study of LAMOST (the Large Sky Area Multi-Object Fiber Spectroscopic Telescope) stellar spectral data. The spectra data are from LAMOST Data Release 5 (DR5). We select 30000 F, G, and K types of spectra data. The short-time Fourier transform (STFT) is used to transform the one-dimensional spectra data into two-dimensional Fourier spectrum images. We classify and test these two-dimensional Fourier spectrum images with a module based on deep convolution network, and the classification accuracy rate is 92.90%. The experimental result shows that the LAMOST stellar spectra data can be transformed into the two-dimensional Fourier spectrum images by the STFT. These spectral images inform new features, and build a new feature space, which is effective for classification. The method is a fully new attempt in spectra classification, which has certainly a pioneering significance for the classification and mining of massive celestial spectra.}
}
@article{ESMAEILZADEH2020104157,
title = {The impact of data entry structures on perceptions of individuals with chronic mental disorders and physical diseases towards health information sharing},
journal = {International Journal of Medical Informatics},
volume = {141},
pages = {104157},
year = {2020},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104157},
url = {https://www.sciencedirect.com/science/article/pii/S1386505620302434},
author = {Pouyan Esmaeilzadeh and Tala Mirzaei and Spurthy Dharanikota},
keywords = {HIE, Information quality, Data entry structure, Physical diseases, Mental disorders},
abstract = {Background and objective
Collecting, integrating, and sharing mental and physical health information can enhance the care process of patients and improve the completeness of patient databases in the health information exchange (HIE) networks. There is a need to encourage patients with physical and mental disorders to share their health information with providers. Data entry interfaces are suggested as an important factor affecting the quality of information. However, little is known about whether individuals with different diseases (mental and physical) care for the data entry structure in sharing personal health information (PHI).
Materials and methods
We conduct four experiments to examine the impact of different health problems (mental vs. physical) and types of data entry interfaces (structured vs. unstructured) on individuals' perceptions of information quality and their willingness to share their health information.
Results
Findings demonstrate that the type of disease and degree of data entry structure significantly influence individuals' perceptions of usefulness, accessibility, concise presentation, understandability, psychological risk, privacy concern, stigma, and willingness to share health information.
Discussion
People with mental disorders prefer structured data interfaces as they perceive that a high degree of data entry structure can protect their privacy and mitigate stigma and psychological risk more than unstructured interfaces. Individuals with physical illnesses favor structured interfaces for their format, which is brief, comprehensive, accessible, useful, and understandable. People suffering from physical diseases are more likely to share their information when a highly-structured data entry interface is used. Moreover, individuals with mental disorders are less likely to disclose their information when providers collect health records using an unstructured data entry interface.
Conclusions
This study suggests that the best level of structure for data entry interfaces could be designed at the point of care consistent with patients' health status and their type of diseases to improve the success of HIE networks.}
}
@article{HU2020105665,
title = {Efficient mapping of crash risk at intersections with connected vehicle data and deep learning models},
journal = {Accident Analysis & Prevention},
volume = {144},
pages = {105665},
year = {2020},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2020.105665},
url = {https://www.sciencedirect.com/science/article/pii/S0001457519319062},
author = {Jiajie Hu and Ming-Chun Huang and Xiong Yu},
keywords = {Connected vehicles, Multi-layer perceptron, Convolutional neural network, Crash-prone intersection, Basic safety messages, Surrogate measures of safety},
abstract = {Traditional methods for identifying crash-prone roadways are mainly based on historical crash data. It usually requires more than three years to collect a sufficient amount of dataset for road safety assessment. However, the emerging connected vehicles (CVs) technology generates rich instantaneous information, which can be used to identify dangerous road sections proactively. Information about the identified crash-prone intersections can be shared with the surrounding vehicles via CVs communication technology to promote cautious driving behaviors; in the longer term, such information will guide the implementation of countermeasures to prevent potential crashes. This study proposed a deep-learning based method to predict the risk level at intersections based on CVs data from the Michigan Safety Pilot program and historical traffic and intersection crash data in areas around Ann Arbor, Michigan, USA. One month of data by CVs at intersections were used for analyses, which accounts for about 3%–12% of overall trips. The risk levels of 774 intersections (i.e., low, medium and high risk) are determined by the annual crash rates. Feature extraction process is applied to both CV’s data and traffic data at each intersection and 24 features are extracted. Two black-box deep-learning models, multi-layer perceptron (MLP) and convolutional neural network (CNN) are trained with the extracted features. A number of hyperparameters that affect prediction performance are fine-tuned using Bayesian optimization algorithm for each model. The performance of the two deep learning models, which are black-box models, were also compared with a decision tree model, a white-box type of simple machine learning model. The results showed that the accuracies of deep learning (DL) models were slightly better (both over 90 %) than the decision tree model (about 87 %). This indicated that the DL models were capable of uncover the inherent complexity from the dataset and therefore provided higher accuracy than the traditional machine learning model. CNN model achieves slightly higher accuracy (93.8 %) and is recommended as the classifier to predict the risk level at intersections in practice. The interpretability analysis of the CNN model is conducted to confirm the validity of the model. This study shows that combination of CVs data (V2V and V2I) and deep learning networks (i.e. MLP and CNN used in this paper) is promising to determine crash risks at intersections with high time efficiency and at low CV penetration rates, which help to deploy countermeasures to reduce the crash rates and resolve traffic safety problems.}
}
@article{FULLER2020496,
title = {Meeting the Challenge of Scientific Dissemination in the Era of COVID-19: Toward a Modular Approach to Knowledge-Sharing for Radiation Oncology},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {108},
number = {2},
pages = {496-505},
year = {2020},
note = {Radiation Oncology: Through and Beyond the COVID-19 Pandemic},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2020.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S036030162031364X},
author = {Clifton D. Fuller and Lisanne V. {van Dijk} and Reid F. Thompson and Jacob G. Scott and Ethan B. Ludmir and Charles R. Thomas}
}
@incollection{BARBA2020149,
title = {Chapter 8 - Genome Informatics Pipelines and Genome Browsers},
editor = {George P. Patrinos},
booktitle = {Applied Genomics and Public Health},
publisher = {Academic Press},
pages = {149-169},
year = {2020},
series = {Translational and Applied Genomics},
isbn = {978-0-12-813695-9},
doi = {https://doi.org/10.1016/B978-0-12-813695-9.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012813695900008X},
author = {Evaggelia Barba and Evangelia-Eirini Tsermpini and George P. Patrinos and Maria Koromina},
keywords = {Genome informatics, NGS, bioinformatics pipelines, genome browsers},
abstract = {During the last two decades, next-generation sequencing (NGS) technologies have led to remarkable advances in the field of genetics. NGS technologies produce a large amount of sequence data leading to a potential substitution of all other high-throughput technologies. Moreover, NGS protocols produce an increasing volume of data highlighting potential associations of genes with complex disease phenotypes. This chapter aims in providing useful information regarding the most widely used and informative genome browsers while demonstrating the basic details of building genome informatics pipeline in the rising era of (NGS) analysis.}
}
@article{2020105405,
title = {Asia Pacific},
journal = {Computer Law & Security Review},
volume = {36},
pages = {105405},
year = {2020},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105405},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920300108}
}
@incollection{2020279,
title = {Index},
editor = {Steven J. Goodman and Timothy J. Schmit and Jaime Daniels and Robert J. Redmon},
booktitle = {The GOES-R Series},
publisher = {Elsevier},
pages = {279-283},
year = {2020},
isbn = {978-0-12-814327-8},
doi = {https://doi.org/10.1016/B978-0-12-814327-8.09990-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012814327809990X}
}
@article{SCHWAB2020111066,
title = {Glucocorticoids and cortical decoding in the phobic brain},
journal = {Psychiatry Research: Neuroimaging},
volume = {300},
pages = {111066},
year = {2020},
issn = {0925-4927},
doi = {https://doi.org/10.1016/j.pscychresns.2020.111066},
url = {https://www.sciencedirect.com/science/article/pii/S092549272030038X},
author = {Simon Schwab and Andrea Federspiel and Yosuke Morishima and Masahito Nakataki and Werner Strik and Roland Wiest and Markus Heinrichs and Dominique {de Quervain} and Leila M. Soravia},
keywords = {Phobia, Anxiety disorder, Cortisol, fMRI, Pattern analysis},
abstract = {Glucocorticoids reduce phobic fear in anxiety disorders and enhance psychotherapy, possibly by reducing the retrieval of fear memories and enhancing the consolidation of new corrective memories. Glucocorticoid signaling in the basolateral amygdala can influence connected fear and memory-related cortical regions, but this is not fully understood. Previous studies investigated specific pathways moderated by glucocorticoids, for example, visual-temporal pathways; however, these analyses were limited to a-priori selected regions. Here, we performed whole-brain pattern analysis to localize phobic stimulus decoding related to the fear-reducing effect of glucocorticoids. We reanalyzed functional magnetic resonance imaging (fMRI) data from a previously published study with spider-phobic patients and healthy controls. The patients received glucocorticoids or a placebo before the exposure to spider images. There was moderate evidence that patients with phobia had higher decoding of phobic content in the anterior cingulate cortex (ACC) and the left and right anterior insula compared to controls. Decoding in the ACC and the right insula showed strong evidence for correlation with experienced fear. Patients with cortisol reported a reduction of fear by 10–13%; however, there was only weak evidence for changes in neural decoding compared to placebo which was found in the precuneus, the opercular cortex, and the left cerebellum.}
}
@article{MINAEI2020102246,
title = {Evolution, density and completeness of OpenStreetMap road networks in developing countries: The case of Iran},
journal = {Applied Geography},
volume = {119},
pages = {102246},
year = {2020},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2020.102246},
url = {https://www.sciencedirect.com/science/article/pii/S0143622818312839},
author = {Masoud Minaei},
keywords = {OSM road Network, Density and diversity, GIS, VGI, Iran},
abstract = {OpenStreetMap (OSM) is a volunteered platform designed to provide up-to-date and freely available geographic information data. OSM is known as one of the most extensively used instances of Crowdsourcing Geographic Data (CSGD)/Crowdsourced Geographic Information (CGI), Volunteered Geographic Information (VGI), or the Neogeography paradigm. Economically, OSM development can be beneficial for governments, especially in developing countries such as Iran, where financial support is limited. This paper analyzes the spatial pattern, evolution, density, and diversity of OSM road (OSMr) networks in Iran between 2008 and 2016 and looks to find casual relations between the OSM and census statistics. This is due to the fact that OSMr completeness reflects the importance of OSM data in human life. The diversity of OSM roads further reflects the concerns, requirements, and worthiness of clients about different roads’ information. The results show that the road network in Iran considerably increased from 2008 to 2016, with road length increasing to 489,400 km in 2016 from 4300 km in 2008. In addition, road density grew while road diversity and evenness declined, which could be due to an increase in main roads. These results propose a more considerable need for a more comprehensive approach using VGI to supplement gaps in authoritative data in developing countries.}
}
@article{LINSCHEID20201132,
title = {Quantitative Proteomics of Human Heart Samples Collected In Vivo Reveal the Remodeled Protein Landscape of Dilated Left Atrium Without Atrial Fibrillation},
journal = {Molecular & Cellular Proteomics},
volume = {19},
number = {7},
pages = {1132-1144},
year = {2020},
issn = {1535-9476},
doi = {https://doi.org/10.1074/mcp.RA119.001878},
url = {https://www.sciencedirect.com/science/article/pii/S153594762034977X},
author = {Nora Linscheid and Pi Camilla Poulsen and Ida Dalgaard Pedersen and Emilie Gregers and Jesper Hastrup Svendsen and Morten Salling Olesen and Jesper Velgaard Olsen and Mario Delmar and Alicia Lundby},
keywords = {Cardiovascular disease, cardiovascular function or biology, clinical proteomics, label-free quantification, tandem mass spectrometry, cardiac proteomics, heart physiology, quantitative proteomics},
abstract = {Genetic and genomic research has greatly advanced our understanding of heart disease. Yet, comprehensive, in-depth, quantitative maps of protein expression in hearts of living humans are still lacking. Using samples obtained during valve replacement surgery in patients with mitral valve prolapse (MVP), we set out to define inter-chamber differences, the intersect of proteomic data with genetic or genomic datasets, and the impact of left atrial dilation on the proteome of patients with no history of atrial fibrillation (AF). We collected biopsies from right atria (RA), left atria (LA) and left ventricle (LV) of seven male patients with mitral valve regurgitation with dilated LA but no history of AF. Biopsy samples were analyzed by high-resolution mass spectrometry (MS), where peptides were pre-fractionated by reverse phase high-pressure liquid chromatography prior to MS measurement on a Q-Exactive-HF Orbitrap instrument. We identified 7,314 proteins based on 130,728 peptides. Results were confirmed in an independent set of biopsies collected from three additional individuals. Comparative analysis against data from post-mortem samples showed enhanced quantitative power and confidence level in samples collected from living hearts. Our analysis, combined with data from genome wide association studies suggested candidate gene associations to MVP, identified higher abundance in ventricle for proteins associated with cardiomyopathies and revealed the dilated LA proteome, demonstrating differential representation of molecules previously associated with AF, in non-AF hearts. This is the largest dataset of cardiac protein expression from human samples collected in vivo. It provides a comprehensive resource that allows insight into molecular fingerprints of MVP and facilitates novel inferences between genomic data and disease mechanisms. We propose that over-representation of proteins in ventricle is consequent not to redundancy but to functional need, and conclude that changes in abundance of proteins known to associate with AF are not sufficient for arrhythmogenesis.}
}
@incollection{PESAPANE2020533,
title = {Chapter 52 - Regulatory issues for artificial intelligence in radiology},
editor = {Joel Faintuch and Salomao Faintuch},
booktitle = {Precision Medicine for Investigators, Practitioners and Providers},
publisher = {Academic Press},
pages = {533-543},
year = {2020},
isbn = {978-0-12-819178-1},
doi = {https://doi.org/10.1016/B978-0-12-819178-1.00052-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191781000526},
author = {Filippo Pesapane and Matteo B. Suter and Marina Codari and Francesca Patella and Caterina Volonté and Francesco Sardanelli},
keywords = {Artificial intelligence (AI), Artificial neural networks (ANNs), Deep learning, Legislation, Machine learning, Policy, Precision medicine, Privacy, Radiology},
abstract = {Medical applications of artificial intelligence (AI) are growing rapidly, projecting future utility in healthcare, with new significant challenges to face. In this chapter, we provide basic definitions of terms such as machine- and deep-learning, analyze the integration of AI into medicine, and summarize the present and the future applications in radiology, particularly in Radiomics and Radiogenomics. Also, we offer an overview of the state of regulation of AI in healthcare, and future strategies to make AI applications safe and useful. We analyze the issue, both legal and ethical, of accountability, and the legal framework regulating medical software and data protection. Finally, we explore the ongoing lines of investigation of AI in the field of precision medicine.}
}
@article{WEBER2020105380,
title = {Socio-ethical values and legal rules on automated platforms: The quest for a symbiotic relationship},
journal = {Computer Law & Security Review},
volume = {36},
pages = {105380},
year = {2020},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2019.105380},
url = {https://www.sciencedirect.com/science/article/pii/S0267364919303917},
author = {Rolf H. Weber},
keywords = {Compliance, Impact assessment, Rule-making, Standard terms, Trust},
abstract = {The deployment of artificial intelligence on automated platforms needs to go hand in hand with the development of a legal framework safeguarding socio-ethical values as well as fundamental rights, particularly the self-determination and the non-discrimination principle. A trust-based approach focused on human values can mitigate a potential clash between a solely market- and technology-oriented use of artificial intelligence and a more inclusive multistakeholder approach. The regulatory tools are to be designed in a manner that leads to a symbiotic relationship between ethics and law.}
}
@article{SUNMOLA20201469,
title = {Evaluation of Motivating and Requiring Factors for Milestones in IT Projects},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1469-1477},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.204},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320679},
author = {Hakeem Omolade Sunmola},
keywords = {Information Technology, Interpretive Structural Modelling, Project Milestones, Project Management},
abstract = {Businesses are having to carry out Information technology (IT) projects of increasing complexity due to several reasons including the advances in information technology and the implementation challenges it brings. Effective project management is key to achieving project success and, in this space, lies the opportunity to use tools for supporting the process. One of such tools are milestones; A milestone is a scheduled event of zero-time duration that marks the completion of one or more important tasks. This research studies the factors influencing the creation of milestones in IT projects and it does so through literature review, focus group, and interpretive structural modelling. Twelve factors were considered for the interpretive structural model and the results show that several factors can influence the creation of milestones in IT projects. Most important it seems are factors arising from internal and external stakeholders. Using more evidence to support inferences about the factor influences and validating the model with wide participation from practitioners are suggested as future work.}
}
@article{ZENG2020102697,
title = {Integrating Internet media into urban flooding susceptibility assessment: A case study in China},
journal = {Cities},
volume = {101},
pages = {102697},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102697},
url = {https://www.sciencedirect.com/science/article/pii/S0264275118310989},
author = {Zhongping Zeng and Jinyu Lan and Abdur Rahim Hamidi and Shangjun Zou},
keywords = {Citizen science, Internet-based media, Urban flooding susceptibility assessment, Wuhan, Urban resilience, Sustainability},
abstract = {This study demonstrates the value of the public and of media observations (i.e. ‘citizen science’) for the modeling and understanding of the predisposing factors of urban flooding as a contribution to flooding hazard assessment. The 2016 Wuhan flood in the center of China utilized observations of non-scientists' from Internet media to capture the spatial distribution of urban flooding. Various sources in online news, blogs, bbs, and governmental websites were included. Independent flood-related factors such as rainfall, wetlands degradation, elevation, land use and land cover, curvature, slope, and normalized difference vegetation index were integrated into a logistic regression model to produce a susceptibility map. The feasibility and accuracy of this model was evaluated via comparison with official inundation maps and shortcomings of previous drainage system design. The accuracy assessment by receiver operating characteristics curve analysis estimated a success rate of 95.4%. This result shows that the data from mainstream media and public observations provide accurate and comparable information on flood occurrence at the urban scale. The assessment can be used to plan and design preventative drainage strategies for decision-makers. Finally, the limitations of Internet media as a new data resource for flooding risk assessment and future directions are discussed.}
}
@article{WANG202029,
title = {Robust 3D reconstruction of building surfaces from point clouds based on structural and closed constraints},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {170},
pages = {29-44},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620302501},
author = {Senyuan Wang and Guorong Cai and Ming Cheng and José {Marcato Junior} and Shangfeng Huang and Zongyue Wang and Songzhi Su and Jonathan Li},
keywords = {3D reconstruction, Building surface reconstruction, Closed-constraints, LiDAR point clouds},
abstract = {The reconstruction of buildings using inhomogeneous and unstructured point clouds is a challenging task for photogrammetry and computer vision research communities. A new approach for 3D building surface modeling, based on closed constraints, is proposed. First, a region growth algorithm is applied to fit the input point clouds by a set of candidate planes. Then, additional candidate planes are generated from the initial planes according to a rigid transformation followed by expanding the original primitive set to the candidate model set through generation rules. Furthermore, an energy function is employed to combine the data fitting errors with the structural constraints at the model selection stage. Finally, the 3D building surface model is generated from the candidate set through energy minimization. More precisely speaking, we adopt the surface optimization scheme that enforces the 3D polygonal surfaces of the building to be consistent with a priori geometric structures. Our approach was assessed using multi-source datasets with different densities, noise levels covering diverse and complex structures. The experimental results demonstrated that the proposed approach achieves better accuracy and robustness than those of several state-of-the-art methods.}
}
@article{WU2020110456,
title = {CVE-assisted large-scale security bug report dataset construction method},
journal = {Journal of Systems and Software},
volume = {160},
pages = {110456},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.110456},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219302304},
author = {Xiaoxue Wu and Wei Zheng and Xiang Chen and Fang Wang and Dejun Mu},
keywords = {Security bug report prediction, Voting classification, Dataset construction, Common vulnerabilities and exposures},
abstract = {Identifying SBRs (security bug reports) is crucial for eliminating security issues during software development. Machine learning are promising ways for SBR prediction. However, the effectiveness of the state-of-the-art machine learning models depend on high-quality datasets, while gathering large-scale datasets are expensive and tedious. To solve this issue, we propose an automated data labeling approach based on iterative voting classification. It starts with a small group of ground-truth traing samples, which can be labeled with the help of authoritative vulnerability records hosted in CVE (Common Vulnerabilities and Exposures). The accuracy of the prediction model is improved with an iterative voting strategy. By using this approach, we label over 80k bug reports from OpenStack and 40k bug reports from Chromium. The correctness of these labels are then manually reviewed by three experienced security testing members. Finally, we construct a large-scale SBR dataset with 191 SBRs and 88,472 NSBRs (non-security bug reports) from OpenStack; and improve the quality of existing SBR dataset Chromium by identifying 64 new SBRs from previously labeled NSBRs and filtering out 173 noise bug reports from this dataset. These share datasets as well as the proposed dataset construction method help to promote research progress in SBR prediction research domain.}
}
@article{RAJA2020112,
title = {1H-NMR-based metabolomics for cancer targeting and metabolic engineering –A review},
journal = {Process Biochemistry},
volume = {99},
pages = {112-122},
year = {2020},
issn = {1359-5113},
doi = {https://doi.org/10.1016/j.procbio.2020.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S1359511320309156},
author = {Ganesan Raja and Youngmi Jung and Sang Hoon Jung and Tae-Jin Kim},
keywords = {Cancer, Metabolomics, Metabolic engineering, Target profiling, Software, Therapeutics},
abstract = {Nuclear magnetic resonance (NMR) spectroscopy acts as the best tool that can be used in tissue engineering scaffolds to investigate unknown metabolites. Moreover, metabolomics is a systems approach for examining in vivo and in vitro metabolic profiles, which promises to provide data on cancer metabolic alterations. However, metabolomic profiling allows for the activity of small molecules and metabolic alterations to be measured. Furthermore, metabolic profiling also provides high-spectral resolution, which can then be linked to potential metabolic relationships. An altered metabolism is a hallmark of cancer that can control many malignant properties to drive tumorigenesis. Metabolite targeting and metabolic engineering contribute to carcinogenesis by proliferation, and metabolic differentiation. The resulting the metabolic differences are examined with traditional chemometric methods such as principal component analysis (PCA), and partial least squares-discriminate analysis (PLS-DA). In this review, we examine NMR-based activity metabolomic platforms that can be used to analyze various fluxomics and for multivariant statistical analysis in cancer. We also aim to provide the reader with a basic understanding of NMR spectroscopy, cancer metabolomics, target profiling, chemometrics, and multifunctional tools for metabolomics discrimination, with a focus on metabolic phenotypic diversity for cancer therapeutics.}
}
@article{IPHAR2020113219,
title = {Data integrity assessment for maritime anomaly detection},
journal = {Expert Systems with Applications},
volume = {147},
pages = {113219},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113219},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420300452},
author = {Clément Iphar and Cyril Ray and Aldo Napoli},
keywords = {Data falsification, Integrity assessment, AIS},
abstract = {In the last years, systems broadcasting mobility data underwent a rise in cyberthreats, jeopardising their normal use and putting both users and their environment at risk. In this respect, anomaly detection methods are needed to ensure an assessment of such systems. In this article, we propose a rule-based method for data integrity assessment, with rules built from the system technical specifications and by domain experts, and formalised by a logic-based framework, resulting in the triggering of situation-specific alerts. A use case is proposed on the Automatic Identification System, a worldwide localisation system for vessels, based on its poor level of security which allows errors, falsifications and spoofing scenarios. The discovery of abnormal reporting cases aims to assist marine traffic surveillance, preserve the human life at sea and mitigate hazardous behaviours against ports, off-shore structures and the environment.}
}
@article{YU202030244,
title = {A new method of predicting the saturation pressure of oil reservoir and its application},
journal = {International Journal of Hydrogen Energy},
volume = {45},
number = {55},
pages = {30244-30253},
year = {2020},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2020.08.042},
url = {https://www.sciencedirect.com/science/article/pii/S0360319920330378},
author = {Guoyi Yu and Feng Xu and Yingzhi Cui and Xiangling Li and Chujuan Kang and Cheng Lu and Siyu Li and Lin Bai and Shuheng Du},
keywords = {Oil reservoir, Saturation pressure, Random forest, Decision tree, ANN, Empirical formula},
abstract = {Saturation pressure is a vital parameter of oil reservoir which can reflect the oilfield characteristics and determine the oilfield development process, and it is determined by experiments in the laboratory in general. However, there was only one well with saturation pressure test in this target reservoir, and it is necessary to determine whether this parameter is right or not. In this work, we present a new method for quickly determining saturation pressure using machine learning algorithms, including random forest regressor (RF), support vector machine (SVM), decision trees (DT), and artificial neural network (ANN or NN). Using these approaches, saturation pressure was obtained by using the initial solution gas-oil ratio (GOR), temperature, API gravity and other reservoir-fluid data available in the oilfields. Compared with the empirical formula for saturation pressure calculation, the calculated result shows that the accuracy given from machine learning is higher than that from other formulas at home and abroad, and has a good match with the lab test. On the basis of the calculated saturation pressure, it can determine whether the reservoir enters into the stage of dissolved gas drive or not, which also provides the basis for maintaining the reservoir pressure by water injection in advance, rational development decision-making and work over measures. This approach above can provide technical guidance for predicting the saturation pressure in the development of different kinds of reservoirs, including the sandstone reservoirs and carbonate reservoirs.}
}
@article{CHEN2020112096,
title = {Mapping horizontal and vertical urban densification in Denmark with Landsat time-series from 1985 to 2018: A semantic segmentation solution},
journal = {Remote Sensing of Environment},
volume = {251},
pages = {112096},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112096},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720304697},
author = {Tzu-Hsin Karen Chen and Chunping Qiu and Michael Schmitt and Xiao Xiang Zhu and Clive E. Sabel and Alexander V. Prishchepov},
keywords = {Urban form, Urban growth, Urbanization, Deep learning, Semantic segmentation, Multi-temporal classification, Spatial and temporal transferability, Landsat},
abstract = {Landsat imagery is an unparalleled freely available data source that allows reconstructing land-cover and land-use change, including urban form. This paper addresses the challenge of using Landsat data, particularly its 30 m spatial resolution, for monitoring three-dimensional urban densification. Unlike conventional convolutional neural networks (CNNs) for scene recognition resulting in resolution loss, the proposed semantic segmentation framework provides a pixel-wise classification and improves the accuracy of urban form mapping. We compare temporal and spatial transferability of an adapted DeepLab model with a simple fully convolutional network (FCN) and a texture-based random forest (RF) model to map urban density in the two morphological dimensions: horizontal (compact, open, sparse) and vertical (high rise, low rise). We test whether a model trained on the 2014 data can be applied to 2006 and 1995 for Denmark, and examine whether we could use the model trained on the Danish data to accurately map ten other European cities. Our results show that an implementation of deep networks and the inclusion of multi-scale contextual information greatly improve the classification and the model's ability to generalize across space and time. Between the two semantic segmentation models, DeepLab provides more accurate horizontal and vertical classifications than FCN when sufficient training data is available. By using DeepLab, the F1 score can be increased by 4 and 10 percentage points for detecting vertical urban growth compared to FCN and RF for Denmark. For mapping the ten other European cities with training data from Denmark, DeepLab also shows an advantage of 6 percentage points over RF for both horizontal and vertical dimensions. The resulting maps across the years 1985 to 2018 reveal different patterns of urban growth between Copenhagen and Aarhus, the two largest cities in Denmark, illustrating that those cities have used various planning policies in addressing population growth and housing supply challenges. In summary, we propose a transferable deep learning approach for automated, long-term mapping of urban form from Landsat images that is effective in areas experiencing a slow pace of urban growth or with small-scale changes.}
}
@incollection{LADLEY202061,
title = {Chapter 6 - Overview of data governance development and deployment},
editor = {John Ladley},
booktitle = {Data Governance (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {61-80},
year = {2020},
isbn = {978-0-12-815831-9},
doi = {https://doi.org/10.1016/B978-0-12-815831-9.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158319000060},
author = {John Ladley},
keywords = {Approach, Methods, Checklist, Iterations, Agile, Approach, Noninvasive},
abstract = {This chapter presents an overview of the process to deploy the data governance program.}
}
@article{FOSSOWAMBA2020107791,
title = {Dynamics between blockchain adoption determinants and supply chain performance: An empirical investigation},
journal = {International Journal of Production Economics},
volume = {229},
pages = {107791},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107791},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320301687},
author = {Samuel {Fosso Wamba} and Maciel M. Queiroz and Laura Trinchera},
keywords = {Blockchain, Supply chain, Transparency, Digital disruption, Performance},
abstract = {The logistics and supply chain management (SCM) field is experimenting with the integration of blockchain, a cutting-edge, and highly disruptive technology. Yet, blockchain is still nascent, and the extant literature on this technology is scarce, especially as regards the relationship between blockchain and SCM. Additionally, existing studies have not yet addressed sufficiently the enablers of blockchain adoption and the interplay with supply chain performance. In order to reduce this gap, this study aims to examine the potential influence of blockchain on supply chain performance. We draw on the literature on technology adoption and supply chain performance, as well as on the emerging blockchain literature, to develop and test a model in two countries, namely India and the US. Accordingly, we administered a survey in order to review the opinions and views of supply chain practitioners. The results support the model and indicate that blockchain applications can improve supply chain performance. In particular, our findings suggest that knowledge sharing and trading partner pressure play an important role in blockchain adoption, and that supply chain performance is significantly influenced by supply chain transparency and blockchain transparency. Another finding was the inexistence of evidence for a moderation effect of the industry variable on the outcomes. The research conclusions have substantial managerial and theoretical implications. Our model contributes mainly to the theoretical advancement of SCM-blockchain, thus allowing scholars to adapt our validated model.}
}
@incollection{2020227,
title = {Index},
editor = {Vladlena Benson and John Mcalaney},
booktitle = {Emerging Cyber Threats and Cognitive Vulnerabilities},
publisher = {Academic Press},
pages = {227-237},
year = {2020},
isbn = {978-0-12-816203-3},
doi = {https://doi.org/10.1016/B978-0-12-816203-3.20001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128162033200014}
}
@incollection{HOVENGA2020263,
title = {Chapter 9 - Digital health ecosystems: Use of informatics, connectivity and system interoperability},
editor = {Evelyn J.S. Hovenga and Cherrie Lowe},
booktitle = {Measuring Capacity to Care Using Nursing Data},
publisher = {Academic Press},
pages = {263-307},
year = {2020},
isbn = {978-0-12-816977-3},
doi = {https://doi.org/10.1016/B978-0-12-816977-3.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128169773000095},
author = {Evelyn J.S. Hovenga and Cherrie Lowe},
keywords = {Data exchange standards, System connectivity, Measuring interoperability, Digital ecosystem, Computing platform, Big data, Data mapping, Data accuracy, Secondary data use, HL7 FHIR, Clinical models, Templates, Application Program Interchange (API), Digital resource management, Research},
abstract = {This chapter focuses on digital health as this is the enabler that informs leaders and managers at every level within any nation's healthcare industry. Data captured and used locally at every point of care needs to be made available for additional uses at all levels of decision making within the health industry. This requires an exploration of the current digital environment and an examination of how and where nursing, midwifery and health data generally fits within this ecosystem. There is a strong relationship with the technical structures of information systems, their ability to connect with each other and data use possibilities and potential benefits to be attained. This chapter details key technical features required to optimize the result of digital transformation strategies, including system connectivity, interoperability, health data exchanges and secondary data use.}
}
@article{DELICATO20201134,
title = {Editorial: Smart Cyber–Physical Systems: Toward Pervasive Intelligence systems},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {1134-1139},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19316619},
author = {Flavia C. Delicato and Adnan Al-Anbuky and Kevin I-Kai Wang},
keywords = {Cyber–Physical Systems, Pervasive systems, Computational intelligence, Smart systems, Security},
abstract = {Cyber–physical systems (CPSs) are the next generation of engineering systems, in which computing, communication and process control technologies are transparently integrated, constituting novel type of autonomous system. In these systems, embedded computers and networks monitor (through sensors) and control (through actuators) the physical processes, usually with feedback loops where physical processes and computations affect each other. By equipping physical objects with interfaces to the virtual world, and by incorporating intelligent mechanisms to leverage collaboration between these objects, the boundaries between the physical and virtual realms become blurred. Interactions occurring in the physical world can change the processing behavior in the virtual world, in a causal relationship that can be exploited for the constant improvement of processes. Intelligent, self-aware, self-managing and self-configuring pervasive systems can be built to improve quality of process in many important application domains such as transportation, energy, industrial and medical systems. The goal of this special issue is to report high-quality research on recent advances toward the realization of the Smart Cyber–Physical Systems paradigm. By presenting a selection of papers on various topics related to Smart Cyber–Physical Systems, we hope to shed light on the multiple aspects of this emerging paradigm. The papers included in this issue propose solutions for data processing and analysis, architecture, platforms and technology enablers, energy and security management, as well as its application to building smart and sustainable spaces.}
}
@article{GAN2020119928,
title = {Machine learning solutions to challenges in finance: An application to the pricing of financial products},
journal = {Technological Forecasting and Social Change},
volume = {153},
pages = {119928},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.119928},
url = {https://www.sciencedirect.com/science/article/pii/S0040162519312399},
author = {Lirong Gan and Huamao Wang and Zhaojun Yang},
keywords = {Machine learning, Finance applications, Asian options, Model-free asset pricing, Financial technology},
abstract = {The recent fast development of machine learning provides new tools to solve challenges in many areas. In finance, average options are popular financial products among corporations, institutional investors, and individual investors for risk management and investment because average options have the advantages of cheap prices and their payoffs are not very sensitive to the changes of the underlying asset prices at the maturity date, avoiding the manipulation of asset prices and option prices. The challenge is that pricing arithmetic average options requires traditional numerical methods with the drawbacks of expensive repetitive computations and non-realistic model assumptions. This paper proposes a machine-learning method to price arithmetic and geometric average options accurately and in particular quickly. The method is model-free and it is verified by empirical applications as well as numerical experiments.}
}
@article{JAWORSKA2020e5,
title = {Looking at the Big Neurochemical Picture of Autism Spectrum Disorder},
journal = {Biological Psychiatry},
volume = {87},
number = {2},
pages = {e5-e6},
year = {2020},
note = {Molecular Mechanisms of Neurodevelopmental Disorders},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2019.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0006322319317779},
author = {Natalia Jaworska}
}
@article{DUTTA2020102067,
title = {Blockchain technology in supply chain operations: Applications, challenges and research opportunities},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {142},
pages = {102067},
year = {2020},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.102067},
url = {https://www.sciencedirect.com/science/article/pii/S1366554520307183},
author = {Pankaj Dutta and Tsan-Ming Choi and Surabhi Somani and Richa Butala},
keywords = {Blockchain, Supply chain, Logistics, Integration, Applications, Literature review},
abstract = {Blockchain is a technology with unique combination of features such as decentralized structure, distributed notes and storage mechanism, consensus algorithm, smart contracting, and asymmetric encryption to ensure network security, transparency and visibility. Blockchain has immense potential to transform supply chain (SC) functions, from SC provenance, business process reengineering to security enhancement. More and more studies exploring the use of blockchain in SCs have appeared in recent years. In this paper, we consider a total of 178 articles and examine all the relevant research done in the field associated with the use of blockchain integration in SC operations. We highlight the corresponding opportunities, possible societal impacts, current state-of-the-art technologies along with major trends and challenges. We examine several industrial sectors such as shipping, manufacturing, automotive, aviation, finance, technology, energy, healthcare, agriculture and food, e-commerce, and education among others that can be successfully revamped with blockchain based technologies through enhanced visibility and business process management. A future research agenda is established which lays the solid foundation for further studies on this important emerging research area.}
}
@article{AFANTITIS2020583,
title = {NanoSolveIT Project: Driving nanoinformatics research to develop innovative and integrated tools for in silico nanosafety assessment},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {583-602},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S2001037019305112},
author = {Antreas Afantitis and Georgia Melagraki and Panagiotis Isigonis and Andreas Tsoumanis and Dimitra Danai Varsou and Eugenia Valsami-Jones and Anastasios Papadiamantis and Laura-Jayne A. Ellis and Haralambos Sarimveis and Philip Doganis and Pantelis Karatzas and Periklis Tsiros and Irene Liampa and Vladimir Lobaskin and Dario Greco and Angela Serra and Pia Anneli Sofia Kinaret and Laura Aliisa Saarimäki and Roland Grafström and Pekka Kohonen and Penny Nymark and Egon Willighagen and Tomasz Puzyn and Anna Rybinska-Fryca and Alexander Lyubartsev and Keld {Alstrup Jensen} and Jan Gerit Brandenburg and Stephen Lofts and Claus Svendsen and Samuel Harrison and Dieter Maier and Kaido Tamm and Jaak Jänes and Lauri Sikk and Maria Dusinska and Eleonora Longhin and Elise Rundén-Pran and Espen Mariussen and Naouale {El Yamani} and Wolfgang Unger and Jörg Radnik and Alexander Tropsha and Yoram Cohen and Jerzy Leszczynski and Christine {Ogilvie Hendren} and Mark Wiesner and David Winkler and Noriyuki Suzuki and Tae Hyun Yoon and Jang-Sik Choi and Natasha Sanabria and Mary Gulumian and Iseult Lynch},
keywords = {Nanoinformatics, Computational toxicology, Hazard assessment, Engineered nanomaterials, (quantitative) Structure–activity relationships, Integrated approach for testing and assessment, Safe-by-design, Machine learning, Read across, Toxicogenomics, Predictive modelling},
abstract = {Nanotechnology has enabled the discovery of a multitude of novel materials exhibiting unique physicochemical (PChem) properties compared to their bulk analogues. These properties have led to a rapidly increasing range of commercial applications; this, however, may come at a cost, if an association to long-term health and environmental risks is discovered or even just perceived. Many nanomaterials (NMs) have not yet had their potential adverse biological effects fully assessed, due to costs and time constraints associated with the experimental assessment, frequently involving animals. Here, the available NM libraries are analyzed for their suitability for integration with novel nanoinformatics approaches and for the development of NM specific Integrated Approaches to Testing and Assessment (IATA) for human and environmental risk assessment, all within the NanoSolveIT cloud-platform. These established and well-characterized NM libraries (e.g. NanoMILE, NanoSolutions, NANoREG, NanoFASE, caLIBRAte, NanoTEST and the Nanomaterial Registry (>2000 NMs)) contain physicochemical characterization data as well as data for several relevant biological endpoints, assessed in part using harmonized Organisation for Economic Co-operation and Development (OECD) methods and test guidelines. Integration of such extensive NM information sources with the latest nanoinformatics methods will allow NanoSolveIT to model the relationships between NM structure (morphology), properties and their adverse effects and to predict the effects of other NMs for which less data is available. The project specifically addresses the needs of regulatory agencies and industry to effectively and rapidly evaluate the exposure, NM hazard and risk from nanomaterials and nano-enabled products, enabling implementation of computational ‘safe-by-design’ approaches to facilitate NM commercialization.}
}
@article{WANG2020117714,
title = {Multi-criteria comprehensive study on predictive algorithm of heating energy consumption of district heating station based on timeseries processing},
journal = {Energy},
volume = {202},
pages = {117714},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.117714},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220308215},
author = {Chendong Wang and Jianjuan Yuan and Ji Zhang and Na Deng and Zhihua Zhou and Feng Gao},
keywords = {Heating energy consumption prediction, Data timeseries processing, Prediction accuracy, Prediction robustness, District heating station},
abstract = {Refined control of district heating station relies on reasonable and accurate prediction of heating energy consumption. Due to the influence of building thermal inertia and time-delay of the district heating system, certain research work is necessary to thoroughly illustrate and analyze the impact of data timeseries processing on various prediction models. Four kinds of prediction algorithms were investigated and compared in this paper. Results showed that all of three timeseries processing methods, namely time feature construction, sliding window and building thermal inertia coefficient (CBTI), can improve the prediction accuracy of all four models and CBTI had the greatest impact on model accuracy improvement. Moreover, timeseries processing method has no limitation on the types of prediction model and it is a general method to improve the model accuracy. Time feature construction and sliding window had greater influence on the non-neural network models while CBTI was the opposite. In terms of model robustness, the robustness had been significantly improved after introducing timeseries processing except random forest, and the comprehensive robustness coefficient for the other three models had been reduced by about 95%. Recurrent neural network had extremely excellent robustness under different temporal granularity.}
}
@article{YIN2020101337,
title = {Task recommendation in crowdsourcing systems: A bibliometric analysis},
journal = {Technology in Society},
volume = {63},
pages = {101337},
year = {2020},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2020.101337},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X20302049},
author = {Xicheng Yin and Hongwei Wang and Wei Wang and Kevin Zhu},
keywords = {Task recommendation, Task assignment, Crowdsourcing, Bibliometrics, Recommender system},
abstract = {Existing studies on task recommendation in crowdsourcing systems provide additional insights into the field from their perspectives, methodologies, frameworks, and disciplines, resulting in a highly productive but unorganized knowledge domain. This paper is motivated to exploit bibliometric techniques to derive insights that exceed the boundaries of individual systems and identify the potentially transformative changes from 268 published articles. The explicit features (i.e., affiliation, author, citation, and keywords) and implicit information (i.e., topic distribution, potential structure, hidden insights, and evolutionary trend) of domain literature are discovered by network analysis, cluster analysis, and timeline analysis. We summarize the generic framework based on knowledge domain structure and highlight the position of knowledge source, especially textual information, in task recommendation models. Drawing on the Shneider four-stage model, the temporal evolution trend is graphically illustrated to emphasize avenues for future research. Our study conveys accumulated and synthesized specialty knowledge to researchers or newcomers to help them design, initiate, implement, manage, and evaluate recommender systems in crowdsourcing.}
}
@article{SANICOLA2020107652,
title = {Guidelines for establishing a 3-D printing biofabrication laboratory},
journal = {Biotechnology Advances},
volume = {45},
pages = {107652},
year = {2020},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2020.107652},
url = {https://www.sciencedirect.com/science/article/pii/S0734975020301543},
author = {Henry W. Sanicola and Caleb E. Stewart and Michael Mueller and Farzad Ahmadi and Dadong Wang and Sean K. Powell and Korak Sarkar and Kenneth Cutbush and Maria A. Woodruff and David A. Brafman},
keywords = {Bioprinting, Biofabrication, Tissue engineering, Cloud manufacturing, Deep learning},
abstract = {Advanced manufacturing and 3D printing are transformative technologies currently undergoing rapid adoption in healthcare, a traditionally non-manufacturing sector. Recent development in this field, largely enabled by merging different disciplines, has led to important clinical applications from anatomical models to regenerative bioscaffolding and devices. Although much research to-date has focussed on materials, designs, processes, and products, little attention has been given to the design and requirements of facilities for enabling clinically relevant biofabrication solutions. These facilities are critical to overcoming the major hurdles to clinical translation, including solving important issues such as reproducibility, quality control, regulations, and commercialization. To improve process uniformity and ensure consistent development and production, large-scale manufacturing of engineered tissues and organs will require standardized facilities, equipment, qualification processes, automation, and information systems. This review presents current and forward-thinking guidelines to help design biofabrication laboratories engaged in engineering model and tissue constructs for therapeutic and non-therapeutic applications.}
}
@article{VIEIRA2020100544,
title = {Renewable energy and energy conservation area policy (REECAP) framework: A novel methodology for bottom-up and top-down principles integration},
journal = {Energy Strategy Reviews},
volume = {32},
pages = {100544},
year = {2020},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2020.100544},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X20300973},
author = {Abel S. Vieira and Rodney A. Stewart and Roberto Lamberts and Cara D. Beal},
keywords = {REECAP framework, Carbon emission reduction, Renewable energy, Energy conservation, Carbon-energy-cash flows},
abstract = {Climate change mitigation strategies are multifaceted and require collaboration among a range of stakeholder groups. The objective of this paper was to develop an overarching Renewable Energy and Energy Conservation Area Policy (REECAP) framework. The framework was developed based on a comprehensive literature review, in which seven principles for Renewable Energy and Energy Conservation Policies were identified. The paper also includes a case study to demonstrate an application of the REECAP framework. The novelty of the framework stems from its integration of carbon-energy-cash flows among different decision-making spheres, scales and area specific characteristics. The framework provides a mathematical understanding of how energy strategies can be transformed and optimised in a cost-effective manner by integrating stakeholders under a shared vision.}
}
@article{LIANG2020789,
title = {Super Resolution Perception for Improving Data Completeness in Smart Grid State Estimation},
journal = {Engineering},
volume = {6},
number = {7},
pages = {789-800},
year = {2020},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2020.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S2095809920301454},
author = {Gaoqi Liang and Guolong Liu and Junhua Zhao and Yanli Liu and Jinjin Gu and Guangzhong Sun and Zhaoyang Dong},
keywords = {State estimation, Low-frequency data, High-frequency data, Super resolution perception, Data completeness},
abstract = {The smart grid is an evolving critical infrastructure, which combines renewable energy and the most advanced information and communication technologies to provide more economic and secure power supply services. To cope with the intermittency of ever-increasing renewable energy and ensure the security of the smart grid, state estimation, which serves as a basic tool for understanding the true states of a smart grid, should be performed with high frequency. More complete system state data are needed to support high-frequency state estimation. The data completeness problem for smart grid state estimation is therefore studied in this paper. The problem of improving data completeness by recovering high-frequency data from low-frequency data is formulated as a super resolution perception (SRP) problem in this paper. A novel machine-learning-based SRP approach is thereafter proposed. The proposed method, namely the Super Resolution Perception Net for State Estimation (SRPNSE), consists of three steps: feature extraction, information completion, and data reconstruction. Case studies have demonstrated the effectiveness and value of the proposed SRPNSE approach in recovering high-frequency data from low-frequency data for the state estimation.}
}
@article{VIJI2020103097,
title = {Efficient fuzzy based K-nearest neighbour technique for web services classification},
journal = {Microprocessors and Microsystems},
volume = {76},
pages = {103097},
year = {2020},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103097},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120301150},
author = {C. Viji and J {Beschi Raja} and R.S. Ponmagal and S.T. Suganthi and P. Parthasarathi and Sanjeevi Pandiyan},
keywords = {Web services, Classification, Improved fuzzy with KNN, KNN classification},
abstract = {Web services playing a vital role in the World Wide Web and generates huge amount of information across various domains of internet. Due to this evolution data in the form of articles, reports, digital galleries and web data of companies were increased everyday. To handle the huge volume of data each and every day, automatic query classification based on internet is more significant method. Research and development community has developed various techniques for the web services discovery, where it offers the mandated data for the improvement method. With respect to the literature survey, most of the researchers are concentrating to provide the efficient web service discovery. The amount of data that is available in the web is keeps on increasing and also it is used to differentiate the services, explanation and work of art. In order to achieve this method, machine learning algorithm is applied extensively for domain categorization. Various machine learning algorithm like KNN is applied for web service discovery. The systems are effectively learning the input and evaluate the performance accuracy with the given datasets. This paper, proposes an improved fuzzy with KNN algorithm for effective web service classification. This is used to increase an outcome in the form of accuracy and performance measures.}
}
@article{KUSH2020103421,
title = {FAIR data sharing: The roles of common data elements and harmonization},
journal = {Journal of Biomedical Informatics},
volume = {107},
pages = {103421},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103421},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420300496},
author = {R.D. Kush and D. Warzel and M.A. Kush and A. Sherman and E.A. Navarro and R. Fitzmartin and F. Pétavy and J. Galvez and L.B. Becnel and F.L. Zhou and N. Harmon and B. Jauregui and T. Jackson and L. Hudson},
abstract = {The value of robust and responsible data sharing in clinical research and healthcare is recognized by patients, patient advocacy groups, researchers, journal editors, and the healthcare industry globally. Privacy and security concerns acknowledged, the act of exchanging data (interoperability) along with its meaning (semantic interoperability) across studies and between partners has been difficult, if not elusive. For shared data to retain its value, a recommendation has been made to follow the Findable, Accessible, Interoperable, Reusable (FAIR) principles. Without applying appropriate data exchange standards with domain-relevant content standards and accessible rich metadata that uses applicable terminologies, interoperability is burdened by the need for transformation and/or mapping. These obstacles to interoperability limit the findability, accessibility and reusability of data, thus diminishing its value and making it impossible to adhere to FAIR principles. One effort to standardize data collection has been through common data elements (CDEs). CDEs are data collection units comprising one or more questions together with a set of valid values. Some CDEs contain standardized terminology concepts that define the meaning of the data, and others include links to unique terminology concept identifiers and unique identifiers for each CDE; however, usually CDEs are defined for specific projects or collaborations and lack traceable or machine readable semantics. While the name implies that these are ‘common’, this has not necessarily been a requirement, and many CDEs have not been commonly used. The National Institutes of Health (NIH) CDEs are, in fact, a conglomerate of CDEs developed in silos by various NIH institutes. Therefore, CDEs have not brought the anticipated benefit to the industry through widescale interoperability, nor is there widespread reuse of CDEs. Certain institutes in the NIH recommend, albeit do not enforce, institute-specific preferred CDEs; however, at the NIH level a preponderance of choice and a lack of any overarching harmonization of CDEs or consistency in linking them to controlled terminology or common identifiers create confusion for researchers in their efforts to identify the best CDEs for their protocol. The problem of comparing data among studies is exacerbated when researchers select different CDEs for the same variable or data collection field. This manuscript explores reasons for the disappointingly low adoption of CDEs and the inability of CDEs or other clinical research standards to broadly solve the interoperability and data sharing problems. Recommendations are offered for rectifying this situation to enable responsible data sharing that will help in adherence to FAIR principles and the realization of Learning Health Systems for the sake of all of us as patients.}
}
@article{MOROZOV2020107504,
title = {Data-driven model for hydraulic fracturing design optimization: focus on building digital database and production forecast},
journal = {Journal of Petroleum Science and Engineering},
volume = {194},
pages = {107504},
year = {2020},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.107504},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520305751},
author = {Anton D. Morozov and Dmitry O. Popkov and Victor M. Duplyakov and Renata F. Mutalova and Andrei A. Osiptsov and Albert L. Vainshtein and Evgeny V. Burnaev and Egor V. Shel and Grigory V. Paderin},
keywords = {Fracture, Machine learning, Predictive modeling, Data collection, Design optimization, Hydraulic fracturing, Production forecast, Gradient boosting, Decision tree, Data analytics, Data preprocessing, Feature analysis},
abstract = {Growing amount of hydraulic fracturing (HF) jobs in the recent two decades resulted in a significant amount of measured data available for development of predictive models via machine learning (ML). In multistage fractured completions, post-fracturing production analysis (e.g., from production logging tools) reveals evidence that different stages produce very non-uniformly, and up to 30% may not be producing at all due to a combination of geomechanics and fracturing design factors. Hence, there is a significant room for improvement of current design practices. We propose a data-driven model for fracturing design optimization, where the workflow is essentially split into two stages. As a result of the first stage, the present paper summarizes the efforts in the creation of a digital database of field data from several thousands of multistage HF jobs on vertical, inclined and near-horizontal wells from circa 20 different oilfields in Western Siberia, Russia. In terms of the number of points (fracturing jobs), the present database is a rare case of a representative dataset of about 5000 data points, compared to typical databases available in the literature, comprising tens or hundreds of points at best. Each point in the data base contains the vector of 92 input variables (the reservoir, well and the frac design parameters) and the vector of production data, which is characterized by 16 parameters, including the target, cumulative oil production. The focus is made on data gathering from various sources, data preprocessing and development of the architecture of the database as well as solving the production forecast problem via ML. Data preparation has been done using various ML techniques: the problem of missing values in the database is solved with collaborative filtering for data imputation; outliers are removed using visualization of cluster data structure by t-SNE algorithm. The production forecast problem is solved via CatBoost algorithm. Prediction capability of the model is measured with the coefficient of determination (R2) and reached 0.815. The inverse problem (selecting an optimum set of fracturing design parameters to maximize production) will be considered in the second part of the study to be published in another paper, along with a recommendation system for advising DESC and production stimulation engineers on an optimized fracturing design.}
}
@article{MATHUPRIYA2020,
title = {Digital twin technology on IoT, industries & other smart environments: A survey},
journal = {Materials Today: Proceedings},
year = {2020},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.11.358},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320389768},
author = {S. Mathupriya and S. {Saira Banu} and S. Sridhar and B. Arthi},
keywords = {Digital twinning, Artificial Intelligence, Internet of Things, Industrial applications, Health care},
abstract = {Digital twinning (DT) technology is a rising technology that gains various investigators' insights in recent days in the industrial and educational field. Improvements in technical concepts have increased growth, specifically in manufacturing fields. It is depicted as the combination of data among virtual machines and physical machines in both directions. There are diverse challenges, applications, and technology integrations like the Internet of Things (IoT), Artificial Intelligence (AI) with digital twinning. An extensive review of the digital twinning concept is examined and analyzed in this work. This survey focussed on the research areas like health care, industrial, smart cities, weather-based applications. It reflects the current perception ofthe research field. This work facilitates an examination of enabling twinning technologies, open challenges, and applications of Digital Twinning.}
}
@article{QIANG2020102115,
title = {Observing community resilience from space: Using nighttime lights to model economic disturbance and recovery pattern in natural disaster},
journal = {Sustainable Cities and Society},
volume = {57},
pages = {102115},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102115},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720301025},
author = {Yi Qiang and Qingxu Huang and Jinwen Xu},
keywords = {Natural disaster, Resilience, Economic recovery, Nighttime lights, Empirical assessment, DMSP/OLS},
abstract = {A major challenge for measuring community resilience is the lack of empirical observations in disasters. As an effective tool to observe human activities on the earth surface, night-time light (NTL) remote sensing images can fill the gap of empirical data for measuring community resilience in natural disasters. This study introduces a quantitative framework to model recovery patterns of economic activity in a natural disaster using the Defense Meteorological Satellite Program-Operational Linescan System (DMSP-OLS) images. The utility of the framework is demonstrated in a retrospective study of Hurricane Katrina, which uncovered the great economic impact of Katrina and spatial variation of the disturbance and recovery pattern of economic activity. Environmental and socio-economic factors that potentially influence economic recovery were explored in statistical analyses. Instead of a static and holistic index, the framework measures resilience as a dynamic process. The analysis results provide actionable information for prompting resilience in diverse communities and in different phases of a disaster. In addition to Hurricane Katrina, the resilience modeling framework is applicable for other disaster types. The introduced approaches and findings increase our understanding about the complexity of community resilience and provide support for developing resilient and sustainable communities.}
}
@article{HARVEY2020102418,
title = {The importance of long-term ecological time series for integrated ecosystem assessment and ecosystem-based management},
journal = {Progress in Oceanography},
volume = {188},
pages = {102418},
year = {2020},
issn = {0079-6611},
doi = {https://doi.org/10.1016/j.pocean.2020.102418},
url = {https://www.sciencedirect.com/science/article/pii/S0079661120301579},
author = {Chris J. Harvey and Jennifer L. Fisher and Jameal F. Samhouri and Gregory D. Williams and Tessa B. Francis and Kym C. Jacobson and Yvonne L. deReynier and Mary E. Hunsicker and Newell Garfield},
keywords = {Ecosystem-based fisheries management, Ecosystem indicators, California Current large marine ecosystem, Copepods, Salmon, Food web ecology, Climate, Long-term monitoring},
abstract = {Long-term mechanistic research and monitoring provides integral science support for ecosystem-based management (EBM) of resources, activities and services. Decades of oceanographic and ecological research by Bill Peterson and colleagues along the Newport Hydrographic Line (NH Line) provides essential context for understanding and managing Pacific salmon Oncorhynchus spp. and other marine resources in the California Current ecosystem. This research program helped federal scientists convey the significance of the northeast Pacific marine heatwave (2013–2016) to fisheries managers and stakeholders. Particularly illustrative were shifts in the composition of the copepod community, which reflected feeding conditions for Pacific salmon and other consumers. We identify six traits of the dataset produced by Peterson and colleagues that have made it especially valuable for informing management: (i) it has generated robust ecosystem indicators; (ii) it is long-term; (iii) it is associated with meaningful ecological mechanisms; (iv) it relates to ecosystem components of high societal value; (v) it can represent processes and lags at meaningful temporal scales; and (vi) it is part of a broader, integrative science effort. This research effort underscores the importance of developing and sustaining long-term mechanistic research and monitoring along the U.S. West Coast and elsewhere in the world.}
}
@article{LU20202002,
title = {General Anesthesia Leads to Increased Adverse Events Compared With Spinal Anesthesia in Patients Undergoing Unicompartmental Knee Arthroplasty},
journal = {The Journal of Arthroplasty},
volume = {35},
number = {8},
pages = {2002-2008},
year = {2020},
issn = {0883-5403},
doi = {https://doi.org/10.1016/j.arth.2020.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0883540320302412},
author = {Yining Lu and William M. Cregar and J. Brett Goodloe and Zain Khazi and Brian Forsythe and Tad L. Gerlinger},
keywords = {unicompartmental arthroplasty, general anesthesia, spinal anesthesia, complications, National Surgical Quality Improvement Project, NSQIP},
abstract = {Background
The volume of unicompartmental knee arthroplasty (UKA) has increased dramatically in recent years with good reported long-term outcomes. UKA can be performed under general or neuraxial (ie, spinal) anesthesia; however, little is known as to whether there is a difference in outcomes based on anesthesia type. The purpose of the present study is to compare perioperative outcomes between anesthesia types for patients undergoing primary elective UKA.
Methods
Patients who underwent primary elective UKA from 2007 to 2017 were identified from the American College of Surgeons-National Surgical Quality Improvement Program Database. Operating room times, length of stay (LOS), 30-day adverse events, and readmission rates were compared between patients who received general anesthesia and those who received spinal anesthesia. Propensity-adjusted multivariate analysis was used to control for selection bias and baseline patient characteristics.
Results
A total of 8639 patients underwent UKA and met the inclusion criteria for this study. Of these, 4728 patients (54.7%) received general anesthesia and 3911 patients (45.3%) received spinal anesthesia. On propensity-adjusted multivariate analyses, general anesthesia was associated with increased operative time (P < .001) and the occurrence of any severe adverse event (odds ratio [OR], 1.39; 95% confidence interval [95% CI], 1.04-1.84; P = .024). In addition, general anesthesia was associated with higher rates of deep venous thrombosis (OR, 2.26; 95% CI, 1.11-4.6; P = .024) and superficial surgical site infection (OR, 1.04; 95% CI, 0.6-1.81; P < .001). Finally, general anesthesia was also associated with a reduced likelihood of discharge to home (OR, 0.72; 95% CI, 0.59-0.88; P < .001). No difference existed in postoperative hospital LOS or readmission rates among cohorts.
Conclusion
General anesthesia was associated with an increased rate of adverse events and increased operating room times as well as a reduced likelihood of discharge to home. There was no difference in hospital LOS or postoperative readmission rates between anesthesia types.}
}
@article{DING2020481,
title = {CO-STAR: A collaborative prediction service for short-term trends on continuous spatio-temporal data},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {481-493},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18332369},
author = {Weilong Ding and Xuefei Wang and Zhuofeng Zhao},
keywords = {Data stream, Sensory data, Spatio-temporal data, Short-term trends, Trend prediction, Collaborative computing paradigm, Internet of Things, Non-parametric regression},
abstract = {Over various sensory data of Internet of Things, not only the current situation but also the future trends of many fields are required instantly to promote the business. As a typical requirement, the short-term prediction on spatio-temporal data stream is imperative, but challenges still remain due to the inherent limitation of long calculative time and insufficient predictive precision. In this paper, a novel prediction service CO-STAR is proposed in the highway domain. On the continuous toll data of the whole highway network, the service employs non-parametric regression model to predict the traffic volume of all the stations periodically. Considering both spatial and temporal business characteristics, a collaborative paradigm of online stream computing and offline batch processing is adopted to balance the efficiency and precision. On the real data of one Chinese provincial highway and the simulated data, our service can hold minute-level executive latency with nearly 10 percent improvement for the predictive precision in extensive experiments.}
}
@article{NGUYEN2020395,
title = {Reviewing trip purpose imputation in GPS-based travel surveys},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {7},
number = {4},
pages = {395-412},
year = {2020},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2020.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095756420301033},
author = {Minh Hieu Nguyen and Jimmy Armoogum and Jean-Loup Madre and Cédric Garcia},
keywords = {Transportation engineering, Travel survey, GPS, Purpose imputation, Human geography, Activity inference},
abstract = {The global positioning system (GPS) has motivated rapid advances in mobility data collection. A massive amount of spatio-temporal information has made it possible to know where a person was and when, but not how and why (s)he travelled, creating the need for inference models. Compared with mode detection, purpose imputation has been insufficiently studied. However, the relative lack of attention to purpose identification does not mean that this field has not emerged. For this paper, which is the first review dedicated to inferring trip purposes from GPS data, 1162 non-duplicate papers from four databases (Scopus, Web of Science, ScienceDirect and TRID) were screened, and a corpus of 25 publications was selected for examination. Based on these papers, the purpose imputation problem is defined in the contexts of the evolution of GPS-based travel surveys and two research domains, transportation science (TS) and human geography (HG). Subsequently, three steps of the purpose detection process, namely trip end detection, input feature selection and main algorithms and validation, are surveyed. During these procedures, the differences between studies in TS and those in HG are highlighted. Finally, unresolved issues related to data and feature selection, algorithms and assessment are discussed substantially to provide potential research directions. This review may be an informative reference for those newly accessing the GPS-based purpose imputation field and/or intending to develop solutions to this problem.}
}
@article{FOUHAD2020427,
title = {Electric Power Production Modeling for Optimal Driving},
journal = {Procedia Computer Science},
volume = {175},
pages = {427-434},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.060},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920317415},
author = {Chaimaa Fouhad and Mohamed {El Khaili} and Mohammed Qbadou},
keywords = {Electrical energy, IIot, Machine Learning, Modelization, Data Mining},
abstract = {Electric power production operators have adopted a new strategy to digitize its industrial processes. This is achieved by integrating connected sensors into equipment to collect data and enable real-time process monitoring, which ensures effective remote control and driving. In this context, our project is to optimize the operating parameters of the steam management and distribution process of Medium Pressure and Low Pressure at the level of a real thermoelectric plant, in order to maximize the production of electrical energy. After collection, we clean, filter and consolidate the data in such a way as to have a database containing all the necessary variables of the process. We have built a predictive model that enables the production of electrical power using a machine learning approach. This model, will be exploited for the development of a decision-making application.}
}
@article{HEMBAGEEKIYANAGE2020106834,
title = {Architectures and algorithms of an autonomous small-scale drilling agent},
journal = {Journal of Petroleum Science and Engineering},
volume = {188},
pages = {106834},
year = {2020},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2019.106834},
url = {https://www.sciencedirect.com/science/article/pii/S0920410519312537},
author = {Suranga Chaminda {Hemba Geekiyanage} and Erik A. Loeken and Dan Sui},
keywords = {Drilling systems automation, Autonomous agent, Applied artificial intelligence, ROP optimization, Laboratory rig},
abstract = {This paper describes the core architectures and algorithms of an autonomous small-scale drilling agent. The agent operates in a laboratory rig, demonstrating drilling scenarios with limited or even no human intervention. The work illustrates its performance through self-coordinating state transition, Rate of Penetration (ROP, drilling speed) optimization capability, formation classification, and drilling incidents management. The agent is an original rule-based system, and its control architecture utilizes finite states automation. The novel ROP optimization strategy employs a gradient search in Weight on Bit (WOB)-rotational speed (RPM, Revolutions per Minute) control parameter space. It generates an increasing ROP trend with time and requires re-iteration at abrupt formation changes. Several drilling incidents are managed using ‘if-then’ logic-based activity decomposition. A key learning outcome from the study is the comprehension of the requirement of standard software architecture and Applications Programming Interfaces (API) for continuous research and development of the agent. Such interfaces enhance interoperability between systems and stimulate innovative thinking among independent developers to produce a better-faster set of algorithms. Laboratory testing and evaluation is an essential part of promoting the adaptation of digital technologies for drilling automation. Such studies are a useful, safe, and cost-effective solution for testing, integrating and improving hardware, software, and data management before expensive full-scale testing and integration.}
}
@article{NETO2020210,
title = {Digital twins in manufacturing: an assessment of drivers, enablers and barriers to implementation},
journal = {Procedia CIRP},
volume = {93},
pages = {210-215},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.131},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307733},
author = {Anis Assad Neto and Fernando Deschamps and Elias Ribeiro {da Silva} and Edson Pinheiro {de Lima}},
keywords = {Industry 4.0, digital twin, smart factory, manufacturing},
abstract = {As we live through the fourth industrial revolution, cutting-edge technologies look to change the way manufacturing systems operate. In this context, an important technological framework gaining popularity is the digital twin, which enables a virtual mirror of a real subject, used in manufacturing to assess performance and predict behavior. In this study, we interview experts and review the literature to gain an overview of what exactly drives companies to look for digital twin solutions in the manufacturing environment, what factors enable these initiatives to be successful, and what are the barriers that compromise or slow down implementation efforts.}
}
@article{SCHENKELBERG202010651,
title = {Supervised Machine Learning for Knowledge-Based Analysis of Maintenance Impact on Profitability},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {10651-10657},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2830},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320335953},
author = {Kai Schenkelberg and Ulrich Seidenberg and Fazel Ansari},
keywords = {Maintenance, Profitability, Supervised learning, Machine learning, Regression, Knowledge-Based Maintenance},
abstract = {Recent empirical studies reveal that predictive maintenance is essential for accomplishing business objectives of manufacturing enterprises. Knowledge-based maintenance strategies for optimal operation of industrial machines and physical assets reasonably require explaining and predicting long term economic impacts, based on exploring historical data. This paper examines how supervised machine learning (ML) techniques may enhance anticipating the economic impact of maintenance on profitability (IMP). Planning and monitoring of maintenance activities supported by various statistical learning and supervised ML algorithms have been investigated in the literature of production management. However, data-driven prediction of IMP has not been largely addressed. A novel data-driven framework is proposed comprising cause-and-effect dependencies between maintenance and profitability, which constructs a set of appropriate features as independent variables.}
}
@article{KREUZER2020103391,
title = {Introduction to the special issue on spatial modelling and analysis of ore-forming processes in mineral exploration targeting},
journal = {Ore Geology Reviews},
volume = {119},
pages = {103391},
year = {2020},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2020.103391},
url = {https://www.sciencedirect.com/science/article/pii/S0169136820300408},
author = {Oliver P. Kreuzer and Mahyar Yousefi and Vesa Nykänen},
keywords = {Geographic information systems (GIS), Spatial modelling, Ore-forming processes, Exploration targeting, Mineral systems},
abstract = {Ore deposits are diverse with much of their diversity attributable to the complex interplay of ore-forming processes with a variety of geological environments, over a range of scales and both in space and time. This diversity makes it difficult for geoscientists to accurately predict the location of undiscovered ore deposits. Improving our understanding of the processes that are critical to ore deposit formation would help us to hone our predictive capabilities. However, this task is difficult to achieve as we cannot observe these genetic processes first-hand and different parameters and ingredients are important at different scales. Modelling offers a means of simulating and analysing ore-forming processes and their mappable expressions. This knowledge can then be used to build a predictive model by translating key process components into spatial proxies that can be mapped or recognized in mineral exploration data. Modelling and analysis of ore-forming processes are therefore critical for the future success of mineral exploration. Currently underutilized in exploration targeting, the application of statistical and mathematical concepts can help steer geoscientists towards a better understanding of the complex geological processes critical in the formation of mineral deposits and, ultimately, improved exploration success rates. This editorial article presents a brief introduction to the main concepts that support a collection of articles published in a virtual special issue (VSI) of Ore Geology Reviews entitled “Spatial modelling and analysis of ore-forming processes in mineral exploration targeting”. The articles examine three critical themes: (1) Translating the expressions of ore-forming processes and critical parameters of mineral systems into mappable spatial proxies; (2) identifying mineral deposit footprints through geochemical and geophysical data analysis; and (3) targeting and improving the discovery chance of mineral deposits by way of spatial data analysis.}
}
@article{OTTESTAD2020104347,
title = {The relevance of tumor mutation profiling in interpretation of NGS data from cell-free DNA in non-small cell lung cancer patients},
journal = {Experimental and Molecular Pathology},
volume = {112},
pages = {104347},
year = {2020},
issn = {0014-4800},
doi = {https://doi.org/10.1016/j.yexmp.2019.104347},
url = {https://www.sciencedirect.com/science/article/pii/S0014480019306094},
author = {Anine Larsen Ottestad and Sissel G.F. Wahl and Bjørn Henning Grønberg and Frank Skorpen and Hong Yan Dai},
keywords = {Non-small cell lung cancer (NSCLC), Next-generation sequencing (NGS), Circulating tumor DNA (ctDNA)},
abstract = {Studies have indicated that detection of circulating tumor DNA (ctDNA) prior to treatment is a negative prognostic marker in non-small cell lung cancer (NSCLC). ctDNA is currently identified by detection of tumor mutations. Commercial next-generation sequencing (NGS) assays for mutation analysis of ctDNA for routine practice usually include small gene panels and are not suitable for general mutation analysis. In this study, we investigated whether mutation analysis of cfDNA could be performed using a commercially available comprehensive NGS gene panel and bioinformatics workflow. Tumor DNA, plasma DNA and peripheral blood leukocyte DNA from 30 NSCLC patients were sequenced. In two patients (7%), tumor mutations in cfDNA were immediately called by the bioinformatic workflow. In 13 patients (43%), tumor mutations were not called, but were present in ctDNA and were identified based on the known tumor mutation profile. In the remaining 15 patients (50%), no concordant mutations were detected. In conclusion, we were able to identify tumor mutations in ctDNA from 57% of NSCLC patients using a comprehensive gene panel. We demonstrated that sequencing paired tumor DNA was helpful to interpret data and confirm ctDNA, and thus increased the ratio of patients with detectable ctDNA. This approach might be feasible for mutation analysis of ctDNA in routine diagnostic practice, especially in case of suboptimal plasma quality and quantity.}
}
@article{BARANDAS2020100456,
title = {TSFEL: Time Series Feature Extraction Library},
journal = {SoftwareX},
volume = {11},
pages = {100456},
year = {2020},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2020.100456},
url = {https://www.sciencedirect.com/science/article/pii/S2352711020300017},
author = {Marília Barandas and Duarte Folgado and Letícia Fernandes and Sara Santos and Mariana Abreu and Patrícia Bota and Hui Liu and Tanja Schultz and Hugo Gamboa},
keywords = {Time series, Machine learning, Feature extraction, Python},
abstract = {Time series feature extraction is one of the preliminary steps of conventional machine learning pipelines. Quite often, this process ends being a time consuming and complex task as data scientists must consider a combination between a multitude of domain knowledge factors and coding implementation. We present in this paper a Python package entitled Time Series Feature Extraction Library (TSFEL), which computes over 60 different features extracted across temporal, statistical and spectral domains. User customisation is achieved using either an online interface or a conventional Python package for more flexibility and integration into real deployment scenarios. TSFEL is designed to support the process of fast exploratory data analysis and feature extraction on time series with computational cost evaluation.}
}
@article{SCHMITT2020101101,
title = {Predictive model-based quality inspection using Machine Learning and Edge Cloud Computing},
journal = {Advanced Engineering Informatics},
volume = {45},
pages = {101101},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101101},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300707},
author = {Jacqueline Schmitt and Jochen Bönig and Thorbjörn Borggräfe and Gunter Beitinger and Jochen Deuse},
keywords = {Edge Cloud Computing, Machine Learning, Quality inspection, Quality prediction, Manufacturing},
abstract = {The supply of defect-free, high-quality products is an important success factor for the long-term competitiveness of manufacturing companies. Despite the increasing challenges of rising product variety and complexity and the necessity of economic manufacturing, a comprehensive and reliable quality inspection is often indispensable. In consequence, high inspection volumes turn inspection processes into manufacturing bottlenecks. In this contribution, we investigate a new integrated solution of predictive model-based quality inspection in industrial manufacturing by utilizing Machine Learning techniques and Edge Cloud Computing technology. In contrast to state-of-the-art contributions, we propose a holistic approach comprising the target-oriented data acquisition and processing, modelling and model deployment as well as the technological implementation in the existing IT plant infrastructure. A real industrial use case in SMT manufacturing is presented to underline the procedure and benefits of the proposed method. The results show that by employing the proposed method, inspection volumes can be reduced significantly and thus economic advantages can be generated.}
}
@article{LEI2020106587,
title = {Applications of machine learning to machine fault diagnosis: A review and roadmap},
journal = {Mechanical Systems and Signal Processing},
volume = {138},
pages = {106587},
year = {2020},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2019.106587},
url = {https://www.sciencedirect.com/science/article/pii/S0888327019308088},
author = {Yaguo Lei and Bin Yang and Xinwei Jiang and Feng Jia and Naipeng Li and Asoke K. Nandi},
keywords = {Machines, Intelligent fault diagnosis, Machine learning, Deep learning, Transfer learning, Review and roadmap},
abstract = {Intelligent fault diagnosis (IFD) refers to applications of machine learning theories to machine fault diagnosis. This is a promising way to release the contribution from human labor and automatically recognize the health states of machines, thus it has attracted much attention in the last two or three decades. Although IFD has achieved a considerable number of successes, a review still leaves a blank space to systematically cover the development of IFD from the cradle to the bloom, and rarely provides potential guidelines for the future development. To bridge the gap, this article presents a review and roadmap to systematically cover the development of IFD following the progress of machine learning theories and offer a future perspective. In the past, traditional machine learning theories began to weak the contribution of human labor and brought the era of artificial intelligence to machine fault diagnosis. Over the recent years, the advent of deep learning theories has reformed IFD in further releasing the artificial assistance since the 2010s, which encourages to construct an end-to-end diagnosis procedure. It means to directly bridge the relationship between the increasingly-grown monitoring data and the health states of machines. In the future, transfer learning theories attempt to use the diagnosis knowledge from one or multiple diagnosis tasks to other related ones, which prospectively overcomes the obstacles in applications of IFD to engineering scenarios. Finally, the roadmap of IFD is pictured to show potential research trends when combined with the challenges in this field.}
}
@article{ISLAM2020101863,
title = {Improving disasters preparedness and response for coastal communities using AIS ship tracking data},
journal = {International Journal of Disaster Risk Reduction},
volume = {51},
pages = {101863},
year = {2020},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2020.101863},
url = {https://www.sciencedirect.com/science/article/pii/S2212420920313650},
author = {Samsul Islam and Floris Goerlandt and Xuran Feng and Mohammad Jasim Uddin and Yangyan Shi and Casey Hilliard},
keywords = {Emergency management, Disaster relief operations, Maritime risk, Humanitarian logistics, Maritime transportation, AIS data, Ship tracking data, Disaster management, Machine learning, Coastal community, Neural network},
abstract = {Many coastal communities are heavily dependent on maritime transportation for the ingress and egress of people and goods. Any major transportation disruption can have a significant negative impact on the safety, health and wellbeing of affected communities, this is due to the interruption in the availability of food and the supply medicines and fuel. Therefore, preparedness and the forward planning of an effective response are essential for successful emergency and recovery management. Accordingly, in this study, the concept of using AIS (Automatic Identification System) vessel tracking data has been applied for the study of disaster management in coastal communities. The AIS vessel tracking system has been an important development in navigational safety; this is because it continuously transmits important information to all other vessels about a particular vessel (including its position, identity, speed and route). One of the limitations of the AIS tracking system is that AIS data does not indicate commodity specifications: that is, the quantity of essential goods that each vessel is carrying to specified coastal communities. To overcome the limitation of the current AIS tracking system, we use an artificial neural network as an estimation tool. In the current study, AIS data are assessed and analyzed in addition to the augmentation of the capacity information of vessels; thus, the study develops a predictive model so that a relief manager can determine the actual needs of affected residents and thus be able to make responsible relief decisions (e.g., how much relief a disaster-affected community is likely to need). The study makes a unique contribution as its focus seeks to remedy the total lack of research on how to use AIS data in disaster operations.}
}
@article{NGUYEN202071,
title = {Dynamic responses of tourist arrivals in Australia to currency fluctuations},
journal = {Journal of Hospitality and Tourism Management},
volume = {45},
pages = {71-78},
year = {2020},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2020.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S144767702030173X},
author = {Jeremy Nguyen and Abbas Valadkhani},
keywords = {MIDAS, Tourist arrivals, Australia, The exchange rate, Economic policy uncertainty},
abstract = {This study examines the dynamic responses of monthly Australian inbound tourist arrivals to higher frequency daily movements in the exchange rate. Many existing models of inbound tourism utilize currency values averaged over months or quarters to match available visitor data, thus discarding valuable dynamic information from higher frequency variations in the exchange rate. We employ a Mixed Data Sampling (MIDAS) approach, modelling monthly Australian inbound tourism from 1998 to 2018, while utilising daily observations of the domestic currency as an explanatory variable. The findings suggest that models that take averages of exchange rate data to match frequencies may substantially underestimate how sensitive tourists are in responding to short-term fluctuations in the exchange rate. Indeed, the overwhelming majority of such responses typically are observed within 15–20 days prior to travel. After considering time horizons of up to 90 days, our findings indicate that travelers are predominantly concerned with exchange rate variations again only within three weeks prior to their departures. The results suggest that the Mixed Data Sampling (MIDAS) methodology can be a useful supplement to traditional approaches to modelling tourism demand. MIDAS models can provide rich insights into the duration and patterns of the dynamic interplay between the variables of interest.}
}
@article{VILLALOBOS2020103257,
title = {A three level hierarchical architecture for an efficient storage of industry 4.0 data},
journal = {Computers in Industry},
volume = {121},
pages = {103257},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103257},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520304917},
author = {K. Villalobos and V.J. Ramírez-Durán and B. Diez and J.M. Blanco and A. Goñi and A. Illarramendi},
keywords = {Industrial internet of things, Industry 4.0, Smart manufacturing, Time series storage},
abstract = {The increasing interest among manufacturers in monitoring and analyzing industrial systems is generating a problem related to the considerable costs associated with the storage of the captured data. This paper presents a three level hierarchical architecture for Industry 4.0 time-series data storage on cloud environments that helps to manage and reduce those costs. In the first level, new raw time series can be stored for a short-period of time on electronic non-volatile storage such as Solid-State Drives (SSDs). In the second level, recent time series can be stored for a medium-period of time on magnetic Hard Disk Drives (HDDs). In the third level, a reduced representation of the time series obtained by applying time series reduction techniques can also be stored in HDDs for a longer period of time. The main contribution of this paper is related to the third level of the architecture, that allows to decrease the required data storage resources by storing reduced representations of time-series data, without almost hampering the use of those data for further analysis purposes. The proposed architecture, has been implemented by using some of the top Database Management Systems (DBMSs) from four different categories: Time series DBMS, Wide column stores, Document stores and Graph DBMS. It has been tested by using industrial time series coming from a real manufacturing environment, and with four different types of queries proposed by domain experts. The performance results regarding storage space, storage costs and total query time for each DBMS are shown and contrasted in this paper.}
}
@article{XU2020402,
title = {Predicting post-discharge self-harm incidents using disease comorbidity networks: A retrospective machine learning study},
journal = {Journal of Affective Disorders},
volume = {277},
pages = {402-409},
year = {2020},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.08.044},
url = {https://www.sciencedirect.com/science/article/pii/S0165032720326501},
author = {Zhongzhi Xu and Qingpeng Zhang and Paul Siu Fai Yip},
keywords = {Self-harm prevention, Self-harm prediction, Deep learning, Network embedding, Comorbidity networks},
abstract = {Background
Self-harm is preventable if the risk can be identified early. The co-occurrence of multiple diseases is related to self-harm risk. This study develops a comorbidity network-based deep learning framework to improve the prediction of individual self-harm.
Methods
Between 01/01/2007-12/31/2010, we obtained 2,323 patients with self-harm records and 46,460 randomly sampled controls from 1,764,094 inpatients across 44 public hospitals in Hong Kong. 80% of the samples were randomly selected for model training, and the remaining 20% were set aside for model testing. We propose a novel patient embedding method, namely Dx2Vec (Diagnoses to Vector), based on the comorbidity network constructed by all historical diagnoses. Dx2Vec represents the comorbidity patterns among diseases and temporal patterns of historical admissions for each patient.
Results
Experiments demonstrate that the Dx2Vec-based model outperforms the baseline deep learning model in identifying patients who would self-harm within 12 months (C-statistic: 0.89). The precision is 0.54 for positive cases and 0.98 for negative cases, whilst the recall is 0.72 for positive cases and 0.96 for negative cases. The model extracted the most predictive diagnoses, and pairwise comorbid diagnoses to help medical professionals identify patients with risk.
Limitations
The inpatient data does not contain lab test information.
Conclusions
Incorporation of a disease comorbidity network can significantly improve self-harm prediction performance, indicating that it is critical to consider comorbidity patterns in self-harm screening and prevention programs. The findings have the potential to be translated into effective self-harm screening systems.}
}
@incollection{PEZOULAS2020105,
title = {Chapter 4 - Data protection},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {105-136},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000049},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Data protection, Federal Trade Commission Act, General Data Protection Regulation, Health Insurance Portability and Accountability Act, Patient privacy},
abstract = {This chapter offers the legal basis for understanding the impact of data protection legislation on data sharing. Toward this direction, significant challenges are presented including legal and ethical barriers, patient privacy issues, technical limitations, and other factors that pose barriers during data sharing. Well-known data protection regulations that deal with such challenges, such as the General Data Protection Regulation (EU) 2016/679 that superseded the Directive 95/46/EC in Europe, as well as the Federal Trade Commission Act and Health Insurance Portability and Accountability Act in the United States, are extensively described in terms of their legal basis. Promising initiatives that envisage to overcome the legal and ethical obstacles that are present during cross-border data sharing are finally discussed toward the establishment of a global data protection framework for different areas of life, especially in healthcare.}
}
@article{MENG2020115,
title = {A survey on machine learning for data fusion},
journal = {Information Fusion},
volume = {57},
pages = {115-129},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519303902},
author = {Tong Meng and Xuyang Jing and Zheng Yan and Witold Pedrycz},
keywords = {Data fusion, Machine learning, Fusion methods, Fusion criteria},
abstract = {Data fusion is a prevalent way to deal with imperfect raw data for capturing reliable, valuable and accurate information. Comparing with a range of classical probabilistic data fusion techniques, machine learning method that automatically learns from past experiences without explicitly programming, remarkably renovates fusion techniques by offering the strong ability of computing and predicting. Nevertheless, the literature still lacks a thorough review of the recent advances of machine learning for data fusion. Therefore, it is beneficial to review and summarize the state of the art in order to gain a deep insight on how machine learning can benefit and optimize data fusion. In this paper, we provide a comprehensive survey on data fusion methods based on machine learning. We first offer a detailed introduction to the background of data fusion and machine learning in terms of definitions, applications, architectures, processes, and typical techniques. Then, we propose a number of requirements and employ them as criteria to review and evaluate the performance of existing fusion methods based on machine learning. Through the literature review, analysis and comparison, we finally come up with a number of open issues and propose future research directions in this field.}
}
@article{BAASHAR2020103442,
title = {Customer relationship management systems (CRMS) in the healthcare environment: A systematic literature review},
journal = {Computer Standards & Interfaces},
volume = {71},
pages = {103442},
year = {2020},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2020.103442},
url = {https://www.sciencedirect.com/science/article/pii/S0920548919304593},
author = {Yahia Baashar and Hitham Alhussian and Ahmed Patel and Gamal Alkawsi and Ahmed Ibrahim Alzahrani and Osama Alfarraj and Gasim Hayder},
keywords = {Customer relationship management (CRM), Patient relationship management (PRM), Healthcare environment, Healthcare industry, Information and communication technologies (ICT)},
abstract = {Customer relationship management (CRM) is an innovative technology that seeks to improve customer satisfaction, loyalty, and profitability by acquiring, developing, and maintaining effective customer relationships and interactions with stakeholders. Numerous researches on CRM have made significant progress in several areas such as telecommunications, banking, and manufacturing, but research specific to the healthcare environment is very limited. This systematic review aims to categorise, summarise, synthesise, and appraise the research on CRM in the healthcare environment, considering the absence of coherent and comprehensive scholarship of disparate data on CRM. Various databases were used to conduct a comprehensive search of studies that examine CRM in the healthcare environment (including hospitals, clinics, medical centres, and nursing homes). Analysis and evaluation of 19 carefully selected studies revealed three main research categories: (i) social CRM ‘eCRM’; (ii) implementing CRMS; and (iii) adopting CRMS; with positive outcomes for CRM both in terms of patients relationship/communication with hospital, satisfaction, medical treatment/outcomes and empowerment and hospitals medical operation, productivity, cost, performance, efficiency and service quality. This is the first systematic review to comprehensively synthesise and summarise empirical evidence from disparate CRM research data (quantitative, qualitative, and mixed) in the healthcare environment. Our results revealed that substantial gaps exist in the knowledge of using CRM in the healthcare environment. Future research should focus on exploring: (i) other potential factors, such as patient characteristics, culture (of both the patient and hospital), knowledge management, trust, security, and privacy for implementing and adopting CRMS and (ii) other CRM categories, such as mobile CRM (mCRM) and data mining CRM.}
}
@incollection{LADLEY20201,
title = {Chapter 1 - Prologue: An executive overview},
editor = {John Ladley},
booktitle = {Data Governance (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {1-6},
year = {2020},
isbn = {978-0-12-815831-9},
doi = {https://doi.org/10.1016/B978-0-12-815831-9.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158319000011},
author = {John Ladley},
keywords = {Infonomics, Data literacy, Data ethics, Artificial intelligence, Data governance, Data management, Leadership, CEO, Chief data officer},
abstract = {This is a prologue. This chapter directly addresses top leadership on the necessity and fundamental concepts of data governance. This chapter offers the initial ideas that will form data literate leaders.}
}
@article{PEIFFERSMADJA2020584,
title = {Machine learning for clinical decision support in infectious diseases: a narrative review of current applications},
journal = {Clinical Microbiology and Infection},
volume = {26},
number = {5},
pages = {584-595},
year = {2020},
issn = {1198-743X},
doi = {https://doi.org/10.1016/j.cmi.2019.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S1198743X1930494X},
author = {N. Peiffer-Smadja and T.M. Rawson and R. Ahmad and A. Buchard and P. Georgiou and F.-X. Lescure and G. Birgand and A.H. Holmes},
keywords = {Artificial intelligence, Clinical decision support system, Infectious diseases, Information technology, Machine learning},
abstract = {Background
Machine learning (ML) is a growing field in medicine. This narrative review describes the current body of literature on ML for clinical decision support in infectious diseases (ID).
Objectives
We aim to inform clinicians about the use of ML for diagnosis, classification, outcome prediction and antimicrobial management in ID.
Sources
References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, ACM Digital Library, arXiV and IEEE Xplore Digital Library up to July 2019.
Content
We found 60 unique ML-clinical decision support systems (ML-CDSS) aiming to assist ID clinicians. Overall, 37 (62%) focused on bacterial infections, 10 (17%) on viral infections, nine (15%) on tuberculosis and four (7%) on any kind of infection. Among them, 20 (33%) addressed the diagnosis of infection, 18 (30%) the prediction, early detection or stratification of sepsis, 13 (22%) the prediction of treatment response, four (7%) the prediction of antibiotic resistance, three (5%) the choice of antibiotic regimen and two (3%) the choice of a combination antiretroviral therapy. The ML-CDSS were developed for intensive care units (n = 24, 40%), ID consultation (n = 15, 25%), medical or surgical wards (n = 13, 20%), emergency department (n = 4, 7%), primary care (n = 3, 5%) and antimicrobial stewardship (n = 1, 2%). Fifty-three ML-CDSS (88%) were developed using data from high-income countries and seven (12%) with data from low- and middle-income countries (LMIC). The evaluation of ML-CDSS was limited to measures of performance (e.g. sensitivity, specificity) for 57 ML-CDSS (95%) and included data in clinical practice for three (5%).
Implications
Considering comprehensive patient data from socioeconomically diverse healthcare settings, including primary care and LMICs, may improve the ability of ML-CDSS to suggest decisions adapted to various clinical contexts. Currents gaps identified in the evaluation of ML-CDSS must also be addressed in order to know the potential impact of such tools for clinicians and patients.}
}
@article{ZHONG2020103416,
title = {Generating pseudo density log from drilling and logging-while-drilling data using extreme gradient boosting (XGBoost)},
journal = {International Journal of Coal Geology},
volume = {220},
pages = {103416},
year = {2020},
issn = {0166-5162},
doi = {https://doi.org/10.1016/j.coal.2020.103416},
url = {https://www.sciencedirect.com/science/article/pii/S0166516219306743},
author = {Ruizhi Zhong and Raymond Johnson and Zhongwei Chen},
keywords = {Density logging, Pseudo density log, Machine learning, Drilling, Coalbed methane (CBM), Coal seam gas (CSG), Extreme Gradient Boosting (XGBoost)},
abstract = {Density log data is one of the key physical attributes used for reservoir characterization by quantifying and qualifying lithological attributes in a wellbore. The density log is most often acquired by geophysical wireline logging techniques after drilling. However, wireline logging can be difficult to execute in highly deviated wells and alternative data acquisition such as logging-while-drilling (LWD) can be very costly. This paper describes the process of using instantaneous drilling attributes together with a machine learning algorithm, extreme gradient boosting (XGBoost), to generate a pseudo density log. The mean absolute error (MAE) and root mean square error (RMSE) are used as the evaluation metrics. Case studies are performed using data from six coalbed methane (or coal seam gas) wells in the Surat Basin, Australia. The inputs include drilling data [weight on bit (WOB), rotations per minute (RPM), torque, true vertical depth (TVD) and rate of penetration (ROP)] and LWD data (i.e., natural gamma ray and hole diameter). The MAE of pseudo density log for most wells is between 0.08 and 0.11 g/cc (except Well 4 is 0.16 g/cc), which results in an average error rate less than 5%. It is found that TVD, gamma ray, ROP, and hole diameter are four most important features. Further experiment shows that the model with these four important features has almost the same performance as the model with all features. The proposed machine learning methodology can assist petroleum engineers and geologists in reservoir characterization by generating pseudo density logs from ongoing LWD and drilling data in real time. It can potentially mitigate the need to run wireline logging tools after drilling or costly LWD techniques whilst drilling.}
}
@article{RAJ2020107546,
title = {Barriers to the adoption of industry 4.0 technologies in the manufacturing sector: An inter-country comparative perspective},
journal = {International Journal of Production Economics},
volume = {224},
pages = {107546},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2019.107546},
url = {https://www.sciencedirect.com/science/article/pii/S092552731930372X},
author = {Alok Raj and Gourav Dwivedi and Ankit Sharma and Ana Beatriz {Lopes de Sousa Jabbour} and Sonu Rajak},
keywords = {Industry 4.0, Barriers, Grey-DEMATEL, Developed economy, Developing economy},
abstract = {This paper examines barriers to the implementation of Industry 4.0 technologies in the manufacturing sector in the context of both developed and developing economies. A comprehensive literature review, followed by discussions with industry experts, identifies 15 barriers, which are analyzed by means of a Grey Decision-Making Trial and Evaluation Laboratory (DEMATEL) approach. The ‘lack of a digital strategy alongside resource scarcity’ emerges as the most prominent barrier in both developed and developing economies. The influencing barriers identified suggest that improvements in standards and government regulation could facilitate the adoption of Industry 4.0 technologies in developing country case, whereas technological infrastructure is needed to promote the adoption of these technologies in developed country case. This study is one of the first to examine the implementation of Industry 4.0 in both developing and developed economies. This article highlights the difficulties in the diffusion of technological innovation resulting from a lack of coordinated national policies on Industry 4.0 in developing countries, which may prevent firms from fully experiencing the Industry 4.0 revolution. The results of this study may help decision makers and practitioners to address the barriers highlighted, paving the way for successful implementation of Industry 4.0 across the manufacturing sector.}
}
@article{HEIKINHEIMO2020103845,
title = {Understanding the use of urban green spaces from user-generated geographic information},
journal = {Landscape and Urban Planning},
volume = {201},
pages = {103845},
year = {2020},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2020.103845},
url = {https://www.sciencedirect.com/science/article/pii/S0169204619313635},
author = {Vuokko Heikinheimo and Henrikki Tenkanen and Claudia Bergroth and Olle Järv and Tuomo Hiippala and Tuuli Toivonen},
keywords = {Urban green space, Social media data, Sports tracking data, Mobile phone data, PPGIS},
abstract = {Parks and other green spaces are an important part of sustainable, healthy and socially equal urban environment. Urban planning and green space management benefit from information about green space use and values, but such data are often scarce and laborious to collect. Temporally dynamic geographic information generated by different mobile devices and social media platforms are a promising source of data for studying green spaces. User-generated data have, however, platform specific characteristics that limit their potential use. In this article, we compare the ability of different user-generated data sets to provide information on where, when and how people use and value urban green spaces. We compare four types of data: social media, sports tracking, mobile phone operator and public participation geographic information systems (PPGIS) data in a case study from Helsinki, Finland. Our results show that user-generated geographic information sources provide useful insights about being in, moving through and perceiving urban green spaces, as long as evident limitations and sample biases are acknowledged. Social media data highlight patterns of leisure time activities and allow further content analysis. Sports tracking data and mobile phone data capture green space use at different times of the day, including commuting through the parks. PPGIS studies allow asking specific questions from active participants, but might be limited in spatial and temporal extent. Combining information from multiple user-generated data sets complements traditional data sources and provides a more comprehensive understanding of green space use and preferences.}
}
@article{GRIMBERG2020104917,
title = {Smartphones vs. in-vehicle data acquisition systems as tools for naturalistic driving studies: A comparative review},
journal = {Safety Science},
volume = {131},
pages = {104917},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104917},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520303143},
author = {Einat Grimberg and Assaf Botzer and Oren Musicant},
abstract = {Naturalistic driving studies (NDS) are increasingly being used to investigate driver on-road behavior. In parallel, smartphones are gaining interest as data acquisition systems (DAS) in NDS instead of costly in-vehicle DAS. However, smartphone and in-vehicle DAS differ across several attributes and no current document outlines the implications of using smartphones as DAS in NDS. In this document, we present a comparative review of the advantages and disadvantages of using smartphone and in-vehicle DAS in NDS and discuss their implications. In addition, we present a brief account on prospective technological developments that might have further implications for using smartphones for studying and advancing road safety. Researchers and practitioners can use this review as a general guide to decide which DAS (smartphone or in-vehicle) to use in their NDS. For example, smartphones would be a cost-effective alternative for studying driving style (e.g., braking and speeding), but an inferior alternative to in-vehicle DAS for reconstructing crashes or near crashes and for studying short-term relationships between events (e.g., smartphone usage and hard braking). Researchers and practitioners can also use this review as an aid for the design of NDS with smartphones. For example, we show that it would be advisable to use beacons to know if participants were driving their vehicle or riding the bus, and that data completeness and accuracy would depend on battery charge and using a cradle. Prospective technologies might mitigate the shortcomings that we have outlined and might even dim the distinction between the different types of DAS.}
}
@article{CHOI2020102279,
title = {Social media analytics and business intelligence research: A systematic review},
journal = {Information Processing & Management},
volume = {57},
number = {6},
pages = {102279},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102279},
url = {https://www.sciencedirect.com/science/article/pii/S030645731931057X},
author = {Jaewoong Choi and Janghyeok Yoon and Jaemin Chung and Byoung-Youl Coh and Jae-Min Lee},
keywords = {Social media, Online VoC, Open data, Business intelligence, Systematic review, Sentiment analysis},
abstract = {Evidently, online voice of customers (VoC) expressed in social media has emerged as quality data for researchers who are willing to conduct customer-driven business intelligence (BI) research. Nevertheless, to the best of authors’ knowledge, there is still a dearth of studies that deal with such remarkable research stream and address various open data (e.g., social media, intellectual property) from a BI research perspective. Therefore, this study has attempted to evaluate the applicability of social media data in BI research and provide a systematic review on the primary research articles in the domain. This study compared social media data with the other open data (e.g., gray literature, public government data) in terms of data content, collection, updatability and structure, which are determined through a thorough discussion with experts. Next, this study selected 57 social media-based BI research articles from the Web of Science (WoS) database and analyzed them with three research questions about the data, methodologies, and results to understand this research domain. Our findings are expected to inform the existing researchers in the research domain about the future research directions, enable newcomers to understand the overall process of analyzing social media data, and provide the practitioners with social media analysis approaches suitable for their environment.}
}
@incollection{GERKE2020295,
title = {Chapter 12 - Ethical and legal challenges of artificial intelligence-driven healthcare},
editor = {Adam Bohr and Kaveh Memarzadeh},
booktitle = {Artificial Intelligence in Healthcare},
publisher = {Academic Press},
pages = {295-336},
year = {2020},
isbn = {978-0-12-818438-7},
doi = {https://doi.org/10.1016/B978-0-12-818438-7.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128184387000125},
author = {Sara Gerke and Timo Minssen and Glenn Cohen},
keywords = {Artificial intelligence (AI), ethical challenges, US and EU law, safety and effectiveness, data protection and privacy},
abstract = {This chapter will map the ethical and legal challenges posed by artificial intelligence (AI) in healthcare and suggest directions for resolving them. Section 1 will briefly clarify what AI is and Section 2 will give an idea of the trends and strategies in the United States (US) and Europe, thereby tailoring the discussion to the ethical and legal debate of AI-driven healthcare. This will be followed in Section 3 by a discussion of four primary ethical challenges, namely, (1) informed consent to use, (2) safety and transparency, (3) algorithmic fairness and biases, and (4) data privacy. Section 4 will then analyze five legal challenges in the US and Europe: (1) safety and effectiveness, (2) liability, (3) data protection and privacy, (4) cybersecurity, and (5) intellectual property law. Finally, Section 5 will summarize the major conclusions and especially emphasize the importance of building an AI-driven healthcare system that is successful and promotes trust and the motto Health AIs for All of Us.}
}
@incollection{DELLASALA20201,
title = {Forest Biome: Trees of Life},
editor = {Michael I. Goldstein and Dominick A. DellaSala},
booktitle = {Encyclopedia of the World's Biomes},
publisher = {Elsevier},
address = {Oxford},
pages = {1-15},
year = {2020},
isbn = {978-0-12-816097-8},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.12007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012409548912007X},
author = {Dominick A. DellaSala},
keywords = {Deforestation, Degradation, Forest quality, Intact forest landscapes, Primary forest},
abstract = {Nearly one third of the Earth’s terrestrial surface are covered by forests of some type, the world’s largest terrestrial biome. Forests collectively cleanse the air we breathe; purify our drinking water; play a vital role in regulating regional and global climatic processes; support at least half of all terrestrial species on Earth, especially in the tropics; and provide economic, spiritual, and cultural benefits for millions of people worldwide. Global rates of deforestation and forest degradation have slowed recently based on country-specific reporting overseen by the Food and Agriculture Organization. However, the alarming loss of primary (unlogged) forests remains a global concern (e.g., Global Forest Watch). Gains in forest cover from tree planting do not offset losses. Thus, international initiatives (e.g., UN Sustainability Goals and REDD+ Programme, Convention on Biological Diversity, Paris Climate Agreement) along with national/regional conservation need to protect remaining primary forests and intact forest landscapes while restoring primary forests through “proforestation.”}
}
@article{ROMERO2020101489,
title = {An Alternative View on Data Processing Pipelines from the DOLAP 2019 Perspective},
journal = {Information Systems},
volume = {92},
pages = {101489},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.101489},
url = {https://www.sciencedirect.com/science/article/pii/S0306437919305411},
author = {Oscar Romero and Robert Wrembel and Il-Yeol Song},
keywords = {Data integration, ETL/ELT, ETL optimization, Data processing pipeline, Metadata, Data management, Data analytics},
abstract = {Data science requires constructing data processing pipelines (DPPs), which span diverse phases such as data integration, cleaning, pre-processing, and analysis. However, current solutions lack a strong data engineering perspective. As consequence, DPPs are error-prone, inefficient w.r.t. human efforts, and inefficient w.r.t. execution time. We claim that DPP design, development, testing, deployment, and execution should benefit from a standardized DPP architecture and from well-known data engineering solutions. This claim is supported by our experience in real projects and trends in the field, and it opens new paths for research and technology. With this spirit, we outline five research opportunities that represent novel trends towards building DPPs. Finally, we highlight that the best DOLAP 2019 papers selected for the DOLAP 2019 Information Systems Special Issue fall in this category and highlight the relevance of advanced data engineering for data science.}
}
@article{BORDIN20201659,
title = {Machine Learning for Hydropower Scheduling: State of the Art and Future Research Directions},
journal = {Procedia Computer Science},
volume = {176},
pages = {1659-1668},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320925},
author = {Chiara Bordin and Hans Ivar Skjelbred and Jiehong Kong and Zhirong Yang},
keywords = {Artificial Neural Networks, Digitalisation, Hydropower Scheduling, Machine Learning},
abstract = {This paper investigates and discusses the current and future role of machine learning (ML) within the hydropower sector. An overview of the main applications of ML in the field of hydropower operations is presented to show the most common topics that have been addressed in the scientific literature in the last years. The objective is to provide recommendations for novel research directions that can be taken in the near future to cover those areas that have not been studied so far. The key contribution of this paper lies in a critical investigation of the state of the art of ML applications in hydropower scheduling. In light of the established literature available in the last years, this study identifies and discusses new roles that can be covered by ML, coupled with cyber-physical systems (CPSs), with a particular focus on short-term hydropower scheduling (STHS) challenges.}
}
@article{DONGELMANS2020305,
title = {Linking of global intensive care (LOGIC): An international benchmarking in critical care initiative},
journal = {Journal of Critical Care},
volume = {60},
pages = {305-310},
year = {2020},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2020.08.031},
url = {https://www.sciencedirect.com/science/article/pii/S0883944120306729},
author = {D.A. Dongelmans and David Pilcher and Abigail Beane and Marcio Soares and Maria {del Pilar Arias Lopez} and Ariel Fernandez and Bertrand Guidet and Rashan Haniffa and Jorge I.F. Salluh},
keywords = {Critical care, Improvement science, Benchmarking, Global, Quality, Quality registry},
abstract = {Benchmarking is a common and effective method for measuring and analyzing ICU performance. With the existence of national registries, objective information can now be obtained to allow benchmarking of ICU care within and between countries. The present manuscript briefly describes the current status of benchmarking in healthcare and critical care and presents the LOGIC project, an initiative to promote international benchmarking for intensive care units. Currently 13 registries have joined LOGIC. We showed large differences in the utilization of ICU as well as resources and in outcomes. Despite the need for careful interpretation of differences due to variation in definitions and limited risk adjustment, LOGIC is a growing worldwide initiative that allows access to insightful epidemiologic data from ICUs in multiple databases and registries.}
}
@article{KAMBLE2020107853,
title = {A performance measurement system for industry 4.0 enabled smart manufacturing system in SMMEs- A review and empirical investigation},
journal = {International Journal of Production Economics},
volume = {229},
pages = {107853},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107853},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320302176},
author = {Sachin S. Kamble and Angappa Gunasekaran and Abhijeet Ghadge and Rakesh Raut},
keywords = {Smart manufacturing system, Industry 4.0, Performance measurement, SMMEs, Automotive industry, Digital transformation},
abstract = {The smart manufacturing systems (SMS) offer several advantages compared to the traditional manufacturing systems and are increasingly being adopted by manufacturing organizations as a strategy to improve their performance. Developing an SMS is expensive and complicated, integrating together various technologies such as automation, data exchanges, cyber-physical systems (CPS), artificial intelligence, internet of things (IoT), and semi-autonomous industrial systems. The Small, Medium and Micro Enterprises (SMMEs) have limited resources and therefore, would like to see the benefits from investments before allowing adopting SMS. This study uses a combination of exploratory and empirical research design to identify and validate the performance measures relevant to the evaluation of SMS investments in auto-component manufacturing SMMEs based in India. The study found that an Industry 4.0 enabled SMS offer more competitive benefits compared to a traditional manufacturing system. The planned investments in SMS can be evaluated on ten performance dimensions namely, cost, quality, flexibility, time, integration, optimized productivity, real-time diagnosis & prognosis, computing, social and ecological sustainability. Proposed novel Smart Manufacturing Performance Measurement System (SMPMS) framework is expected to guide the practitioners in SMMEs to evaluate their SMS investments.}
}
@article{LOHMER2020106789,
title = {Blockchain in operations management and manufacturing: Potential and barriers},
journal = {Computers & Industrial Engineering},
volume = {149},
pages = {106789},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106789},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220304988},
author = {Jacob Lohmer and Rainer Lasch},
keywords = {Blockchain technology, Production networks, Distributed manufacturing, Industry 4.0, Case study, Digital manufacturing},
abstract = {Transparency, visibility, and disintermediation are some of the prospects of the aspiring blockchain technology in the business-to-business context. The digital transformation and Industry 4.0 trends also facilitate blockchain applications in operations management (OM) and manufacturing. However, scientific contributions and successful industrial applications in this area are still scarce and mainly at a proof-of-concept stage. The empirical research in this article is based on an expert interview study to uncover and analyse the potential and barriers to the adoption of blockchain technology in OM and manufacturing from within the industry. Semi-structured interviews with industry experts are employed to elaborate on promising practices for the industry to efficiently promote blockchain adoption and meaningful research directions for scholars. Findings include unexplored potential regarding distributed production networks and collaboration, expected evolutionary steps of IoT, disintermediation leading to new business models like tokenisation, and short-term rather than long-term relationships. Current barriers include staff difficulties, legal uncertainties, missing infrastructure and standardisation, and unclear governance structures. Improving smart contract security and interoperability of private and public protocols will enable further dissemination of the technology. Managers and academic scholars can address these findings and new propositions of this study in future application development and implementation.}
}
@article{HONG2020106508,
title = {Ten questions on urban building energy modeling},
journal = {Building and Environment},
volume = {168},
pages = {106508},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2019.106508},
url = {https://www.sciencedirect.com/science/article/pii/S0360132319307206},
author = {Tianzhen Hong and Yixing Chen and Xuan Luo and Na Luo and Sang Hoon Lee},
keywords = {Building energy use, Energy efficiency, Urban systems, Urban building energy modeling (UBEM), Urban energy planning, Building performance simulation},
abstract = {Buildings in cities consume up to 70% of all primary energy. To achieve cities’ energy and climate goals, it is necessary to reduce energy use and associated greenhouse gas emissions in buildings through energy conservation and efficiency improvements. Computational tools empowered with rich urban datasets can model performance of buildings at the urban scale to provide quantitative insights for stakeholders and inform their decision making on urban energy planning, as well as building energy retrofits at scale, to achieve efficiency, sustainability, and resilience of urban buildings. Designing and operating urban buildings as a group (from a city block to a district to an entire city) rather than as single individuals requires simulation and optimization to account for interactions among buildings and between buildings and their surrounding urban environment, and for district energy systems serving multiple buildings with diverse thermal loads across space and time. When hundreds or more buildings are involved in typical urban building energy modeling (UBEM) to estimate annual energy demand, evaluate design or retrofit options, and quantify impacts of extreme weather events or climate change, it is crucial to integrate urban datasets and UBEM tools in a seamless automatic workflow with cloud or high-performance computing for users including urban planners, designers and researchers. This paper presents ten questions that highlight significant UBEM research and applications. The proposed answers aim to stimulate discussion and provide insights into the current and future research on UBEM, and more importantly, to inspire new and important questions from young researchers in the field.}
}
@article{CHAN20201523,
title = {Data Mining to Understand How Health Status Preceding Traumatic Brain Injury Affects Functional Outcome: A Population-Based Sex-Stratified Study},
journal = {Archives of Physical Medicine and Rehabilitation},
volume = {101},
number = {9},
pages = {1523-1531},
year = {2020},
issn = {0003-9993},
doi = {https://doi.org/10.1016/j.apmr.2020.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0003999320303695},
author = {Vincy Chan and Mitchell Sutton and Tatyana Mollayeva and Michael D. Escobar and Mackenzie Hurst and Angela Colantonio},
keywords = {Brain injuries, Comorbidity, Data mining, International Classification of Diseases, Rehabilitation},
abstract = {Objectives
To understand how health status preceding traumatic brain injury (TBI) affects relative functional gain after inpatient rehabilitation using a data mining approach.
Design
Population-based, sex-stratified, retrospective cohort study using health administrative data from Ontario, Canada (39% of the Canadian population).
Setting
Inpatient rehabilitation.
Participants
Patients 14 years or older (N=5802; 63.4% male) admitted to inpatient rehabilitation within 1 year of a TBI-related acute care discharge between April 1, 2008, and March 31, 2015.
Interventions
Not applicable.
Main Outcome Measures
Relative functional gain (RFG) in percentage, calculated as ([discharge FIM−admission FIM]/[126−admission FIM]×100). Health status prior to TBI was identified and internally validated using a data mining approach that categorized all International Classification of Diseases, 10th revision, codes for each patient.
Results
The average RFG was 52.8%±27.6% among male patients and 51.6%±27.1% among female patients. Sex-specific Bonferroni adjusted multivariable linear regressions identified 10 factors of preinjury health status related to neurology, emergency medicine, cardiology, psychiatry, geriatrics, and gastroenterology that were significantly associated with reduced RFG in FIM for male patients. Only 1 preinjury health status category, geriatrics, was significantly associated with RFG in female patients.
Conclusions
Comorbid health conditions present up to 5 years preceding the TBI event were significantly associated with RFG. These findings should be considered when planning and executing interventions to maximize functional gain and to support an interdisciplinary approach. Best practices guidelines and clinical interventions for older male and female patients with TBI should be developed given the increasingly aging population with TBI.}
}
@article{LORUSSO2020e000924,
title = {Clinical research disruption in the post-COVID-19 era: will the pandemic lead to change?},
journal = {ESMO Open},
volume = {5},
number = {5},
pages = {e000924},
year = {2020},
issn = {2059-7029},
doi = {https://doi.org/10.1136/esmoopen-2020-000924},
url = {https://www.sciencedirect.com/science/article/pii/S2059702920327186},
author = {Domenica Lorusso and Isabelle Ray-Coquard and Ana Oaknin and Susana Banerjee},
keywords = {clinical research, pandemic telemedicine, remote monitoring disruption}
}
@article{AGGESTAM2020178,
title = {Introduction to the special issue on a Shared Environmental Information System (SEIS) for evidence-based policymaking},
journal = {Environmental Science & Policy},
volume = {114},
pages = {178-181},
year = {2020},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2020.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S1462901120310364},
author = {Filip Aggestam and Annukka Lipponen and Diana Mangalagiu and Michael Vardon and Uta Wehn}
}
@article{CHANG2020103335,
title = {Risk factors of enterprise internal control under the internet of things governance: A qualitative research approach},
journal = {Information & Management},
volume = {57},
number = {6},
pages = {103335},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103335},
url = {https://www.sciencedirect.com/science/article/pii/S037872062030272X},
author = {She-I Chang and Li-Min Chang and Jhan-Cyun Liao},
keywords = {Internet of things (IoT), Risk factors, Enterprise internal control, Qualitative research, IT governance},
abstract = {This study aims to (1) define the critical risk factors that influence the governance of enterprise internal control in an IoT environment, and (2) classify the risk factors and study their importance in such an environment. The study uses Gowin’s Vee knowledge map as a research strategy to mitigate the limitations of qualitative research through a set of strict research procedures. In addition, the Delphi method is used to test and provide feedback to justify and revise the critical risk factors. Finally, 83 items were obtained and categorized into eight different types of critical risk factors. For emphasizing how the risk factors of enterprise internal control involve diverse stakeholders, the critical risk factors are further classified based on the three-layer DCM architecture for mapping with various perceptions. The results of this research can be used as a reference in managing risk factors under the IoT environment. In the new generation of IoT governance practice, the related factors can also be regarded as the essential measurement items for enterprises in conducting effective internal control and auditing.}
}