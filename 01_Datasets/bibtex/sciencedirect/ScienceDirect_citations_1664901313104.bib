@article{ZHANG2018146,
title = {A survey on deep learning for big data},
journal = {Information Fusion},
volume = {42},
pages = {146-157},
year = {2018},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1566253517305328},
author = {Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li},
keywords = {Deep learning, Big data, Stacked auto-encoders, Deep belief networks, Convolutional neural networks, Recurrent neural networks},
abstract = {Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.}
}
@article{GUPTA201878,
title = {Big data with cognitive computing: A review for the future},
journal = {International Journal of Information Management},
volume = {42},
pages = {78-89},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218304110},
author = {Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter},
keywords = {Big data, Cognitive computing, Literature review, Resource based View, Institutional theory},
abstract = {Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V’s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.}
}
@article{COX2018111,
title = {Big data: Some statistical issues},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {111-115},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300609},
author = {D.R. Cox and Christiana Kartsonaki and Ruth H. Keogh},
keywords = {Big data, Electronic health records, Epidemiology, Metrology, Precision},
abstract = {A broad review is given of the impact of big data on various aspects of investigation. There is some but not total emphasis on issues in epidemiological research.}
}
@article{SUN201827,
title = {Lossless Pruned Naive Bayes for Big Data Classifications},
journal = {Big Data Research},
volume = {14},
pages = {27-36},
year = {2018},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2214579616301320},
author = {Nanfei Sun and Bingjun Sun and Jian (Denny) Lin and Michael Yu-Chi Wu},
keywords = {Big data, Classification, Naive Bayes, Large-scale taxonomy, Lossless, Pruned},
abstract = {In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies.}
}
@article{SAIF2018118,
title = {Performance Analysis of Big Data and Cloud Computing Techniques: A Survey},
journal = {Procedia Computer Science},
volume = {132},
pages = {118-127},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918309062},
author = {Subia Saif and Samar Wazir},
keywords = {Big Data, Big Data Analytics (BDA), Cloud Computing, Cloud based Big Data Enterprise Solutions, Big Data Storage, Big Data Warehouse, Streaming Data, Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud, Microsoft Azure},
abstract = {A cloud framework refers to the aggregation of components like development tools, middleware and database services, needed for cloud computing, which aids in developing, deploying and managing cloud based applications strenuously, consequently making it an efficacious paradigm for massive scaling of dynamically allocated resources and their complex computing. Big Data Analytics (BDA) delivers data management solutions in the cloud architecture for storing, analysing and processing a huge volume of data. This paper presents a survey for performance based comparative analysis of cloud-based big data frameworks from leading enterprises like Amazon, Google, IBM, and Microsoft, which will assist researcher, IT analysts, reader and business user in picking the framework best suited for their work ensuring success in terms of favourable outcomes.}
}
@article{GENENDERFELTHEIMER2018112,
title = {Visualizing High Dimensional and Big Data},
journal = {Procedia Computer Science},
volume = {140},
pages = {112-121},
year = {2018},
note = {Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.308},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319811},
author = {Amy Genender-Feltheimer},
keywords = {Data Visualization, Dimensionality Reduction, Parallel Processing, Hadoop, Machine Learning, Big Data},
abstract = {The amount of data created by people, machines and corporations around the world is growing every year. Thanks to innovations such as the Internet of Things, this trend will continue, giving rise to the creation of Big Data. Data visualization leverages principles of visual psychology to help stakeholders identify patterns, trends and correlations that might go undetected in text-based or spreadsheet data. The return on investment (ROI) of big data visualization is well-documented in numerous studies and use cases. However, to achieve ROI from analytics investments, key insights must be uncovered, understood and communicated. Synthesizing huge quantities of data into key insights grows more challenging as data volumes and varieties increase. To address visualization challenges posed by big and high-dimensional data, this paper explores algorithms and techniques that compress the amount of data and/or reduce the number of attributes to be analyzed and visualized. Specifically, this paper examines applying dimensionality reduction and data compression algorithms to reduce attributes, tuples and data points returned to the visualization. By reducing data returned to the visualization, trends, patterns and correlations are easier to view and visualization tool performance is optimized.}
}
@article{CHENG20181,
title = {Data and knowledge mining with big data towards smart production},
journal = {Journal of Industrial Information Integration},
volume = {9},
pages = {1-13},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300584},
author = {Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao},
keywords = {Big data, Data mining techniques (DMTs), Production management, Smart manufacturing, Statistical analysis, Knowledge discovery},
abstract = {Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.}
}
@article{SAGGI2018758,
title = {A survey towards an integration of big data analytics to big insights for value-creation},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {758-790},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316307178},
author = {Mandeep Kaur Saggi and Sushma Jain},
keywords = {Big data, Data analytics, Machine learning, Big data visualization, Decision-making, Smart agriculture, Smart city application, Value-creation, Value-discover, Value-realization},
abstract = {Big Data Analytics (BDA) is increasingly becoming a trending practice that generates an enormous amount of data and provides a new opportunity that is helpful in relevant decision-making. The developments in Big Data Analytics provide a new paradigm and solutions for big data sources, storage, and advanced analytics. The BDA provide a nuanced view of big data development, and insights on how it can truly create value for firm and customer. This article presents a comprehensive, well-informed examination, and realistic analysis of deploying big data analytics successfully in companies. It provides an overview of the architecture of BDA including six components, namely: (i) data generation, (ii) data acquisition, (iii) data storage, (iv) advanced data analytics, (v) data visualization, and (vi) decision-making for value-creation. In this paper, seven V's characteristics of BDA namely Volume, Velocity, Variety, Valence, Veracity, Variability, and Value are explored. The various big data analytics tools, techniques and technologies have been described. Furthermore, it presents a methodical analysis for the usage of Big Data Analytics in various applications such as agriculture, healthcare, cyber security, and smart city. This paper also highlights the previous research, challenges, current status, and future directions of big data analytics for various application platforms. This overview highlights three issues, namely (i) concepts, characteristics and processing paradigms of Big Data Analytics; (ii) the state-of-the-art framework for decision-making in BDA for companies to insight value-creation; and (iii) the current challenges of Big Data Analytics as well as possible future directions.}
}
@article{PAPANAGNOU2018343,
title = {Coping with demand volatility in retail pharmacies with the aid of big data exploration},
journal = {Computers & Operations Research},
volume = {98},
pages = {343-354},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2017.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0305054817302162},
author = {Christos I. Papanagnou and Omeiza Matthews-Amune},
keywords = {Retail pharmacy, Data mining, Time series, Forecasting, Big data, Demand uncertainty},
abstract = {Data management tools and analytics have provided managers with the opportunity to contemplate inventory performance as an ongoing activity by no longer examining only data agglomerated from ERP systems, but also, considering internet information derived from customers’ online buying behaviour. The realisation of this complex relationship has increased interest in business intelligence through data and text mining of structured, semi-structured and unstructured data, commonly referred to as “big data” to uncover underlying patterns which might explain customer behaviour and improve the response to demand volatility. This paper explores how sales structured data can be used in conjunction with non-structured customer data to improve inventory management either in terms of forecasting or treating some inventory as “top-selling” based on specific customer tendency to acquire more information through the internet. A medical condition is considered - namely pain - by examining 129 weeks of sales data regarding analgesics and information seeking data by customers through Google, online newspapers and YouTube. In order to facilitate our study we consider a VARX model with non-structured data as exogenous to obtain the best estimation and we perform tests against several univariate models in terms of best fit performance and forecasting.}
}
@article{BLAZQUEZ201899,
title = {Big Data sources and methods for social and economic analyses},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {99-113},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517310946},
author = {Desamparados Blazquez and Josep Domenech},
keywords = {Big Data architecture, Forecasting, Nowcasting, Data lifecycle, Socio-economic data, Non-traditional data sources, Non-traditional analysis methods},
abstract = {The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.}
}
@article{STOREY201750,
title = {Big data technologies and Management: What conceptual modeling can do},
journal = {Data & Knowledge Engineering},
volume = {108},
pages = {50-67},
year = {2017},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300277},
author = {Veda C. Storey and Il-Yeol Song},
abstract = {The era of big data has resulted in the development and applications of technologies and methods aimed at effectively using massive amounts of data to support decision-making and knowledge discovery activities. In this paper, the five Vs of big data, volume, velocity, variety, veracity, and value, are reviewed, as well as new technologies, including NoSQL databases that have emerged to accommodate the needs of big data initiatives. The role of conceptual modeling for big data is then analyzed and suggestions made for effective conceptual modeling efforts with respect to big data.}
}
@article{BALACHANDRAN20171112,
title = {Challenges and Benefits of Deploying Big Data Analytics in the Cloud for Business Intelligence},
journal = {Procedia Computer Science},
volume = {112},
pages = {1112-1122},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.138},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917314953},
author = {Bala M. Balachandran and Shivika Prasad},
keywords = {Cloud Computing, Big Data Analytics, Cloud Analytics, Security, Privacy, Business Intelligence, MapReduce, AaaS, CLaaS},
abstract = {Cloud computing and big data analytics are, without a doubt, two of the most important technologies to enter the mainstream IT industry in recent years. Surprisingly, the two technologies are coming together to deliver powerful results and benefits for businesses. Cloud computing is already changing the way IT services are provided by so called cloud companies and how businesses and users interact with IT resources. Big Data is a data analysis methodology enables by recent advances in information and communications technology. However, big data analysis requires a huge amount of computing resources making adoption costs of big data technology is not affordable for many small to medium enterprises. In this paper, we outline the the benefits and challenges involved in deploying big data analytics through cloud computing. We argue that cloud computing can support the storage and computing requirements of big data analytics. We discuss how the consolidation of these two dominant technologies can enhance the process of big data mining enabling businesses to improve decision-making processes. We also highlight the issues and risks that should be addressed when using a so called CLaaS, cloud-based service model.}
}
@article{GUNTHER2017191,
title = {Debating big data: A literature review on realizing value from big data},
journal = {The Journal of Strategic Information Systems},
volume = {26},
number = {3},
pages = {191-209},
year = {2017},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717302615},
author = {Wendy Arianne Günther and Mohammad H. {Rezazade Mehrizi} and Marleen Huysman and Frans Feldberg},
keywords = {Big data, Analytics, Literature review, Value realization, Portability, Interconnectivity},
abstract = {Big data has been considered to be a breakthrough technological development over recent years. Notwithstanding, we have as yet limited understanding of how organizations translate its potential into actual social and economic value. We conduct an in-depth systematic review of IS literature on the topic and identify six debates central to how organizations realize value from big data, at different levels of analysis. Based on this review, we identify two socio-technical features of big data that influence value realization: portability and interconnectivity. We argue that, in practice, organizations need to continuously realign work practices, organizational models, and stakeholder interests in order to reap the benefits from big data. We synthesize the findings by means of an integrated model.}
}
@article{CANO201736,
title = {Perspectives on Big Data applications of health information},
journal = {Current Opinion in Systems Biology},
volume = {3},
pages = {36-42},
year = {2017},
note = {• Mathematical modelling • Mathematical modelling, Dynamics of brain activity at the systems level • Clinical and translational systems biology},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2452310017300409},
author = {Isaac Cano and Akos Tenyi and Emili Vela and Felip Miralles and Josep Roca},
keywords = {Digital health, Secondary use of data, Health analytics, Predictive modeling, Health forecasting},
abstract = {Recent advances on prospective monitoring and retrospective analysis of health information at national or regional level are generating high expectations for the application of Big Data technologies that aim to analyze at real time high-volumes and/or complex of data from healthcare delivery (e.g., electronic health records, laboratory and radiology information, electronic prescriptions, etc.) and citizens' lifestyles (e.g., personal health records, personal monitoring devices, social networks, etc.). Along these same lines, advances in the field of genomics are revolutionizing biomedical research, both in terms of data volume and prospects, as well as in terms of the social impact it entails. The potential of Big Data applications that consider all of the above levels of health information lies in the possibility of combining and integrating de-identified health information to allow secondary uses of data. This is the use and re-use of various sources of health information for purposes in addition to the direct clinical care of specific patients or the direct investigation of specific biomedical research hypotheses. Current applications include: epidemiological and pharmacovigilance studies, facilitating recruitment to randomized controlled trials, carrying out audits and benchmarking studies, financial and service planning, and ultimately supporting the generation of novel biomedical research outcomes.}
}
@article{TIWARI2018319,
title = {Big data analytics in supply chain management between 2010 and 2016: Insights to industries},
journal = {Computers & Industrial Engineering},
volume = {115},
pages = {319-330},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217305508},
author = {Sunil Tiwari and H.M. Wee and Yosef Daryanto},
keywords = {Big data analytics, Supply chain management, Big data application},
abstract = {This paper investigates big data analytics research and application in supply chain management between 2010 and 2016 and provides insights to industries. In recent years, the amount of data produced from end-to-end supply chain management practices has increased exponentially. Moreover, in current competitive environment supply chain professionals are struggling in handling the huge data. They are surveying new techniques to investigate how data are produced, captured, organized and analyzed to give valuable insights to industries. Big Data analytics is one of the best techniques which can help them in overcoming their problem. Realizing the promising benefits of big data analytics in the supply chain has motivated us to write a review on the importance/impact of big data analytics and its application in supply chain management. First, we discuss big data analytics individually, and then we discuss the role of big data analytics in supply chain management (supply chain analytics). Current research and application are also explored. Finally, we outline the insights to industries. Observations and insights from this paper could provide the guideline for academia and practitioners in implementing big data analytics in different aspects of supply chain management.}
}
@article{WANG20183,
title = {Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations},
journal = {Technological Forecasting and Social Change},
volume = {126},
pages = {3-13},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516000500},
author = {Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd},
keywords = {Big data analytics, Big data analytics architecture, Big data analytics capabilities, Business value of information technology (IT), Health care},
abstract = {To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.}
}
@article{BAYNE2018481,
title = {Big Data in Neonatal Health Care: Big Reach, Big Reward?},
journal = {Critical Care Nursing Clinics of North America},
volume = {30},
number = {4},
pages = {481-497},
year = {2018},
note = {Neonatal Nursing},
issn = {0899-5885},
doi = {https://doi.org/10.1016/j.cnc.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899588518309754},
author = {Lynn E. Bayne},
keywords = {Big data, Electronic health record (EHR), Neonatology, Cost-savings, Clinical decision making, Healthcare analytics}
}
@article{COBB2018640,
title = {Big data: More than big data sets},
journal = {Surgery},
volume = {164},
number = {4},
pages = {640-642},
year = {2018},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2018.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0039606018303660},
author = {Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo},
abstract = {The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.}
}
@article{BIVAND201887,
title = {Big data sampling and spatial analysis: “which of the two ladles, of fig-wood or gold, is appropriate to the soup and the pot?”},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {87-91},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300579},
author = {Roger Bivand and Konstantin Krivoruchko},
keywords = {Change of support, Sampling design, Data transformation, Prediction standard error},
abstract = {Following from Krivoruchko and Bivand (2009), we consider some general points related to challenges to the usefulness of big data in spatial statistical applications when data collection is compromised or one or more model assumptions are violated. We look further at the desirability of comparison of new methods intended to handle large spatial and spatio-temporal datasets.}
}
@article{DALLAVALLE201876,
title = {Social media big data integration: A new approach based on calibration},
journal = {Expert Systems with Applications},
volume = {111},
pages = {76-90},
year = {2018},
note = {Big Data Analytics for Business Intelligence},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417308667},
author = {Luciana {Dalla Valle} and Ron Kenett},
keywords = {Bayesian networks, Calibration, Data integration, Social media, Information quality (InfoQ), Resampling techniques},
abstract = {In recent years, the growing availability of huge amounts of information, generated in every sector at high speed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an opportunity to obtain more accurate analyses and to improve decision-making in industry, government and many other organizations. However, handling big data may be challenging and proper data integration is a key dimension in achieving high information quality. In this paper, we propose a novel approach to data integration that calibrates online generated big data with interview based customer survey data. A common issue of customer surveys is that responses are often overly positive, making it difficult to identify areas of weaknesses in organizations. On the other hand, online reviews are often overly negative, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the levels of unbalanced responses in different data sources via resampling and performs data integration using Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a case study example, how the novel data integration approach allows businesses and organizations to get a bias corrected appraisal of the level of satisfaction of their customers. The application is based on the integration of online data of review blogs and customer satisfaction surveys from the San Francisco airport. We illustrate how this integration enhances the information quality of the data analytic work in four of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology of Data and Goal.}
}
@article{BONNEL20181,
title = {Transport survey methods - in the era of big data facing new and old challenges},
journal = {Transportation Research Procedia},
volume = {32},
pages = {1-15},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301522},
author = {Patrick Bonnel and Marcela A. Munizaga},
keywords = {ISCTSC, transport, survey methodology, big data},
abstract = {This document presents an introduction to the ISCTSC Special Issue of Transport Research Procedia. It synthesizes the discussions held at the 11th International Conference on Transport Survey Methods, and describes the contents of the selected contributions. This conference has been held in different countries from all over the world, involving an increasing group of enthusiastic and generous specialists, willing to share their knowledge. This 11th conference was an opportunity to discuss the state of the art on transport survey methods, but also to question the way transport surveys are conducted in the era of big data. We took the opportunity to identify the main challenges, and the most important questions.}
}
@article{XIA2018191,
title = {Using spatiotemporal patterns to optimize Earth Observation Big Data access: Novel approaches of indexing, service modeling and cloud computing},
journal = {Computers, Environment and Urban Systems},
volume = {72},
pages = {191-203},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518300401},
author = {Jizhe Xia and Chaowei Yang and Qingquan Li},
keywords = {Spatiotemporal optimization, Big Data, Cloud computing, Global operation, GEOSS},
abstract = {Based on our GEOSS Clearinghouse operating experience, we summarized the three Earth Observation (EO) Big Data access challenges, namely, fast access, accurate service estimation and global access, and two essential research questions: are there any spatiotemporal patterns when end users access EO data, and how can these spatiotemporal patterns be utilized to better facilitate EO Big Data access? To tackle these two research questions, we conducted a two-year pattern analysis with 2+ million user access records. The spatial pattern, temporal pattern and spatiotemporal pattern of user-data interactions were explored. For the second research question, we developed three spatiotemporal optimization strategies to respond to the three access challenges: a) spatiotemporal indexing to accelerate data access, b) spatiotemporal service modeling to improve data access accuracy and c) spatiotemporal cloud computing to enhance global access. This research is a pioneering framework for spatiotemporal optimization of EO Big Data access and valuable for other multidisciplinary geographic data and information research.}
}
@article{BALA2017114,
title = {A Fine‐Grained Distribution Approach for ETL Processes in Big Data Environments},
journal = {Data & Knowledge Engineering},
volume = {111},
pages = {114-136},
year = {2017},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16300611},
author = {Mahfoud Bala and Omar Boussaid and Zaia Alimazighi},
keywords = {Data Warehousing, ETL, Parallel and Distributed Processing, Big Data, MapReduce},
abstract = {Among the so-called “4Vs” (volume, velocity, variety, and veracity) that characterize the complexity of Big Data, this paper focuses on the issue of “Volume” in order to ensure good performance for Extracting-Transforming-Loading (ETL) processes. In this study, we propose a new fine-grained parallelization/distribution approach for populating the Data Warehouse (DW). Unlike prior approaches that distribute the ETL only at coarse-grained level of processing, our approach provides different ways of parallelization/distribution both at process, functionality and elementary functions levels. In our approach, an ETL process is described in terms of its core functionalities which can run on a cluster of computers according to the MapReduce (MR) paradigm. The novel approach allows thereby the distribution of the ETL process at three levels: the “process” level for coarse-grained distribution and the “functionality” and “elementary functions” levels for fine-grained distribution. Our performance analysis reveals that employing 25 to 38 parallel tasks enables the novel approach to speed up the ETL process by up to 33% with the improvement rate being linear.}
}
@article{HONG2018175,
title = {Big Data in Health Care: Applications and Challenges},
journal = {Data and Information Management},
volume = {2},
number = {3},
pages = {175-197},
year = {2018},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2018-0014},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000791},
author = {Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu},
keywords = {Big Data, public health, cloud computing, medical applications},
abstract = {The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.}
}
@article{JANSSEN2017338,
title = {Factors influencing big data decision-making quality},
journal = {Journal of Business Research},
volume = {70},
pages = {338-345},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304945},
author = {Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi},
keywords = {Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality},
abstract = {Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.}
}
@article{REY201837,
title = {Causes of deaths data, linkages and big data perspectives},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {37-40},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X16301652},
author = {Grégoire Rey and Karim Bounebache and Claire Rondet},
keywords = {Causes of death data, Data linkages, Big data},
abstract = {The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called “big data” field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.}
}
@article{ALBADI2018271,
title = {Exploring Big Data Governance Frameworks},
journal = {Procedia Computer Science},
volume = {141},
pages = {271-277},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318313},
author = {Ali Al-Badi and Ali Tarhini and Asharul Islam Khan},
keywords = {Big Data, Big Data model, Big Data governance, Data management, Big Data governance framework, Big Data analytic},
abstract = {The recent explosion in ICT and digital data has led organizations, both private and public, to efficient decision-making. Nowadays organizations can store huge amounts of data, which can be accessible at any time. Big Data governance refers to the management of huge volumes of an organization’s data, exploiting it in the organization’s decision-making using different analytical tools. Big Data emergence provides great convenience, but it also brings challenges. Nevertheless, for Big Data governance, data has to be prepared in a timely manner, keeping in view the consistency and reliability of the data, and being able to trust its source and the meaningfulness of the result. Hence, a framework for Big Data governance would have many advantages. There are Big Data governance frameworks, which guide the management of Big Data. However, there are also limitations associated with these frameworks. Therefore, this study aims to explore the existing Big Data governance frameworks and their shortcomings, and propose a new framework. The proposed framework consists of eight components. As a framework validation, the proposed framework has been compared with the ISO 8000 data governance framework.}
}
@article{HUANG2018165,
title = {A technology delivery system for characterizing the supply side of technology emergence: Illustrated for Big Data & Analytics},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {165-176},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517311927},
author = {Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu},
keywords = {Technology delivery system, Tech mining, Emerging technology, Big Data, Technology assessment, Impact assessment},
abstract = {While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (“BDA”).}
}
@article{ZAMAN2017537,
title = {Challenges and Opportunities of Big Data Analytics for Upcoming Regulations and Future Transformation of the Shipping Industry},
journal = {Procedia Engineering},
volume = {194},
pages = {537-544},
year = {2017},
note = {10th International Conference on Marine Technology, MARTEC 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.182},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817333386},
author = {Ibna Zaman and Kayvan Pazouki and Rose Norman and Shervin Younessi and Shirley Coleman},
keywords = {Carbon emission, data-oriented, MRV, big data},
abstract = {Shipping is a heavily regulated industry and responsible for around 3% of global carbon emissions. Global trade is highly dependent on shipping which covers around 90% of commercial demand. Now the industry is expected to navigate through many twists and turns of different situations like upcoming regulations, climate change, energy shortages and technological revolutions. Technological development is apparent across all marine sectors due to the rapid development of sensor technology, IT, automation and robotics. The industry must continue to develop at a rapid pace over the next decade in order to be able to adapt to upcoming regulations and market pressure. Ship intelligence will be the driving force shaping the future of the industry. Ships generate a large volume of data from different sources and in different formats. So big data has become the talk of the industry nowadays. Big data analysis discovers correlations between different measurable or unmeasurable parameters to determine hidden patterns and trends. This analysis will have a significant impact on vessel performance monitoring and provide performance prediction, real-time transparency, and decision-making support to the ship operator. Big data will also bring new opportunities and challenges for the maritime industry. It will increase the capability of performance monitoring, remove human error and increase interdependencies of components. However, the industry will have to face many challenges such as data processing, reliability, and data security. Many regulations rely on ship data including the new EU MRV (Monitoring, Reporting and Verification) regulation to quantify the CO2 emissions for ships above 5000 gross tonnage. As a result, ship operators will have to monitor and report the verified amount of CO2 emitted by their vessels on voyages to, from and between EU ports and will also be required to provide information on energy efficiency parameters. The MRV is a data-oriented regulation requiring ship operators to capture and monitor the ship emissions and other related data and although it is a regional regulation at the moment there is scope for the International Maritime Organisation (IMO) to implement it globally in the near future.}
}
@article{LIU201797,
title = {Practical-oriented protocols for privacy-preserving outsourced big data analysis: Challenges and future research directions},
journal = {Computers & Security},
volume = {69},
pages = {97-113},
year = {2017},
note = {Security Data Science and Cyber Threat Management},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2016.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167404816301778},
author = {Zhe Liu and Kim-Kwang Raymond Choo and Minghao Zhao},
keywords = {Big data analysis, Privacy-preserving, Outsourced big data, Oblivious RAM, Security, Practical-oriented, Secure query},
abstract = {With the significant increase in the volume, variety, velocity and veracity of data generated, collected and transmitted through computing and networking systems, it is of little surprise that big data analysis and processing is the subject of focus from enterprise, academia and government. Outsourcing is one popular solution considered in big data processing, although security and privacy are two key concerns often attributed to the underutilization of outsourcing and other promising big data analysis and processing technologies. In this paper, we survey the state-of-the-art literature on cryptographic solutions designed to ensure the security and/or privacy in big data outsourcing. For example, we provide concrete examples to explain how these cryptographic solutions can be deployed. We summarize the existing state-of-play before discussing research opportunities.}
}
@article{GU201722,
title = {Visualizing the knowledge structure and evolution of big data research in healthcare informatics},
journal = {International Journal of Medical Informatics},
volume = {98},
pages = {22-32},
year = {2017},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1386505616302556},
author = {Dongxiao Gu and Jingjing Li and Xingguo Li and Changyong Liang},
keywords = {Big data, Healthcare informatics, Bibliometrics, Knowledge structure, Knowledge management},
abstract = {Background
In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.
Methods
To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers’ production trends in the field and the trend of each paper’s co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.
Results
By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People’s Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).
Conclusion
This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.}
}
@article{ARDAGNA2018548,
title = {Context-aware data quality assessment for big data},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {548-562},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329151},
author = {Danilo Ardagna and Cinzia Cappiello and Walter Samá and Monica Vitali},
keywords = {Data quality, Big data, Context-awareness, Data profiling, DQ assessment},
abstract = {Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.}
}
@article{SANTOS2017750,
title = {A Big Data system supporting Bosch Braga Industry 4.0 strategy},
journal = {International Journal of Information Management},
volume = {37},
number = {6},
pages = {750-760},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217306023},
author = {Maribel Yasmina Santos and Jorge {Oliveira e Sá} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and João Galvão},
keywords = {Big Data, Industry 4.0, Big Data analytics, Big Data architecture, Bosch},
abstract = {People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia – Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).}
}
@article{LI2018301,
title = {Big data in tourism research: A literature review},
journal = {Tourism Management},
volume = {68},
pages = {301-323},
year = {2018},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2018.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0261517718300591},
author = {Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li},
keywords = {Tourism research, Big data, Literature review, Tourism management, Tourist behavior},
abstract = {Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.}
}
@article{DECAMARGOFIORINI2018112,
title = {Management theory and big data literature: From a review to a research agenda},
journal = {International Journal of Information Management},
volume = {43},
pages = {112-129},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830553X},
author = {Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour}},
keywords = {Big data, Big data analytics, Organizational theory, Firms’ performance, Research agenda},
abstract = {The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.}
}
@article{VANDENBROEK2018330,
title = {Governance of big data collaborations: How to balance regulatory compliance and disruptive innovation},
journal = {Technological Forecasting and Social Change},
volume = {129},
pages = {330-338},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517314695},
author = {Tijs {van den Broek} and Anne Fleur {van Veenstra}},
keywords = {Disruptive innovation, Data protection regulation, Big data, Governance, Inter-organizational collaboration},
abstract = {Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.}
}
@incollection{WICKERSHAM202359,
title = {Chapter 3 - Clinical applications of big data to child and adolescent mental health care},
editor = {Matthew Hodes and Petrus J. {De Vries}},
booktitle = {Shaping the Future of Child and Adolescent Mental Health},
publisher = {Academic Press},
pages = {59-79},
year = {2023},
isbn = {978-0-323-91709-4},
doi = {https://doi.org/10.1016/B978-0-323-91709-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917094000056},
author = {Alice Wickersham and Johnny Downs},
keywords = {Big data, Child and adolescent mental health, Data linkage, Electronic healthcare records, Natural language processing},
abstract = {Over the past twodecades, healthcare providers across the world have adopted digital methods for capturing clinical and administrative information. Clinicians take contemporaneous records of their interactions with patients, so many health service providers have accrued vast repositories of longitudinally collected data. These data, coupled with advances in data extraction methods, computer processing power, and linkage to nonhealth public services data, now provide child and adolescent mental health researchers unique opportunities for tackling a broad range of clinical questions; especially those where the considerations of scale and generalizability make individually funded studies unaffordable. However, these “big” data have their limitations. Best practice requires clinicians, informaticians, and data scientists to work together, so assumptions over data quality or validity are not misplaced. This chapter explains why the evidence base for child and adolescent mental healthcare needs big data applications as well as conventional research, to move the field forward. This chapter provides illustrations of big data applications to child and adolescent mental healthcare, primarily from England and the United Kingdom, but also offers a section on the global perspective. This chapter also reviews the methodological strengths and weaknesses of big data and describes the ethical and governance implications for their use.}
}
@article{ARUNACHALAM2018416,
title = {Understanding big data analytics capabilities in supply chain management: Unravelling the issues, challenges and implications for practice},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {416-436},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1366554516303799},
author = {Deepak Arunachalam and Niraj Kumar and John Paul Kawalek},
keywords = {Supply chain management, Big data analytics, Capabilities, Maturity model},
abstract = {In the era of Big Data, many organisations have successfully leveraged Big Data Analytics (BDA) capabilities to improve their performance. However, past literature on BDA have put limited focus on understanding the capabilities required to extract value from big data. In this context, this paper aims to provide a systematic literature review of BDA capabilities in supply chain and develop the capabilities maturity model. The paper presents the bibliometric and thematic analysis of research papers from 2008 to 2016. This paper contributes in theorizing BDA capabilities in context of supply chain, and provides future direction of research in this field.}
}
@article{BRADLOW201779,
title = {The Role of Big Data and Predictive Analytics in Retailing},
journal = {Journal of Retailing},
volume = {93},
number = {1},
pages = {79-95},
year = {2017},
note = {The Future of Retailing},
issn = {0022-4359},
doi = {https://doi.org/10.1016/j.jretai.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022435916300835},
author = {Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti},
keywords = {Big data, Predictive analytics, Retailing, Pricing},
abstract = {The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions—data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.}
}
@article{CUQUET201874,
title = {The societal impact of big data: A research roadmap for Europe},
journal = {Technology in Society},
volume = {54},
pages = {74-86},
year = {2018},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X17300131},
author = {Martí Cuquet and Anna Fensel},
keywords = {Big data, Research roadmap, Societal externalities, Skills development, Standardisation},
abstract = {With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.}
}
@article{YADEGARIDEHKORDI2018199,
title = {Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {199-210},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.07.043},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518304141},
author = {Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim},
keywords = {Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS},
abstract = {Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.}
}
@article{LABRIE201845,
title = {Big data analytics sentiment: US-China reaction to data collection by business and government},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {45-55},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517308612},
author = {Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier},
keywords = {Big data ethics, Business data usage, Corporate data collection, Government data usage, Technology ethics, US-China similarities, US-China differences},
abstract = {As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.}
}
@article{NIMMAGADDA2018143,
title = {On big data-guided upstream business research and its knowledge management},
journal = {Journal of Business Research},
volume = {89},
pages = {143-158},
year = {2018},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318302054},
author = {Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood},
keywords = {Upstream business, Heterogeneous and multidimensional data, Data warehousing and mining, Big Data paradigm, Spatial-temporal dimensions},
abstract = {The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.}
}
@article{KORTESNIEMI201890,
title = {The European Federation of Organisations for Medical Physics (EFOMP) White Paper: Big data and deep learning in medical imaging and in relation to medical physics profession},
journal = {Physica Medica},
volume = {56},
pages = {90-93},
year = {2018},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718313152},
author = {Mika Kortesniemi and Virginia Tsapaki and Annalisa Trianni and Paolo Russo and Ad Maas and Hans-Erik Källman and Marco Brambilla and John Damilakis},
abstract = {Big data and deep learning will profoundly change various areas of professions and research in the future. This will also happen in medicine and medical imaging in particular. As medical physicists, we should pursue beyond the concept of technical quality to extend our methodology and competence towards measuring and optimising the diagnostic value in terms of how it is connected to care outcome. Functional implementation of such methodology requires data processing utilities starting from data collection and management and culminating in the data analysis methods. Data quality control and validation are prerequisites for the deep learning application in order to provide reliable further analysis, classification, interpretation, probabilistic and predictive modelling from the vast heterogeneous big data. Challenges in practical data analytics relate to both horizontal and longitudinal analysis aspects. Quantitative aspects of data validation, quality control, physically meaningful measures, parameter connections and system modelling for the future artificial intelligence (AI) methods are positioned firmly in the field of Medical Physics profession. It is our interest to ensure that our professional education, continuous training and competence will follow this significant global development.}
}
@article{RIGGINS201723,
title = {Data governance case at KrauseMcMahon LLP in an era of self-service BI and Big Data},
journal = {Journal of Accounting Education},
volume = {38},
pages = {23-36},
year = {2017},
note = {Special Issue on Big Data},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301208},
author = {Frederick J. Riggins and Bonnie K. Klamm},
keywords = {Big Data, Data governance, Self-service business intelligence},
abstract = {This case increases your understanding of data governance in an era of sophisticated analytics and Big Data where corporate data integrity and data quality may be at risk. KrauseMcMahon, a large certified public accounting and business consulting firm, faces a tradeoff of increasing control of the company’s data assets versus unleashing end user innovation due to the proliferation of self-service business intelligence tools. You are required to analyze the issues in the case from organizational, financial, and technical perspectives to propose alternatives the organization should consider and make specific recommendations on how the company should proceed. By completing this case, you will demonstrate cross-disciplinary abilities related to foundational business, accounting, and broad management competencies. By addressing such competencies, the case requires your use of accounting, MIS, and upper-level business skills.}
}
@article{MCKINNEY201763,
title = {The need for ‘skeptical’ accountants in the era of Big Data},
journal = {Journal of Accounting Education},
volume = {38},
pages = {63-80},
year = {2017},
note = {Special Issue on Big Data},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301051},
author = {Earl McKinney and Charles J. Yoos and Ken Snead},
keywords = {Big Data, Analysis, Informed skepticism, Questioning},
abstract = {Big Data is now readily available for analysis, and analysts trained to conduct effective analysis in this area are in high demand. However, there is a dearth of discussion in the literature related to identifying the important cognitive skills required for accountants to conduct effective Big Data analysis. Here we argue that accountants need to approach Big Data analysis as informed skeptics, being ever ready to challenge the analysis by asking good questions in appropriate topical areas. These areas include understanding the limits of measurement and representation, the subjectiveness of insight, the challenges of statistics and integrating data sets, and the effects of underdetermination and inductive reasoning. Accordingly, we develop a framework and an illustrative example to facilitate the training of accounting students to become informed skeptics in the era of Big Data by explaining the conceptual relevance of each of the topical areas to Big Data analysis. In addition, example questions are identified that accountants conducting Big Data analysis should be asking regarding each topic. Further, for each topic, references to additional resources are provided that students can access to learn more about effectively conducting Big Data analysis.}
}
@article{WANG2017287,
title = {Exploring the path to big data analytics success in healthcare},
journal = {Journal of Business Research},
volume = {70},
pages = {287-299},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304891},
author = {Yichuan Wang and Nick Hajli},
keywords = {Big data analytics, Business value, Capability building view, Resource-based theory, Information technology source management, Health care industries},
abstract = {Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.}
}
@article{MANTELERO2017584,
title = {Regulating big data. The guidelines of the Council of Europe in the context of the European data protection framework},
journal = {Computer Law & Security Review},
volume = {33},
number = {5},
pages = {584-602},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917301644},
author = {Alessandro Mantelero},
keywords = {Big data, Data protection, Council of Europe, Risk assessment, Data protection by design, Consent, Data anonymization, Open data, Algorithms},
abstract = {In January 2017 the Consultative Committee of Convention 108 adopted its Guidelines on the Protection of Individuals with Regard to the Processing of Personal Data in a World of Big Data. These are the first guidelines on data protection provided by an international body which specifically address the issues surrounding big data applications. This article examines the main provisions of these Guidelines and highlights the approach adopted by the Consultative Committee, which contextualises the traditional principles of data protection in the big data scenario and also takes into account the challenges of the big data paradigm. The analysis of the different provisions adopted focuses primarily on the core of the Guidelines namely the risk assessment procedure. Moreover, the article discusses the novel solutions provided by the Guidelines with regard to the data subject's informed consent, the by-design approach, anonymization, and the role of the human factor in big data-supported decisions. This critical analysis of the Guidelines introduces a broader reflection on the divergent approaches of the Council of Europe and the European Union to regulating data processing. Where the principle-based model of the Council of Europe differs from the approach adopted by the EU legislator in the detailed Regulation (EU) 2016/679. In the light of this, the provisions of the Guidelines and their attempt to address the major challenges of the new big data paradigm set the stage for concluding remarks about the most suitable regulatory model to deal with the different issues posed by the development of technology.}
}
@article{GOVINDAN2018343,
title = {Big data analytics and application for logistics and supply chain management},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {343-349},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1366554518302606},
author = {Kannan Govindan and T.C.E. Cheng and Nishikant Mishra and Nagesh Shukla},
keywords = {Big data analytics, Supply chain management, Logistics},
abstract = {This special issue explores big data analytics and applications for logistics and supply chain management by examining novel methods, practices, and opportunities. The articles present and analyse a variety of opportunities to improve big data analytics and applications for logistics and supply chain management, such as those through exploring technology-driven tracking strategies, financial performance relations with data driven supply chains, and implementation issues and supply chain capability maturity with big data. This editorial note summarizes the discussions on the big data attributes, on effective practices for implementation, and on evaluation and implementation methods.}
}
@article{BROEDERS2017309,
title = {Big Data and security policies: Towards a framework for regulating the phases of analytics and use of Big Data},
journal = {Computer Law & Security Review},
volume = {33},
number = {3},
pages = {309-323},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917300675},
author = {Dennis Broeders and Erik Schrijvers and Bart {van der Sloot} and Rosamunde {van Brakel} and Josta {de Hoog} and Ernst {Hirsch Ballin}},
keywords = {Big Data, Security, Data protection, Privacy, Regulation, Fraud, Policing, Surveillance, Algorithmic accountability, the Netherlands},
abstract = {Big Data analytics in national security, law enforcement and the fight against fraud have the potential to reap great benefits for states, citizens and society but require extra safeguards to protect citizens' fundamental rights. This involves a crucial shift in emphasis from regulating Big Data collection to regulating the phases of analysis and use. In order to benefit from the use of Big Data analytics in the field of security, a framework has to be developed that adds new layers of protection for fundamental rights and safeguards against erroneous and malicious use. Additional regulation is needed at the levels of analysis and use, and the oversight regime is in need of strengthening. At the level of analysis – the algorithmic heart of Big Data processes – a duty of care should be introduced that is part of an internal audit and external review procedure. Big Data projects should also be subject to a sunset clause. At the level of use, profiles and (semi-) automated decision-making should be regulated more tightly. Moreover, the responsibility of the data processing party for accuracy of analysis – and decisions taken on its basis – should be anchored in legislation. The general and security-specific oversight functions should be strengthened in terms of technological expertise, access and resources. The possibilities for judicial review should be expanded to stimulate the development of case law.}
}
@article{SMIRNOVA201825,
title = {A practical guide to big data},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {25-29},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300592},
author = {Ekaterina Smirnova and Andrada Ivanescu and Jiawei Bai and Ciprian M. Crainiceanu},
keywords = {Big data, Wearable and implantable computing, Accelerometer},
abstract = {Big Data is increasingly prevalent in science and data analysis. We provide a short tutorial for adapting to these changes and making the necessary adjustments to the academic culture to keep Biostatistics truly impactful in scientific research.}
}
@article{MOGHADAM2018401,
title = {Information security governance in big data environments: A systematic mapping},
journal = {Procedia Computer Science},
volume = {138},
pages = {401-408},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316909},
author = {Reza Saneei Moghadam and Ricardo Colomo-Palacios},
keywords = {information security governance, big data, framework, systematic mapping},
abstract = {Information security governance is an important aspects for all organizations. Given the crucial importance of IT systems and the increasing range of threats these systems are facing, there is an increasing interest on the topic. On the other hand, Big Data environments are also beginning to be more pervasive as IT is increasing its importance for organizations worldwide. In order to better know which aspects are the most important for the intersection of Big Data and information security governance, authors present in this paper a systematic mapping on this topic. Authors illustrate challenges and gaps concerning the topic and clarify these challenges by means of a classification of the environments they take place, the security risk spectrums they concern, and the security governance measures they take to mitigate them; by providing solutions as in a framework, model, software or tool, wherever possible. Results are expected to be useful for IT security professionals and information systems practitioners as a whole.}
}
@article{ZAIN2018140,
title = {Big Data Analytics based on PANFIS MapReduce},
journal = {Procedia Computer Science},
volume = {144},
pages = {140-152},
year = {2018},
note = {INNS Conference on Big Data and Deep Learning},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.514},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918322233},
author = {Choiru Za’in and Mahardhika Pratama and Edwin Lughofer and Meftahul Ferdaus and Qing Cai and Mukesh Prasad},
keywords = {Big data stream analytic, Distributed evolving algorithm, Scalable real-time data mining, Parallel learning, Rule merging strategy},
abstract = {In this paper, a big data analytic framework is introduced for processing high-frequency data stream. This framework architecture is developed by combining an advanced evolving learning algorithm namely Parsimonious Network Fuzzy Inference System (PANFIS) with MapReduce parallel computation, where PANFIS has the capability of processing data stream in large volume. Big datasets are learnt chunk by chunk by processors in MapReduce environment and the results are fused by rule merging method, that reduces the complexity of the rules. The performance measurement has been conducted, and the results are showing that the MapReduce framework along with PANFIS evolving system helps to reduce the processing time around 22 percent in average in comparison with the PANFIS algorithm without reducing performance in accuracy.}
}
@article{YAO20181,
title = {Big data quality prediction in the process industry: A distributed parallel modeling framework},
journal = {Journal of Process Control},
volume = {68},
pages = {1-13},
year = {2018},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418300660},
author = {Le Yao and Zhiqiang Ge},
keywords = {Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics},
abstract = {With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.}
}
@article{ELOUAZZANI201852,
title = {A new technique ensuring privacy in big data: K-anonymity without prior value of the threshold k},
journal = {Procedia Computer Science},
volume = {127},
pages = {52-59},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S187705091830108X},
author = {Zakariae {El Ouazzani} and Hanan {El Bakkali}},
keywords = {k-anonymity, quasi identifier attributes, big data, anonymization, privacy},
abstract = {Big data has become omnipresent and crucial for many application domains. Big data makes reference to the explosive quantity of data generated in today’s society that might contain personally identifiable information (PII). That’s why the challenge from the point of view of data privacy is one of the major hurdles for the application of big data. In that situation, several techniques were exposed in order to ensure privacy in big data including generalization, randomization and cryptographic techniques as well. It is well known that there exist two main types of attributes in the literature, quasi identifier and sensitive attributes. In this paper, we are going to focus on quasi identifier attributes. Over the years, k-anonymity has been treated with great interest as an anonymization technique ensuring privacy in big data when we are dealing with quasi identifier attributes. Despite the fact that many algorithms of k-anonymity have been proposed, most of them admit that the threshold k of k-anonymity has to be known before anonymizing the data set. Here, a novel way in applying k-anonymity for quasi identifier attributes is presented. It’s a new algorithm called “k-anonymity without prior value of the threshold k”. Our proposed algorithm was experimentally evaluated using a test table of quasi identifier attributes. Furthermore, we highlight all the steps of our proposed algorithm with detailed comments.}
}
@article{LEE2017293,
title = {Big data: Dimensions, evolution, impacts, and challenges},
journal = {Business Horizons},
volume = {60},
number = {3},
pages = {293-303},
year = {2017},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681317300046},
author = {In Lee},
keywords = {Big data, Internet of things, Data analytics, Sentiment analysis, Social network analysis, Web analytics},
abstract = {Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.}
}
@article{RAMBUR2018176,
title = {A plea to nurse educators: Incorporate big data use as a foundational skill for undergraduate and graduate nurses},
journal = {Journal of Professional Nursing},
volume = {34},
number = {3},
pages = {176-181},
year = {2018},
issn = {8755-7223},
doi = {https://doi.org/10.1016/j.profnurs.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S8755722316303283},
author = {Betty Rambur and Therese Fitzpatrick}
}
@article{KHENNOU201860,
title = {Improving the Use of Big Data Analytics within Electronic Health Records: A Case Study based OpenEHR},
journal = {Procedia Computer Science},
volume = {127},
pages = {60-68},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301091},
author = {Fadoua Khennou and Youness Idrissi Khamlichi and Nour El Houda Chaoui},
keywords = {Electronic Health Records, EHRs, Analytic tools, Big Data, Health Practitioners},
abstract = {Recently there has been an increasing adoption of electronic health records (EHRs) in different countries. Thanks to these systems, multiple health bodies can now store, manage and process their data effectively. However, the existence of such powerful and meticulous entities raise new challenges and issues for health practitioners. In fact, while the main objective of EHRs is to gain actionable big data insights from the health workflow, very few physicians exploit widely analytic tools, this is mainly due to the fact of having to deal with multiple systems and steps, which completely discourage them from engaging more and more. In this paper, we shed light and explore precisely the proper adaptation of analytical tools to EHRs in order to upgrade their use by health practitioners. For that, we present a case study of the implementation process of an EHR based OpenEHR and investigate health analytics adoption in each step of the methodology.}
}
@article{WANG201864,
title = {An integrated big data analytics-enabled transformation model: Application to health care},
journal = {Information & Management},
volume = {55},
number = {1},
pages = {64-79},
year = {2018},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617303129},
author = {Yichuan Wang and LeeAnn Kung and William Yu Chung Wang and Casey G. Cegielski},
keywords = {Big data analytics, IT-enabled transformation, Practice-based view, Business value of IT, Healthcare, Content analysis},
abstract = {A big data analytics-enabled transformation model based on practice-based view is developed, which reveals the causal relationships among big data analytics capabilities, IT-enabled transformation practices, benefit dimensions, and business values. This model was then tested in healthcare setting. By analyzing big data implementation cases, we sought to understand how big data analytics capabilities transform organizational practices, thereby generating potential benefits. In addition to conceptually defining four big data analytics capabilities, the model offers a strategic view of big data analytics. Three significant path-to-value chains were identified for healthcare organizations by applying the model, which provides practical insights for managers.}
}
@article{JI2017187,
title = {Big data analytics based fault prediction for shop floor scheduling},
journal = {Journal of Manufacturing Systems},
volume = {43},
pages = {187-194},
year = {2017},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2017.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612517300389},
author = {Wei Ji and Lihui Wang},
keywords = {Big data analytics, Fault prediction, Shop floor, Scheduling},
abstract = {The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.}
}
@article{NADAL201775,
title = {A software reference architecture for semantic-aware Big Data systems},
journal = {Information and Software Technology},
volume = {90},
pages = {75-92},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304287},
author = {Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio},
keywords = {Big Data, Software reference architecture, Semantic-aware, Data management, Data analysis},
abstract = {Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.}
}
@article{AHMED2017459,
title = {The role of big data analytics in Internet of Things},
journal = {Computer Networks},
volume = {129},
pages = {459-471},
year = {2017},
note = {Special Issue on 5G Wireless Networks for IoT and Body Sensors},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2017.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1389128617302591},
author = {Ejaz Ahmed and Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Imran Khan and Abdelmuttlib Ibrahim Abdalla Ahmed and Muhammad Imran and Athanasios V. Vasilakos},
keywords = {Internet of Things, Big data, Analytics, Distributed computing, Smart city},
abstract = {The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.}
}
@article{LEFEVRE20181,
title = {Big data in forensic science and medicine},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {1-6},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X17301154},
author = {Thomas Lefèvre},
keywords = {Forensic science, Big data, Personalized medicine, Predictive medicine, Machine learning, Dimensionality},
abstract = {In less than a decade, big data in medicine has become quite a phenomenon and many biomedical disciplines got their own tribune on the topic. Perspectives and debates are flourishing while there is a lack for a consensual definition for big data. The 3Vs paradigm is frequently evoked to define the big data principles and stands for Volume, Variety and Velocity. Even according to this paradigm, genuine big data studies are still scarce in medicine and may not meet all expectations. On one hand, techniques usually presented as specific to the big data such as machine learning techniques are supposed to support the ambition of personalized, predictive and preventive medicines. These techniques are mostly far from been new and are more than 50 years old for the most ancient. On the other hand, several issues closely related to the properties of big data and inherited from other scientific fields such as artificial intelligence are often underestimated if not ignored. Besides, a few papers temper the almost unanimous big data enthusiasm and are worth attention since they delineate what is at stakes. In this context, forensic science is still awaiting for its position papers as well as for a comprehensive outline of what kind of contribution big data could bring to the field. The present situation calls for definitions and actions to rationally guide research and practice in big data. It is an opportunity for grounding a true interdisciplinary approach in forensic science and medicine that is mainly based on evidence.}
}
@article{KHAN2017923,
title = {A survey on scholarly data: From big data perspective},
journal = {Information Processing & Management},
volume = {53},
number = {4},
pages = {923-944},
year = {2017},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316305994},
author = {Samiya Khan and Xiufeng Liu and Kashish A. Shakil and Mansaf Alam},
keywords = {Scholarly data, Big data, Cloud-based big data analytics, Big scholarly data, Big data platform, Scholarly applications},
abstract = {Recently, there has been a shifting focus of organizations and governments towards digitization of academic and technical documents, adding a new facet to the concept of digital libraries. The volume, variety and velocity of this generated data, satisfies the big data definition, as a result of which, this scholarly reserve is popularly referred to as big scholarly data. In order to facilitate data analytics for big scholarly data, architectures and services for the same need to be developed. The evolving nature of research problems has made them essentially interdisciplinary. As a result, there is a growing demand for scholarly applications like collaborator discovery, expert finding and research recommendation systems, in addition to several others. This research paper investigates the current trends and identifies the existing challenges in development of a big scholarly data platform, with specific focus on directions for future research and maps them to the different phases of the big data lifecycle.}
}
@article{MAUDSLEY2018961,
title = {Intelligent and effective informatic deconvolution of “Big Data” and its future impact on the quantitative nature of neurodegenerative disease therapy},
journal = {Alzheimer's & Dementia},
volume = {14},
number = {7},
pages = {961-975},
year = {2018},
issn = {1552-5260},
doi = {https://doi.org/10.1016/j.jalz.2018.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S1552526018300402},
author = {Stuart Maudsley and Viswanath Devanarayan and Bronwen Martin and Hugo Geerts},
keywords = {Big data, Informatics, High-dimensionality, Alzheimer's disease, Aging, Molecular signature, Transcriptomics, Metabolomics, Proteomics, Genomics},
abstract = {Biomedical data sets are becoming increasingly larger and a plethora of high-dimensionality data sets (“Big Data”) are now freely accessible for neurodegenerative diseases, such as Alzheimer's disease. It is thus important that new informatic analysis platforms are developed that allow the organization and interrogation of Big Data resources into a rational and actionable mechanism for advanced therapeutic development. This will entail the generation of systems and tools that allow the cross-platform correlation between data sets of distinct types, for example, transcriptomic, proteomic, and metabolomic. Here, we provide a comprehensive overview of the latest strategies, including latent semantic analytics, topological data investigation, and deep learning techniques that will drive the future development of diagnostic and therapeutic applications for Alzheimer's disease. We contend that diverse informatic “Big Data” platforms should be synergistically designed with more advanced chemical/drug and cellular/tissue-based phenotypic analytical predictive models to assist in either de novo drug design or effective drug repurposing.}
}
@article{KOSELEVA2017544,
title = {Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges},
journal = {Procedia Engineering},
volume = {172},
pages = {544-549},
year = {2017},
note = {Modern Building Materials, Structures and Techniques},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.02.064},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817305702},
author = {Natalija Koseleva and Guoda Ropaite},
keywords = {Energy Efficiency, Big Data, Characteristics of Big Data, Big Data analysis, Construction, Web of Science},
abstract = {Data generation has increased drastically over the past few years. Data management has also grown in importance because extracting the significant value out of a huge pile of raw data is of prime important thing to make different decisions. One of the important sectors nowadays is construction sector, especially building energy efficiency field. Collecting big amount of data, using different kinds of big data analysis can help to improve construction process from the energy efficiency perspective. This article reviews the understanding of Big Data, methods used for Big Data analysis and the main problems with Big Data in the field of energy.}
}
@incollection{LOPES2017167,
title = {Chapter 10 - Big Data: A Practitioners Perspective},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {167-179},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000107},
author = {Darshan Lopes and Kevin Palmer and Fiona O'Sullivan},
keywords = {Pitfalls, Considerations, Implementation, Migration pattern, Practitioner's perspective, Open source, Data warehouse},
abstract = {Big data solutions represent a significant challenge for some organizations. There are a huge variety of software products, deployment patterns and solution options that need to be considered to ensure a successful outcome for an organization trying to implement a big data solution. With that in mind, the chapter “Big Data: a practitioner's perspective” will focus on four key areas associated with big data that require consideration from a practical and implementation perspective: (i) Big Data is a new Paradigm – Differences with Traditional Data Warehouse, Pitfalls and Considerations; (ii) Product considerations for Big Data – Use of Open Source products for Big Data, Pitfalls and Considerations; (iii) Use of Cloud for hosting Big Data – Why use Cloud, Pitfalls and Considerations; and (iv) Big Data Implementation – Architecture definition, processing framework and migration patterns from Data Warehouse to Big Data.}
}
@article{BROTHERS201884,
title = {Integrity, standards, and QC-related issues with big data in pre-clinical drug discovery},
journal = {Biochemical Pharmacology},
volume = {152},
pages = {84-93},
year = {2018},
issn = {0006-2952},
doi = {https://doi.org/10.1016/j.bcp.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0006295218301199},
author = {John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko},
keywords = {Big data, Genomics, Transcriptomics, RNA-seq, Microarray, Exome},
abstract = {The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.}
}
@article{SHARPLES2018105,
title = {The role of statistics in the era of big data: Electronic health records for healthcare research},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {105-110},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300890},
author = {Linda D. Sharples},
keywords = {Electronic healthcare records},
abstract = {The transferring of medical records into huge electronic databases has opened up opportunities for research but requires attention to data quality, study design and issues of bias and confounding.}
}
@article{WORDSWORTH20181048,
title = {Using “Big Data” in the Cost-Effectiveness Analysis of Next-Generation Sequencing Technologies: Challenges and Potential Solutions},
journal = {Value in Health},
volume = {21},
number = {9},
pages = {1048-1053},
year = {2018},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2018.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1098301518322654},
author = {Sarah Wordsworth and Brett Doble and Katherine Payne and James Buchanan and Deborah A. Marshall and Christopher McCabe and Dean A. Regier},
keywords = {Big data, cost-effectiveness, next generation sequencing},
abstract = {Next-generation sequencing (NGS) is considered to be a prominent example of “big data” because of the quantity and complexity of data it produces and because it presents an opportunity to use powerful information sources that could reduce clinical and health economic uncertainty at a patient level. One obstacle to translating NGS into routine health care has been a lack of clinical trials evaluating NGS technologies, which could be used to populate cost-effectiveness analyses (CEAs). A key question is whether big data can be used to partially support CEAs of NGS. This question has been brought into sharp focus with the creation of large national sequencing initiatives. In this article we summarize the main methodological and practical challenges of using big data as an input into CEAs of NGS. Our focus is on the challenges of using large observational datasets and cohort studies and linking these data to the genomic information obtained from NGS, as is being pursued in the conduct of large genomic sequencing initiatives. We propose potential solutions to these key challenges. We conclude that the use of genomic big data to support and inform CEAs of NGS technologies holds great promise. Nevertheless, health economists face substantial challenges when using these data and must be cognizant of them before big data can be confidently used to produce evidence on the cost-effectiveness of NGS.}
}
@article{VERMA2018791,
title = {An extension of the technology acceptance model in the big data analytics system implementation environment},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {791-806},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317300043},
author = {Surabhi Verma and Som Sekhar Bhattacharyya and Saurav Kumar},
keywords = {Technology acceptance model, Big data analytics system, System quality, Information quality},
abstract = {Research on the adoption of systems for big data analytics has drawn enormous attention in Information Systems research. This study extends big data analytics adoption research by examining the effects of system characteristics on the attitude of managers towards the usage of big data analytics systems. A research model has been proposed in this study based on an extensive review of literature pertaining to the Technology Acceptance Model, with further validation by a survey of 150 big data analytics users. Results of this survey confirm that characteristics of the big data analytics system have significant direct and indirect effects on belief in the benefits of big data analytics systems and perceived usefulness, attitude and adoption. Moreover, there are mediation effects that exist among the system characteristics, benefits of big data analytics systems, perceived usefulness and the attitude towards using big data analytics system. This study expands the existing body of knowledge on the adoption of big data analytics systems, and benefits big data analytics providers and vendors while helping in the formulation of their business models.}
}
@article{KAUR20181049,
title = {Big Data and Machine Learning Based Secure Healthcare Framework},
journal = {Procedia Computer Science},
volume = {132},
pages = {1049-1059},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S187705091830752X},
author = {Prableen Kaur and Manik Sharma and Mamta Mittal},
keywords = {Big Data, Healthcare, Big data analytics, disease diagnosis, predictive analysis, security, privacy},
abstract = {The paper presents a brief introduction to big data and its role in healthcare applications. It is observed that the use of big data architecture and techniques are continuously assisting in managing the expeditious data growth in healthcare industry. Here, initially an empirical study is performed to analyze the role of big data in healthcare industry. It has been observed that significant work has been done using big data in healthcare sector. Nowadays, it is intricate to envision the way the machine learning and big data can influence the healthcare industries. It has been observed that most of the authors who implemented the use of machine learning and big data analytics in disease diagnosis have not given significant weightage to the privacy and security of the data. Here, a novel design of smart and secure healthcare information system using machine learning and advanced security mechanism has been proposed to handle big data of medical industry. The innovation lies in the incorporation of optimal storage and data security layer used to maintain security and privacy. Different techniques like masking encryption, activity monitoring, granular access control, dynamic data encryption and end point validation have been incorporated. The proposed hybrid four layer healthcare model seems to be more effective disease diagnostic big data system.}
}
@article{OUYANG201860,
title = {Methodologies, principles and prospects of applying big data in safety science research},
journal = {Safety Science},
volume = {101},
pages = {60-71},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517304320},
author = {Qiumei Ouyang and Chao Wu and Lang Huang},
keywords = {Safety big data (SBD), Safety small data (SSD), Safety science, Big data application, Method, Principle, Prospect and challenge},
abstract = {It is clear that big data has numerous potential impacts in many fields. However, few papers discussed its applications in the field of safety science research. Additionally, there exist many problems that cannot be ignored when big data is applied to safety science, most outstanding of which is lack of universal supporting theory that guides how to apply big data to safety science research like methods, principles and approaches, etc. In other terms, it is not enough for big data to be viewed asa strong enabler for safety science applications mainly due to lack of universal and basic theory from the perspective of safety science. Considering the above analyzes, the two key objectives of this paper are: (1) to propose the connotation of safety big data (SBD) and its applying rules, methods and principles, and (2) to put forward some application prospects and challenges of big data to safety science research seen from theoretical research. First, by comparing SBD and traditional safety small data (SSD) from four aspects including theoretical research, typical research method, specific analysis method and processing mode, this paper puts forward the definition and connotation of SBD. Subsequently this paper further summarizes and extracts the application rules and methods of SBD. And then nine principles of SBD are explored and their relationship and application are addressed from the view of theory architecture and working framework in data processing flow. At last, this paper also discusses the potential applications and some hot issues of SBD. Overall, this paper will play an essential role in supporting the application of SBD. In addition, it will fill in the theory gaps in the field of SBD beyond traditional safety statistics, and further enriches the contents of safety science.}
}
@article{CHATFIELD2018336,
title = {Customer agility and responsiveness through big data analytics for public value creation: A case study of Houston 311 on-demand services},
journal = {Government Information Quarterly},
volume = {35},
number = {2},
pages = {336-347},
year = {2018},
note = {Agile Government and Adaptive Governance in the Public Sector},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17300394},
author = {Akemi Takeoka Chatfield and Christopher G. Reddick},
keywords = {On-demand services, Customer agility, Systemic use, Big data, Big data analytics, IT assimilation, Process-level strategic alignment, Digital infrastructures, 311 services, Government},
abstract = {A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.}
}
@article{OUSSOUS2018431,
title = {Big Data technologies: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {30},
number = {4},
pages = {431-448},
year = {2018},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817300034},
author = {Ahmed Oussous and Fatima-Zahra Benjelloun and Ayoub {Ait Lahcen} and Samir Belfkih},
keywords = {Big Data, Hadoop, Big Data distributions, Big Data analytics, NoSQL, Machine learning},
abstract = {Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.}
}
@article{WESTRA2017549,
title = {Big data science: A literature review of nursing research exemplars},
journal = {Nursing Outlook},
volume = {65},
number = {5},
pages = {549-561},
year = {2017},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2016.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0029655416303967},
author = {Bonnie L. Westra and Martha Sylvia and Elizabeth F. Weinfurter and Lisiane Pruinelli and Jung In Park and Dianna Dodd and Gail M. Keenan and Patricia Senk and Rachel L. Richesson and Vicki Baukner and Christopher Cruz and Grace Gao and Luann Whittenburg and Connie W. Delaney},
keywords = {Big data, Data science, Nursing informatics, Nursing research, Nurse scientist},
abstract = {Background
Big data and cutting-edge analytic methods in nursing research challenge nurse scientists to extend the data sources and analytic methods used for discovering and translating knowledge.
Purpose
The purpose of this study was to identify, analyze, and synthesize exemplars of big data nursing research applied to practice and disseminated in key nursing informatics, general biomedical informatics, and nursing research journals.
Methods
A literature review of studies published between 2009 and 2015. There were 650 journal articles identified in 17 key nursing informatics, general biomedical informatics, and nursing research journals in the Web of Science database. After screening for inclusion and exclusion criteria, 17 studies published in 18 articles were identified as big data nursing research applied to practice.
Discussion
Nurses clearly are beginning to conduct big data research applied to practice. These studies represent multiple data sources and settings. Although numerous analytic methods were used, the fundamental issue remains to define the types of analyses consistent with big data analytic methods.
Conclusion
There are needs to increase the visibility of big data and data science research conducted by nurse scientists, further examine the use of state of the science in data analytics, and continue to expand the availability and use of a variety of scientific, governmental, and industry data resources. A major implication of this literature review is whether nursing faculty and preparation of future scientists (PhD programs) are prepared for big data and data science.}
}
@article{HUANG201846,
title = {Big-data-driven safety decision-making: A conceptual framework and its influencing factors},
journal = {Safety Science},
volume = {109},
pages = {46-56},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518300973},
author = {Lang Huang and Chao Wu and Bing Wang and Qiumei Ouyang},
keywords = {Big data (BD), Safety big data (SBD), Safety decision-making (SDM), Safety insight (SI), Data-driven},
abstract = {Safety data and information are the most valuable assets for organizations’ safety decision-making (SDM), especially in the era of big data (BD). In this study, a conceptual framework for SDM based on BD, known as BD-driven SDM, was developed and its detailed structure and elements as well as strategies were presented. Other theoretical and practical contributions include: (a) the description of the meta-process and interdisciplinary research area of BD-driven SDM, (b) the design of six types of general analytics and five types of special analytics for SBD mining according to different requirements of safety management applications, (c) the analysis of influencing factors of BD-driven SDM, and (d) the discussion of advantages and limitations in this research as well as suggestions for future research. The results obtained from this study are of important implications for research and practice on BD-driven SDM.}
}
@article{GE2018601,
title = {Big Data for Internet of Things: A Survey},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {601-614},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17316953},
author = {Mouzhi Ge and Hind Bangui and Barbora Buhnova},
keywords = {Big Data, Data analytics, Internet of Things, Healthcare, Energy, Transportation, Building automation, Smart Cities},
abstract = {With the rapid development of the Internet of Things (IoT), Big Data technologies have emerged as a critical data analytics tool to bring the knowledge within IoT infrastructures to better meet the purpose of the IoT systems and support critical decision making. Although the topic of Big Data analytics itself is extensively researched, the disparity between IoT domains (such as healthcare, energy, transportation and others) has isolated the evolution of Big Data approaches in each IoT domain. Thus, the mutual understanding across IoT domains can possibly advance the evolution of Big Data research in IoT. In this work, we therefore conduct a survey on Big Data technologies in different IoT domains to facilitate and stimulate knowledge sharing across the IoT domains. Based on our review, this paper discusses the similarities and differences among Big Data technologies used in different IoT domains, suggests how certain Big Data technology used in one IoT domain can be re-used in another IoT domain, and develops a conceptual framework to outline the critical Big Data technologies across all the reviewed IoT domains.}
}
@article{LIM201886,
title = {Smart cities with big data: Reference models, challenges, and considerations},
journal = {Cities},
volume = {82},
pages = {86-99},
year = {2018},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2018.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0264275117308545},
author = {Chiehyeon Lim and Kwang-Jae Kim and Paul P. Maglio},
keywords = {Smart city, Big data, Reference model, Challenge, Consideration},
abstract = {Cities worldwide are attempting to transform themselves into smart cities. Recent cases and studies show that a key factor in this transformation is the use of urban big data from stakeholders and physical objects in cities. However, the knowledge and framework for data use for smart cities remain relatively unknown. This paper reports findings from an analysis of various use cases of big data in cities worldwide and the authors' four projects with government organizations toward developing smart cities. Specifically, this paper classifies the urban data use cases into four reference models and identifies six challenges in transforming data into information for smart cities. Furthermore, building upon the relevant literature, this paper proposes five considerations for addressing the challenges in implementing the reference models in real-world applications. The reference models, challenges, and considerations collectively form a framework for data use for smart cities. This paper will contribute to urban planning and policy development in the modern data-rich economy.}
}
@article{GUO20181,
title = {Research on case retrieval of Bayesian network under big data},
journal = {Data & Knowledge Engineering},
volume = {118},
pages = {1-13},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18300624},
author = {Yuan Guo and Yuan Guo and K. Wu},
keywords = {Case retrieval, Big data, BN model, Hadoop platform},
abstract = {Although case retrieval of Bayesian network has greatly promoted the application of CBR technique in engineering fields, it is facing huge challenges with the arrival of the era of big data. First, huge computation task of BN learning caused by big data seriously hampers the efficiency of case retrieval; Second, with the increasing data size, the accuracy of case retrieval becomes poorer and poorer because existing methods of improving probability learning become unfit for new situation. Aiming at the first problem, this paper proposes Within-Cross algorithm to assign computation task to improve the result of parallel data processing and gain better efficiency of case retrieval. For the second problem, this paper proposes a new method called Weighted Index Coefficient of Dirichlet Distribution (WICDD) algorithm, which first measures the influence of different factors on probability learning and then gives a weight to each super parameter of Dirichlet Distribution to adjust the result of probability learning. Thus with WICDD algorithm, the effect of probability learning is greatly improved, which then further enhances the accuracy of case retrieval. Finally, lots of experiments are executed to validate the effectiveness of the proposed method.}
}
@article{WOLFERT201769,
title = {Big Data in Smart Farming – A review},
journal = {Agricultural Systems},
volume = {153},
pages = {69-80},
year = {2017},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2017.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16303754},
author = {Sjaak Wolfert and Lan Ge and Cor Verdouw and Marc-Jeroen Bogaardt},
keywords = {Agriculture, Data, Information and communication technology, Data infrastructure, Governance, Business modelling},
abstract = {Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.}
}
@article{ZHU2018107,
title = {Review and big data perspectives on robust data mining approaches for industrial process modeling with outliers and missing data},
journal = {Annual Reviews in Control},
volume = {46},
pages = {107-133},
year = {2018},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578818301056},
author = {Jinlin Zhu and Zhiqiang Ge and Zhihuan Song and Furong Gao},
keywords = {Data mining, Robustness, Process modeling, Statistical process monitoring, Big data analytics},
abstract = {Industrial process data are usually mixed with missing data and outliers which can greatly affect the statistical explanation abilities for traditional data-driven modeling methods. In this sense, more attention should be paid on robust data mining methods so as to investigate those stable and reliable modeling prototypes for decision-making. This paper gives a systematic review of various state-of-the-art data preprocessing tricks as well as robust principal component analysis methods for process understanding and monitoring applications. Afterwards, comprehensive robust techniques have been discussed for various circumstances with diverse process characteristics. Finally, big data perspectives on potential challenges and opportunities have been highlighted for future explorations in the community.}
}
@article{SHADROO201819,
title = {Systematic survey of big data and data mining in internet of things},
journal = {Computer Networks},
volume = {139},
pages = {19-47},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618301579},
author = {Shabnam Shadroo and Amir Masoud Rahmani},
keywords = {Internet of things, Systematic survey, Big data, Data mining},
abstract = {In recent years, the Internet of Things (IoT) has emerged as a new opportunity. Thus, all devices such as smartphones, transportation facilities, public services, and home appliances are used as data creator devices. All the electronic devices around us help our daily life. Devices such as wrist watches, emergency alarms, and garage doors and home appliances such as refrigerators, microwaves, air conditioning, and water heaters are connected to an IoT network and controlled remotely. Methods such as big data and data mining can be used to improve the efficiency of IoT and storage challenges of a large data volume and the transmission, analysis, and processing of the data volume on the IoT. The aim of this study is to investigate the research done on IoT using big data as well as data mining methods to identify subjects that must be emphasized more in current and future research paths. This article tries to achieve the goal by following the conference and journal articles published on IoT-big data and also IoT-data mining areas between 2010 and August 2017. In order to examine these articles, the combination of Systematic Mapping and literature review was used to create an intended review article. In this research, 44 articles were studied. These articles are divided into three categories: Architecture & Platform, framework, and application. In this research, a summary of the methods used in the area of IoT-big data and IoT-data mining is presented in three categories to provide a starting point for researchers in the future.}
}
@article{MEHTA201857,
title = {Concurrence of big data analytics and healthcare: A systematic review},
journal = {International Journal of Medical Informatics},
volume = {114},
pages = {57-65},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618302466},
author = {Nishita Mehta and Anil Pandit},
keywords = {Big data, Analytics, Healthcare, Predictive analytics, Evidence-based medicine},
abstract = {Background
The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care.
Purpose
This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges.
Data sources
A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered.
Study selection
Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected.
Data extraction
Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare.
Results
A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare.
Conclusion
This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.}
}
@article{SHENG201797,
title = {A multidisciplinary perspective of big data in management research},
journal = {International Journal of Production Economics},
volume = {191},
pages = {97-112},
year = {2017},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S092552731730169X},
author = {Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang},
keywords = {Big data, Management research, Literature review},
abstract = {In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.}
}
@article{MANTELERO2018754,
title = {AI and Big Data: A blueprint for a human rights, social and ethical impact assessment},
journal = {Computer Law & Security Review},
volume = {34},
number = {4},
pages = {754-772},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0267364918302012},
author = {Alessandro Mantelero},
keywords = {Data protection, Impact assessment, Data protection impact assessment, Human rights, Human rights impact assessment, Ethical impact assessment, Social impact assessment, General Data Protection Regulation},
abstract = {The use of algorithms in modern data processing techniques, as well as data-intensive technological trends, suggests the adoption of a broader view of the data protection impact assessment. This will force data controllers to go beyond the traditional focus on data quality and security, and consider the impact of data processing on fundamental rights and collective social and ethical values. Building on studies of the collective dimension of data protection, this article sets out to embed this new perspective in an assessment model centred on human rights (Human Rights, Ethical and Social Impact Assessment-HRESIA). This self-assessment model intends to overcome the limitations of the existing assessment models, which are either too closely focused on data processing or have an extent and granularity that make them too complicated to evaluate the consequences of a given use of data. In terms of architecture, the HRESIA has two main elements: a self-assessment questionnaire and an ad hoc expert committee. As a blueprint, this contribution focuses mainly on the nature of the proposed model, its architecture and its challenges; a more detailed description of the model and the content of the questionnaire will be discussed in a future publication drawing on the ongoing research.}
}
@article{AKOKA2017105,
title = {Research on Big Data – A systematic mapping study},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {105-115},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917300211},
author = {Jacky Akoka and Isabelle Comyn-Wattiau and Nabil Laoufi},
keywords = {Big Data, Systematic mapping study, Framework, Artefact, Usage, Analytics},
abstract = {Big Data has emerged as a significant area of study for both practitioners and researchers. Big Data is a term for massive data sets with large structure. In 2012, Big Data passed the top of the Gartner Hype Cycle, attesting the maturity level of this technology and its applications. The aim of this paper is to examine how do researchers grasp the big data concept? We will answer the following questions: How many research papers are produced? What is the annual trend of publications? What are the hot topics in big data research? What are the most investigated big data topics? Why the research is performed? What are the most frequently obtained research artefacts? What does big data research produces? Who are the active authors? Which journals include papers on Big Data? What are the active disciplines? For this purpose, we provide a framework identifying existing and emerging research areas of Big Data. This framework is based on eight dimensions, including the SMACIT (Social Mobile Analytics Cloud Internet of Things) perspective. Current and past research in Big Data are analyzed using a systematic mapping study of publications based on more than a decade of related academic publications. The results have shown that significant contributions have been made by the research community, attested by a continuous increase in the number of scientific publications that address Big Data. We found that researchers are increasingly involved in research combining Big Data and Analytics, Cloud, Internet of things, mobility or social media. As for quality objectives, besides an interest in performance, other topics as scalability is emerging. Moreover, security and quality aspects become important. Researchers on Big Data provide more algorithms, frameworks, and architectures than other artifacts. Finally, application domains such as earth, energy, medicine, ecology, marketing, and health attract more attention from researchers on big data. A complementary content analysis on a subset of papers sheds some light on the evolving field of big data research.}
}
@article{CANITO20181,
title = {Unfolding the relations between companies and technologies under the Big Data umbrella},
journal = {Computers in Industry},
volume = {99},
pages = {1-8},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S016636151830040X},
author = {João Canito and Pedro Ramos and Sérgio Moro and Paulo Rita},
keywords = {Big data companies, Big data technologies, Online news, Gartner magic quadrant},
abstract = {Big Data is dominating the landscape as data originated in many sources keeps piling up. Information Technology (IT) business companies are making tremendous efforts to keep the pace with this wave of innovative technologies. This study aims to identify how the different IT companies are aligned with emerging Big Data technologies. The approach consisted in analyzing 11,505 news published between 2013 and 2016 and aggregated through Google News. The companies were categorized according to their position in the 2017 Gartner Magic Quadrant for advanced analytics. A text mining and topic modeling procedure assisted in summarizing the main findings. Leaders dominated a large fraction of the published news. Challengers are making a significant effort in investing in predictive analytics, overlooking other technologies such as those related to data preparation and integration. The results helped to shed light on the emerging field of Big Data from a corporate perspective.}
}
@article{PAPADOPOULOS20171108,
title = {The role of Big Data in explaining disaster resilience in supply chains for sustainability},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {1108-1118},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.059},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616301275},
author = {Thanos Papadopoulos and Angappa Gunasekaran and Rameshwar Dubey and Nezih Altay and Stephen J. Childe and Samuel Fosso-Wamba},
keywords = {Resilience, Big Data, Sustainability, Disaster, Exploratory factor analysis, Confirmatory factor analysis},
abstract = {The purpose of this paper is to propose and test a theoretical framework to explain resilience in supply chain networks for sustainability using unstructured Big Data, based upon 36,422 items gathered in the form of tweets, news, Facebook, WordPress, Instagram, Google+, and YouTube, and structured data, via responses from 205 managers involved in disaster relief activities in the aftermath of Nepal earthquake in 2015. The paper uses Big Data analysis, followed by a survey which was analyzed using content analysis and confirmatory factor analysis (CFA). The results of the analysis suggest that swift trust, information sharing and public–private partnership are critical enablers of resilience in supply chain networks. The current study used cross-sectional data. However the hypotheses of the study can be tested using longitudinal data to attempt to establish causality. The article advances the literature on resilience in disaster supply chain networks for sustainability in that (i) it suggests the use of Big Data analysis to propose and test particular frameworks in the context of resilient supply chains that enable sustainability; (ii) it argues that swift trust, public private partnerships, and quality information sharing link to resilience in supply chain networks; and (iii) it uses the context of Nepal, at the moment of the disaster relief activities to provide contemporaneous perceptions of the phenomenon as it takes place.}
}
@article{TCHAGNAKOUANOU201868,
title = {An optimal big data workflow for biomedical image analysis},
journal = {Informatics in Medicine Unlocked},
volume = {11},
pages = {68-74},
year = {2018},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352914818300844},
author = {Aurelle {Tchagna Kouanou} and Daniel Tchiotsop and Romanic Kengne and Djoufack Tansaa Zephirin and Ngo Mouelas {Adele Armele} and René Tchinda},
keywords = {Biomedical images, Big data, Artificial intelligence, Machine learning, Hadoop/spark},
abstract = {Background and objective
In the medical field, data volume is increasingly growing, and traditional methods cannot manage it efficiently. In biomedical computation, the continuous challenges are: management, analysis, and storage of the biomedical data. Nowadays, big data technology plays a significant role in the management, organization, and analysis of data, using machine learning and artificial intelligence techniques. It also allows a quick access to data using the NoSQL database. Thus, big data technologies include new frameworks to process medical data in a manner similar to biomedical images. It becomes very important to develop methods and/or architectures based on big data technologies, for a complete processing of biomedical image data.
Method
This paper describes big data analytics for biomedical images, shows examples reported in the literature, briefly discusses new methods used in processing, and offers conclusions. We argue for adapting and extending related work methods in the field of big data software, using Hadoop and Spark frameworks. These provide an optimal and efficient architecture for biomedical image analysis. This paper thus gives a broad overview of big data analytics to automate biomedical image diagnosis. A workflow with optimal methods and algorithm for each step is proposed.
Results
Two architectures for image classification are suggested. We use the Hadoop framework to design the first, and the Spark framework for the second. The proposed Spark architecture allows us to develop appropriate and efficient methods to leverage a large number of images for classification, which can be customized with respect to each other.
Conclusions
The proposed architectures are more complete, easier, and are adaptable in all of the steps from conception. The obtained Spark architecture is the most complete, because it facilitates the implementation of algorithms with its embedded libraries.}
}
@article{SANTOSO201793,
title = {Data Warehouse with Big Data Technology for Higher Education},
journal = {Procedia Computer Science},
volume = {124},
pages = {93-99},
year = {2017},
note = {4th Information Systems International Conference 2017, ISICO 2017, 6-8 November 2017, Bali, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.12.134},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917329022},
author = {Leo Willyanto Santoso and  Yulia},
keywords = {Data Warehouse, Big Data, Academic, Hadoop, Higher Education, Analysis},
abstract = {Nowadays, data warehouse tools and technologies cannot handle the load and analytic process of data into meaningful information for top management. Big data technology should be implemented to extend the existing data warehouse solutions. Universities already collect vast amounts of data so the academic data of university has been growing significantly and become a big academic data. These datasets are rich and growing. University’s top-level management needs tools to produce information from the records. The generated information is expected to support the decision-making process of top-level management. This paper explores how big data technology could be implemented with data warehouse to support decision making process. In this framework, we propose Hadoop as big data analytic tools to be implemented for data ingestion/staging. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.}
}
@article{HE201835,
title = {Statistical process monitoring as a big data analytics tool for smart manufacturing},
journal = {Journal of Process Control},
volume = {67},
pages = {35-43},
year = {2018},
note = {Big Data: Data Science for Process Control and Operations},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2017.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959152417301257},
author = {Q. Peter He and Jin Wang},
keywords = {Statistical process monitoring, Big data, Smart manufacturing, Feature extraction, Internet of things},
abstract = {With ever-accelerating advancement of information, communication, sensing and characterization technologies, such as industrial Internet of Things (IoT) and high-throughput instruments, it is expected that the data generated from manufacturing will grow exponentially, generating so called ‘big data’. One of the focuses of smart manufacturing is to create manufacturing intelligence from real-time data to support accurate and timely decision-making. Therefore, big data analytics is expected to contribute significantly to the advancement of smart manufacturing. In this work, a roadmap of statistical process monitoring (SPM) is presented. Most recent developments in SPM are briefly reviewed and summarized. Specific challenges and potential solutions in handling manufacturing big data are discussed. We suggest that process characteristics or feature based SPM, instead of process variable based SPM, is a promising route for next generation SPM and could play a significant role in smart manufacturing. The advantages of feature based SPM are discussed to support the suggestion and future directions in SPM are discussed in the context of smart manufacturing.}
}
@article{YAN2018457,
title = {Energy-efficient shipping: An application of big data analysis for optimizing engine speed of inland ships considering multiple environmental factors},
journal = {Ocean Engineering},
volume = {169},
pages = {457-468},
year = {2018},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2018.08.050},
url = {https://www.sciencedirect.com/science/article/pii/S0029801818316421},
author = {Xinping Yan and Kai Wang and Yupeng Yuan and Xiaoli Jiang and Rudy R. Negenborn},
keywords = {Ship energy efficiency, Speed optimization, Big data analysis, Parallel k-means algorithm, Hadoop},
abstract = {Energy efficiency of inland ships is significantly influenced by navigational environment, including wind speed and direction as well as water depth and speed. The complexity of the inland navigational environment makes it rather difficult to determine the optimal speeds under different environmental conditions to achieve the best energy efficiency. Route division according to the characteristics of these environmental factors could provide a good solution for the optimization of ship engine speed under different navigational environments. In this paper, the distributed parallel k-means clustering algorithm is adopted to achieve an elaborate route division by analyzing the corresponding environmental factors based on a self-developed big data analytics platform. Subsequently, a ship energy efficiency optimization model considering multiple environmental factors is established through analyzing the energy transfer among hull, propeller and main engine. Then, decisions are made concerning the optimal engine speeds in different segments along the path. Finally, a case study on the Yangtze River is performed to validate the present optimization method. The results show that the proposed method can effectively reduce energy consumption and CO2 emissions of ships.}
}
@article{SCOTT201820,
title = {The role of Statistics in the era of big data: Crucial, critical and under-valued},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {20-24},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300956},
author = {E. Marian Scott},
keywords = {Data, Sampling, Variability, Inference, Uncertainty},
abstract = {What is the role of Statistics in the era of big data, or is Statistics still relevant? I will start this rather personal view with my answer. Statistics remains highly relevant irrespective of ‘bigness’ of data, its role remains what is has always been, but is even more important now. As a community, we need to improve our explanations and presentations to make more visible our relevance.}
}
@article{AKHAVANHEJAZI201891,
title = {Power systems big data analytics: An assessment of paradigm shift barriers and prospects},
journal = {Energy Reports},
volume = {4},
pages = {91-100},
year = {2018},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352484717300616},
author = {Hossein Akhavan-Hejazi and Hamed Mohsenian-Rad},
keywords = {Energy, Big data analytics, Internet of energy, Smart grid},
abstract = {Electric power systems are taking drastic advances in deployment of information and communication technologies; numerous new measurement devices are installed in forms of advanced metering infrastructure, distributed energy resources (DER) monitoring systems, high frequency synchronized wide-area awareness systems that with great speed are generating immense volume of energy data. However, it is still questioned that whether the today’s power system data, the structures and the tools being developed are indeed aligned with the pillars of the big data science. Further, several requirements and especial features of power systems and energy big data call for customized methods and platforms. This paper provides an assessment of the distinguished aspects in big data analytics developments in the domain of power systems. We perform several taxonomy of the existing and the missing elements in the structures and methods associated with big data analytics in power systems. We also provide a holistic outline, classifications, and concise discussions on the technical approaches, research opportunities, and application areas for energy big data analytics.}
}
@article{XU2018309,
title = {A Platform for Fault Diagnosis of High-Speed Train based on Big Data⁎⁎Project supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802).},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {309-314},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.318},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320007},
author = {Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin}},
keywords = {Fault Diagnosis, High-Speed Train, Big Data, Cloud Computing, Edge Computing},
abstract = {High-speed trains are very fast (e.g. 350km/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.}
}
@article{MCCOY201774,
title = {Geospatial Big Data and archaeology: Prospects and problems too great to ignore},
journal = {Journal of Archaeological Science},
volume = {84},
pages = {74-94},
year = {2017},
note = {Archaeological GIS Today: Persistent Challenges, Pushing Old Boundaries, and Exploring New Horizons},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2017.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0305440317300821},
author = {Mark D. McCoy},
keywords = {Geospatial, Big Data, Spatial technology, Cyberinfrastructure, Data science},
abstract = {As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) – broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.}
}
@article{KUMARI2018169,
title = {Multimedia big data computing and Internet of Things applications: A taxonomy and process model},
journal = {Journal of Network and Computer Applications},
volume = {124},
pages = {169-195},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303011},
author = {Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar and Michele Maasberg and Kim-Kwang Raymond Choo},
keywords = {Multimedia big data, Data acquisition, Data representation, Data reduction, Data analysis, Data security and privacy, System intelligence, Distributed system, Social media},
abstract = {With an exponential increase in the provisioning of multimedia devices over the Internet of Things (IoT), a significant amount of multimedia data (also referred to as multimedia big data – MMBD) is being generated. Current research and development activities focus on scalar sensor data based IoT or general MMBD and overlook the complexity of facilitating MMBD over IoT. This paper examines the unique nature and complexity of MMBD computing for IoT applications and develops a comprehensive taxonomy for MMBD abstracted into a novel process model reflecting MMBD over IoT. This process model addresses a number of research challenges associated with MMBD, such as scalability, accessibility, reliability, heterogeneity, and Quality of Service (QoS) requirements. A case study is presented to demonstrate the process model.}
}
@article{ZERBINO2018818,
title = {Big Data-enabled Customer Relationship Management: A holistic approach},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {818-846},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317300067},
author = {Pierluigi Zerbino and Davide Aloini and Riccardo Dulmin and Valeria Mininno},
keywords = {Big Data, CRM, Literature review, Critical Success Factors (CSFs), Word tree},
abstract = {This paper aims to figure out the potential impact of Big Data (BD) on Critical Success Factors (CSFs) of Customer Relationship Management (CRM). In fact, while some authors have posited a relationship between BD and CRM, literature lacks works that go into the heart of the matter. Through an extensive up-to-date in-depth literature review about CRM, twenty (20) CSFs were singled out from 104 selected papers, and organized within an ad-hoc classification framework. The consistency of the classification was checked by means of a content analysis. Evidences were discussed and linked to the BD literature, and five propositions about how BD could affect CRM CSFs were formalized. Our results suggest that BD-enabled CRM initiatives could require several changes in the pertinent CSFs. In order to get rid of the hype effect surrounding BD, we suggest to adopt an explorative approach towards them by defining a mandatory business direction through sound business cases and pilot tests. From a general standpoint, BD could be framed as an enabling factor of well-known projects, like CRM initiatives, in order to reap the benefits from the new technologies by addressing the efforts through already acknowledged management paths.}
}
@incollection{JOINER201895,
title = {Chapter 5 - Information Seeking With Big Data: Not Just the Facts},
editor = {Ida Arlene Joiner},
booktitle = {Emerging Library Technologies},
publisher = {Chandos Publishing},
pages = {95-110},
year = {2018},
series = {Chandos Information Professional Series},
isbn = {978-0-08-102253-5},
doi = {https://doi.org/10.1016/B978-0-08-102253-5.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022535000058},
author = {Ida Arlene Joiner},
keywords = {Big data, libraries, security, privacy, infrastructure, Hadoop},
abstract = {As experts at searching, retrieving, analyzing, and managing information, librarians are uniquely suited to work with big data. This chapter provides an overview of the popular big data technology. We examine what big data is, challenges and opportunities, and how it is currently being used in many industries and libraries. The chapter concludes with additional resources, some technologies for managing big data, big data terminology, and questions for further discussion.}
}