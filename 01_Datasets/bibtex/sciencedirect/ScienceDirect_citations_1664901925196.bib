@article{SECINARO2022296,
title = {Smart city reporting: A bibliometric and structured literature review analysis to identify technological opportunities and challenges for sustainable development},
journal = {Journal of Business Research},
volume = {149},
pages = {296-313},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322004672},
author = {Silvana Secinaro and Valerio Brescia and Federico Lanzalonga and Gabriele Santoro},
keywords = {Smart city, Sustainability, Reporting, Technology},
abstract = {While policies and academic interest in smart cities gain momentum, there remain significant gaps in practice and academic conceptualisations explaining it as a new source of innovation. Moreover, there is a need to synthesise reporting behaviours and tools thereof, supporting communication and transparency for citizens along with their involvement in innovation processes. An Organisation for Economic Co-operation and Development (OECD) country analysis highlights gaps in transparency, reporting and communication of results, and the consequent allocation of resources in smart cities. Thus, this study identifies literature streams embracing the notion of smart cities and reporting. It employs a bibliometric and structured literature review analysis. Accordingly, this study proposes a framework comprising four macro-areas, several micro-elements, and the most appropriate implementation of technologies for sustainability challenges. Notably, it contributes to strengthening the smart city as an unconventional source of innovation, providing policymakers an opportunity to account for the smart city's weaknesses and identify areas for significant improvement efforts to be channelled.}
}
@article{ZHANG2022103642,
title = {Data Matters: A Strategic Action Framework for Data Governance},
journal = {Information & Management},
volume = {59},
number = {4},
pages = {103642},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103642},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000544},
author = {Qingqiang Zhang and Xinbo Sun and Mingchao Zhang},
keywords = {Data, Data governance, Digitalization, Digital transformation, Strategic action, Framework},
abstract = {While there has been a wealth of research exploring data governance, there are still some gaps in how firms deploy data governance and what strategic actions they take to do so, especially as the volume of data increases dramatically and the pace of data assetization accelerates. To achieve this end, through an in-depth case study of a Chinese gold mining company, namely Shandong Gold, we develop a framework to explain how firms configure data governance activities and conduct related strategic actions. Our study identifies four key data governance activities that are supported by two strategic actions. Overall, we contribute to research in data governance and strategic action fields and also provide an alternative implementation framework for practitioners.}
}
@article{CORSI2022100310,
title = {Ultimate approach and technologies in smart healthcare: A broad systematic review focused on citizens},
journal = {Smart Health},
volume = {26},
pages = {100310},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2022.100310},
url = {https://www.sciencedirect.com/science/article/pii/S2352648322000447},
author = {Alana Corsi and Fabiane {Florencio de Souza} and Regina Negri Pagani and João Luiz Kovaleski},
keywords = {Smart cities, Smart healthcare. smart citizens. Citizen's role. technology},
abstract = {Smart Cities are city models that emerged to face the problems of current city configurations, promoting improvement in infrastructure and the provision of essential services through technological implementation. The provision of health services is necessary and a civil right, being one of the dimensions proposed in Smart Cities, represented by Smart Healthcare. This model of cities, as well as its dimensions, have great appeal in the technological application, as the main means of promoting smart services. However, it appears that the approach to the role of the citizen in the promotion of intelligence is neglected, leaving questions about the role of the citizen in Smart Cities. Thus, the present study aimed to identify the technological structures applied in Smart Healthcare, allowing us to identify the role of the citizen in the promotion of smarter health services. A systematic literature review was carried out using the Methodi Ordinatio, resulting in a portfolio composed of 26 articles with scientific relevance. From the content analysis, four main approaches were identified, as well as technologies and methods to address one of the biggest challenges of health technologies: data security and privacy. From this, the role of the citizen in the Smart Healthcare was evidenced, aiming to contribute to the planning of projects and the development of future research to consider with more attention the participation of citizens.}
}
@article{HU2022159103,
title = {Mitigating environmental impacts using net energy system in feed formulation in China's pig production},
journal = {Science of The Total Environment},
pages = {159103},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.159103},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722062027},
author = {Qile Hu and Huangwei Shi and Li Wang and Lu Wang and Yong Hou and Hongliang Wang and Changhua Lai and Shuai Zhang},
keywords = {Formulation strategy, Carbon footprint, Nitrogen footprint, Life cycle assessment, Pig production system},
abstract = {As the world's largest pork producer, China is facing substantial environmental pressures caused by pig production and the relevant feed production. The net energy (NE) system is promoted as a new evaluation method to evaluate energy content in feed and energy requirements of pigs, but its application lacks of comprehensive and comparative evaluation from the environmental perspective. To identify influence factors and to develop mitigation strategies, the carbon and nitrogen footprints and land use (LU) of pigs (25–120 kg) in China were explored through scenario analysis and cradle-to-farm gate life cycle assessment (LCA). Functional unit (FU) was defined as 1 kg of live weight increase in pig. Among all the procedures of pig production, feed crop production and manure management were the principal contributors to the greenhouse gas (GHG) and nitrogen emissions. As for the carbon footprint, the GHG emissions ranged from 2.37 to 2.55 kg CO2-eq. FU−1 for scenarios using the NE system, 2 % lower than that of the metabolizable energy (ME) system. Cottonseed meal-based scenario generated the lowest GHG emissions, and anaerobic digestion achieved the same effects as other manure management methods. As for the nitrogen footprint, reactive nitrogen (Nr) emissions ranged from 53.4 to 66.2 g Nr FU−1 for scenarios using the NE system, 4 % lower than that of the ME system. Peanut-based scenario won the lowest Nr losses. Moreover, arable LU ranged from 4.63 to 5.85 m2 FU−1 for scenarios using the NE system, 4 % lower than that of the ME system, and economic advantage by using the NE system was also proved. Sensitivity analysis and data quality assessment were conducted to quantify the uncertainties of the above models. In conclusion, the application of the NE system in feed formulation was an effective strategy to improve the environmental sustainability of China's pig production.}
}
@article{KONG2022110352,
title = {Data-driven EUR modeling and optimization in the liquid-rich Duvernay Formation, western Canada sedimentary basin, Canada},
journal = {Journal of Petroleum Science and Engineering},
volume = {213},
pages = {110352},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2022.110352},
url = {https://www.sciencedirect.com/science/article/pii/S092041052200239X},
author = {Bing Kong and Zhuoheng Chen},
keywords = {EUR, Machine learning, Shapley value, Stacked model},
abstract = {Estimated Ultimate Recovery (EUR) is one of the focuses of the feasibility assessment for oil and gas development projects. EUR is the utmost recoverable oil and gas volume under the current assumption of technology and economics. Many factors including geology, drilling, completion, operation, and commodity prices influence EUR, which makes the prediction a difficult task. Reservoir numerical simulation and production decline curve analysis (DCA) are two broadly accepted method to calculate EUR. However, the former requires substantial data and resources, while the latter is lack of causative mechanism to associate the fundamentals to productivity. This study proposes a machine learning (ML) procedure in EUR modeling, by which EUR is linked to fundamental variables from available data and the variation in EUR can be explained by various factors so that the results can be applied to optimize future projects. In the proposed procedure, the EUR was estimated using a probabilistic dual flow regime model and Markov Chain Monte Carlo (MCMC) simulation. The resulting EUR in each well was then modeled using a two-level stacked ensemble ML approach, while Shapley value was used to explain feature sensitivity in the trained model. In the last, the EUR is optimized by adjusting the most sensitive factors in the trained model. The trained ML model achieved high accuracy on EUR prediction, and the Shapley value analysis showed that completion length, condensate gas ratio and fracturing fluid volume are among the most important features to EUR. The EUR optimization result showed that there is large room for improvement by adjusting the key features. This proposed approach provides a new perspective to find associations between the fundamental factors and the well EUR which improves the understanding of oil and gas production in unconventional reservoirs.}
}
@article{YOON2022104578,
title = {Virtual sensing in intelligent buildings and digitalization},
journal = {Automation in Construction},
volume = {143},
pages = {104578},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104578},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522004484},
author = {Sungmin Yoon},
keywords = {Virtual sensors, Virtual sensor calibration, Virtual sensing applications, Intelligent buildings, Digital twins, Building digitalization},
abstract = {Virtual sensing technologies have a huge potential in an informative and reliable sensing environment, which is essential to provide and maintain intelligent services in building life-cycle and digitalization. However, there remains a lack of comprehensive literature reviews and suggestions regarding virtual sensing applications in the building sector. The existing virtual sensing classification affords limited insight into virtual sensor modeling, verification, and calibration in building operations. Therefore, this paper reviews existing virtual sensors and applications and characterizes up-to-date virtual sensing technologies, with a focus on virtual sensors and virtual sensor calibration. First, virtual sensors are classified into built-in and in-situ ones according to their modeling and verification environment. The review also examines how virtual sensors have been used for observation, fault detection and diagnosis, and control in buildings. Second, virtual sensor calibration methods are mainly classified into virtual built-in calibration and virtual in-situ calibration. Finally, key calibration strategies proposed in previous studies are summarized. Through this comprehensive review and a consideration of the field-centric building characteristics and building life-cycle, this review proposes virtual sensor categorizations and highlights research challenges and directions in virtual-sensing-driven intelligent building systems to provide greater insight into the fundamentals and methodologies of holistic virtual sensing. Thus, the results of this review are expected to contribute to enhancing the availability and reliability of virtual sensors in the long-term in order to harness them as a novel sensing system in the context of the building life-cycle and digitalization.}
}
@article{STVILIA2022101160,
title = {Seeking and sharing datasets in an online community of data enthusiasts},
journal = {Library & Information Science Research},
volume = {44},
number = {3},
pages = {101160},
year = {2022},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2022.101160},
url = {https://www.sciencedirect.com/science/article/pii/S0740818822000238},
author = {Besiki Stvilia and Leila Gibradze},
keywords = {Dataset practices, Data curation, Online community, Metadata, Data seeking, Data sharing},
abstract = {This study examined discussions of the r/Datasets community on Reddit. It identified three activities in which the community engaged: question answering, data sharing, and community building. Members of the community used 21 types of data and information sources in their activities. The findings of this research enhance our understanding of the activity structures, data and information sources used, and challenges and problems encountered when users search for, share, and make sense of datasets on the web, outside the traditional information and data ecosystems. Data librarians and curators can use the findings of this study in the design of their data management and reference services. The typology of data sources and the metadata model developed through this study can be used in annotating and categorizing data sources and informing the design of metadata schemas and vocabularies for datasets.}
}
@article{KAMAL2022108562,
title = {Super-encoder with cooperative autoencoder networks},
journal = {Pattern Recognition},
volume = {126},
pages = {108562},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108562},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000437},
author = {Imam Mustafa Kamal and Hyerim Bae},
keywords = {Autoencoder, Dimensionality reduction, Feature extraction, Pattern recognition, Cooperative neural networks},
abstract = {Dimensionality reduction plays a crucial role in classification, object detection, and pattern recognition tasks. Its main objective is to decrease the dimension of the original data while retaining the most distinctive information. With the emergence of deep learning, an autoencoder has become a state-of-the-art non-linear dimensionality-reduction method. Nonetheless, as the existing autoencoder models are devised to follow the data distribution and employ similarity techniques, preserving distinctive information can be problematic. To tackle this issue, we propose super-encoder (SE) networks trained in a supervised and cooperative manner. The SE consists of an encoder, separator, and decoder networks. The encoder combined with separator networks are dedicated to generating separable latent representation based on the label, and the decoder network should be able to reconstruct it to the original data simultaneously. Herein, we introduce a novel cooperative learning mechanism with a new loss function; therefore, the encoder, separator, and decoder networks can cooperate to achieve these objectives. Extensive experiments using benchmark datasets were conducted. The results indicated that the SE is more effective in extracting separable latent code than the existing supervised and unsupervised dimensionality-reduction models. Furthermore, as a generator, it can obtain highly competitive realistic images.}
}
@article{USHIZIMA2022118790,
title = {Deep learning for Alzheimer's disease: Mapping large-scale histological tau protein for neuroimaging biomarker validation},
journal = {NeuroImage},
volume = {248},
pages = {118790},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118790},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010570},
author = {Daniela Ushizima and Yuheng Chen and Maryana Alegro and Dulce Ovando and Rana Eser and WingHung Lee and Kinson Poon and Anubhav Shankar and Namrata Kantamneni and Shruti Satrawada and Edson Amaro Junior and Helmut Heinsen and Duygu Tosun and Lea T. Grinberg},
keywords = {Machine learning, Deep learning, Convolutional neural networks, Alzheimer's disease, Histopathology, Digital pathology, Big data, Imaging},
abstract = {Abnormal tau inclusions are hallmarks of Alzheimer's disease and predictors of clinical decline. Several tau PET tracers are available for neurodegenerative disease research, opening avenues for molecular diagnosis in vivo. However, few have been approved for clinical use. Understanding the neurobiological basis of PET signal validation remains problematic because it requires a large-scale, voxel-to-voxel correlation between PET and (immuno) histological signals. Large dimensionality of whole human brains, tissue deformation impacting co-registration, and computing requirements to process terabytes of information preclude proper validation. We developed a computational pipeline to identify and segment particles of interest in billion-pixel digital pathology images to generate quantitative, 3D density maps. The proposed convolutional neural network for immunohistochemistry samples, IHCNet, is at the pipeline's core. We have successfully processed and immunostained over 500 slides from two whole human brains with three phospho-tau antibodies (AT100, AT8, and MC1), spanning several terabytes of images. Our artificial neural network estimated tau inclusion from brain images, which performs with ROC AUC of 0.87, 0.85, and 0.91 for AT100, AT8, and MC1, respectively. Introspection studies further assessed the ability of our trained model to learn tau-related features. We present an end-to-end pipeline to create terabytes-large 3D tau inclusion density maps co-registered to MRI as a means to facilitate validation of PET tracers.}
}
@article{CHEN2022109070,
title = {Leaf chlorophyll contents dominates the seasonal dynamics of SIF/GPP ratio: Evidence from continuous measurements in a maize field},
journal = {Agricultural and Forest Meteorology},
volume = {323},
pages = {109070},
year = {2022},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2022.109070},
url = {https://www.sciencedirect.com/science/article/pii/S0168192322002581},
author = {Ruonan Chen and Liangyun Liu and Xinjie Liu},
keywords = {SIF, GPP, Physiological property, Seasonal variation},
abstract = {The coupling of solar-induced chlorophyll fluorescence (SIF) and gross primary production (GPP) is the foundation of SIF-based GPP estimations; however, the relationship between them varies in different conditions. Structural changes contribute much to the dynamics of their relationship at the canopy scale, whereas the role of physiological mechanisms is not very clear. Here, based on three-year continuous observations from 2018 to 2020 in a maize field in Northwest China, we obtained the total SIF (tSIF) at the photosystem scale and investigated the seasonal dynamics of its link with GPP. Using the ratio of tSIF to GPP, we eliminated the contribution of canopy structure and discovered an increase in the ratio during the late reproductive and ripening stages. Seasonal variation in the ratio was tracked by the leaf chlorophyll contents (LCC) related to the photosynthetic functional maturity (represented by maximum carboxylation rate, Vcmax). In addition, we also found that there was variation in the regression slope of the relationship between SIF/GPP and LCC at different growth stages. The correlation between tSIF/GPP and LCC was better than that between dSIF/GPP (dSIF, the ratio of directional SIF at canopy scale to GPP) and LCC, which demonstrated that the physiological information is reinforced after the elimination of structural contributions. Overall, the seasonal dynamics of the GPP–tSIF relationship in our study highlighted the necessity of considering the growing stage in SIF-based GPP estimations. Although they are usually covered up by the contribution of the canopy structure, physiological mechanisms still impacted the dynamics of the GPP–SIF relationship.}
}
@article{CRONIN2022100213,
title = {A review of in silico toxicology approaches to support the safety assessment of cosmetics-related materials},
journal = {Computational Toxicology},
volume = {21},
pages = {100213},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000019},
author = {Mark T.D. Cronin and Steven J. Enoch and Judith C. Madden and James F. Rathman and Andrea-Nicole Richarz and Chihae Yang},
keywords = {Cosmetics, Risk assessment, , Computational, Read-across, Quantitative structure-activity relationship},
abstract = {In silico tools and resources are now used commonly in toxicology and to support the “Next Generation Risk Assessment” (NGRA) of cosmetics ingredients or materials. This review provides an overview of the approaches that are applied to assess the exposure and hazard of a cosmetic ingredient. For both hazard and exposure, databases of existing information are used routinely. In addition, for exposure, in silico approaches include the use of rules of thumb for systemic bioavailability as well as physiologically-based kinetics (PBK) and multi-scale models for estimating internal exposure at the organ or tissue level. (Internal) Thresholds of Toxicological Concern are applicable for the safety assessment of ingredients at low concentrations. The use of structural rules, (Quantitative) Structure-Activity Relationships ((Q)SARs) and read-across are the most typically applied modelling approaches to predict hazard. Data from exposure and hazard assessment are increasingly being brought together in NGRA to provide an overall assessment of the safety of a cosmetic ingredient. All in silico approaches are reviewed in terms of their maturity and robustness for use.}
}
@article{LNENICKA2022103906,
title = {Transparency of open data ecosystems in smart cities: Definition and assessment of the maturity of transparency in 22 smart cities},
journal = {Sustainable Cities and Society},
volume = {82},
pages = {103906},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103906},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722002281},
author = {Martin Lnenicka and Anastasija Nikiforova and Mariusz Luterek and Otmane Azeroual and Dandison Ukpabi and Visvaldis Valtenbergs and Renata Machova},
keywords = {Open data, Smart city, Transparency, Maturity, Ecosystem, Expert assessment},
abstract = {This paper focuses on the issue of the transparency maturity of open data ecosystems seen as the key for the development and maintenance of sustainable, citizen-centered, and socially resilient smart cities. This study inspects smart cities’ data portals and assesses their compliance with transparency requirements for open (government) data. The expert assessment of 34 portals representing 22 smart cities, with 36 features, allowed us to rank them and determine their level of transparency maturity according to four predefined levels of maturity - developing, defined, managed, and integrated. In addition, recommendations for identifying and improving the current maturity level and specific features have been provided. An open data ecosystem in the smart city context has been conceptualized, and its key components were determined. Our definition considers the components of the data-centric and data-driven infrastructure using the systems theory approach. We have defined five predominant types of current open data ecosystems based on prevailing data infrastructure components. The results of this study should contribute to the improvement of current data ecosystems and build sustainable, transparent, citizen-centered, and socially resilient open data-driven smart cities.}
}
@article{ZHANG2022111697,
title = {Motor current signal analysis using hypergraph neural networks for fault diagnosis of electromechanical system},
journal = {Measurement},
volume = {201},
pages = {111697},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111697},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122009058},
author = {Kongliang Zhang and Hongkun Li and Shunxin Cao and Chen Yang and Fubiao Sun and Zibo Wang},
keywords = {Deep learning, Time shifting, Hypergraph neural networks, Motor current signal analysis, Electromechanical system fault diagnosis},
abstract = {Graph based networks are becoming an emerging trend in the field of fault diagnosis because of their powerful ability to mine the interrelationships between nodes. However, the existing graph-based networks are limited to mining the association relationship between adjacent nodes, which cannot reflect the strong association relationship between multiple nodes and thus affect the graph data quality. To solve these problems, a time-shifting based hypergraph neural network (TS-HGNN) is proposed for the accurate classification of fault types in electromechanical coupled systems. First, the time shifting method is applied to pre-process the original current signal to remove the power-line interference. Then, a hypergraph structure applicable to current signal is established to form complex interrelationships and a hyperedge convolution operation is designed to obtain the interrelationships of higher-order data for representation learning. Finally, several datasets are designed to verify the superiority and robustness of TS-HGNN in current signal fault classification.}
}
@article{ZHAO202256,
title = {Research on the evolution of the innovation ecosystem of the Internet of Things: A case study of Xiaomi(China)},
journal = {Procedia Computer Science},
volume = {199},
pages = {56-62},
year = {2022},
note = {The 8th International Conference on Information Technology and Quantitative Management (ITQM 2020 & 2021): Developing Global Digital Economy after COVID-19},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922000084},
author = {Wu Zhao and Lei Yi},
keywords = {IoT ecosystem, Business system, Hardware intelligent ecological chain, Bamboo forest ecology, Case studies},
abstract = {With the support of industry trends and policies, the Internet of Things is seen as another revolution in the world’s information technology industry. Many companies hope to establish a huge Internet of Things ecosystem through the business ecosystem to achieve rapid expansion and growth. This research selected Xiaomi, the world’s largest consumer-level IoT platform, and tried to explore the growth and development mechanism of its IoT innovation ecosystem by analyzing the development logic of Xiaomi enterprises. The research conclusions show that the Xiaomi IoT innovation ecosystem has evolved through four stages of development, expansion, layout, and maturity. Through the four paths of building market entry, investing in incubating niche products, establishing a bamboo forest ecology, and building an IoT open platform, the ecological map of the Xiaomi IoT ecosystem is finally formed. This research aims to provide guidance and suggestions on business models, evolution strategies, and dynamic capabilities for other companies through path analysis of typical cases.}
}
@article{FONTES2022102137,
title = {AI-powered public surveillance systems: why we (might) need them and how we want them},
journal = {Technology in Society},
pages = {102137},
year = {2022},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2022.102137},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X22002780},
author = {Catarina Fontes and Ellen Hohma and Caitlin C. Corrigan and Christoph Lütge},
keywords = {AI, Surveillance, Dataveillance, AI governance},
abstract = {In this article, we address the introduction of AI-powered surveillance systems in our society by looking at the deployment of real-time facial recognition technologies (FRT) in public spaces and public health surveillance technologies, in particular contact tracing applications. Both cases of surveillance technologies assist public authorities in the enforcement of the law by allowing the tracking of individual movements and extrapolating results towards monitoring and predicting social behavior. Therefore, they are considered as potentially useful tools in response to societal crises, such as those generated by crime and health related pandemics. To approach the assessment of the potentials and threats of such tools, we offer a framework with three dimensions: a function dimension, examines the type, quality and quantity of data the system needs to employ to work effectively; the consent dimension considers the user's right to be informed about and reject the use of surveillance, questioning whether consent is achievable and whether the user can decide fully autonomously/independently; and a societal dimension that frames vulnerabilities and the impacts of the increased empowerment of established political regimes through new means to control populations based on data surveillance. Our analysis framework can assist public authorities in their decisions on how to design and deploy public surveillance tools in a way that enables compliance with the law while highlighting individual and societal tradeoffs.}
}
@article{COLANGELO2022493,
title = {Maturity Model for AI in Smart Production Planning and Control System},
journal = {Procedia CIRP},
volume = {107},
pages = {493-498},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002980},
author = {Eduardo Colangelo and Christian Fries and Theresa-Franziska Hinrichsen and Ádám Szaller and Gábor Nick},
keywords = {Production Planning, Control, Artificial Intelligence, Maturity Model, Smart Production},
abstract = {The utilization of Artificial Intelligence (AI) to improve processes constitutes a main subject for many enterprises. The area of Production Planning and Control (PPC) possesses several functions that could profit from such approaches. However, manufacturing companies find themselves often limited in the application of these approaches. This paper concentrates on three elements to assist enterprises: 1) the clarification of what AI is (in the manufacturing context) and its application to the field of PPC; 2) a review performed together with manufacturing enterprises in Germany and Hungary in order to understand the obstacles for the implementation of AI; and 3) the proposal of a maturity model to help enterprises understand where they are in regards to AI, as a way to help them create a roadmap to achieve their objectives.}
}
@article{WAN2022186,
title = {EEG fading data classification based on improved manifold learning with adaptive neighborhood selection},
journal = {Neurocomputing},
volume = {482},
pages = {186-196},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.039},
url = {https://www.sciencedirect.com/science/article/pii/S092523122101715X},
author = {Zitong Wan and Rui Yang and Mengjie Huang and Weibo Liu and Nianyin Zeng},
keywords = {Adaptive neighborhood selection, Data fading, Electroencephalogram, Manifold learning},
abstract = {In electroencephalogram (EEG) signal analysis, data fading problem exists from signal production to collection by brain-computer interface (BCI) device, which can be raised by BCI device deficiency, dynamic network limitation and subject issue. EEG data fading problem changes the distribution of data, which results in the movement of the cluster center and fuzzy class boundary after feature extraction with negative effects in EEG classification results. To decrease the adverse influence of data fading, a novel fading data classification method based on manifold learning and adaptive neighborhood selection is proposed in this paper to mitigate this adverse effect of data fading. In the proposed method, after neighborhood selection according to local linearity, data are mapped into manifold space through local tangent space alignment (LTSA) for dimensionality reduction. The method is carried out on BCI Competition 2008 – Graz data set A of four-class EEG data of motor imagery (MI) experiments. The experimental results are compared with conventional LTSA and indicate that the proposed method effectively improves the classification accuracy of fading data.}
}
@article{PAHREN202273,
title = {A Novel Method in Intelligent Synthetic Data Creation for Machine Learning-based Manufacturing Quality Control},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {73-78},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.186},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014008},
author = {Laura Pahren and Paul Thomas and Xiaodong Jia and Jay Lee},
keywords = {quality inspections, artificial intelligence, deep learning, resampling, manufacturing},
abstract = {This paper aims to help improve the creation of deep learning-based vision systems for consumer goods manufacturing at Procter and Gamble (P&G) using smart, realistic synthetic data creation. This synthetic data creation is based on a novel data resampling technique that utilizes ordinal class information to create hard-to-capture minority class defects, which has been appropriately name Class Ordered Minority Oversampling Technique (COSMOTE), while also minimizing the overall data collection efforts, which can be a costly and disruptive process to the plants themselves. A particular challenge to these applications is the small number of samples that may be captured for defective products, especially when changes in the process or artwork are made. By leveraging these ordinal quality classes, the information from the classes themselves can enable a minimal training dataset size for faster start to finish model development. A brief literature review of existing resampling techniques is provided to highlight the gaps in these sparse sample use cases and the workflow to generate and validate these synthetic images is also outlined. This paper explains the benefits of intelligent synthetic data creation within this particular manufacturing space by addressing both data imbalance and sparse sample datasets.}
}
@article{ZHANG202281,
title = {Adjoint dynamical kernel density for anomaly detection},
journal = {Neurocomputing},
volume = {499},
pages = {81-92},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005379},
author = {Panpan Zhang and Hui Cao and Yanbin Zhang and Jingcheng Wang and Lixin Jia and Feihu Hu},
keywords = {Anomaly detection, Outlier factor, The density changes, Adjoin dynamical kernel density, Kernel function},
abstract = {Anomalies affect data quality and lead to unexpected analysis results in data mining. Although techniques to handle anomalies do exist, they can fail in considering density changes of object along with incremental neighbors. This paper proposes an outlier factor based on the change of adjoint dynamical kernel density (ADD) to represent the degree of the object being an anomaly. The factor is equal to the ratios of the adjoint dynamical kernel density fluctuation (ADDF) of the object and the average ADDF of its neighborhood. ADDF is estimated by the difference of ADD, which describes the difference of every two consecutive adjoint kernel densities (AKD) of the object. AKD indicates kernel densities of the object while its neighbors are added one by one. Importantly, the kernel function is adopted to measure the distance between objects where the kernel trick improves discriminability between objects and reduces the computational burden of the algorithm. The experiments are performed on eight datasets to evaluate the effectiveness of the proposed method with different kernel functions. The experimental results have shown that the proposed method with the Gaussian kernel function has better performance of anomalies recognition and higher adaption of the parameter k of k-nearest neighbors over some other anomaly detection methods.}
}
@article{TIAN2022119297,
title = {A deep learning-based multisite neuroimage harmonization framework established with a traveling-subject dataset},
journal = {NeuroImage},
volume = {257},
pages = {119297},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119297},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922004165},
author = {Dezheng Tian and Zilong Zeng and Xiaoyi Sun and Qiqi Tong and Huanjie Li and Hongjian He and Jia-Hong Gao and Yong He and Mingrui Xia},
keywords = {Big data, Machine learning, Multicenter, Gray matter, Convolutional network, Site effect},
abstract = {The accumulation of multisite large-sample MRI datasets collected during large brain research projects in the last decade has provided critical resources for understanding the neurobiological mechanisms underlying cognitive functions and brain disorders. However, the significant site effects observed in imaging data and their derived structural and functional features have prevented the derivation of consistent findings across multiple studies. The development of harmonization methods that can effectively eliminate complex site effects while maintaining biological characteristics in neuroimaging data has become a vital and urgent requirement for multisite imaging studies. Here, we propose a deep learning-based framework to harmonize imaging data obtained from pairs of sites, in which site factors and brain features can be disentangled and encoded. We trained the proposed framework with a publicly available traveling subject dataset from the Strategic Research Program for Brain Sciences (SRPBS) and harmonized the gray matter volume maps derived from eight source sites to a target site. The proposed framework significantly eliminated intersite differences in gray matter volumes. The embedded encoders successfully captured both the abstract textures of site factors and the concrete brain features. Moreover, the proposed framework exhibited outstanding performance relative to conventional statistical harmonization methods in terms of site effect removal, data distribution homogenization, and intrasubject similarity improvement. Finally, the proposed harmonization network provided fixable expandability, through which new sites could be linked to the target site via indirect schema without retraining the whole model. Together, the proposed method offers a powerful and interpretable deep learning-based harmonization framework for multisite neuroimaging data that can enhance reliability and reproducibility in multisite studies regarding brain development and brain disorders.}
}
@article{GO2022700,
title = {Application of data mining algorithms to study data trends for corneal transplantation},
journal = {Journal Français d'Ophtalmologie},
volume = {45},
number = {7},
pages = {700-709},
year = {2022},
issn = {0181-5512},
doi = {https://doi.org/10.1016/j.jfo.2022.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0181551222002212},
author = {J.A. Go and J. Tran and M. Khan and Z. Al-Mohtaseb},
keywords = {Cornea, Transplant, Epidemiology, Demographic, Socioeconomic, Cornée, Transplantation, Épidémiologie, Démographique, Socio-économique},
abstract = {Summary
Purpose
To utilize data mining for analysis of corneal transplantations (CT) in Florida from 2005-2014, segmented by demographics, geography, and transplantation technique.
Methods
A retrospective, database study was performed utilizing data queried from the Healthcare and Cost Utilization Project using Current Procedural Terminology codes for lamellar keratoplasty (ALK), endothelial keratoplasty (EK), and penetrating keratoplasty (PKP). Payer status, ethnic group, age, gender, and geography (urban versus rural) was extracted from each surgical encounter and reconfigured to provide a “clean”, congruous dataset for statistical analysis. This Institutional Review Board-approved study did not utilize identifiable patient information; thus, individual informed consent was not required.
Results
From 2005–2014, CT (n=28,607) represented less than 1% of the total ambulatory surgeries (n=12,695,932) performed in Florida. EK volume increased while PKP and ALK volume decreased, year-over-year. Statistical significance was found between transplantation technique by sex (P<0.001) and ethnic group (P<0.001). The largest sex discrepancy was EK (59% female, 41% male). White patients underwent relatively fewer PKP than EK (71% vs. 83% of totals), while Black patients underwent relatively more PKP than EK (14% vs 6% of totals). Statistical significance was found between techniques by payer (P<0.001). Medicare was the most common payer for all techniques, but ALK and PKP had higher percentages of private insurance and self-pay. No statistical significance was found between techniques by geographic location. Corneal edema (22.4%), endothelial dystrophy (17.5%), and bullous keratopathy (10.9%) were erroneously coded as indications for ALK. Corneal scars (2.5%) and corneal opacity (1.7%) were erroneously coded as indications for EK.
Conclusions
CT rates in Florida appear to overrepresent the female sex and underrepresent ethnic minorities, with propensities between PKP and African Americans, EK and female patients, and EK and Medicare reimbursement. Our study further confirms the utility of data mining for providing efficient, detailed, and practical insights into ophthalmology procedures, while highlighting the intrinsic challenges of large datasets.
Résumé
Objectif
Utiliser l’exploration de données pour analyser les transplantations de cornée (CT) en Floride de 2005 à 2014, segmentées par démographie, géographie et technique de transplantation.
Méthodes
Une étude rétrospective de la base de données a été réalisée à partir de données extraites du Healthcare and Cost Utilization Project en utilisant les codes de la terminologie procédurale courante pour la kératoplastie lamellaire (ALK), la kératoplastie endothéliale (EK) et la kératoplastie pénétrante (PKP). Le statut du payeur, le groupe ethnique, l’âge, le sexe et la géographie (urbaine ou rurale) ont été extraits de chaque rencontre chirurgicale et reconfigurés pour fournir un ensemble de données « propre » et congruent pour l’analyse statistique. Cette étude approuvée par l’Institutional Review Board n’a pas utilisé d’informations identifiables sur les patients; le consentement éclairé individuel n’était donc pas nécessaire.
Résultats
De 2005 à 2014, les CT (n=28 607) ont représenté moins de 1 % du total des chirurgies ambulatoires (n=12 695 932) réalisées en Floride. Le volume des EK a augmenté tandis que celui des PKP et des ALK a diminué, d’une année sur l’autre. Une signification statistique a été trouvée entre la technique de transplantation par sexe (p<0,001) et par groupe ethnique (p<0,001). L’écart le plus important entre les sexes était l’EK (59 % de femmes, 41 % d’hommes). Les patients blancs ont subi relativement moins de PKP que d’EK (71 % vs. 83 % des totaux), tandis que les patients noirs ont subi relativement plus de PKP que d’EK (14 % vs. 6 % des totaux). Une significativité statistique a été trouvée entre les techniques par payeur (p<0,001). Medicare était le payeur le plus courant pour toutes les techniques, mais les techniques ALK et PKP présentaient des pourcentages plus élevés d’assurance privée et d’auto-paiement. Aucune significativité statistique n’a été trouvée entre les techniques selon la localisation géographique. L’œdème cornéen (22,4 %), la dystrophie endothéliale (17,5 %) et la kératopathie bulleuse (10,9 %) ont été codés par erreur comme des indications de l’ALK. Les cicatrices cornéennes (2,5 %) et l’opacité cornéenne (1,7 %) ont été codées par erreur comme des indications de l’EK.
Conclusions
Les taux de CT en Floride semblent surreprésenter le sexe féminin et sous-représenter les minorités ethniques, avec des propensions entre PKP et Afro-Américains, EK et patients féminins, et EK et remboursement Medicare. Notre étude confirme l’utilité de l’exploration de données pour fournir des informations efficaces, détaillées et pratiques sur les procédures ophtalmologiques, tout en soulignant les défis intrinsèques des grands ensembles de données.}
}
@article{ZHANG2022106594,
title = {Artificial intelligence-aided railroad trespassing detection and data analytics: Methodology and a case study},
journal = {Accident Analysis & Prevention},
volume = {168},
pages = {106594},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2022.106594},
url = {https://www.sciencedirect.com/science/article/pii/S0001457522000306},
author = {Zhipeng Zhang and Asim Zaman and Jinxuan Xu and Xiang Liu},
keywords = {Trespassing, Railroad safety, Artificial Intelligence, Computer vision, Risk management},
abstract = {The railroad industry plays a principal role in the transportation infrastructure and economic prosperity of the United States, and safety is of the utmost importance. Trespassing is the leading cause of rail-related fatalities and there has been little progress in reducing the trespassing frequency and deaths for the past ten years in the United States. Although the widespread deployment of surveillance cameras and vast amounts of video data in the railroad industry make witnessing these events achievable, it requires enormous labor-hours to monitor real-time videos or archival video data. To address this challenge and leverage this big data, this study develops a robust Artificial Intelligence (AI)-aided framework for the automatic detection of trespassing events. This deep learning-based tool automatically detects trespassing events, differentiates types of violators, generates video clips, and documents basic information of the trespassing events into one dataset. This study aims to provide the railroad industry with state-of-the-art AI tools to harness the untapped potential of video surveillance infrastructure through the risk analysis of their data feeds in specific locations. In the case study, the AI has analyzed over 1,600 h of archival video footage and detected around 3,000 trespassing events from one grade crossing in New Jersey. The data generated from these big video data will potentially help understand human factors in railroad safety research and contribute to specific trespassing proactive safety risk management initiatives and improve the safety of the train crew, rail passengers, and road users through engineering, education, and enforcement solutions to trespassing.}
}
@article{ALKEZ2022133633,
title = {Exploring the sustainability challenges facing digitalization and internet data centers},
journal = {Journal of Cleaner Production},
volume = {371},
pages = {133633},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133633},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622032115},
author = {Dlzar {Al Kez} and Aoife M. Foley and David Laverty and Dylan Furszyfer {Del Rio} and Benjamin Sovacool},
keywords = {Data center emissions, Dark data, Data storage, Environmental footprints, Power consumption},
abstract = {Internet data centers have received significant scientific, public, and media attention due to the challenges associated with their greenhouse gas, water, and land footprint. This resource greedy data services sector continues to rapidly grow driven by data storage, data mining, and file sharing activities by a wide range of end-users. A fundamentally important question then arises; what impact does data storage have on the environment and is it sustainable? Water is used extensively in data centers, both directly for liquid cooling and indirectly to generate electricity. Data centers house a huge number of servers, which consume a vast amount of energy to respond to information requests and store files and large amounts of resulting data. Here we examine the environmental footprint of global data storage utilizing extensive datasets from the latest global electricity generation mix to throw light on this data sustainability issue. The analysis also provides a broad perspective of carbon, water, and land footprints due to worldwide data storage to through some light on the real impact of data centers globally. The findings indicate that if not properly handled, the annual global carbon, water and land footprints resulting from storing dark data might approach 5.26 million tons, 41.65 Gigaliters, and 59.45 square kilometers, respectively.}
}
@article{ALHADDAD202283,
title = {A data–information–knowledge cycle for modeling driving behavior},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {85},
pages = {83-102},
year = {2022},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2021.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S1369847821002953},
author = {Christelle {Al Haddad} and Constantinos Antoniou},
keywords = {Data collection, Information extraction, Impacts of AVs, Behavioral modeling, Data analytics, Data fusion},
abstract = {When talking about automation, “autonomous vehicles”, often abbreviated as AVs, come to mind. In transitioning from the “driver” mode to the different automation levels, there is an inevitable need for modeling driving behavior. This often happens through data collection from experiments and studies, but also information extraction, a key step in behavioral modeling. Particularly, naturalistic driving studies and field operational trials are used to collect meaningful data on drivers’ interactions in real–world conditions. On the other hand, information extraction methods allow to predict or mimic driving behavior, by using a set of statistical learning methods. In simple words, the way to understand drivers’ needs and wants in the era of automation can be represented in a data–information cycle, starting from data collection, and ending with information extraction. To develop this cycle, this research reviews studies with keywords “data collection”, “information extraction”, “AVs”, while keeping the focus on driving behavior. The resulting review led to a screening of about 161 papers, out of which about 30 were selected for a detailed analysis. The analysis included an investigation of the methods and equipment used for data collection, the features collected, the size and frequency of the data along with the main problems associated with the different sensory equipment; the studies also looked at the models used to extract information, including various statistical techniques used in AV studies. This paved the way to the development of a framework for data analytics and fusion, allowing the use of highly heterogeneous data to reach the defined objectives; for this paper, the example of impacts of AVs on a network level and AV acceptance is given. The authors suggest that such a framework could be extended and transferred across the various transportation sectors.}
}
@article{SHUAI2022100301,
title = {A Full-Sample Clustering Model Considering Whole Process Optimization of Data},
journal = {Big Data Research},
volume = {28},
pages = {100301},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100301},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621001180},
author = {Yong Shuai},
keywords = {Optimization of whole process, Principal component analysis, Self organizing maps, K-means cluster, Collaborative filtering},
abstract = {With the continuous increase of data volume and data dimensions, it becomes more and more difficult to improve the accuracy and interpretability of the algorithm only from the clustering algorithm itself. In order to improve the accuracy of the clustering algorithm and improve the interpretability of the clustering results, we propose an Improved feature selection and combined clustering model considering whole process optimization. In this model, we processed the data from the whole process of data mining and carried out clustering analysis. Firstly, we started data preprocessing, and then used the feature selection algorithm of text weight + principal component analysis (PCA) to reduce the feature dimension and obtain important features and data sets for clustering. Secondly, we used the improved Self organizing maps (SOM) neural network and K-means clustering combination model to perform clustering analysis and established clustering algorithm evaluation indicators. Thirdly, we used collaborative filtering to cluster data sets that included missing data to ensure that all sample data can obtain results. Finally, through case analysis, it was verified that the model proposed in this paper had high clustering accuracy and interpretability.}
}
@article{PENG2022,
title = {East Asian new techno-humanities report},
journal = {New Techno Humanities},
year = {2022},
issn = {2664-3294},
doi = {https://doi.org/10.1016/j.techum.2022.100003},
url = {https://www.sciencedirect.com/science/article/pii/S2664329422000012},
author = {Qinglong Peng and Man Zhou},
keywords = {Humanities, New techno perspective, Digital humanities, East Asia},
abstract = {Overall, the development of digital humanities (DH) in Asia is slower than that in Europe and North America, while within East Asia, specifically China, Japan, and South Korea, there are differences in DH's acceptance and progress. This paper reviews the history and current situation of DH in China, Japan, and South Korea. In comparison to Japan and South Korea, the paper also analyzes the characteristics, problems, and possible causes of the problems presented by Chinese DH. Such a retrospective process demonstrates not only the integration and cooperation between humanities and technology, but also the transformation of humanities in the perspective of new technologies. The transformation, whether active or passive, has led many Chinese humanities scholars to worry and reflect, which is one of the focuses of this paper.}
}
@article{LI2022124771,
title = {Data-driven battery state of health estimation based on interval capacity for real-world electric vehicles},
journal = {Energy},
volume = {257},
pages = {124771},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124771},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222016747},
author = {Renzheng Li and Jichao Hong and Huaqin Zhang and Xinbo Chen},
keywords = {Electric vehicle, Battery system, SOH estimation, Interval capacity, Catboost},
abstract = {State of health (SOH) estimation is critical to the safety of battery systems in real-world electric vehicles. Accurate battery health status is difficult to be measured during dynamic and robust vehicular operation conditions. This paper proposes a novel SOH estimation model based on Catboost and interval capacity during the charging process. A year-long operation dataset of an electric taxi is derived with all charging segments separated to construct the research dataset. The charging patterns are analyzed, and the segments with rich aging information are extracted, then a general aging feature of interval capacity is extracted by incremental capacity analysis. Furthermore, comparison with the other six machine learning methods is conducted, and five inputs are determined through Pearson correlation analysis, including start charging state of charge (SOC), end charging SOC, mileage, temperature of probe, and current. The results show the Catboost-based model achieves the best accuracy, with the mean absolute percentage error and root mean squared error limited within 2.74% and 1.12%, respectively. More importantly, a battery aging evaluation strategy and its further research plan is proposed for the application in real-world electric vehicles.}
}
@article{FIROUZI2022101840,
title = {The convergence and interplay of edge, fog, and cloud in the AI-driven Internet of Things (IoT)},
journal = {Information Systems},
volume = {107},
pages = {101840},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101840},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000776},
author = {Farshad Firouzi and Bahar Farahani and Alexander Marinšek},
keywords = {Internet of Things (IoT), Cloud Computing, Fog Computing, Edge Computing, Mobile Computing, Edge-Fog-Cloud, Cloud IoT, Cloudlet, Offloading, Resource Management, Service Placement, Privacy-Preserving Machine Learning, Security and Privacy, Healthcare IoT, Case Studies},
abstract = {The Internet of Things (IoT) tsunami, public embracement, and the ubiquitous adoption of smart devices are affecting virtually every industry, directly or indirectly. The success of the current and future landscape of IoT and connected devices requires service provision characterized by scalability, ubiquity, reliability, and high-performance, among others. In order to achieve this attribution, the integration of IoT and Cloud Computing (CC), known as cloud IoT, has emerged as a new paradigm providing advanced services specific to aggregating, storing, and processing data generated by IoT. While the convergence of IoT and Cloud brings opportunities, it suffers from specific limitations such as bandwidth, latency, and connectivity. The increasing need for supporting interaction between cloud and IoT led to Edge and Fog Computing (FC) in which computing and storage resources are located not only in the cloud but also at the edges near the source of data. The hierarchical and collaborative edge–fog–cloud architecture brings tremendous benefits as it enables us to distribute the intelligence and computation – including Artificial Intelligence (AI), Machine Learning (ML), and big data analytics – to achieve an optimal solution while satisfying the given constraints e.g., delay-energy tradeoff. Due to the hierarchical, cross-layer, and distributed nature of this model, achieving an osmotic and effective convergence of IoT, edge, fog, and cloud computing requires overcoming many challenges with respect to design and implementation, as well as deployment and evaluation. This paper provides a comprehensive insight into the edge-fog-cloud computing paradigm by providing a blend of discussions on all important aspects of the underlying technologies to offer opportunities for more holistic studies and to accelerate knowledge acquisition. To gain a deep understanding of edge–fog–cloud, we will begin this paper by providing an in-depth tutorial and presenting the main requirements, state-of-the-art reference architectures, building blocks, components, protocols, applications, and other similar computing paradigms, including their similarities and differences. Following this, a holistic reference architecture for edge–fog–cloud IoT is presented and the major corresponding design and deployment considerations (e.g., service models, infrastructure design, provisioning, resource allocation, offloading, service migration, performance evaluation, and security concerns) are discussed. Next, we will take a look at the role of privacy-preserving, distributed, and collaborative analytics as well as the interaction between edge, fog, and cloud computing. Finally, we will overview the main challenges in the field of edge–fog–cloud computing that need to be tackled to realize the full potential of IoT.}
}
@article{NG202223,
title = {National and international kidney failure registries: characteristics, commonalities, and contrasts},
journal = {Kidney International},
volume = {101},
number = {1},
pages = {23-35},
year = {2022},
issn = {0085-2538},
doi = {https://doi.org/10.1016/j.kint.2021.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S0085253821010206},
author = {Monica S.Y. Ng and Vivek Charu and David W. Johnson and Michelle M. O’Shaughnessy and Andrew J. Mallett},
keywords = {data sharing, dialysis, inter-registry collaboration, kidney failure, registry, transplantation},
abstract = {Registries are essential for health infrastructure planning, benchmarking, continuous quality improvement, hypothesis generation, and real-world trials. To date, data from these registries have predominantly been analyzed in isolated “silos,” hampering efforts to analyze “big data” at the international level, an approach that provides wide-ranging benefits, including enhanced statistical power, an ability to conduct international comparisons, and greater capacity to study rare diseases. This review serves as a valuable resource to clinicians, researchers, and policymakers, by comprehensively describing kidney failure registries active in 2021, before proposing approaches for inter-registry research under current conditions, as well as solutions to enhance global capacity for data collaboration. We identified 79 kidney-failure registries spanning 77 countries worldwide. International Society of Nephrology exemplar initiatives, including the Global Kidney Health Atlas and Sharing Expertise to support the set-up of Renal Registries (SharE-RR), continue to raise awareness regarding international healthcare disparities and support the development of universal kidney-disease registries. Current barriers to inter-registry collaboration include underrepresentation of lower-income countries, poor syntactic and semantic interoperability, absence of clear consensus guidelines for healthcare data sharing, and limited researcher incentives. This review represents a call to action for international stakeholders to enact systemic change that will harmonize the current fragmented approaches to kidney-failure registry data collection and research.}
}
@article{SHARMA20226962,
title = {Artificial intelligence framework for MSME sectors with focus on design and manufacturing industries},
journal = {Materials Today: Proceedings},
volume = {62},
pages = {6962-6966},
year = {2022},
note = {International Conference on Additive Manufacturing and Advanced Materials (AM2)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.12.360},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321081013},
author = {Paawan Sharma and Jigarkumar Shah and Reema Patel},
keywords = {Artificial Intelligence (AI), Industry 4.0, Internet of Things (IoT), Cyber Physical Systems (CPS), Industrial IoT (IIoT)},
abstract = {Artificial Intelligence (AI) is gaining high popularity in multiple domains targeting different industries. Though initial adoption of AI was thought of as an enabler for automation in industries, but later it presented vast possibilities in innovation and design. Also, with advent of Industry 4.0 standards, AI emerged to play a big role in its adoption across different sectors of industries. Micro, Small and medium enterprises (MSMEs) with limited resources look upon AI as a tool for accelerating their growth. In India, the MSME sector although varies with different States, but dominantly includes textiles, machinery and parts, mining and quarrying, basic metal industries, electrical machinery and apparatus, transport equipment and parts, paper products and printing, food products, chemical and chemical products, leather, wood, rubber, plastic and other non-metallic mineral products, beverages and tobacco products. This paper proposes architectural framework and also provides probable applications of Internet of Things (IoT) and AI for design and manufacturing MSMEs of India.}
}
@article{MENG2022101,
title = {Applications of neural networks in liver transplantation},
journal = {iLIVER},
volume = {1},
number = {2},
pages = {101-110},
year = {2022},
issn = {2772-9478},
doi = {https://doi.org/10.1016/j.iliver.2022.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S277294782200038X},
author = {Jinwen Meng and Zhikun Liu and Xiao Xu},
keywords = {Artificial intelligence, Liver transplantation, Machine learning, Neural network, Deep learning},
abstract = {The use of neural networks (NNs) as a cutting-edge technique in the medical field has drawn considerable attention. NN models “learn” from a large amount of data and then find corresponding clinical patterns that are challenging for clinicians to recognize. In this study, we focus on liver transplantation (LT), which is an effective treatment for end-stage liver diseases. The management before and after LT produces a massive quantity of medical data, which can be fully processed by NNs. We describe recent progress in the clinical application of NNs to LT in five respects: pre-transplantation evaluation of the donor and recipient, recipient outcome prediction, allocation system development, operation monitoring, and post-transplantation complication prediction. This review provides clinicians and researchers with a description of forefront applications of NNs in the field of LT and discusses prospects and pitfalls.}
}
@article{POURMEHDI2022107808,
title = {Analysis and evaluation of challenges in the integration of Industry 4.0 and sustainable steel reverse logistics network},
journal = {Computers & Industrial Engineering},
volume = {163},
pages = {107808},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107808},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221007129},
author = {Mohammad Pourmehdi and Mohammad Mahdi Paydar and Pezhman Ghadimi and Amir Hossein Azadnia},
keywords = {Steel industry, Industry 4.0, Adoption challenges, Reverse logistics, Sustainability, Interpretive structural modelling, Fuzzy analytical network process},
abstract = {Industry 4.0 (I4.0) is a comparatively new phenomenon, and it is most probable that developing countries would face challenges in adapting it for improving the processes of supply chains and moving toward sustainability. The steel industry is the core of industrial growth, and it has an indispensable role in the development of countries. Steel is a highly recyclable product, meaning that it can be reused infinitely, increasing the significance of its reverse logistics. Although many studies have been conducted in the area of I4.0 and supply chain management, less attention has been devoted to finding and analyzing potential challenges of I4.0 technologies integration in steel reverse logistics activities. Therefore, this study is conducted to identify and analyse the challenges to efficient integration of I4.0 and sustainable steel reverse logistics system. Data collection is conducted with the assistance of qualified experts familiar with the steel supply chain and I4.0 concept. The interrelations of challenges are specified by Interpretive Structural Modeling, and the final ranking of challenges is determined through the Fuzzy Analytical Network Process. After validating the completed questionnaires, the absence of experts in I4.0, lack of clear comprehension of I4.0 concepts, training programs, and governmental policies and support are determined as the most critical challenges. Finally, the results and discussion, which can help practitioners in the efficient adoption of I4.0 to have a sustainable reverse logistics system, are presented.}
}
@article{JANSEN2022100020,
title = {The illusion of data validity: Why numbers about people are likely wrong},
journal = {Data and Information Management},
pages = {100020},
year = {2022},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2022.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122001188},
author = {Bernard J. Jansen and Joni Salminen and Soon-gyo Jung and Hind Almerekhi},
keywords = {People data, Measurement, Quantitative paradigm, Statistics},
abstract = {This reflection article addresses a difficulty faced by scholars and practitioners working with numbers about people, which is that those who study people want numerical data about these people. Unfortunately, time and time again, this numerical data about people is wrong. Addressing the potential causes of this wrongness, we present examples of analyzing people numbers, i.e., numbers derived from digital data by or about people, and discuss the comforting illusion of data validity. We first lay a foundation by highlighting potential inaccuracies in collecting people data, such as selection bias. Then, we discuss inaccuracies in analyzing people data, such as the flaw of averages, followed by a discussion of errors that are made when trying to make sense of people data through techniques such as posterior labeling. Finally, we discuss a root cause of people data often being wrong – the conceptual conundrum of thinking the numbers are counts when they are actually measures. Practical solutions to address this illusion of data validity are proposed. The implications for theories derived from people data are also highlighted, namely that these people theories are generally wrong as they are often derived from people numbers that are wrong.}
}
@article{REJEB2022131439,
title = {The Internet of Things and the circular economy: A systematic literature review and research agenda},
journal = {Journal of Cleaner Production},
volume = {350},
pages = {131439},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.131439},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622010617},
author = {Abderahman Rejeb and Zailani Suhaiza and Karim Rejeb and Stefan Seuring and Horst Treiblmaier},
keywords = {Internet of things, Circular economy, Industry 4.0, Enablers, Barriers, Systematic literature review, Framework},
abstract = {In recent years, the concept of a circular economy (CE) has gained importance and attracted significant attention among scholars and practitioners. Research that examines the role of modern technologies in supporting the transition from a linear economy to the CE is therefore highly needed. This article analyzes and classifies existing research at the intersection of the CE and the Internet of Things (IoT), as an enabling technology. While studies on both concepts have proliferated, there is a lack of research that systematizes the literature and clarifies the relationship between the IoT and the CE. In order to achieve this, we reviewed a total of 170 academic articles published between 2007 and 2021 from the Scopus and Web of Science databases. Based on the coding of keywords, four categories can be identified: (1) IoT-related technologies in the CE context, (2) enablers of IoT in the CE, (3) barriers to IoT adoption in the CE, and (4) the impacts of the IoT on the sustainability of (circular) economies. The current study is the first attempt to use a keyword coding approach to better understand IoT research in the CE domain. The review findings identify important drivers and enablers and provide a structured framework for research in this field. Finally, this study highlights several research directions that may provide valuable insights for researchers and practitioners.}
}
@article{YANG2022104007,
title = {Characterizing residential load patterns on multi-time scales utilizing LSTM autoencoder and electricity consumption data},
journal = {Sustainable Cities and Society},
volume = {84},
pages = {104007},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104007},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722003274},
author = {Wei Yang and Xinhao Li and Chao Chen and Jingke Hong},
keywords = {Multi-time scale, Load patterns, LSTM Autoencoder, Electricity consumption data, Clustering},
abstract = {Load patterns represent a clear picture of electricity usage, reflecting the consumer's habits. Previous works mainly focused on load patterns discovery on a fixed scale, but limited to characterize load patterns on multi-time scales utilizing electricity consumption data (ECD). Therefore, we propose a novel framework to characterize residential load patterns on multi-time scales. The long-short-term memory autoencoder (LSTM-AE) model is designed for dimensionality reduction and feature extraction. Furthermore, a two-level clustering method is proposed to discover and characterize typical load patterns (TLPs) and multifaceted load patterns (MLPs) on multi-time scales. The proposed framework is comprehensively evaluated via extensive experiments on three real ECD. Results show that: (1) Reconstruction errors of LSTM-AE are lower than 6 benchmark models across different time scales, which validates the superiority of LSTM-AE. (2) TLPs and MLPs on daily, weekly, monthly and yearly scale are discovered by the two-level clustering method. TLPs profile the resident's electricity usages from a global view. (3) MLPs present the consumer segmentation and characterize residential load patterns of individual and groups. Especially, customer groups and electricity usage habits or lifestyles are revealed thoroughly to customize personal demand response strategies. This study can provide new valuable insights for smart grid applications.}
}
@article{YONG2022438,
title = {Robust deep auto-encoding network for real-time anomaly detection at nuclear power plants},
journal = {Process Safety and Environmental Protection},
volume = {163},
pages = {438-452},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S095758202200430X},
author = {Shi Yong and Zhang Linzi},
keywords = {Nuclear Power Plant, Anomaly detection, Multi-sensor, Time series, ConvGRU},
abstract = {Detecting anomaly conditions in nuclear reactor is a critical issue in safety management of Nuclear Power Plants (NPPs). Conventionally, the operating status are monitored in transient data with pre-designed labels by human operators or basic diagnosis systems. Nowadays, continuous time series data from multi-sensors are increasingly collected and emerging unlabeled abnormal status are monitored during the operation, making it challenging to capture both spatial and temporal dependency at each time steps without supervised labels. In this paper, a robust unsupervised Multi-Variate Convolutional GRU Encoder-Dncoder (MVCGED) method is proposed to perform anomaly detection and fault diagnosis in multi-sensor operation time series data. Specifically, MVCGED first construct each time steps into signature matrices to maintain both spatial and temporal features via sliding windows with inner-correlation and forget mechanism. Subsequently, A CNN feature extraction network, CNN-based GRU encoding network and CNN decoding network are implemented successively to capture and reconstruct the hidden patterns of the signature matrices. Finally, the reconstruction loss are further utilized to detect anomalies and diagnose faults. Extensive empirical studies based on PCTRAN nuclear power plant operation data demonstrate that MVCGED outperforms commonly-used baseline methods.}
}
@article{YANG2022117018,
title = {ISBFK-means: A new clustering algorithm based on influence space},
journal = {Expert Systems with Applications},
volume = {201},
pages = {117018},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117018},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422004365},
author = {Yuqing Yang and Jianghui Cai and Haifeng Yang and Yating Li and Xujun Zhao},
keywords = {Clustering, Influence space, Region partition, Representative data objects},
abstract = {The time overhead is huge and the clustering quality is unstable when running the K-means algorithm on massive raw data. To solve these problems, the concept of the influence space is introduced, and on this basis, a new clustering algorithm named ISBFK-means based on the influence space is proposed in this paper. First, the influence space divides the given data set into multiple small regions. Then, the representative data objects in each region are obtained to form a new data set, in which the class labels of representative data objects are those of all the data objects in the correlation influence space. Next, the K-means clustering is performed on the new data set, thereby obtaining the final clustering result. Theoretical analysis and experimental results show that this approach effectively reduces the amount of data in the clustering process and improves the stability of clustering quality. As a major feature of this work, the celestial spectral data observed by the LAMOST survey are especially employed to verify the algorithm ISBFK-means. The experimental results indicate that this algorithm has higher performance than other similar algorithms on the correctness, efficiency and sensitivity to the quality of spectral data.}
}
@article{IPENZA2022109093,
title = {QDS-COVID: A visual analytics system for interactive exploration of millions of COVID-19 healthcare records in Brazil},
journal = {Applied Soft Computing},
volume = {124},
pages = {109093},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109093},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622003787},
author = {Juan Carlos Carbajal Ipenza and Noemi Maritza Lapa Romero and Melina Loreto and Nivan Ferreira Júnior and João Luiz Dihl Comba},
keywords = {COVID-19, Electronic healthcare records, Visual analytics},
abstract = {COVID-19 is responsible for the deaths of millions of people around the world. The scientific community has devoted its knowledge to finding ways that reduce the impact and understand the pandemic. In this work, the focus is on analyzing electronic health records for one of the largest public healthcare systems globally, the Brazilian public healthcare system called Sistema Único de Saúde (SUS). SUS collected more than 42 million flu records in a year of the pandemic and made this data publicly available. It is crucial, in this context, to apply analysis techniques that can lead to the optimization of the health care resources in SUS. We propose QDS-COVID, a visual analytics prototype for creating insights over SUS records. The prototype relies on a state-of-the-art datacube structure that supports slicing and dicing exploration of charts and Choropleth maps for all states and municipalities in Brazil. A set of analysis questions drives the development of the prototype and the construction of case studies that demonstrate the potential of the approach. The results include comparisons against other studies and feedback from a medical expert.}
}
@article{CHIEN2022108245,
title = {Decision-based virtual metrology for advanced process control to empower smart production and an empirical study for semiconductor manufacturing},
journal = {Computers & Industrial Engineering},
volume = {169},
pages = {108245},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108245},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222003151},
author = {Chen-Fu Chien and Wei-Tse Hung and Chin-Wei Pan and Tran Hong {Van Nguyen}},
keywords = {Virtual metrology, Digital decision, Isolation forest, Advanced process control, Semiconductor manufacturing},
abstract = {Virtual metrology (VM) has been employed to improve the performance of advanced process control for semiconductor manufacturing. A number of VM models have been proposed to predict the quality characteristics for the wafers that have not been sampled and measured. However, little research has been done to address the interrelations between the VM model and associated decisions for advanced process control and yield enhancement. There is a research need for developing a framework that can integrate the confidence level of VM prediction and domain knowledge to derive appropriate decisions for real-time control. To fill the gaps, this study aims to develop a decision-based virtual metrology framework that integrates clustering and regression models to enhance the prediction and ensure the decision quality for the R2R controller. In particular, Isolation Forest is employed to cluster the data group for multi-recipes and multi-tools. Random Forest Regression is developed for the prediction model for each category respectively to enhance the accuracy of predicted results. Furthermore, this approach designs an overall confidence score based on data integrity and predicted results to suggest the optimal decision rules for R2R control in real time. This approach is validated with an empirical study in a leading semiconductor manufacturing company in Taiwan. Indeed, the results have demonstrated practical viability and the developed solution has been implemented.}
}
@article{NAFA2022107729,
title = {Active deep learning on entity resolution by risk sampling},
journal = {Knowledge-Based Systems},
volume = {236},
pages = {107729},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107729},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121009679},
author = {Youcef Nafa and Qun Chen and Zhaoqiang Chen and Xingyu Lu and Haiyang He and Tianyi Duan and Zhanhuai Li},
keywords = {Active learning, Deep learning, Risk analysis, Entity resolution},
abstract = {While the state-of-the-art performance on entity resolution (ER) has been achieved by deep learning, its effectiveness depends on large quantities of accurately labeled training data. To alleviate the data labeling burden, Active Learning (AL) presents itself as a feasible solution that focuses on data deemed useful for model training. Building upon the recent advances in risk analysis for ER, which can provide a more refined estimate on label misprediction risk than the simpler classifier outputs, we propose a novel AL approach of risk sampling for ER. Risk sampling leverages misprediction risk estimation for active instance selection. Based on the core-set characterization for AL, we theoretically derive an optimization model which aims to minimize core-set loss with non-uniform Lipschitz continuity. Since the defined weighted K-medoids problem is NP-hard, we then present an efficient heuristic algorithm. Finally, we empirically verify the efficacy of the proposed approach on real data by a comparative study. Our extensive experiments have shown that it outperforms the existing alternatives by considerable margins.}
}
@article{LIU2022102956,
title = {Trust secure data aggregation in WSN-based IIoT with single mobile sink},
journal = {Ad Hoc Networks},
volume = {136},
pages = {102956},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.102956},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522001330},
author = {Xiaowu Liu and Jiguo Yu and Kan Yu and Guijuan Wang and Xingjian Feng},
keywords = {Industrial Internet of Things, Secure data aggregation, Trust evaluation},
abstract = {Wireless Sensor Networks (WSNs), as the fundamental infrastructure, are indispensable for the Industrial Internet of Things (IIoT). In particular, the security and effectiveness of WSNs in IIoT are universal and inevitable issues. In this paper, a WSN-based IIoT Model (WIM) is designed characterized by a single mobile sink. Based on WIM, a high robust aggregation tree algorithm and an outlier elimination scheme are proposed in a virtual grid network with the mobile sink, which can acquire data in a real-time and accurate manner. Moreover, a trust secure data aggregation mechanism is applied in WIM which takes both the direct trust and the indirect trust into consideration to perform the secure data aggregation without losing the effectiveness of network even if the sink moves randomly. The simulation results show that the proposed model and algorithms can promote the performances of WSNs in terms of accuracy, effectiveness and delay.}
}
@article{TURET2022102056,
title = {Hybrid methodology for analysis of structured and unstructured data to support decision-making in public security},
journal = {Data & Knowledge Engineering},
volume = {141},
pages = {102056},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102056},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000544},
author = {Jean Gomes Turet and Ana Paula Cabral Seixas Costa},
keywords = {Machine learning, Public security, Decision-making process},
abstract = {This work proposes a hybrid methodology that enables the integration of structured and unstructured data to support the decision-making process in public security contexts. The proposed methodology facilitates classification and prediction of crime in a given region, making it possible to identify actions to improve public security based on the results. The integration of the data takes place in two main steps: (1) loading and analyzing structured data made available by government agencies; and (2) absorbing, classifying, and analyzing unstructured data from digital platforms such as Twitter, Where I Was Robbed, and CityCop. In this way, it becomes possible to transform these unstructured data into structured data to be incorporated into a historical database on which algorithms can act to classify, measure, and predict crime. To illustrate the applicability of this methodology, we conducted a study in the city of Recife, Brazil. Structured and unstructured data were gathered in order to conduct a neighborhood classification analysis of crime hot spots. Based on that analysis, we conducted a series of actions intended to bring improvements to the region by the local police. We obtained an increase in the algorithms’ accuracy rate of 80%, indicating that public security organizations can base their actions on the results of the proposed methodology.}
}
@article{GHAZALBASH2022197,
title = {Impact of multimorbidity and frailty on adverse outcomes among older delayed discharge patients: Implications for healthcare policy},
journal = {Health Policy},
volume = {126},
number = {3},
pages = {197-206},
year = {2022},
issn = {0168-8510},
doi = {https://doi.org/10.1016/j.healthpol.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0168851022000045},
author = {Somayeh Ghazalbash and Manaf Zargoush and Fabrice Mowbray and Andrew Costa},
keywords = {Multimorbidity, Frailty, Discharge policy, Delayed discharge, Hospital readmission, Mortality, Geriatrics},
abstract = {Objective
To assess the impacts of multiple chronic conditions (MCC) and frailty on 30-day post-discharge readmission and mortality among older patients with delayed discharge.
Data source/extraction
We used a retrospective cohort of older patients in the Discharge Abstract Database (DAD) between 2004 and 2017 in Ontario, Canada. We extracted data on patients aged ≥ 65 who experienced delayed discharge during hospitalization (N = 353,106).
Study design
We measured MCC and frailty using the Elixhauser Comorbidity Index (ECI) and the Hospital Frailty Risk Score (HFRS), respectively. We used multinomial logistic regression to model the main and interactive effects of MCC and frailty on the adverse outcomes.
Principal findings
After adjusting for sex, discharge destination, urban/rural residency, wait time for alternative care, and socioeconomic status, the coexistence of MCC and high frailty increased the relative risk of 30-day mortality and readmission when compared to the references group, i.e., non-MCC patients with low-to-moderate frailty.
Conclusions
Multimorbidity and frailty each provide unique information about adverse outcomes among older patients with delayed discharge but are most informative when examined in unison.
Implications for health policy
To minimize the risk of adverse outcomes among older delayed discharge patients, discharge planning must be tailored to their concurrent multimorbidity and frailty status.}
}
@article{DAUM2022103353,
title = {Connected cows and cyber chickens? Stocktaking and case studies of digital livestock tools in Kenya and India},
journal = {Agricultural Systems},
volume = {196},
pages = {103353},
year = {2022},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2021.103353},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X21003061},
author = {Thomas Daum and Thanammal Ravichandran and Juliet Kariuki and Mizeck Chagunda and Regina Birner},
keywords = {Farming 4.0, Digital farming, Digital livestock, Smartphones, Africa, India},
abstract = {CONTEXT
There are high hopes that digital tools can reduce constraints to livestock development, which in turn promises to alleviate poverty, improve food and nutrition security, and reduce environmental footprints. Yet, little systematic evidence exists on the state of digital livestock in low- and middle-income-countries. Thus, it remains unclear whether such high hopes are justified.
OBJECTIVE
Focusing on India and Kenya, we aim to better understand, among others, the degree of technological sophistication of the digital tools used, the types of value chains and constraints addressed, the types of business models pursued, and more broadly the opportunities and challenges of digital tools for agricultural development.
METHOD
We combine a review of digital tools in India and Kenya with three “on-the-ground” case studies: Herdman, a tool for Indian dairy organizations working with small-scale livestock keepers, facilitating data collection and supervision of field agents; Farmtree, a tool supporting medium-scale livestock keepers in India to manage their herds, and iCow, an e-extension tool for farmers in Kenya. For the review, we develop a conceptual framework that distinguishes different types of tools: 1) “simple digital tools”, providing generic information, 2) “smart digital tools”, providing tailored information based on data entered by livestock keepers, 3) “smart digital tools”, using data from sensors, 4) “digital tools for value chains”, enabling the integration of value chain actors, 5) “automated digital systems”, which are coupled with robots, allowing for automation.
RESULTS AND CONCLUSIONS
Digital tools provide many new options to address constraints to livestock development. So far, most tools are “simple digital tools”, followed by “smart digital tools” using manual data and tools for value chains. Such tools that only require smartphone ownership are the “sweet spot” for supporting digital livestock development; however, even embodied “smart digital tools” using sensors can be of relevance for small-scale livestock keepers with appropriate organizational models. Most digital tools focus on dairy production, suggesting neglect of other types of livestock, and there are few tools for pastoralists.
SIGNIFICANCE
The conceptual framework as well as many of the lessons learned are of relevance to understanding the contribution of digital tools to livestock development - and agricultural development more broadly - in low- and middle-income-countries. While digital tools are no silver bullets – and come with some new challenges such as data security and sovereignty concerns - they are likely to become a key pillar of agricultural and livestock development in the near future.}
}
@article{WANG2022109056,
title = {Data acquisition for urban building energy modeling: A review},
journal = {Building and Environment},
volume = {217},
pages = {109056},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109056},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322002955},
author = {Chao Wang and Martina Ferrando and Francesco Causone and Xing Jin and Xin Zhou and Xing Shi},
keywords = {Urban building energy modeling, UBEM, Urban energy simulation, Data acquisition, Data science},
abstract = {Urban Building Energy Modeling (UBEM) is essential for urban energy-related applications. Its generation mainly requires four data inputs, including geometric data, non-geometric data, weather data, and validation and calibration data. A reliable UBEM depends on the quantity and accuracy of the data inputs. However, the lack of available data and the difficulty in determining stochastic data are two of the main barriers in the development of UBEM. To bridge the research gaps, this paper reviews appropriate acquisition approaches for four data inputs, learning from both building science and other disciplines such as geography, transportation and computer science. In addition, detailed evaluations are also conducted in each part of the study, and the performance of the approaches are discussed, as well as the availability and cost of the implemented data. Systematic discussion, multidisciplinary analysis and comprehensive evaluation are the highlights of this review.}
}
@article{CHEN2022102907,
title = {The role of imaging radar in cultural heritage: From technologies to applications},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102907},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102907},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001091},
author = {Fulong Chen and Huadong Guo and Deodato Tapete and Francesca Cigna and Salvatore Piro and Rosa Lasaponara and Nicola Masini},
keywords = {Imaging radar, GPR, Cultural heritage, Technical integration, Interdisciplinary},
abstract = {Imaging radar has been dramatically developed over the past decades enabling a better understanding of cultural heritage from a microwave perspective. Nonetheless, a dedicated survey and analysis of the performance of such technology in cultural heritage monitoring and management is required. In order to fill this gap, we first review the technology advance of imaging radar, including ground penetration radar, ground-based and airborne/satellite radar, in the focused cultural applications to grasp the development trend of these technologies. We then analyse the performance and limitations of imaging radar technologies based on their respective characteristics to facilitate the technology service in practical applications. Finally, we propose a flexible solution of imaging radar in cultural heritage through technical integration with pilot synergy applications in archaeological prospection and cultural heritage diagnosis and conservation.}
}
@article{KALAITZI2022108466,
title = {Supply chain analytics adoption: Determinants and impacts on organisational performance and competitive advantage},
journal = {International Journal of Production Economics},
volume = {248},
pages = {108466},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108466},
url = {https://www.sciencedirect.com/science/article/pii/S0925527322000597},
author = {Dimitra Kalaitzi and Naoum Tsolakis},
keywords = {Supply chain analytics, TOE Framework, Acceptance and adoption, Survey, Manufacturing industry},
abstract = {Despite manufacturing companies recognising the potential benefits associated with the adoption of Supply Chain Analytics (SCA), only a few firms adopt data-based decision-making processes due to fundamental technical, organisational and environmental challenges. In this regard, this research explores the determinants influencing SCA adoption and the impacts on firm performance and competitive advantage. Specifically, the Technological, Organisational, and Environmental (TOE) framework was applied to identify the key determinants influencing SCA adoption. Data was collected from 217 executives working in the UK manufacturing sector through a questionnaire-based survey. The research model was tested using a quantitative approach, i.e., Partial Least Squares Structural Equation Modelling. Surprisingly, none of the identified technological factors leads manufacturing companies to adopt SCA. On the contrary, organisational and environmental factors have a crucial role in influencing supply chain and logistics managers to adopt SCA. This research also emphasises and validates the importance of SCA adoption in improving firm performance and fostering competitive advantage. On evaluating SCA adoption, supply chain managers should concentrate on aspects other than technological competence. Manufacturing companies looking to make investment decisions regarding SCA adoption should mainly consider organisational and environmental factors; hence, SCA systems can be used effectively and efficiently. This study is the first to explore the TOE framework regarding the adoption determinants within an SCA context along with its implications on organisational performance and competitive edge.}
}
@article{LOPES202255,
title = {Effective network intrusion detection via representation learning: A Denoising AutoEncoder approach},
journal = {Computer Communications},
volume = {194},
pages = {55-65},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002742},
author = {Ivandro O. Lopes and Deqing Zou and Ihsan H. Abdulqadder and Francis A. Ruambo and Bin Yuan and Hai Jin},
keywords = {Deep learning, Denoising autoencoder, Intrusion detection, Cybersecurity},
abstract = {The introduction of deep learning techniques in intrusion detection problems has enabled an enhanced standard of detection effectiveness. However, most of the progress has occurred in supervised learning, which required a vast amount of labeled training samples. In the real world, there is a limited amount of labeled data available to train a deep neural network, affecting the classifier’s detection performance. Therefore, to address the lack of labeled network traffic required to train an effective supervised classifier, this study introduces a semi-supervised intrusion detection framework that combines the unsupervised and supervised techniques. The unsupervised pre-training approach is implemented based on a denoising autoencoder (DAE), to compress the intrusion dataset and obtain the lower-dimensional features representation. Then a portion of the compressed data is used to train the DNN classifier based on a multiclass supervised approach. The network architecture is optimized by tuning hyper-parameters using a trial-and-error approach. Comparative analysis is performed between the proposed approach and the most relevant deep learning methods available in the literature against the CICIDS2018 dataset, consisting of recent network attack traces. Our approach outperforms competitive methods while maintaining stable classification results above 99.6% on F1-score, precision, and recall metrics. Additionally, it is trained in 64 min while achieving a low false alarm rate. Furthermore, the DAE module reduces the input network traffic data to one-tenth of the size of the input dataset.}
}
@article{CHEN2022108895,
title = {Pest incidence forecasting based on Internet of Things and Long Short-Term Memory Network},
journal = {Applied Soft Computing},
volume = {124},
pages = {108895},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108895},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002666},
author = {Ching-Ju Chen and Yuan-Shuo Li and Chen-Yu Tai and Ying-Cheng Chen and Yueh-Min Huang},
keywords = {Smart agriculture, Long Short-Term Memory, Machine Learning, Artificial intelligence, Internet of Things, Long-range, Pest prevention and cure, Weather factors},
abstract = {The infestation of litchi stink bugs (Tessaratoma papillosa) has always had a significant impact on the yield of longan plantations. Pest control is critical for farmers to detect and timely suppress the occurrence of pests while effectively reducing damages. Environmental factors, climate change in particular, have contributed to the growing population of pests whereas weather can vary in different terrains, locations, and time. Due to the geographical and topographical conditions of Taiwan, this study focuses on investigating fruit plantations on sloping land in subtropics with distinct seasonal changes. The article aims at forecasting meteorological data based on Long short-term memory network (LSTM) and identifying the correlation between pest infestation and environmental factors through Machine Learning (ML). In this section, the structure and experimental process of the research will be outlined. At the first stage, meteorological information of the experimented site is obtained through the self-designed IoT (Internet of Things) system and wireless long-distance transmission technology. Since meteorological information forecasted is displayed in time series, multi-layer LSTM and bidirectional LSTM are used to solve the problem. Finally, environmental data and field surveys conducted for pest surveillance will be employed to forecast the severity of pest infestation through KNN, SVM, and random forest models. The result of the experiment shows that LSTM performs well in weather forecasting with 96% R-Squared values whereas the accuracy rate of pest prediction conducted by Machine Learning (ML) is 85%. The study verifies that meteorological factors do affect pest incidence. For example, the population of litchi stink bugs increase easily under suitable temperature, humidity, and sunlight. LSTM is superior in providing solutions for long-range dependence in statistics. This article shall present regions with shifting weather patterns, meteorological conditions and time length forecasted corresponding to the oceanic climate, as well as the correlation between pest population and environmental factors.}
}
@article{ZHA2022,
title = {Microbial dark matter: from discovery to applications},
journal = {Genomics, Proteomics & Bioinformatics},
year = {2022},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2022.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1672022922000377},
author = {Yuguo Zha and Hui Chong and Pengshuo Yang and Kang Ning},
keywords = {Microbiome, Dark matter, Artificial intelligence, Knowledge discovery, Applications},
abstract = {With the rapid increase of the microbiome samples and sequencing data, more and more knowledge about microbial communities has been gained. However, there is still much more to learn about microbial communities, including billions of novel species and genes, as well as countless spatiotemporal dynamic patterns within the microbial communities, which together form the microbial dark matter. In this work, we summarized the dark matter in microbiome research and reviewed current data mining methods, especially artificial intelligence (AI) methods, for different types of knowledge discovery from microbial dark matter. We also provided case studies on using AI methods for microbiome data mining and knowledge discovery. In summary, we view microbial dark matter not as a problem to be solved but as an opportunity for AI methods to explore, with the goal of advancing our understanding of microbial communities, as well as developing better solutions to global concerns about human health and the environment.}
}
@article{OJO2022107266,
title = {Internet of Things and Machine Learning techniques in poultry health and welfare management: A systematic literature review},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107266},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107266},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005798},
author = {Rasheed O. Ojo and Anuoluwapo O. Ajayi and Hakeem A. Owolabi and Lukumon O. Oyedele and Lukman A. Akanbi},
keywords = {Behavioral parameters, Environmental parameters, Deep learning, Computer vision, Vocalization},
abstract = {The advent of digital technologies has brought substantial improvements in various domains. This article provides a comprehensive review of research emphasizing AI-enabled IoT applications in poultry health and welfare management. This study focused on poultry welfare since modern poultry management is confronted with issues relating to standardized parameters for welfare assessment and robust monitoring systems, particularly for broilers' health and disease outbreak prevention. Evidence has shown that modern digital technologies have high possibilities for intelligent automation of current and future poultry management operations to facilitate high-quality and low-cost poultry production. Therefore, this study presents a systematic review of the current state-of-the-art AI-enabled IoT systems and their recent advances in developing intelligent systems in this domain. Also, the study provides an overview of the critical applications of identified digital technologies in poultry welfare management. Lastly, the study discusses the challenges and opportunities of AI and IoT in poultry farming.}
}
@article{IBRAHIM2022112446,
title = {A review on the deployment of demand response programs with multiple aspects coexistence over smart grid platform},
journal = {Renewable and Sustainable Energy Reviews},
volume = {162},
pages = {112446},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112446},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122003525},
author = {Charles Ibrahim and Imad Mougharbel and Hadi Y. Kanaan and Nivine Abou Daher and Semaan Georges and Maarouf Saad},
keywords = {Demand response programs, Approach, Marketing, Technical, Economic objectives, Architecture, Business intelligence},
abstract = {A modern strategy for improving the control of generation, distribution and consumption of electrical energy consists of launching programs affecting the demand on the consumer side. Objectives related to the so-called Demand Response Programs (DRP) are of technical, economic and marketing order. Multiple DRPs are actually proposed and researchers keep on suggesting others. Although DRPs suggestions are still under tests for validation and implementation, an extensive literature exists in this domain. Multiple interdependent objectives are targeted and individual treatment might lead to undesired outcome. It is paramount to perform a holistic review showing the implications of the objectives on each other. The aim is to include them in a consolidated model with assigned weights where intensive what if scenarios will be applied to reach the optimal model settings. Existing reviews and literature in this domain focused on individual or partially correlated aspects. This paper presents a review providing a wide coverage on existing approaches and objectives for implementing DRPs and their relation. Therefore, authors decided to focus on the technical, economic and marketing aspects with the consideration of architectures and business intelligence. Model structures with their variability and dynamicity enable a financially viable model with various market options correlated with the economic and technical aspects. Hence, this mechanism assists in planning adequately and supporting the decision-making process.}
}
@article{KHAN2022100074,
title = {Information sharing in supply chains – Interoperability in an era of circular economy},
journal = {Cleaner Logistics and Supply Chain},
volume = {5},
pages = {100074},
year = {2022},
issn = {2772-3909},
doi = {https://doi.org/10.1016/j.clscn.2022.100074},
url = {https://www.sciencedirect.com/science/article/pii/S2772390922000476},
author = {Athar Ajaz Khan and János Abonyi},
keywords = {Supply chains informatics, Data Integration, Interoperability, Supply chain data standards, Circular economy, SSCM, Optimization, Industry 5.0},
abstract = {In order to realize the goals of Industry 5.0 (I5.0), which has data interoperability as one of its core principles, the future research in the Supply Chain (SC) visibility has to be aligned with socially, economically and environmentally sustainable objectives. Within the purview of circular economy, this paper indicates various aspects and implications of data sharing in the SCs in light of the published research. Taking into consideration the heterogeneity of data sources and standards, this article also catalogs all the major data-sharing technologies being employed in sharing data digitally across the SCs. Drawing on the published research from 2015 to 2021, following the PRISMA framework, this paper presents the state of research in the field of data sharing in SCs in terms of their standardization, optimization, simulation, automation, security and more notably sustainability. Using the co-occurrence metric, bibliometric analysis has been conducted such that the collected research is categorized under various keyword clusters and regional themes. This article brings together two major themes in reviewing the research in the field. Firstly, the bibliometric analysis of the published articles demonstrates the contours of the current state of research and the future possibilities in the field. Secondly, in synthesizing the research on the foundations of sustainability within the CRoss Industry Standard Process for Data Mining (CRISP-DM) framework, this article deals with various aspects and implications of information sharing in the SCs. By bringing these two themes together, this paper affords a prospective researcher with the research vis-à-vis the information sharing in SC, starting from the actual data standards in use to the modality and consequence of their application within the perspective of the circular economy. This article, in essence, indicates how all the aspects of data sharing in SCs may be brought together in service of the paradigm of I5.0.}
}
@article{TANG2022103679,
title = {A literature review of Artificial Intelligence applications in railway systems},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {140},
pages = {103679},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103679},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22001206},
author = {Ruifan Tang and Lorenzo {De Donato} and Nikola Bes̆inović and Francesco Flammini and Rob M.P. Goverde and Zhiyuan Lin and Ronghui Liu and Tianli Tang and Valeria Vittorini and Ziyulong Wang},
keywords = {Artificial Intelligence, Railways, Transportation, Machine Learning, Autonomous driving, Maintenance, Smart mobility, Train control, Traffic management},
abstract = {Nowadays it is widely accepted that Artificial Intelligence (AI) is significantly influencing a large number of domains, including railways. In this paper, we present a systematic literature review of the current state-of-the-art of AI in railway transport. In particular, we analysed and discussed papers from a holistic railway perspective, covering sub-domains such as maintenance and inspection, planning and management, safety and security, autonomous driving and control, revenue management, transport policy, and passenger mobility. This review makes an initial step towards shaping the role of AI in future railways and provides a summary of the current focuses of AI research connected to rail transport. We reviewed about 139 scientific papers covering the period from 2010 to December 2020. We found that the major research efforts have been put in AI for rail maintenance and inspection, while very limited or no research has been found on AI for rail transport policy and revenue management. The remaining sub-domains received mild to moderate attention. AI applications are promising and tend to act as a game-changer in tackling multiple railway challenges. However, at the moment, AI research in railways is still mostly at its early stages. Future research can be expected towards developing advanced combined AI applications (e.g. with optimization), using AI in decision making, dealing with uncertainty and tackling newly rising cybersecurity challenges.}
}
@article{TRUDGETT2022101302,
title = {A framework for operationalising Aboriginal and Torres Strait Islander data sovereignty in Australia: Results of a systematic literature review of published studies},
journal = {eClinicalMedicine},
volume = {45},
pages = {101302},
year = {2022},
issn = {2589-5370},
doi = {https://doi.org/10.1016/j.eclinm.2022.101302},
url = {https://www.sciencedirect.com/science/article/pii/S2589537022000323},
author = {Skye Trudgett and Kalinda Griffiths and Sara Farnbach and Anthony Shakeshaft},
abstract = {Summary
Background
Racial health disparities are only likely to be meaningfully improved by tailoring public health and clinical interventions to the specific needs of Indigenous people and their communities. Accurate tailoring relies on the availability of high-quality Indigenous-specific data. The potential benefits of increased availability of Indigenous data need to be balanced by efforts to ensure those data are collected and used appropriately. This paper identifies characteristics of Indigenous Data Sovereignty (IDS) principles and considers a framework for operationalisation.
Methods
A PRISMA compliant search of the literature was undertaken, using methods detailed in the Cochrane Collaboration Handbook on Systematic Reviews of Health Promotion and Public Health Interventions (1). The search strategy comprised two steps: a search of 11 scientific electronic databases and five grey literature sources. The search was limited by date of publication (1 January 2000 to 1 December 2021). The following keywords and subject heading terms were used: (exp Aboriginal and Torres Strait Islander or Aborigin* or Torres Strait Island* or, Oceanic ancestry group) and (exp research or biomedical research or population surveillance or translational medical research or, research design) and (exp data or datasets or data collection or data management or health surveys or information dissemination or, intellectual property) and (exp self-determination or ownership or control or access or possession or OCAP or sovereignty or, ethics) and, (exp Australia). IDS principles: (i) ownership; (ii) control; (iii) accessibility; (iv) custodianship; (v) accountability to Indigenous people; (vi) amplify Community voice; (vii) relevant and reciprocal; and (viii) sustainably self-determining. Using standard data extraction forms, we examined relevant Australian studies to identify key characteristics and frequency with which they cited IDS principles. These findings were consolidated into an operationalisation framework.
Findings
34 relevant Australian published studies were identified. The most frequently cited IDS principles were Accountability to Aboriginal and Torres Strait Islander peoples and sustainably self-determining. The least frequently cited principle was Access. A framework to operationalise IDS principles is proposed that is both standardised internationally and able to be tailored to the diverse contexts of Indigenous peoples.
Interpretation
IDS is emergent in Australia and there is a clear need to establish an agreed set of International IDS principles and a framework for their operationalisation and contextualisation across diverse Indigenous communities and contexts.
Funding
This research project is funded through an Australian Research Council (ARC) Discovery Grant from 2017 to 2022. The National Drug and Alcohol Research Centre (NDARC) is funded by the Australian Government Department of Health. The 1st author (ST) is supported by a scholarship co-funded by NDARC and the Lowitja Institute.}
}
@article{KOSTAL2022105256,
title = {O Data, Where Art Thou? Revolutionizing data sharing to advance our sustainability goals through smart chemical innovation},
journal = {iScience},
pages = {105256},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105256},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222015280},
author = {Jakub Kostal and Bryan W. Brooks and Christopher A. Smith and Geetesh Devineni},
abstract = {Summary
Antiquated and inefficient data-sharing practices represent one of the key obstacles to advancing sustainability goals through green chemistry. To this end, we need to robustly link data on chemical impacts with new chemical design strategies, which requires the development of next-generation data-sharing platforms to harmonize both data and efforts. These decentralized and interactive programs should be structured as live ecosystems for data generation and exchange, inviting conversations about the reliability and relevance of information used to make decisions regarding chemical performance and safety.}
}
@article{VARELA2022982,
title = {Risks of Data Science Projects - A Delphi Study},
journal = {Procedia Computer Science},
volume = {196},
pages = {982-989},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.100},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921023231},
author = {Cristina Varela and Luísa Domingues},
keywords = {Data Science, project sucess, project risk management, risk assessment, Delphi study},
abstract = {Risk is one of the most crucial components of a project. Its proper evaluation and treatment increase the chances of a project’s success. This article presents the risks in Data Science projects, assessed through a study conducted with the Delphi technique, to answer the question, "What are the risks of Data Science projects". The study allowed the identification of specific risks related to data science projects, however it was possible to verify that over a half of the most mentioned risks are similar to other types of IT projects. This paper describes the research from expert selection, risk identification and analysis, and the first conclusions.}
}
@article{YAN2022112155,
title = {Research on repair method of abnormal energy consumption data of lighting and plug based on similar features},
journal = {Energy and Buildings},
volume = {268},
pages = {112155},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112155},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822003267},
author = {Huiyu Yan and Liangdong Ma and Tianyi Zhao and Jili Zhang},
keywords = {Energy consumption monitoring, NN-BS algorithm, Abnormal data, Data repair},
abstract = {The public building energy consumption monitoring platforms (BECMPs) play an important role in building energy conservation and energy-related decision-making. However, problems such as data missing and data mutation are quite common that they impede the effectiveness of the BECMPs. Based on the kNN algorithm, this paper proposes a kNN-BS data repair algorithm with the characteristics of electricity consumption trends taken into consideration. After data classification and cleaning, the data of hourly lighting and plug power consumption of an office building of Dalian University of Technology is repaired using the kNN-BS algorithm. The cross-validation method is used to determine the optimal k value. The relative repair error of the total daily electricity consumption of kNN-BS is less than 4% for working days and less than 6% for non-working days. Compared with kNN, the average CVRMSE of the kNN-BS algorithm is reduced by 1.36%∼1.59% for working days and 5.49%∼8.17% for non-working days. The kNN-BS algorithm shows excellent stability and robustness compared with kNN, polynomial and BPNN, which makes it very suitable for practical use, especially in efficiency-demanding scenarios. The kNN-BS algorithm can effectively improve the quality of public building energy consumption monitoring data and provide a theoretical basis and technical means for solving data quality problems in the BECMPs.}
}
@article{YANG2022551,
title = {Automated Analysis of Doppler Echocardiographic Videos as a Screening Tool for Valvular Heart Diseases},
journal = {JACC: Cardiovascular Imaging},
volume = {15},
number = {4},
pages = {551-563},
year = {2022},
issn = {1936-878X},
doi = {https://doi.org/10.1016/j.jcmg.2021.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X21006434},
author = {Feifei Yang and Xiaotian Chen and Xixiang Lin and Xu Chen and Wenjun Wang and Bohan Liu and Yao Li and Haitao Pu and Liwei Zhang and Dangsheng Huang and Meiqing Zhang and Xin Li and Hui Wang and Yueheng Wang and Huayuan Guo and Yujiao Deng and Lu Zhang and Qin Zhong and Zongren Li and Liheng Yu and Yongjie Duan and Peifang Zhang and Zhenzhou Wu and Daniel Burkhoff and Qiushuang Wang and Kunlun He},
keywords = {aortic regurgitation, aortic stenosis, deep learning, mitral regurgitation, mitral stenosis},
abstract = {Objectives
This study sought to develop a deep learning (DL) framework to automatically analyze echocardiographic videos for the presence of valvular heart diseases (VHDs).
Background
Although advances in DL have been applied to the interpretation of echocardiograms, such techniques have not been reported for interpretation of color Doppler videos for diagnosing VHDs.
Methods
The authors developed a 3-stage DL framework for automatic screening of echocardiographic videos for mitral stenosis (MS), mitral regurgitation (MR), aortic stenosis (AS), and aortic regurgitation (AR) that classifies echocardiographic views, detects the presence of VHDs, and, when present, quantifies key metrics related to VHD severities. The algorithm was trained (n = 1,335), validated (n = 311), and tested (n = 434) using retrospectively selected studies from 5 hospitals. A prospectively collected set of 1,374 consecutive echocardiograms served as a real-world test data set.
Results
Disease classification accuracy was high, with areas under the curve of 0.99 (95% CI: 0.97-0.99) for MS; 0.88 (95% CI: 0.86-0.90) for MR; 0.97 (95% CI: 0.95-0.99) for AS; and 0.90 (95% CI: 0.88-0.92) for AR in the prospective test data set. The limits of agreement (LOA) between the DL algorithm and physician estimates of metrics of valve lesion severities compared to the LOAs between 2 experienced physicians spanned from −0.60 to 0.77 cm2 vs −0.48 to 0.44 cm2 for MV area; from −0.27 to 0.25 vs −0.23 to 0.08 for MR jet area/left atrial area; from −0.86 to 0.52 m/s vs −0.48 to 0.54 m/s for peak aortic valve blood flow velocity (Vmax); from −10.6 to 9.5 mm Hg vs −10.2 to 4.9 mm Hg for average peak aortic valve gradient; and from −0.39 to 0.32 vs −0.31 to 0.32 for AR jet width/left ventricular outflow tract diameter.
Conclusions
The proposed deep learning algorithm has the potential to automate and increase efficiency of the clinical workflow for screening echocardiographic images for the presence of VHDs and for quantifying metrics of disease severity.}
}
@article{CHEN2022106826,
title = {Effects of assignments of dedicated automated vehicle lanes and inter-vehicle distances of automated vehicle platoons on car-following performance of nearby manual vehicle drivers},
journal = {Accident Analysis & Prevention},
volume = {177},
pages = {106826},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2022.106826},
url = {https://www.sciencedirect.com/science/article/pii/S0001457522002615},
author = {Facheng Chen and Guangquan Lu and Haitian Tan and Miaomiao Liu and Hongfei Wan},
keywords = {DAVL, AV platoons, Lane assignment, Inter-vehicle distance, AV platoon speed, Car-following performance},
abstract = {Deploying dedicated lanes for automated vehicles (AVs) can effectively alleviate the coordination issues between AVs and manual vehicles (MVs). However, AV platoons running on dedicated AV lanes (DAVLs) have a prominent collective behavior characteristic of small inter-vehicle distance. The nearby MV drivers’ imitation of this characteristic may reduce their car-following time headway (THW). The researchers conducted a simulation experiment to investigate the influence of DAVL assignments, inter-vehicle distances of AV platoons and AV platoon speed on the car-following performance of nearby MV drivers. The data of mean THW, standard deviation of THW, standard deviation of lateral position, standard deviation of velocity, standard deviation of horizontal gaze position and mean saccadic peak velocity were collected from 36 participants. Statistical analysis results show that the three factors considerably affected the MV drivers’ car-following performance. In particular, the MV drivers showed a worse car-following safety but a better driving stability when the left lane was dedicated to AVs than when the right lane was dedicated to AVs (Note the experiments were done in a drive-on-the-left environment.). With respect to the inter-vehicle distances of AV platoons, the MV drivers’ car-following safety was poorer under the 4 m condition than that under the 10 and 18 m conditions. In addition, the MV drivers showed a poorer car-following safety and bore a larger mental workload when driving next to the AV platoons running at 110 km/h. This study may provide some suggestions for DAVLs. Assigning the right lane of a three-lane motorway as the DAVL may have a slighter negative impact on the nearby MV drivers in China. In terms of traffic management in DAVLs, the inter-vehicle distance of AV platoons can be reduced to 10 m, and the speed of AVs should not be higher than the design speed of adjacent MV lanes.}
}
@article{REN2022133,
title = {Ensuring the quality of meat in cold chain logistics: A comprehensive review},
journal = {Trends in Food Science & Technology},
volume = {119},
pages = {133-151},
year = {2022},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2021.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924224421006634},
author = {Qing-Shan Ren and Kui Fang and Xin-Ting Yang and Jia-Wei Han},
keywords = {Meat, Cold chain logistics, Packaging, Quality perception, Intelligent development},
abstract = {Background
Meat packaging and intelligent evaluation and monitoring of key parameters not only are important technologies to ensure meat quality and safety but also form the key foundation for optimizing packaging materials and improving the efficiency of cold chain operations. In recent years, numerous studies have focused on comprehensive (or multi-functional) packaging materials, multiple parameter evaluation methods, quality intelligent monitoring technology, and optimization of the control of various links in cold chain logistics (CCL). Such research has significant practical application value for extending meat shelf-life and reducing the risk of foodborne diseases.
Scope and approach
This paper reviews the current research status, existing problems, and future evolution of CCL by focusing on meat packaging, meat quality evaluation and monitoring, and meat quality prediction and control. We also elaborate in detail the challenges faced in researching these topics and discuss the focal points of future research aiming to improve the quality and efficiency of CCL.
Key findings and conclusions
Packaging material optimization and dynamic quality perception are vital for achieving meat quality and safety over the entire CCL and demand the digital and intelligent development of the meat cold chain. A key finding of this review is that the comprehensive (or composite) packaging and intelligent quality assessment and monitoring are important forces promoting the transformation of traditional meat CCL to smart, green, and efficient CLL involving the intelligent management and control of all links therein.}
}
@article{SAW202212,
title = {Current challenges of implementing artificial intelligence in medical imaging},
journal = {Physica Medica},
volume = {100},
pages = {12-17},
year = {2022},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2022.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1120179722019962},
author = {Shier Nee Saw and Kwan Hoong Ng},
keywords = {Artificial intelligence, Medical imaging, Challenges, Ethics, Data governance, Algorithm robustness},
abstract = {The idea of using artificial intelligence (AI) in medical practice has gained vast interest due to its potential to revolutionise healthcare systems. However, only some AI algorithms are utilised due to systems’ uncertainties, besides the never-ending list of ethical and legal concerns. This paper intends to provide an overview of current AI challenges in medical imaging with an ultimate aim to foster better and effective communication among various stakeholders to encourage AI technology development. We identify four main challenges in implementing AI in medical imaging, supported with consequences and past events when these problems fail to mitigate. Among them is the creation of a robust AI algorithm that is fair, trustable and transparent. Another issue is on data governance, in which best practices in data sharing must be established to promote trust and protect the patients’ privacy. Next, stakeholders, such as the government, technology companies and hospital management, should come to a consensus in creating trustworthy AI policies and regulatory frameworks, which is the fourth challenge, to support, encourage and spur innovation in digital AI healthcare technology. Lastly, we discussed the efforts of various organizations such as the World Health Organisation (WHO), American College of Radiology (ACR), European Society of Radiology (ESR) and Radiological Society of North America (RSNA), who are already actively pursuing ethical developments in AI. The efforts by various stakeholders will eventually overcome hurdles and the deployment of AI-driven healthcare applications in clinical practice will become a reality and hence lead to better healthcare services and outcomes.}
}
@article{HUANG2022105152,
title = {Location-Refining neural network: A new deep learning-based framework for Heavy Rainfall Forecast},
journal = {Computers & Geosciences},
volume = {166},
pages = {105152},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105152},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422001078},
author = {Xu Huang and Chuyao Luo and Yunming Ye and Xutao Li and Bowen Zhang},
keywords = {Weather forecast, Radar images prediction, Deep learning, Artificial neural networks},
abstract = {Precipitation nowcasting aims to predict the rainfall distribution within a short-term period. However, it pays the same attention to all locations instead of emphasizing those regions with heavy rainfall that has more threats to human activity. Therefore, we develop an important task named Heavy Rainfall Forecast (HRF), which mainly focuses on the movement and change of heavy rainfall areas. It sets aside one hour to give meteorological administration sufficient time to issue warning information. To tackle this task, firstly, we rebuild the meteorological radar dataset based on three criteria to obtain the samples involving heavy rainfall. Secondly, we propose the Location-Refining (LR) neural network to combine the advantages of the optical flow-based and deep learning-based methods in predicting higher intensity and more accurate position, respectively. LR neural network consists of a location network and a refining network. The former is responsible for the accurate predictions of position and trend of rainfall, and the later accounts for more accurately estimating the intensity. To make the model pay more attention to the high echo region, we design new loss functions and introduce auxiliary information of high echo values. A series of experiments show that our model has a significant improvement on this task. Specifically, compared with existing methods, we improve the valid mean square error by 6.4% for the threshold being 20 and 15.1% for the threshold being 30. The critical success indexes are improved by 12.8% for the threshold being 20 and 24.8% for the threshold being 30. We also improve the heidke skill score by 9.9% for the threshold being 20 and 21.4% for the threshold being 30. Furthermore, the proposed framework can be well transferred to other deep learning-based models, and improves their performance.}
}
@article{BERGMANN20221204,
title = {Tool failure recognition using inconsistent data},
journal = {Procedia CIRP},
volume = {107},
pages = {1204-1209},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.132},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004164},
author = {Júlia Bergmann and Klaudia Éva Zeleny and József Váncza and Andrea Kő},
keywords = {Type your keywords here, separated by semicolons, artificial intelligence, failure detection, data preparation, t-SNE, deep insight, linear programming},
abstract = {Data is everything - at least this is one of the main messages of the ongoing industrial revolution. Manufacturing companies all over the world are expanding their digital infrastructure and knowledge on data analysis in the hope of increasing their KPIs with the help of artificial intelligence (AI). Although several well-designed data-driven solutions are available, the most crucial part, data preparation is still not fully supported. In this paper a framework is presented for processing sensor data of machining processes with variable cycle times in an unstable environment. Traditional and novel AI algorithms are tested on the data of a vulcanization process from the automotive industry, namely from tire manufacturing’s curing phase. The process in question consists of several subprocesses, and the quality of curing is mostly dependent of the status of a specific type of machine tool. Conventional methods (e.g., examining the cured product manually) are currently used for failure recognition, however the examination is only feasible after a long delay due to the extreme level of heat, which leads to unnecessary and unwanted scrap production. Therefore, a more sophisticated and complex approach is required to increase quality score. A combination of mathematical methods is proposed combining t-SNE feature representation, convolutional neural network, and linear programming optimization. The model highly relies on the tool’s continuous degradation characteristics. The threshold for the given binary classification is set by maximizing the accuracy of the detection model. The main contribution of the research is the method of inconsistent sensor data manipulation which supports a unique combination of AI models for early failure recognition.}
}
@article{CHEN2022104445,
title = {A training pattern recognition algorithm based on weight clustering for improving cooling load prediction accuracy of HVAC system},
journal = {Journal of Building Engineering},
volume = {52},
pages = {104445},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.104445},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222004582},
author = {Sihao Chen and Liangzhu (Leon) Wang and Jing Li and Guang Zhou and Xiaoqing Zhou},
keywords = {Cooling load prediction, Pattern recognition, Clustering algorithm, Data preprocessing, Mode identification},
abstract = {The cooling load-based optimal control is an advanced technology for the efficient operation of heating, ventilation, and air conditioning (HVAC). Thus, the prediction reliability of cooling load plays a key role in HVAC's optimal control. Current publications primarily focused on the structure optimization of prediction models, while less on the clustering-based cooling load prediction. However, the data quality determines the upper limit of the model's prediction performance. Thus, a training pattern recognition algorithm based on weight clustering is proposed for improving cooling load prediction accuracy. Compared with the existing clustering-based prediction methods, the main innovations of the proposed method are: (i) considering the input variables' weights on cooling load in the clustering process; and (ii) investigating the matching between the various prediction models and the K-means clustering algorithm. The case studies showed that the proposed method achieves a significant improvement in the prediction performance, such as MAPEs of the MLR, MNR, and ANN decrease by 34.67%, 35.56%, and 14.53% on average, respectively. Compared with the non-weights clustering method, the introduction of the weights can further improve the above models' prediction accuracy, such as their MAPEs decrease by 6.30%, 7.59%, and 3.07% on average, respectively. These results also demonstrated that the clustering-based prediction method is more suitable for the regression models (e.g., MLR and MNR) with low complexity compared to the ANN. When the clustering number is about 4, the models' prediction performances were more robust. Applying the proposed method to the time-series models (i.e., AR, ARX, and ANN) resulted in their MAPEs as low as 1.79%, 1.78%, and 2.06%, respectively. the proposed method can provide a new idea for improving the accuracy of cooling load prediction.}
}
@article{BURKE2022101598,
title = {Tag Frequency Difference: Rapid estimation of image set relevance for species occurrence data using general-purpose image classifiers},
journal = {Ecological Informatics},
volume = {69},
pages = {101598},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101598},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000474},
author = {Hannah M. Burke and Reid Tingley and Alan Dorin},
keywords = {Biodiversity monitoring, Computer vision, Data mining, iEcology, Social media},
abstract = {iEcology is used to supplement traditional ecological data by sourcing large quantities of media from the internet. Images and their metadata are widely available online and can provide information on species occurrence, behaviour and visible traits. However, this data is inherently noisy and data quality varies significantly between sources. Many iEcology studies utilise data from a single source for simplicity and efficiency. Hence, a tool to compare the suitability of different media sources in addressing a particular research question is needed. We provide a simple, novel way to estimate the fraction of images within multiple unverified datasets that potentially depict a specified target fauna. Our method, the Sum of Tag Frequency Differences (STFD), uses any pretrained, general-purpose image classifier. One of the method's innovations is that it does not require training the classifier to recognise the target fauna. Instead, STFD analyses the frequency of the generic text-tags returned by a classifier for multiple datasets and compares them to the corresponding frequencies of an authoritative image dataset that depicts only the target organism. From this comparison, STFD allows us to deduce the fraction of images of the target in unverified datasets. To validate the STFD approach, we processed images from five sources: Flickr, iNaturalist, Instagram, Reddit and Twitter. For each media source, we conducted an STFD analysis of three fauna invasive to Australia: Cane toads (Rhinella marina), German wasps (Vespula germanica), and the higher-level colloquial taxonomic classification, “wild rabbits”. We found the STFD provided an accurate assessment of image source relevance across all data sources and target organisms. This was demonstrated by the consistent, very strong correlation (toads r ≥0.97, wasps r ≥0.95, wild rabbits≥ 0.95) between STFD predictions, and the fraction of target images in a source dataset observed by a human expert. The STFD provides a low-cost, simple and accurate comparison of the relevance of online image sources to specific fauna for iEcology applications. It does not require expertise in machine learning or training neural-network species-specific classifiers. The method enables researchers to assess multiple image sources to select those warranting detailed investigation for the development of tools for web-scraping, citizen science campaigns, further monitoring or analysis.}
}
@article{DEBRAH2022104192,
title = {Artificial intelligence in green building},
journal = {Automation in Construction},
volume = {137},
pages = {104192},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104192},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522000656},
author = {Caleb Debrah and Albert P.C. Chan and Amos Darko},
keywords = {Artificial intelligence, Green building, Bibliometric analysis, Systematic analysis, Sustainability},
abstract = {The Architecture, Engineering and Construction (AEC) sector faces severe sustainability and efficiency challenges. The application of artificial intelligence in green building (AI-in-GB) is an effective solution to enhance the sustainability and efficiency of the sector. While studies have been conducted in the AI-in-GB domain, an in-depth study on the state-of-the-art of AI-in-GB research is hitherto lacking. To provide a better understanding of this underexplored area, this study was initiated via a bibliometric-systematic analysis method. The study aims to reveal the synthesis between AI and GB, as well as to highlight research trends along with knowledge gaps that may be tackled in future AI-in-GB research. A quantitative bibliometric analysis was conducted to objectively identify the major research hotspots, trends, knowledge gaps and future research needs based on 383 research publications identified from Scopus. A further qualitative systematic analysis was also conducted on 76 screened research publications on AI-in-GB. Through this mixed-methods systematic review, knowledge gaps were identified, and future research directions of AI-in-GB were proposed as follows: digital twins and AI of things; blockchain; robotics and 4D printing; and legal, ethical, and moral responsibilities of AI-in-GB. This study adds to the GB knowledge domain by synthesizing the state-of-the-art of AI-in-GB and revealing the research needs in this field to enhance the sustainability and efficiency of the AEC sector.}
}
@article{LI2022,
title = {Intelligent Drilling and Completion: A Review},
journal = {Engineering},
year = {2022},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922006257},
author = {Gensheng Li and Xianzhi Song and Shouceng Tian and Zhaopeng Zhu},
keywords = {Intelligent drilling and completion, Artificial intelligence, Intelligent application scenarios, Literature review, Systematic discuss},
abstract = {The application of artificial intelligence (AI) has become inevitable in the petroleum industry. In drilling and completion engineering, AI is regarded as a transformative technology that can lower costs and significantly improve drilling efficiency (DE). In recent years, numerous studies have focused on intelligent algorithms and their application. Advanced technologies, such as digital twins and physics-guided neural networks, are expected to play roles in drilling and completion engineering. However, many challenges remain to be addressed, such as the automatic processing of multi-source and multi-scale data. Additionally, in intelligent drilling and completion, methods for the fusion of data-driven and physics-based models, few-sample learning, uncertainty modeling, and the interpretability and transferability of intelligent algorithms are research frontiers. Based on intelligent application scenarios, this study comprehensively reviews the research status of intelligent drilling and completion and discusses key research areas in the future. This study aims to enhance the berthing of AI techniques in drilling and completion engineering.}
}
@article{KYEONG2022117660,
title = {Mechanism design for data reliability improvement through network-based reasoning model},
journal = {Expert Systems with Applications},
volume = {205},
pages = {117660},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117660},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422009629},
author = {Nohkyum Kyeong and Kihwan Nam},
keywords = {Business intelligence, Data reliability, Network effect, Influence, Recommender system},
abstract = {The importance of business intelligence is increasing every day and many corporations are using data analysis and its results in the decision-making process. Although collecting reliable data is a key prerequisite for the effectiveness of business intelligence, the existing research has focused on improving the reliability of data that is already collected. This research points out the limitations of previous research related to data reliability and presents a new theoretical model that can secure highly reliable data to enhance the business intelligence effect. We designed a mechanism that introduces the concept of the power of influence by using the network effect based reasoning model and applying the two-step flow theory of social exchange theory and information. More than 2 million pieces of real users’ preference data was collected and verified by applying them to a recommendation system. More specifically, I made them recognize what the influence of the users’ preference data input behavior on each individual would be. Also, the limitation on the user input data with the low existing reliability was overcome by applying the recommendation system based on the network effect, which weights the preference data of the users having high influence. As a result, we have shown that the data collection mechanisms based on influence are more efficient in terms of data collection and data analysis.}
}
@article{DURAIVELU2022,
title = {Digital transformation in manufacturing industry – A comprehensive insight},
journal = {Materials Today: Proceedings},
year = {2022},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.07.409},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322051008},
author = {K Duraivelu},
keywords = {Digital transformation, Manufacturing industry, Smart manufacturing, Digital factory, Industry 4.0},
abstract = {Digital disruption has upended the entire manufacturing industry across the world. Industry 4.0 has witnessed many opportunities from the advanced technologies to enhance efficiency in the existing manufacturing processes through automation, artificial intelligence and machine learning approaches. This article states the necessity of digital transformation (DX) in the manufacturing industry for improving quality and efficiency, reducing waste and cost reduction, adapting quickly to changes in the customer demands and market, and creating services and innovative products. The two most common types of DX in the manufacturing industry are process DX – digitizing the existing processes to improve the operational efficiency; product and service DX – creating new experiences and digital services for customers to improve customer satisfaction. This article also sets out to explore the challenges being faced by the manufacturing industry during the course of DX. The success factors of DX in the manufacturing industry could be associated with one of the three dimensions: organization, environment, and technology. The major steps for effective implementation of DX that a manufacturing company should consider are also discussed.}
}
@article{HOFMAN2022101246,
title = {Distant calibration of low-cost PM and NO2 sensors; evidence from multiple sensor testbeds},
journal = {Atmospheric Pollution Research},
volume = {13},
number = {1},
pages = {101246},
year = {2022},
issn = {1309-1042},
doi = {https://doi.org/10.1016/j.apr.2021.101246},
url = {https://www.sciencedirect.com/science/article/pii/S1309104221003093},
author = {Jelle Hofman and Mania Nikolaou and Sharada Prasad Shantharam and Christophe Stroobants and Sander Weijs and Valerio Panzica {La Manna}},
keywords = {Air quality, Urban, Sensors, Calibration, Cloud, Network},
abstract = {Air quality improved significantly over the past decades. Nevertheless, air pollution continuous to have significant health impacts worldwide. To better assess people's exposure to air pollution, there is a need for higher, more personalized monitoring granularity. IoT sensor technologies can meet these requirements and pave the way towards more fine-grained air quality monitoring, improving our understanding while creating a higher public awareness driving behavioural change. This work tested the validity of scalable PM and NO2 calibration algorithms on various types of sensors (SDS011, OPC-N3, SPS30, NO2-A43F) in five different sensor testbeds deployed at various locations in Belgium and the Netherlands. The calibration models account for sensor gain and offset, while compensating for observed sensitivities of low-cost optical and electrochemical sensors. The calibration improves sensor data considerably (accuracy, linearity and correlation) up to sensitizing and supplementary (EU Class 1) categories at hourly and daily resolutions. Thanks to its cloud implementation and openly available input data, this calibration can be provided “as a service” on top of existing sensor networks in any city, on any sensor. Although distant calibration approaches improve sensor data, the ultimate performance will still depend on the applied sensor type, unit (design of sensor box) and granularity of the available reference monitoring network.}
}
@article{NING2022101808,
title = {Converting street view images to land cover maps for metric mapping: A case study on sidewalk network extraction for the wheelchair users},
journal = {Computers, Environment and Urban Systems},
volume = {95},
pages = {101808},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2022.101808},
url = {https://www.sciencedirect.com/science/article/pii/S0198971522000527},
author = {Huan Ning and Zhenlong Li and Cuizhen Wang and Michael E. Hodgson and Xiao Huang and Xiaoming Li},
keywords = {Street view image, Land cover, Sidewalk, Width, Wheelchair users},
abstract = {Street view images are now widely used in web map services, providing on-site photos of street scenes for users to explore without physically being in the field. These photos record detailed visual information of the street environment with geospatial controls; therefore, they can be used for metric mapping purposes. In this study, we present a method to convert street view images to measurable land cover maps using their associated depthmap data. The proposed method can autonomously extract and measure land cover objects over large areas covered by a mosaic of street view images. In the case study, we demonstrated the use of land cover maps derived from Google Street View images to extract sidewalk features and to measure sidewalk clear widths for wheelchair users. Sidewalk feature slopes were also extracted from the metadata of street view images. Using the Washington D.C., U.S. as the study area, our method extracted a sidewalk network of 2561 km in length with the precision of 0.8662 and recall of 0.8525. The width mean error of extracted sidewalks wide between 1 and 2 m is 0.24 m, and the slope mean error is 0.638°. In Washington D.C., most sidewalks meet the minimum width requirement (0.9 m), but 20% of them have slopes that exceed the maximum allowance (1:20 or about 2.9°). These results demonstrate the converted land cover maps from street view images can be used for metric mapping purposes. The extracted sidewalk network can serve as a valuable inventory for urban planners to promote equitable walkability for mobility disabled users. And if widely available, mobility-impaired users could consult them prior to planning a route.}
}
@article{YOU202268,
title = {Mapping global cropping system: Challenges, opportunities, and future perspectives},
journal = {Crop and Environment},
volume = {1},
number = {1},
pages = {68-73},
year = {2022},
issn = {2773-126X},
doi = {https://doi.org/10.1016/j.crope.2022.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S2773126X22000065},
author = {Liangzhi You and Zhanli Sun},
keywords = {Cropping system, Food security, Global crop mapping, Remote sensing, SPAM},
abstract = {Spatially explicit global cropping system data products, which provide critical information on harvested areas, crop yields, and other management variables, are imperative to tackle current grand challenges such as global food security and climate change. These cropping system datasets are also very useful for researchers as they can support various scientific analyses in research projects. Yet, effectively searching, navigating, and fully understanding various global datasets can be a daunting task for researchers and policy analysts. In this review, we first compare a few selected global data products, which use crop census and statistical data as the main data source, and identify key problems and challenges of the global crop mapping such as data accuracy and consistency. We then pointed out the future perspectives and directions in further improving the global cropping data products. Collective mechanisms and efforts with the support of open-access data hosting platforms, standard protocols, and consistent financial support are necessary to produce high-quality datasets for researchers, practitioners, and policymakers. Moreover, machine learning and data fusion approaches can also be further explored in future mapping exercises.}
}
@article{FLAHERTY2022114546,
title = {The conspiracy of Covid-19 and 5G: Spatial analysis fallacies in the age of data democratization},
journal = {Social Science & Medicine},
volume = {293},
pages = {114546},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.114546},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621008789},
author = {Eoin Flaherty and Tristan Sturm and Elizabeth Farries},
keywords = {Conspiracy theories, Spatial data, Health geography, Public data, COVID-19, 5G},
abstract = {In a context of mistrust in public health institutions and practices, anti-COVID/vaccination protests and the storming of Congress have illustrated that conspiracy theories are real and immanent threat to health and wellbeing, democracy, and public understanding of science. One manifestation of this is the suggested correlation of COVID-19 with 5G mobile technology. Throughout 2020, this alleged correlation was promoted and distributed widely on social media, often in the form of maps overlaying the distribution of COVID-19 cases with the instillation of 5G towers. These conspiracy theories are not fringe phenomena, and they form part of a growing repertoire for conspiracist activist groups with capacities for organised violence. In this paper, we outline how spatial data have been co-opted, and spatial correlations asserted by conspiracy theorists. We consider the basis of their claims of causal association with reference to three key areas of geographical explanation: (1) how social properties are constituted and how they exert complex causal forces, (2) the pitfalls of correlation with spatial and ecological data, and (3) the challenges of specifying and interpreting causal effects with spatial data. For each, we consider the unique theoretical and technical challenges involved in specifying meaningful correlation, and how their discarding facilitates conspiracist attribution. In doing so, we offer a basis both to interrogate conspiracists’ uses and interpretation of data from elementary principles and offer some cautionary notes on the potential for their future misuse in an age of data democratization. Finally, this paper contributes to work on the basis of conspiracy theories in general, by asserting how – absent an appreciation of these key methodological principles – spatial health data may be especially prone to co-option by conspiracist groups.}
}
@article{HERRMANN2022101716,
title = {The arcanum of artificial intelligence in enterprise applications: Toward a unified framework},
journal = {Journal of Engineering and Technology Management},
volume = {66},
pages = {101716},
year = {2022},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2022.101716},
url = {https://www.sciencedirect.com/science/article/pii/S0923474822000467},
author = {Heinz Herrmann},
keywords = {Artificial intelligence, Unified framework, Systematic review, Science mapping, Systematic science mapping},
abstract = {Disagreement and confusion about artificial intelligence (AI) terminology impede researchers, innovators, and practitioners when developing and implementing enterprise applications. The prevailing ambiguities and use of buzzwords are exacerbated by media and vendor marketing hype. This study identifies several ambiguities within and across AI fields and subfields. Combining a systematic review with a sequential mixed-models design, a total of 26,143 publications were reviewed and mapped, making this the largest conceptual study in the AI field. A unified framework is proposed as an Euler diagram to bring about clarity through a "common language" for AI researchers, innovators, and practitioners.}
}
@article{SEGUNDOSEVILLA2022107772,
title = {State-of-the-art of data collection, analytics, and future needs of transmission utilities worldwide to account for the continuous growth of sensing data},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {137},
pages = {107772},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107772},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521009947},
author = {Felix Rafael {Segundo Sevilla} and Yanli Liu and Emilio Barocio and Petr Korba and Manuel Andrade and Federica Bellizio and Jorrit Bos and Balarko Chaudhuri and Hector Chavez and Jochen Cremer and Robert Eriksson and Camille Hamon and Miguel Herrera and Marnick Huijsman and Michael Ingram and Danny Klaar and Venkat Krishnan and Jorge Mola and Marcos Netto and Mario Paolone and Panagiotis Papadopoulos and Miguel Ramirez and Jose Rueda and Walter Sattinger and Vladimir Terzija and Simon Tindemans and Alberto Trigueros and Yajun Wang and Junbo Zhao},
keywords = {Data handling, Data analytics, Phasor measurement units, Wide-area monitoring, System dynamic performance, Stability assessment, Survey, Transmission system operator, Grid operation and management},
abstract = {Nowadays, transmission system operators require higher degree of observability in real-time to gain situational awareness and improve the decision-making process to guarantee a safe and reliable operation. Digitalization of energy systems allows utilities to monitor the system dynamic performance in real-time at fast time scales. The use of such technologies has unlocked new opportunities to introduce new data driven algorithms for improving the stability assessment and control of the system. Motivated by these challenges, a group of experts have worked together to highlight and establish a baseline set of these common concerns, which can be used as motivation to propose innovative analytics and data-driven solutions. In this document, the results of a survey on 10 transmission system operators around the world are presented and it aims to understand the current practices of the participating companies, in terms of data acquisition, handling, storage, modelling and analytics. The overall objective of this document is to capture the actual needs from the interviewed utilities, thereby laying the groundwork for setting valid assumptions for the development of advanced algorithms in this field.}
}
@article{CAO2022102731,
title = {An analysis on the role of blockchain-based platforms in agricultural supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {163},
pages = {102731},
year = {2022},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2022.102731},
url = {https://www.sciencedirect.com/science/article/pii/S1366554522001223},
author = {Yu Cao and Chaoqun Yi and Guangyu Wan and Hanli Hu and Qingsong Li and Shouyang Wang},
keywords = {Blockchain-based platforms, Agricultural supply chain, Financing risk, Counterparty risk, Consumer trust},
abstract = {The traditional agricultural supply chain (ASC) has been overwhelmed by several challenges, including financing risk, counterparty risk, and lack of consumer trust. Platforms based on blockchain technology combined with Internet-of-Things technology have emerged to address these challenges by improving supply chain visibility, guaranteeing the execution of contracts, and increasing the authenticity of products’ provenance information in the ASC. This study analyzes how the adoption of a blockchain-based platform can affect the decisions of ASC participants and identifies how the platform creates value for the supply chain by addressing these three challenges. We consider a two-level supply chain featuring a typical cooperative and a buyer and establish stylized game models with and without the blockchain-based platform. By comparing equilibrium outcomes with and without the blockchain-based platform, we show that the involvement of the blockchain-based platform can lead to increased production quantity and total surplus of the supply chain. This can also motivate more sustainability/green investment to produce greener products. Interestingly, we show that the value of the blockchain-based platform decreases in the credibility of the business environment in which the supply chain operates. Furthermore, the buyer will always benefit from the established blockchain-based platform, whereas the cooperative can benefit in most cases but could be worse off under certain conditions. The adoption and operational costs could outweigh the benefits caused by the addition of the blockchain-based platform.}
}
@article{WANG2022104464,
title = {Construction and maintenance of urban underground infrastructure with digital technologies},
journal = {Automation in Construction},
volume = {141},
pages = {104464},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104464},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003375},
author = {Mingzhu Wang and Xianfei Yin},
keywords = {Underground infrastructure, Literature review, Digital technologies, Inspection and maintenance, Condition assessment, Underground construction, Infrastructure operation & maintenance},
abstract = {Urban underground infrastructure is a critical component in cities to provide essential services to residents. Research efforts have been made to facilitate different activities of underground infrastructure projects using various methods, particularly digital technologies. To obtain deeper insights from existing research and provide directions for future research, this study conducts a comprehensive review of research on underground infrastructure construction and Operation & Maintenance (O&M) with a focus on digital technologies. The in-depth review was conducted based on 145 publications from the perspective of locating and mapping, construction and coordination, as well as O&M. Consequently, critical limitations and challenges are revealed, such as the lack of as-built and as-is information, the requirement of data quality and quantity for deep learning methods, the lack of fully automated robotic systems, etc. Afterwards, a status matrix was presented to identify the level of different digital technologies being studied and their future application potential for key activities of underground infrastructure projects. In the end, future research trends are proposed, including (1) digital twinning of underground infrastructure, (2) quality and uncertainty of inspection data, (3) data generation and semi-supervised learning, (4) predictive maintenance, and (5) fully automated robotic systems for inspection and maintenance. This study contributes to the body of knowledge by identifying the challenges and limitations of existing studies through a systematic review, providing a clear view of the achievements and potentials of digital technologies for underground infrastructure, and proposing future research directions to facilitate digital transformation in this area.}
}
@article{FURSTENAU2022,
title = {Internet of things: Conceptual network structure, main challenges and future directions},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000827},
author = {Leonardo B. Furstenau and Yan Pablo Reckziegel Rodrigues and Michele Kremer Sott and Pedro Leivas and Michael S. Dohan and José Ricardo López-Robles and Manuel J. Cobo and Nicola Luigi Bragazzi and Kim-Kwang {Raymond Choo}},
keywords = {Internet of things, Strategic intelligence, Industry 4.0, SciMAT, Bibliometric analysis, Science mapping},
abstract = {Internet of Things (IoT) is a key technology trend that supports our digitalized society in applications such as smart countries and smart cities. In this study, we investigated the existing strategic themes, thematic evolution structure, key challenges, and potential research opportunities associated with the IoT. For this study, we conducted a Bibliometric Performance and Network Analysis (BPNA), supplemented by an exhaustive Systematic Literature Review (SLR). Specifically, in BPNA, the software SciMAT was used to analyze 14,385 documents and 30,381 keywords in the Web of Science (WoS) database, which was released between 2002 and 2019. The results revealed that 31 clusters are classified according to their importance and development, and the conceptual structures of key clusters are presented, along with their performance analysis and the relationship with other subthemes. The thematic evolution structure described the important cluster(s) over time. For the SLR, 23 documents were analyzed. The SLR revealed key challenges and limitations associated with the IoT. We expect the results will form the basis of future research and guide decision-making in the IoT and other supporting industries.}
}
@article{WANG2022103925,
title = {Unsupervised machine learning in urban studies: A systematic review of applications},
journal = {Cities},
volume = {129},
pages = {103925},
year = {2022},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2022.103925},
url = {https://www.sciencedirect.com/science/article/pii/S026427512200364X},
author = {Jing Wang and Filip Biljecki},
keywords = {GeoAI, Urban planning, GIScience, Urban data science, k-means, Latent Dirichlet allocation},
abstract = {Unsupervised learning (UL) has a long and successful history in untangling the complexity of cities. As the counterpart of supervised learning, it discovers patterns from intrinsic data structures without crafted labels, which is believed to be the key to real AI-generated decisions. This paper provides a systematic review of the use of UL in urban studies based on 140 publications. Firstly, the topic, technique, application, data type, and evaluation method of each paper are recorded, deriving statistical insights into the evolution and trends. Clustering is the most prominent method, followed by topic modeling. With the strong momentum of deep learning, a growing application field of UL methods is representing the complex real-world urban systems at multiple scales through multi-source data integration. Subsequently, a detailed review discusses how UL is applied in a broad range of urban topics, which are concluded by four dominant themes: urbanization and regional studies, built environment, urban sustainability, and urban dynamics. Finally, the review addresses common limitations regarding data quality, subjective interpretation, and validation difficulty of the results, which increasingly require interdisciplinary knowledge. Research opportunities are found in the rapidly evolving technological landscape of UL and in certain domains where supervised learning dominates.}
}
@article{MARDIA2022104862,
title = {Principal component analysis and clustering on manifolds},
journal = {Journal of Multivariate Analysis},
volume = {188},
pages = {104862},
year = {2022},
note = {50th Anniversary Jubilee Edition},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2021.104862},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X21001408},
author = {Kanti V. Mardia and Henrik Wiechers and Benjamin Eltzner and Stephan F. Huckemann},
keywords = {Adaptive linkage clustering, Circular mode hunting, Dimension reduction, Multivariate wrapped normal, SARS-CoV-2 geometry, Stratified spheres, Torus PCA},
abstract = {Big data, high dimensional data, sparse data, large scale data, and imaging data are all becoming new frontiers of statistics. Changing technologies have created this flood and have led to a real hunger for new modeling strategies and data analysis by scientists. In many cases data are not Euclidean; for example, in molecular biology, the data sit on manifolds. Even in a simple non-Euclidean manifold (circle), to summarize angles by the arithmetic average cannot make sense and so more care is needed. Thus non-Euclidean settings throw up many major challenges, both mathematical and statistical. This paper will focus on the PCA and clustering methods for some manifolds. Of course, the PCA and clustering methods in multivariate analysis are one of the core topics. We basically deal with two key manifolds from a practical point of view, namely spheres and tori. It is well known that dimension reduction on non-Euclidean manifolds with PCA-like methods has been a challenging task for quite some time but recently there has been some breakthrough. One of them is the idea of nested spheres and another is transforming a torus into a sphere effectively and subsequently use the technology of nested spheres PCA. We also provide a new method of clustering for multivariate analysis which has a fundamental property required for molecular biology that penalizes wrong assignments to avoid chemically no go areas. We give various examples to illustrate these methods. One of the important examples includes dealing with COVID-19 data.}
}
@article{HARTMANN2022101782,
title = {A text and image analysis workflow using citizen science data to extract relevant social media records: Combining red kite observations from Flickr, eBird and iNaturalist},
journal = {Ecological Informatics},
volume = {71},
pages = {101782},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101782},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002321},
author = {Maximilian C. Hartmann and Moritz Schott and Alishiba Dsouza and Yannick Metz and Michele Volpi and Ross S. Purves},
keywords = {User-generated content, Volunteered geographic information, Data integration, Image content analysis, Convolutional neural networks},
abstract = {There is an urgent need to develop new methods to monitor the state of the environment. One potential approach is to use new data sources, such as User-Generated Content, to augment existing approaches. However, to date, studies typically focus on a single date source and modality. We take a new approach, using citizen science records recording sightings of red kites (Milvus milvus) to train and validate a Convolutional Neural Network (CNN) capable of identifying images containing red kites. This CNN is integrated in a sequential workflow which also uses an off-the-shelf bird classifier and text metadata to retrieve observations of red kites in the Chilterns, England. Our workflow reduces an initial set of more than 600,000 images to just 3065 candidate images. Manual inspection of these images shows that our approach has a precision of 0.658. A workflow using only text identifies 14% less images than that including image content analysis, and by combining image and text classifiers we achieve almost perfect precision of 0.992. Images retrieved from social media records complement those recorded by citizen scientists spatially and temporally, and our workflow is sufficiently generic that it can easily be transferred to other species.}
}
@article{LIN2022100440,
title = {DAISM-DNNXMBD: Highly accurate cell type proportion estimation with in silico data augmentation and deep neural networks},
journal = {Patterns},
volume = {3},
number = {3},
pages = {100440},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100440},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922000137},
author = {Yating Lin and Haojun Li and Xu Xiao and Lei Zhang and Kejia Wang and Jingbo Zhao and Minshu Wang and Frank Zheng and Minwei Zhang and Wenxian Yang and Jiahuai Han and Rongshan Yu},
keywords = {cell type proportion estimation, deconvolution, data augmentation, data simulation, deep learning},
abstract = {Summary
Understanding the immune cell abundance of cancer and other disease-related tissues has an important role in guiding disease treatments. Computational cell type proportion estimation methods have been previously developed to derive such information from bulk RNA sequencing data. Unfortunately, our results show that the performance of these methods can be seriously plagued by the mismatch between training data and real-world data. To tackle this issue, we propose the DAISM-DNNXMBD (XMBD: Xiamen Big Data, a biomedical open software initiative in the National Institute for Data Science in Health and Medicine, Xiamen University, China.) (denoted as DAISM-DNN) pipeline that trains a deep neural network (DNN) with dataset-specific training data populated from a certain amount of calibrated samples using DAISM, a novel data augmentation method with an in silico mixing strategy. The evaluation results demonstrate that the DAISM-DNN pipeline outperforms other existing methods consistently and substantially for all the cell types under evaluation in real-world datasets.}
}
@article{SUN2022e10568,
title = {Distribution characteristics of ABO blood groups in China},
journal = {Heliyon},
volume = {8},
number = {9},
pages = {e10568},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e10568},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022018564},
author = {Yang Sun and Liqin Wang and Jiameng Niu and Ting Ma and Lili Xing and Aowei Song and Wenhua Wang and Yuan Shen and Jiangcun Yang},
keywords = {China population, ABO blood group, Phenotype,  frequency, Distribution characteristics},
abstract = {ABO blood groups distribution shows obvious geographical differences globally, but the reliability of the Blood data for assessing relationships between population groups is limited. This is mostly due to the lack of availability and interchange of this important data. We collected data of 23 million ABO blood group population from 34 provincial-level administrative regions in China. To ensure the reliability of the results, we standardized the 23 million data by the China seventh census data. The ranking of ABO blood groups phenotypic distribution in China is O > A > B > AB. The proportions of A, B, O and AB type in China population are 28.72%, 28.17%, 34.20%, and 8.91%, respectively. Accordingly, the frequencies of p [A], q [B], and r [O] gene at the ABO blood group are 0.211, 0.208, and 0.584, respectively. China blood phenotype is dominated by O type, but the r gene frequency is obviously lower than other countries. The distribution of ABO blood groups in China varies geographically. Clustering analysis results show that ABO blood groups divide into four regions from north to south in China, and reveal that the r [O] gene shows an increasing trend from North to South, and conversely the q [B] gene exhibited a decreasing trend at these coordinates. These analyses present interesting characteristics of the blood group distribution across the geography of China.}
}
@article{WU2022101522,
title = {An integrated framework for blockchain-enabled supply chain trust management towards smart manufacturing},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101522},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101522},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002706},
author = {Yue Wu and Yingfeng Zhang},
keywords = {Blockchain, Smart manufacturing, Supply chain, Trust management, Coal mine industry},
abstract = {With the development of a new generation of information technology, smart manufacturing has put forward higher requirements for supply chain. It is necessary to ensure the synchronization of the supply chain operation and maintain the reliability of the supply chain management, therefore the trust evaluation for the supply chain becomes extremely important. Traditional supply chain management has problems such as information flow is easy to be tampered with, logistics is difficult to trace, and capital flow is not true, which leads to increased opportunity costs due to the lack of trust among transaction entities in the supply chain. The emergence of blockchain technology provides an opportunity to improve the supply chain ecosystem. In this paper, an integrated framework for blockchain-enabled supply chain trust management towards smart manufacturing is proposed to explain how to enhance trust management with the help of blockchain from the perspectives of information flow, logistics, and capital flow. An optimized trust management model is designed for better entities evaluation in supply chain. A coal mine equipment manufacturing industry scenario is presented to illustrate the effectiveness of the proposed framework.}
}
@article{SHREE2022108131,
title = {Autonomous development of theoretical framework for intelligence automation system using decision tree algorithm},
journal = {Computers and Electrical Engineering},
volume = {102},
pages = {108131},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108131},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622003810},
author = {S. Raja Shree},
keywords = {Large data classification, Probability statistics, Functional, Differential equations},
abstract = {A novel technique is proposed to arrange enormous information on likelihood which actually depends on the Numerical Model. Having adopted the Two-Fold Development Investigation Model, the non-linear differential conditions are presented using information measurements, characterization of stable and curved capacity. This becomes an ideal estimation technique to build the parameters such as test insights, likelihood, thickness, capacity of the dispersion of information and grouping. The present research also aims to get the information arrangement by Sigma Test, examines the likelihood rule, rejects the span, and demonstrates the solidness of the numerical model & the progressive union. Through the reenactment, information investigation, the available results of the proposed method show that the model precision rate is high, the normal mistake rate is low, and it also becomes acceptable in the context of assembly. Here, the ideal forecast of enormous data is being done by control input arrangement, the ideal target work, utilizing the mayhem factors, nonlinear arbitrary crossing of classification, focus crossing. It also sets up the topological connection between information focuses, and further develops the order calculation. Recreation results show that the proposed calculation can adequately work on the exactness of huge information order and decrease the misclassification rate.}
}
@article{KIM2022104759,
title = {Development and validation of a management system and dataset quality assessment tool for the Radiology Common Data Model (R_CDM): A case study in liver disease},
journal = {International Journal of Medical Informatics},
volume = {162},
pages = {104759},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104759},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622000739},
author = {Tae-Hoon Kim and SiHyeong Noh and Youe Ree Kim and ChungSub Lee and Ji Eon Kim and Chang-Won Jeong and Kwon-Ha Yoon},
keywords = {Chronic liver disease (CLD), Metadata, Radiology_common data model (R_CDM), Standardization},
abstract = {Background
The Observational Medical Outcomes Partnership—Common Data Model (OMOP-CDM), a distributed research network, has low clinical data coverage. Radiological data are valuable, but imaging metadata are often incomplete, and a standardized recording format in the OMOP-CDM is lacking. We developed a web-based management system and data quality assessment (RQA) tool for a radiology_CDM (R_CDM) and evaluated the feasibility of clinically applying this dataset.
Methods
We designed an R_CDM with Radiology_Occurrence and Radiology_Image tables. This was seamlessly linked to the OMOP-CDM clinical data. We adopted the standardized terminology using the RadLex playbook and mapped 5,753 radiology protocol terms to the OMOP vocabulary. An extract, transform, and load (ETL) process was developed to extract detailed information that was difficult to extract from metadata and to compensate for missing values. Image-based quantification was performed to measure liver surface nodularity (LSN), using customized Wonkwang abdomen and liver total solution (WALTS) software.
Results
On a PACS, 368,333,676 DICOM files (1,001,797 cases) were converted to R_CDM chronic liver disease (CLD) data (316,596 MR images, 228 cases; 926,753 CT images, 782 cases) and uploaded to a web-based management system. Acquisition date and resolution were extracted accurately, but other information, such as “contrast administration status” and “photography direction”, could not be extracted from the metadata. Using WALTS, 9,609 pre-contrast axial-plane abdominal MR images (197 CLD cases) were assigned LSN scores by METAVIR fibrosis grades, which differed significantly by ANOVA (p < 0.001). The mean RQA score (83.5) indicated good quality.
Conclusion
This study developed a web-based system for management of the R_CDM dataset, RQA tool, and constructed a CLD R_CDM dataset, with good quality for clinical application. Our management system and R_CDM CLD dataset would be useful for multicentric and image-based quantification researches.}
}
@article{ASGARIMEHR2022112801,
title = {GNSS reflectometry global ocean wind speed using deep learning: Development and assessment of CyGNSSnet},
journal = {Remote Sensing of Environment},
volume = {269},
pages = {112801},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112801},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721005216},
author = {Milad Asgarimehr and Caroline Arnold and Tobias Weigel and Chris Ruf and Jens Wickert},
keywords = {Wind speed, Ocean, GNSS reflectometry, CyGNSS, Deep learning, CNN, Fully connected layers},
abstract = {GNSS Reflectometry (GNSS-R) is a novel remote sensing technique for the monitoring of geophysical parameters using reflected GNSS signals from the Earth's surface. Ocean wind speed monitoring is the main objective of the recently launched Cyclone GNSS (CyGNSS), a GNSS-R constellation of eight microsatellites, launched in late 2016. In this study, the capability of deep learning, especially, for an operational wind speed data derivation from the measured Delay-Doppler Maps (DDMs) is characterized. CyGNSSnet is based on convolutional layers for the feature extraction from bistatic radar cross section (BRCS) DDMs, along with fully connected layers for processing ancillary technical and higher-level input parameters. The best architecture is determined on a validation set and is evaluated over a completely blind dataset from a different time span than that of the training data to validate the generality of the model for operational usage. After a data quality control, CyGNSSnet results in an RMSE of 1.36 m/s leading to a significant improvement by 28% in comparison to the officially operational retrieval algorithm. The RMSE is the lowest among those seen in the literature for any conventional or machine learning-based algorithm. The benefits of the convolutional layers, the advantages and weaknesses of the model are discussed. CyGNSSnet offers efficient processing of GNSS-R measurements for high-quality global ocean winds.}
}
@article{LI2022119120,
title = {Health-Conscious vehicle battery state estimation based on deep transfer learning},
journal = {Applied Energy},
volume = {316},
pages = {119120},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119120},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922005013},
author = {Shuangqi Li and Hongwen He and Pengfei Zhao and Shuang Cheng},
keywords = {Transportation electrification, Electric vehicles, Battery energy storage, Deep transfer learning, Battery management system, Battery state estimation},
abstract = {Establishing an accurate mathematical model is fundamental to managing, monitoring, and protecting the battery pack in electric vehicles (EVs). The application of the deep learning algorithm-based state estimation method can significantly improve the accuracy and stability of the battery model but is hindered by the great demand for training data. This paper addresses the challenge of health-conscious battery modeling by utilizing multi-source data based on a novel deep transfer learning method. Firstly, a cloud-based battery management framework is designed, which is able to collect and process battery operation data from various EVs and provide a foundation for deploying the transfer learning method. Battery healthy state information in the collected dataset is labeled by a generic perception model, which can be commonly used to quantify the aging state of different battery packs and facilitate the knowledge transfer process. Additionally, a deep transfer learning method is developed to boost the training process of the battery model, where the operation data from different types of EVs can be used for establishing state estimators. The method is verified by the battery operation data collected from two types of electric buses. With the developed healthy state perception model and transfer learning method, battery model error can be limited to 2.43% and 1.27% in the whole life cycle.}
}
@article{DU2022130798,
title = {Life cycle assessment of recycled NiCoMn ternary cathode materials prepared by hydrometallurgical technology for power batteries in China},
journal = {Journal of Cleaner Production},
volume = {340},
pages = {130798},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130798},
url = {https://www.sciencedirect.com/science/article/pii/S095965262200436X},
author = {Shiwei Du and Feng Gao and Zuoren Nie and Yu Liu and Boxue Sun and Xianzheng Gong},
keywords = {EVs, End-of-life power battery, Life cycle assessment, Environmental improvement potential, Recycled ternary cathode materials},
abstract = {Power lithium-ion batteries (LIBs) are core components of electric vehicles (EVs), and the cathode material is the key to the performance of LIBs. Nickel-cobalt-manganese oxide (NCM) cathode formulations have emerged as dominant choices in the battery industry. This work presents a life cycle assessment of recycled NCM ternary cathode materials produced from spent batteries in China. The environmental impacts of virgin and recycled material production were compared based on the ReCiPe 2016 method. The results demonstrated that the highest environmental pressure was generated during the leaching and extraction process due to the high consumption of electricity and auxiliary materials, which contributed nearly half to all three endpoint impact categories. Sensitivity analysis revealed that the environmental impacts of the leaching and extraction process could be effectively reduced by optimizing the production process to reduce the consumption of sulfuric acid, electricity, hydrogen peroxide, and sodium hydroxide. The comparative results indicated that the production of recycled NCM materials consumes 74% less energy, and compared to virgin NCM materials, the three endpoint environmental impact categories are reduced by 72%, 59% and 57%. Comparison of the global warming potential (GWP) between different recycling techniques in the literature indicated that the GWP of hydrometallurgical technology is lower. In addition, we estimated the GWP reduction potential per kg under optimized power scenarios. The annual GWP reduction and energy savings benefits of recycling in China from 2021 to 2035 were predicted. Based on the results, it is necessary to maintain high recycling rates through a variety of initiatives. This Chinese case study demonstrated that the adoption of cathode material production by recycling spent power LIBs through cleaner technology is of great practical significance for resource conservation and sustainable development of the EV industry.}
}
@article{RUAN2022157075,
title = {Spatial-temporal NDVI pattern of global mangroves: A growing trend during 2000–2018},
journal = {Science of The Total Environment},
volume = {844},
pages = {157075},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.157075},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722041729},
author = {Linlin Ruan and Min Yan and Li Zhang and XiangShun Fan and Haoxiang Yang},
keywords = {Global mangroves, NDVI, Precipitation, Temperature, Sea surface salinity},
abstract = {Mangroves are coastal vegetation with high ecological and economic value that are mainly distributed in tropical and subtropical intertidal zones. In the past, they have been degraded by extensive deforestation for agricultural and aquatic land. In recent years, mangroves have been protected and sustainably used through considerable measures of conservation, restoration and afforestation, but the health trends of mangroves during this process are not clear. To identify the mangrove health conditions and dynamics, we investigated the spatial-temporal trends of global mangroves using the Moderate Resolution Imaging Spectroradiometer (MODIS) normalized difference vegetation index (NDVI) dataset during 2000–2018. The results illustrated that 1) Asian mangroves had the highest NDVI values, especially in Southeast Asia (0.80), while the average NDVI of African mangroves was the lowest (0.67). NDVI values higher than 0.80 were mainly located in Southeast Asia and South America, which accounted for 24.0 % and 7.1 % of the global mangrove area, respectively. 2) Globally, the proportion of mangrove forests that increased significantly (23.6 %, p value < 0.05) was approximately twice as large as the significant decrease (10.7 %, p value < 0.05). Asia, where mangroves are widespread, accounts for nearly half of the world's significant increase (10.8 %) and decrease (4.6 %). Generally, the annual average NDVI for global mangroves exhibited a slow increasing trend from 2000 to 2018 (p value = 0.13). 3) The global mangrove NDVI showed a positive correlation with precipitation (Rprep = 0.79, p value < 0.01) and temperature (Rtemp = 0.37, p value < 0.01), while it was inhibited by sea surface salinity (Rsss = −0.45, p value < 0.01) on a scale of 1° of latitude. 4) The results of the overall growth trend of mangroves indicated that global mangrove conservation appeared to achieve initial success, but direct or potential factors, such as salinity stress, natural disasters, small-scale deforestation, construction of coastal facilities, and sea level rise, still threaten the survival of mangroves, leading to a decline in their health status. This study provides information on the health status of mangrove ecosystems and can assist in formulating subsequent conservation and management measures.}
}
@article{BEITNER2022,
title = {Knee registries: state of the art},
journal = {Journal of ISAKOS},
year = {2022},
issn = {2059-7754},
doi = {https://doi.org/10.1136/jisakos-2021-000625},
url = {https://www.sciencedirect.com/science/article/pii/S2059775421003060},
author = {Eran {Beit Ner} and Norimasa Nakamura and Christian Lattermann and Michael James McNicholas},
keywords = {knee injuries, anterior cruciate ligament, arthroplasty, replacement, patient outcome assessment, osteotomy, articular cartilage restoration, registry, post marketing surveillance},
abstract = {ABSTRACT
Sports injuries, trauma and the globally ageing and obese population require increasing levels of knee surgery. Shared decision making has replaced the paternalistic approach to patient management. Evidence-based medicine underpins surgical treatment strategies, from consenting an individual patient to national healthcare system design. The evolution of successful knee-related registries starting from specific arthroplasty registries has given rise to ligament reconstruction, osteotomy and cartilage surgery registries developing as platforms for surgical outcome data collection. Stakeholders include surgeons and their patients, researchers, healthcare systems, as well as the funding insurers and governments. Lately, implant manufacturers have also been mandated to perform postmarket surveillance with some hoping to base that on registry data. Aiming to assess the current status of knee-related registries, we performed a comprehensive literature and web search, which yielded 23 arthroplasty, 8 ligament, 4 osteotomy and 3 articular cartilage registries. Registries were evaluated for their scope, measured variables, impact and limitations. Registries have many advantages as they aim to increase awareness of outcomes; identify trends in practice over time, early failing implants, outlier surgeon or institution performance; and assist postmarketing surveillance. International collaborations have highlighted variations in practice. The limitations of registries are discussed in detail. Inconsistencies are found in collected data and measured variables. Potential measurement and selection biases are outlined. Without mandated data collection and with apparent issues such as unverified patient reporting of complications, registries are not designed to replace adverse event recording in place of a proper safety and efficacy study, as demanded by regulators. Registry ‘big data’ can provide evidence of associations of problems. However, registries cannot provide evidence of causation. Hence, without careful consideration of the data and its limitations, registry data are at risk of incorrectly drawn conclusions and the potential of misuse of the results. That must be guarded against. Looking at the future, registry operators benefit from a collective experience of running registries as they mature, allowing for improvements across specialties. Large-scale registries are not only of merit, improving with stakeholder acceptance, but also are critical in furthering our understanding of our patients’ outcomes. In doing so, they are a critical element for our future scientific discourse.}
}
@article{BRESTER20221,
title = {Epidemiological predictive modeling: lessons learned from the Kuopio ischemic heart disease risk factor study☆},
journal = {Annals of Epidemiology},
volume = {70},
pages = {1-8},
year = {2022},
issn = {1047-2797},
doi = {https://doi.org/10.1016/j.annepidem.2022.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1047279722000448},
author = {Christina Brester and Ari Voutilainen and Tomi-Pekka Tuomainen and Jussi Kauhanen and Mikko Kolehmainen},
keywords = {Machine learning, Prediction of cardiovascular death, Population study, Epidemiology},
abstract = {ABSTRACT
Purpose
The use of predictive models in epidemiology is relatively narrow as most of the studies report results of traditional statistical models such as Linear, Logistic, or Cox regressions. In this study, a high-dimensional epidemiological cohort, collected within the Kuopio Ischemic Heart Disease Risk Factor Study in 1984–1989, was used to investigate the predictive ability of models with embedded variable selection.
Methods
Simple Logistic Regression with seven preselected risk factors was compared to k-Nearest Neighbors, Logistic Lasso Regression, Decision Tree, Random Forest, and Multilayer Perceptron in predicting cardiovascular death for the aged men from Kuopio Ischemic Heart Disease Risk Factor for the long horizon of 30 ± 3 years: 746 predictor variables were available for 2682 men (705 cardiovascular deaths were registered). We considered two scenarios of handling competing risks (removing subjects and treating them as non-cases).
Results
The best average AUC on the test sample was 0.8075 (95%CI, 0.8051–0.8099) in scenario 1 and 0.7155 (95%CI, 0.7128–0.7183) in scenario 2 achieved with Logistic Lasso Regression, which was 6.04% and 5.50% higher than the baseline AUC provided by Logistic Regression with manually preselected predictors.
Conclusions
In both scenarios Logistic Lasso Regression, Random Forest, and Multilayer Perceptron outperformed Simple Logistic Regression.}
}
@article{TUMBAJOY2022681,
title = {Enabling Industry 4.0 impact assessment with manufacturing system simulation: an OEE based methodology},
journal = {Procedia CIRP},
volume = {107},
pages = {681-686},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.045},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003298},
author = {Luisa M. Tumbajoy and Mariela Muñoz-Añasco and Sebastian Thiede},
keywords = {Smart Manufacturing, Industry 4.0, Key Performance Indicator, Simulation},
abstract = {Increasing digitalization in manufacturing, often associated with terms like Industry 4.0 (I4.0) or Smart Manufacturing, is a topic of crucial concern for manufacturing companies. Different digital technologies (DTs) can be integrated into manufacturing processes and systems aiming at increasing flexibility, product quality or productivity. The type and scope of potential DTs must be carefully selected when planning and improving a manufacturing system. The definition and configuration could be supported by simulation techniques that assess the DTs' impact on the manufacturing system and its final performance. However, parametrizing the DTs into a simulation tool is not straightforward since appropriate models might be challenging to obtain and actual impacts of DTs are uncertain. Against this background, the paper presents methods to enable a simulation-based assessment while considering the impact of not just individual but also a combination of DTs. The paper introduces a framework to define the base characteristics of selected DTs within a manufacturing system and their parameterization into a commercial simulation tool. Furthermore, the usability and expectable results are demonstrated in a case study.}
}
@article{AKHMATOVA20221512,
title = {Integrating quality management systems (TQM) in the digital age of intelligent transportation systems industry 4.0},
journal = {Transportation Research Procedia},
volume = {63},
pages = {1512-1520},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.163},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522004185},
author = {Malika-Sofi Akhmatova and Antonina Deniskina and Dzhennet-Mari Akhmatova and Larisa Prykina},
keywords = {TQM, Industry 4.0, digitalization, quality, improvement},
abstract = {The rapid expansion of Total Quality Management (TQM) was a response to the challenges posed by increased levels of competition in the global market and heightened attention to issues of quality planning, assurance, control, and improvement. Currently, entering the digital age and the advancement of human life in every field affect the development of TQM through the diversification of Industry 4.0 techniques and applications. This article is to explore the digital concepts relevant for TQM and identify possible challenges emerging while implementing these concepts in practice. In line with this, this article integrates three stages, thus filling in gaps in the existing research. First of all, it tracks the transition from the concept of quality control to digital-friendly TQM, highlighting the meaning of quality, specific features of TQM development, and breakthroughs in the history of TQM. It is noted that the contemporary TQM represents quality as a category open to the achievements of scientific and technological progress that can assist in meeting the customers’ expectations and attaining competitiveness. Second, the article analyzes the TQM in the context of the fourth industrial revolution. Finally, the research results emphasize the most distressing issues faced by quality management systems (QMS) in the digital age and suggest recommendations to combat them.}
}
@article{FERNANDEZBASSO2022108870,
title = {A fuzzy-based medical system for pattern mining in a distributed environment: Application to diagnostic and co-morbidity},
journal = {Applied Soft Computing},
volume = {122},
pages = {108870},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108870},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002538},
author = {Carlos Fernandez-Basso and Karel Gutiérrez-Batista and Roberto Morcillo-Jiménez and Maria-Amparo Vila and Maria J. Martin-Bautista},
keywords = {Association rules, Fuzzy logic, Data mining, Medical records},
abstract = {In this paper we have addressed the extraction of hidden knowledge from medical records using data mining techniques such as association rules in conjunction with fuzzy logic in a distributed environment. A significant challenge in this domain is that although there are a lot of studies devoted to analysing health data, very few focus on the understanding and interpretability of the data and the hidden patterns present within the data. A major challenge in this area is that many health data analysis studies have focussed on classification, prediction or knowledge extraction and end users find little interpretability or understanding of the results. This is due to the use of black-box algorithms or because the nature of the data is not represented correctly. This is why it is necessary to focus the analysis not only on knowledge extraction but also on the transformation and processing of the data to improve the modelling of the nature of the data. Techniques such as association rule mining and fuzzy logic help to improve the interpretability of the data and treat it with the inherent uncertainty of real-world data. To this end, we propose a system that automatically: a) pre-processes the database by transforming and adapting the data for the data mining process and enriching the data to generate more interesting patterns, b) performs the fuzzification of the medical database to represent and analyse real-world medical data with its inherent uncertainty, c) discovers interrelations and patterns amongst different features (diagnostic, hospital discharge, etc.), and d) visualizes the obtained results efficiently to facilitate the analysis and improve the interpretability of the information extracted. Our proposed system yields a significant increase in the compression and interpretability of medical data for end-users, allowing them to analyse the data correctly and make the right decisions. We present one practical case using two health-related datasets to demonstrate the feasibility of our proposal for real data.}
}
@article{GUO2022128245,
title = {Consistency and uncertainty of gridded terrestrial evapotranspiration estimations over China},
journal = {Journal of Hydrology},
volume = {612},
pages = {128245},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.128245},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422008174},
author = {Linan Guo and Yanhong Wu and Hongxing Zheng and Bing Zhang and Lanxin Fan and Haojing Chi and Bokun Yan and Xiaoqi Wang},
keywords = {Terrestrial evapotranspiration, Remote sensing, Consistency and uncertainty, China},
abstract = {Terrestrial evapotranspiration (ET) is a critical process in water, energy and carbon cycles but challenging to estimate. Several gridded terrestrial ET products have been developed for regional or global applications based on different algorithms and forcing inputs and with different temporal-spatial resolutions and accuracy. This study systematically investigates the consistency and uncertainty of eight ET products over China for the period from 2003 to 2014. Temporal and spatial pairwise correlation analysis indicates that the ET products are largely consistent with each other, among which the ET estimates from GLEAM and EB-ET presenting highest spatial consistency with each other (R2 = 0.91). Compared to the ground-based observations from the flux network, the centered root-mean-squared-deviation (RMSD) for each of the eight products is found lower than 30 mm/month, according to which CR and PMLv2 outperform the others. The ET products however show considerable difference in the estimated mean annual ET averaged over China (from 378 mm/a to 528 mm/a). Uncertainty assessment based on the extended collocation method (ECM) suggests the uncertainties in ET estimates are more evident in the arid region than in the humid region. It also indicates that model averaging (like that in generating the GLASS ET product) could substantially reduce the uncertainties. The findings of this research could facilitate the application of the ET products to assess water balance dynamics over China while considering their uncertainties.}
}
@article{KAUR2022100035,
title = {Face mask recognition system using CNN model},
journal = {Neuroscience Informatics},
volume = {2},
number = {3},
pages = {100035},
year = {2022},
note = {Multimedia-based Emerging Technologies and Data Analytics for Neuroscience as a Service (NaaS)},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2021.100035},
url = {https://www.sciencedirect.com/science/article/pii/S2772528621000352},
author = {Gagandeep Kaur and Ritesh Sinha and Puneet Kumar Tiwari and Srijan Kumar Yadav and Prabhash Pandey and Rohit Raj and Anshu Vashisth and Manik Rakhra},
keywords = {Artificial Intelligence (AL), Machine learning (ML), Deep neural learning (DL), Convolutional Neural Network Model (CNN), Artificial Neural Networks (ANN), Security},
abstract = {COVID-19 epidemic has swiftly disrupted our day-to-day lives affecting the international trade and movements. Wearing a face mask to protect one's face has become the new normal. In the near future, many public service providers will expect the clients to wear masks appropriately to partake of their services. Therefore, face mask detection has become a critical duty to aid worldwide civilization. This paper provides a simple way to achieve this objective utilising some fundamental Machine Learning tools as TensorFlow, Keras, OpenCV and Scikit-Learn. The suggested technique successfully recognises the face in the image or video and then determines whether or not it has a mask on it. As a surveillance job performer, it can also recognise a face together with a mask in motion as well as in a video. The technique attains excellent accuracy. We investigate optimal parameter values for the Convolutional Neural Network model (CNN) in order to identify the existence of masks accurately without generating over-fitting.}
}
@article{PERNO2022103558,
title = {Implementation of digital twins in the process industry: A systematic literature review of enablers and barriers},
journal = {Computers in Industry},
volume = {134},
pages = {103558},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103558},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001652},
author = {Matteo Perno and Lars Hvam and Anders Haug},
keywords = {Digital twin, Simulation, Process industry, Literature review, Barrier, Enabler},
abstract = {Since the introduction of the concept of “digital twins” (DTs) in 2002, the number of practical applications in different industrial sectors has grown rapidly. Despite the hype surrounding this technology, companies face significant challenges upon deciding to implement DTs in their organizations due to the novelty of the concept. Furthermore, little research on DT has been conducted for the process industry, which may be explained by the high complexity of accurately representing and modeling the physics behind production processes. To consolidate the fragmented literature on the enabling factors and challenges in DT implementation in the process industry, this study organizes the existing studies on DTs with a focus on barriers and enablers. On this basis, this study contributes to the existing body of knowledge on DTs by organizing the DT literature and by proposing conceptual models describing enablers of and barriers to DT implementation, as well as their mutual relationships.}
}
@article{REJEB2022100580,
title = {The Interplay between the Internet of Things and agriculture: A bibliometric analysis and research agenda},
journal = {Internet of Things},
volume = {19},
pages = {100580},
year = {2022},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2022.100580},
url = {https://www.sciencedirect.com/science/article/pii/S2542660522000701},
author = {Abderahman Rejeb and Karim Rejeb and Alireza Abdollahi and Fadi Al-Turjman and Horst Treiblmaier},
keywords = {Internet of Things, Agriculture, Bibliometrics, Sustainability, Challenges, Resource-based view, Precision agriculture},
abstract = {The proliferation of the Internet of Things (IoT) has fundamentally reshaped the agricultural sector. In recent years, academic research on the IoT has grown at an unprecedented pace. However, the broad picture of how this technology can benefit the agricultural sector is still missing. To close this research gap, we conduct a bibliometric study to investigate the current state of the IoT and agriculture in academic literature. Using a resource-based view (RBV), we also identify those agricultural resources that are mostly impacted by the introduction of the IoT (i.e., seeds, soil, water, fertilizers, pesticides, energy, livestock, human resources, technology infrastructure, business relations) and propose numerous themes for future research.}
}