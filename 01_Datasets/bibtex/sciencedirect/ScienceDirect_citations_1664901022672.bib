@article{SWAIN20151048,
title = {Feasibility of 30-day hospital readmission prediction modeling based on health information exchange data},
journal = {International Journal of Medical Informatics},
volume = {84},
number = {12},
pages = {1048-1056},
year = {2015},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2015.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1386505615300381},
author = {Matthew J. Swain and Hadi Kharrazi},
keywords = {Health information exchange, Hospital readmissions, Health information organization, Risk prediction model, Health information technology},
abstract = {Introduction
Unplanned 30-day hospital readmission account for roughly $17 billion in annual Medicare spending. Many factors contribute to unplanned hospital readmissions and multiple models have been developed over the years to predict them. Most researchers have used insurance claims or administrative data to train and operationalize their Readmission Risk Prediction Models (RRPMs). Some RRPM developers have also used electronic health records data; however, using health informatics exchange data has been uncommon among such predictive models and can be beneficial in its ability to provide real-time alerts to providers at the point of care.
Methods
We conducted a semi-systematic review of readmission predictive factors published prior to March 2013. Then, we extracted and merged all significant variables listed in those articles for RRPMs. Finally, we matched these variables with common HL7 messages transmitted by a sample of health information exchange organizations (HIO).
Results
The semi-systematic review resulted in identification of 32 articles and 297 predictive variables. The mapping of these variables with common HL7 segments resulted in an 89.2% total coverage, with the DG1 (diagnosis) segment having the highest coverage of 39.4%. The PID (patient identification) and OBX (observation results) segments cover 13.9% and 9.1% of the variables. Evaluating the same coverage in three sample HIOs showed data incompleteness.
Discussion
HIOs can utilize HL7 messages to develop unique RRPMs for their stakeholders; however, data completeness of exchanged messages should meet certain thresholds. If data quality standards are met by stakeholders, HIOs would be able to provide real-time RRPMs that not only predict intra-hospital readmissions but also inter-hospital cases.
Conclusion
A RRPM derived using HIO data exchanged through may prove to be a useful method to prevent unplanned hospital readmissions. In order for the RRPM derived from HIO data to be effective, hospitals must actively exchange clinical information through the HIO and develop actionable methods that integrate into the workflow of providers to ensure that patients at high-risk for readmission receive the care they need.}
}
@article{HARPER201624,
title = {Sharing public health data saves lives},
journal = {International Journal of Infectious Diseases},
volume = {53},
pages = {24-25},
year = {2016},
note = {International Meeting on Emerging Diseases and Surveillance (IMED) 2016 Abstracts},
issn = {1201-9712},
doi = {https://doi.org/10.1016/j.ijid.2016.11.067},
url = {https://www.sciencedirect.com/science/article/pii/S1201971216312851},
author = {D. Harper}
}
@article{CHI2016565,
title = {Special Issue on Trajectory-based Behaviour Analytics},
journal = {Journal of Computer and System Sciences},
volume = {82},
number = {4},
pages = {565},
year = {2016},
note = {Trajectory-based Behaviour Analytics},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2016.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022000016000052},
author = {Chi-Hung Chi and Can Wang and Yu Zheng}
}
@incollection{PALMER2014ix,
title = {Foreword},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {ix-xiii},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00023-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080560000230},
author = {Andy Palmer}
}
@incollection{2014335,
title = {Index},
editor = {James V. Luisi},
booktitle = {Pragmatic Enterprise Architecture},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {335-346},
year = {2014},
isbn = {978-0-12-800205-6},
doi = {https://doi.org/10.1016/B978-0-12-800205-6.09994-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002056099947}
}
@incollection{2012xv,
title = {Preface},
editor = {John Ladley},
booktitle = {Data Governance},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xv-xvii},
year = {2012},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-415829-0},
doi = {https://doi.org/10.1016/B978-0-12-415829-0.05001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124158290050012}
}
@article{BELLINI2015222,
title = {Graph databases methodology and tool supporting index/store versioning},
journal = {Journal of Visual Languages & Computing},
volume = {31},
pages = {222-229},
year = {2015},
note = {Special Issue on DMS2015},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2015.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X15000750},
author = {Pierfrancesco Bellini and Ivan Bruno and Paolo Nesi and Nadia Rauch},
keywords = {RDF knowledge base versioning, Graph stores versioning, RDF store management, Knowledge base life cycle},
abstract = {Graph databases are taking place in many different applications: smart city, smart cloud, smart education, etc. In most cases, the applications imply the creation of ontologies and the integration of a large set of knowledge to build a knowledge base as an RDF KB store, with ontologies, static data, historical data and real time data. Most of the RDF stores are endowed with inferential engines that materialize some knowledge as triples during indexing or querying. In these cases, deleting concepts may imply the removal and change of many triples, especially if the triples are those modeling the ontological part of the knowledge base, or are referred by many other concepts. For these solutions, the graph database versioning feature is not provided at level of the RDF stores tool, and it is quite complex and time consuming to be addressed as black box approach. In most cases the indexing is a time consuming process, and the rebuilding of the KB may imply manually edited long scripts that are error prone. Therefore, in order to solve these kinds of problems, this paper proposes a lifecycle methodology and a tool supporting versioning of indexes for RDF KB store. The solution proposed has been developed on the basis of a number of knowledge oriented projects as Sii-Mobility (smart city), RESOLUTE (smart city risk assessment), ICARO (smart cloud). Results are reported in terms of time saving and reliability.}
}
@incollection{2015335,
title = {Glossary},
editor = {W.H. Inmon and Daniel Linstedt},
booktitle = {Data Architecture: a Primer for the Data Scientist},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {335-344},
year = {2015},
isbn = {978-0-12-802044-9},
doi = {https://doi.org/10.1016/B978-0-12-802044-9.00045-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128020449000453}
}
@article{BRUTON2016296,
title = {Enabling Effective Operational Decision Making on a Combined Heat and Power System Using the 5C Architecture},
journal = {Procedia CIRP},
volume = {55},
pages = {296-301},
year = {2016},
note = {5th CIRP Global Web Conference - Research and Innovation for Future Production (CIRPe 2016)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.08.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116309258},
author = {Ken Bruton and Brendan P. Walsh and Donal óg Cusack and Peter O’Donovan and D.T.J. O'Sullivan},
keywords = {5C Architecture, Cyber Physical Systems, Smart Manufacturing, Smart Grid, Integrated Energy Systems},
abstract = {The use of Cyber Physical Systems (CPS) to optimise industrial energy systems is an approach which has the potential to positively impact on manufacturing sector energy efficiency. The need to obtain data to facilitate the implementation of a CPS in an industrial energy system is however a complex task which is often implemented in a non-standardised way. The use of the 5C CPS architecture has the potential to standardise this approach. This paper describes a case study where data from a Combined Heat and Power (CHP) system located in a large manufacturing company was fused with grid electricity and gas models as well as a maintenance cost model using the 5C architecture with a view to making effective decisions on its cost efficient operation. A control change implemented based on the cognitive analysis enabled via the 5C architecture implementation has resulted in energy cost savings of over €7400 over a four-month period, with energy cost savings of over €150,000 projected once the 5C architecture is extended into the production environment.}
}
@article{BOTTA2016684,
title = {Integration of Cloud computing and Internet of Things: A survey},
journal = {Future Generation Computer Systems},
volume = {56},
pages = {684-700},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003015},
author = {Alessio Botta and Walter {de Donato} and Valerio Persico and Antonio Pescapé},
keywords = {Cloud computing, Internet of Things, Ubiquitous networks, Cloud of things, Pervasive applications, Smart city},
abstract = {Cloud computing and Internet of Things (IoT) are two very different technologies that are both already part of our life. Their adoption and use are expected to be more and more pervasive, making them important components of the Future Internet. A novel paradigm where Cloud and IoT are merged together is foreseen as disruptive and as an enabler of a large number of application scenarios. In this paper, we focus our attention on the integration of Cloud and IoT, which is what we call the CloudIoT paradigm. Many works in literature have surveyed Cloud and IoT separately and, more precisely, their main properties, features, underlying technologies, and open issues. However, to the best of our knowledge, these works lack a detailed analysis of the new CloudIoT paradigm, which involves completely new applications, challenges, and research issues. To bridge this gap, in this paper we provide a literature survey on the integration of Cloud and IoT. Starting by analyzing the basics of both IoT and Cloud Computing, we discuss their complementarity, detailing what is currently driving to their integration. Thanks to the adoption of the CloudIoT paradigm a number of applications are gaining momentum: we provide an up-to-date picture of CloudIoT applications in literature, with a focus on their specific research challenges. These challenges are then analyzed in details to show where the main body of research is currently heading. We also discuss what is already available in terms of platforms–both proprietary and open source–and projects implementing the CloudIoT paradigm. Finally, we identify open issues and future directions in this field, which we expect to play a leading role in the landscape of the Future Internet.}
}
@incollection{ALLEN2015161,
title = {Chapter 10 - Metadata Management},
editor = {Mark Allen and Dalton Cervo},
booktitle = {Multi-Domain Master Data Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {161-178},
year = {2015},
isbn = {978-0-12-800835-5},
doi = {https://doi.org/10.1016/B978-0-12-800835-5.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128008355000105},
author = {Mark Allen and Dalton Cervo},
keywords = {Metadata, Terms, Definition, Context, Data model, Lineage, Structure, Dictionary, Glossary, Reference},
abstract = {This chapter covers the discipline and practice of metadata management in a multi-domain Master Data Management (MDM) model. In addition, it discusses a standard approach to defining, identifying, and managing enterprise-level metadata assets such as enterprise business terms, reference data, data models, data dictionaries, and other artifacts that express the data flow and life cycle; as well as how well-organized and -maintained metadata is critical to the efficiency and success of data governance and data analysis.}
}
@incollection{ZACHMAN2013xxix,
title = {Foreword},
editor = {David Loshin},
booktitle = {Business Intelligence (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {xxix-xxx},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-385889-4},
doi = {https://doi.org/10.1016/B978-0-12-385889-4.06001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123858894060014},
author = {John A. Zachman}
}
@incollection{KRISHNAN2013147,
title = {Chapter 7 - Reengineering the Data Warehouse},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {147-162},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000076},
author = {Krish Krishnan},
keywords = {data warehouse appliances, virtualization, cloud computing},
abstract = {The goal of this chapter is to discuss the different options for reengineering the data warehouse with new technologies like data warehouse appliances, cloud computing, virtualization, and other infrastructure options.}
}
@article{GEBAUER2016298,
title = {Is it time for a HIPAA for physicians?},
journal = {Healthcare},
volume = {4},
number = {4},
pages = {298-301},
year = {2016},
issn = {2213-0764},
doi = {https://doi.org/10.1016/j.hjdsi.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2213076415300725},
author = {Sarah Gebauer and Timothy Petersen and Elizabeth Steele},
abstract = {Practices, hospitals, and healthcare systems are increasingly able to collect data about individual physician clinical performance. There is a strong temptation to use the data to make decisions about physicians' quality of care without first taking the time to establish a system that ensures valid conclusions. In addition, physicians are not informed that their data are being used, and thus do not have an opportunity to correct any inaccuracies. A HIPAA-equivalent law or regulation for physicians would help patients and physicians more accurately address these and other issues related to complex healthcare data. FERPA provides a useful framework for these concerns.}
}
@article{GREENPLATE201677,
title = {Systems immune monitoring in cancer therapy},
journal = {European Journal of Cancer},
volume = {61},
pages = {77-84},
year = {2016},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2016.03.085},
url = {https://www.sciencedirect.com/science/article/pii/S0959804916320470},
author = {Allison R. Greenplate and Douglas B. Johnson and P. Brent Ferrell and Jonathan M. Irish},
keywords = {Systems immunology, Tumour immunology, Immunotherapy, Mass cytometry, Human immune monitoring},
abstract = {Treatments that successfully modulate anti-cancer immunity have significantly improved outcomes for advanced stage malignancies and sparked intense study of the cellular mechanisms governing therapy response and resistance. These responses are governed by an evolving milieu of cancer and immune cell subpopulations that can be a rich source of biomarkers and biological insight, but it is only recently that research tools have developed to comprehensively characterize this level of cellular complexity. Mass cytometry is particularly well suited to tracking cells in complex tissues because >35 measurements can be made on each of hundreds of thousands of cells per sample, allowing all cells detected in a sample to be characterized for cell type, signalling activity, and functional outcome. This review focuses on mass cytometry as an example of systems level characterization of cancer and immune cells in human tissues, including blood, bone marrow, lymph nodes, and primary tumours. This review also discusses the state of the art in single cell tumour immunology, including tissue collection, technical and biological quality controls, computational analysis, and integration of different experimental and clinical data types. Ex vivo analysis of human tumour cells complements both in vivo monitoring, which generally measures far fewer features or lacks single cell resolution, and laboratory models, which incur cell type losses, signalling alterations, and genomic changes during establishment. Mass cytometry is on the leading edge of a new generation of cytomic tools that work with small tissue samples, such as a fine needle aspirates or blood draws, to monitor changes in rare or unexpected cell subsets during cancer therapy. This approach holds great promise for dissecting cellular microenvironments, monitoring how treatments affect tissues, revealing cellular biomarkers and effector mechanisms, and creating new treatments that productively engage the immune system to fight cancer and other diseases.}
}
@article{2015542,
title = {Subject Index},
journal = {Procedia Computer Science},
volume = {61},
pages = {542-547},
year = {2015},
note = {Complex Adaptive Systems San Jose, CA November 2-4, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.214},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030446}
}
@incollection{REEVE20137,
title = {Chapter 2 - What Is Data Integration?},
editor = {April Reeve},
booktitle = {Managing Data in Motion},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {7-13},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397167-8},
doi = {https://doi.org/10.1016/B978-0-12-397167-8.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971678000017},
author = {April Reeve}
}
@incollection{2014xvii,
title = {Introduction},
editor = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
booktitle = {Total Information Risk Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xvii-xxiv},
year = {2014},
isbn = {978-0-12-405547-6},
doi = {https://doi.org/10.1016/B978-0-12-405547-6.02001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124055476020010}
}
@incollection{2013357,
editor = {David Loshin},
booktitle = {Business Intelligence (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {357-370},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-385889-4},
doi = {https://doi.org/10.1016/B978-0-12-385889-4.18001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123858894180019}
}
@incollection{SHERMAN2015493,
title = {Chapter 19 - Centers of Excellence},
editor = {Rick Sherman},
booktitle = {Business Intelligence Guidebook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {493-512},
year = {2015},
isbn = {978-0-12-411461-6},
doi = {https://doi.org/10.1016/B978-0-12-411461-6.00019-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114616000198},
author = {Rick Sherman},
keywords = {BI COE, Business and IT, Center of excellence, COE, Data integration, DI COE, Enterprise-wide},
abstract = {Centers of excellence (COE) are people-oriented solutions that systemically address enterprise-wide problems of disconnected application and data silos. They operate cross-functionally with different business groups along with their supporting processes, applications, and technologies. COEs enable enterprises to share skills, expertise, and data across applications used in different groups so, as a whole, the enterprise is making better use of the applications, and resources are more easily shared, while investments are maximized. A COE can be an actual organizational unit or a virtual unit. The book covers two COEs: BI and data integration. The Business Intelligence COE (BI COE) coordinates and oversees BI activities, including resources and expertise. The Data Integration COE (DI COE) team focuses specifically on data integration: establishing its scope, defining its architecture and vision, and helping to implement that vision.}
}
@incollection{LOSHIN2013333,
title = {Chapter 21 - Quick Reference Guide},
editor = {David Loshin},
booktitle = {Business Intelligence (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {333-353},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-385889-4},
doi = {https://doi.org/10.1016/B978-0-12-385889-4.00021-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123858894000211},
author = {David Loshin}
}
@article{KREIS2016213,
title = {Status and perspectives of claims data analyses in Germany—A systematic review},
journal = {Health Policy},
volume = {120},
number = {2},
pages = {213-226},
year = {2016},
note = {The policy contribution of the German Health Economic Centres},
issn = {0168-8510},
doi = {https://doi.org/10.1016/j.healthpol.2016.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S016885101600021X},
author = {Kristine Kreis and Sarah Neubauer and Mike Klora and Ansgar Lange and Jan Zeidler},
keywords = {Claims data analysis, Administrative data, Health services research, Germany, Statutory health insurance, Data source},
abstract = {Background
The aim of this article is to evaluate the status, development, and perspectives of German claims data analyses in the international and health political context.
Methods
We conducted a comprehensive literature search in PubMed, Scopus, and DIMDI to identify empirical and methodological articles focusing on health insurance claims data studies published between 2000 and 2014. Inclusion criteria were (1) English/German full text articles or chapters in edited books that (2) focused on the claims data of statutory health insurance funds.
Findings
In total, 435 articles were included. Over time, the number of claims data studies has increased strongly and the frequency of policy-relevant research types increased. Along with the historical improvement path of claims data in Germany, we observed a rising percentage of international publications and an increase in the average quality of publications. In contrast to the US or Canada where comprehensive databases have been established, the most common data source in this search was data from a single SHI fund, while databases were rarely used.
Conclusions
Claims data are an important source of information for healthcare stakeholders, and their use for research purposes has further increased during recent years in Germany. Despite its potential in optimising the health system, we found a lack of German comprehensive all-payer claims databases compared to the US and Canada.}
}
@article{TENENBAUM201631,
title = {Translational Bioinformatics: Past, Present, and Future},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {14},
number = {1},
pages = {31-41},
year = {2016},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2016.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1672022916000401},
author = {Jessica D. Tenenbaum},
keywords = {Translational bioinformatics, Biomarkers, Genomics, Precision medicine, Personalized medicine},
abstract = {Though a relatively young discipline, translational bioinformatics (TBI) has become a key component of biomedical research in the era of precision medicine. Development of high-throughput technologies and electronic health records has caused a paradigm shift in both healthcare and biomedical research. Novel tools and methods are required to convert increasingly voluminous datasets into information and actionable knowledge. This review provides a definition and contextualization of the term TBI, describes the discipline’s brief history and past accomplishments, as well as current foci, and concludes with predictions of future directions in the field.}
}
@incollection{2016649,
title = {Subject Index},
editor = {Daniel Linstedt and Michael Olschimke},
booktitle = {Building a Scalable Data Warehouse with Data Vault 2.0},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {649-661},
year = {2016},
isbn = {978-0-12-802510-9},
doi = {https://doi.org/10.1016/B978-0-12-802510-9.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128025109000167}
}
@incollection{2015215,
title = {Index},
editor = {Mark Allen and Dalton Cervo},
booktitle = {Multi-Domain Master Data Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {215-219},
year = {2015},
isbn = {978-0-12-800835-5},
doi = {https://doi.org/10.1016/B978-0-12-800835-5.09985-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128008355099851}
}
@article{MORLING2016278,
title = {Completeness of primary intracranial tumour recording in the Scottish Cancer Registry 2011–2012},
journal = {Public Health},
volume = {140},
pages = {278-281},
year = {2016},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2016.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0033350616300932},
author = {J.R. Morling and R. Grant and D.H. Brewster}
}
@incollection{SIMON201565,
title = {Chapter 6 - Program Key Performance Indicators (KPIs) and Key Operating Indicators (KOIs)},
editor = {Alan Simon},
booktitle = {Enterprise Business Intelligence and Data Warehousing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {65-72},
year = {2015},
isbn = {978-0-12-801540-7},
doi = {https://doi.org/10.1016/B978-0-12-801540-7.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128015407000068},
author = {Alan Simon},
keywords = {Business intelligence, BI, analytics, predictive analytics, program manager, program management, risk management, key performance indicators (KPIs), key operating indicators (KOIs), challenges, data warehouse, data warehousing, enterprise data warehouse, EDW},
abstract = {A program manager should always be authoritatively and objectively able to answer the following question: “How’s the program going?” This chapter presents an overview of some of the most important key performance indicators (KPIs) and key operating indicators (KOIs) for an enterprise-scale BI/data warehousing program.}
}
@article{CHIANG20161076,
title = {Incidence and survival of adult cancer patients in Taiwan, 2002–2012},
journal = {Journal of the Formosan Medical Association},
volume = {115},
number = {12},
pages = {1076-1088},
year = {2016},
issn = {0929-6646},
doi = {https://doi.org/10.1016/j.jfma.2015.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0929664615003496},
author = {Chun-Ju Chiang and Wei-Cheng Lo and Ya-Wen Yang and San-Lin You and Chien-Jen Chen and Mei-Shu Lai},
keywords = {average annual percentage change of incidence, cancer, population-based cancer registry, relative survival, Taiwan},
abstract = {Background/Purpose
Little is known about the annual changes in cancer incidence and survival that occurred after the establishment of the long-form cancer registry database in Taiwan. Therefore, this study aimed to investigate the updated incidence and stage-specific relative survival rates (RSRs) among adult cancer patients in Taiwan.
Methods
Cancer incidence data from 2002 to 2012 were collected using the Taiwan Cancer Registry Database. Age-standardized incidence rates, average annual percent changes (AAPCs), and sex ratios were calculated for adults. Five-year stage-specific RSRs were estimated for cases diagnosed between 2004 and 2008 and were followed up to 2013 for major cancers.
Results
The overall age-standardized incidence rates per 100,000 populations increased from 348.39 in 2002 to 401.18 in 2012, and the AAPC was 1.7% (p < 0.05), whereas the male:female ratio was approximately 1:3 during the entire period. Most cancer sites showed a trend of increasing incidence, with the exception of common cancers such as cervix uteri (AAPC = −6.2%, p < 0.05), bladder (AAPC = −2.5%, p < 0.05), stomach (AAPC = −2.4%, p < 0.05), nasopharynx (AAPC = −1.2%, p < 0.05), and liver (AAPC = −1.1%, p < 0.05). The 5-year RSRs for Stage I cancers were greater than 93% for the colon and rectum, female breast, and cervix uteri, whereas RSRs for patients with Stage IV cancers ranged from 2.9% to 38.9%, with patients with liver cancer and those with oral cancer showing the lowest and highest RSRs, respectively.
Conclusion
Our study showed increased incidence in most cancers and provided baseline estimates of stage-specific RSRs among the Taiwanese adult population. Continuous surveillance may help politicians to improve health policies and cancer care in Taiwan.}
}
@article{CHEN20163,
title = {Information from imagery: ISPRS scientific vision and research agenda},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {3-21},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S092427161500218X},
author = {Jun Chen and Ian Dowman and Songnian Li and Zhilin Li and Marguerite Madden and Jon Mills and Nicolas Paparoditis and Franz Rottensteiner and Monika Sester and Charles Toth and John Trinder and Christian Heipke},
keywords = {Research agenda, Photogrammetry, Remote sensing, Spatial information science, ISPRS},
abstract = {With the increased availability of very high-resolution satellite imagery, terrain based imaging and participatory sensing, inexpensive platforms, and advanced information and communication technologies, the application of imagery is now ubiquitous, playing an important role in many aspects of life and work today. As a leading organisation in this field, the International Society for Photogrammetry and Remote Sensing (ISPRS) has been devoted to effectively and efficiently obtaining and utilising information from imagery since its foundation in the year 1910. This paper examines the significant challenges currently facing ISPRS and its communities, such as providing high-quality information, enabling advanced geospatial computing, and supporting collaborative problem solving. The state-of-the-art in ISPRS related research and development is reviewed and the trends and topics for future work are identified. By providing an overarching scientific vision and research agenda, we hope to call on and mobilise all ISPRS scientists, practitioners and other stakeholders to continue improving our understanding and capacity on information from imagery and to deliver advanced geospatial knowledge that enables humankind to better deal with the challenges ahead, posed for example by global change, ubiquitous sensing, and a demand for real-time information generation.}
}
@incollection{MCKNIGHT201486,
title = {Chapter Nine - Data Virtualization: The Perpetual Short-Term Solution},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {86-96},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080560000096},
author = {William McKnight},
keywords = {business intelligence, data virtualization, master data management, big data, data warehouse, self-service business intelligence},
abstract = {Accessing data in multiple places across the information ecosystem makes sense for edge and one-off workloads. Data virtualization could be essential to managing redundancy.}
}
@article{SIMON2014126,
title = {Open Data as Universal Service. New perspectives in the Information Profession},
journal = {Procedia - Social and Behavioral Sciences},
volume = {147},
pages = {126-132},
year = {2014},
note = {3rd International Conference on Integrated Information (IC-ININFO)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.07.128},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814040397},
author = {L. Fernando Ramos Simón and Rosario Arquero Avilés and Iuliana Botezan and Félix del Valle Gastaminza and Silvia Cobo Serrano},
keywords = {Open Data, information profession, Open Content, public information, librarians},
abstract = {The Internet provides a global information flow, which improves living conditions in poor countries as well as in rich countries. Owing to its abundance and quality, public information (meteorological, geographic, transport information. and also the content managed in libraries, archives and museums) is an incentive for change, becoming invaluable and accessible to all citizens. However, it is clear that Open Data plays a significant role and provides a business service in the digital economy. Nevertheless, it is unknown how this amount of public data may be introduced as universal service to make it available to all citizens in matters of education, health, culture . In fact, a function or role which has traditionally been assumed by libraries. In addition, information professionals will have to acquire new skills that enable them to assume a new role in the information management: data management (Open Data) and content management (Open Content). Thus, this study analyzes new roles, which will be assumed by new information professionals such as metadata, interoperability, access licenses, information search and retrieval tools and applications for data queries.}
}
@article{JAYADEV2013115,
title = {Basel III implementation: Issues and challenges for Indian banks},
journal = {IIMB Management Review},
volume = {25},
number = {2},
pages = {115-130},
year = {2013},
issn = {0970-3896},
doi = {https://doi.org/10.1016/j.iimb.2013.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0970389613000293},
author = {M. Jayadev},
keywords = {Basel III, Capital regulation, Capital management, Indian banking},
abstract = {The Basel III framework, whose main thrust has been enhancing the banking sector's safety and stability, emphasises the need to improve the quality and quantity of capital components, leverage ratio, liquidity standards, and enhanced disclosures. This article first lays the context of Basel III and then incorporates the views of senior executives of Indian banks and risk management experts on addressing the challenges of implementing the Basel III framework, especially in areas such as augmentation of capital resources, growth versus financial stability, challenges for enhanced profitability, deposit pricing, cost of credit, maintenance of liquidity standards, and strengthening of risk architecture.}
}
@article{GRANELL2016231,
title = {Beyond data collection: Objectives and methods of research using VGI and geo-social media for disaster management},
journal = {Computers, Environment and Urban Systems},
volume = {59},
pages = {231-243},
year = {2016},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2016.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516300060},
author = {Carlos Granell and Frank O. Ostermann},
keywords = {VGI, Crowdsourcing, Geo-social media, data analysis methods, Disaster management, Systematic mapping},
abstract = {This paper investigates research using VGI and geo-social media in the disaster management context. Relying on the method of systematic mapping, it develops a classification schema that captures three levels of main category, focus, and intended use, and analyzes the relationships with the employed data sources and analysis methods. It focuses the scope to the pioneering field of disaster management, but the described approach and the developed classification schema are easily adaptable to different application domains or future developments. The results show that a hypothesized consolidation of research, characterized through the building of canonical bodies of knowledge and advanced application cases with refined methodology, has not yet happened. The majority of the studies investigate the challenges and potential solutions of data handling, with fewer studies focusing on socio-technological issues or advanced applications. This trend is currently showing no sign of change, highlighting that VGI research is still very much technology-driven as opposed to theory- or application-driven. From the results of the systematic mapping study, the authors formulate and discuss several research objectives for future work, which could lead to a stronger, more theory-driven treatment of the topic VGI in GIScience.}
}
@article{KOHLMAYER201537,
title = {The cost of quality: Implementing generalization and suppression for anonymizing biomedical data with minimal information loss},
journal = {Journal of Biomedical Informatics},
volume = {58},
pages = {37-48},
year = {2015},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2015.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1532046415002002},
author = {Florian Kohlmayer and Fabian Prasser and Klaus A. Kuhn},
keywords = {Security, Privacy, De-identification, Anonymization, Statistical disclosure control, Optimization},
abstract = {Objective
With the ARX data anonymization tool structured biomedical data can be de-identified using syntactic privacy models, such as k-anonymity. Data is transformed with two methods: (a) generalization of attribute values, followed by (b) suppression of data records. The former method results in data that is well suited for analyses by epidemiologists, while the latter method significantly reduces loss of information. Our tool uses an optimal anonymization algorithm that maximizes output utility according to a given measure. To achieve scalability, existing optimal anonymization algorithms exclude parts of the search space by predicting the outcome of data transformations regarding privacy and utility without explicitly applying them to the input dataset. These optimizations cannot be used if data is transformed with generalization and suppression. As optimal data utility and scalability are important for anonymizing biomedical data, we had to develop a novel method.
Methods
In this article, we first confirm experimentally that combining generalization with suppression significantly increases data utility. Next, we proof that, within this coding model, the outcome of data transformations regarding privacy and utility cannot be predicted. As a consequence, existing algorithms fail to deliver optimal data utility. We confirm this finding experimentally. The limitation of previous work can be overcome at the cost of increased computational complexity. However, scalability is important for anonymizing data with user feedback. Consequently, we identify properties of datasets that may be predicted in our context and propose a novel and efficient algorithm. Finally, we evaluate our solution with multiple datasets and privacy models.
Results
This work presents the first thorough investigation of which properties of datasets can be predicted when data is anonymized with generalization and suppression. Our novel approach adopts existing optimization strategies to our context and combines different search methods. The experiments show that our method is able to efficiently solve a broad spectrum of anonymization problems.
Conclusion
Our work shows that implementing syntactic privacy models is challenging and that existing algorithms are not well suited for anonymizing data with transformation models which are more complex than generalization alone. As such models have been recommended for use in the biomedical domain, our results are of general relevance for de-identifying structured biomedical data.}
}
@incollection{NETTLETON2014239,
title = {Chapter 20 - Summary},
editor = {David Nettleton},
booktitle = {Commercial Data Mining},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {239},
year = {2014},
isbn = {978-0-12-416602-8},
doi = {https://doi.org/10.1016/B978-0-12-416602-8.00020-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166028000200},
author = {David Nettleton},
keywords = {conclusion, summary, data mining, methodology, web data, golden nugget},
abstract = {This chapter summarizes and concludes the book.}
}
@incollection{2013169,
editor = {April Reeve},
booktitle = {Managing Data in Motion},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {169-174},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397167-8},
doi = {https://doi.org/10.1016/B978-0-12-397167-8.00036-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971678000364}
}
@article{WEBER2015618,
title = {Internet of things: Privacy issues revisited},
journal = {Computer Law & Security Review},
volume = {31},
number = {5},
pages = {618-627},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364915001156},
author = {Rolf H. Weber},
keywords = {Data minimization, Internet of things, Quality of data, Privacy challenges, Privacy enhancing technologies, Transparency},
abstract = {The Internet of Things presents unique challenges to the protection of individual privacy. This article highlights the growing need for appropriate regulatory as well as technical action in order to bridge the gap between the automated surveillance by IoT devices and the rights of individuals who are often unaware of the potential privacy risk to which they are exposed. As a result, new legal approaches for the protection of privacy need to be developed.}
}
@article{SHAHIAN2016841,
title = {The Society of Thoracic Surgeons National Database: “What’s Past Is Prologue”},
journal = {The Annals of Thoracic Surgery},
volume = {101},
number = {3},
pages = {841-845},
year = {2016},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2016.01.058},
url = {https://www.sciencedirect.com/science/article/pii/S0003497516000710},
author = {David M. Shahian}
}
@incollection{HALAMKA2015xix,
title = {Foreword by John Halamka},
editor = {Linda A. Winters-Miner and Pat S. Bolding and Joseph M. Hilbe and Mitchell Goldstein and Thomas Hill and Robert Nisbet and Nephi Walton and Gary D. Miner},
booktitle = {Practical Predictive Analytics and Decisioning Systems for Medicine},
publisher = {Academic Press},
pages = {xix},
year = {2015},
isbn = {978-0-12-411643-6},
doi = {https://doi.org/10.1016/B978-0-12-411643-6.00064-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124116436000648},
author = {John Halamka}
}
@article{NGUYEN201437,
title = {A Near-Linear Time Subspace Search Scheme for Unsupervised Selection of Correlated Features},
journal = {Big Data Research},
volume = {1},
pages = {37-51},
year = {2014},
note = {Special Issue on Scalable Computing for Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2014.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2214579614000057},
author = {Hoang-Vu Nguyen and Emmanuel Müller and Klemens Böhm},
keywords = {Correlation, Unsupervised feature selection, Subspace search, Outlier mining, Clustering, Classification},
abstract = {In many real-world applications, data is collected in high dimensional spaces. However, not all dimensions are relevant for data analysis. Instead, interesting knowledge is hidden in correlated subsets of dimensions (i.e., subspaces of the original space). Detecting these correlated subspaces independently of the underlying mining task is an open research problem. It is challenging due to the exponential search space. Existing methods have tried to tackle this by utilizing Apriori search schemes. However, their worst case complexity is exponential in the number of dimensions; and even in practice they show poor scalability while missing high quality subspaces. This paper features a scalable subspace search scheme (4S), which overcomes the efficiency problem by departing from the traditional levelwise search. We propose a new generalized notion of correlated subspaces which gives way to transforming the search space to a correlation graph of dimensions. We perform a direct mining of correlated subspaces in this graph, and then, merge subspaces based on the MDL principle in order to obtain high dimensional subspaces with minimal redundancy. We theoretically show that our search scheme is more general than existing search schemes. Our empirical results reveal that 4S in practice scales near-linearly with both database size and dimensionality, and produces higher quality subspaces than state-of-the-art methods.}
}
@article{HEALY2016342,
title = {Regulatory bioinformatics for food and drug safety},
journal = {Regulatory Toxicology and Pharmacology},
volume = {80},
pages = {342-347},
year = {2016},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2016.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0273230016301349},
author = {Marion J. Healy and Weida Tong and Stephen Ostroff and Hans-Georg Eichler and Alex Patak and Margaret Neuspiel and Hubert Deluyker and William Slikker},
keywords = {Bioinformatics, Regulatory bioinformatics, Drug safety, Food safety, Regulatory science, GCRSR, GSRS, Next-generation sequencing, Microbiome, Genomics},
abstract = {“Regulatory Bioinformatics” strives to develop and implement a standardized and transparent bioinformatic framework to support the implementation of existing and emerging technologies in regulatory decision-making. It has great potential to improve public health through the development and use of clinically important medical products and tools to manage the safety of the food supply. However, the application of regulatory bioinformatics also poses new challenges and requires new knowledge and skill sets. In the latest Global Coalition on Regulatory Science Research (GCRSR) governed conference, Global Summit on Regulatory Science (GSRS2015), regulatory bioinformatics principles were presented with respect to global trends, initiatives and case studies. The discussion revealed that datasets, analytical tools, skills and expertise are rapidly developing, in many cases via large international collaborative consortia. It also revealed that significant research is still required to realize the potential applications of regulatory bioinformatics. While there is significant excitement in the possibilities offered by precision medicine to enhance treatments of serious and/or complex diseases, there is a clear need for further development of mechanisms to securely store, curate and share data, integrate databases, and standardized quality control and data analysis procedures. A greater understanding of the biological significance of the data is also required to fully exploit vast datasets that are becoming available. The application of bioinformatics in the microbiological risk analysis paradigm is delivering clear benefits both for the investigation of food borne pathogens and for decision making on clinically important treatments. It is recognized that regulatory bioinformatics will have many beneficial applications by ensuring high quality data, validated tools and standardized processes, which will help inform the regulatory science community of the requirements necessary to ensure the safe introduction and effective use of these applications.}
}
@incollection{KALAITZOPOULOS201635,
title = {Chapter 2 - Advancements in Data Management and Data Mining Approaches},
editor = {Aamir Shahzad},
booktitle = {Translational Medicine},
publisher = {Academic Press},
address = {Boston},
pages = {35-53},
year = {2016},
isbn = {978-0-12-803460-6},
doi = {https://doi.org/10.1016/B978-0-12-803460-6.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034606000027},
author = {Dimitris Kalaitzopoulos and Ketan Patel and Erfan Younesi},
keywords = {Data integration, Data management, Data mining, Electronic medical records, Health systems},
abstract = {Health systems are facing a number of challenges in the cost-effective delivery of health care with aging populations and a number of diseases such as obesity, cancer, and diabetes increasing in prevalence. At the same time the life sciences industry is also faced with historically low productivity and a dearth of new drugs to replace medicines reaching loss of exclusivity. Translational medicine has emerged as a science that can help tackle these challenges. The move toward electronic medical records in health systems has provided a rich source of new data for conducting research into the pathophysiology of disease. Increasingly, it is understood that not all drugs work the same in all patients, and tailoring the right drug to the right patient at the right time will help improve medical outcomes while also reducing the cost associated with mistreatment or overtreatment. Key to achieving this is the use of new molecular diagnostic techniques such as next-generation sequencing, which can help scientists and clinicians understand the pathophysiology of disease and also identify which drugs will work in which patients. In this chapter we outline a data management framework that can be used to properly integrate and analyze clinical data from medical records or clinical trials and molecular data from new sequencing technologies. The use of different data integration platforms is discussed and approaches to how these can be used as a backbone to enable data mining. Best practices in data mining are described and common techniques that are used in biomedical research are introduced with some use case examples.}
}
@article{SUN2015194,
title = {Generalized optimal wavelet decomposing algorithm for big financial data},
journal = {International Journal of Production Economics},
volume = {165},
pages = {194-214},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.033},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004277},
author = {Edward W. Sun and Yi-Ting Chen and Min-Teh Yu},
keywords = {Big financial data, DWT, High-frequency data, MODWT, Wavelet},
abstract = {Using big financial data for the price dynamics of U.S. equities, we investigate the impact that market microstructure noise has on modeling volatility of the returns. Based on wavelet transforms (DWT and MODWT) for decomposing the systematic pattern and noise, we propose a new wavelet-based methodology (named GOWDA, i.e., the generalized optimal wavelet decomposition algorithm) that allows us to deconstruct price series into the true efficient price and microstructure noise, particularly for the noise that induces the phase transition behaviors. This approach optimally determines the wavelet function, level of decomposition, and threshold rule by using a multivariate score function that minimizes the overall approximation error in data reconstruction. The data decomposition method enables us to estimate and forecast the volatility in a more efficient way than the traditional methods proposed in the literature. Through the proposed method we illustrate our simulation and empirical results of improving the estimation and forecasting performance.}
}
@incollection{SEBASTIANCOLEMAN20133,
title = {Chapter 1 - Data},
editor = {Laura Sebastian-Coleman},
booktitle = {Measuring Data Quality for Ongoing Improvement},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {3-15},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397033-6},
doi = {https://doi.org/10.1016/B978-0-12-397033-6.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970336000018},
author = {Laura Sebastian-Coleman}
}
@article{TAN2016878,
title = {Developing business analytic capabilities for combating e-commerce identity fraud: A study of Trustev’s digital verification solution},
journal = {Information & Management},
volume = {53},
number = {7},
pages = {878-891},
year = {2016},
note = {Special Issue on Papers Presented at Pacis 2015},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2016.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378720616300751},
author = {Felix Ter Chian Tan and Zixiu Guo and Michael Cahalane and Daniel Cheng},
keywords = {Online fraud detection, Business analytics capabilities, Social fingerprinting},
abstract = {Given the significant growth in e-commerce, organizations are seeking novel capabilities and technological innovations to deal simultaneously with the volume of data generated and the need to combat potentially damaging fraudulent activity. Although recent studies identify business analytics (BA) as a potential means of combating fraud, significant inroads into the interrelationships between capabilities and the articulation of a pathway to analytical capability have yet to be made. This study presents an investigation of Trustev, a global provider of digital verification technology, and its development of the profile-based social fingerprinting fraud detection solution. Adopting an interpretive structural modeling technique for data analysis, we construct a framework and reveal a road map for organizations to become analytically capable in online fraud detection. Our study adds to the discourse of the application of BA to combat online fraud.}
}
@article{BAGHERI2016677,
title = {An Integrated Framework of Knowledge Transfer and ICT Issues in Co-creation Value Networks},
journal = {Procedia Computer Science},
volume = {100},
pages = {677-685},
year = {2016},
note = {International Conference on ENTERprise Information Systems/International Conference on Project MANagement/International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.211},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916323791},
author = {Samaneh Bagheri and Rob J. Kusters and Jos J.M. Trienekens},
keywords = {knowledge management system (KMS), issues, knowledge transfer, ICT, co-creation, integrated solution, value network},
abstract = {In dynamic value networks (VNs), knowledge serves as a basis for close collaboration of actors (i.e. firms with their partners and customers) to enhance co-creation of integrated solutions. In order to provide a technical foundation for seamless knowledge transfer among actors, VNs require distributed and interoperable intra- and inter-organizational knowledge management systems (KMS), which rely largely on advanced information and communication technology (ICT). KMS cannot be seen as stand-alone information systems but should address a variety of issues (e.g. organizational or social issues) in relation to knowledge management. Therefore, the new generation of such complex systems, appropriate for VNs, must deal with this variety of issues, ranging from merely business-oriented to pure technologically-oriented issues. Although scholars have studied knowledge transfer and ICT issues in VN settings, their insights and results remain fragmented because of the diverse research efforts from different perspectives (either from a more business or a more technical perspective). This necessitates that a broader perspective needs to be established. This study endeavors to address simultaneously potential issues of both knowledge transfer and ICT in a VN by developing a comprehensive integrated framework. The development of the framework is based on a three-step methodology which comprises the exploration and classification of generic ICT issues in a VN setting, and subsequently integrating knowledge transfer issues with ICT issues. The proposed integrated framework provides a well-structured theoretical basis for future KMS requirement engineering in VN environments.}
}
@incollection{LI201895,
title = {1.08 - Geocoding and Reverse Geocoding},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {95-109},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09593-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489095932},
author = {Dapeng Li},
keywords = {Geocoding, GIS, Location privacy, Location-based services, Match rate, Mobile computing, Online geocoding/reverse geocoding services, Positional accuracy, Repeatability, Reverse geocoding},
abstract = {This article introduces the principles and methods of geocoding/reverse geocoding. Specifically, the components of geocoding/reverse geocoding procedures are covered. Primary geocoding methods based on address point, parcel, street centerline, and area unit data are included. Three popular metrics for evaluating geocoding/reverse geocoding quality are discussed: positional accuracy, match rate, and repeatability. Moreover, a review of various geocoding/reverse geocoding applications is also given. Since location privacy has received significant attention, relevant privacy issues in geocoding/reverse geocoding are also covered. Finally, the recent trends and challenges are listed to shed light on future research directions in this field. In summary, this work aims to help users and researchers develop a better understanding of geocoding/reverse geocoding such that they can use it effectively and contribute to its future development.}
}
@incollection{2013xix,
title = {Preface},
editor = {David Loshin},
booktitle = {Business Intelligence (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {xix-xxviii},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-385889-4},
doi = {https://doi.org/10.1016/B978-0-12-385889-4.05001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123858894050018}
}
@incollection{2015227,
title = {Index},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {227-235},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.18001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378180019}
}
@article{HOLT2015163,
title = {The usage of best practices and procedures in the database community},
journal = {Information Systems},
volume = {49},
pages = {163-181},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001914},
author = {Victoria Holt and Magnus Ramage and Karen Kear and Nick Heap},
keywords = {Database administration, Database management, Data management, Database lifecycle, Best practice and procedures, Database operations},
abstract = {Database management has an important role to play in the management of data assets which are at the heart of every organization. In a fast moving technological era, where data is rapidly expanding, understanding the current best practices and procedures is important for continuous improvement. This paper investigates how databases are actually administered and identifies what practices and procedures are utilized throughout the database lifecycle. The paper highlights the demographics of people who manage database systems and the diverse requirements of database systems given the wide range of software and hardware available. The results of this paper show the breadth of issues relevant to database management. The paper concludes by showing where existing practice and procedures are not optimal, and by highlighting the complexities in the field.}
}
@incollection{SIMON201477,
title = {Chapter 6 - The End Game},
editor = {Alan Simon},
booktitle = {Enterprise Business Intelligence and Data Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {77-82},
year = {2014},
isbn = {978-0-12-801539-1},
doi = {https://doi.org/10.1016/B978-0-12-801539-1.00006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801539100006X},
author = {Alan Simon},
keywords = {Data Management, Critical Success Factors, CSFs, Key Performance Indicators, KPIs, Consensus, Stakeholders, Architectural Tune-Ups},
abstract = {When the enterprise data management (EDM) future state has been realized, work must continue to sustain the hard-won gains from the EDM effort. Lasting buy-in must be maintained; the EDM architecture must undergo regular “tune-ups” to address the inevitable work-arounds; vendor relationships need to be nurtured and proactively managed; and seismic shifts such as disruptive technology and global economic conditions must be factored into ongoing EDM activities.}
}
@incollection{CAPOZZOLI2016353,
title = {Chapter 11 - Enhancing energy efficiency in buildings through innovative data analytics technologiesa},
editor = {Ciprian Dobre and Fatos Xhafa},
booktitle = {Pervasive Computing},
publisher = {Academic Press},
address = {Boston},
pages = {353-389},
year = {2016},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-803663-1},
doi = {https://doi.org/10.1016/B978-0-12-803663-1.00011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128036631000115},
author = {A. Capozzoli and T. Cerquitelli and M.S. Piscitelli},
keywords = {Energy efficiency in buildings, Building energy performance, Building energy modeling, Building-related data, Knowledge discovery process, Data analytics technologies, Database management systems, Big data services, Unsupervised data mining algorithms, Exploratory data analytics algorithms},
abstract = {This chapter discusses different platforms for buildings exploiting novel technologies based on sensor networks, smart meters and database management systems to collect and store energy-related data. It also discusses novel analytical tools and data mining algorithms proposed in the literature for buildings to (i) characterize energy consumption, (ii) identify the main factors that increase energy consumption, (iii) detect faults, and (iv) enhance user energy awareness. Finally, the perspectives offered by energy-related data analytics are outlined, showing how analysis techniques can be profitably exploited to enhance user energy awareness and reduce building energy consumption.}
}
@incollection{BARRY2013195,
title = {Chapter 16 - Terminology},
editor = {Douglas K. Barry and David Dick},
booktitle = {Web Services, Service-Oriented Architectures, and Cloud Computing (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {195-216},
year = {2013},
series = {The Savvy Manager's Guides},
isbn = {978-0-12-398357-2},
doi = {https://doi.org/10.1016/B978-0-12-398357-2.00016-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123983572000166},
author = {Douglas K. Barry and David Dick}
}
@article{AUSTINBOOTH2015695,
title = {Libraries and Institutional Data Analytics: Challenges and Opportunities},
journal = {The Journal of Academic Librarianship},
volume = {41},
number = {5},
pages = {695-699},
year = {2015},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2015.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0099133315001597},
author = {H. {Austin Booth} and Dean Hendrix}
}
@article{2016IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {55},
pages = {IFC},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(15)00173-8},
url = {https://www.sciencedirect.com/science/article/pii/S0306437915001738}
}
@article{SPIEKERMANN2015181,
title = {A vision for global privacy bridges: Technical and legal measures for international data markets},
journal = {Computer Law & Security Review},
volume = {31},
number = {2},
pages = {181-200},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0267364915000321},
author = {Sarah Spiekermann and Alexander Novotny},
keywords = {Information privacy, Personal data market, Economics of personal information, Privacy regulation},
abstract = {From the early days of the information economy, personal data has been its most valuable asset. Despite data protection laws and an acknowledged right to privacy, trading personal information has become a business equated with “trading oil”. Most of this business is done without the knowledge and active informed consent of the people. But as data breaches and abuses are made public through the media, consumers react. They become irritated about companies' data handling practices, lose trust, exercise political pressure and start to protect their privacy with the help of technical tools. As a result, companies' Internet business models that are based on personal data are unsettled. An open conflict is arising between business demands for data and a desire for privacy. As of 2015 no true answer is in sight of how to resolve this conflict. Technologists, economists and regulators are struggling to develop technical solutions and policies that meet businesses' demand for more data while still maintaining privacy. Yet, most of the proposed solutions fail to account for market complexity and provide no pathway to technological and legal implementation. They lack a bigger vision for data use and privacy. To break this vicious cycle, we propose and test such a vision of a personal information market with privacy. We accumulate technical and legal measures that have been proposed by technical and legal scholars over the past two decades. And out of this existing knowledge, we compose something new: a four-space market model for personal data.}
}
@article{2016IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {58},
pages = {IFC},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(16)30105-3},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916301053}
}
@article{2016iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {98},
pages = {iii-vi},
year = {2016},
note = {The 7th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2016)/The 6th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2016)/Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(16)32272-4},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916322724}
}
@article{NAKAMURA20142,
title = {Localized algorithms for information fusion in resource constrained networks},
journal = {Information Fusion},
volume = {15},
pages = {2-4},
year = {2014},
note = {Special Issue: Resource Constrained Networks},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1566253513000766},
author = {Eduardo Freire Nakamura and Antonio Alfredo Ferreira Loureiro and Azzedine Boukerche and Albert Y. Zomaya}
}
@article{MARGARA201424,
title = {Streaming the Web: Reasoning over dynamic data},
journal = {Journal of Web Semantics},
volume = {25},
pages = {24-44},
year = {2014},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2014.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826814000067},
author = {Alessandro Margara and Jacopo Urbani and Frank {van Harmelen} and Henri Bal},
keywords = {Semantic Web, Stream reasoning, Survey, Stream processing, Complex Event Processing},
abstract = {In the last few years a new research area, called stream reasoning, emerged to bridge the gap between reasoning and stream processing. While current reasoning approaches are designed to work on mainly static data, the Web is, on the other hand, extremely dynamic: information is frequently changed and updated, and new data is continuously generated from a huge number of sources, often at high rate. In other words, fresh information is constantly made available in the form of streams of new data and updates. Despite some promising investigations in the area, stream reasoning is still in its infancy, both from the perspective of models and theories development, and from the perspective of systems and tools design and implementation. The aim of this paper is threefold: (i) we identify the requirements coming from different application scenarios, and we isolate the problems they pose; (ii) we survey existing approaches and proposals in the area of stream reasoning, highlighting their strengths and limitations; (iii) we draw a research agenda to guide the future research and development of stream reasoning. In doing so, we also analyze related research fields to extract algorithms, models, techniques, and solutions that could be useful in the area of stream reasoning.}
}
@incollection{RYAN201627,
title = {Chapter 2 - Improved agility and insights through (visual) discovery},
editor = {Lindy Ryan},
booktitle = {The Visual Imperative},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {27-43},
year = {2016},
isbn = {978-0-12-803844-4},
doi = {https://doi.org/10.1016/B978-0-12-803844-4.00002-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038444000029},
author = {Lindy Ryan},
keywords = {data discovery, BI, friction, visual discovery, strategy, agility, insights},
abstract = {This chapter focuses on the expansion of traditional business intelligence to include data discovery. It discusses how discovery should be seen as a complementary process that drives BI going forward, rather than replace it, to supports data-driven competencies for evolving data-centric organizations. It also reviews the role of friction in discovery and how to navigate the four forms of discovery to maximize the value of data discovery as a key strategic process. Finally, it explores the emergence of visual discovery and briefly touches on the role of discovery in the days to come, and the unique challenges and opportunities that discovery will bring as it becomes an increasingly fundamental strategic process.}
}
@article{2016IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {57},
pages = {IFC},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(16)00005-3},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916000053}
}
@article{VINUKIRAN2016215,
title = {Incentive Compatible E-mandi with Large Scale Consumer Producer Matching Using BigData Based on Gale-shapely Algorithm for Perishable Commodities SCM},
journal = {Procedia Computer Science},
volume = {87},
pages = {215-220},
year = {2016},
note = {Fourth International Conference on Recent Trends in Computer Science & Engineering (ICRTCSE 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.151},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916304902},
author = {S. {Vinu Kiran} and S. {Prasanna Devi} and S. Manivannan},
abstract = {In this paper, we propose to transform the global matching mechanism in an electronic exchange between the producers and consumers in the SCM system for perishable commodities over large scale data sets. Matching of of consumers and producers satisfactions are mathematically modeled based on preferential evaluations based on the bidding request and the requirements data which is supplied as a matrix to Gale Shapely matching algorithm. The matching works over a very transparent approach in a e-trading environment over large scale data. Since, Bigdata is involved; the global SCM could be much clearer and easier for allocation of perishable commodities. These matching outcomes are compared with the matching and profit ranges obtained using simple English auction method which results Pareto-optimal matches. We are observing the proposed method produces stable matching, which is preference-strategy proof with incentive compatibility for both consumers and producers. Our design involves the preference revelation or elicitation problem and the preference-aggregation problem. The preference revelation problem involves eliciting truthful information from the agents about their types that are used for computation of Incentive compatible results. We are using Bayesian incentive compatible mechanism design in our match-making settings where the agents’ preference types are multidimensional. This preserves profitability up to an additive loss that can be made arbitrarily small in polynomial time in the number of agents and the size of the agents’ type spaces.}
}
@article{GILLAN201619,
title = {Taking Psychiatry Research Online},
journal = {Neuron},
volume = {91},
number = {1},
pages = {19-23},
year = {2016},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2016.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0896627316302550},
author = {Claire M. Gillan and Nathaniel D. Daw},
abstract = {Psychiatry is in need of a major overhaul. In order to improve the precision with which we can treat, classify, and research mental health problems, we need bigger datasets than ever before. Web-based data collection provides a novel solution.}
}
@article{2016IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {56},
pages = {IFC},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(15)00196-9},
url = {https://www.sciencedirect.com/science/article/pii/S0306437915001969}
}
@article{GIABBANELLI20161968,
title = {Teaching Computational Modeling in the Data Science Era},
journal = {Procedia Computer Science},
volume = {80},
pages = {1968-1977},
year = {2016},
note = {International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.517},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916310055},
author = {Philippe J. Giabbanelli and Vijay K. Mago},
keywords = {Course content, Data analytics, Simulations},
abstract = {Integrating data and models is an important and still challenging goal in science. Computational modeling has been taught for decades and regularly revised, for example in the 2000s where it became more inclusive of data mining. As we are now in the ‘data science’ era, we have the occasion (and often the incentive) to teach in an integrative manner computational modeling and data science. In this paper, we reviewed the content of courses and programs on computational modeling and/or data science. From this review and our teaching experience, we formed a set of design principles for an integrative course. We independently implemented these principles in two public research universities, in Canada and the US, for a course targeting graduate students and upper-division undergraduates. We discuss and contrast these implementations, and suggest ways in which the teaching of computational science can continue to be revised going forward.}
}
@incollection{2015513,
title = {Index},
editor = {Rick Sherman},
booktitle = {Business Intelligence Guidebook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {513-525},
year = {2015},
isbn = {978-0-12-411461-6},
doi = {https://doi.org/10.1016/B978-0-12-411461-6.18001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114616180013}
}
@incollection{KIBBE201641,
title = {Chapter 3 - Cancer Clinical Research: Enhancing Data Liquidity and Data Altruism},
editor = {Bradford W. Hesse and David K. Ahern and Ellen Beckjord},
booktitle = {Oncology Informatics},
publisher = {Academic Press},
address = {Boston},
pages = {41-53},
year = {2016},
isbn = {978-0-12-802115-6},
doi = {https://doi.org/10.1016/B978-0-12-802115-6.00003-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021156000033},
author = {Warren Kibbe},
keywords = {Data sharing, data liquidity, consent, mHealth, open data, open science, data altruism, ontology},
abstract = {A number of converging factors (including ubiquitous computing, decreased cost of sequencing, imaging, uptake of EHRs driven by the Accountable Care Act and Meaningful Use) have made it possible to generate and aggregate much more detailed molecular, lab, clinical, and patient-reported data. Our increased understanding of the genomic changes associated with cancer and the complex mutational load present in a given tumor have made it apparent that identifying targeted therapies that are relevant and effective will require new strategies for engaging patients in cancer research and in sharing and analyzing these data. This chapter highlights some of the opportunities for increasing data liquidity through data sharing and data altruism.}
}
@article{2016622,
title = {To the Cloud! A Grassroots Proposal to Accelerate Brain Science Discovery},
journal = {Neuron},
volume = {92},
number = {3},
pages = {622-627},
year = {2016},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2016.10.033},
url = {https://www.sciencedirect.com/science/article/pii/S0896627316307838},
author = {Joshua T. Vogelstein and Brett Mensh and Michael Häusser and Nelson Spruston and Alan C. Evans and Konrad Kording and Katrin Amunts and Christoph Ebell and Jeff Muller and Martin Telefont and Sean Hill and Sandhya P. Koushika and Corrado Calì and Pedro Antonio Valdés-Sosa and Peter B. Littlewood and Christof Koch and Stephan Saalfeld and Adam Kepecs and Hanchuan Peng and Yaroslav O. Halchenko and Gregory Kiar and Mu-Ming Poo and Jean-Baptiste Poline and Michael P. Milham and Alyssa Picchini Schaffer and Rafi Gidron and Hideyuki Okano and Vince D. Calhoun and Miyoung Chun and Dean M. Kleissas and R. Jacob Vogelstein and Eric Perlman and Randal Burns and Richard Huganir and Michael I. Miller},
abstract = {The revolution in neuroscientific data acquisition is creating an analysis challenge. We propose leveraging cloud-computing technologies to enable large-scale neurodata storing, exploring, analyzing, and modeling. This utility will empower scientists globally to generate and test theories of brain function and dysfunction.}
}
@incollection{TALBURT201589,
title = {Chapter 6 - Resolve and Retrieve Phase – Identity Resolution},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {89-103},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000065},
author = {John R. Talburt and Yinle Zhou},
keywords = {Identity resolution, batch access, interactive access, closed universe, open universe},
abstract = {The resolve and retrieve phase of the CSRUD life cycle is the primary use case for MDM as an application. Client systems provide entity identity information in exchange for the identifier of an entity, a process called identity resolution. From an MDM perspective, two important aspects of identity resolution guides its implementation and underlying architecture. These are its mode of access – batch versus interactive – and its universe model – open universe versus closed universe.}
}
@incollection{SHERMAN201565,
title = {Chapter 4 - Architecture Framework},
editor = {Rick Sherman},
booktitle = {Business Intelligence Guidebook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {65-84},
year = {2015},
isbn = {978-0-12-411461-6},
doi = {https://doi.org/10.1016/B978-0-12-411461-6.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114616000046},
author = {Rick Sherman},
keywords = {Accidental architecture, BI architecture framework, Data architecture, Information architecture, Product architecture, Technical architecture},
abstract = {The BI architecture framework helps all of an enterprise's BI projects complement one another and create cohesive, cost-effective BI solutions. The framework accommodates expansion and renovation based on evolving requirements, capabilities, and skills. It is made up of four parts. The first is the information architecture, which defines the purpose of the project, the business processes and analytics, who will have access, and where the data is and how it will be integrated and consumed. The second is data architecture, which defines the data along with the schemas, integration, transformations, storage, and workflow required to enable the analytical requirements of the information architecture. Third is the technical architecture, which defines the technologies that are used to implement and support a BI solution that fulfills the information and data architecture requirements. Fourth is the product architecture, which defines the products, their configurations, and how they are interconnected to implement the technology requirements of the BI framework. The overall architecture must also accommodate metadata and requirements for security and privacy. Companies that do not plan end up with an accidental architecture, which leads to data silos and other inefficiencies.}
}
@article{LAM20141,
title = {Issues and opportunities in spatiotemporal transport analysis: An introduction to the special issue},
journal = {Travel Behaviour and Society},
volume = {1},
number = {1},
pages = {1-2},
year = {2014},
note = {Advances in Spatiotemporal Transport Analysis},
issn = {2214-367X},
doi = {https://doi.org/10.1016/j.tbs.2013.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214367X13000021},
author = {Winnie W.Y. Lam and Soora Rasouli and Harry Timmermans}
}
@incollection{REEVE2013113,
title = {Chapter 16 - Data Warehousing with Real-Time Updates},
editor = {April Reeve},
booktitle = {Managing Data in Motion},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {113-117},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397167-8},
doi = {https://doi.org/10.1016/B978-0-12-397167-8.00016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971678000169},
author = {April Reeve}
}
@article{KNOTTNERUS2016270,
title = {Research data as a global public good},
journal = {Journal of Clinical Epidemiology},
volume = {70},
pages = {270-271},
year = {2016},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2015.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S0895435615003303},
author = {J. André Knottnerus}
}
@article{LEE20151057,
title = {Suitable organization forms for knowledge management to attain sustainable competitive advantage in the renewable energy industry},
journal = {Energy},
volume = {89},
pages = {1057-1064},
year = {2015},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2015.06.047},
url = {https://www.sciencedirect.com/science/article/pii/S0360544215008038},
author = {Amy H.I. Lee and Hsing Hung Chen and Silu Chen},
keywords = {Interactive learning framework, KM (Knowledge management), Renewable energy industry, Sustainable competitive advantage, Organization form},
abstract = {The rapid growth of China's economy has accelerated its energy demand. The exploitation of renewable energy is essential because of limited conventional energy sources, high energy consumption, unstable and escalating oil prices, and detrimental environmental pollutions. Firms in the renewable energy industry are currently facing challenges to maintain competitiveness and productivity while minimizing environmental impacts. The ability to manage knowledge is a key feature in the process for firms to obtain competitive advantages. In addition, interactive learning framework provides a platform that can respond to the need for adjustment in time of great uncertainty. This paper adds evidence to the literature of interactive learning environment based on China context. It examines critical characteristics of interactive learning framework in the renewable energy industry, and then investigates suitable organization forms for knowledge management at different levels of a supply chain. On this basis, this paper proposes suitable organizational forms under different situations for sustainable competitive advantage.}
}
@article{KADOM20151296,
title = {Data Drives Quality Improvement},
journal = {Journal of the American College of Radiology},
volume = {12},
number = {12, Part A},
pages = {1296-1297},
year = {2015},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2015.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S1546144015009898},
author = {Nadja Kadom and Paul Nagy}
}
@article{BERNSTEIN2015480,
title = {Ensuring Public Health’s Future in a National-Scale Learning Health System},
journal = {American Journal of Preventive Medicine},
volume = {48},
number = {4},
pages = {480-487},
year = {2015},
issn = {0749-3797},
doi = {https://doi.org/10.1016/j.amepre.2014.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0749379714006710},
author = {Jennifer A. Bernstein and Charles Friedman and Peter Jacobson and Joshua C. Rubin},
abstract = {Data and information are fundamental to every function of public health and crucial to public health agencies, from outbreak investigations to environmental surveillance. Information allows for timely, relevant, and high-quality decision making by public health agencies. Evidence-based practice is an important, grounding principle within public health practice, but resources to handle and analyze public health data in a meaningful way are limited. The Learning Health System is a platform that seeks to leverage health data to allow evidence-based real-time analysis of data for a broad range of uses, including primary care decision making, public health activities, consumer education, and academic research. The Learning Health System is an emerging endeavor that is gaining support throughout the health sector and presents an important opportunity for collaboration between primary care and public health. Public health should be a key stakeholder in the development of a national-scale Learning Health System because participation presents many potential benefits, including increased workforce capacity, enhanced resources, and greater opportunities to use health information for the improvement of the public’s health. This article describes the framework and progression of a national-scale Learning Health System, considers the advantages of and challenges to public health involvement in the Learning Health System, including the public health workforce, gives examples of small-scale Learning Health System projects involving public health, and discusses how public health practitioners can better engage in the Learning Health Community.}
}
@article{2015IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {53},
pages = {IFC},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(15)00098-8},
url = {https://www.sciencedirect.com/science/article/pii/S0306437915000988}
}
@article{MEERSMAN20163,
title = {Challenges and future research needs towards international freight transport modelling},
journal = {Case Studies on Transport Policy},
volume = {4},
number = {1},
pages = {3-8},
year = {2016},
note = {Data to Freight Modelling},
issn = {2213-624X},
doi = {https://doi.org/10.1016/j.cstp.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213624X15300225},
author = {Hilde Meersman and Verena Charlotte Ehrler and Dirk Bruckmann and Ming Chen and Jan Francke and Peter Hill and Clare Jackson and Jens Klauenberg and Martin Kurowski and Saskia Seidel and Inge Vierth},
keywords = {International freight transport, Freight transport model, Data sourcing},
abstract = {The advanced internationalisation of markets and production processes continuously adds to the complexity of supply chains. At the same time improving the sustainability of the related international freight transport processes and optimising their efficiency is becoming a topic of central relevance. International freight transport models are an important tool to simulate impacts of measures taken to achieve such improvements of transport processes. Yet, the requirements towards international freight transport models are complex: they need to include various modes of transport, they need to cover different industries and their dynamics, they need to consider seasonality of supply and demand of goods, demographic parameters, economic developments, technological developments including their impact on production processes and structures, and many other aspects. Furthermore, international freight transport models need to include freight flows within countries as well as freight flows between the considered countries. This paper discusses the challenges which need to be confronted when developing international freight transport models which are able to correspond to the described complexity of international freight transport. Furthermore, it maps out the most important research gaps which need to be addressed by international freight transport modelling research in order to ensure that the challenges identified are captured within the models developed to improve international freight transport.}
}
@article{JAGER2016116,
title = {Advanced Complexity Management Strategic Recommendations of Handling the “Industrie 4.0” Complexity for Small and Medium Enterprises},
journal = {Procedia CIRP},
volume = {57},
pages = {116-121},
year = {2016},
note = {Factories of the Future in the digital environment - Proceedings of the 49th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S221282711631174X},
author = {Jens Jäger and Oliver Schöllhammer and Michael Lickefett and Thomas Bauernhansl},
keywords = {advanced Complexity Management, “Industrie 4.0”, small and medium-sized enterprises},
abstract = {The new chances and perspectives of “Industrie 4.0”, with cloud computing, cyber-physical systems and smart factory, lead to an increasing complexity and hence opacity, especially for small and medium enterprises. The current Fraunhofer IPA empirical study “Industrie 4.0 - opportunities and prospects in the Metropolitan Region Rhine-Neckar”, on behalf of the Chambers of Commerce “IHK Rhein-Neckar”, “IHK Darmstadt Rhein Main Neckar” and “IHK Pfalz”, shows that enterprises expect an increase in future complexity in “Industrie 4.0”, but have not yet the knowhow for operational implementation. The preparation of this study involved more than 200 companies in the Metropolitan Region Rhine-Neckar and neighbouring areas Westpfalz and Greater Darmstadt. 195 companies participated in the study survey, which was conducted in the form of 34 personal interviews and 161 online surveys. Furthermore, a workshop was organised with 25 enterprise participants. The participants of the survey and the workshop were mostly managing directors or production directors. This paper presents the results of the study and sets out a developed approach, based on the aforementioned results, to the advanced management strategy for handling complexity with “Industrie 4.0”. This contains an enterprise-specific and stepwise approach, four principles for Industry 4.0 introduction, and specific recommendations for small and medium enterprises.}
}
@incollection{BOREK20143,
title = {Chapter 1 - Data and Information Assets},
editor = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
booktitle = {Total Information Risk Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {3-22},
year = {2014},
isbn = {978-0-12-405547-6},
doi = {https://doi.org/10.1016/B978-0-12-405547-6.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124055476000018},
author = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
keywords = {Data and Information Assets, Characteristics, Information Manufacturing, Data and Information Quality, Business Impact},
abstract = {This chapter introduces key concepts about data and information assets, including a discussion on characteristics of data and information assets, quality of data and information assets, and their business impact.}
}
@article{2015IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {54},
pages = {IFC},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(15)00139-8},
url = {https://www.sciencedirect.com/science/article/pii/S0306437915001398}
}
@incollection{2015vii,
title = {Endorsements},
editor = {Mark Allen and Dalton Cervo},
booktitle = {Multi-Domain Master Data Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {vii},
year = {2015},
isbn = {978-0-12-800835-5},
doi = {https://doi.org/10.1016/B978-0-12-800835-5.09986-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128008355099863}
}
@article{NABELSI2015106,
title = {Detecting Constraints in Supply Chain Reengineering Projects: Case Study of Data and Process Integration in a Hospital Pharmacy},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {3},
pages = {106-111},
year = {2015},
note = {15th IFAC Symposium onInformation Control Problems inManufacturing},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315003055},
author = {Véronique Nabelsi and Stéphane Gagnon},
keywords = {Supply Chain Management (SCM), Business Process Management (BPM), Decision Support System (DSS), Extract Transform and Load (ETL), Data Models, Verification of Information System},
abstract = {This paper discusses how messy data may be a hidden failure factor that Business Process Reengineering (BPR) projects typically cannot detect during the planning phase. Our case study deals with Supply Chain Management (SCM) within two major urban hospitals, involving $2 million in minimum stocks for drug inventory. Our project addresses the feasibility of the hospital's data warehousing integration, especially at the stage of Extract, Transform, and Load (ETL). We conclude with a proposed system architecture audit and verification method that may serve to guide reengineering project planning and execution.}
}
@incollection{SHEIKH2013129,
title = {Chapter 8 - Requirements Gathering for Analytics Projects},
editor = {Nauman Sheikh},
booktitle = {Implementing Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {129-145},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-401696-5},
doi = {https://doi.org/10.1016/B978-0-12-401696-5.00008-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124016965000086},
author = {Nauman Sheikh},
keywords = {requirements gathering, requirements extraction, requirements analysis, data profiling, decision strategy, operational integration},
abstract = {This chapter addresses the challenges of gathering requirements in an analytics project. This is challenging since a business has probably never seen anything like it and might not know exactly what they want to accomplish. In Chapter 7 we relied on a pilot project to make a case for the adoption of analytics; this chapter shows how to convert that into requirements for an analytics project.}
}
@incollection{LUISI2014189,
title = {Part IV - Information Architecture},
editor = {James V. Luisi},
booktitle = {Pragmatic Enterprise Architecture},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {189-261},
year = {2014},
isbn = {978-0-12-800205-6},
doi = {https://doi.org/10.1016/B978-0-12-800205-6.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002056000044},
author = {James V. Luisi},
keywords = {Information architecture, data architecture, data governance, business data glossary architecture, data ownership architecture, data access rights architecture, ETL data masking architecture, canned report access architecture, data stewardship, data discovery, semantic modeling, architecture governance component registry, data governance dashboard, data obfuscation architecture, data modeling architecture, reference data management—product master, reference data management—code tables, reference data management—external files, data in motion architecture, data virtualization architecture, ETL architecture, ESB architecture, CEP architecture, content management architecture, master data management, MDM, logical data architecture, LDA, code tables, external files, operational workflow, reference data, activity data, initial business setup, conducting business, analyzing business, business data glossary, business metadata, identifying the source of data, extracting data to a landing zone, data profiling, data standardization, data integration, data ownership, data access rights, sensitive data, masked, encrypted, canned report with variable data, canned report with fixed data, treaty zone, jurisdictional level, authority document level, data steward level, data discovery architecture, data landscape, Big Data, ontology, document management, taxonomy of legislative jurisdictions, administration, document development, approval process, production use, metrics, data obfuscation, data access restrictions, data masking, data encryption, data at rest, DAR, data in motion, DIM, protection of business communications, SSN, data modeling, conceptual data model, logical data model, physical data model, normalization, 1NF, 2NF, 3NF, 4NF, 5NF, 6NF, 7NF, DKNF, BCNF, weaknesses or normalization, abstraction, rules of data abstraction, class words, 1AF, 2AF, 3AF, 4AF, transaction path analysis, TAPA, reference data management, product master management, metadata, code tables management, ISO, International Organization for Standardization, external files management, A.M Best, Bank of Canada, Bank of England, Dun & Bradstreet, Equifax, Experian, Fitch, Moody’s, Morningstar, United Nations, ETL, ESB, CEP, FTP, XML, data streaming, data virtualization, ODS, data warehouse, extract transform and load, ETL CASE tool, enterprise service bus, SOAP, service-oriented architecture protocol, complex event processing, content management},
abstract = {This part separates one of the most important areas of specialization, information architecture, from the rest of the vast area of enterprise architecture so as to provide it the appropriate degree of focus and attention. Information architecture is the brain and central nervous system of any large organization and as such should be called out as a distinct set of disciplines with its own philosophy and mindset. While arguably the most important part of an organization's ecosystem, information architecture is among the most difficult for the general population to understand due to the need to intimately understand the business as well as a vast array of IT areas of specialization involving data architecture, reference data, master data, data governance, data stewardship, data discovery, data in motion, and a variety of associated disciplines that reside in operations architecture, business architecture, and the main body of enterprise architecture in the previous major section.}
}
@article{RODRIGUEZDONCEL2016799,
title = {Legal aspects of linked data – The European framework},
journal = {Computer Law & Security Review},
volume = {32},
number = {6},
pages = {799-813},
year = {2016},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2016.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0267364916301194},
author = {Víctor Rodríguez-Doncel and Cristiana Santos and Pompeu Casanovas and Asunción Gómez-Pérez},
keywords = {Linked data, Open data, Copyright, Database right, Data protection, Licences},
abstract = {This paper portrays a general overview of the existing European legal framework that applies to the publication and consumption of linked data resources in typical settings. The point of view of both data publishers and data consumers is considered, identifying their rights and obligations, with special attention to those derived from the copyright and data protection laws. The goal of this analysis is to identify the practices that help to make the publication and consumption of linked data resources legally compliant processes. An insight on broader regulations, best practices and common situations is given.}
}
@article{STURROCK2016635,
title = {Mapping Malaria Risk in Low Transmission Settings: Challenges and Opportunities},
journal = {Trends in Parasitology},
volume = {32},
number = {8},
pages = {635-645},
year = {2016},
issn = {1471-4922},
doi = {https://doi.org/10.1016/j.pt.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1471492216300472},
author = {Hugh J.W. Sturrock and Adam F. Bennett and Alemayehu Midekisa and Roly D. Gosling and Peter W. Gething and Bryan Greenhouse},
abstract = {As malaria transmission declines, it becomes increasingly focal and prone to outbreaks. Understanding and predicting patterns of transmission risk becomes an important component of an effective elimination campaign, allowing limited resources for control and elimination to be targeted cost-effectively. Malaria risk mapping in low transmission settings is associated with some unique challenges. This article reviews the main challenges and opportunities related to risk mapping in low transmission areas including recent advancements in risk mapping low transmission malaria, relevant metrics, and statistical approaches and risk mapping in post-elimination settings.}
}
@article{CREUZOTGARCHER20161414,
title = {Incidence of Acute Postoperative Endophthalmitis after Cataract Surgery: A Nationwide Study in France from 2005 to 2014},
journal = {Ophthalmology},
volume = {123},
number = {7},
pages = {1414-1420},
year = {2016},
issn = {0161-6420},
doi = {https://doi.org/10.1016/j.ophtha.2016.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0161642016002001},
author = {Catherine Creuzot-Garcher and Eric Benzenine and Anne-Sophie Mariet and Aurélie {de Lazzer} and Christophe Chiquet and Alain M. Bron and Catherine Quantin},
abstract = {Purpose
To report the incidence of acute postoperative endophthalmitis (POE) after cataract surgery from 2005 to 2014 in France.
Design
Cohort study.
Participants
Patients undergoing operation for cataract surgery by phacoemulsification and presenting acute POE.
Methods
We identified acute POE occurring within 6 weeks after phacoemulsification cataract surgery and the use of intracameral antibiotic injection during the surgical procedure by means of billing codes from a national database.
Main Outcome Measures
Incidence of acute POE.
Results
From January 2005 to December 2014, 6 371 242 eyes in 3 983 525 patients underwent phacoemulsification cataract surgery. The incidence of acute POE after phacoemulsification decreased from 0.145% to 0.053% during this 10-year period; the unadjusted incidence rate ratio (IRR) (95% confidence interval) was 0.37 (0.32–0.42; P < 0.001). In multivariate analysis, intracameral antibiotic injection was associated with a lower risk of acute POE 0.53 (0.50–0.57; P < 0.001), whereas intraoperative posterior capsule rupture, combined surgery, and gender (male) were associated with a higher risk of acute POE: 5.24 (4.11–6.68), 1.77 (1.53–2.05), and 1.48 (1.40–1.56) (P < 0.001), respectively.
Conclusions
Access to a national database allowed us to observe a decrease in acute POE after phacoemulsification cataract surgery from 2005 to 2014. Within the same period, the use of intracameral antibiotics during the surgical procedures increased.}
}
@incollection{TALBURT2015xvii,
title = {Acknowledgements},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xvii-xviii},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.04001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378040011},
author = {John R. Talburt and Yinle Zhou}
}
@article{SHARPLES2013853,
title = {Usability, human factors and geographic information},
journal = {Applied Ergonomics},
volume = {44},
number = {6},
pages = {853-854},
year = {2013},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0003687013000288},
author = {Sarah Sharples and Michael Brown and Jenny Harding and Mike Jackson}
}
@article{2015IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {48},
pages = {IFC},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(14)00167-7},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001677}
}
@article{FORTINO201457,
title = {Integration of Cloud computing and body sensor networks},
journal = {Future Generation Computer Systems},
volume = {35},
pages = {57-61},
year = {2014},
note = {Special Section: Integration of Cloud Computing and Body Sensor Networks; Guest Editors: Giancarlo Fortino and Mukaddim Pathan},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2014.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1400017X},
author = {Giancarlo Fortino and Mukaddim Pathan}
}
@incollection{BOREK2014283,
title = {Chapter 14 - Conclusions and Outlook},
editor = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
booktitle = {Total Information Risk Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {283-284},
year = {2014},
isbn = {978-0-12-405547-6},
doi = {https://doi.org/10.1016/B978-0-12-405547-6.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124055476000146},
author = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall}
}
@incollection{RYAN2016243,
title = {Chapter 12 - Visual discovery by design},
editor = {Lindy Ryan},
booktitle = {The Visual Imperative},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {243-255},
year = {2016},
isbn = {978-0-12-803844-4},
doi = {https://doi.org/10.1016/B978-0-12-803844-4.00012-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038444000121},
author = {Lindy Ryan},
keywords = {discovery by design, disruption, high-performance, mobile, discovery},
abstract = {This chapter converges discussions from the preceding chapters in an integrated model of visual discovery by design. It is the capstone chapter of the book, applying all preceding learning into one application before ending with a look at the future and emerging technologies in the IOT.}
}
@incollection{LUISI201457,
title = {Part III - Information Systems},
editor = {James V. Luisi},
booktitle = {Pragmatic Enterprise Architecture},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {57-188},
year = {2014},
isbn = {978-0-12-800205-6},
doi = {https://doi.org/10.1016/B978-0-12-800205-6.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002056000032},
author = {James V. Luisi},
keywords = {information systems architecture, enterprise architecture, business architecture, operations architecture, systems architecture, control systems architecture, cross-discipline capabilities, solution architect, subject matter expert, stakeholders, Legal, Compliance, Auditing, Chief Customer Officer, outsourcing partners, customers, investors, shareholders, regulators, enterprise architects, hedgehog principle, architectural standards, technology portfolio management, TPM, organizing technologies into portfolios, architecture ROI framework, application impact, infrastructure impact, personnel impact, vendor impact, operational workflow impact, business impact, after tax implications, net present value, NPV, internal rate of return, IRR, personnel severance costs, airfare, lodging, enhanced technology portfolio management, technology categories, technology subcategories, technology metadata, reporting architecture, OLTP, online transaction processing, OLAP, online analytical processing, business intelligence, nonstatistical analysis, data mining, predictive analytics, operational data store, ODS, DW, data warehouse, data mart, DM, GIS, geographic information system, Big Data, mashup technology, data warehouse architecture, BI Architecture, rollups, aggregates, dimensions, MOLAP, ROLAP, HOLAP, WOLAP, DOLAP, RTOLAP, CEP, complex event processing, NLP, natural language processing, data visualization, cluster diagrams, terrain maps, architectural drawings, floor plans, shelf layouts, routes, connectivity diagrams, bubbles, histograms, heat maps, scatter plots, rose charts, cockpit gauges, radar diagrams, stem and leaf plots, predict behavior, longitudinal analysis, survey sampling models, stimulus-response predictive models, statistical methods, nonstatistical models, neural networks, neural nets, polynomial, Big Data architecture, structured data, unstructured data, semistructured data, velocity, volume, frequency of updates, variety, concurrent users, OldSQL, New SQL, SQL, free space, query language, ACID, atomicity, consistency, isolation, durability, node, symmetric multiprocessing, SMP, massively parallel processing, MPP, asymmetric massively parallel processing, AMPP, Hadoop, DFS, distributed file system, MapReduce, Apache Software Foundation, ASF, FOSS, IP, Google, GFS, HDSF, GPFS, BigTable, NoSQL, Hbase, R Language, R Programming Language, Sqoop, Hive, Pig, Impala, FSF, free software foundation, CRAN, Comprehensive R Archive Network, R extensions, Luisi Prime Numbers, competing Hadoop frameworks, Cloudera, Hortonworks, MapR, IBM, Microsoft, Intel, Syncsort, SAS, Big Data is use case driven, document management/content management, knowledge management, graph DB, online transaction processing systems, data warehousing, real-time analytics, algorithmic approaches, batch analysis, advanced search, relational database technology in Hadoop, novelty discoveries, class discoveries, association discoveries, document management, content management, government archival records, business document management, IT document management, customer document management, Basho Riak, MarkLogic, MongoDB, Cassandra, Couchbase, Hadoop HDFS, Hadoop HBase, KYC, AML, SAP Hana, HP Vertica, Greenplum, Teradata, lifetime value, LTV, forecasting, Fair Isaac’s, HNC, Ward Systems, SAS complex event processing, algorithm based, matrix vector multiplication, relational algebraic operations, selections and projections, union, intersection and difference, grouping and aggregation, reducer size and replication rates, similarity joins, graph modeling, Netezza, NewSQL, e-Commerce, Akiban, Clustrix, Google Spanner, NuoDB, SQLFire, VoltDB, batch analytics, HDFS, address geocoding, linear measures event modeling, routing, topological, cartography, Neo4j, PostGIS, Oracle Spatial, Geotime, search and discovery, Lucidworks, Solr, Splunk, relational database technology in Hadoop, Splice Machine, Citus Data, use case driven, life cycle, ad hoc deployment, Big Data deployment, metadata, Big Data ecosystem, use case planning, business metadata, use case requirements, internal data discovery, external data discovery, inbound metadata, ingestion metadata, data persistence layer metadata, outbound metadata, lifecycle metadata, operations metadata, data governance metadata, compliance metadata, configuration management metadata, team metadata, directory services metadata, ecosystem administrator metadata, stakeholder metadata, workflow metadata, decommissioning metadata, metadata summary, There is no magic, Big Data accelerators, parallel and distributed processing, reduced code set that eliminates large amounts of DBMS code, fewer features, compression, proprietary hardware, SOA, OOA, LDA, CPU, LZ77, LZ78, Huffman, columnar, Big Data the future, quantum computing, code breaking, cryptography, prime number generation, traveling salesman problem, labeling images and objects within images, identifying correlations in genetic code, testing a scientific hypothesis, machine learning for problem solving (aka self-programming), adiabatic, AQC, gate model, D-Wave, qubit, quantum error correction, quantum processor, titanium, niobium, Columbian, Kelvin, Tesla, Josephson junction, Boolean SAT, SAPI interface, compiler, client libraries, frameworks, applications, mashup architecture, data virtualization layer, cross data landscape metrics, data security, data visualization styles, self-service, LDAP, compliance architecture, treaty zone, business compliance, IT compliance, ISO 17799, ISO 27000, COSO, Committee of Sponsoring Organizations of the Treadway Commission, OFAC, Office of Foreign Assets Control, Treasury department, United and Strengthening America by Providing Appropriate Tools Required to Intercept and Obstruct Terrorism, Section 314(a), USA PATRIOT Act, Office of Federal Contract Compliance Programs, OFCCP, Equal Employment Opportunity Act, EEOA, Financial Stability Board, FSB, Global Financial Markets Association, GFMA, Bank Secrecy Act, BSA, Regulation E of the Electronic Fund Transfer Act, EFTA, Dodd-Frank, Securities and Exchange Commission, SEC, Federal Trade Commission, FTC, Office of the Comptroller of the Currency, OCC, Commodity Futures Trading Commission, CFTC, International Swaps and Derivatives Association, ISDA, Sarbanes Oxley, SOX, Basel II, Solvency II, Blocked Persons List, Targeted Countries List, Denied Persons List, Denied Entities List, FBI’s Most Wanted, Debarred Parties List, Global Watch List, Politically Exposed Persons, PEP, anti-money laundering, know your client, suspicious activity report, SAR, CIP, DTCC, NSCC, DTC, SWIFT, CICI, LEI, CFTC Interim Compliant Identifier, National Securities Clearing Corporation, Customer Identification Program, Society for the Worldwide Interbank Financial Telecommunication, Legal Entity Identifier, RIM, legal hold, records information management, Governance Risk and Compliance, GRC, XBRL, 10-Q, 10-K, 20-F, 8-K, 6-K, legal compliance, HR compliance, financial compliance, application portfolio architecture, APM, applications architecture, business rules, workflow architecture, business capabilities, BPMN, workflow automation, BPM technology, application architecture, singular application, first-generation language, second-generation language, third-generation language, fourth-generation language, drag-and-drop self-service, array language, vector language, assembly language, command line interfaces, compiled language, interpreted language, data manipulation language, object-oriented language, scripting language, procedural language, rules engine, requirements traceability, error handling, software reuse, application architecture design patterns, integration pattern, distribution pattern, tier pattern, procedural pattern, processing pattern, usage pattern, analytical pattern, interactive pattern, data communication pattern, message dissemination pattern, resource sequence pattern, pipeline pattern, pipe and filter, event-driven pattern, blackboard pattern, MV pattern, integration architecture, hub and spoke, ESB, service-oriented architecture, enterprise service bus, extract transform and load, ETL, FTP, file transfer protocol, B2B, B2C, partner integration, life cycle architecture, software development life cycle, data centric life cycle, DCLC, DGLC, DLC, merger and acquisition life cycle, MALC, data center consolidation life cycle, DCCLC, corporate restructuring life cycle, CRLC, outsourcing life cycle, OSLC, insourcing life cycle, ISLC, operations life cycle, OLC, SDLC, ISO/IEC 12207, inception, high-level analysis, detail analysis, logical design, physical design, build, validation, deployment, post-implementation, logical data architecture, data requirements, data analysis, data profiling, conceptual data modeling, logical data modeling, physical data modeling, data discovery, data acquisition, data cleansing, data standardization, data integration, user acceptance, production, data governance life cycle, identifying data points, populating business data glossary, conceptual data architecture, business designated access rights, legal and compliance oversight, secure canned reporting data points, report and querying, production to nonproduction data movement, AGLC, architecture governance life cycle, analyze business direction, analyze business pain points, analyze types of technological issues, analyze all business initiative types, assess business alignment, initial architecture review, postmortem architecture review, divestiture life cycle, identify scope of business being divested, identify divested business capabilities, identify shared operations, automation supporting divested capabilities, identify dedicated operations, detach general ledger, identify unstructured data of divested areas, identify RIM data, define RIM business data, safeguard RIM data, validate RIM reporting, identify legal holds, safeguard legal hold data, validate legal hold reporting, downsize business operations, downsize automation, decommission, downsize IT operations, mergers and acquisitions, identify business scope being acquired, identify business organization impact, identify acquired automation, analyze overlapping automation, identify legal holds, compare data landscapes, identify automation impact, identify development environment impact, implement automation strategy, identify IT organization impact, general ledger integration, right-size business operations, right-size automation, right-size IT operations, data center consolidation, insourcing life cycle},
abstract = {This part enters into the main territory of enterprise architecture for information systems which is as rich in technology specialties as the IT ecosystem is diverse. Most organizations fail to recognize the need for diverse specialization within architecture because they fail to understand the depth of complexity and the costs associated with mediocrity within each area of specialization. They also believe that a general practioner, which we will call a solution architect, is qualified and appropriate to address the complexities across a wide array of technology areas. In reality, this is equivalent to staffing a medical center primarily with general practioners that act as the specialists. A healthy organization maintains top specialists with which the general practioners can participate in getting expertise that is in alignment with a future state vision that reduces complexity and costs.}
}
@incollection{BARRY2013145,
title = {Chapter 12 - Getting Started with Service-Oriented Architectures},
editor = {Douglas K. Barry and David Dick},
booktitle = {Web Services, Service-Oriented Architectures, and Cloud Computing (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {145-162},
year = {2013},
series = {The Savvy Manager's Guides},
isbn = {978-0-12-398357-2},
doi = {https://doi.org/10.1016/B978-0-12-398357-2.00012-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123983572000129},
author = {Douglas K. Barry and David Dick}
}
@incollection{BROWELL201687,
title = {Chapter 6 - From Linked Open Data to Linked Open Knowledge},
editor = {David Baker and Wendy Evans},
booktitle = {Digital Information Strategies},
publisher = {Chandos Publishing},
pages = {87-99},
year = {2016},
isbn = {978-0-08-100251-3},
doi = {https://doi.org/10.1016/B978-0-08-100251-3.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002513000068},
author = {Geoff Browell},
keywords = {archives, Linked Data, Linked Open Data, visualisation},
abstract = {This chapter explores the latest trends in the use of linked data in archives, reviewing recent projects that over the last two years have sought to build reliable controlled vocabularies of linked data and develop robust editing and mark-up tools for archive catalogues. It draws on first-hand experience of managing the ‘Step Change’ project and of collaboration between the AIM25 aggregation service, the historical mapping specialists, Historypin, and the leading archive catalogue software vendor, Axiell, and provides a critical appraisal of the opportunities and challenges of such public-private partnership to facilitate widespread professional adoption of linked data. It argues that the true potential of linked data for archivists lies less in its role as a technology for handling data and more as a renewed opportunity for archivists to flourish both as custodians of their collections and as storytellers, using linked data to build a new relationship of trust and engagement with users and fellow professionals.}
}
@article{PURVES20141,
title = {Moving beyond the point: An agenda for research in movement analysis with real data},
journal = {Computers, Environment and Urban Systems},
volume = {47},
pages = {1-4},
year = {2014},
note = {Progress in Movement Analysis – Experiences with Real Data},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2014.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971514000660},
author = {Ross S. Purves and Patrick Laube and Maike Buchin and Bettina Speckmann}
}
@incollection{CHOI20161,
title = {1 - Introduction: Key decision points and information requirements in fast fashion supply chains},
editor = {Tsan-Ming Choi},
booktitle = {Information Systems for the Fashion and Apparel Industry},
publisher = {Woodhead Publishing},
pages = {1-8},
year = {2016},
series = {Woodhead Publishing Series in Textiles},
isbn = {978-0-08-100571-2},
doi = {https://doi.org/10.1016/B978-0-08-100571-2.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081005712000014},
author = {T.-M. Choi},
keywords = {Fast fashion, Information requirements, Information systems, Information systems management, Information technology, Key decision points, Supply chain management},
abstract = {Fast fashion is a critical industrial trend which affects the fashion and textiles industry, and the respective supply chains. Undoubtedly, information systems are critically important in fast fashion supply chains as they are essential for many crucial business operations. In this introductory chapter, we first discuss some key decision points in fast fashion supply chains. Then, we examine the information requirements with the goal of achieving an efficient fast fashion supply chain system. Finally, we conclude this chapter by presenting how the articles featured in this handbook are related to enhancing decision making in fast fashion supply chains.}
}