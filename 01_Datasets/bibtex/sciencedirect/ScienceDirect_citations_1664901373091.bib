@article{CHATFIELD2018123,
title = {The role of policy entrepreneurs in open government data policy innovation diffusion: An analysis of Australian Federal and State Governments},
journal = {Government Information Quarterly},
volume = {35},
number = {1},
pages = {123-134},
year = {2018},
note = {Internet Plus Government: Advancement of Networking Technology and Evolution of the Public Sector},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2017.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X16302143},
author = {Akemi Takeoka Chatfield and Christopher G. Reddick},
keywords = {Open government data policy, Policy innovation diffusion, Innovation diffusion theory, Australia, Policy entrepreneur},
abstract = {Open government data (OGD) policy differs substantially from the existing Freedom of Information policies. Consequently OGD can be viewed as a policy innovation. Drawing on both innovation diffusion theory and its application to public policy innovation research, we examine Australia's OGD policy diffusion patterns at both the federal and state government levels based on the policy adoption timing and CKAN portal “Organization” and “Category” statistics. We found that state governments that had adopted OGD policies earlier had active policy entrepreneurs (or lead departments/agencies) responsible for the policy innovation diffusion across the different government departments. We also found that their efficacy ranking was relatively high in terms of OGD portal openness when openness is measured by the greater number of datasets proactively and systematically published through their OGD portals. These findings have important implications for the role played by OGD policy entrepreneurs in openly sharing the government-owned datasets with the public.}
}
@article{2017I,
title = {News on PSWC 2017},
journal = {European Journal of Pharmaceutical Sciences},
volume = {100},
pages = {I},
year = {2017},
issn = {0928-0987},
doi = {https://doi.org/10.1016/S0928-0987(17)30095-7},
url = {https://www.sciencedirect.com/science/article/pii/S0928098717300957}
}
@article{SIOUTAS20181086,
title = {Introduction to the special section on New Trends in Humanistic Informatics: Implementations and Applications},
journal = {Computers & Electrical Engineering},
volume = {70},
pages = {1086-1088},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S004579061831838X},
author = {Spyros Sioutas and Yannis Velegrakis and Valia Kordoni}
}
@article{PRATT201882,
title = {Decoding the non-coding genome: Opportunities and challenges of genomic and epigenomic consortium data},
journal = {Current Opinion in Systems Biology},
volume = {11},
pages = {82-90},
year = {2018},
note = {• Big data acquisition and analysis • Development and differentiation},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452310018300751},
author = {Henry Pratt and Zhiping Weng},
abstract = {Publicly-available next-generation sequencing data has greatly expanded in the past decade, particularly through the work of several international consortia. These collaborative efforts have applied numerous assays profiling diverse features of gene regulation, such as genome-wide chromatin structure, transcriptional activity, and transcription factor binding, to thousands of biosamples from several organisms. Newly-developed computational analyses and statistical methods link findings to gene expression changes and phenotypic changes. Integrative analysis of these datasets holds the potential to revolutionize our understanding of organismal development, cell type differentiation, cellular response to stimuli, and disease mechanisms. However, standardized methods for data access, uniform data processing, and integrative analysis largely do not exist, hindering the impacts of these efforts. Here we review advancements made by consortia and directions of ongoing efforts, as well as challenges in accessing and analyzing publicly-available consortium data and emerging tools for addressing these challenges.}
}
@article{THEODOROU201780,
title = {Data generator for evaluating ETL process quality},
journal = {Information Systems},
volume = {63},
pages = {80-100},
year = {2017},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2016.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916301648},
author = {Vasileios Theodorou and Petar Jovanovic and Alberto Abelló and Emona Nakuçi},
keywords = {Data generator, ETL, Process quality},
abstract = {Obtaining the right set of data for evaluating the fulfillment of different quality factors in the extract-transform-load (ETL) process design is rather challenging. First, the real data might be out of reach due to different privacy constraints, while manually providing a synthetic set of data is known as a labor-intensive task that needs to take various combinations of process parameters into account. More importantly, having a single dataset usually does not represent the evolution of data throughout the complete process lifespan, hence missing the plethora of possible test cases. To facilitate such demanding task, in this paper we propose an automatic data generator (i.e., Bijoux). Starting from a given ETL process model, Bijoux extracts the semantics of data transformations, analyzes the constraints they imply over input data, and automatically generates testing datasets. Bijoux is highly modular and configurable to enable end-users to generate datasets for a variety of interesting test scenarios (e.g., evaluating specific parts of an input ETL process design, with different input dataset sizes, different distributions of data, and different operation selectivities). We have developed a running prototype that implements the functionality of our data generation framework and here we report our experimental findings showing the effectiveness and scalability of our approach.}
}
@article{TARDIEU2017R770,
title = {Plant Phenomics, From Sensors to Knowledge},
journal = {Current Biology},
volume = {27},
number = {15},
pages = {R770-R783},
year = {2017},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2017.05.055},
url = {https://www.sciencedirect.com/science/article/pii/S0960982217306218},
author = {François Tardieu and Llorenç Cabrera-Bosquet and Tony Pridmore and Malcolm Bennett},
abstract = {Summary
Major improvements in crop yield are needed to keep pace with population growth and climate change. While plant breeding efforts have greatly benefited from advances in genomics, profiling the crop phenome (i.e., the structure and function of plants) associated with allelic variants and environments remains a major technical bottleneck. Here, we review the conceptual and technical challenges facing plant phenomics. We first discuss how, given plants’ high levels of morphological plasticity, crop phenomics presents distinct challenges compared with studies in animals. Next, we present strategies for multi-scale phenomics, and describe how major improvements in imaging, sensor technologies and data analysis are now making high-throughput root, shoot, whole-plant and canopy phenomic studies possible. We then suggest that research in this area is entering a new stage of development, in which phenomic pipelines can help researchers transform large numbers of images and sensor data into knowledge, necessitating novel methods of data handling and modelling. Collectively, these innovations are helping accelerate the selection of the next generation of crops more sustainable and resilient to climate change, and whose benefits promise to scale from physiology to breeding and to deliver real world impact for ongoing global food security efforts.}
}
@article{ALGULIYEV2018212,
title = {Cyber-physical systems and their security issues},
journal = {Computers in Industry},
volume = {100},
pages = {212-223},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517304244},
author = {Rasim Alguliyev and Yadigar Imamverdiyev and Lyudmila Sukhostat},
keywords = {Cyber-physical system, Cyber-physical system security, Cyber-physical system attacks, Cyber-physical system security threats, Philosophical issues, Tree of attacks},
abstract = {The creation of cyber-physical systems posed new challenges for people. Ensuring the information security of cyber-physical systems is one of the most complex problems in a wide range of defenses against cyber-attacks. The aim of this paper is to analyse and classify existing research papers on the security of cyber-physical systems. Philosophical issues of cyber-physical systems are raised. Their influence on the aspects of people's lives is investigated. The principle of cyber-physical system operation is described. The main difficulties and solutions in the estimation of the consequences of cyber-attacks, attacks modeling and detection and the development of security architecture are noted. The main types of attacks and threats against cyber-physical systems are analysed. A tree of attacks on cyber-physical systems is proposed. The future research directions are shown.}
}
@article{DONNOLLEY2017332,
title = {More than a name: Heterogeneity in characteristics of models of maternity care reported from the Australian Maternity Care Classification System validation study},
journal = {Women and Birth},
volume = {30},
number = {4},
pages = {332-341},
year = {2017},
issn = {1871-5192},
doi = {https://doi.org/10.1016/j.wombi.2017.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1871519217300124},
author = {Natasha R. Donnolley and Georgina M. Chambers and Kerryn A. Butler-Henderson and Michael G. Chapman and Elizabeth A. Sullivan},
keywords = {Classification, Delivery of health care, Models of care, Maternity care, Health care evaluation mechanisms},
abstract = {Background
Without a standard terminology to classify models of maternity care, it is problematic to compare and evaluate clinical outcomes across different models. The Maternity Care Classification System is a novel system developed in Australia to classify models of maternity care based on their characteristics and an overarching broad model descriptor (Major Model Category).
Aim
This study aimed to assess the extent of variability in the defining characteristics of models of care grouped to the same Major Model Category, using the Maternity Care Classification System.
Method
All public hospital maternity services in New South Wales, Australia, were invited to complete a web-based survey classifying two local models of care using the Maternity Care Classification System. A descriptive analysis of the variation in 15 attributes of models of care was conducted to evaluate the level of heterogeneity within and across Major Model Categories.
Results
Sixty-nine out of seventy hospitals responded, classifying 129 models of care. There was wide variation in a number of important attributes of models classified to the same Major Model Category. The category of ‘Public hospital maternity care’ contained the most variation across all characteristics.
Conclusion
This study demonstrated that although models of care can be grouped into a distinct set of Major Model Categories, there are significant variations in models of the same type. This could result in seemingly ‘like’ models of care being incorrectly compared if grouped only by the Major Model Category.}
}
@article{DEZELAR2018128,
title = {Use of parental disability as a removal reason for children in foster care in the U.S.},
journal = {Children and Youth Services Review},
volume = {86},
pages = {128-134},
year = {2018},
issn = {0190-7409},
doi = {https://doi.org/10.1016/j.childyouth.2018.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0190740917309295},
author = {Sharyn DeZelar and Elizabeth Lightfoot},
keywords = {Parents with disabilities, Child welfare, Foster care},
abstract = {This study uses a large administrative dataset, the Adoption and Foster Care Analysis and Reporting System (AFCARS), to explore how public child welfare agencies in the United States use parental disability in their data collection efforts through examining the use of parental disability as a removal reason. Using data from the 2012 AFCARS foster care file, this study explores how the parental disability removal reason is used and how this removal reason relates to parent and child demographics. The study found that 19% of foster children had parental disability as a removal reason. Children with disabilities and children of certain races had higher odds of having parental disability as a removal reason, as did both younger and older parents. The study also found great variation amongst states in the use of parental disability as a removal. Recommendations for more appropriate collection of parental disability related data are suggested, as basing child welfare decisions on diagnoses versus behavior contradicts guidance jointly put forth by the Departments of Justice and Health and Human Services.}
}
@incollection{GROOT20171,
title = {Chapter 1 - The Changing Financial Services Landscape},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {1-18},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000016},
author = {Martijn Groot},
keywords = {financial services, information technology, data management, enterprise data management, big data},
abstract = {This chapter introduces data management as the foundation of financial services' business processes. It discusses recent developments in data management including technology developments, rise in data volumes, and new requirements on data management processes driven by regulators and clients. This opening chapter introduces the supply chain perspective of data management. This logistical perspective will be one of the common elements throughout the book to look at capture, storage, quality control, and consumption. The chapter ends with an introduction of the data management problem: Why do many firms struggle to get it right despite spending a relatively large portion of revenue on data and information technology compared to other industries? What is the path from data to information to intelligence?}
}
@article{MA2018113,
title = {A geographically and temporally weighted regression model to explore the spatiotemporal influence of built environment on transit ridership},
journal = {Computers, Environment and Urban Systems},
volume = {70},
pages = {113-124},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517306075},
author = {Xiaolei Ma and Jiyu Zhang and Chuan Ding and Yunpeng Wang},
keywords = {Transit ridership, Built environment, Spatio-temporal analysis, Traffic analysis zone},
abstract = {Understanding the influence of the built environment on transit ridership can provide transit authorities with insightful information for operation management and policy making, and ultimately, increase the attractiveness of public transportation. Existing studies have resorted to either traditional ordinary least squares (OLS) regression or geographically weighted regression (GWR) to unravel the complex relationship between ridership and the built environment. Time is a critical dimension that traditional GWR cannot recognize well when performing spatiotemporal analysis on transit ridership. This study addressed this issue by introducing temporal variation into traditional GWR and leveraging geographically and temporally weighted regression (GTWR) to explore the spatiotemporal influence of the built environment on transit ridership. An empirical study conducted in Beijing using one-month transit smart card and point-of-interest data at the traffic analysis zone (TAZ) level demonstrated the effectiveness of GTWR. Compared with those of the traditional OLS and GWR models, a significantly better goodness-of-fit was observed for GTWR. Moreover, the spatiotemporal pattern of coefficients was further analyzed in several TAZs with typical land use types, thereby highlighting the importance of temporal features in spatiotemporal data. Transit authorities can develop transit planning and traffic demand management policies with improved accuracy by utilizing the enhanced precision and spatiotemporal modeling of GTWR to alleviate urban traffic problems.}
}
@article{RAVI201817,
title = {Analytics in/for cloud-an interdependence: A review},
journal = {Journal of Network and Computer Applications},
volume = {102},
pages = {17-37},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517303764},
author = {Kumar Ravi and Yogesh Khandelwal and Boora Shiva Krishna and Vadlamani Ravi},
keywords = {Analytics in cloud, Analytics for cloud, Cloud resource demand prediction, Cloud resource usage optimization, Cloud provisioning},
abstract = {Cloud computing has brought a paradigmatic shift in providing data storage as well as computing resources. With the ever-increasing demand for cloud computing, the number of cloud providers is also increasing evidently, which poses challenges as well as opportunities for consumers and providers. From a consumer point of view, efficient selection of cloud resources at a minimum cost is a big challenge. On the other hand, a provider has to meet consumers’ requirements with sufficient profit in the fiercely competitive market. The relationship between cloud computing is truly symbiotic in the sense that cloud computing makes the practice of analytics more pervasive while analytics makes cloud computing more efficient and optimal in a lot of ways. In addressing these issues, analytics plays an important role. In this paper, we reviewed some important research articles, which focus on cloud computing from the viewpoint of analytics. Analytics and cloud computing are found to be quite interdependent. From analytics perspective, cloud computing makes available high-end computing resources even to an individual customer at an affordable price. We call this thread “Analytics in Cloud”. From the point of view of cloud computing, efficient management, allocation, and demand prediction can be performed using analytics. We call this thread “Analytics for Cloud”. This review paper is mainly based on these two threads of thought process. In this regard, we reviewed eighty-eight research articles published during 2003–2017 related to the formidable duo of cloud computing and analytics.}
}
@article{LIZOTTELATENDRESSE20181143,
title = {Implementing self-service business analytics supporting lean manufacturing: A state-of-the-art review},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1143-1148},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.436},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318315635},
author = {Simon Lizotte-Latendresse and Yvan Beauregard},
keywords = {Self-service analytics, Implementation, Lean manufacturing, BPMN, Constructive Research},
abstract = {Piloting lean manufacturing projects requires dynamically tailoring suitable sets of metrics. Quick turnaround in implementing such metrics is critical, as the typical duration of a Lean Six Sigma project is 3 to 6 months. Self-service Business Analytics (SSBA) can provide managers with the much-needed flexibility to efficiently design and redesign comprehensive metrics in fragmented information system contexts. A review of state-of-the-art practices for SSBA implementation is performed, which lays down the foundations for an upcoming framework geared towards lean manufacturing. Key SSBA planning and architecture findings are summarized. Practical evaluation of the framework in a complex information system landscape through Design Science Research (DSR) is projected with the Canadian division of an international steel parts manufacturing company}
}
@article{CLARK2018258,
title = {Seeing through the clouds: Processes and challenges for sharing geospatial data for disaster management in Haiti},
journal = {International Journal of Disaster Risk Reduction},
volume = {28},
pages = {258-270},
year = {2018},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2018.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S2212420918301997},
author = {Nathan Clark and Flore Guiffault},
keywords = {Geospatial data, Disaster management, Humanitarianism, Haiti},
abstract = {This article examines the ways in which the production and sharing of geospatial data for disaster management purposes have evolved in Haiti, within the context of the 2010 earthquake and 2016 Hurricane Matthew. The conditions for these developments are traced through the institutional and operational dynamics among key stakeholders at international, State and local levels. The article is presented as a case study and is based on reports, field observations and interviews with relevant stakeholders. Overall, the article finds that the increasing recognition, use and value of data for disaster management activities since the earthquake, is contributing to a number of interrelated economic, technical and legal processes and challenges for data sharing among stakeholders in the country. Economic findings are primarily centered around the impact which donor funding, project based work and “new market” dymnamics are having on data sharing. These issues feed into technical findings, where the increasing number of stakeholders and geospatial based projects has led to data access uncertainty and quality concerns among stakeholders. Lastly, legal findings are generally concerned with uncertainty regarding license conditions. Underlying each of these findings is the increasing value and implication of open data. The article concludes with an analytical discussion which frames these main findings within broader developments taking place in the global disaster management sector.}
}
@article{SELTZER201776,
title = {Use of endpoint adjudication to improve the quality and validity of endpoint assessment for medical device development and post marketing evaluation: Rationale and best practices. A report from the cardiac safety research consortium},
journal = {American Heart Journal},
volume = {190},
pages = {76-85},
year = {2017},
issn = {0002-8703},
doi = {https://doi.org/10.1016/j.ahj.2017.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0002870317301503},
author = {Jonathan H. Seltzer and Ted Heise and Peter Carson and Daniel Canos and Jo Carol Hiatt and Pascal Vranckx and Thomas Christen and Donald E. Cutlip},
abstract = {This white paper provides a summary of presentations, discussions and conclusions of a Thinktank entitled “The Role of Endpoint Adjudication in Medical Device Clinical Trials”. The think tank was cosponsored by the Cardiac Safety Research Committee, MDEpiNet and the US Food and Drug Administration (FDA) and was convened at the FDA's White Oak headquarters on March 11, 2016. Attention was focused on tailoring best practices for evaluation of endpoints in medical device clinical trials, practical issues in endpoint adjudication of therapeutic, diagnostic, biomarker and drug-device combinations, and the role of adjudication in regulatory and reimbursement issues throughout the device lifecycle. Attendees included representatives from medical device companies, the FDA, Centers for Medicare and Medicaid Services (CMS), end point adjudication specialist groups, clinical research organizations, and active, academically based adjudicators. The manuscript presents recommendations from the think tank regarding (1) rationale for when adjudication is appropriate, (2) best practices establishment and operation of a medical device adjudication committee and (3) the role of endpoint adjudication for post market evaluation in the emerging era of real world evidence.}
}
@article{AMINI2018322,
title = {MLCPM: A process monitoring framework for 3D metal printing in industrial scale},
journal = {Computers & Industrial Engineering},
volume = {124},
pages = {322-330},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.07.041},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218303644},
author = {Mohammadhossein Amini and Shing I. Chang},
keywords = {Machine learning, Additive manufacturing, Process monitoring, Selective laser melting, Powder bed fusion},
abstract = {Metal 3D printing is one of the fastest growing additive manufacturing (AM) technologies in recent years. Despite much improvements in its technical capabilities, reliable metal printing is still not well understood. One of the barriers of industrialization of metal AM is process monitoring and quality assurance of the printed product. These barriers are especially much highlighted in aerospace and medical device manufacturing industries where the high reliable and quality products are needed. Selective Laser Melting (SLM) is one of the main metal 3D printing methods where more than 50 parameters may affect the quality of the print. However, current SLM printing processes only utilize a fraction of the collected data for quality related tasks. This study proposes a process monitoring framework named MLCPM (Multi-Layer Classifier for Process Monitoring) to predict the likelihood of successful printing at critical printing stages based on collective data provided by identical 3D printing machines producing the same part. The proposed framework provides a blueprint for control strategies during a printing process and aims to prevent defects using data-driven techniques. A numerical study using simulated data is provided to demonstrate how the proposed method can be implemented.}
}
@incollection{RAJAN2018119,
title = {Chapter 6 - Nanoinformatics: Data-Driven Materials Design for Health and Environmental Needs},
editor = {Matthew S. Hull and Diana M. Bowman},
booktitle = {Nanotechnology Environmental Health and Safety (Third Edition)},
publisher = {William Andrew Publishing},
edition = {Third Edition},
address = {Boston},
pages = {119-150},
year = {2018},
series = {Micro and Nano Technologies},
isbn = {978-0-12-813588-4},
doi = {https://doi.org/10.1016/B978-0-12-813588-4.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128135884000063},
author = {Krishna Rajan},
keywords = {Informatics, Materials Science, Nanotoxicology},
abstract = {In this chapter, we describe the statistical learning and informatics approaches to link the foundations of materials chemistry, crystallography, and microstructure to toxicology assessment and prediction using informatics. The field of nanotoxicology lies at the nexus of biology, materials science, environmental sciences, and public health; and we endeavor to highlight how nanoinformatics lies at the core of enabling that scientific convergence to design public policy.}
}
@incollection{LI2018313,
title = {1.22 - Spatial Data Uncertainty},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {313-340},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09610-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012409548909610X},
author = {Linna Li and Hyowon Ban and Suzanne P. Wechsler and Bo Xu},
keywords = {Accuracy, Ambiguity, Big data, Data quality, Error, Fuzzy sets, MAUP, Monte Carlo simulation, Ontology, Scale, Semantic uncertainty, Uncertainty, Vagueness},
abstract = {Uncertainty is an attendant characteristic of all spatial data. Spatial data are complex, as are the phenomena and processes we use these data to represent, model, and understand. Although not exhaustive, this article reviews fundamental concepts related to spatial data uncertainty and methods the geospatial research communities have developed to understand and represent uncertainty. Addressing uncertainty is an ongoing creative exploration and challenge. Especially in the era of big geospatial data, spatial analyses and spatial datasets evolve with technological advances; therefore, new methods for studying uncertainty will be required. In the meantime, existing methods reviewed here should be more widely integrated into standard geospatial practice.}
}
@article{BAILEY201740,
title = {Secure and robust cloud computing for high-throughput forensic microsatellite sequence analysis and databasing},
journal = {Forensic Science International: Genetics},
volume = {31},
pages = {40-47},
year = {2017},
issn = {1872-4973},
doi = {https://doi.org/10.1016/j.fsigen.2017.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1872497317301771},
author = {Sarah F. Bailey and Melissa K. Scheible and Christopher Williams and Deborah S.B.S. Silva and Marina Hoggan and Christopher Eichman and Seth A. Faith},
keywords = {Cloud, Bioinformatics, Microsatellite, Database, Sequencing, Security},
abstract = {Next-generation Sequencing (NGS) is a rapidly evolving technology with demonstrated benefits for forensic genetic applications, and the strategies to analyze and manage the massive NGS datasets are currently in development. Here, the computing, data storage, connectivity, and security resources of the Cloud were evaluated as a model for forensic laboratory systems that produce NGS data. A complete front-to-end Cloud system was developed to upload, process, and interpret raw NGS data using a web browser dashboard. The system was extensible, demonstrating analysis capabilities of autosomal and Y-STRs from a variety of NGS instrumentation (Illumina MiniSeq and MiSeq, and Oxford Nanopore MinION). NGS data for STRs were concordant with standard reference materials previously characterized with capillary electrophoresis and Sanger sequencing. The computing power of the Cloud was implemented with on-demand auto-scaling to allow multiple file analysis in tandem. The system was designed to store resulting data in a relational database, amenable to downstream sample interpretations and databasing applications following the most recent guidelines in nomenclature for sequenced alleles. Lastly, a multi-layered Cloud security architecture was tested and showed that industry standards for securing data and computing resources were readily applied to the NGS system without disadvantageous effects for bioinformatic analysis, connectivity or data storage/retrieval. The results of this study demonstrate the feasibility of using Cloud-based systems for secured NGS data analysis, storage, databasing, and multi-user distributed connectivity.}
}
@article{LU2018945,
title = {Herding boosts too-connected-to-fail risk in stock market of China},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {505},
pages = {945-964},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118304497},
author = {Shan Lu and Jichang Zhao and Huiwen Wang and Ruoen Ren},
keywords = {Herding behavior, Complex network, Stock market crash, Systemic risk, Too-connected-to-fail},
abstract = {The crowd panic and its contagion play non-negligible roles at the time of the stock crash, especially for China where inexperienced investors dominate the market. However, existing models rarely consider investors in networking stocks and accordingly miss the exact knowledge of how panic contagion leads to abrupt crash. In this paper, by networking stocks of sharing common mutual funds, a new methodology of investigating the market crash is presented. It is surprisingly revealed that the herding, which origins in the mimic of seeking for high diversity across investment strategies to lower individual risk, will produce too-connected-to-fail stocks and reluctantly boosts the systemic risk of the entire market. Though too-connected stocks might be relatively stable during the crisis, they are so influential that a small downward fluctuation will cascade to trigger severe drops of massive successor stocks, implying that their falls might be unexpectedly amplified by the collective panic and result in the market crash. Our findings suggest that the whole picture of portfolio strategy has to be carefully supervised to reshape the stock network.}
}
@article{2017IFC,
title = {Editorial Board},
journal = {Information Systems},
volume = {67},
pages = {IFC},
year = {2017},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(17)30249-1},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917302491}
}
@article{PROSSER2018322,
title = {Evolutionary ARMS Race: Antimalarial Resistance Molecular Surveillance},
journal = {Trends in Parasitology},
volume = {34},
number = {4},
pages = {322-334},
year = {2018},
issn = {1471-4922},
doi = {https://doi.org/10.1016/j.pt.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1471492218300011},
author = {Christiane Prosser and Wieland Meyer and John Ellis and Rogan Lee},
keywords = {antimalarial resistance surveillance, artemisinin resistance, molecular surveillance, mHealth},
abstract = {Molecular surveillance of antimalarial drug resistance markers has become an important part of resistance detection and containment. In the current climate of multidrug resistance, including resistance to the global front-line drug artemisinin, there is a consensus to upscale molecular surveillance. The most salient limitation to current surveillance efforts is that skill and infrastructure requirements preclude many regions. This includes sub-Saharan Africa, where Plasmodium falciparum is responsible for most of the global malaria disease burden. New molecular and data technologies have emerged with an emphasis on accessibility. These may allow surveillance to be conducted in broad settings where it is most needed, including at the primary healthcare level in endemic countries, and extending to the village health worker.}
}
@article{ERGUN201751,
title = {A new method for analysis of whole exome sequencing data (SELIM) depending on variant prioritization},
journal = {Informatics in Medicine Unlocked},
volume = {8},
pages = {51-53},
year = {2017},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2017.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352914817300126},
author = {Mehmet Ali Ergun and Abdullah Unal and Sezen Guntekin Ergun and E.Ferda Percin},
keywords = {Whole exome sequencing, Variant prioritization, Workflow},
abstract = {Background
After the first genome had been sequenced in 2003 with an international project, Human Genome Project, the 1000 Genomes Project also revealed the analysis of 1092 and 2504 genomes respectively. Whole exome sequencing of human samples was reported to detect approximately 20,000–30,000 SNV and indel calls on average. It is very important to choose the best tool that suits the related study.
Methods
In this study, it is aimed to demonstrate the results of an in-house method (SELIM) for variant prioritization of WES data without using in-silico methods.
Results
By this method, the annotated data have been decreased by 7.4–13.8 times (mean=10.9).
Conclusion
By the initiation of 1.000.000 genome project, powerful databases are needed. In this respect, SELIM is an in-house workflow that can easily be used for simplifying the annotated data without using any in-silico methods.}
}
@article{BAKKER2018201,
title = {Smart Earth: A meta-review and implications for environmental governance},
journal = {Global Environmental Change},
volume = {52},
pages = {201-211},
year = {2018},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2018.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959378017313730},
author = {Karen Bakker and Max Ritts},
keywords = {Eco-informatics, Environmental governance, Smart earth, Ecology, ICT, IoT, Information and communications technology, Internet of things, Sensors, Digital},
abstract = {Environmental governance has the potential to be significantly transformed by Smart Earth technologies, which deploy enhanced environmental monitoring via combinations of information and communication technologies (ICT), conventional monitoring technologies (e.g. remote sensing), and Internet of Things (IoT) applications (e.g. Environmental Sensor Networks (ESNs)). This paper presents a systematic meta-review of Smart Earth scholarship, focusing our analysis on the potential implications and pitfalls of Smart Earth technologies for environmental governance. We present a meta-review of academic research on Smart Earth, covering 3187 across the full range of academic disciplines from 1997 to 2017, ranging from ecological informatics to the digital humanities. We then offer a critical perspective on potential pathways for evolution in environmental governance frameworks, exploring five key Smart Earth issues relevant to environmental governance: data; real-time regulation; predictive management; open source; and citizen sensing. We conclude by offering suggestions for future research directions and trans-disciplinary conversations about environmental governance in a Smart Earth world.}
}
@article{FANG20181,
title = {The New Frontiers of Cybersecurity},
journal = {Engineering},
volume = {4},
number = {1},
pages = {1-2},
year = {2018},
note = {Cybersecurity},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S2095809918301255},
author = {Binxing Fang and Kui Ren and Yan Jia}
}
@article{2017IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {63},
pages = {IFC},
year = {2017},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(16)30434-3},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916304343}
}
@article{ANTLE2017186,
title = {Next generation agricultural system data, models and knowledge products: Introduction},
journal = {Agricultural Systems},
volume = {155},
pages = {186-190},
year = {2017},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2016.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16304887},
author = {John M. Antle and James W. Jones and Cynthia E. Rosenzweig},
keywords = {Agricultural systems, Data, Models, Knowledge products, Next generation},
abstract = {Agricultural system models have become important tools to provide predictive and assessment capability to a growing array of decision-makers in the private and public sectors. Despite ongoing research and model improvements, many of the agricultural models today are direct descendants of research investments initially made 30–40years ago, and many of the major advances in data, information and communication technology (ICT) of the past decade have not been fully exploited. The purpose of this Special Issue of Agricultural Systems is to lay the foundation for the next generation of agricultural systems data, models and knowledge products. The Special Issue is based on a “NextGen” study led by the Agricultural Model Intercomparison and Improvement Project (AgMIP) with support from the Bill and Melinda Gates Foundation.}
}
@article{KARNITIS20173,
title = {Modelling of Water Supply Costs},
journal = {Procedia Computer Science},
volume = {104},
pages = {3-11},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917300418},
author = {Edvins Karnitis and Girts Karnitis and Janis Zuters and Viktorija Bobinaite},
keywords = {Water utilities, Benchmarking methodologies, Data mining, Artificial neural networks},
abstract = {Water supply tariffs’ setting is a labour intensive regulatory procedure; currently number of informative and procedural shortages and problems exist. The aim of the current research is improvement of methodology for determination of the substantiated costs for provision of water services. A working hypothesis was advanced to modernize the methodology: the specific costs (€/m3) required for the provision of water services in a specific region is a variable multi-parameter function of key performance indicators. There is preferred a benchmark modelling procedure, which is based on the factual cases (declared indicators of water utilities) and synthesis of the general regularity. The model is developed using two independent modelling procedures. The correlation of the synthesized model with declared specific costs of Latvian water utilities is strong (0.88). The correlation between the respective modelled indications exceeds 0.95; hence, the trustworthiness in the results is high. The prospect is the determination of the price ceilings and then an operative tariff setting, thus significantly improving the methodology.}
}
@article{DARWISH2017627,
title = {Towards Composable Threat Assessment for Medical IoT (MIoT)},
journal = {Procedia Computer Science},
volume = {113},
pages = {627-632},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.314},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917317246},
author = {Salaheddin Darwish and Ilia Nouretdinov and Stephen D. Wolthusen},
keywords = {Medical IoT, Security, Privacy, Threat, risk analysis, Federated Network Systems},
abstract = {The Medical Internet of Things (MIoT) has applications beyond clinical settings including in outpatient and care environments where monitoring is occurring over public networks and may involve non-dedicated devices. This poses a number of security and privacy challenges exacerbated by a heterogeneous and dynamic environment, but still requires standards for handling personally identifiable and medical information of patients and in some cases caregivers to be maintained. Whilst risk and threat assessments generally assume a stable and well-defined environment, this cannot be done in MIoT environments where devices may be added, removed, or changed in their configuration including connectivity to server back ends. Conducting a complete threat assessment for each such configuration changes is infeasible. In this paper, we seek to define a mechanism for prioritising MIoT threats and aspects of the analysis that are likely to be affected by composition and related alterations. We propose a mechanism based on the UK HMG IS11 approach and provide a case study in the form of the Technology Integrated Health Management (TIHM)2 test bed.}
}
@incollection{BATARSEH2018193,
title = {Chapter 12 - Intelligent Automation Tools and Software Engines for Managing Federal Agricultural Data},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Federal Data Science},
publisher = {Academic Press},
pages = {193-210},
year = {2018},
isbn = {978-0-12-812443-7},
doi = {https://doi.org/10.1016/B978-0-12-812443-7.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124437000120},
author = {Feras A. Batarseh and Gowtham Ramamoorthy and Manish Dashora and Ruixin Yang},
keywords = {Agricultural data, Automation, Data management, Data warehouse, Federal analyst, Federal tool, Math engine, Requirements, Validation and verification},
abstract = {This chapter dives deep into the technical forest of data management at the government. The main purpose of this chapter is to provide step-by-step technical processes and guidelines for implementing data science and Artificial Intelligence projects at the federal government. The presented systems and intelligent methods were successfully implemented at the US Department of Agriculture. Federal analysts are currently using the mentioned federal tools, validation engine, math engine, and other parts of an intelligent data system that is fully presented in this chapter. The chapter also presents best practices and code snippets to help federal software developers regenerate what is proposed. The work presented here could be used by any software development team at any federal agency. The proposed methods and automated systems aim to override many known challenges that injecting data science at a federal agency might cause. The engines, tools, technical details, and experiments are presented and evaluated.}
}
@incollection{2018xxvii,
title = {Biographies of the Primary Authors of This Book},
editor = {Robert Nisbet and Gary Miner and Ken Yale},
booktitle = {Handbook of Statistical Analysis and Data Mining Applications (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Boston},
pages = {xxvii-xxix},
year = {2018},
isbn = {978-0-12-416632-5},
doi = {https://doi.org/10.1016/B978-0-12-416632-5.09983-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166325099837}
}
@article{AMATO2018412,
title = {Multimedia story creation on social networks},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {412-420},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17322483},
author = {Flora Amato and Aniello Castiglione and Fabio Mercorio and Mario Mezzanzanica and Vincenzo Moscato and Antonio Picariello and Giancarlo Sperlì},
keywords = {Visual analytics, Multimedia summarization, Online social networks, Influence analysis},
abstract = {The paper aims at proposing an original summarization technique from Online Social Networks (OSNs) for multimedia stories’ creation. In particular, for each Multimedia Social Network (MuSN) – i.e. an OSN focusing on the management and sharing of multimedia information – we leverage a graph-based modeling approach and exploit influence analysis methodologies to detect the most important multimedia objects related to one or more topics of interest. Consecutively, from the list of candidate objects we obtain a multimedia summary on the basis of a novel summarization model and some heuristics, whose purpose is to generate multimedia stories with priority (w.r.t. some user keywords), continuity, variety and not receptiveness features. The effectiveness of the proposed approach is shown by the performed experiments on Flickr.}
}
@article{YANG2017256,
title = {Personalized user engagement modeling for mobile videos},
journal = {Computer Networks},
volume = {126},
pages = {256-267},
year = {2017},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1389128617302931},
author = {Lin Yang and Mingxuan Yuan and Yanjiao Chen and Wei Wang and Qian Zhang and Jia Zeng},
keywords = {User engagement, User modeling, Mobile video},
abstract = {The ever-increasing mobile video services and users’ demand for better video quality have boosted research into the video Quality-of-Experience. Recently, the concept of Quality-of-Experience has evolved to Quality-of-Engagement, a more actionable metric to evaluate users’ engagement to the video services and directly relate to the service providers’ revenue model. Existing works on user engagement mostly adopt uniform models to quantify the engagement level of all users, overlooking the essential distinction of individual users. In this paper, we first conduct a large-scale measurement study on a real-world data set to demonstrate the dramatic discrepancy in user engagement, which implies that a uniform model is not expressive enough to characterize the distinctive engagement pattern of each user. To address this problem, we propose PE, a personalized user engagement model for mobile videos, which, for the first time, addresses the user diversity in the engagement modeling. Evaluation results on a real-world data set show that our system significantly outperforms the uniform engagement models, with a 19.14% performance gain.}
}
@article{SILVA20171925,
title = {Organizational Performance and Indicators: Trends and Opportunities},
journal = {Procedia Manufacturing},
volume = {11},
pages = {1925-1932},
year = {2017},
note = {27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, 27-30 June 2017, Modena, Italy},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.07.336},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917305449},
author = {Fernanda Antunes da Silva and Milton Borsato},
keywords = {Organization, Supply Chain, Domain, Knowledge, Indicator, Measurement},
abstract = {Given the current competition into markets, it's necessary for companies to monitor their practices and results in order to ensure competitiveness. To survive these challenges and compete successfully, organizations need to monitor processes through key performance indicators (KPIs). Currently, indicators are analyzed in an isolated way within the organizations. Therefore, it's important that companies use a harmonization approach both in the creation and monitoring process of indicators. Based on it, this article carries out a research to find the state of the art and the research opportunities. To do that, a bibliographic portfolio was constructed and bibliometric and systemic analyzes were performed.}
}
@article{MARSHALL20181824,
title = {Recommendations toward a human pathway-based approach to disease research},
journal = {Drug Discovery Today},
volume = {23},
number = {11},
pages = {1824-1832},
year = {2018},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2018.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S1359644617304737},
author = {Lindsay J. Marshall and Christopher P. Austin and Warren Casey and Suzanne C. Fitzpatrick and Catherine Willett},
abstract = {Failures in the current paradigm for drug development have resulted in soaring research and development costs and reduced numbers of new drug approvals. Over 90% of new drug programs fail, the majority terminated at the level of Phase 2/3 clinical trials, largely because of efficacy failures or unexplained toxicity. A recent workshop brought together members from research institutions, regulatory agencies, industry, academia, and nongovernmental organizations to discuss how existing programs could be better applied to understanding human biology and improving drug discovery. Recommendations include increased emphasis on human relevance, better access and curation of data, and improved interdisciplinary and international collaboration.}
}
@article{ING20171934,
title = {Review of the Society of Thoracic Surgeons Congenital Heart Surgery Database: 2017 Update on Outcomes and Quality Implications for the Anesthesiologist},
journal = {Journal of Cardiothoracic and Vascular Anesthesia},
volume = {31},
number = {6},
pages = {1934-1938},
year = {2017},
issn = {1053-0770},
doi = {https://doi.org/10.1053/j.jvca.2017.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S1053077017305694},
author = {Richard J. Ing and Mark Twite and Cindy Barrett},
keywords = {congenital heart disease, Society of Thoracic Surgeons, databases, quality, pediatric cardiac surgery, outcomes, anesthesia}
}
@incollection{BALL201815,
title = {Chapter 3 - Bibliometric Methods: Basic Principles and Indicators},
editor = {Rafael Ball},
booktitle = {An Introduction to Bibliometrics},
publisher = {Chandos Publishing},
pages = {15-56},
year = {2018},
isbn = {978-0-08-102150-7},
doi = {https://doi.org/10.1016/B978-0-08-102150-7.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780081021507000037},
author = {Rafael Ball},
keywords = {Citation rate, Impact factor, Hirsch index, Altmetrics.},
abstract = {Bibliometric indicators form the basis for measuring scientific research. In the 1960s, Eugene Garfield first developed the Impact Factor, which was conceived as a tool for librarians and library management. Quickly, however, this indicator was used to assess the quality of journals and the scientific articles it contained. The other basic indicators are the amount of scientific output and the citation rate, as well as an immense amount of derivatives of these indicators. In addition to the Hirsch index, a whole series of alternative metrics has emerged in recent decades, which, on the basis of internet publications, also enables the perception and distribution of scientific publications on the internet and all its applications.}
}
@article{MACCHI2018790,
title = {Exploring the role of Digital Twin for Asset Lifecycle Management},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {790-795},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.415},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318315416},
author = {Marco Macchi and Irene Roda and Elisa Negri and Luca Fumagalli},
keywords = {Asset management, Lifecycle management, Digital twin, Decision support},
abstract = {This work is an explorative study to reflect on the role of digital twins to support decisionmaking in asset lifecycle management. The study remarks the current convergence of needs for decision support from Asset Management and of potentials for decision support offered by Digital Twin modeling. The importance of digital twins is evident through state of the art as well as practical use case analysis.}
}
@article{SCHILSKY20171,
title = {Finding the Evidence in Real-World Evidence: Moving from Data to Information to Knowledge},
journal = {Journal of the American College of Surgeons},
volume = {224},
number = {1},
pages = {1-7},
year = {2017},
issn = {1072-7515},
doi = {https://doi.org/10.1016/j.jamcollsurg.2016.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S1072751516315423},
author = {Richard L. Schilsky}
}
@article{TATEM201798,
title = {The geography of imported malaria to non-endemic countries: a meta-analysis of nationally reported statistics},
journal = {The Lancet Infectious Diseases},
volume = {17},
number = {1},
pages = {98-107},
year = {2017},
issn = {1473-3099},
doi = {https://doi.org/10.1016/S1473-3099(16)30326-7},
url = {https://www.sciencedirect.com/science/article/pii/S1473309916303267},
author = {Andrew J Tatem and Peng Jia and Dariya Ordanovich and Michael Falkner and Zhuojie Huang and Rosalind Howes and Simon I Hay and Peter W Gething and David L Smith},
abstract = {Summary
Background
Malaria remains a problem for many countries classified as malaria free through cases imported from endemic regions. Imported cases to non-endemic countries often result in delays in diagnosis, are expensive to treat, and can sometimes cause secondary local transmission. The movement of malaria in endemic countries has also contributed to the spread of drug resistance and threatens long-term eradication goals. Here we focused on quantifying the international movements of malaria to improve our understanding of these phenomena and facilitate the design of mitigation strategies.
Methods
In this meta-analysis, we studied the database of publicly available nationally reported statistics on imported malaria in the past 10 years, covering more than 50 000 individual cases. We obtained data from 40 non-endemic countries and recorded the geographical variations.
Findings
Infection movements were strongly skewed towards a small number of high-traffic routes between 2005 and 2015, with the west Africa region accounting for 56% (13 947/24 941) of all imported cases to non-endemic countries with a reported travel destination, and France and the UK receiving the highest number of cases, with more than 4000 reported cases per year on average. Countries strongly linked by movements of imported cases are grouped by historical, language, and travel ties. There is strong spatial clustering of plasmodium species types.
Interpretation
The architecture of the air network, historical ties, demographics of travellers, and malaria endemicity contribute to highly heterogeneous patterns of numbers, routes, and species compositions of parasites transported. With global malaria eradication on the international agenda, malaria control altering local transmission, and the threat of drug resistance, understanding these patterns and their drivers is increasing in importance.
Funding
Bill & Melinda Gates Foundation, National Institutes of Health, UK Medical Research Council, UK Department for International Development, Wellcome Trust.}
}
@article{GRASSINI201718,
title = {Robust spatial frameworks for leveraging research on sustainable crop intensification},
journal = {Global Food Security},
volume = {14},
pages = {18-22},
year = {2017},
note = {Food Security Governance in Latin America},
issn = {2211-9124},
doi = {https://doi.org/10.1016/j.gfs.2017.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S2211912416300827},
author = {Patricio Grassini and Cameron M. Pittelkow and Kenneth G. Cassman and Haishun S. Yang and Sotirios Archontoulis and Mark Licht and Kendall R. Lamkey and Ignacio A. Ciampitti and Jeffrey A. Coulter and Sylvie M. Brouder and Jeffrey J. Volenec and Noemi Guindin-Garcia},
keywords = {Spatial framework, Agricultural data, Productivity, Sustainability, Crop intensification, Crop system},
abstract = {Meeting demand for food, fiber, feed, and fuel in a world with 9.7 billion people by 2050 without negative environmental impact is the greatest scientific challenge facing humanity. We hypothesize that this challenge can only be met with current and emerging technologies if guided by proactive use of a broad array of relevant data and geospatial scaling approaches to ensure local to global relevance for setting research priorities and implementing agricultural systems responsive to real-time status of weather, soils, crops, and markets. Despite increasing availability of field-scale agricultural data, robust spatial frameworks are lacking to convert these data into actionable knowledge. This commentary article highlights this knowledge gap and calls attention to the need for developing robust spatial frameworks that allow appropriate scaling to larger spatial domains by discussing a recently developed example of a data-driven strategy for estimating yield gaps of agricultural systems. To fully leverage research on sustainable intensification of cropping systems and inform policy development at different scales, we call for new approaches combining the strengths of top-down and bottom-up approaches which will require coordinated efforts between field scientists, crop modelers, and geospatial researchers at an unprecedented level.}
}
@incollection{2017309,
title = {Index},
editor = {Mashrur Chowdhury and Amy Apon and Kakan Dey},
booktitle = {Data Analytics for Intelligent Transportation Systems},
publisher = {Elsevier},
pages = {309-316},
year = {2017},
isbn = {978-0-12-809715-1},
doi = {https://doi.org/10.1016/B978-0-12-809715-1.00028-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097151000286}
}
@incollection{GABBAR2017433,
title = {Chapter 16 - Data centers for smart energy grids},
editor = {Hossam A. Gabbar},
booktitle = {Smart Energy Grid Engineering},
publisher = {Academic Press},
pages = {433-452},
year = {2017},
isbn = {978-0-12-805343-0},
doi = {https://doi.org/10.1016/B978-0-12-805343-0.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053430000164},
author = {H.A. Gabbar and A. Zidan and M. Xiaoli},
keywords = {Data center, System architecture, Big data, Function structure, Database, Data analytics, Management},
abstract = {A smart energy grid (SEG) should satisfy the main functions of system monitoring, supplier-client interactions, energy security, power quality, business intelligence, and data analytics. This requires data acquisition, storage, and utilization. A database is a collection of information that is organized to be easily accessed, managed, and updated. The introduction of a database solves the problem of highly intensive data acquisition, storage, and quick retrieval. A data center is a centralized repository for the storage, management, and dissemination of data and information organized around a particular body of knowledge or pertaining to a particular business. A data center can easily build energy distribution and utilization data from devices and heterogeneous data synchronized from existing systems. Thus, they provide effective and efficient data supporting for advanced applications and allow for realtime data transfer to be used in management applications. This chapter aims to provide in-depth details design, operation, communication, and management for data centers within SEGs.}
}
@article{KULKARNI2017471,
title = {Challenges in the Setup of Large-scale Next-Generation Sequencing Analysis Workflows},
journal = {Computational and Structural Biotechnology Journal},
volume = {15},
pages = {471-477},
year = {2017},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2017.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2001037017300776},
author = {Pranav Kulkarni and Peter Frommolt},
abstract = {While Next-Generation Sequencing (NGS) can now be considered an established analysis technology for research applications across the life sciences, the analysis workflows still require substantial bioinformatics expertise. Typical challenges include the appropriate selection of analytical software tools, the speedup of the overall procedure using HPC parallelization and acceleration technology, the development of automation strategies, data storage solutions and finally the development of methods for full exploitation of the analysis results across multiple experimental conditions. Recently, NGS has begun to expand into clinical environments, where it facilitates diagnostics enabling personalized therapeutic approaches, but is also accompanied by new technological, legal and ethical challenges. There are probably as many overall concepts for the analysis of the data as there are academic research institutions. Among these concepts are, for instance, complex IT architectures developed in-house, ready-to-use technologies installed on-site as well as comprehensive Everything as a Service (XaaS) solutions. In this mini-review, we summarize the key points to consider in the setup of the analysis architectures, mostly for scientific rather than diagnostic purposes, and provide an overview of the current state of the art and challenges of the field.}
}
@article{AHMED20171009,
title = {Data-driven Weld Nugget Width Prediction with Decision Tree Algorithm},
journal = {Procedia Manufacturing},
volume = {10},
pages = {1009-1019},
year = {2017},
note = {45th SME North American Manufacturing Research Conference, NAMRC 45, LA, USA},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.07.092},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917302743},
author = {Fahim Ahmed and Kyoung-Yun Kim},
keywords = {Data-driven weldability prediction, resistance spot welding, decision tree algorithm, weld nugget},
abstract = {This paper presents the capability of a decision tree algorithm to realize a data-driven resistance spot welding (RSW) weldability prediction. Although RSW provides commendable advantages, such as low cost and high speed/high volume operations, the RSW processes are often inconsistent and these significant inconsistencies are a well-known reliability issue. RSW process and data challenges including inconsistency often hinder the utilization of the data-driven weldability prediction. In this paper, we apply a decision tree algorithm on the RSW dataset collected from an automotive OEM to plot regression trees and to extract decision rules for the weld nugget width prediction. With three RSW test datasets, we conclude that the decision trees help in predicting the nugget width and in determining the impact of design and process parameters to the nugget width response variable.}
}
@article{CHEN20181063,
title = {Energy Consumption Modelling Using Deep Learning Technique — A Case Study of EAF},
journal = {Procedia CIRP},
volume = {72},
pages = {1063-1068},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.095},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118301999},
author = {Chong Chen and Ying Liu and Maneesh Kumar and Jian Qin},
keywords = {Energy modelling, Intelligent manufacturing, Deep learning, Data mining},
abstract = {Energy consumption is a global issue which government is taking measures to reduce. Steel plant can have a better energy management once its energy consumption can be modelled and predicted. The purpose of this study is to establish an energy value prediction model for electric arc furnace (EAF) through a data-driven approach using a large amount of real-world data collected from the melt shop in an established steel plant. The data pre-processing and feature selection are carried out. Several data mining algorithms are used separately to build the prediction model. The result shows the predicting performance of the deep learning model is better than the conventional machine learning models, e.g., linear regression, support vector machine and decision tree.}
}
@article{WANG201776,
title = {Evaluation of climate on the Tibetan Plateau using ERA-Interim reanalysis and gridded observations during the period 1979–2012},
journal = {Quaternary International},
volume = {444},
pages = {76-86},
year = {2017},
note = {Third Pole: The Last 20,000 Years - Part 2},
issn = {1040-6182},
doi = {https://doi.org/10.1016/j.quaint.2016.12.041},
url = {https://www.sciencedirect.com/science/article/pii/S1040618216311338},
author = {Xuejia Wang and Guojin Pang and Meixue Yang and Guohui Zhao},
keywords = {Tibetan Plateau, ERA-Interim, Gridded observations, Climate, Spatial–temporal trends},
abstract = {The Tibetan Plateau (TP) is a vast elevated plateau in central Asia, and profoundly impacts regional weather and climate, and even global atmospheric circulation. Here, two frequently used ERA-Interim reanalyses with a spatial resolution of 1.5° × 1.5° (EIN15) and 0.75° × 0.75° (EIN75) are evaluated using a gridded observation dataset at 0.25° spatial resolution from the National Climate Center in China across the TP that covers the period 1979–2012. Climatological characteristics, mean monthly changes, and spatial–temporal trends are examined, with a focus on air temperature and precipitation. Topographic corrections for temperature in ERA-Interim are first conducted based on a vertical temperature lapse rate. The results show that EIN15 and EIN75 with topographic correction closely reproduce the spatial distribution and mean monthly change of temperature on the TP, notwithstanding some cold biases not seen in the observations. The two reanalysis datasets exhibit significant temperature increases over most of the TP, which is similar to the observations. However, the trends exhibit different spatial patterns for all seasons aside from summer, and have lower magnitudes than the observations. EIN15 and EIN75 also reproduce the broad spatial distribution of precipitation, but overestimate precipitation amounts, especially on the southern TP. They also capture some of the observed spatial patterns in the precipitation trend for the period 1979–2012, particularly in winter. Overall, the mean monthly change, mean annual, winter, and summer climatology, and their temporal trends of temperature reproduced by the ERA-Interim data are much better than those of precipitation. As a result of its higher resolution and more accurate topography, EIN75 generates a closer fit to the observed temperatures on the TP than EIN15, but there are no significant differences in precipitation between the two reanalysis datasets. Evaluation of these datasets would be very informative for further climate research and simulations on the TP.}
}
@article{CARNEIRO2017252,
title = {Clinical Intelligence: A study on Corneal Transplantation},
journal = {Procedia Computer Science},
volume = {121},
pages = {252-259},
year = {2017},
note = {CENTERIS 2017 - International Conference on ENTERprise Information Systems / ProjMAN 2017 - International Conference on Project MANagement / HCist 2017 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917322263},
author = {Brian Carneiro and Filipe Portela and António Abelha and Manuel Filipe Santos},
keywords = {Clinical Intelligence, Corneal Transplantation, Clinical Data Warehouse, Key Perfomance Indicators},
abstract = {The Oporto Hospital Center (CHP) is considered a reference in the area of corneal transplantation, having performed more than 4000 transplants to date. Corneas are the most transplanted tissues worldwide and is usually the main method to recover from the blindness caused by corneal diseases. With the purpose of obtaining a better comprehension on the corneal transplantation process, a solution was required. As traditional database systems are not sufficient for proper analysis of health data, a Clinical Intelligence (CI) system was designed. CI allows organizations to acquire new insights and knowledge as well as to integrate enormous volume of health data, mostly non-structured, ambiguous and inclusive. The purpose of this study was to evaluate the process of corneal transplantation, in the context of different key performance indicators that cover not only the analysis to the affected population but also the transplant process, using a CI system. This study was conducted with a sample size of 428 eyes, throughout the period 2013-2016 and 25 business indicators were developed in order to obtain a full comprehension of the subject. With the CI system, it was possible to understand and identify the most common type of procedures, diagnostics, anaesthetics, along with main patient’s characteristics. In addition, an analysis on the input and output flow of the number of requests and interventions as well as the averaging waiting days between them was conducted.}
}
@incollection{ROZINAJOVA201823,
title = {Chapter 2 - Computational Intelligence in Smart Grid Environment},
editor = {Arun Kumar Sangaiah and Michael Sheng and Zhiyong Zhang},
booktitle = {Computational Intelligence for Multimedia Big Data on the Cloud with Engineering Applications},
publisher = {Academic Press},
pages = {23-59},
year = {2018},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-813314-9},
doi = {https://doi.org/10.1016/B978-0-12-813314-9.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128133149000025},
author = {Viera Rozinajová and Anna Bou Ezzeddine and Marek Lóderer and Jaroslav Loebl and Róbert Magyar and Petra Vrablecová},
keywords = {Smart grid, Intelligent data analysis, Computational intelligence, Power load prediction, Optimization, Bio-inspired algorithms, Ensemble models, Support vector regression},
abstract = {This chapter presents one way of incorporating computational intelligence into smart grid environment. We introduce an energy ecosystem, where contemporary technologies are used and by involving advanced methods of data analysis and optimization, we aim to ensure its effective operation. In order to schedule reliable energy supply, the prediction models for power load consumption and for energy spot prices are inevitable. We provide an overview of forecasting and optimization methods and propose solutions, which deal with stream and online processing as well as adaptivity of the proposed solutions. Several different prediction methods including statistical methods and computational intelligence methods, as well as our proposed ensemble and online SVR method are compared. We take into account the current trends of distributed energy generation from renewable sources and anticipate massive usage of electro vehicles in the near future, where the optimization of the whole environment is needed.}
}
@article{WANG2018247,
title = {Understanding travellers’ preferences for different types of trip destination based on mobile internet usage data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {90},
pages = {247-259},
year = {2018},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2018.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18303346},
author = {Yihong Wang and Gonçalo Homem de Almeida Correia and Bart {van Arem} and H.J.P. (Harry) Timmermans},
keywords = {Mobile internet usage, Mobile phone data, Travel behaviour, Mobility analysis, Data fusion},
abstract = {New mobility data sources like mobile phone traces have been shown to reveal individuals’ movements in space and time. However, socioeconomic attributes of travellers are missing in those data. Consequently, it is not possible to partition the population and have an in-depth understanding of the socio-demographic factors influencing travel behaviour. Aiming at filling this gap, we use mobile internet usage behaviour, including one’s preferred type of website and application (app) visited through mobile internet as well as the level of usage frequency, as a distinguishing element between different population segments. We compare the travel behaviour of each segment in terms of the preference for types of trip destinations. The point of interest (POI) data are used to cluster grid cells of a city according to the main function of a grid cell, serving as a reference to determine the type of trip destination. The method is tested for the city of Shanghai, China, by using a special mobile phone dataset that includes not only the spatial-temporal traces but also the mobile internet usage behaviour of the same users. We identify statistically significant relationships between a traveller’s favourite category of mobile internet content and more frequent types of trip destinations that he/she visits. For example, compared to others, people whose favourite type of app/website is in the “tourism” category significantly preferred to visit touristy areas. Moreover, users with different levels of internet usage intensity show different preferences for types of destinations as well. We found that people who used mobile internet more intensively were more likely to visit more commercial areas, and people who used it less preferred to have activities in predominantly residential areas.}
}
@article{2017iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {121},
pages = {iii-x},
year = {2017},
note = {CENTERIS 2017 - International Conference on ENTERprise Information Systems / ProjMAN 2017 - International Conference on Project MANagement / HCist 2017 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(17)32755-2},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917327552}
}
@article{LI2023109044,
title = {Rethinking referring relationships from a perspective of mask-level relational reasoning},
journal = {Pattern Recognition},
volume = {133},
pages = {109044},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109044},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005246},
author = {Chengyang Li and Liping Zhu and Gangyi Tian and Yi Hou and Heng Zhou},
keywords = {Referring relationship, Multimodal learning, Image and text, Visual grounding, Deep learning},
abstract = {Referring relationship aims at localizing subject and object entities in an image, according to a triple text <subject, predicate, object>. Previous methods use iterative attention to shift between image regions for modeling predicate. However, predicate sometimes is implicit and difficult to be represented in the image domain. Convolution modeling method to express predicate is simple and inappropriate. Besides, relational reasoning information in the text itself is not fully utilized. To this end, we rethink referring relationship from a mask-level relational reasoning perspective to improve model interpretability. For text-to-image reasoning, we design Mask Generate and Mask Transfer modules, so as to fully integrate the text priors into the reasoning and prediction of masks. For image-to-text reasoning, we propose an unsupervised triple reconstruction method to guide text-to-image reasoning and improve multimodal generalization. By bi-directional reasoning between image and text, the proposed method MRR fully conforms to the multimodal relational reasoning process. Experiments show that MRR achieves state-of-the-art performance on two datasets of referring relationships, VRD and Visual Genome.}
}
@article{2017IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {65},
pages = {IFC},
year = {2017},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(17)30044-3},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917300443}
}
@incollection{ALI20191124,
title = {Challenges in Creating Online Biodiversity Repositories With Taxonomic Classification},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {1124-1130},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20199-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338201994},
author = {Mohd N.M. Ali and Amy Y. Then Hui and Sarinder K. Dhillon},
keywords = {Database, Fish ontology, Ontology, Taxonomy classification},
abstract = {Taxonomically accurate and well documented biological information are essential for scientific identification and management of organisms. Incorrect identifications which are based on low quality and incorrect literature can significantly impede ecological studies. In this article, we discuss the challenges faced in creating repositories for taxonomic classification, focusing on the differences in classification in several fish-related databases, and propose ways to overcome taxonomic problems. Taxonomic classification for some known ontologies is also discussed i comparison to the Fish Ontology scheme, an ontology for automated recognition of fish species using taxonomic classification.}
}
@article{DREWER2018806,
title = {The canary in the data mine},
journal = {Computer Law & Security Review},
volume = {34},
number = {4},
pages = {806-815},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0267364918302115},
author = {Daniel Drewer and Vesela Miladinova},
keywords = {Data Protection Officer, Independence, Expert-based opinion, Informed advisor, Assurance provider, Data protection, Accountability, Compliance, Privacy on the ground, Privacy by design and default},
abstract = {The present article aims at portraying the type of profile best required to fulfil the function of a Data Protection Officer (DPO) within the EU public sector. The article proposes the idiom of the “canary in a coal mine” as best positioned to describe the multidisciplinary role of DPOs. Due to the particularity and sensitivity of their function, Data Protection Officers act as early indicators of data protection incompliance within their respective area of expertise. Only when being functionally independent, Data Protection Officers could master the role of “canaries in the data mine” thus preventing possible data protection breaches and violations.}
}
@incollection{MACAULAY2017221,
title = {Chapter 12 - Threats and Impacts to the IoT},
editor = {Tyson Macaulay},
booktitle = {RIoT Control},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {221-278},
year = {2017},
isbn = {978-0-12-419971-2},
doi = {https://doi.org/10.1016/B978-0-12-419971-2.00012-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124199712000121},
author = {Tyson Macaulay},
keywords = {Internet of Things (IoT), risk and the Internet of Things (RIoT), threat containment, security},
abstract = {In this chapter, the reader should gain an understanding of the range of threats that may face the Internet of Things (IoT)—they are probably broader than is supposed. The reader should acquire a decent understanding of how threats are assessed and differentiated.}
}
@article{2017IFC,
title = {Editorial Board},
journal = {Information Systems},
volume = {69},
pages = {IFC},
year = {2017},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(17)30409-X},
url = {https://www.sciencedirect.com/science/article/pii/S030643791730409X}
}
@article{LENZ2018180,
title = {Holistic approach to machine tool data analytics},
journal = {Journal of Manufacturing Systems},
volume = {48},
pages = {180-191},
year = {2018},
note = {Special Issue on Smart Manufacturing},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518300360},
author = {Juergen Lenz and Thorsten Wuest and Engelbert Westkämper},
keywords = {Industry 4.0, Smart manufacturing, Machine tool data, Data analytics, Machine learning, Machine tool controller},
abstract = {Recent developments across all phases of the knowledge discovery process in the machine tool data analytics process call for a paradigm shift regarding how to combine the different analytics objectives. Several machine tool data analytics processes are carried out individually by different departments. They are highly dependent on the specific analytics objectives of the individual department. All these individual tasks make use of the data coming from the same source – the machine tool controller and connected sensors. One result of today’s rather diverse machine tool data analytics landscape in many manufacturing companies is that we exhibit several pockets of expertise and large numbers of individual dedicated solutions. Hence, processes and structures tend be inefficient, e.g., exhibit redundant processes, and the exchange between the different domains is difficult. Manufacturers face heavy competition for manufacturing experts, interested and qualified in data analytics. Therefore, it is in their best interest to utilizing this scarce resource as efficiently and effectively as possible. In this paper, we discuss the current situation exhibited in machine tool data analytics in manufacturing. Based on these insights, we propose a holistic approach to machine tool data analytics in order to tackle some of the identified shortcomings of current practices. We propose combining the tasks and bundling up analytics objectives across different departments and/or functions at the production line, factory or even the supply chain level. To evaluate our proposed approach, we provide selected implementation examples following the identified analytics objectives, including cross-domain analytics that focus on the interface between domains. Following, we critically discuss our proposed approach focused on the associated potential benefits, challenges and limitations. Lastly, we conclude the paper and provide an outlook on further research.}
}
@article{GIULIANI201744,
title = {Spatially enabling the Global Framework for Climate Services: Reviewing geospatial solutions to efficiently share and integrate climate data & information},
journal = {Climate Services},
volume = {8},
pages = {44-58},
year = {2017},
issn = {2405-8807},
doi = {https://doi.org/10.1016/j.cliser.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2405880716300772},
author = {Gregory Giuliani and Stefano Nativi and Andre Obregon and Martin Beniston and Anthony Lehmann},
keywords = {Climate services, Essential Climate Variables, Interoperability, OGC standards, GFCS, GEO/GEOSS},
abstract = {In November 2016, the Paris Agreement entered into force calling Parties to strengthen their cooperation for enhancing adaptation and narrowing the gap between climate science and policy. Moreover, climate change has been identified as a central challenge for sustainable development by the United Nations 2030 Agenda for Sustainable Development. Data provide the basis for a reliable scientific understanding and knowledge as well as the foundation for services that are required to take informed decisions. In consequence, there is an increasing need for translating the massive amount of climate data and information that already exists into customized tools, products and services to monitor the range of climate change impacts and their evolution. It is crucial that these data and information should be made available not in the way that they are collected, but in the way that they are being used by the largest audience possible. Considering that climate data is part of the broader Earth observation and geospatial data domain, the aim of this paper is to review the state-of-the-art geospatial technologies that can support the delivery of efficient and effective climate services, and enhancing the value chain of climate data in support of the objectives of the Global Framework for Climate Services. The major benefit of spatially-enabling climate services is that it brings interoperability along the entire climate data value chain. It facilitates storing, visualizing, accessing, processing/analyzing, and integrating climate data and information and enables users to create value-added products and services.}
}
@article{TEIPEL20181216,
title = {Use of nonintrusive sensor-based information and communication technology for real-world evidence for clinical trials in dementia},
journal = {Alzheimer's & Dementia},
volume = {14},
number = {9},
pages = {1216-1231},
year = {2018},
issn = {1552-5260},
doi = {https://doi.org/10.1016/j.jalz.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1552526018301560},
author = {Stefan Teipel and Alexandra König and Jesse Hoey and Jeff Kaye and Frank Krüger and Julie M. Robillard and Thomas Kirste and Claudio Babiloni},
abstract = {Cognitive function is an important end point of treatments in dementia clinical trials. Measuring cognitive function by standardized tests, however, is biased toward highly constrained environments (such as hospitals) in selected samples. Patient-powered real-world evidence using information and communication technology devices, including environmental and wearable sensors, may help to overcome these limitations. This position paper describes current and novel information and communication technology devices and algorithms to monitor behavior and function in people with prodromal and manifest stages of dementia continuously, and discusses clinical, technological, ethical, regulatory, and user-centered requirements for collecting real-world evidence in future randomized controlled trials. Challenges of data safety, quality, and privacy and regulatory requirements need to be addressed by future smart sensor technologies. When these requirements are satisfied, these technologies will provide access to truly user relevant outcomes and broader cohorts of participants than currently sampled in clinical trials.}
}
@article{ZHANG2017142,
title = {Constructing a nationwide interoperable health information system in China: The case study of Sichuan Province},
journal = {Health Policy and Technology},
volume = {6},
number = {2},
pages = {142-151},
year = {2017},
issn = {2211-8837},
doi = {https://doi.org/10.1016/j.hlpt.2017.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S2211883717300023},
author = {Huiping Zhang and Bernard T. Han and Zhiwei Tang},
keywords = {Population health, Health information system, Health information interoperability, Electronic health record, Electronic medical record, Healthcare policy},
abstract = {Objectives
China has set up an ambitious goal to complete the construction of a nationwide interoperable health information system (HIS) by the end of 2020. This paper provides a policy analysis, from the perspective of a province, on how China achieves nationwide interoperability through integrating Population Health Information Platforms (PHIP), developed by healthcare authorities at different levels, with HIS implemented by healthcare institutions.
Methods
An analytical framework, with a focus on interoperability between PHIPs and healthcare institutions’ HIS, is proposed and used to analyze Sichuan Province׳s interoperable HIS to shed light on China׳s approach. To assure the validity of our research, this study analyzed data collected from multiple sources including literature review, web-based search, and interviews with staff from healthcare institutions.
Results
China׳s approach to constructing a nationwide HIS offers great potential and flexibility through delegating PHIP construction to healthcare authorities at different levels. Our findings reveal that developed PHIPs have strong capacities for health information exchange. China׳s approach provides clear guidelines and standards such that healthcare authorities able to complete the construction of PHIPs on time. However, remedial policies are needed to improve the effective use and sustainability of completed systems.
Conclusions
To maximize use of developed systems, China government should: a) define a monitoring policy to ensure full observation of construction guidelines; b) promote a new payment mechanism to motivate information sharing; c) clarify the role of PHIPs, at different levels, to assure their effective use; d) provide incentives for non-public institutions to participate in EMR adoption.}
}
@incollection{GALAR2017371,
title = {Chapter 7 - Maintenance Decision Support Systems},
editor = {Diego Galar and Uday Kumar},
booktitle = {eMaintenance},
publisher = {Academic Press},
pages = {371-474},
year = {2017},
isbn = {978-0-12-811153-6},
doi = {https://doi.org/10.1016/B978-0-12-811153-6.00007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128111536000075},
author = {Diego Galar and Uday Kumar},
keywords = {Artificial intelligence, Asset management, Decision support systems, Exponential technologies, Full life cycle, Industry 4.0, Maintenance},
abstract = {Asset management enables the realization of value from assets throughout their full life cycle. It involves the coordinated and optimized planning, selection, acquisition/development, utilization, care (maintenance), and ultimate disposal or renewal of assets and asset systems.}
}
@article{TSOI2017892,
title = {Predicted Increases in Incidence of Colorectal Cancer in Developed and Developing Regions, in Association With Ageing Populations},
journal = {Clinical Gastroenterology and Hepatology},
volume = {15},
number = {6},
pages = {892-900.e4},
year = {2017},
issn = {1542-3565},
doi = {https://doi.org/10.1016/j.cgh.2016.09.155},
url = {https://www.sciencedirect.com/science/article/pii/S1542356516308680},
author = {Kelvin K.F. Tsoi and Hoyee W. Hirai and Felix C.H. Chan and Sian Griffiths and Joseph J.Y. Sung},
keywords = {IARC, Old Age, Colon Cancer, Rectal Cancer, Cross-National Comparison},
abstract = {Background & Aims
Population growth and changes in demographic structure are linked to trends in colorectal cancer (CRC) incidence. The aim of this study is to estimate future CRC incidence in the ageing population, and compare trends across developing and developed regions.
Methods
Cancer and population data were extracted from the International Agency for Research on Cancer. Annual incidence rates for the major types of cancer in 118 selected populations were extracted from 102 cancer registries in 39 countries worldwide. We selected 8 jurisdictions (from the United States, Europe, and Asia) that reported 20-year cancer incidence rates since 1988. Time series models were constructed to project cancer incidence, by sex and age, to 2030. Incidence rates for persons older than 65 years were combined and further adjusted for change of ageing population. We compared age-adjusted incidence rates among the jurisdictions.
Results
The total population older than 65 years old was 12,917,794 in 1988, and the number increased by almost 40% to 17,950,115 in 2007. In developed countries in the West CRC incidence is predicted to decrease by 16.3% in the United States, increase by 4.8% in the United Kingdom, and increase by 4.7% in Sweden by 2030. In developing countries, such as China (Shanghai), Croatia, and Costa Rica, CRC incidence is predicted to increase in a steep curve by 2030 because of the growing population and ageing effect; in 2030, the incidence increases were 60.5% for China, 47.0% for Croatia, and 18.5% for Costa Rica. We also predict CRC incidence will increase greatly by 2030 in Japan and Hong Kong, which are developed regions.
Conclusions
With the exception of the United States, the incidence of CRC is expected to continue to rise in most regions in the coming decades, due to population growth and changes in demographic structure. The predicted increases are more marked in developing regions with limited health care resources.}
}
@article{AFYOUNI2018291,
title = {Insight and inference for DVARS},
journal = {NeuroImage},
volume = {172},
pages = {291-312},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.12.098},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917311229},
author = {Soroosh Afyouni and Thomas E. Nichols},
keywords = {DVARS, Mean square of successive differences, Autocorrelation, Sum of squares decomposition, Time series, fMRI, Resting-state},
abstract = {Estimates of functional connectivity using resting state functional Magnetic Resonance Imaging (rs-fMRI) are acutely sensitive to artifacts and large scale nuisance variation. As a result much effort is dedicated to preprocessing rs-fMRI data and using diagnostic measures to identify bad scans. One such diagnostic measure is DVARS, the spatial root mean square of the data after temporal differencing. A limitation of DVARS however is the lack of concrete interpretation of the absolute values of DVARS, and finding a threshold to distinguish bad scans from good. In this work we describe a sum of squares decomposition of the entire 4D dataset that shows DVARS to be just one of three sources of variation we refer to as D-var (closely linked to DVARS), S-var and E-var. D-var and S-var partition the sum of squares at adjacent time points, while E-var accounts for edge effects; each can be used to make spatial and temporal summary diagnostic measures. Extending the partitioning to global (and non-global) signal leads to a rs-fMRI DSE table, which decomposes the total and global variability into fast (D-var), slow (S-var) and edge (E-var) components. We find expected values for each component under nominal models, showing how D-var (and thus DVARS) scales with overall variability and is diminished by temporal autocorrelation. Finally we propose a null sampling distribution for DVARS-squared and robust methods to estimate this null model, allowing computation of DVARS p-values. We propose that these diagnostic time series, images, p-values and DSE table will provide a succinct summary of the quality of a rs-fMRI dataset that will support comparisons of datasets over preprocessing steps and between subjects.}
}
@article{MARIANI2018126,
title = {Evaluation of an automated pipeline for large-scale EEG spectral analysis: the National Sleep Research Resource},
journal = {Sleep Medicine},
volume = {47},
pages = {126-136},
year = {2018},
issn = {1389-9457},
doi = {https://doi.org/10.1016/j.sleep.2017.11.1128},
url = {https://www.sciencedirect.com/science/article/pii/S1389945717315629},
author = {Sara Mariani and Leila Tarokh and Ina Djonlagic and Brian E. Cade and Michael G. Morrical and Kristine Yaffe and Katie L. Stone and Kenneth A. Loparo and Shaun M. Purcell and Susan Redline and Daniel Aeschbach},
keywords = {Large-scale spectral analysis, Sleep EEG, Artifact detection},
abstract = {Study objectives
We present an automated sleep electroencephalogram (EEG) spectral analysis pipeline that includes an automated artifact detection step, and we test the hypothesis that spectral power density estimates computed with this pipeline are comparable to those computed with a commercial method preceded by visual artifact detection by a sleep expert (standard approach).
Methods
EEG data were analyzed from the C3-A2 lead in a sample of polysomnograms from 161 older women participants in a community-based cohort study. We calculated the sensitivity, specificity, accuracy, and Cohen's kappa measures from epoch-by-epoch comparisons of automated to visual-based artifact detection results; then we computed the average EEG spectral power densities in six commonly used EEG frequency bands and compared results from the two methods using correlation analysis and Bland–Altman plots.
Results
Assessment of automated artifact detection showed high specificity [96.8%–99.4% in non-rapid eye movement (NREM), 96.9%–99.1% in rapid eye movement (REM) sleep] but low sensitivity (26.7%–38.1% in NREM, 9.1–27.4% in REM sleep). However, large artifacts (total power > 99th percentile) were removed with sensitivity up to 87.7% in NREM and 90.9% in REM, with specificities of 96.9% and 96.6%, respectively. Mean power densities computed with the two approaches for all EEG frequency bands showed very high correlation (≥0.99). The automated pipeline allowed for a 100-fold reduction in analysis time with regard to the standard approach.
Conclusion
Despite low sensitivity for artifact rejection, the automated pipeline generated results comparable to those obtained with a standard method that included manual artifact detection. Automated pipelines can enable practical analyses of recordings from thousands of individuals, allowing for use in genetics and epidemiological research requiring large samples.}
}
@article{TOUNSI2018212,
title = {A survey on technical threat intelligence in the age of sophisticated cyber attacks},
journal = {Computers & Security},
volume = {72},
pages = {212-233},
year = {2018},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167404817301839},
author = {Wiem Tounsi and Helmi Rais},
keywords = {Technical threat intelligence, Indicators of compromise (IOC), Malware, Trust, Reputation, Ontologies, Cyber crime, Preventative strategies, Risk analysis, Threat sharing},
abstract = {Today's cyber attacks require a new line of security defenses. The static approach of traditional security based on heuristic and signature does not match the dynamic nature of new generation of threats that are known to be evasive, resilient and complex. Organizations need to gather and share real-time cyber threat information and to transform it to threat intelligence in order to prevent attacks or at least execute timely disaster recovery. Threat Intelligence (TI) means evidence-based knowledge representing threats that can inform decisions. There is a general awareness for the need of threat intelligence while vendors today are rushing to provide a diverse array of threat intelligence products, specifically focusing on Technical Threat Intelligence (TTI). Although threat intelligence is being increasingly adopted, there is little consensus on what it actually is, or how to use it. Without any real understanding of this need, organizations risk investing large amounts of time and money without solving existing security problems. Our paper aims to classify and make distinction among existing threat intelligence types. We focus particularly on the TTI issues, emerging researches, trends and standards. Our paper also explains why there is a reluctance among organizations to share threat intelligence. We provide sharing strategies based on trust and anonymity, so participating organizations can do away with the risks of business leak. We also show in this paper why having a standardized representation of threat information can improve the quality of TTI, thus providing better automated analytics solutions on large volumes of TTI which are often non-uniform and redundant. Finally, we evaluate most popular open source/free threat intelligence tools, and compare their features with those of a new AlliaCERT TI tool.}
}
@article{DAI2018447,
title = {Geo-QTI: A quality aware truthful incentive mechanism for cyber–physical enabled Geographic crowdsensing},
journal = {Future Generation Computer Systems},
volume = {79},
pages = {447-459},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.04.033},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17307070},
author = {Wei Dai and Yufeng Wang and Qun Jin and Jianhua Ma},
keywords = {Cyber–Physical world, Mobile crowdsensing (MCS), Quality aware, Incentive mechanism},
abstract = {Nowadays, the cyber, social and physical worlds are increasingly integrating and merging. Especially, combining the strengths of humans and machines helps tackle increasing hard tasks that neither can be done alone. Following this trend, this paper designs a Quality aware Truthful Incentive mechanism for cyber–physical enabled Geographic crowdsensing called Geo-QTI. Different from existing work, Geo-QTI appropriately accommodates the utilities of various stakeholders: requesters, participants and the crowdsourcing platform, and explicitly takes the requesters’ quality requirements, and participants’ quality provision into account. Geo-QTI explicitly includes four components: requester selection, participant selection, pricing and allocation. Requester selection with feasible analysis removes the requesters whose job cannot be completed by all participants or suffers from the monopoly participant (without the participant’s contribution, others cannot cover requesters’ requirement), obtains winning requesters set and determines actual payments. In participant selection phase, the platform aggregates the requested tasks (submitted by all winning requesters) in the sensed geographic area, and chooses the appropriate participants satisfying the winning requesters’ quality requirements with total cost as low as possible. Pricing phase determines the payments to winning participants. The phase of allocation assigns the specific participants to minimally cover the quality requirements of those winning requesters. Rigid theoretical analysis demonstrates Geo-QTI can achieve both requesters’ and participants’ individual rationality and truthfulness, computational efficiency and budget balance for the platform. Furthermore, the extensive simulations confirm our theoretical analysis, and illustrate that Geo-QTI can reduce requesters’ expenses greatly and ensure the fairness of allocation.}
}
@article{KOPCZYNSKI2017808,
title = {Multi-OMICS: a critical technical perspective on integrative lipidomics approaches},
journal = {Biochimica et Biophysica Acta (BBA) - Molecular and Cell Biology of Lipids},
volume = {1862},
number = {8},
pages = {808-811},
year = {2017},
note = {BBALIP_Lipidomics Opinion Articles},
issn = {1388-1981},
doi = {https://doi.org/10.1016/j.bbalip.2017.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1388198117300276},
author = {Dominik Kopczynski and Cristina Coman and Rene P. Zahedi and Kristina Lorenz and Albert Sickmann and Robert Ahrends},
abstract = {During the past decades, high-throughput approaches for analyzing different molecular classes such as nucleic acids, proteins, metabolites, and lipids have grown rapidly. These approaches became powerful tools for getting a fundamental understanding of biological systems. Considering each approach and its results separately, relations and causal connections between these classes have no chance to be revealed, since only separate molecular snapshots are provided. Only a combined approach, not fully established yet, with the integration of the corresponding data, might yield a comprehensive and complete understanding of biological processes, such as crosstalk and interactions in signaling pathways. Taking two or more omics-methods into consideration for analysis is referred to as a multi-omics approach, which is gradually evolving. In this critical note, we briefly discuss the relevance, challenges, current state, and potential of data integration from multi-omics approaches, with a special focus on lipidomics analysis, listing the advantages and gaps in this field. This article is part of a Special Issue entitled: BBALIP_Lipidomics Opinion Articles edited by Sepp Kohlwein.}
}
@incollection{2018309,
title = {Index},
editor = {Lipika Deka and Mashrur Chowdhury},
booktitle = {Transportation Cyber-Physical Systems},
publisher = {Elsevier},
pages = {309-329},
year = {2018},
isbn = {978-0-12-814295-0},
doi = {https://doi.org/10.1016/B978-0-12-814295-0.18001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128142950180013}
}
@article{KARCHER20181142,
title = {Sensor-driven Analysis of Manual Assembly Systems},
journal = {Procedia CIRP},
volume = {72},
pages = {1142-1147},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.241},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304116},
author = {Susann Kärcher and Emir Cuk and Timo Denner and David Görzig and Lisa C. Günther and Anna Hansmersmann and Günther Riexinger and Thomas Bauernhansl},
keywords = {assembly, sensor, analysis, manual, modular, evaluation},
abstract = {A reliable knowledge of processing times and following analysis are the basis for successful production planning. In particular, manual assembly processes are associated with great efforts to get data about processing times. That is why manual assembly processes are often not sufficiently transparent to enable good production planning, scheduling and control. This paper describes a system for automatic analysis and evaluation of manual assembly processes. It is driven by sensor and measurement technology, constructed modularly, and can be used in a wide range of assembly processes.}
}
@article{YANG2018A1,
title = {Editorial: Submission of Data Article is now open},
journal = {Solar Energy},
volume = {171},
pages = {A1-A2},
year = {2018},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2018.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X18306698},
author = {Dazhi Yang and Christian A. Gueymard and Jan Kleissl}
}
@article{WANG2018536,
title = {Understanding the dynamic mechanism of interagency government data sharing},
journal = {Government Information Quarterly},
volume = {35},
number = {4},
pages = {536-546},
year = {2018},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18300728},
author = {Fang Wang},
keywords = {Interagency government data sharing, IGDS, Acting forces, Dynamic mechanism, Force-field theory of change},
abstract = {Interagency government data sharing plays an important role in promoting the coordination of government departments and improving public services. Under the guidance of a theoretical framework that combines the force-field theory of change and the theory of mechanism, this study conducted a case study on two Chinese urban governments and built a dynamic mechanism model for IGDS. The model consists of six forces acting on IGDS, as well as their activities, effects and interactions. Some effects of them are context-dependent. This model can be used to explain the reasons of various barriers to IGDS and thus to guide government departments and policy makers design more specific and targeted dynamic mechanisms to promote IGDS. Finally, several mechanisms were discussed in the context of policy making.}
}
@article{VALLURUPALLI201872,
title = {Business intelligence for performance measurement: A case based analysis},
journal = {Decision Support Systems},
volume = {111},
pages = {72-85},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300848},
author = {Vamsi Vallurupalli and Indranil Bose},
keywords = {Business intelligence, Case study, Critical success factors, Framework, Key performance indicators, Manufacturing firm, Performance measurement systems},
abstract = {The adoption of IT-based performance measurement systems (PMS) has increased in recent times. The proliferation of business intelligence (BI) has significantly impacted performance measurement in organizations. In this paper a novel process-based framework is proposed to enable end-to-end analysis of technology driven PMS implementation in an organization. The framework has been used to study PMS implementation in a large manufacturing firm in India. The analysis of the case provides key lessons about successful planning, execution and adoption of a BI based PMS as well as identification of critical success factors (CSF) in the implementation of PMS, that would be of interest to organizations planning to implement a similar system.}
}
@article{SUAREZCETRULO201767,
title = {An online classification algorithm for large scale data streams: iGNGSVM},
journal = {Neurocomputing},
volume = {262},
pages = {67-76},
year = {2017},
note = {Online Real-Time Learning Strategies for Data Streams},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.12.093},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217309852},
author = {Andrés L. Suárez-Cetrulo and Alejandro Cervantes},
keywords = {Data classification, Topology extraction, Online learning, Large datasets, Growing Neural Gas, Support Vector Machines},
abstract = {Stream Processing has recently become one of the current commercial trends to face huge amounts of data. However, normally these techniques need specific infrastructures and high resources in terms of memory and computing nodes. This paper shows how mini-batch techniques and topology extraction methods can help making gigabytes of data to be manageable for just one server using computationally costly Machine Learning techniques as Support Vector Machines. The algorithm iGNGSVM is proposed to improve the performance of Support Vector Machines in datasets where the data is continuously arriving. It is benchmarked against a mini-batch version of LibSVM, achieving good accuracy rates and performing faster than this.}
}
@incollection{TIRADORAMOS2019154,
title = {Models for Computable Phenotyping},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {154-159},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20419-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204196},
author = {Alfredo Tirado-Ramos and Laura Manuel},
keywords = {Clinical trials, Computable phenotype, Data warehousing, Electronic medical record, EPIC, i2b2, Patient cohort identification, Patient data, PCORI},
abstract = {A computable phenotype is a set of computer-readable inclusion/exclusion criteria for a patient cohort. Such criteria should be specific and objective enough in order to turn them into a machine-readable query, yet also generalized enough to make them portable between different data sources; verbal descriptions are not computable phenotypes, no matter how readable they are to physicians, nor are a set of proprietary codes for a specific electronic health record system. This article describes the state of the art in computable phenotype models in the context of electronic health records, and illustrates their use with a use case in heart disease.}
}
@article{LARSEN201755,
title = {Identifying power elites—k-cores in heterogeneous affiliation networks},
journal = {Social Networks},
volume = {50},
pages = {55-69},
year = {2017},
issn = {0378-8733},
doi = {https://doi.org/10.1016/j.socnet.2017.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0378873316303811},
author = {Anton Grau Larsen and Christoph Houman Ellersgaard},
abstract = {Specifying network boundaries is fundamental in the study of social structures of elite networks. However, traditional methods do not offer clear criteria on either size or composition of the elite, and rely on numerous ad hoc decisions. A methodological framework that is inductive, reproducible and suitable for comparative research is proposed. First, a comprehensive dataset of the 5079 affiliation networks of all potentially powerful sectors in Denmark was assembled. Second, these heterogeneous affiliation networks were weighted to account for potential level of social integration. Third, a weighted modification of k-cores is used to identify a power elite of 423 individuals.}
}
@article{SIOUTAS2018425,
title = {Introduction to the special section on New Trends in Humanistic Informatics: Implementations and Applications},
journal = {Computers & Electrical Engineering},
volume = {65},
pages = {425-427},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618300363},
author = {Spyros Sioutas and Yannis Velegrakis and Valia Kordoni}
}
@article{2017IFC,
title = {Editorial Board},
journal = {Information Systems},
volume = {66},
pages = {IFC},
year = {2017},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(17)30205-3},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917302053}
}
@incollection{2018441,
title = {Index},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {441-461},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-804660-9.18001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046609180015}
}
@article{BADII201714,
title = {Analysis and assessment of a knowledge based smart city architecture providing service APIs},
journal = {Future Generation Computer Systems},
volume = {75},
pages = {14-29},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17302273},
author = {C. Badii and P. Bellini and D. Cenni and A. Difino and P. Nesi and M. Paolucci},
keywords = {Smart city, Smart city ontology, Smart city API, Smart mobility, Multi-domain smart city, Smart services},
abstract = {The main technical issues regarding smart city solutions are related to data gathering, aggregation, reasoning, data analytics, access, and service delivering via Smart City APIs (Application Program Interfaces). Different kinds of Smart City APIs enable smart city services and applications, while their effectiveness depends on the architectural solutions to pass from data to services for city users and operators, exploiting data analytics, and presenting services via APIs. Therefore, there is a strong activity on defining smart city architectures to cope with this complexity, putting in place a significant range of different kinds of services and processes. In this paper, the work performed in the context of Sii-Mobility smart city project on defining a smart city architecture addressing a wide range of processes and data is presented. To this end, comparisons of the state of the art solutions of smart city architectures for data aggregation and for Smart City API are presented by putting in evidence the usage semantic ontologies and knowledge base in the data aggregation in the production of smart services. The solution proposed aggregate and re-conciliate data (open and private, static and real time) by using reasoning/smart algorithms for enabling sophisticated service delivering via Smart City API. The work presented has been developed in the context of the Sii-Mobility national smart city project on mobility and transport integrated with smart city services with the aim of reaching a more sustainable mobility and transport systems. Sii-Mobility is grounded on Km4City ontology and tools for smart city data aggregation, analytics support and service production exploiting smart city API. To this end, Sii-Mobility/Km4City APIs have been compared to the state of the art solutions. Moreover, the proposed architecture has been assessed in terms of performance, computational and network costs in terms of measures that can be easily performed on private cloud on premise. The computational costs and workloads of the data ingestion and data analytics processes have been assessed to identify suitable measures to estimate needed resources. Finally, the API consumption related data in the recent period are presented.}
}
@incollection{FRYMAN201723,
title = {Chapter 2 - Executive Call to Action—How Chief Data Officers and Business Sponsors Can Empower Results},
editor = {Lowell Fryman and Gregory Lampshire and Dan Meers},
booktitle = {The Data and Analytics Playbook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {23-43},
year = {2017},
isbn = {978-0-12-802307-5},
doi = {https://doi.org/10.1016/B978-0-12-802307-5.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023075000022},
author = {Lowell Fryman and Gregory Lampshire and Dan Meers},
keywords = {Analytics, Big Data, Business sponsors, Data analysis, Data officers, Regulatory compliance},
abstract = {This chapter asks readers to define the scope of the data they wish to improve and create an actionable plan that can be passed on to data stewards and executives. This is done through three key areas. First, it requires a clear, crisp, and consistently communicated data leadership message with the vision, scope, and targeted outcomes of the journey (implicitly or explicitly including improved data capabilities and behaviors). Second, users must develop a data leadership team with a chief, where appropriate, and officers who are empowered to lead and execute on your vision. Finally, the user must define a set of measures, methods, and models or templates that demonstrate the commitment to ongoing execution and progress tracking and rewards.}
}
@article{RUMMENS2017255,
title = {The use of predictive analysis in spatiotemporal crime forecasting: Building and testing a model in an urban context},
journal = {Applied Geography},
volume = {86},
pages = {255-261},
year = {2017},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2017.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0143622816304957},
author = {Anneleen Rummens and Wim Hardyns and Lieven Pauwels},
keywords = {Predictive analysis, Predictive policing, Crime forecasting, Spatiotemporal modeling, Crime mapping},
abstract = {Police databases hold a large amount of crime data that could be used to inform us about current and future crime trends and patterns. Predictive analysis aims to optimize the use of these data to anticipate criminal events. It utilizes specific statistical methods to predict the likelihood of new crime events at small spatiotemporal units of analysis. The aim of this study is to investigate the potential of applying predictive analysis in an urban context. To this end, the available crime data for three types of crime (home burglary, street robbery, and battery) are spatially aggregated to grids of 200 by 200 m and retrospectively analyzed. An ensemble model is applied, synthesizing the results of a logistic regression and neural network model, resulting in bi-weekly predictions for 2014, based on crime data from the previous three years. Temporally disaggregated (day versus night predictions) monthly predictions are also made. The quality of the predictions is evaluated based on the following criteria: direct hit rate (proportion of incidents correctly predicted), precision (proportion of correct predictions versus the total number of predictions), and prediction index (ratio of direct hit rate versus proportion of total area predicted as high risk). Results indicate that it is possible to attain functional predictions by applying predictive analysis to grid-level crime data. The monthly predictions with a distinction between day and night produce better results overall than the bi-weekly predictions, indicating that the temporal resolution can have an important impact on the prediction performance.}
}
@article{GANDHI2018261,
title = {Towards data mining based decision support in manufacturing maintenance},
journal = {Procedia CIRP},
volume = {72},
pages = {261-265},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.076},
url = {https://www.sciencedirect.com/science/article/pii/S221282711830180X},
author = {Kanika Gandhi and Bernard Schmidt and Amos H.C. Ng},
keywords = {Maintenance, Decision Support System, Data Mining, Classification Methods, Knowledge Extraction},
abstract = {The current work presents a decision support system architecture for evaluating the features representing the health status to predict maintenance actions and remaning useful life of component. The evaluation is possible through pattern analysis of past and current measurements of the focused research components. Data mining visualization tools help in creating the most suitable patterns and learning insights from them. Estimations like features split values or measurement frequency of the component is achieved through classification methods in data mining. This paper presents how the quantitative results generated from data mining can be used to support decision making of domain experts.}
}
@article{REN201823,
title = {Minimum-cost mobile crowdsourcing with QoS guarantee using matrix completion technique},
journal = {Pervasive and Mobile Computing},
volume = {49},
pages = {23-44},
year = {2018},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2018.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S1574119218302013},
author = {Yingying Ren and Yuxin Liu and Ning Zhang and Anfeng Liu and Neal N. Xiong and Zhiping Cai},
keywords = {Mobile crowdsourcing, Matrix completion, Participant selection, Minimum cost, Quality of service guarantee},
abstract = {Mobile crowdsourcing is a promising solution for data collection, whereby a crowd of participants are recruited and paid for participation in data collection. To minimize the total crowdsourcing cost while guaranteeing the quality of service (QoS) of the tasks, this paper proposes a novel Matrix Completion Technique based Data Collection (MCTDC) scheme. Specifically, we explore the multi-dimensional correlation of data to reduce the data amount required while guaranteeing the QoS, by means of Matrix Completion Technique. Furthermore, to select the minimum set of appropriate participants, we redefine the contribution degree as the ratio of the valid data from a given participant and the total amount data it collects. The participants with high contribution degree are recruited to sense and report data. By doing so, the system can satisfy the demand of application quickly with less participants and less data amount, namely, with minimum cost and QoS guarantee. Extensive simulation results are provided, which demonstrates the proposed MFTDC scheme can significantly reduce the data redundancy and the number of participants.}
}
@article{2017iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {104},
pages = {iii-v},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(17)30194-1},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917301941}
}
@article{2017IFC,
title = {Editorial Board},
journal = {Information Systems},
volume = {68},
pages = {IFC},
year = {2017},
note = {Special issue on DOLAP 2015: Evolving data warehousing and OLAP cubes to big data analytics},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(17)30284-3},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917302843}
}
@article{GENON2017144,
title = {Searching for behavior relating to grey matter volume in a-priori defined right dorsal premotor regions: Lessons learned},
journal = {NeuroImage},
volume = {157},
pages = {144-156},
year = {2017},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917304524},
author = {Sarah Genon and Tobias Wensing and Andrew Reid and Felix Hoffstaedter and Svenja Caspers and Christian Grefkes and Thomas Nickl-Jockschat and Simon B. Eickhoff},
keywords = {Structural brain behavior, Functional characterization, Voxel-based morphometry, Replication, Type S error},
abstract = {Recently, we showed that the functional heterogeneity of the right dorsal premotor (PMd) cortex could be better understood by dividing it into five subregions that showed different behavioral associations according to task-based activations studies. The present study investigated whether the revealed behavioral profile could be corroborated and complemented by a structural brain behavior correlation approach in two healthy adults cohorts. Grey matter volume within the five volumes of interest (VOI-GM) was computed using voxel-based morphometry. Associations between the inter-individual differences in VOI-GM and performance across a range of neuropsychological tests were assessed in the two cohorts with and without correction for demographical variables. Additional analyses were performed in random smaller subsamples drawn from each of the two cohorts. In both cohorts, correlation coefficients were low; only few were significant and a considerable number of correlations were counterintuitive in their directions (i.e., higher performance related to lower grey matter volume). Furthermore, correlation patterns were inconsistent between the two cohorts. Subsampling revealed that correlation patterns could vary widely across small samples and that negative correlations were as likely as positive correlations. Thus, the structural brain-behavior approach did not corroborate the functional profiles of the PMd subregions inferred from activation studies, suggesting that local recruitment by fMRI studies does not necessarily imply covariance of local structure with behavioral performance in healthy adults. We discuss the limitations of such studies and related recommendations for future studies.}
}
@article{XU20183,
title = {Mobile crowd sensing of human-like intelligence using social sensors: A survey},
journal = {Neurocomputing},
volume = {279},
pages = {3-10},
year = {2018},
note = {Advances in Human-like Intelligence towards Next-Generation Web},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.01.127},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217317769},
author = {Zheng Xu and Lin Mei and Kim-Kwang Raymond Choo and Zhihan Lv and Chuanping Hu and Xiangfeng Luo and Yunhuai Liu},
keywords = {Mobile crowd sensing, Social sensors, Social sensing},
abstract = {Recently, with the fast proliferation of smart phones, mobile phone has the powerful ability of not only communication but also computation. Human beings are not only data consumers, but data producer with their objective or subjective sensing needs. Mobile crowd sensing is an emerging computing paradigm that tasks everyday mobile devices to form participatory sensor networks. It allows the increasing number of mobile phone users to share local knowledge acquired by their sensor-enhanced devices. Social sensors, social sensor receiver platform, and mobile crowd sensing paradigm compose a process by which physical sensors present in mobile devices such as GPS are used to infer social relationships and human activities. In this survey, we review the mobile crowd sensing applications on social sensors based on social sensor receiver platform (e.g., Weibo and Twitter) from three categories: public security, smart city, and location based services. Most applications adopted in current works fit in one of these categories. Existing works on applications of mobile crowd sensing on social sensors are collected and studied. Some possible future directions of potential new application category are proposed and analyzed.}
}
@article{WANG2018263,
title = {Privacy Preservation for Dating Applications},
journal = {Procedia Computer Science},
volume = {129},
pages = {263-269},
year = {2018},
note = {2017 INTERNATIONAL CONFERENCE ON IDENTIFICATION,INFORMATION AND KNOWLEDGEIN THE INTERNET OF THINGS},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.03.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918302990},
author = {Weicheng Wang and Shengling Wang},
keywords = {privacy preservation, social applications, zero knowledge, homomorphic encryption algorithm, service accuracy},
abstract = {Dating applications can satisfy the social contact needs of different users and become tools for developing a social relationship. However, the privacy leakage has turned into an insurmountable obstacle to the market success of social applications. Existing privacy protection schemes for social applications either introduce untrusted third parties or sacrifice information accuracy. In this paper, we put forward the privacy protection mechanism based on zero knowledge. In detail, the server knows nothing about the users information, but can still provide accurate services to users. We also analyze the potential attack methods and propose the corresponding solutions. Our simulation results verify the effectivity of our scheme.}
}
@article{BRAUN20172259,
title = {Game Data Mining: Clustering and Visualization of Online Game Data in Cyber-Physical Worlds},
journal = {Procedia Computer Science},
volume = {112},
pages = {2259-2268},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.141},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917314989},
author = {Peter Braun and Alfredo Cuzzocrea and Timothy D. Keding and Carson K. Leung and Adam G.M. Padzor and Dell Sayson},
keywords = {Data mining, clustering, visual analytics, cluster visualization, cyber-physical world, online game, applications, innovative artificial intelligence technologies},
abstract = {Since its debut in May 2016, Overwatch has quickly become a popular team-based online video game. Despite the popularity of Overwatch, many new players—who join the game unsure how to compete with the game’s veterans—feel overwhelmed with the vast knowledge required to properly play at higher skill levels. In this paper, a data mining algorithm is designed and developed for clustering and visualization of online game data at the cyber-physical world boundary. Scientifically, the algorithm uses affinity propagation for clustering and two-dimensional graphs for visualizing online game data. The algorithm analyzes the Overwatch game data for the discovery of new knowledge about current players and the clustering of data for each hero character. This knowledge enables the analysis of individual clusters and provides statistics that have a high correlation with winning player strategies. These statistics are expected to have a large influence on how a character is played, and thus can aid new players in learning their priorities as each hero character. In other words, the algorithm helps analyze the online game playing data, get insight about the grouping or clusters of players, and offer suggestions to new players of the game.}
}
@incollection{FRYMAN201745,
title = {Chapter 3 - Assessing Conditions, Controls and Capabilities},
editor = {Lowell Fryman and Gregory Lampshire and Dan Meers},
booktitle = {The Data and Analytics Playbook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {45-63},
year = {2017},
isbn = {978-0-12-802307-5},
doi = {https://doi.org/10.1016/B978-0-12-802307-5.00003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023075000034},
author = {Lowell Fryman and Gregory Lampshire and Dan Meers},
keywords = {Assessment, Audit, Data, Data capabilities, Data controls, Data management},
abstract = {This chapter takes readers through the process of assessing their company's data needs. It describes assessment as the first step in data management and introduces the 3Cs of data assessment: conditions, controls, and capabilities. Because of the varied needs each company has for data management, the chapter details various methods for data assessment. The chapter then stresses the importance of using the resulting information to form a coherent image of the data needs. This final image includes understanding the capability level of the data stewards, the needs of the data and the effect that data can ultimately have on the company. Finally, the chapter stresses the importance of adjusting the Playbook to fit the individual needs of those using it.}
}
@incollection{2017369,
title = {Index},
editor = {Tyson Macaulay},
booktitle = {RIoT Control},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {369-381},
year = {2017},
isbn = {978-0-12-419971-2},
doi = {https://doi.org/10.1016/B978-0-12-419971-2.00025-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012419971200025X}
}
@article{ZHOU20181683,
title = {Emissions and low-carbon development in Guangdong-Hong Kong-Macao Greater Bay Area cities and their surroundings},
journal = {Applied Energy},
volume = {228},
pages = {1683-1692},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.07.038},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918310675},
author = {Ya Zhou and Yuli Shan and Guosheng Liu and Dabo Guan},
keywords = {Guangdong-Hong Kong-Macao Greater Bay Area, Belt and Road Initiative, CO emissions, Low-carbon development pathways, Urban agglomeration},
abstract = {Cities are the major contributors to energy consumption and CO2 emissions, as well as being leading innovators and implementers of policy measures in climate change mitigation. Guangdong-Hong Kong-Macao Greater Bay Area (GBA) is an agglomeration of cities put forward by China to strengthen international cooperation among “Belt and Road” countries and promote low-carbon, inclusive, coordinated and sustainable development. Few studies have discussed the emission characteristics of GBA cities. This study, for the first time, compiles emission inventories of 11 GBA cities and their surroundings based on IPCC territorial emission accounting approach, which are consistent and comparable with the national and provincial inventories. Results show that (a) total emissions increased from 426 Mt in 2000 to 610 Mt in 2016, while emissions of GBA cities increased rapidly by 6.9% over 2000–2011 and peaked in 2014 (334 Mt); (b) raw coal and diesel oil are the top two emitters by energy type, while energy production sector and tertiary industry are the top two largest sectors; (c) GBA cities take the lead in low-carbon development, emitted 4% of total national emissions and contributed 13% of national GDP with less than a third of national emission intensities and less than three-quarters of national per capita emissions; (d) Macao, Shenzhen and Hong Kong have the top three lowest emission intensity in the country; (e) most of GBA cities are experiencing the shift from an industrial economy to a service economy, while Hong Kong, Shenzhen, Foshan and Huizhou reached their peak emissions and Guangzhou, Dongguan and Jiangmen remained decreasing emission tendencies; (g) for those coal-dominate or energy-production cities (i.e. Zhuhai, Zhongshan, Zhaoqing, Maoming, Yangjiang, Shanwei, Shaoguan and Zhanjiang) in mid-term industrialization, total emissions experienced soaring increases. The emission inventories provide robust, self-consistent, transparent and comparable data support for identifying spatial–temporal emission characteristics, developing low-carbon policies, monitoring mitigation progress in GBA cities as well as further emissions-related studies at a city-level. The low-carbon roadmaps designed for GBA cities and their surroundings also provide a benchmark for other developing countries/cities to adapting changing climate and achieve sustainable development.}
}
@incollection{LATIF202363,
title = {Wearable Cyberphysical Systems for Biomedicine},
editor = {Roger Narayan},
booktitle = {Encyclopedia of Sensors and Biosensors (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {63-85},
year = {2023},
isbn = {978-0-12-822549-3},
doi = {https://doi.org/10.1016/B978-0-12-822548-6.00124-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128225486001242},
author = {Tahmid Latif and James Dieffenderfer and Rafael Luiz {da Silva} and Edgar Lobaton and Alper Bozkurt},
keywords = {Asthma, Biomarkers, Biosensor, Cardiac health, Cyberphysical systems, Environmental sensors, Machine learning, Physiologic sensors, Pulmonary health, Respiratory conditions, Wearables},
abstract = {This chapter surveys the state-of-the-art related to the building blocks of wearable cyberphysical systems for health monitoring and highlights its potential to revolutionize healthcare, specifically chronic disease management. The common sensing modalities and their corresponding wearable form factors are summarized for the application areas of cardiovascular diseases, asthma and chronic obstructive pulmonary disease. The use of these measurements with estimation approaches using signal processing and machine learning techniques is also reviewed. The outcomes of these estimation tasks can be used to provide feedback internally to optimize device performance and externally to the users about their health situation. The chapter concludes with a discussion of deployment barriers for wearable cyberphysical systems in real life.}
}
@article{2017IFC,
title = {IFC EDBD/Aims and Scope},
journal = {Information Systems},
volume = {64},
pages = {IFC},
year = {2017},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(16)30557-9},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916305579}
}
@article{WANG201779,
title = {Parallelizing maximal clique and k-plex enumeration over graph data},
journal = {Journal of Parallel and Distributed Computing},
volume = {106},
pages = {79-91},
year = {2017},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517300837},
author = {Zhuo Wang and Qun Chen and Boyi Hou and Bo Suo and Zhanhuai Li and Wei Pan and Zachary G. Ives},
keywords = {Maximal clique enumeration, Maximal k-plex enumeration, Parallel graph processing, MapReduce},
abstract = {In a wide variety of emerging data-intensive applications, such as social network analysis, Web document clustering, entity resolution, and detection of consistently co-expressed genes in systems biology, the detection of dense subgraphs cliques and k-plex is an essential component. Unfortunately, these problems are NP-Complete and thus computationally intensive at scale — hence there is a need to come up with techniques for distributing the computation across multiple machines such that the computation, which is too time-consuming on a single machine, can be efficiently performed on a machine cluster given that it is large enough. In this paper, we first propose a new approach for maximal clique and k-plex enumeration, which identifies dense subgraphs by binary graph partitioning. Given a connected graph G=(V,E), it has a space complexity of O(|E|) and a time complexity of O(|E|μ(G)), where μ(G) represents the number of different cliques (k-plexes) existing in G. It recursively divides a graph until each task is sufficiently small to be processed in parallel. We then develop parallel solutions and demonstrate how graph partitioning can enable effective load balancing. Finally, we evaluate the performance of the proposed approach on real and synthetic graph data and show that it performs considerably better than existing approaches in both centralized and parallel settings. In the parallel setting, it can achieve the speedups of up to 10x over existing approaches on large graphs. Our parallel algorithms are primarily implemented and evaluated on MapReduce, a popular shared-nothing parallel framework, but can easily generalize to other shared-nothing or shared-memory parallel frameworks. The work presented in this paper is an extension of our preliminary work on the approach of binary graph partitioning for maximal clique enumeration. In this work, we extend the proposed approach to handle maximal k-plex detection as well.}
}
@article{KLURFELD2018e117,
title = {Technology Innovations in Dietary Intake and Physical Activity Assessment: Challenges and Recommendations for Future Directions},
journal = {American Journal of Preventive Medicine},
volume = {55},
number = {4},
pages = {e117-e122},
year = {2018},
issn = {0749-3797},
doi = {https://doi.org/10.1016/j.amepre.2018.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0749379718320233},
author = {David M. Klurfeld and Eric B. Hekler and Camille Nebeker and Kevin Patrick and Chor San H. Khoo}
}
@article{DENG2017767,
title = {Management of trade-offs between cultivated land conversions and land productivity in Shandong Province},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {767-774},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.04.050},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616303183},
author = {Xiangzheng Deng and John Gibson and Pei Wang},
keywords = {Land productivity, Land use, Land conversion, Data fusion},
abstract = {This study aims to analyze the trade-offs between cultivated land conversions and land productivity using data fusion. First, 1-km area percentage data model, which integrates advantages of grid data and vector data, is applied to detect cultivated land conversion in each 1 km × 1 km grid cell in Shandong Province. Then land productivity in the study area is assessed with the Estimation System of Land Production (ESLP) model based on agro-ecological zones, which integrates multi-source data, including land use data, climatic data, radiation parameters, soil properties. Estimation result shows that the average land productivity of the whole study area is 7509 kg hm−2 during 1985–2010, while land productivity of built-up land and water areas with low vegetation is zero. Furthermore, results of comparative analysis on cultivated land conversion and land productivity shows that land productivity in Shandong Province is unevenly distributed, which is higher in the west part of the study area, and lower in the regions where cultivated land conversion occurs. And the overall trend of land productivity is in a decreasing trend during 2003–2010. The measures of management of this trade-off should be focused on preventing cultivated land conversion.}
}
@article{PIELMEIER20181345,
title = {Development of a methodology for event-based production control},
journal = {Procedia CIRP},
volume = {72},
pages = {1345-1350},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.170},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303287},
author = {Julia Pielmeier and Philipp Theumer and Stefan Braunreuther and Gunther Reinhart},
keywords = {Decision making, Optimization, Simulation},
abstract = {Nowadays, industrial production environments are complex, volatile and driven by uncertainties. Manufacturing companies are striving for flexibility and adaptability to cope with these challenges and remain competitive. Decision-making and real-time reaction systems are promising concepts to cope with complex situations in industrial production environments. However, digitalization leads to an increasing amount of data describing the status of products and resources within an industrial production environment. In order to achieve near real-time monitoring and control of production and logistics processes, intelligent processing and analysis of the acquired data is necessary. As a result of this development, continuous data analysis plays a decisive role for analyzing extensive data streams in real-time. Patterns have to be recognized during the production process and actions have to be defined in order to realize an event-based production control. The main focus within this paper is the development of a methodology for an event-based production control. Furthermore, an exemplary implementation of the methodology will be presented by means of a prototypical implementation to optimize the exchange of material.}
}
@article{MAKAREVITCH2017166,
title = {Killing two birds with one stone: Model plant systems as a tool to teach the fundamental concepts of gene expression while analyzing biological data},
journal = {Biochimica et Biophysica Acta (BBA) - Gene Regulatory Mechanisms},
volume = {1860},
number = {1},
pages = {166-173},
year = {2017},
note = {Plant Gene Regulatory Mechanisms and Networks},
issn = {1874-9399},
doi = {https://doi.org/10.1016/j.bbagrm.2016.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1874939916300803},
author = {Irina Makarevitch and Betsy Martinez-Vaz},
keywords = {Regulation of gene expression, Undergraduate classroom, Student research experiences},
abstract = {Plants are ideal systems to teach core biology concepts due to their unique physiological and developmental features. Advances in DNA sequencing technology and genomics have allowed scientists to generate genome sequences and transcriptomics data for numerous model plant species. This information is publicly available and presents a valuable tool to introduce undergraduate students to the fundamental concepts of gene expression in the context of modern quantitative biology and bioinformatics. Modern biology classrooms must provide authentic research experiences to allow developing core competencies such as scientific inquiry, critical interpretation of experimental results, and quantitative analyses of large dataset using computational approaches. Recent educational research has shown that undergraduate students struggle when connecting gene expression concepts to classic genetics, phenotypic analyses, and overall flow of biological information in living organisms, suggesting that novel approaches are necessary to enhance learning of gene expression and regulation. This review describes different strategies and resources available to instructors willing to incorporate authentic research experiences, genomic tools, and bioinformatics analyses when teaching transcriptional regulation and gene expression in undergraduate courses. A variety of laboratory exercises and pedagogy materials developed to teach gene expression using plants are discussed. This article is part of a Special Issue entitled: Plant Gene Regulatory Mechanisms and Networks, edited by Dr. Erich Grotewold and Dr. Nathan Springer.}
}