@article{WANG2022123178,
title = {Lithium-ion battery state-of-charge estimation for small target sample sets using the improved GRU-based transfer learning},
journal = {Energy},
volume = {244},
pages = {123178},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123178},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222000810},
author = {Ya-Xiong Wang and Zhenhang Chen and Wei Zhang},
keywords = {State-of-charge (SOC), Open-source battery datasets, Deep learning, Gated recurrent unit (GRU), Source domain model, Transfer learning},
abstract = {Accurate estimation of the state-of-charge (SOC) of lithium-ion batteries is a key technique for automotive battery management systems to overcome the non-linearity and complications of practical applications. The data-driven approach for estimating SOC requires a large number of training samples and costly input. To this end, an improved gated recurrent unit (GRU)-based transfer learning SOC estimation is proposed for small target sample sets. To ensure the completeness and consistency of data features, Lagrangian interpolations and standard normalization are used for analyzing the open-source battery datasets. The source domain GRU model is pre-trained to obtain rich battery characteristics with the preprocessed datasets; the GRU hidden unit structure can be enhanced, and it is advantageously used in conjunction with transfer learning. Moreover, weight parameters of the source domain are transferred to the GRU model of target batteries. The experimental results show that the proposed improved GRU-based transfer learning can use small target samples to achieve fast and accurate SOC estimations by ordinary computing hardware. In particular, the RMSEs are 1.115%, 1.867%, and 1.141% under dynamic conditions, 32 °C-FUDS, 36 °C-US06, and 50 °C-UDDS, respectively. The proposed method demonstrates the potential of SOC estimation using small target samples-based big data techniques in practice.}
}
@article{DERARDJA2022107136,
title = {A deep learning model for mapping the perturbation in pressurised irrigation systems},
journal = {Computers and Electronics in Agriculture},
volume = {199},
pages = {107136},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107136},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922004537},
author = {Bilal Derardja and Umberto Fratino and Nicola Lamaddalena and R. {González Perea} and J.A. {Rodríguez Díaz}},
keywords = {Pressurized irrigation systems, Unsteady state flow, Perturbation, On-demand irrigation networks, Deep neural networks},
abstract = {Nowadays, the management of pressurised irrigation networks requires plenty of information to provide an efficient and reliable service to farmers. Perturbation is the propagation of pressure waves through the networks pipes which could expose the network to a serious risk that could cause components damaging. Several computational codes were developed to simulate such phenomenon. The most recent ones are efficient enough to provide a good image of the perturbation occurrence through different indicators, but they are time and computationally expensive. For real time decision making and more flexible management, there is a need for faster models to be developed. In this study the directly programmed models were used as big data generators to train a developed deep learning model. This approach was applied on a pressurised on-demand irrigation system located in south of Italy that consists of 19 hydrants (service outlets) and covers 57 ha. Two thousand configurations (operational scenarios) were simulated using a predeveloped directly programmed model and fed to train a deep learning model with the objective of forecasting the maximum pressure occurred due the perturbation at each section. The occurred pressure is represented as classes according to the case sensitivity and the required precision. In the present work, scenarios for 1, 2 and 3 bars steps were simulated. The model proved to be significantly time saving compared to previous approaches as the results are produced instantaneously with a forecasting accuracy of 85 %. Furthermore, from the called confusion matrix, the error committed by the model is of one class lower or higher that may be considered tolerable according to the system sensitivity. Thus, modelling the perturbation in the on-demand pressurised irrigation networks would add a significant contribution to provide practical recommendations for real-time decision-making processes.}
}
@article{MORKNER2022104945,
title = {Distilling data to drive carbon storage insights},
journal = {Computers & Geosciences},
volume = {158},
pages = {104945},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104945},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421002314},
author = {Paige Morkner and Jennifer Bauer and C. Gabriel Creason and Michael Sabbatino and Patrick Wingo and Randall Greenburg and Samuel Walker and David Yeates and Kelly Rose},
keywords = {Geologic carbon sequestration, Energy data exchange, Natural language processing, FAIR (Findable, Accessible, Interoperable, Reusable), Data availability, Spatial data density analysis},
abstract = {Wide-spread implementation of carbon capture and storage has the potential to decrease carbon emissions and aid in meeting global climate change mitigation goals. Data availability is one of the biggest challenges faced by the carbon capture and storage (CCS) community for modeling risks associated with CCS, necessary for wide-spread implementation in coming years. Collecting, integrating, and intuitively managing data is a time-consuming process, but one which is fundamental to establishing necessary access to carbon storage data. The US Department of Energy (US DOE) has been a major supporter of energy research in the US, including significant investment into carbon capture and storage research and technology development over the last ten years. The US DOE investments into the Regional Carbon Sequestration Partnerships, the National Risk Assessment Partnership, and other CCS related research has resulted in a large volume of data, of which much has been made public through the National Energy Technology Laboratories data repository, the Energy Data eXchange (EDX). Researchers at the National Energy Technology Laboratory have developed workflows, tools, and other methods that leverage EDX, open-source software, machine learning, and natural language processing to discover, curate, label, organize and visualize available data. This paper describes the available data on EDX for carbon storage applications, describes the results of a spatial and temporal analysis of the data, describes where it is most geographically available, makes a general assessment of the quality of the available data, and discusses visualization tools and natural language processing tools developed for understanding, discovering and reusing the data.}
}
@article{ZACHLOD20221064,
title = {Analytics of social media data – State of characteristics and application},
journal = {Journal of Business Research},
volume = {144},
pages = {1064-1076},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322001321},
author = {Cécile Zachlod and Olga Samuel and Andrea Ochsner and Sarah Werthmüller},
keywords = {Social media analytics, Social media analysis, Social media data, Social media monitoring, Social media listening, Literature review},
abstract = {The spread and use of social networks provide a rich data source that can be used to answer a wide range of research questions from various disciplines. However, the nature of social media data poses a challenge to the analysis. The aim of this study is to provide an in-depth overview of the research that analyzes social media data since 2017. An extensive literature review based on 94 papers led to the findings that clear definitions are neither established nor commonly applied. Predominant research domains include marketing, hospitality and tourism, disaster management, and disruptive technology. The majority of analyzed social media data are taken from Twitter. Sentiment and content analysis are the current prevailing methods. Half of the studies include practical implications. Based on the literature review, clear definitions are provided, and future avenues for high-quality research are suggested.}
}
@article{CHEN2022102474,
title = {EVFL: An explainable vertical federated learning for data-oriented Artificial Intelligence systems},
journal = {Journal of Systems Architecture},
volume = {126},
pages = {102474},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102474},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122000583},
author = {Peng Chen and Xin Du and Zhihui Lu and Jie Wu and Patrick C.K. Hung},
keywords = {Data-oriented AI, Federated counterfactual explanation, Feature importance, Vertical federated learning, Data cleansing},
abstract = {Vertical federated learning (VFL), as one of the latest advances of security in the data-oriented Artificial Intelligence (AI) systems, facilitates better keeping the AI systems converge faster with higher performance and security. Since a large amount of data from these systems is often of low quality, the training data needs to be interpreted and evaluated. While there have been some research efforts, they still have significant shortcomings, such as high computational complexity and impracticality. Considering the characteristics of the data, the interpretation of machine learning models allows for data cleansing, which can improve data quality and help regulators understand the decision-making process. In this paper, we propose an explainable vertical federated learning (EVFL) framework, including the credibility assessment strategy, the federated counterfactual explanation and the importance rate (IR) metric. Furthermore, we initialize the knowledge-based counterfactual instance based on prior knowledge and retrain the federated counterfactual method for feasible counterfactual features. We report experimental results obtained on the Lending Club and Zhongyuan datasets for implementing our framework to show that our approach is significantly effective. Notably, on the Lending Club dataset, our method can have a +4.9% improvement over other selections.}
}
@article{PRODHAN2022105327,
title = {A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions},
journal = {Environmental Modelling & Software},
volume = {149},
pages = {105327},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105327},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222000330},
author = {Foyez Ahmed Prodhan and Jiahua Zhang and Shaikh Shamim Hasan and Til Prasad {Pangali Sharma} and Hasiba Pervin Mohana},
keywords = {Machine learning, Deep learning, Forecasting, Drought, Big data},
abstract = {Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.}
}
@article{XU2022105396,
title = {An overview of visualization and visual analytics applications in water resources management},
journal = {Environmental Modelling & Software},
volume = {153},
pages = {105396},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105396},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001025},
author = {Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal},
keywords = {Big Data, Hydroinformatics, Visual Analytics, Visualization, Human-Computer Interaction},
abstract = {Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for water resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of hydroinformatics applications.}
}
@article{CAPPIELLO2022101874,
title = {Assessing and improving measurability of process performance indicators based on quality of logs},
journal = {Information Systems},
volume = {103},
pages = {101874},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101874},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000995},
author = {Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim},
keywords = {Business process, Event log, Data quality assessment, Data quality improvement},
abstract = {The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.}
}
@article{MOZZONI2022402,
title = {Transfer’s monitoring in bus transit services by Automatic Vehicle Location data},
journal = {Transportation Research Procedia},
volume = {60},
pages = {402-409},
year = {2022},
note = {New scenarios for safe mobility in urban areasProceedings of the XXV International Conference Living and Walking in Cities (LWC 2021), September 9-10, 2021, Brescia, Italy},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.12.052},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521009534},
author = {Sara Mozzoni and Massimo Di Francesco and Giulio Maternini and Benedetto Barabino},
keywords = {Big Data, Transfer diagnosis, Automatic Vehicle Location Data},
abstract = {Since transfers increase the connectivity of routes, they improve the characteristics of transit networks. Designing and managing transfers are well-investigated issues arising at the tactical and operational level. Conversely, the monitoring phase was rarely faced to verify the consistency between well planned and/or delivered transfers. In this paper, we tailor an innovative methodology for measuring the rate of transfers between two routes by using archived Automatic Vehicle Location (AVL) data. This measurement is performed spatially, at shared and unshared (but reasonably quite close) bus stops, and temporally at each time period. The results are represented by easy-to-read control dashboards. This methodology is tested by about 240,000 AVL real records provided by the local bus operator of Cagliari (Italy) and provides valuable insights into the characterization of transfers.}
}
@article{PHOON2022967,
title = {Unpacking data-centric geotechnics},
journal = {Underground Space},
volume = {7},
number = {6},
pages = {967-989},
year = {2022},
issn = {2467-9674},
doi = {https://doi.org/10.1016/j.undsp.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2467967422000514},
author = {Kok-Kwang Phoon and Jianye Ching and Zijun Cao},
keywords = {Data-centric geotechnics, Bayesian machine learning, Data-driven site characterization (DDSC), Project DeepGeo, Data-informed decision support index},
abstract = {The purpose of this paper (presented online as a keynote lecture at the 25th Annual Indonesian Geotechnical Conference on 10 Nov 2021) is to broadly conceptualize the agenda for data-centric geotechnics, an emerging field that attempts to prepare geotechnical engineering for digital transformation. The agenda must include (1) development of methods that make sense of all real-world data (not selective input data for a physical model), (2) offering insights of significant value to critical real-world decisions for current or future practice (not decisions for an ideal world or decisions of minor concern to geotechnical engineers), and (3) sensitivity to the physical context of geotechnics (not abstract data-driven analysis connected to geotechnics in a peripheral way, i.e., engagement with the knowledge and experience base should be substantial). These three elements are termed “data centricity”, “fit for (and transform) practice”, and “geotechnical context” in the agenda. Given that a knowledge of the site is central to any geotechnical engineering project, data-driven site characterization (DDSC) must constitute one key application domain in data-centric geotechnics, although other infrastructure lifecycle phases such as project conceptualization, design, construction, operation, and decommission/reuse would benefit from data-informed decision support as well. One part of DDSC that addresses numerical soil data in a site investigation report and soil property databases is pursued under Project DeepGeo. In principle, the source of data can also go beyond site investigation, and the type of data can go beyond numbers, such as categorical data, text, audios, images, videos, and expert opinion. The purpose of Project DeepGeo is to produce a 3D stratigraphic map of the subsurface volume below a full-scale project site and to estimate relevant engineering properties at each spatial point based on actual site investigation data and other relevant Big Indirect Data (BID). Uncertainty quantification is necessary, as current real-world data is insufficient, incomplete, and/or not directly relevant to construct a deterministic map. The value of a deterministic map for decision support is debatable. The computational cost to do this for a 3D true scale subsurface volume must be reasonable. Ultimately, geotechnical structures need to be a part of a completely smart infrastructure that fits the circular economy and need to focus on delivering service to end-users and the community from project conceptualization to decommission/reuse with full integration to smart city and smart society. Although current geotechnical practice has been very successful in taking “calculated risk” informed by limited data, imperfect theories, prototype testing, observations, among others and exercising judicious caution and engineering judgment, there is no clear pathway forward to leverage on big data and digital technologies such as machine learning, BIM, and digital twin to meet more challenging needs such as sustainability and resilience engineering.}
}
@article{LIU2022102936,
title = {A review of spatially-explicit GeoAI applications in Urban Geography},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102936},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102936},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001339},
author = {Pengyuan Liu and Filip Biljecki},
keywords = {Urban studies, Deep learning, Socio-economics, Location encoder, Graph neural network},
abstract = {Urban Geography studies forms, social fabrics, and economic structures of cities from a geographic perspective. Catalysed by the increasingly abundant spatial big data, Urban Geography seeks new models and research paradigms to explain urban phenomena and address urban issues. Recent years have witnessed significant advances in spatially-explicit geospatial artificial intelligence (GeoAI), which integrates spatial studies and AI, primarily focusing on incorporating spatial thinking and concept into deep learning models for urban studies. This paper provides an overview of techniques and applications of spatially-explicit GeoAI in Urban Geography based on 581 papers identified using a systematic review approach. We examined and screened papers in three scopes of Urban Geography (Urban Dynamics, Social Differentiation of Urban Areas, and Social Sensing) and found that although GeoAI is a trending topic in geography and the applications of deep neural network-based methods are proliferating, the development of spatially-explicit GeoAI models is still at their early phase. We identified three challenges of existing models and advised future research direction towards developing multi-scale explainable spatially-explicit GeoAI. This review paper acquaints beginners with the basics of GeoAI and state-of-the-art and serve as an inspiration to attract more research in exploring the potential of spatially-explicit GeoAI in studying the socio-economic dimension of the city and urban life.}
}
@article{KHAN2022278,
title = {Handling missing data through deep convolutional neural network},
journal = {Information Sciences},
volume = {595},
pages = {278-293},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.02.051},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522001931},
author = {Hufsa Khan and Xizhao Wang and Han Liu},
keywords = {Missing value, Data imputation, Fuzzy clustering, Convolutional neural network},
abstract = {The presence of missing data is a challenging issue in processing real-world datasets. It is necessary to improve the data quality by imputing the missing values so that effective learning from data can be achieved. Recently, deep learning has become the most powerful type of machine learning techniques, which can be used for discovering the hidden knowledge that exists in a large dataset to make accurate predictions. In this paper, we propose an imputation method that involves using a convolutional neural network to impute the missing values. The missing value of each instance is imputed essentially by using a trained kernel. The weights of the kernel are determined by learning from the given data that are arranged spatially in the data matrix. The kernel carries out a weighted sum of neighboring elements in an array for imputing the missing values. In addition, in the absence of the true values with which the missing values are expected to be replaced, a loss function is designed without the need to know the true value. Our method is evaluated on UCI datasets in comparison with state-of-the-art methods. The experimental results show that the proposed approach performs closely to or better than other methods.}
}
@article{MUNAPPY2022111359,
title = {Data management for production quality deep learning models: Challenges and solutions},
journal = {Journal of Systems and Software},
volume = {191},
pages = {111359},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111359},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000905},
author = {Aiswarya Raj Munappy and Jan Bosch and Helena Holmström Olsson and Anders Arpteg and Björn Brinne},
keywords = {Deep learning, Data management, Production quality DL models, Challenges, Solutions, Validation},
abstract = {Deep learning (DL) based software systems are difficult to develop and maintain in industrial settings due to several challenges. Data management is one of the most prominent challenges which complicates DL in industrial deployments. DL models are data-hungry and require high-quality data. Therefore, the volume, variety, velocity, and quality of data cannot be compromised. This study aims to explore the data management challenges encountered by practitioners developing systems with DL components, identify the potential solutions from the literature and validate the solutions through a multiple case study. We identified 20 data management challenges experienced by DL practitioners through a multiple interpretive case study. Further, we identified 48 articles through a systematic literature review that discuss the solutions for the data management challenges. With the second round of multiple case study, we show that many of these solutions have limitations and are not used in practice due to a combination of four factors: high cost, lack of skill-set and infrastructure, inability to solve the problem completely, and incompatibility with certain DL use cases. Thus, data management for data-intensive DL models in production is complicated. Although the DL technology has achieved very promising results, there is still a significant need for further research in the field of data management to build high-quality datasets and streams that can be used for building production-ready DL systems. Furthermore, we have classified the data management challenges into four categories based on the availability of the solutions.}
}
@article{WEI2022100675,
title = {How to improve learning experience in MOOCs an analysis of online reviews of business courses on Coursera},
journal = {The International Journal of Management Education},
volume = {20},
number = {3},
pages = {100675},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100675},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722000775},
author = {Xiaoxia Wei and Viriya Taecharungroj},
keywords = {MOOC learning experience, Online reviews, Sentiment analysis, Textual analysis, Coursera},
abstract = {This study enriched the research horizon of the social sciences and contents of MOOCs by providing stakeholders with authentic data-driven recommendations to better conceptualize, design, develop and deliver MOOCs in today's higher education context. Key factors driving positive/negative learning experiences in business MOOCs were identified and explored. A topic modelling algorithm—Latent Dirichlet allocation (LDA)—was used to examine 144,946 online reviews of 729 business courses on Coursera between August 7, 2015 and August 16, 2021. Two major themes and 11 topics emerged as MOOC delivery (professor, information, comprehension, assessment and materials) and subject matter (finance, marketing, people management, computer skills, technology and project management). A textual salience-valence analysis was employed to analyze the factors driving positive/negative learning experiences regarding MOOC delivery. Findings suggested that 1) business MOOCs should value the importance of instructors' professional and celebrity image to appeal to learners, 2) course design and structure should be easy and simple to manage by learners, 3) course contents, information and assessment should be challenging rather than hard, and 4) the application and validation of peer reviews in both learning process and assessment should be more responsive to eliminate potential issues that could negatively impact learning experiences.}
}
@article{PANTHI2022155641,
title = {Saltwater intrusion into coastal aquifers in the contiguous United States — A systematic review of investigation approaches and monitoring networks},
journal = {Science of The Total Environment},
volume = {836},
pages = {155641},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.155641},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722027371},
author = {Jeeban Panthi and Soni M. Pradhanang and Annika Nolte and Thomas B. Boving},
keywords = {Saltwater intrusion, Monitoring networks, Geophysical techniques, Modeling, Review},
abstract = {Saltwater intrusion (SWI) into coastal aquifers is a growing problem for the drinking water supply of coastal communities worldwide, including for the sustainability of coastal ecosystems depending on freshwater inflow. The interface between freshwater and seawater in coastal aquifers is highly dynamic and is sensitive to changes in the hydraulic gradient between the sea- and groundwater levels. Sea level rise, storm surges, and drought are natural drivers changing the hydrostatic equilibrium between fresh- and saltwater. Coastal aquifers are further stressed by groundwater over-pumping because of the increasing needs of coastal populations. A systematic literature review and analysis of the current state of understanding the SWI drivers is presented, focusing on recent (1980 to 2020) investigations in the contiguous United States (CONUS). Results confirm that SWI is an active research area in CONUS. The drivers of SWI are increasingly better understood and quantified; however, the need for increased monitoring is also recognized. Our study shows that the number of monitoring sites have not increased significantly over the review period. Additionally, geophysical, and geochemical investigation techniques and numerical modeling tools are not utilized to their full potential, and data on SWI is not readily available from some sources. We conclude that there is a need for more SWI monitoring networks and closer multi-disciplinary collaboration, particularly between practitioners in the field and emerging modeling technique experts. Though we focus primarily on CONUS, our insights may be of value to the broader SWI research community and coastal water quality managers around the globe.}
}
@article{KHOURY20221178,
title = {A Framework for Augmented Intelligence in Allergy and Immunology Practice and Research—A Work Group Report of the AAAAI Health Informatics, Technology, and Education Committee},
journal = {The Journal of Allergy and Clinical Immunology: In Practice},
volume = {10},
number = {5},
pages = {1178-1188},
year = {2022},
issn = {2213-2198},
doi = {https://doi.org/10.1016/j.jaip.2022.01.047},
url = {https://www.sciencedirect.com/science/article/pii/S221321982200143X},
author = {Paneez Khoury and Renganathan Srinivasan and Sujani Kakumanu and Sebastian Ochoa and Anjeni Keswani and Rachel Sparks and Nicholas L. Rider},
keywords = {Artificial intelligence, Asthma, Primary immunodeficiency, Atopic dermatitis, Augmented intelligence, Clinical decision support, Electronic health records, Equity, Machine learning, Natural language processing, Medical education},
abstract = {Artificial and augmented intelligence (AI) and machine learning (ML) methods are expanding into the health care space. Big data are increasingly used in patient care applications, diagnostics, and treatment decisions in allergy and immunology. How these technologies will be evaluated, approved, and assessed for their impact is an important consideration for researchers and practitioners alike. With the potential of ML, deep learning, natural language processing, and other assistive methods to redefine health care usage, a scaffold for the impact of AI technology on research and patient care in allergy and immunology is needed. An American Academy of Asthma Allergy and Immunology Health Information Technology and Education subcommittee workgroup was convened to perform a scoping review of AI within health care as well as the specialty of allergy and immunology to address impacts on allergy and immunology practice and research as well as potential challenges including education, AI governance, ethical and equity considerations, and potential opportunities for the specialty. There are numerous potential clinical applications of AI in allergy and immunology that range from disease diagnosis to multidimensional data reduction in electronic health records or immunologic datasets. For appropriate application and interpretation of AI, specialists should be involved in the design, validation, and implementation of AI in allergy and immunology. Challenges include incorporation of data science and bioinformatics into training of future allergists-immunologists.}
}
@article{XIE202272,
title = {Real-World Data for Healthcare Research in China: Call for Actions},
journal = {Value in Health Regional Issues},
volume = {27},
pages = {72-81},
year = {2022},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212109921000765},
author = {Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu},
keywords = {administrative claims, data access, electronic health records, real-world data},
abstract = {Objectives
This study aimed to provide an overview of major data sources in China that can be potentially used for epidemiology, health economics, and outcomes research; compare them with similar data sources in other countries; and discuss future directions of healthcare data development in China.
Methods
The study was conducted in 2 phases. First, various data sources were identified through a targeted literature review and recommendations by experts. Second, an in-depth assessment was conducted to evaluate the strengths and limitations of administrative claims and electronic health record data, which were further compared with similar data sources in developed countries.
Results
Secondary databases, including administrative claims and electronic health records, are the major types of real-world data in China. There are substantial variations in available data elements even within the same type of databases. Compared with similar databases in developed countries, the secondary databases in China have some general limitations such as variations in data quality, unclear data usage mechanism, and lack of longitudinal follow-up data. In contrast, the large sample size and the potential to collect additional data based on research needs present opportunities to further improve real-world data in China.
Conclusions
Although healthcare data have expanded substantially in China, high-quality real-world evidence that can be used to facilitate decision making remains limited in China. To support the generation of real-world evidence, 2 fundamental issues in existing databases need to be addressed—data access/sharing and data quality.}
}
@article{JOSEPH2022115,
title = {A Predictive Maintenance Application for A Robot Cell using LSTM Model},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {115-120},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.193},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014082},
author = {Doyel Joseph and Tilani Gallege and Ebru Turanoglu Bekar and Catarina Dudas and Anders Skoogh},
keywords = {Smart Maintenance, Predictive Maintenance, Machine Learning, Long Short-Term Memory (LSTM), CRISP-DM, Industrial Robots, Manufacturing},
abstract = {Maintaining equipment is critical for increasing production capacity and decreasing production time. With the advent of digitalization, industries are able to access massive amounts of data that can be used to ensure their long-term viability and competitive advantage by implementing predictive maintenance. Therefore, this study aims to demonstrate a predictive maintenance application for a robot cell using real-world manufacturing big data coming from a company in the automotive industry. A hyperparameter tuned Long Short-Term Memory (LSTM) model is developed, and the results show that this model is capable of predicting the day of failure with good accuracy. The difficulties inherent in conducting real-world industrial initiatives are analyzed, and recommendations for improvement are presented.}
}
@article{SARNTHEIN2022100860,
title = {Neurosurgery outcomes and complications in a monocentric 7-year patient registry},
journal = {Brain and Spine},
volume = {2},
pages = {100860},
year = {2022},
issn = {2772-5294},
doi = {https://doi.org/10.1016/j.bas.2022.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2772529422000017},
author = {Johannes Sarnthein and Victor E. Staartjes and Luca Regli and Kevin Akeret and Delal Bektas and David Bellut and Oliver Bichsel and Oliver Bozinov and Elisa Colombo and Sandra Dias and Giuseppe Esposito and Menno R. Germans and Anna-Sophie Hofer and Michael Hugelshofer and Arian Karbe and Niklaus Krayenbühl and Alexander Küffer and Marian C. Neidert and Markus F. Oertel and Luis Padevit and Luca Regli and Jonas Rohr and Ahmed Samma and Johannes Sarnthein and Martina Sebök and Carlo Serra and Victor Staartjes and Lennart Stieglitz and Martin N. Stienen and Lazar Tosic and Tristan {van Doormaal} and Bas {van Niftrik} and Flavio Vasella and Stefanos Voglis and Fabio {von Faber-Castell}},
keywords = {Adverse events, Morbidity and mortality rounds, Quality monitoring},
abstract = {Introduction
Capturing adverse events reliably is paramount for clinical practice and research alike. In the era of “big data”, prospective registries form the basis of clinical research and quality improvement.
Research question
To present results of long-term implementation of a prospective patient registry, and evaluate the validity of the Clavien-Dindo grade (CDG) to classify complications in neurosurgery.
Materials and methods
A prospective registry for cranial and spinal neurosurgical procedures was implemented in 2013. The CDG – a complication grading focused on need for unplanned therapeutic intervention – was used to grade complications. We assess construct validity of the CDG.
Results
Data acquisition integrated into our hospital workflow permitted to include all eligible patients into the registry. We have registered 8226 patients that were treated in 11994 surgeries and 32494 consultations up until December 2020. Similarly, we have captured 1245 complications on 6308 patient discharge forms (20%) since full operational status of the registry. The majority of complications (819/6308 ​= ​13%) were treated without invasive treatment (CDG 1 or CDG 2). At discharge, there was a clear correlation of CDG and the Karnofsky Performance Status (KPS, rho ​= ​-0.29, slope -7 KPS percentage points per increment of CDG) and the length of stay (rho ​= ​0.43, slope 3.2 days per increment of CDG).
Discussion and conclusion
Patient registries with high completeness and objective capturing of complications are central to the process of quality improvement. The CDG demonstrates construct validity as a measure of complication classification in a neurosurgical patient population.}
}
@article{MA2022100002,
title = {Individual mobility prediction review: Data, problem, method and application},
journal = {Multimodal Transportation},
volume = {1},
number = {1},
pages = {100002},
year = {2022},
issn = {2772-5863},
doi = {https://doi.org/10.1016/j.multra.2022.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2772586322000028},
author = {Zhenliang Ma and Pengfei Zhang},
keywords = {Individual mobility prediction, Personalized transportation, Deep learning, Data quality, Model interpretability},
abstract = {The ‘sharing’ business models and on-demand services have been altering city dwellers’ travel habits from buying the means of transport to buying mobility services based on needs. The capability to proactively provide personalized services (e.g., travel recommendations and dynamic pricing) is the future of smart multimodal mobility systems, in which the individual mobility prediction technique (predict where/when for a next trip) is a key enabler to achieve that. With the advancement of data collection and computing techniques, the individual mobility prediction problem has been gaining increasing interest, but yet receives little attention in applications and differs in problem definitions and methodologies developed given varied data sources and application contexts. In addition, there are many review studies on collective mobility predictions (e.g., travel demand/traffic flows), but no review study on the individual mobility prediction. To address these issues and fill the gap, the review synthesizes existing studies on individual mobility prediction in transport (data/problem/methodology/applications), identifies remaining research needs, as well as discusses methodological considerations and potential future transport applications. The review highlights the value of individual mobility prediction in driving proactive service provisions and mobility management in multimodal mobility systems. Methodologically, it is critical to pay more attention to data validation, data specification, and model interpretability in developing learning-based prediction models. Practically, it is of high value to study the individual mobility prediction with arbitrary prediction times (the time when the prediction is made) and prediction horizons (how far in the future).}
}
@article{SOUIFI2022103666,
title = {Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions},
journal = {Computers in Industry},
volume = {140},
pages = {103666},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103666},
url = {https://www.sciencedirect.com/science/article/pii/S016636152200063X},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Industry 4.0, Performance management, Decision support, Big Data, Uncertainty modeling},
abstract = {For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.}
}
@article{MAHAJAN2022415,
title = {Data to the people: a review of public and proprietary data for transport models},
journal = {Transport Reviews},
volume = {42},
number = {4},
pages = {415-440},
year = {2022},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2021.1977414},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722007036},
author = {Vishal Mahajan and Nico Kuehnel and Aikaterini Intzevidou and Guido Cantelmo and Rolf Moeckel and Constantinos Antoniou},
keywords = {Open data, public data, transport model, big data, transport modelling},
abstract = {ABSTRACT
Data play an indispensable role in transport modelling. The availability of data from non-conventional sources, such as mobile phones, social media, and public transport smart cards, changes the way we conduct mobility analyses and travel forecasting. Existing studies have demonstrated the multitude and varied applications of these emerging data in transport modelling. The transferability of current research and further endeavours depend mostly on the availability of these data. Therefore, the openness or public availability of the prominent data for transport modelling needs to be adequately investigated. Such a discussion should also encompass these data’s application aspects to provide a holistic overview. This paper defines a typology for the data classification based on a set of availability or openness attributes from the existing literature. Subsequently, we use the developed typology to classify the prominent transport data into four categories: (i) Commercial data, (ii) Inaccessible data, (iii) Gratis and accessible data with restricted use, and (iv) Open data. Using this typology, we conclude that the public data, which refer to the data that are accessible and free of cost, are a superset of open data. Further, we discuss the applications and limitations of the selected data in transport modelling and highlight in which task(s) certain data excel. Lastly, we synthesise our review using a Strengths, Weaknesses, Opportunities and Threats (SWOT) analysis to bring out the aspects relevant to data owners and data consumers. Public availability of data can help in various modelling steps such as trip generation, accessibility, destination choice, route choice, network modelling. Complementary datasets such as General Transit Feed Specification (GTFS) and Volunteered Geographic Information (VGI) increase the usability of other data. Thus, modellers can gain from the positive cascade effect by prioritising these data. There is also a potential for data owners to release proprietary data, such as mobile phone data, with restricted-use licenses after addressing privacy risks. Our study contributes by dealing with two problems at the same time. On the one hand, the paper analyses existing data based on their potential for mobility studies. On the other hand, we classify them based on how open they are. Hence, we identify the most promising public data for developing the next generation of transport models.}
}
@article{TAYLOR2022107492,
title = {An interdisciplinary research perspective on the future of multi-vector energy networks},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {135},
pages = {107492},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107492},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521007316},
author = {P.C. Taylor and M. Abeysekera and Y. Bian and D. Ćetenović and M. Deakin and A. Ehsan and V. Levi and F. Li and R. Oduro and R. Preece and P.G. Taylor and V. Terzija and S.L. Walker and J. Wu},
keywords = {Energy markets, Information and communication technologies, Modelling, Multi-vector energy networks, Policy, Risk},
abstract = {Understanding the future of multi-vector energy networks in the context of the transition to net zero and the energy trilemma (energy security, environmental impact and social cost) requires novel interdisciplinary approaches. A variety of challenges regarding systems, plant, physical infrastructure, sources and nature of uncertainties, technological in general and more specifically Information and Communication Technologies requirements, cyber security, big data analytics, innovative business models and markets, policy and societal changes, are critically important to ensure enhanced flexibility and higher resilience, as well as reduced costs of an integrated energy system. Integration of individual energy networks into multi-vector entities opens a number of opportunities, but also presents a number of challenges requiring interdisciplinary perspectives and solutions. Considering drivers like societal evolution, climate change and technology advances, this paper describes the most important aspects which have to be taken into account when designing, planning and operating future multi-vector energy networks. For this purpose, the issues addressing future architecture, infrastructure, interdependencies and interactions of energy network infrastructures are elaborated through a novel interdisciplinary perspective. Aspects related to optimal operation of multi-vector energy networks, implementation of novel technologies, jointly with new concepts and algorithms, are extensively discussed. The role of policy, markets and regulation in facilitating multi-vector energy networks is also reported. Last but not least, the aspects of risks and uncertainties, relevant for secure and optimal operation of future multi-vector energy networks are discussed.}
}
@article{WU202248,
title = {Process modeling by integrating quantitative and qualitative information using a deep embedding network and its application to an extrusion process},
journal = {Journal of Process Control},
volume = {115},
pages = {48-57},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422000749},
author = {Haibin Wu and Yu-Han Lo and Le Zhou and Yuan Yao},
keywords = {Process modeling, Small data, Deep neural network, Embedding, Autoencoder},
abstract = {In the big data era, small data problems still exist in many industrial sectors. Taking the high-value process industries as an example, a large number of materials and processing methods are often tested at the design stage. However, only a small amount of data can be collected for each material-process combination, which poses a serious challenge to data-driven process modeling. There is a great necessity to integrate the small data measured in different tasks and build the process model by sharing the information. In this work, a deep embedding neural network is proposed to extract the qualitative task information for process modeling. Specifically, an autoencoder is used to learn embeddings which are combined with the quantitative process conditions as the inputs of a feed-forward neural network to produce the final predictions. The feasibility, including interpretability and prediction accuracy, of the developed method is illustrated with an extrusion process.}
}
@article{KONGBOON2022130711,
title = {Greenhouse gas emissions inventory data acquisition and analytics for low carbon cities},
journal = {Journal of Cleaner Production},
volume = {343},
pages = {130711},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130711},
url = {https://www.sciencedirect.com/science/article/pii/S095965262200350X},
author = {Ratchayuda Kongboon and Shabbir H. Gheewala and Sate Sampattagul},
keywords = {Sustainable city, Low carbon city, Greenhouse gas inventory, Greenhouse gas emissions, Municipalities},
abstract = {This paper studied greenhouse gas inventory data acquisition and analytics for municipalities in Thailand. A complete and transparent GHG inventory of eight municipalities was developed to document the current situation, and to help decision-makers to clarify their priorities for reducing greenhouse gas emissions. The Global Protocol for Community-Scale Greenhouse Gas Emissions Inventories guidelines was used to investigate and calculate the greenhouse gas emissions and assess data accuracy. The results indicated that the data source, data format, and data collection of each municipality are relatively similar. Moreover, the activity data needed to be obtained from several authorities. The results showed that Nonthaburi Municipality had the highest greenhouse gas emissions at 2,286,838 tCO2e/yr and Buriram Municipality, the lowest at 239,795 tCO2e/yr. On a per-capita basis, Lamphun Municipality was the highest with 10.1 tCO2e/capita and Buriram Municipality the lowest with 3.8 tCO2e/capita. The results suggest that the municipalities should continually develop a GHG database by creating a routine procedure. An information management system should be produced in the shape of big data which can lead to state policies, plans, and actions for city development to ensure the reduction of greenhouse gas emissions. This in turn will lead to a low carbon city.}
}
@article{LI2022101808,
title = {Urban population distribution in China: Evidence from internet population},
journal = {China Economic Review},
volume = {74},
pages = {101808},
year = {2022},
issn = {1043-951X},
doi = {https://doi.org/10.1016/j.chieco.2022.101808},
url = {https://www.sciencedirect.com/science/article/pii/S1043951X22000669},
author = {Huixuan Li and Jing Chen and Zihao Chen and Jianguo Xu},
keywords = {Internet population, Population distribution, Zipf's law, Public resource distortions, Big data},
abstract = {Based on mobile internet user data, we construct an “Internet population” measure and reexamine spatial population distribution in China. The location based service (LBS) data of mobile internet uses is able to capture the accurate location of users' residence and solve the underestimation problem of missing migrants. We have three main findings. First, contrary to previous studies based on traditional population statistics, city size distribution of Internet population fits well into Zipf's law with a R2 of 90.7%. Second, the Internet population indicator is superior to traditional population statistics in explaining inelastic household consumption such as water consumption, electricity consumption, and garbage disposal. It suggests that the “Internet population” is a better proxy of actual city population. Third, the traditional population statistics systematically overestimate population in small cities and underestimate population in large cities. It indicates that the public resource distortions will continue to exist or even worsen off in China if the allocation process relies greatly on traditional population statistics. Although no measures are perfect, our new population measure provides important incremental information for future discussion.}
}
@article{RODRIGUES2022101625,
title = {Species misidentification affects biodiversity metrics: Dealing with this issue using the new R package naturaList},
journal = {Ecological Informatics},
volume = {69},
pages = {101625},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101625},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000747},
author = {Arthur Vinicius Rodrigues and Gabriel Nakamura and Vanessa Graziele Staggemeier and Leandro Duarte},
keywords = {Occurrence records, Biodiversity, Species misidentification, Taxonomy, Ecological patterns},
abstract = {Biodiversity databases are increasingly available and have fostered accelerated advances in many disciplines within ecology and evolution. However, the quality of the evidence generated depends critically on the quality of the input data, and species misidentifications are present in virtually any occurrence dataset. Yet, the lack of automatized tools makes the assessment of the quality of species identification in big datasets time-consuming, which often induces researchers to assume that all species are reliably identified. In this study, we address this issue by evaluating how species misidentification can impact our ability to capture ecological patterns, and by presenting an R package, called naturaList, designed to classify species occurrence data according to identification reliability. naturaList allows the classification of species occurrences up to six confidence levels, in which the highest level is assigned to records identified by specialists. We obtained a list of specialists by using the species occurrence dataset itself, based on the identifier names within it, and by entering an independent list, obtained by contacting experts. Further, we evaluate the effects of filtering out occurrence records not identified by specialists on the estimations of species niche and diversity patterns. We used the tribe Myrteae (Myrtaceae) as a study model, which is a species-rich group in Central and South America and with challenging taxonomy. We found a significant change in species niche in 13% of species when using only occurrences identified by specialists. We found changes in patterns of alpha diversity in four genera and changes in beta diversity in all genera analyzed. We show how the uncertainty in species identification in occurrence datasets affects conclusions on macroecological patterns by generating bias or noise in different aspects of macroecological patterns (niche, alpha, and beta diversity). Therefore, to guarantee reliability in species identification in big data sets we recommend the use of automated tools such as the naturaList package, especially when analyzing variation in species composition. This study also represents a step forward to increasing the quality of large-scale studies that rely on species occurrence data.}
}
@article{MMOHAMMED2022197,
title = {Selective ensemble of classifiers trained on selective samples},
journal = {Neurocomputing},
volume = {482},
pages = {197-211},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221017227},
author = {Amgad {M. Mohammed} and Enrique Onieva and Michał Woźniak},
keywords = {Meta-heuristics, Ensemble selection, Data reduction, Ensemble pruning, Multiple classifier systems, Big data, Machine learning, Difficult samples, Ordering-based pruning},
abstract = {Classifier ensembles are characterized by the high quality of classification, thanks to their generalizing ability. Most existing ensemble algorithms use all learning samples to learn the base classifiers that may negatively impact the ensemble’s diversity. Also, the existing ensemble pruning algorithms often return suboptimal solutions that are biased by the selection criteria. In this work, we present a proposal to alleviate these drawbacks. We employ an instance selection method to query a reduced training set that reduces both the space complexity of the formed ensemble members and the time complexity to classify an instance. Additionally, we propose a guided search-based pruning schema that perfectly explores large-size ensembles and brings on a near-optimal subensemble with less computational requirements in reduced memory space and improved prediction time. We show experimentally how the proposed method could be an alternative to large-size ensembles. We demonstrate how to form less-complex, small-size, and high-accurate ensembles through our proposal. Experiments on 25 datasets show that the proposed method can produce effective ensembles better than Random Forest and baseline classifier pruning methods. Moreover, our proposition is comparable with the Extreme Gradient Boosting Algorithm in terms of accuracy.}
}
@article{DAHIYA2022100066,
title = {Analysis of the single-regime speed-density fundamental relationships for varying spatiotemporal resolution using Zen Traffic Data},
journal = {Asian Transport Studies},
volume = {8},
pages = {100066},
year = {2022},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2022.100066},
url = {https://www.sciencedirect.com/science/article/pii/S2185556022000128},
author = {Garima Dahiya and Yasuo Asakura and Wataru Nakanishi},
keywords = {Macroscopic traffic flow, Vehicle trajectory data, Big data and naturalistic datasets, Speed-density relationship, Statistical and theoretical analysis, Spatiotemporal resolutions},
abstract = {This study analyzed the single-regime speed-density (v-k) relationships for urban expressways using high resolution Zen Traffic Data (ZTD) containing all vehicles’ trajectory data obtained using image sensing technology. The steady-state traffic data were extracted for varying spatiotemporal resolutions, followed by estimation of traffic flow parameters, namely, jam density, kinematic-wave-speed, and proportionality factor, a behavioral parameter, using empirical data. Functional and shape parameters were estimated using the Levenberg–Marquardt algorithm. Statistical metrics were used to assess the performance and model fitness in all categories of linear, exponential and logarithmic, and complex forms of v-k relationships for different resolutions. The theoretical analysis revealed that certain relationships satisfy all the static properties and that only one satisfies both the dynamic properties of traffic behavior. Highly parameterized forms had the lowest errors. However, the linear form of model developed by May and Keller has high application potential.}
}
@article{YU2022103483,
title = {Towards a privacy-preserving smart contract-based data aggregation and quality-driven incentive mechanism for mobile crowdsensing},
journal = {Journal of Network and Computer Applications},
volume = {207},
pages = {103483},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103483},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001291},
author = {Ruiyun Yu and Ann Move Oguti and Dennis Reagan Ochora and Shuchen Li},
keywords = {Mobile crowdsensing, Smart contracts, Data aggregation, Incentive mechanism, dApp, IPFS},
abstract = {The crowd's power, combined with the sensing capabilities of smart mobile de-vices, has resulted in the emergence of a revolutionary data acquisition paradigm known as Mobile Crowdsensing. In exchange for rewards, mobile users collect and share location-specific data values. However, most existing crowdsensing systems built on traditional centralized architectures are highly prone to attacks, intrusions, single point of failure, manipulations, and low reliability. Recently, decentralized blockchain technologies are being applied in mobile crowdsensing systems to ensure workers' privacy, data privacy, and the quality of sensed data at a low service fee. By leveraging blockchain technology, this paper inherits the advantages of the public blockchain without the need for any trusted third-parties. We propose a smart contract-based privacy-preserving data aggregation and quality assessment protocol to infer reliable aggregated results and estimate data quality, wherein, we design a fair quality-driven incentive mechanism to distribute rewards based on the data quality. The protocol ensures a secure, cost-optimal, and reliable aggregation and estimation of the sensed data quality on the public blockchain without disclosing the sensed data's and participants' privacy. We adopt Interplanetary File Systems to offset the blockchain's expensive storage costs. Experiments were conducted using real-world datasets which were implemented on a full-stack on-chain and off-chain decentralized application on the Ethereum blockchain. The experimental results demonstrate our design is highly efficient in achieving privacy-preserving data aggregation and significantly reduces on-chain computation costs.}
}
@article{MELO2022107964,
title = {Open benchmarks for assessment of process monitoring and fault diagnosis techniques: A review and critical analysis},
journal = {Computers & Chemical Engineering},
volume = {165},
pages = {107964},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107964},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422003003},
author = {Afrânio Melo and Maurício M. Câmara and Nayher Clavijo and José Carlos Pinto},
keywords = {Process monitoring, Fault detection, Fault diagnosis, Benchmarks, Datasets, Exploratory data analysis, Open source},
abstract = {The present paper brings together openly available datasets and simulators for testing of process monitoring and fault diagnosis techniques. Some general characteristics of these benchmark models and datasets are then assessed, including the richness of the information, availability of real data, ease of use, challenges that must be considered and potential for scientific advances. A critical comparison is made based on Exploratory Data Analysis (EDA) of readily available datasets. Previous works that used each benchmark are also reviewed and discussed. This reference can be used by researchers and practitioners who wish to choose a reference benchmark that best suits their particular interests and needs.}
}
@article{ALAGADOR2022115172,
title = {Operations research applicability in spatial conservation planning},
journal = {Journal of Environmental Management},
volume = {315},
pages = {115172},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.115172},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722007459},
author = {Diogo Alagador and Jorge Orestes Cerdeira},
keywords = {Climate change, Computational sustainability, Conservation planning, Mathematical programming, Optimization, Spatial analysis},
abstract = {A large fraction of the current environmental crisis derives from the large rates of human-driven biodiversity loss. Biodiversity conservation questions human practices towards biodiversity and, therefore, largely conflicts with ordinary societal aspirations. Decisions on the location of protected areas, one of the most convincing conservation tools, reflect such a competitive endeavor. Operations Research (OR) brings a set of analytical models and tools capable of resolving the conflicting interests between ecology and economy. Recent technological advances have boosted the size and variety of data available to planners, thus challenging conventional approaches bounded on optimized solutions. New models and methods are needed to use such a massive amount of data in integrative schemes addressing a large variety of concerns. This study provides an overview on the past, present and future challenges that characterize spatial conservation models supported by OR. We discuss the progress of OR models and methods in spatial conservation planning and how those models may be optimized through sophisticated algorithms and computational tools. Moreover, we anticipate possible panoramas of modern spatial conservation studies supported by OR and we explore possible avenues for the design of optimized interdisciplinary collaborative platforms in the era of Big Data, through consortia where distinct players with different motivations and services meet. By enlarging the spatial, temporal, taxonomic and societal horizons of biodiversity conservation, planners navigate around multiple socioecological/environmental equilibria and are able to decide on cost-effective strategies to improve biodiversity persistence under complex environments.}
}
@article{POWELL2022100261,
title = {Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100261},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100261},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000595},
author = {Warwick Powell and Marcus Foth and Shoufeng Cao and Valéri Natanelov},
keywords = {Blockchain, Data quality, Distributed ledger technology, Meat industry, Internet of things, Food supply chains},
abstract = {The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of ‘common knowledge’ fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.}
}
@article{SHAO2022102736,
title = {IoT data visualization for business intelligence in corporate finance},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102736},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102736},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002181},
author = {Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam},
keywords = {IoT, Data visualization, Business intelligence, Corporate finance},
abstract = {Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.}
}
@article{KAYABAY2022121264,
title = {Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121264},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121264},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006983},
author = {Kerem Kayabay and Mert Onuralp Gökalp and Ebru Gökalp and P. {Erhan Eren} and Altan Koçyiğit},
keywords = {Technology roadmapping, Technology management, Data science, Digital transformation, Data-driven organization, Big data},
abstract = {Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.}
}
@article{TSUMURA2022100043,
title = {Examining potentials and practical constraints of mobile phone data for improving transport planning in developing countries},
journal = {Asian Transport Studies},
volume = {8},
pages = {100043},
year = {2022},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2021.100043},
url = {https://www.sciencedirect.com/science/article/pii/S2185556021000110},
author = {Yuma Tsumura and Yoshihisa Asada and Hiroshi Kanasugi and Ayumi Arai and Ryosuke Shibasaki and Hirohisa Kawaguchi and Kaoru Yamada},
keywords = {Call detail records, Mobile phone data, Big data, OD matrix, Travel behavior, Household travel survey},
abstract = {The advantages and constraints of mobile phone data, call detail records (CDRs), for comprehending travel patterns were argued in previous studies. However, the spatio-temporal tendencies of the estimated travel patterns from CDR data have been underrepresented from the viewpoint of transport planning practice. This study scrutinizes the benefits and constraints of CDR data for grasping spatio-temporal travel patterns through multidimensional comparison between origin-destination matrices generated from aggregated CDR data and household travel survey (HTS) data from Colombo, Sri Lanka. The results show the potential of CDR data to complement the conventional drawbacks of HTSs. They also indicate practical constraints owing to the nature of CDR data and the possible impacts of data protection measures. This study summarizes how the results could be used for interpreting the time coverage and distribution, trip type, spatial distribution, and population projection and coverage to discuss future directions toward improving transport planning in developing countries.}
}
@article{LI2022341,
title = {Development of a machine learning-based risk prediction model for cerebral infarction and comparison with nomogram model},
journal = {Journal of Affective Disorders},
volume = {314},
pages = {341-348},
year = {2022},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2022.07.045},
url = {https://www.sciencedirect.com/science/article/pii/S0165032722008138},
author = {Xuewen Li and Yiting Wang and Jiancheng Xu},
keywords = {Cerebral infarction, Machine learning, Fibrinogen, Extreme gradient boosting, Risk prediction, Nomogram},
abstract = {Background
Development of a cerebral infarction (CI) risk prediction model by mining routine test big data with machine learning algorithms.
Methods
Cohort 1 included 2017 CI patients and health checkers, and the optimal machine learning algorithms in Extreme gradient Boosting (XgBoost), Logistic Regression (LR), Support Vector Machine (SVM), Random Forest (RF) were selected to mine all routine test data of the enrolled subjects for screening CI model features. Cohort 2 included patients with CI and Non-CI from 2018 to 2020 to develop an early warning model for CI and was analyzed in subgroups with a cutoff of 50 years. Cohort 3 included CI patients versus Non-CI patients in 2021, and a nomogram models was developed for comparison with the machine learning model.
Results
The optimal algorithm XgBoost was used to develop a CI risk prediction model CI-Lab8 containing eight characteristics of fibrinogen, age, glucose, mean erythrocyte hemoglobin concentration, albumin, neutrophil absolute value, activated partial thromboplastin time, and triglycerides. The model had an AUC of 0.823 in cohort 2, significantly higher than the FIB (AUC = 0.737), which ranked first in feature importance. CI-Lab8 also had higher diagnostic accuracy in CI patients <50 years of age (AUC = 0.800), slightly lower than in CI patients ≥50 years of age (AUC = 0.856). Receiver operating characteristic curve, calibration curve, and decision curve analysis in cohort 3 showed CI-Lab8 to be superior to nomogram.
Conclusion
In this study, the CI risk prediction model developed by XgBoost algorithm outperformed the nomogram model and had higher diagnostic accuracy for CI patients in both <50 and ≥50 years old, which may assist clinical assessment for CI.}
}
@article{DING2022136,
title = {An Internet of Things based scalable framework for disaster data management},
journal = {Journal of Safety Science and Resilience},
volume = {3},
number = {2},
pages = {136-152},
year = {2022},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S2666449621000542},
author = {Zhiming Ding and Shan Jiang and Xinrun Xu and Yanbo Han},
keywords = {Disaster data management, IoT, Disaster detection, Big data, Artificial intelligence},
abstract = {In recent years, undesirable disasters attacked the cities frequently, leaving heavy casualties and serious economic losses. Meanwhile, disaster detection based on the Internet of Things(IoT) has become a hot spot that benefited from the established development of smart city construction. And the IoT is visibly sensitive to the management and monitoring of disasters, but massive amounts of monitoring data have brought huge challenges to data storage and data analysis. This article develops a new and much more general framework for disaster emergency management under the IoT environment. The framework is a bottom-up integration of highly scalable Raw Data Storages(RD-Stores) technology, hybrid indexing and queries technology, and machine learning technology for emergency disasters. Experimental results show that hybrid index and query technology have better performance under the condition of supporting multi-modal retrieval, and providing a better solution to offer real-time retrieval for the massive sensor sampling data in the IoT. In addition, further works to evaluate the top-level sub-application system in this framework were performed based on the GPS trajectory data of 35,000 Beijing taxis and the volumetric ground truth data of 7,500 images. The results show that the framework has desirable scalability and higher utility.}
}
@article{NKIKABAHIZI2022108908,
title = {Chaining Zscore and feature scaling methods to improve neural networks for classification},
journal = {Applied Soft Computing},
volume = {123},
pages = {108908},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108908},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002733},
author = {Calpephore Nkikabahizi and Wilson Cheruiyot and Ann Kibe},
keywords = {Feature scaling, Outliers, Data quality, Classification, Chaining},
abstract = {Neural networks for classification aim at identifying the class label of new observation based training data containing instances whose category memberships are known. Therefore the data fed into neural networks has to be preprocessed to enhance its quality resulting in promoting the extraction of meaningful insights of data. However, the fact of processing data until you have the required high quality is challenging and time-consuming to manually search for the best method in a sequence of preprocessing independent methods. For feature scaling methods, they consist of scaling the dataset into the same range of data without monitoring data outliers that should eventually occur in the data source. Zscore for outlier’s detection suffers from the issue of predefining the parameters. This paper discussed various approaches that are applied to scale features and detect outliers during data pre-processing. Thereafter, the paper proposed the algorithm that combines Zscore as an outlier’s detection method with every classical feature scaling method in high-dimensional data. The proposed algorithm has benefits in selecting the optimal subset of methods from a sequence of chained methods, detecting outliers, and removing zero variance predictors. The study findings from five sample sizes revealed that the proposed method significantly excels the classical method in terms of accuracy. The outstanding from them was performed at the rate of 99.67% and had a significant difference of 0.20% over classical feature scaling.}
}
@article{LIU2022101687,
title = {Review on automated condition assessment of pipelines with machine learning},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101687},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101687},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001471},
author = {Yiming Liu and Yi Bao},
keywords = {Big data, Condition assessment, Machine learning, Nondestructive testing, Pipeline, SWOT},
abstract = {Pipelines carrying energy products play vital roles in economic wealth and public safety, but incidents continue occurring. Condition assessment of pipelines is essential to identify anomalies timely. Advanced sensing technologies obtain informative data for condition assessment, while data analysis by human has limited efficiency, accuracy, and reliability. Advances in machine learning offer exciting opportunities for automated condition assessment with minimum human intervention. This paper reviews machine learning approaches to detect, classify, locate, and quantify pipeline anomalies based on intelligent interpretation of routine operation data, nondestructive testing data, and computer vision data. Statistics and uncertainties of performance metrics of machine learning approaches are discussed. An analysis on strengths, weaknesses, opportunities, and threats (SWOT) is performed. Guides for practitioners to perform automated pipeline condition assessment are recommended. This review provide insights into the machine learning approaches for automated pipeline condition assessment. The SWOT analysis will support decision making in the pipeline industry.}
}
@article{MIA2022100238,
title = {A privacy-preserving National Clinical Data Warehouse: Architecture and analysis},
journal = {Smart Health},
volume = {23},
pages = {100238},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100238},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000544},
author = {Md Raihan Mia and Abu Sayed Md Latiful Hoque and Shahidul Islam Khan and Sheikh Iqbal Ahamed},
keywords = {Privacy, Standardization, Clinical data warehousing, Data modeling, Big data analytics, Decision supports},
abstract = {A centralized clinical data repository is essential for inspecting patients’ medical history, disease analysis, population-wide disease research, treatment decision support, and improving existing healthcare policies and services. Bangladesh, a rapidly developing country, poses several unusual challenges for developing such a centralized clinical data repository as the existing Electronic Health Records (EHR) are stored in unconnected, heterogeneous sources with no unique patient identifier and consistency. Data integration with secure record linkage, privacy preservation, quality control, and data standardization are the main challenges for developing a consistent and interoperable centralized clinical data repository. Based on the findings from our previous researches, we have designed an anonymous National Clinical Data Warehouse (NCDW) framework to reinforce research and analysis. The architecture of NCDW is divided into five stages to overcome the challenges: (1) Wrapper-based anonymous data acquisition; (2) Data loading and staging; (3) Transformation, standardization, and uploading to the data warehouse; (4) Management and monitoring; (5) Data Mart design, OLAP server, data mining, and applications. A prototype of NCDW has been developed with a complete pipeline from data collection to analytics by integrating three data sources. The proposed NCDW model facilitates regional and national decision support, intelligent disease analysis, knowledge discovery, and data-driven research. We have inspected the analytical efficacy of the framework by qualitative evaluation of the national decision support from two derived disease data marts. The experimental result based on the analysis is satisfactory to extend the NCDW on a large scale.}
}
@article{NYOMANKUTHAKRISNAWIJAYA2022106813,
title = {Data analytics platforms for agricultural systems: A systematic literature review},
journal = {Computers and Electronics in Agriculture},
volume = {195},
pages = {106813},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106813},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922001302},
author = {Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol},
keywords = {Data analytics platforms, Agriculture, Systematic literature review, Big Data},
abstract = {With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.}
}
@article{SEN2022100052,
title = {Are data markets a solution to big tech market power? A competitive analysis},
journal = {Journal of Government and Economics},
volume = {7},
pages = {100052},
year = {2022},
issn = {2667-3193},
doi = {https://doi.org/10.1016/j.jge.2022.100052},
url = {https://www.sciencedirect.com/science/article/pii/S2667319322000234},
author = {Anindya Sen},
keywords = {Big tech, Data markets, Data portability, Data governance, Targeted advertising},
abstract = {This paper explores whether the establishment of data markets based on individual data portability can result in better societal outcomes. The results suggest that markets where individuals can sell data generated through their online engagement to third parties, could result in pareto improving outcomes for subscribers to digital platforms and purchasers of targeted advertising services. Data markets would enable third parties to combine their own proprietary data with other individual level data and produce information for targeted advertising, reducing the market power of Big Tech firms. However, successful data markets require strong regulatory measures by governments that ensure privacy and that data collected by Big Tech firms are considered the property of individuals. Such policies have the potential to guarantee that the benefits of Big Data are not confined to a few large firms.}
}
@article{MOHAMAD2022,
title = {Using a Decision Tree to Compare Rural versus Highway Motorcycle Fatalities in Thailand},
journal = {Case Studies on Transport Policy},
year = {2022},
issn = {2213-624X},
doi = {https://doi.org/10.1016/j.cstp.2022.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S2213624X22001857},
author = {Ittirit Mohamad and Sajjakaj JomnonKwao and Vatanavongs Ratanavaraha},
keywords = {Comparative Analysis, Decision Tree, Accidents, Big data, Transportation, Machine Learning},
abstract = {Thailand ranks first in Asia and ninth in the world in term of road accident. As of 2020, the number of vehicles registered in Thailand was over 41 million, with motorcycles accounting for half of all vehicles. This study aimed to determine the cause of fatalities to reduce motorcycle accidents. The research entailed separating the accidents and fatalities into those occurring on highways (HWs) versus those occurring on rural roadways (RRs) and focused solely on rider at fault accidents to involve any confounding factors related to passengers or others involved. In Thailand, HWs have higher speed limits and allow more vehicle types than some RRs. Thailand’s Department of Public Disaster Prevention and Mitigation recorded 115,154 motorcycle accidents from 2015 to 2020. Decision trees allow for processing large amounts of data to drill down into associations between the individual variables in a large data set; in this study, the tree also separated accidents into whether or not the driver was exceeding the speed limit. The model’s performance for HWs, predicted misclassifications were found to be 28.3% (fatality to nonfatality) and a 44.5% (nonfatality to fatality) while predicted misclassification for RRs were 15.5% (fatality to nonfatality) and 60% (nonfatality to fatality). At all ages, the most fatalities were among male riders on dry straightaways in clear daytime weather; notably, however, on RRs, even when the rider was driving responsibly, fatalities were high at night on roads with no light. Following the presentation of the study findings, suggestions are made for ways the Thai government can improve the motorcycle accident and fatality statistics, including increasing the age limit for a motorcycle license, with engine size limits further divided according to age; proper enforcement of the existing rules will also improve the country’s accident statistics. It will also be highly effective to improve road lighting, particularly on RRs.}
}
@article{JAVAID2022124,
title = {Evolutionary trends in progressive cloud computing based healthcare: Ideas, enablers, and barriers},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {3},
pages = {124-135},
year = {2022},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666307422000134},
author = {Mohd Javaid and Abid Haleem and Ravi Pratap Singh and Shanay Rab and Rajiv Suman and Ibrahim Haleem Khan},
keywords = {Cloud computing, Healthcare, Applications, Data, Information},
abstract = {Cloud computing is one of the significant facilitators of the health information revolution in the healthcare business. The global exchange of records in the health sector through electronic media is facilitated by cloud computing. In healthcare, this technology increases safety and creates innovation. Communication with the health matrix throughout the world makes feasible by the application of this technology. Cloud computing has been utilised in health care for many years and has evolved in conjunction with developments in business. This technology establishes standard accessible hardware for diverse healthcare applications via a network connection. Cloud computing and processing ensure safe communication, and the cloud servers secure all essential data. Doctors can counsel their individuals on their health and broadcast their patient's daily health regimes, typically keeping their minds and bodies healthy. Psychologists and psychiatrists can use videoconferencing that makes patients comfortable with their patients. This paper discusses cloud computing and its need for healthcare. Major key advantages, barriers, and challenges of Cloud computing for the healthcare industry are identified. Finally, it discusses the significant applications of cloud computing for healthcare. Today more and more healthcare suppliers are providing Internet of Things (IoT) enabled gadgets to patients, and patient data are instantly communicated to their doctors by linking such devices to the cloud system of hospitals. As a result, cloud computing, in conjunction with fast-expanding technologies such as Big Data analytics, artificial intelligence, and the internet of medical things, improves efficiencies and expands the number of ways to streamline healthcare delivery. It improves resource availability, improves interoperability, and reduces costs.}
}
@article{MICHAILIDOU2022101953,
title = {EQUALITY: Quality-aware intensive analytics on the edge},
journal = {Information Systems},
volume = {105},
pages = {101953},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101953},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001496},
author = {Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas},
keywords = {Fog computing, Optimization, Sensors, Data quality},
abstract = {Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.}
}
@article{KHOSRAVI2022111707,
title = {Modification of correlation optimized warping method for position alignment of condition measurements of linear assets},
journal = {Measurement},
volume = {201},
pages = {111707},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111707},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122009149},
author = {Mahdi Khosravi and Iman Soleimanmeigouni and Alireza Ahmadi and Arne Nissen and Xun Xiao},
keywords = {Position alignment, Correlation optimized warping, Data quality, Linear assets, Positional error, Condition measurements},
abstract = {This paper proposes a modification to a well-known alignment method, correlation optimized warping (COW), to improve the efficiency of the method and reduce the positional errors in the measurements of linear assets. The modified method relaxes the restrictions of COW in aligning the start and end of datasets and decreases the computational time. Furthermore, the method takes advantage of the interdependencies between simultaneously measured channels to overcome the missing data problem. A case study on railway track geometry measurements was conducted to implement the proposed method and assess its performance in reducing the positioning inaccuracy of the measurements. The findings revealed that the modified method could decrease the positional errors of defects to below 25 cm in 94 % of the trials.}
}
@article{HE2022100140,
title = {GARD: Gender difference analysis and recognition based on machine learning},
journal = {Array},
volume = {14},
pages = {100140},
year = {2022},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2022.100140},
url = {https://www.sciencedirect.com/science/article/pii/S259000562200011X},
author = {Shiwen He and Jian Song and Yeyu Ou and Yuanhong Yuan and Xiaojie Zhang and Xiaohua Xu},
keywords = {Gender difference analysis, Gender recognition, Medical examination data, Machine learning},
abstract = {In recent years, intelligent diagnosis and intelligent medical treatment based on big data of medical examinations have become the main trend of medical development in the future. In this paper, we propose a method for analyzing the difference between males and females in medical examination items (medical attributes) and find that males and females of different ages have differences in medical attributes. Then, the cluster analysis method is used to further analyze the differences between male and female in medical examination items, such that some common important attributes (CIAs) that can be used for gender recognition are found within a specific age range. Following, we propose two gender recognition models (GRMs) by using the found CIAs to identify the gender. A large number of experimental results are provided to validate the effectiveness of the proposed GRMs. Experimental results show that the medical attributes with a large value of difference really contribute to gender recognition. Within a certain age range, such as 17 to 51 years old, the proposed GRM can reach 92.8% accuracy using only six medical attributes.}
}
@article{JASEENA20223393,
title = {Deterministic weather forecasting models based on intelligent predictors: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part B},
pages = {3393-3412},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820304729},
author = {K.U. Jaseena and Binsu C. Kovoor},
keywords = {Weather forecasting, Artificial neural networks, Deep learning, Autoencoders, Recurrent neural networks},
abstract = {Weather forecasting is the practice of predicting the state of the atmosphere for a given location based on different weather parameters. Weather forecasts are made by gathering data about the current state of the atmosphere. Accurate weather forecasting has proven to be a challenging task for meteorologists and researchers. Weather information is essential in every facet of life like agriculture, tourism, airport system, mining industry, and power generation. Weather forecasting has now entered the era of Big Data due to the advancement of climate observing systems like satellite meteorological observation and also because of the fast boom in the volume of weather data. So, the traditional computational intelligence models are not adequate to predict the weather accurately. Hence, deep learning-based techniques are employed to process massive datasets that can learn and make predictions more effectively based on past data. The effective implementation of deep learning in various domains has motivated its use in weather forecasting and is a significant development for the weather industry. This paper provides a thorough review of different weather forecasting approaches, along with some publicly available datasets. This paper delivers a precise classification of weather forecasting models and discusses potential future research directions in this area.}
}
@article{LAMAAZI2022151,
title = {Smart-3DM: Data-driven decision making using smart edge computing in hetero-crowdsensing environment},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {151-165},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200022X},
author = {Hanane Lamaazi and Rabeb Mizouni and Hadi Otrok and Shakti Singh and Ernesto Damiani},
keywords = {Smart edge computing, Crowdsensing, Distributed architecture, Data assessment, Data quality},
abstract = {Mobile Edge Computing (MEC) has recently emerged as a promising paradigm for Mobile Crowdsensing (MCS) environments. In a given Area of Interest (AoI), the sensing process is performed based on task requirements, which usually ask for a specific quality of the sensing outcome. In this work, a two-stage Data-Driven Decision-making Mechanism using smart edge computing (Smart-3DM) is proposed. It advocates the use of smart edge to better fulfill the data-related task requirements. Depending on the type of data to be collected, the minimum quality of the data required, and the heuristics to apply for each type of crowdsensing service, the smart edge orchestrates the selection of workers in MEC. Our approach relies on (a) smart-edge deployment: where a cluster-based distributed architecture using smart edge nodes is considered. Here, two entities are defined: the main edge node (MEN) and the local edge nodes (LENs); and (b) data management offloading where a two-layer re-selection strategy that considers data type and context-awareness is adopted, to reduce data computation complexity and to increase data quality while meeting the task target. The proposed Smart-3DM is evaluated using a real-life dataset and is compared to one-stage local and global approaches. The overall results show that by using two-stage re-selection strategies, better performance with lower processing power (CPU), less Storage(RAM), and improved execution time is achieved, when compared to the benchmarks.}
}
@article{CLAPP2022658,
title = {Does accreditation matter? An analysis of complications of bariatric cases using the Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program and National Quality Improvement Program databases},
journal = {Surgery for Obesity and Related Diseases},
volume = {18},
number = {5},
pages = {658-665},
year = {2022},
issn = {1550-7289},
doi = {https://doi.org/10.1016/j.soard.2022.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S1550728922000302},
author = {Benjamin Clapp and Samuel Grasso and Jesus Gamez and Jensen Edwards and Cristopher Dodoo and Ray Portela and Omar M. Ghanem and Brian R. Davis},
keywords = {MBSAQIP, NSQIP, Database, Big Data, Bariatric, Complications},
abstract = {Background
Two large nationwide databases collect data on common operations in the United States. The Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program (MBSAQIP) collects bariatric data, whereas the National Quality Improvement Program (NSQIP) gathers details on a broader range of general surgical cases.
Objective
Evaluate the differences in rates of complications between both databases regarding Roux-en-Y gastric bypass and sleeve gastrectomy.
Setting
National databases, United States.
Methods
We evaluated the MBSAQIP and NSQIP from 2017 to 2019 using the procedure codes 43644 and 43775. Fifteen common complications were evaluated. Propensity-matched analyses (PMAs) were done to control for differences across databases. Significantly different variables after a PMA were included in multivariable models. The data were examined for differences between the 2 databases before and after the PMA, with and without adjustment for operation type.
Results
There were 483,361 cases reported in the MBSAQIP and 57,598 in the NSQIP. PMA matched 57,479 cases for each database. Seven complications were different, with higher rates reported in the NSQIP than in the MBSAQIP: myocardial infarction, sepsis, organ/space surgical site infections, deep vein thrombosis, urinary tract infections, pulmonary embolism, ventilator dependence >48 hours, and pneumonia. When adjusting for the procedure performed, sleeve gastrectomy in the NSQIP had higher rates of organ/space surgical site infections, deep vein thrombosis, sepsis, and death. Roux-en-Y gastric bypass in the NSQIP had higher rates of organ/space surgical site infections, ventilator dependence >48 hours, urinary tract infections, myocardial infarction, deep vein thrombosis, and sepsis.
Conclusion
When compared with the MBSAQIP, the NSQIP reports higher rates of bariatric complications. Further studies are needed to confirm the reasons behind this.}
}
@article{SUN2022105034,
title = {A review of Earth Artificial Intelligence},
journal = {Computers & Geosciences},
volume = {159},
pages = {105034},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105034},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422000036},
author = {Ziheng Sun and Laura Sandoval and Robert Crystal-Ornelas and S. Mostafa Mousavi and Jinbo Wang and Cindy Lin and Nicoleta Cristea and Daniel Tong and Wendy Hawley Carande and Xiaogang Ma and Yuhan Rao and James A. Bednar and Amanda Tan and Jianwu Wang and Sanjay Purushotham and Thomas E. Gill and Julien Chastang and Daniel Howard and Benjamin Holt and Chandana Gangodagamage and Peisheng Zhao and Pablo Rivas and Zachary Chester and Javier Orduz and Aji John},
keywords = {Geosphere, Hydrology, Atmosphere, Artificial intelligence/machine learning, Big data, Cyberinfrastructure},
abstract = {In recent years, Earth system sciences are urgently calling for innovation on improving accuracy, enhancing model intelligence level, scaling up operation, and reducing costs in many subdomains amid the exponentially accumulated datasets and the promising artificial intelligence (AI) revolution in computer science. This paper presents work led by the NASA Earth Science Data Systems Working Groups and ESIP machine learning cluster to give a comprehensive overview of AI in Earth sciences. It holistically introduces the current status, technology, use cases, challenges, and opportunities, and provides all the levels of AI practitioners in geosciences with an overall big picture and to “blow away the fog to get a clearer vision” about the future development of Earth AI. The paper covers all the majorspheres in the Earth system and investigates representative AI research in each domain. Widely used AI algorithms and computing cyberinfrastructure are briefly introduced. The mandatory steps in a typical workflow of specializing AI to solve Earth scientific problems are decomposed and analyzed. Eventually, it concludes with the grand challenges and reveals the opportunities to give some guidance and pre-warnings on allocating resources wisely to achieve the ambitious Earth AI goals in the future.}
}
@article{TEMIZ2022102535,
title = {Open data: Lost opportunity or unrealized potential?},
journal = {Technovation},
volume = {114},
pages = {102535},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102535},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000827},
author = {Serdar Temiz and Marcus Holgersson and Joakim Björkdahl and Martin W. Wallin},
keywords = {Big data, Business model, Digitalization, Open data, Open innovation, Value capture, Value creation},
abstract = {The promise of open data is grand, but the results are often meager. To resolve this conundrum and make headway in the adoption of effective open data practices, we take a step back and investigate the underlying reasons for investing in open data. Based on survey results, interviews, and complementary evidence from secondary sources, we explore the motives and beliefs about open data investment expressed by open data experts in both public and private organizations. To our surprise, in both public and private organizations we find that open data investments are driven more by legitimacy-seeking than a quest to realize the value creation potential of open data. The results are worrisome, as such motives and beliefs do not necessarily lead to investment in the complementary assets needed to realize the potential associated with open data—instead, open data risks becoming a lost opportunity. Clearly, it's time to move beyond the open data hype and get down to business. Our paper provides insights for practice and calls on future research to unpack antecedents and mechanisms for value creation, and to identify appropriate complementary investments in open data, for example in terms of technologies, tools, and systems.}
}
@article{QUEST2022100167,
title = {A 3D indicator for guiding AI applications in the energy sector},
journal = {Energy and AI},
volume = {9},
pages = {100167},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100167},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000234},
author = {Hugo Quest and Marine Cauz and Fabian Heymann and Christian Rod and Lionel Perret and Christophe Ballif and Alessandro Virtuani and Nicolas Wyrsch},
keywords = {Artificial intelligence, Digitalisation, AI application, Big data, AI policy, Energy sector},
abstract = {The utilisation of Artificial Intelligence (AI) applications in the energy sector is gaining momentum, with increasingly intensive search for suitable, high-quality and trustworthy solutions that displayed promising results in research. The growing interest comes from decision makers of both the industry and policy domains, searching for applications to increase companies’ profitability, raise efficiency and facilitate the energy transition. This paper aims to provide a novel three-dimensional (3D) indicator for AI applications in the energy sector, based on their respective maturity level, regulatory risks and potential benefits. Case studies are used to exemplify the application of the 3D indicator, showcasing how the developed framework can be used to filter promising AI applications eligible for governmental funding or business development. In addition, the 3D indicator is used to rank AI applications considering different stakeholder preferences (risk-avoidance, profit-seeking, balanced). These results allow AI applications to be better categorised in the face of rapidly emerging national and intergovernmental AI strategies and regulations that constrain the use of AI applications in critical infrastructures.}
}
@article{HE20227724,
title = {A closed-loop data-fusion framework for air conditioning load prediction based on LBF},
journal = {Energy Reports},
volume = {8},
pages = {7724-7734},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.05.289},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722011337},
author = {Ning He and Liqiang Liu and Cheng Qian and Lijun Zhang and Ziqi Yang and Shang Li},
keywords = {Air conditioning load prediction, Long short-term memory, Back propagation neural network, Particle filter, Locally weighted scatterplot smoothing},
abstract = {Accurate air conditioning load prediction is a key component of intelligent building management system for ensuring energy saving and safe operation of air conditioning system. In order to improve the prediction accuracy, a particle filter (PF) load prediction fusion estimation method based on long short-term memory (LSTM) and back propagation neural network (BP) is proposed. Firstly, spearman correlation analysis is used to select the influencing factors with high correlation as feature input. Aiming at the problem that the original signal is easy to be disturbed by noise and the data features are not obvious, locally weighted scatterplot smoothing (LOWESS) method is used to denoise the data to improve the data quality for further accurate prediction. Secondly, the data-driven air conditioning load state-space representation is established, which takes air conditioning load as the state variable and takes the load features collected by the sensor in real-time as the input variables. Thirdly, combined with the space representation method of air conditioning load based on LSTM-BP, PF is introduced to estimate the air conditioning load by using the fusion model. Meanwhile, the output load value of BP is fed back to the fusion model as the observation value to update the state-space representation of air conditioning load. Finally, two practical cases are used to verify the effectiveness of the method. The results indicate that the proposed method can provide more accurate and robust air conditioning load prediction.}
}
@article{BILJECKI2022101809,
title = {Global Building Morphology Indicators},
journal = {Computers, Environment and Urban Systems},
volume = {95},
pages = {101809},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2022.101809},
url = {https://www.sciencedirect.com/science/article/pii/S0198971522000539},
author = {Filip Biljecki and Yoong Shin Chow},
keywords = {Urban planning, GIScience, Morphometrics, OpenStreetMap, GeoAI, Spatial analysis},
abstract = {Characterising and analysing urban morphology is a continuous task in urban data science, environmental analyses, and many other domains. As the availability and quality of data on them have been increasing, buildings have gained more attention. However, tools and data facilitating large-scale studies, together with an interdisciplinary consensus on metrics, remain scarce and often inadequate. We present Global Building Morphology Indicators (GBMI) — a three-pronged contribution addressing such shortcomings: (i) a comprehensive list of hundreds of building form multi-scale measures derived through a systematic literature review; (ii) a methodology and tool for the computation of these metrics in a database suited for big data and comparative studies, and release the code freely and open-source; and (iii) we carry out the computations using high performance computing, generating a public repository with data quantifying the form of selected urban areas around the world, and demonstrate their value with novel analyses comparing morphological parameters across cities. GBMI introduces a formalised, structured, modular, and extensible method to compute, manage, and disseminate urban indicators at a large scale and high resolution, while the precomputed dataset facilitates comparative studies. The theory and implementation traverse multiple scales: at the building level, both individual and contextual ones based on encircling buildings by multiple buffers, and aggregations at several hierarchical administrative levels and at multiple grids. Our open dataset, comprising billions of records on a growing scope of urban areas worldwide, is the most comprehensive instance of morphological data parametrising the individual building stock, supporting studies in urban analytics and a range of disciplines.}
}
@article{KACHA20224075,
title = {KAB: A new k-anonymity approach based on black hole algorithm},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {7},
pages = {4075-4088},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001002},
author = {Lynda Kacha and Abdelhafid Zitouni and Mahieddine Djoudi},
keywords = {Privacy, Anonymization, K-anonymity, Clustering, Black hole algorithm},
abstract = {K-anonymity is the most widely used approach to privacy preserving microdata which is mainly based on generalization. Although generalization-based k-anonymity approaches can achieve the privacy protection objective, they suffer from information loss. Clustering-based approaches have been successfully adapted for k-anonymization as they enhance the data quality, however, the computational complexity of finding an optimal solution has shown as NP-hard. Nature-inspired optimization algorithms are effective in finding solutions to complex problems. We propose, in this paper, a novel algorithm based on a simple nature-inspired metaheuristic called Black Hole Algorithm (BHA), to address such limitations. Experiments on real data set show that data utility has been improved by our approach compared to k-anonymity, BHA-based k-anonymity and clustering-based k-anonymity approaches.}
}
@article{ITO2022468,
title = {Improved root cause analysis supporting resilient production systems},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {468-478},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001273},
author = {Adriana Ito and Malin Hagström and Jon Bokrantz and Anders Skoogh and Mario Nawcki and Kanika Gandhi and Dag Bergsjö and Maja Bärring},
keywords = {Root cause analysis, Production disturbances, Production systems, Resilience},
abstract = {Manufacturing companies struggle to be efficient and effective when conducting root cause analyses of production disturbances; a fact which hinders them from creating and developing resilient production systems. This article aims to describe the challenges and enablers identified in current research relating to the different phases of root cause analysis. A systematic literature review was conducted, in which a total of 14 challenges and 17 enablers are identified and described. These correlate to the different phases of root cause analysis. Examples of challenges are “need for expertise”, “employee bias”, “poor data quality” and “lack of data integration”, among others. Examples of enablers are “visualisation tools”, “collaborative platforms”, “thesaurus” and “machine learning techniques”. Based on these findings, the authors also propose potential areas for further research and then design inputs for new solutions to improve root cause analysis. This article provides a theoretical contribution in that it describes the challenges and enablers of root cause analysis and their correlation to the creation of resilient production systems. The article also provides practical contributions, with an overview of current research to support practitioners in gaining insights into potential solutions to be implemented and further developed, with the aim of improving root cause analysis in production systems.}
}
@article{WANG2022103939,
title = {Zooming into mobility to understand cities: A review of mobility-driven urban studies},
journal = {Cities},
volume = {130},
pages = {103939},
year = {2022},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2022.103939},
url = {https://www.sciencedirect.com/science/article/pii/S026427512200378X},
author = {Ruoxi Wang and Xinyuan Zhang and Nan Li},
keywords = {Mobility, Conceptualization, Big geodata, Urban issue, Urban application, Review},
abstract = {Emerging big datasets about human mobility provide new and powerful ways of studying cities and addressing various urban issues. However, human mobility has usually been defined narrowly in prior research that limits the understanding of its values for urban applications. The aim of this study is to reveal the complexity and multiplicity of human mobility concept for various urban application scenarios, and present a comprehensive review of mobility-driven urban studies through four re-conceptualized urban mobility perspectives. Using a systematic review approach, existing mobility-driven urban studies are classified based on whether they interpret urban mobility as spatial movements, a social phenomenon, an economic indicator or a policy tool. Then, the core values of knowledge about urban mobility for addressing contemporary urban challenges are analyzed, and the current trends and future directions of mobility-driven urban studies are also discussed. Moving forward, the application of urban mobility knowledge can be further advanced by the evolution of mobility concepts, the improvement of mobility data quality and the innovation of mobility analytical methods. This review can contribute to the understanding the state of the art of mobility-driven urban studies, and provide inspiration and guidelines for studies of this area in the future.}
}
@article{CHAN2022112017,
title = {Development and performance evaluation of a chiller plant predictive operational control strategy by artificial intelligence},
journal = {Energy and Buildings},
volume = {262},
pages = {112017},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822001888},
author = {K.C. Chan and Victor T.T. Wong and Anthony K.F. Yow and P.L. Yuen and Christopher Y.H. Chao},
keywords = {Chiller plant optimization, Artificial intelligence, Artificial neural network, Particle swarm optimization, VSD chiller, Building energy saving},
abstract = {Traditionally, chiller plants are controlled and monitored by a predetermined control strategy to ensure appropriate operation based on the designed system configuration. With the use of new technology of variable speed drive (VSD) for compressors, smart control strategies could be leveraged to enhance the system efficiency in lieu of traditional control strategies. For example, using orderly and straightforward switching procedures without considering various factors in switching the units, including the high-efficiency partial load range benefitted from the VSD, the actual performance of the units as a whole and the variable chilled water flow rate, result in the chiller plant not operating at maximum performance and efficiency. To address these issues, a hybrid predictive operational chiller plant control strategy is proposed to optimize the performance of the chiller plant. Artificial intelligence is employed as the data mining algorithm, with big data analysis based on the actual acquired voluminous operation data by fully considering the characteristics of chiller plants without additional installation of large-sized and high-priced equipment. Artificial neural network (ANN) was employed in the control strategy to predict the future outdoor temperature, building cooling load demand and the corresponding power consumption of the chiller plants. At the same time, particle swarm optimization (PSO) was applied to search for the optimized setpoints, e.g., chilled water supply temperature, operating sequence, chilled water flow rate, for the chiller plants. The developed control strategy has been launched in a chiller plant with a cooling capacity of 7,700 kW installed in a hospital in Hong Kong. The system coefficient of performance (COP) and overall energy consumption of the chiller plants were enhanced by about 8.6% and reduced by about 7.9%, respectively, compared with the traditional control strategy. This real-time, continuous, automatic optimization control strategy can determine the most efficient combination of operating parameters of a chiller plant with different control settings. This ensures that the chiller plant operates in its most efficient mode year-round under various operational conditions.}
}
@article{HU2022123195,
title = {Industrial artificial intelligence based energy management system: Integrated framework for electricity load forecasting and fault prediction},
journal = {Energy},
volume = {244},
pages = {123195},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123195},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222000986},
author = {Yusha Hu and Jigeng Li and Mengna Hong and Jingzheng Ren and Yi Man},
keywords = {Electricity load, Dynamic forecasting model, Energy system analysis, Energy system optimisation, Artificial intelligence},
abstract = {Forecasting accuracy electricity load can help industrial enterprises optimise production scheduling based on peak and off-peak electricity prices. The electricity load forecasting results can be provided to an electricity system to improve electricity generation efficiency and minimize energy consumption by developing electricity generation plans in advance and by avoiding over or under the generation of electricity. However, because of the different informatization levels in different industries, few reliable intelligent electricity management systems are applied on the power supply side. Based on industrial big data and machine learning algorithms, this study proposes an integrated model to forecast short-term electricity load. The hybrid model based on the hybrid mode decomposition algorithms is proposed to decompose the total electricity load signal. To improve the generalisation ability of the forecasting model, a dynamic forecasting model is proposed based on the improved hybrid intelligent algorithm to forecast the short-term electricity load. The results show that the accuracy of the proposed dynamic integrated electricity load forecasting model is as high as 99%. The integrated framework could forecast abnormal electricity consumption in time and provide reliable evidence for production process scheduling.}
}
@article{BECKSCHULTE2022804,
title = {Digital Vehicle Protocol based on Distributed Ledger Technology in Production},
journal = {Procedia CIRP},
volume = {107},
pages = {804-809},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.066},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200350X},
author = {Sebastian Beckschulte and Louis Huebser and Raphael Kiesel and Robert H. Schmitt},
keywords = {Business model, digital twin, distributed ledger technology, production, protocol, value chain, vehicle},
abstract = {This paper describes the utilization of Distributed Ledger Technology (DLT) as means of a digital backbone – the so-called digital vehicle protocol – across the commercial vehicle industry. Enabling a digital vehicle protocol along the value chain resolves common data management problems, which are still the main inhibitor for advanced analytics methods and serves as a basis for new business models. This contribution demonstrates a per product-data centered approach in which low structured data can be written to and read from any point along the value chain on a product unit-based scope by using DLT. By embedding data post-processing pipelines per product-unit, data easily can be retrofitted to the task at hand. Therefore, it decouples data post-processing from data generation as well as overcoming current data silos within organizations. DLT hereby allows tying processing routines to data leading to a temper-proof digital vehicle protocol. Besides obtaining a stronger product-individual focus, our approach enables an easier integration of stakeholders into the entire business process landscape such as suppliers as well as sellers and leads to future business models such as billing depending on quality defects found during production. Our conceptual framework resolves current problems, i.e. data access, data quality and data processing costs. We align conceptual applicability with regards to a Truck Original Equipment Manufacturer (OEM) within the scope of failure management in production in order to specify implementation details, which reveal new business process models that further can transform commercial vehicle manufacturers from producers to service providers.}
}
@article{WANG2022107040,
title = {Toxicology of respiratory system: Profiling chemicals in PM10 for molecular targets and adverse outcomes},
journal = {Environment International},
volume = {159},
pages = {107040},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2021.107040},
url = {https://www.sciencedirect.com/science/article/pii/S0160412021006656},
author = {Junyu Wang and Yixia Zhang and Zhijun Zhang and Wancong Yu and Ang Li and Xin Gao and Danyu Lv and Huaize Zheng and Xiaohong Kou and Zhaohui Xue},
keywords = {PM, Respiratory system injury, Bioinformatics analysis, Molecular targets, Signaling pathway},
abstract = {Numerous studies have shown that the increasing trend of respiratory diseases have been closely associated with the endogenous toxic chemicals (polycyclic aromatic hydrocarbons, heavy metal ions, etc.) in PM10. In the present study, we aim to determine the strong correlations between the chemicals in PM10 and the adverse consequences. We used the ChemView DB, the ToxRef DB and a comprehensive literature analysis to collect, identify, and evaluate the chemicals in PM10 and their adverse effects on respiratory system, and then used the ToxCast DB to analyze their bioactivity and key targets through 1192 molecular targets and cell characteristic endpoints. Meanwhile, the bioinformatics analysis were carried out on the molecular targets to screen out prevention and treatment targets. A total of 310 chemicals related to the respiratory system were identified. An unsupervised two-directional heatmap was constructed based on hierarchical clustering of 227 chemicals by their effect scores. A subset of 253 chemicals with respiratory system toxicity had in vitro bioactivity on 318 molecular targets that could be described, clustered and annotated in the heatmap and bipartite network, which were analyzed based on the protein information in UniProt KB database and the software of GO, STRING, and KEGG. These results showed that the chemicals in PM10 have strong correlation with different types of respiratory system injury. The main pathways of respiratory system injury caused by PM10 are the Calcium signaling pathway, MAPK signaling pathway, and PI3K-AKT signaling pathway, and the core proteins in which are likely to be the molecular targets for the prevention and treatment of damage caused by PM10.}
}
@article{ZAMAN2022109355,
title = {Feature selection for online streaming high-dimensional data: A state-of-the-art review},
journal = {Applied Soft Computing},
volume = {127},
pages = {109355},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109355},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622005154},
author = {Ezzatul Akmal Kamaru Zaman and Azlinah Mohamed and Azlin Ahmad},
keywords = {Online feature selection, Streaming features, Feature relevancy, Feature redundancy, High-dimensional data, Feature drift},
abstract = {Knowledge discovery for data streaming requires online feature selection to reduce the complexity of real-world datasets and significantly improve the learning process. This is achieved by selecting highly relevant subsets and minimising irrelevant and redundant features. However, researchers have difficulties in addressing various forms of data. The goal of this article is to present a state-of-the-art review of feature subset selection based on the data form for the high-dimensional data used in online streaming. Through a systematic literature review assessing journal and conference papers from the past five years, detailed discussions on traditional feature selection and online feature selection were presented. Subsequently, a taxonomy of the challenges related to OFS provides a comprehensive review of state-of-the-art OFS and the benchmark methods. Several data forms were identified based on the extensive review: group stream, multi-label, capricious, imbalance, and feature drift. Using critical analysis, the evaluation metrics of online feature selection methods were compared from the perspectives of threshold initialisation, accuracy, high dimensionality, running time, relevancy, and redundancy for the optimal feature subset. An online feature selection framework was derived to illustrate the relationship between the application area, data form, online feature selection methods, evaluation metrics, and tools. Finally, the findings and potential directions for future research were thoroughly discussed. It is suggested that future researchers explore the derived framework and aim to advance each method.}
}
@article{SUDRE2022,
title = {A mega-analytic study of white matter microstructural differences across five cohorts of youth with attention deficit hyperactivity disorder},
journal = {Biological Psychiatry},
year = {2022},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2022.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0006322322016298},
author = {Gustavo Sudre and Luke Norman and Marine Bouyssi-Kobar and Jolie Price and Gauri Shastri and Philip Shaw},
keywords = {ADHD, diffusion tensor imaging, mega-analysis, white matter tracts, big data, fractional anisotropy},
abstract = {Background
While ADHD has been associated with differences in the structural connections formed by the brain’s white matter tracts, studies of such differences have returned inconsistent findings, likely reflecting small sample sizes. Thus, we conducted a mega-analysis on in vivo measures of white matter microstructure obtained through diffusion tensor imaging of over 6000 participants, from five cohorts.
Methods
In a mega-analysis, linear mixed models tested for associations between the fractional anisotropy of 42 white matter tracts and ADHD traits and diagnosis. Contrasts were made against measures of mood, anxiety, and other externalizing problems.
Findings
Overall, 6993 participants (between ages 6 to 18 years, mean 10.62 [SD 1.99]; 3,368 girls, 3,625 boys; 4146 white, non-Hispanic, 764 African American, 2083 other race/ethnicities) had either measures of ADHD and other emotional/behavioral symptoms (N=6933) and/or enough clinical data to allow a diagnosis of ADHD (N=951) or its absence (N=4884). Both the diagnosis and symptoms of ADHD were associated with lower fractional anisotropy of inferior longitudinal and left uncinate fasciculi (at FDR adjusted p<0.05). Associated effect sizes were small (the strongest association with ADHD traits had an effect size of partial-r=-0.14, while the largest case control difference was associated with an effect size of d=-0.3). Similar microstructural anomalies were not present for anxiety, mood, or externalizing problems. Findings held when ADHD cases and controls were matched on in-scanner motion.
Interpretation
While present across cohorts, ADHD-associated microstructural differences had small effects, underscoring the limited clinical utility of this imaging modality in isolation.}
}
@article{SOLTANI2022100016,
title = {Transfer learning from citizen science photographs enables plant species identification in UAV imagery},
journal = {ISPRS Open Journal of Photogrammetry and Remote Sensing},
volume = {5},
pages = {100016},
year = {2022},
issn = {2667-3932},
doi = {https://doi.org/10.1016/j.ophoto.2022.100016},
url = {https://www.sciencedirect.com/science/article/pii/S2667393222000059},
author = {Salim Soltani and Hannes Feilhauer and Robbert Duker and Teja Kattenborn},
keywords = {Remote sensing, Convolutional Neural Network (CNN), Crowd-sourced data, Plant species, Transfer learning, Drones},
abstract = {Accurate information on the spatial distribution of plant species and communities is in high demand for various fields of application, such as nature conservation, forestry, and agriculture. A series of studies has shown that Convolutional Neural Networks (CNNs) accurately predict plant species and communities in high-resolution remote sensing data, in particular with data at the centimeter scale acquired with Unoccupied Aerial Vehicles (UAV). However, such tasks often require ample training data, which is commonly generated in the field via geocoded in-situ observations or labeling remote sensing data through visual interpretation. Both approaches are laborious and can present a critical bottleneck for CNN applications. An alternative source of training data is given by using knowledge on the appearance of plants in the form of plant photographs from citizen science projects such as the iNaturalist database. Such crowd-sourced plant photographs typically exhibit very different perspectives and great heterogeneity in various aspects, yet the sheer volume of data could reveal great potential for application to bird’s eye views from remote sensing platforms. Here, we explore the potential of transfer learning from such a crowd-sourced data treasure to the remote sensing context. Therefore, we investigate firstly, if we can use crowd-sourced plant photographs for CNN training and subsequent mapping of plant species in high-resolution remote sensing imagery. Secondly, we test if the predictive performance can be increased by a priori selecting photographs that share a more similar perspective to the remote sensing data. We used two case studies to test our proposed approach with multiple RGB orthoimages acquired from UAV with the target plant species Fallopia japonica and Portulacaria afra respectively. Our results demonstrate that CNN models trained with heterogeneous, crowd-sourced plant photographs can indeed predict the target species in UAV orthoimages with surprising accuracy. Filtering the crowd-sourced photographs used for training by acquisition properties increased the predictive performance. This study demonstrates that citizen science data can effectively anticipate a common bottleneck for vegetation assessments and provides an example on how we can effectively harness the ever-increasing availability of crowd-sourced and big data for remote sensing applications.}
}
@article{LYU2022878,
title = {How accident causation theory can facilitate smart safety management: An application of the 24Model},
journal = {Process Safety and Environmental Protection},
volume = {162},
pages = {878-890},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.04.068},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022003846},
author = {Qian Lyu and Gui Fu and Yuxin Wang and Jing Li and Meng Han and Feng Peng and Chun Yang},
keywords = {Smart safety management, Solution design, Accident causation theory, 24Model},
abstract = {Smart safety management (SSM) in organizations is an inevitable trend in a more intelligent era. However, the adoption of safety science theory lags behind the application of intelligent technology in SSM, posing several challenges (functional dispersion, low-quality data, and lack of versatility). Thus, the accident causation theory (ACT) is adopted to address the existing problem. This study develops a conceptual framework for SSM using the 24Model, a popular ACT in China. The main work conducted in this study is summarized as follows: (a) the description of 24Model and its characteristics, as well as an analysis of its feasibility and applicability in SSM; (b) a detailed presentation of the functions, operation principle, and control paths of unsafe acts in the 24Model-based SSM framework; and (c) a discussion of the framework’s advantages, limitations in this research, and suggestions for future research. Research shows that the SSM framework based on the ACT can integrate the functions of the current SSM, establish management sustainability, enhance data quality, and ensure the versatility of the industry, which are the key factors that facilitate SSM. This study can offer a theoretical and practical basis for safety management in the intelligent era and provide implications for the application of the ACT.}
}
@incollection{KHAN2022,
title = {Bivariate, cluster, and suitability analysis of NoSQL solutions for big graph applications},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2022},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000590},
author = {Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam},
keywords = {NoSQL, Big data system, Storage solution, Bivariate analysis, Cluster analysis, Classification},
abstract = {With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.}
}
@article{SHARMA2022107217,
title = {Technological revolutions in smart farming: Current trends, challenges & future directions},
journal = {Computers and Electronics in Agriculture},
volume = {201},
pages = {107217},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107217},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005324},
author = {Vivek Sharma and Ashish Kumar Tripathi and Himanshu Mittal},
keywords = {Smart farming, Current Trends in smart farming, Precision agriculture, Agriculture 4.0, Machine Learning},
abstract = {With increasing population, the demand for agricultural productivity is rising to meet the goal of “Zero Hunger”. Consequently, farmers have optimized the agricultural activities in a sustainable way with the modern technologies. This integration has boosted the agriculture production due to high potentiality in assisting the farmers. The impulse towards the technological advancement has revived the traditional agriculture methods and resulted in eco-friendly, sustainable, and efficient farming. This has revolutionized the era of smart farming which primarily alliance with modern technologies like, big data, machine learning, deep learning, swarm intelligence, internet-of-things, block chain, robotics and autonomous system, cloud-fog-edge computing, cyber physical systems, and generative adversarial networks (GAN). To cater the same, a detailed survey on ten hot-spots of smart farming is presented in this paper. The survey covers the technology-wise state-of-the-art methods along with their application domains. Moreover, the publicly available data sets with existing research challenges are investigated. Lastly, the paper concludes with suggestions to the identified problems and possible future research directions.}
}
@article{RIBEIRO2022109674,
title = {Issues with species occurrence data and their impact on extinction risk assessments},
journal = {Biological Conservation},
volume = {273},
pages = {109674},
year = {2022},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2022.109674},
url = {https://www.sciencedirect.com/science/article/pii/S0006320722002270},
author = {Bruno R. Ribeiro and Karlo Guidoni-Martins and Geiziane Tessarolo and Santiago José Elías Velazco and Lucas Jardim and Steven P. Bachman and Rafael Loyola},
keywords = {Biodiversity data, Fitness-for-use, Data quality, GBIF, Plants, Rapid extinction risk assessment},
abstract = {Species extinction risk status is critical to support conservation actions. However, full assessments published on the Red List are slow and resource intensive. To tackle assessments for mega-diverse groups, gains can be made through preliminary assessments that can help prioritize efforts toward full assessments. Here, we quantified how incomplete data collation and errors in the taxonomic, spatial, and temporal dimensions of species-occurrence data translate into misclassifications of extinction risk. Using a dataset of >30 million records of terrestrial plants occurring in Brazil compiled from nine databases we conducted preliminary risk assessments for ~94 % of the 6046 species assessed by the Brazilian Red List authority. We found that no unique database contained data sufficient to perform extinction risk assessment of all species; e.g., the risk of 78 % of species can be assessed using data from GBIF. The overall accuracy (66–75 %) and specificity (89–98 %, correct prediction of non-threatened species) were less affected by incomplete data collation and issues in species-occurrence records. Sensitivity rates (correct prediction of threatened species) were commonly low to moderate and strongly affected by incomplete data collation (13–47 %) and spatial issues (38 %). Our results demonstrate that species' preliminary risk assessments have high accuracy in identifying non-threatened species, even when data collection is low and in the presence of issues in species occurrence data highlighting that such an approach can be used to efficiently prioritize species for full Red List assessments. In addition, caution is needed before declaring a species as threatened without considering data collation intensity and quality.}
}
@article{LI2022,
title = {Artificial intelligence in radiotherapy},
journal = {Seminars in Cancer Biology},
year = {2022},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2022.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X22001912},
author = {Guangqi Li and Xin Wu and Xuelei Ma},
keywords = {Artificial intelligence, Radiotherapy, Auto-segmentation, Auto-planning, Quality assurance},
abstract = {Radiotherapy is a discipline closely integrated with computer science. Artificial intelligence (AI) has developed rapidly over the past few years. With the explosive growth of medical big data, AI promises to revolutionize the field of radiotherapy through highly automated workflow, enhanced quality assurance, improved regional balances of expert experiences, and individualized treatment guided by multi-omics. In addition to independent researchers, the increasing number of large databases, biobanks, and open challenges significantly facilitated AI studies on radiation oncology. This article reviews the latest research, clinical applications, and challenges of AI in each part of radiotherapy including image processing, contouring, planning, quality assurance, motion management, and outcome prediction. By summarizing cutting-edge findings and challenges, we aim to inspire researchers to explore more future possibilities and accelerate the arrival of AI radiotherapy.}
}
@article{MERINO2022217,
title = {Lessons learned from an IoT deployment for condition monitoring at the Port of Felixstowe},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {217-222},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.210},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014288},
author = {Jorge Merino and Manu Sasidharan and Manuel Herrera and Hang Zhou and Adolfo {Crespo del Castillo} and Ajith K. Parlikad and Richard Brooks and Karen Poulter},
keywords = {IoT, Sensors, Monitoring, Data quality, Data management, 5G, Smart ports},
abstract = {The ports sector is critical to global trade. While digitalisation of infrastructure asset management in other sectors such as manufacturing, healthcare, water supply, railway and road is rapidly growing with the possibilities of the Internet of Things (IoT) solutions, the maritime industry lags significantly behind. IoT solutions and the near real-time data they produce provide new impetus to improve fault diagnosis of assets and prevent disruptions caused due to asset breakdown. Such solutions also require reliable communication systems to support low latency and high bandwidth. To this end, we are building an IoT-based asset management solution at the Port of Felixstowe, the UK's largest container port, using 5G technology. This paper presents the steps taken, challenges faced and the lessons learned with sourcing, installation, calibration and communication of sensors in this deployment.}
}
@article{CHAUHAN2022121508,
title = {Linking circular economy and digitalisation technologies: A systematic literature review of past achievements and future promises},
journal = {Technological Forecasting and Social Change},
volume = {177},
pages = {121508},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121508},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522000403},
author = {Chetna Chauhan and Vinit Parida and Amandeep Dhir},
keywords = {Circular economy, Sustainability, Product-service system (PSS), Circular business model, Artificial intelligence, Internet of things},
abstract = {The circular economy (CE) has the potential to capitalise upon emerging digital technologies, such as big data, artificial intelligence (AI), blockchain and the Internet of things (IoT), amongst others. These digital technologies combined with business model innovation are deemed to provide solutions to myriad problems in the world, including those related to circular economy transformation. Given the societal and practical importance of CE and digitalisation, last decade has witnessed a significant increase in academic publication on these topics. Therefore, this study aims to capture the essence of the scholarly work at the intersection of the CE and digital technologies. A detailed analysis of the literature based on emerging themes was conducted with a focus on illuminating the path of CE implementation. The results reveal that IoT and AI play a key role in the transition towards the CE. A multitude of studies focus on barriers to digitalisation-led CE transition and highlight policy-related issues, the lack of predictability, psychological issues and information vulnerability as some important barriers. In addition, product-service system (PSS) has been acknowledged as an important business model innovation for achieving the digitalisation enabled CE. Through a detailed assessment of the existing literature, a viable systems-based framework for digitalisation enabled CE has been developed which show the literature linkages amongst the emerging research streams and provide novel insights regarding the realisation of CE benefits.}
}
@article{SHARMA2022100383,
title = {Digital Twins: State of the art theory and practice, challenges, and open research questions},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100383},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000516},
author = {Angira Sharma and Edward Kosasih and Jie Zhang and Alexandra Brintrup and Anisoara Calinescu},
keywords = {Digital Twin, Internet of Things, Autonomous systems, Big data, Machine learning},
abstract = {Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation, optimisation and accurate forecasting. However, the theoretical framework and practical implementations of digital twin (DT) are yet to fully achieve this vision at scale. Although an increasing number of successful implementations exist in research and industrial works, sufficient implementation details are not publicly available, making it difficult to fully assess their components and effectiveness, to draw comparisons, identify successful solutions, share lessons, and thus to jointly advance and benefit from the DT methodology. This work first presents a review of relevant DT research and industrial works, focusing on the key DT features, current approaches in different domains, and successful DT implementations, to infer the key DT components and properties, and to identify current limitations and reasons behind the delay in the widespread implementation and adoption of digital twin. This work identifies that the major reasons for this delay are: the fact the DT is still a fast evolving concept; the lack of a universal DT reference framework, e.g. DT standards are scarce and still evolving; problem- and domain-dependence; security concerns over shared data; lack of DT performance metrics; and reliance of digital twin on other fast-evolving technologies. Advancements in machine learning, Internet of Things (IoT) and big data have led to significant improvements in DT features such as real-time monitoring and accurate forecasting. Despite this progress and individual company-based efforts, certain research and implementation gaps exist in the field, which have so far prevented the widespread adoption of the DT concept and technology; these gaps are also discussed in this work. Based on reviews of past work and the identified gaps, this work then defines a conceptualisation of DT which includes its components and properties; these also validate the uniqueness of DT as a concept, when compared to similar concepts such as simulation, autonomous systems and optimisation. Real-life case studies are used to showcase the application of the conceptualisation. This work discusses the state-of-the-art in DT, addresses relevant and timely DT questions, and identifies novel research questions, thus contributing to a better understanding of the DT paradigm and advancing the theory and practice of DT and its allied technologies.}
}
@article{CONDE2022101723,
title = {Applying digital twins for the management of information in turnaround event operations in commercial airports},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101723},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101723},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001811},
author = {Javier Conde and Andres Munoz-Arcentales and Mario Romero and Javier Rojo and Joaquín Salvachúa and Gabriel Huecas and Álvaro Alonso},
keywords = {Aviation, Flight turnaround events, Digital twin, Internet of Things, Data modelling, Big data},
abstract = {The aerospace sector is one of the many sectors in which large amounts of data are generated. Thanks to the evolution of technology, these data can be exploited in several ways to improve the operation and management of industrial processes. However, to achieve this goal, it is necessary to define architectures and data models that allow to manage and homogenise the heterogeneous data collected. In this paper, we present an Airport Digital Twin Reference Conceptualisation’s and data model based on FIWARE Generic Enablers and the Next Generation Service Interfaces-Linked Data standard. Concretely, we particularise the Airport Digital Twin to improve the efficiency of flight turnaround events. The architecture proposed is validated in the Aberdeen International Airport with the aim of reducing delays in commercial flights. The implementation includes an application that shows the real state of the airport, combining two-dimensional and three-dimensional virtual reality representations of the stands, and a mobile application that helps ground operators to schedule departure and arrival flights.}
}
@article{HUSEIEN2022100116,
title = {A review on 5G technology for smart energy management and smart buildings in Singapore},
journal = {Energy and AI},
volume = {7},
pages = {100116},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100116},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000653},
author = {Ghasan Fahim Huseien and Kwok Wei Shah},
keywords = {5G technology, Sustainability, Smart building, Facilities management, Build environment},
abstract = {Sustainable and smart building is a recent concept that is gaining momentum in public opinion, and thus, it is making its way into the agendas of researchers and city authorities all over the world. To move towards sustainable development goals, 5G technology would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities. It's well known that the Singapore is one of top smart cities in this world and from the first counties that adopted of 5G technology in various sectors including smart buildings. Based on these facts, this paper discusses the international trends in 5G applications for smart buildings, and R&D and test bedding works conducted in 5G labs. As well as, the manuscript widely reviewed and discussed the 5G technology development, use cases, applications and future projects which supported by Singapore government. Finally, the 5G use cases for smart buildings and build environment improvement application were discussed. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{SKARPATHIOTAKI2022100274,
title = {Cross-Industry Process Standardization for Text Analytics},
journal = {Big Data Research},
volume = {27},
pages = {100274},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100274},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000915},
author = {Christina G. Skarpathiotaki and Konstantinos E. Psannis},
keywords = {Big data analytics, Advanced analysis, Artificial intelligence, Machine learning, Text analytics, Cross-industry processes},
abstract = {We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.}
}
@article{BOOMGARDZAGRODNIK2022106580,
title = {Machine learning imputation of missing Mesonet temperature observations},
journal = {Computers and Electronics in Agriculture},
volume = {192},
pages = {106580},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106580},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921005974},
author = {Joseph P. Boomgard-Zagrodnik and David J. Brown},
keywords = {Machine learning, Big data, Surface weather observations, Degree day models, Missing data imputation},
abstract = {Uninterrupted and reliable weather data is a necessary foundation for agricultural decision making, required for models based on accumulated growing degree days (GDD), chill units, and evapotranspiration. When a weather station experiences a mechanical or communications failure, a replacement (imputed) value should be substituted for any missing data. This study introduces a machine learning, network-based approach to imputing missing 15-minute and daily maximum/minimum air temperature observations from 8.5 years of air temperature, relative humidity, wind, and solar radiation observations at 134 AgWeatherNet (AWN) stations in Washington State. A random forest imputation model trained on temperature and humidity observations from the full network predicted 15-minute, daily maximum, and daily minimum temperature values with mean absolute errors of 0.43 °C, 0.53 °C, and 0.70 °C, respectively. Sensitivity experiments determined that imputation skill was related a number of external factors including volume and type of training data, proximity of surrounding stations, and regional topography. In particular, nocturnal cold air flows in the upper Yakima Valley of south-central Washington caused temperature to be less correlated with surrounding stations in the overnight hours. In a separate experiment, the imputation model was used to predict base- 10 °C GDD on 2020 July 1 trained entirely on 15-minute station data from previous years. Even with the entire season of observations missing, the model predicted the GDD value within an average error 1.4% with 125 of 134 stations within 5% of observations. Since missing data can typically be resolved within a timeframe of a few days, the network-based imputation model is a sufficient substitute for short periods of missing observational weather data. Other potential applications of an imputation model are briefly discussed.}
}
@article{BURNS2022420,
title = {Real-World Evidence for Regulatory Decision-Making: Guidance From Around the World},
journal = {Clinical Therapeutics},
volume = {44},
number = {3},
pages = {420-437},
year = {2022},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2022.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0149291822000170},
author = {Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell},
keywords = {Efficiency, Product effectiveness, Real-world evidence, Regulatory decision making},
abstract = {Purpose
Interest in leveraging real-world evidence (RWE) to support regulatory decision making for product effectiveness has been increasing globally as evident by the increasing number of regulatory frameworks and guidance documents. However, acceptance of RWE, especially before marketing for regulatory approval, differs across countries. In addition, guidance on the design and conduct of innovative clinical trials, such as randomized controlled registry studies, pragmatic trials, and other hybrid studies, is lacking.
Methods
We assessed the global regulatory environment with regard to RWE based on regional availability of the following 3 key regulatory elements: (1) RWE regulatory framework, (2) data quality and standards guidance. and (3) study methods guidance.
Findings
This article reviews the available frameworks and existing guidance from across the globe and discusses the observed gaps and opportunities for further development and harmonization.
Implications
Cross-country collaborations are encouraged to further shape and align RWE policies and help establish frameworks in countries without current policies with the goal of creating efficiencies when considering RWE to support regulatory decision-making globally.}
}
@article{KIRTLEY2022243,
title = {Translating promise into practice: a review of machine learning in suicide research and prevention},
journal = {The Lancet Psychiatry},
volume = {9},
number = {3},
pages = {243-252},
year = {2022},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(21)00254-6},
url = {https://www.sciencedirect.com/science/article/pii/S2215036621002546},
author = {Olivia J Kirtley and Kasper {van Mens} and Mark Hoogendoorn and Navneet Kapur and Derek {de Beurs}},
abstract = {Summary
In ever more pressured health-care systems, technological solutions offering scalability of care and better resource targeting are appealing. Research on machine learning as a technique for identifying individuals at risk of suicidal ideation, suicide attempts, and death has grown rapidly. This research often places great emphasis on the promise of machine learning for preventing suicide, but overlooks the practical, clinical implementation issues that might preclude delivering on such a promise. In this Review, we synthesise the broad empirical and review literature on electronic health record-based machine learning in suicide research, and focus on matters of crucial importance for implementation of machine learning in clinical practice. The challenge of preventing statistically rare outcomes is well known; progress requires tackling data quality, transparency, and ethical issues. In the future, machine learning models might be explored as methods to enable targeting of interventions to specific individuals depending upon their level of need—ie, for precision medicine. Primarily, however, the promise of machine learning for suicide prevention is limited by the scarcity of high-quality scalable interventions available to individuals identified by machine learning as being at risk of suicide.}
}
@article{FU2022128312,
title = {Extracting historical flood locations from news media data by the named entity recognition (NER) model to assess urban flood susceptibility},
journal = {Journal of Hydrology},
volume = {612},
pages = {128312},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.128312},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422008848},
author = {Shengnan Fu and Heng Lyu and Ze Wang and Xin Hao and Chi Zhang},
keywords = {Urban flood susceptibility, News media data, Flood locations, Named entity recognition, Data quality control},
abstract = {Flood susceptibility assessment for identifying flood-prone areas plays a significant role in flood hazard mitigation. Machine learning is an optional assessment method because of its high objectivity and computational efficiency, but how to get enough and accurate information of historical flood locations to train the machine learning models has been a key problem. In recent years, news media data from both news websites and social media accounts has emerged as a promising source for natural science studies. However, the application of news media data in urban flood susceptibility assessment is still inadequate. This study proposed an approach to fill this gap. Firstly, flood locations were extracted from news media data based on a named entity recognition (NER) model. Then, a frequency or distance-based data quality control method was employed to improve the representativeness of the extracted flooded locations. Finally, flood conditioning factors with information of historical flood locations were input into a Support Vector Machine (SVM) model for flood susceptibility assessment. We took the central city of Dalian, China as a case study. The T-test results show that there was no significant difference between the distributions of most flood conditioning factors at the flood locations from the news media data and the official planning report. In the obtained flood susceptibility map, the high flood susceptibility areas got a recall of 90% compared with the high flood hazard areas in the planning report. Performing data quality control in the frequency-based method can improve the precision of the flood susceptibility map by up to 5%, while the distance-based method is ineffective. This study provides an example and offers the value of applying new data sources and modern deep learning techniques for urban flood management.}
}
@article{GACUTAN2022150742,
title = {Continental patterns in marine debris revealed by a decade of citizen science},
journal = {Science of The Total Environment},
volume = {807},
pages = {150742},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.150742},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721058204},
author = {Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark},
keywords = {Environmental monitoring, Plastic pollution, Bioregional management, Litter, Citizen science, Marine debris},
abstract = {Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.}
}
@article{WANG2022105028,
title = {Principles, research status, and prospects of feature engineering for data-driven building energy prediction: A comprehensive review},
journal = {Journal of Building Engineering},
volume = {58},
pages = {105028},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.105028},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222010385},
author = {Zeyu Wang and Lisha Xia and Hongping Yuan and Ravi S. Srinivasan and Xiangnan Song},
keywords = {Feature engineering, Feature construction, Feature selection, Feature extraction, Building energy prediction, Machine learning},
abstract = {With the rapid growth in the volume of relevant and available data, feature engineering is emerging as a popular research subject in data-driven building energy prediction owing to its essential role in improving data quality. Many studies have examined the feasibility of applying feature engineering methods to data-driven building energy prediction. However, a systematic review of this area's research status, characteristics, and limitations is lacking. Therefore, this study analyzes the current status of research and directions of future work in feature engineering for building energy prediction. In this article, we first discuss the concept of feature engineering and its main methods, including the construction, selection, and extraction of features. We, then, summarize the status and characteristics of feature engineering research in the building energy domain using a comprehensive study of 172 relevant articles. We also discuss critical issues in feature engineering in data-driven building energy prediction, including why feature engineering has recently received increasing attention, whether it is useful in this domain, and effective ways to apply it. Finally, we identify promising research directions in the area based on its current state and limitations. The results here provide researchers and the industry with a better understanding of the state of the art and future research trends in feature engineering for data-driven building energy prediction.}
}
@article{WEVER2022102882,
title = {Designing early warning systems for detecting systemic risk: A case study and discussion},
journal = {Futures},
volume = {136},
pages = {102882},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2021.102882},
url = {https://www.sciencedirect.com/science/article/pii/S0016328721001919},
author = {Mark Wever and Munir Shah and Niall O’Leary},
keywords = {Systemic risk, Early warning system, Risk detection, Design process, Artificial intelligence, Complex systems},
abstract = {Systemic risks are potential trigger events or developments that could undermine the viability of entire networks or systems. Growing complexity in systems make such risks both more likely to occur and more difficult to anticipate. The tools for detecting systemic risk have not kept pace with these challenges; traditional methods are too intermittent, too slow, and too narrow in focus for timely systemic risk detection. However, recent developments in big data analysis and artificial intelligence (AI) have the potential to revolutionize Early Warning Systems (EWSs) for detecting systemic risk. EWSs that are supported by these technologies could provide users with earlier warning signals of a wider range of risks and more up-to-date measures of the fragility of the system against these risks. This area of research is nascent and lacks a robust methodology for designing such EWSs. Addressing this issue, the present paper: 1) identifies the characteristics of competent EWSs; 2) outlines an approach for designing such EWSs; and 3) illustrates the value of this approach, by discussing the conceptual design of an EWS for detecting biosecurity incursions in the New Zealand pastoral industries.}
}
@article{PLAZAS2022101971,
title = {Sense, Transform & Send for the Internet of Things (STS4IoT): UML profile for data-centric IoT applications},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {101971},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101971},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000926},
author = {Julian Eduardo Plazas and Sandro Bimonte and Michel Schneider and Christophe {de Vaulx} and Pietro Battistoni and Monica Sebillo and Juan Carlos Corrales},
keywords = {Data-centric conceptual modelling, Model-driven architecture, Automatic code generation, Internet of Things},
abstract = {The Internet of Things is currently one of the most representative sources of Big Data. It can acquire real-time data from multiple spatially distributed points, allowing for the extraction of valuable insights. However, an appropriate integration, processing, and analysis of these data depends on several factors starting from the correct definition of the information systems. This paper introduces STS4IoT, a UML profile and automatic code-generation tool for model-driven IoT, to address this issue. STS4IoT allows designing and implementing an IoT application from the required data only, bridging the gaps between the IoT and database design worlds. The IoT data design includes both different in-network transformations and the join of streams from multiple sources. Besides, it follows the Model-Driven Architecture (MDA) guidelines to provide abstraction levels oriented to the different roles participating in the application design. The STS4IoT validation shows it has an excellent structure and is highly understandable. Its instance models are well-formed, highly abstract and readable. And the automatic implementation tool can generate complete code for complex real-world applications involving multiple IoT devices. Then, STS4IoT simplifies the definition and development of IoT applications and their integration into other information systems, such as stream data warehouses.}
}
@article{GONZALEZVIDAL2022328,
title = {Intrinsic and extrinsic quality of data for open data repositories},
journal = {ICT Express},
volume = {8},
number = {3},
pages = {328-333},
year = {2022},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S240595952200090X},
author = {Aurora González-Vidal and Alfonso P. Ramallo-González and Antonio F. Skarmeta},
keywords = {Data quality, Open data, IoT, Machine learning},
abstract = {This work assesses the quality of Internet of Things data not only as an intrinsic quality on how well it represents the related phenomenon but also, on how much information it contains to educate an artificial entity. The quality metrics here proposed are tested with real datasets. Also, they are implemented on OpenCPU, so the open data repositories can use them off-the-shelf to rate their datasets without computational cost and minimum human intervention, making them more attractive to potential users and gaining visibility and impact.}
}
@article{CHANG2022235,
title = {Information Quality for Effective Asset Management: A literature review},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {235-240},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.213},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014318},
author = {Janet Y. Chang and Jorge Merino Garcia and Xiang Xie and Nicola Moretti and Ajith Parlikad},
keywords = {Information Quality, Data Quality, Asset Information Management, Asset Management, Building Information Modelling},
abstract = {Information quality is critical to successful asset management decision-making. Substandard quality information will likely cause significant negative short- and long-term consequences. The ongoing digital transformation in the Architecture, Engineering, and Construction (AEC) industry has influenced ways to manage physical assets. Yet many asset owners lack a clear understanding of identifying indispensable quality dimensions that satisfy the business, system, and technical requirements. This paper aims to comprehensively analyse asset information quality management with a systematic literature review. The study reveals that the quality dimension of ‘accuracy’ alone cannot support various asset management functions. Additionally, quality deficiencies remain in Building Information Modelling (BIM)-based project delivery handover documents, establishing insufficient asset baselines for future planning. Moreover, the limited knowledge of the quality complications of information generated through technical solutions suggests additional work is required to gain insights into vital quality dimensions. The findings of this study underpin the basis for classifying quality dimensions to support essential asset management processes while pointing to future study areas.}
}
@article{DADEBAYEV20224385,
title = {EEG-based emotion recognition: Review of commercial EEG devices and machine learning techniques},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {7},
pages = {4385-4401},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821000732},
author = {Didar Dadebayev and Wei Wei Goh and Ee Xion Tan},
keywords = {Electroencephalography, Emotion recognition, Consumer-grade EEG, Machine learning, Classification},
abstract = {Emotion recognition based on electroencephalography (EEG) signal features is now one of the booming big data research areas. As the number of commercial EEG devices in the current market increases, there is a need to understand current trends and provide researchers and young practitioners with insights into future investigations of emotion recognition systems. This paper aims to evaluate popular consumer-grade EEG devices’ status and review relevant studies that examined the reliability of these low-cost devices for emotion recognition over the last five years. Additionally, a comparison with research-grade devices is conducted. This paper also highlights EEG-based emotion recognition research’s key areas, including different feature extraction capabilities, characteristics, and machine learning algorithms. Finally, the main challenges for building an EEG-based emotion recognition system, focusing on the data collection process with commercial EEG devices and machine learning algorithms’ performance, are presented.}
}
@article{SUN2022191,
title = {Advances in optical phenotyping of cereal crops},
journal = {Trends in Plant Science},
volume = {27},
number = {2},
pages = {191-208},
year = {2022},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2021.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S1360138521002028},
author = {Dawei Sun and Kelly Robbins and Nicolas Morales and Qingyao Shu and Haiyan Cen},
keywords = {cereal crops, high-throughput phenotyping, optical sensors, traits},
abstract = {Optical sensors and sensing-based phenotyping techniques have become mainstream approaches in high-throughput phenotyping for improving trait selection and genetic gains in crops. We review recent progress and contemporary applications of optical sensing-based phenotyping (OSP) techniques in cereal crops and highlight optical sensing principles for spectral response and sensor specifications. Further, we group phenotypic traits determined by OSP into four categories – morphological, biochemical, physiological, and performance traits – and illustrate appropriate sensors for each extraction. In addition to the current status, we discuss the challenges of OSP and provide possible solutions. We propose that optical sensing-based traits need to be explored further, and that standardization of the language of phenotyping and worldwide collaboration between phenotyping researchers and other fields need to be established.}
}
@article{ESKANDARITORBAGHAN2022106543,
title = {Understanding the potential of emerging digital technologies for improving road safety},
journal = {Accident Analysis & Prevention},
volume = {166},
pages = {106543},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106543},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521005741},
author = {Mehran {Eskandari Torbaghan} and Manu Sasidharan and Louise Reardon and Leila C.W. Muchanga-Hvelplund},
keywords = {Safety, Road, Transport, Digital technology, Information},
abstract = {Each year, 1.35 million people are killed on the world’s roads and another 20–50 million are seriously injured. Morbidity or serious injury from road traffic collisions is estimated to increase to 265 million people between 2015 and 2030. Current road safety management systems rely heavily on manual data collection, visual inspection and subjective expert judgment for their effectiveness, which is costly, time-consuming, and sometimes ineffective due to under-reporting and the poor quality of the data. A range of innovations offers the potential to provide more comprehensive and effective data collection and analysis to improve road safety. However, there has been no systematic analysis of this evidence base. To this end, this paper provides a systematic review of the state of the art. It identifies that digital technologies - Artificial Intelligence (AI), Machine-Learning, Image-Processing, Internet-of-Things (IoT), Smartphone applications, Geographic Information System (GIS), Global Positioning System (GPS), Drones, Social Media, Virtual-reality, Simulator, Radar, Sensor, Big Data – provide useful means for identifying and providing information on road safety factors including road user behaviour, road characteristics and operational environment. Moreover, the results show that digital technologies such as AI, Image processing and IoT have been widely applied to enhance road safety, due to their ability to automatically capture and analyse data while preventing the possibility of human error. However, a key gap in the literature remains their effectiveness in real-world environments. This limits their potential to be utilised by policymakers and practitioners.}
}
@article{GARRIDO2022103465,
title = {Revealing the landscape of privacy-enhancing technologies in the context of data markets for the IoT: A systematic literature review},
journal = {Journal of Network and Computer Applications},
volume = {207},
pages = {103465},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103465},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001126},
author = {Gonzalo Munilla Garrido and Johannes Sedlmeir and Ömer Uludağ and Ilias Soto Alaoui and Andre Luckow and Florian Matthes},
keywords = {Anonymization, Big data, Copy problem, Data exchange, Marketplace, Platform, Secure computation},
abstract = {IoT data markets in public and private institutions have become increasingly relevant in recent years because of their potential to improve data availability and unlock new business models. However, exchanging data in markets bears considerable challenges related to disclosing sensitive information. Despite considerable research focused on different aspects of privacy-enhancing data markets for the IoT, none of the solutions proposed so far seems to find a practical adoption. Thus, this study aims to organize the state-of-the-art solutions, analyze and scope the technologies that have been suggested in this context, and structure the remaining challenges to determine areas where future research is required. To accomplish this goal, we conducted a systematic literature review on privacy enhancement in data markets for the IoT, covering 50 publications dated up to July 2020, and provided updates with 24 publications dated up to May 2022. Our results indicate that most research in this area has emerged only recently, and no IoT data market architecture has established itself as canonical. Existing solutions frequently lack the required combination of anonymization and secure computation technologies. Furthermore, there is no consensus on the appropriate use of blockchain technology for IoT data markets and a low degree of leveraging existing libraries or reusing generic data market architectures. We also identified significant challenges remaining, such as the copy problem and the recursive enforcement problem that – while solutions have been suggested to some extent – are often not sufficiently addressed in proposed designs. We conclude that privacy-enhancing technologies need further improvements to positively impact data markets so that, ultimately, the value of data is preserved through data scarcity and users’ privacy and businesses-critical information are protected.}
}
@article{MOSAVI2022503,
title = {Intelligent energy management using data mining techniques at Bosch Car Multimedia Portugal facilities},
journal = {Procedia Computer Science},
volume = {201},
pages = {503-510},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.065},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004781},
author = {Nasim Sadat Mosavi and Francisco Freitas and Rogério Pires and César Rodrigues and Isabel Silva and Manuel Santos and Paulo Novais},
keywords = {Energy consumption, Prediction, Optimization, Data Mining, Machine Learning, Forecasting, Industry 4.0},
abstract = {The fusion of emerged technologies such as Artificial Intelligence, cloud computing, big data, and the Internet of Things in manufacturing has pioneered this industry to meet the fourth stage of the industrial revolution (industry 4.0). One major approach to keeping this sector sustainable and productive is intelligent energy demand planning. Monitoring and controlling the consumption of energy under industry 4.0, directly results in minimizing the cost of operation and maximizing efficiency. To advance the research on the adoption of industry 4.0, this study examines CRISP-DM methodology to project data mining approach over data from 2020 to 2021 which was collected from industrial sensors to predict/forecast future electrical consumption at Bosch car multimedia facilities located at Braga, Portugal. Moreover, the influence of indicators such as humidity and temperature on electrical energy consumption was investigated. This study employed five promising regression algorithms and FaceBook prophet (FB prophet) to apply over data belonging to two HVAC (heating, ventilation, and air conditioning) sensors (E333, 3260). Results indicate Random Forest (RF) algorithms as a potential regression approach for prediction and the outcome of FB prophet to forecast the demand of future usage of electrical energy associated with HVAC presented. Based on that, it was concluded that predicting the usage of electrical energy for both data points requires time series techniques. Where “timestamp” was identified as the most effective feature to predict consume of electrical energy by regression technique (RF). The result of this study was integrated with Intelligent Industrial Management System (IIMS) at Bosch Portugal.}
}
@article{MATHERI2022103152,
title = {Sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant},
journal = {Physics and Chemistry of the Earth, Parts A/B/C},
volume = {126},
pages = {103152},
year = {2022},
issn = {1474-7065},
doi = {https://doi.org/10.1016/j.pce.2022.103152},
url = {https://www.sciencedirect.com/science/article/pii/S1474706522000468},
author = {Anthony Njuguna Matheri and Belaid Mohamed and Freeman Ntuli and Esther Nabadda and Jane Catherine Ngila},
keywords = {Circular bioeconomy, Data pipeline, Digital twin, Process design, Sensor, Wastewater treatment},
abstract = {Rapid urbanization, population increase, emerging contaminants and increasing water scarcity have put a major constraint on the wastewater treatment system. Scarcity of water is steering current way of water recycle, and the drive focus towards resource recovery. Zero waste pathway in circular bioeconomy can bring transformation of wastewater commercialization by adding value with resource recovery. The complex biological reactions, unforeseen microbial behaviours, lack of reliable on-line instrumentation, complex modelling, lack of visualize techniques, low-quality industrial measurements and highly time-varying intensive data-driven operations call for the intelligence techniques and operations. The study is a review of sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant. Water surveillance and monitoring, circular economy and sustainability, automation pyramid, digital transformation, artificial intelligence, data pipeline, digital twin, data mining, and data-driven visualization, cyber-physical systems and water-energy-health management were reviewed. The deployment of the digital systems has evidently proven to bridges the gap between the data-driven soft sensor, operation and control systems in WWTP. Accurate prediction of the WWTP variables can support process design and control, reduce operation cost, improve system reliability, predictive maintenance and troubleshooting, increase water quality, increase stakeholder's engagement and endorse optimization of the plant performance. This procures the best compliance with international standards and diversification. The inclusion of life cycle environmental or cost management technologies in optimization models is an interesting pathway towards sustainable water treatment in-line with sustainable development goals, circular bioeconomy and industry 4.0.}
}
@article{SKAF2022104082,
title = {Topological data analysis in biomedicine: A review},
journal = {Journal of Biomedical Informatics},
volume = {130},
pages = {104082},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104082},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000983},
author = {Yara Skaf and Reinhard Laubenbacher},
keywords = {Biomedical informatics, Personalized medicine, Big data analytics, Topological data analysis, TDA, Applied topology, Mapper, Persistent homology, Machine learning},
abstract = {Significant technological advances made in recent years have shepherded a dramatic increase in utilization of digital technologies for biomedicine– everything from the widespread use of electronic health records to improved medical imaging capabilities and the rising ubiquity of genomic sequencing contribute to a “digitization” of biomedical research and clinical care. With this shift toward computerized tools comes a dramatic increase in the amount of available data, and current tools for data analysis capable of extracting meaningful knowledge from this wealth of information have yet to catch up. This article seeks to provide an overview of emerging mathematical methods with the potential to improve the abilities of clinicians and researchers to analyze biomedical data, but may be hindered from doing so by a lack of conceptual accessibility and awareness in the life sciences research community. In particular, we focus on topological data analysis (TDA), a set of methods grounded in the mathematical field of algebraic topology that seeks to describe and harness features related to the “shape” of data. We aim to make such techniques more approachable to non-mathematicians by providing a conceptual discussion of their theoretical foundations followed by a survey of their published applications to scientific research. Finally, we discuss the limitations of these methods and suggest potential avenues for future work integrating mathematical tools into clinical care and biomedical informatics.}
}
@article{SHARMA2022101624,
title = {Implementing challenges of artificial intelligence: Evidence from public manufacturing sector of an emerging economy},
journal = {Government Information Quarterly},
volume = {39},
number = {4},
pages = {101624},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101624},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000605},
author = {Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar},
keywords = {Artificial intelligence, Implementing challenges, Public manufacturing sector, AI enabled systems, Emerging economies},
abstract = {The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.}
}
@article{WEI2022100153,
title = {Perspective: Predicting and optimizing thermal transport properties with machine learning methods},
journal = {Energy and AI},
volume = {8},
pages = {100153},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100153},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000143},
author = {Han Wei and Hua Bao and Xiulin Ruan},
keywords = {Thermal transport properties, Machine learning, Prediction, Optimization},
abstract = {In recent years, (big) data science has emerged as the “fourth paradigm” in physical science research. Data-driven techniques, e.g. machine learning, are advantageous in dealing with problems of high-dimensional features and complex mappings between quantities, which are otherwise of great difficulty or huge cost with other scientific paradigms. In the past five years or so, there has been a rapid growth of machine learning-assisted research on thermal transport. In this perspective, we review the recent progress in the intersection between machine learning and thermal transport, where machine learning methods generally serve as surrogate models for predicting the thermal transport properties, or as tools for designing structures for the desired thermal properties and exploring thermal transport mechanisms. We provide perspectives about the advantages of machine learning methods in comparison to the physics-based methods for studying thermal transport properties. We also discuss how to improve the accuracy of predictive analytics and efficiency of structural optimization, to provide guidance for better utilizing machine learning-based methods to advance thermal transport research. Finally, we identify several outstanding challenges in this active area as well as opportunities for future developments, including developing machine learning methods suitable for small datasets, discovering effective physics-based descriptors, generating dataset from experiments and validating machine learning results with experiments, and making breakthroughs via discovering new physics.}
}
@article{MACIAS2022,
title = {Nowcasting food inflation with a massive amount of online prices},
journal = {International Journal of Forecasting},
year = {2022},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2022.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S016920702200036X},
author = {Paweł Macias and Damian Stelmasiak and Karol Szafranek},
keywords = {Inflation nowcasting, Online prices, Big data, Nowcasting competition, Web scraping},
abstract = {The consensus in the literature on providing accurate inflation forecasts underlines the importance of precise nowcasts. In this paper, we focus on this issue by employing a unique, extensive dataset of online food and non-alcoholic beverages prices gathered automatically from the webpages of major online retailers in Poland since 2009. We perform a real-time nowcasting experiment by using a highly disaggregated framework among popular, simple univariate approaches. We demonstrate that pure estimates of online price changes are already effective in nowcasting food inflation, but accounting for online food prices in a simple, recursively optimized model delivers further gains in the nowcast accuracy. Our framework outperforms various other approaches, including judgmental methods, traditional benchmarks, and model combinations. After the outbreak of the COVID-19 pandemic, its nowcasting quality has improved compared to other approaches and remained comparable with judgmental nowcasts. We also show that nowcast accuracy increases with the volume of online data, but their quality and relevance are essential for providing accurate in-sample fit and out-of-sample nowcasts. We conclude that online prices can markedly aid the decision-making process at central banks.}
}
@article{MUNINGER2022140,
title = {Social media use: A review of innovation management practices},
journal = {Journal of Business Research},
volume = {143},
pages = {140-156},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322000510},
author = {Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi},
keywords = {Social media, Innovation, Systematic review, Framework and research agenda},
abstract = {The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.}
}
@article{KUO2022,
title = {Public transport for smart cities: Recent innovations and future challenges},
journal = {European Journal of Operational Research},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.06.057},
url = {https://www.sciencedirect.com/science/article/pii/S037722172200546X},
author = {Yong-Hong Kuo and Janny M.Y. Leung and Yimo Yan},
keywords = {Transportation, Public transit, Smart cities, Analytics, Survey},
abstract = {The idea of a smart city is one that utilises Internet-of-Things (IoT) technologies and data analytics to optimise the efficiency of city operations and services, so as to provide a high quality of life for its citizens. Due to reduced public funding, many public transport systems are already facing challenges to maintain their services. For a smart city, the goal of public transport is not simply the movement of people, but the provision and enhancement of mobility for living. This will be particularly challenging due to changes in habitation trends and work patterns. For example, the growth of mega-cities has led to extreme traffic congestion in city centres and urban sprawl on their outskirts. In order to provide sufficient coverage and frequency of service, an integrated co-ordinated multi-modal public transport system is needed, leading to substantial increase in operational complexity. Environmental concerns and the recent pandemic may also change work and commuting patterns in the future, with more people working from home and companies adopting flexible work shifts. For smart cities, public transport must offer ubiquitous access, real-time response to demand, convenience and quality service, and energy-efficient operations. This paper will discuss the challenges in network design, operations planning, scheduling and management of smart public transport systems. A brief survey of recent research and innovations will also be presented.}
}
@article{AHMAD2022112128,
title = {Data-driven probabilistic machine learning in sustainable smart energy/smart energy systems: Key developments, challenges, and future research opportunities in the context of smart grid paradigm},
journal = {Renewable and Sustainable Energy Reviews},
volume = {160},
pages = {112128},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112128},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122000569},
author = {Tanveer Ahmad and Rafal Madonski and Dongdong Zhang and Chao Huang and Asad Mujeeb},
keywords = {Data-driven probabilistic machine learning, Energy distribution, Discovery and design of energy materials, Big data analytics and smart grid, Strategic energy planning and smart manufacturing, Energy demand-side response},
abstract = {The current trend indicates that energy demand and supply will eventually be controlled by autonomous software that optimizes decision-making and energy distribution operations. New state-of-the-art machine learning (ML) technologies are integral in optimizing decision-making in energy distribution networks and systems. This study was conducted on data-driven probabilistic ML techniques and their real-time applications to smart energy systems and networks to highlight the urgency of this area of research. This study focused on two key areas: i) the use of ML in core energy technologies and ii) the use cases of ML for energy distribution utilities. The core energy technologies include the use of ML in advanced energy materials, energy systems and storage devices, energy efficiency, smart energy material manufacturing in the smart grid paradigm, strategic energy planning, integration of renewable energy, and big data analytics in the smart grid environment. The investigated ML area in energy distribution systems includes energy consumption and price forecasting, the merit order of energy price forecasting, and the consumer lifetime value. Cybersecurity topics for power delivery and utilization, grid edge systems and distributed energy resources, power transmission, and distribution systems are also briefly studied. The primary goal of this work was to identify common issues useful in future studies on ML for smooth energy distribution operations. This study was concluded with many energy perspectives on significant opportunities and challenges. It is noted that if the smart ML automation is used in its targeting energy systems, the utility sector and energy industry could potentially save from $237 billion up to $813 billion.}
}