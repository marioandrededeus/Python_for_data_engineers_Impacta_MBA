@article{MENG20191017,
title = {Critical review of the energy-water-carbon nexus in cities},
journal = {Energy},
volume = {171},
pages = {1017-1032},
year = {2019},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219300519},
author = {Fanxin Meng and Gengyuan Liu and Sai Liang and Meirong Su and Zhifeng Yang},
keywords = {Energy, Water, Carbon, Nexus, Urban metabolism, City},
abstract = {Energy, water, and carbon dioxide are interdependent and have complex interactions through the economic supply chains of cities. Reviewing the state-of-art advances in the urban energy-water-carbon (EWC) nexus can help identify suitable tools for EWC management in cities and hotspots for future urban EWC nexus studies. However, systematic reviews of studies on the urban EWC nexus are lacking. This study fills this knowledge gap by conducting a systematic review of the advances in urban EWC nexus studies. The results indicate that nearly 94% of the urban EWC nexus studies focus on the nexus between two of the three aspects, with the ‘energy-water nexus’ representing the mainstream topic, especially at the product/sector scale (energy/water products or sectors). However, relatively few nexus analyses have been performed for the synergies and trade-offs among the three aspects as an integrated whole. This review reveals that unclear system boundaries of a nexus or city and imprecise urban inner structures may be the main limitations for urban EWC nexus studies. Moreover, the development of a theoretical framework is proposed, and methodological breakthroughs and key research directions urgently needed for urban EWC nexus in the future are highlighted.}
}
@incollection{MANZ201989,
title = {Chapter 8 - Capabilities and MEDICS for an effective QMS},
editor = {Susanne Manz},
booktitle = {Medical Device Quality Management Systems},
publisher = {Academic Press},
pages = {89-148},
year = {2019},
isbn = {978-0-12-814221-9},
doi = {https://doi.org/10.1016/B978-0-12-814221-9.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128142219000087},
author = {Susanne Manz},
keywords = {Monitor, measure, metrics, analyze, embrace, culture, prevention, define, risk, prioritize, identify, self-identify, feedback look, CAPA and improvement, share, communicate, management review, culture of quality, capabilities, MEDICS, data driven, quality objectives, sources of data, statistical techniques, governance, risk tolerance, 5 Whys, process approach, resources, red flags, warning, compliance pendulum, risk tolerance, decision making, norms, profile, Big Q, customer, negligence, obstruction, contempt, undermining, contributing cause, correction, corrective action, preventive action, learning, surviving, thriving, leading, maturity, CTQ tree, cost, capability, training, data governance board, well-dressed metrics, SMART metrics},
abstract = {In previous chapters, we discussed what an effective and efficient quality management system (QMS) is. Now, we begin to discuss some of the capabilities needed to establish a QMS. Medical device manufacturers need to have six key capabilities to create an effective and efficient QMS. Without these capabilities they cannot thrive, or even survive, in a competitive and ever-changing industry. These six key capabilities, or Quality System MEDICS, essential for a healthy QMS are: (i) Monitor—Ability to measure, monitor, and analyze the health of the QMS; (ii) Embrace—Ability of companies to embrace a culture of quality, compliance, and prevention; (iii) Define—Ability to define risks, prioritize issues, and drive key improvement activities; (iv) Identify—Ability to self-identify problems and nonconformities with a short feedback loop; (v) Corrective and preventive action and Improvement—Ability to fix problems robustly and establish controls to sustain the gains; (vi) Share and Communicate—Ability to share and communicate key information in a transparent manner, especially management review. If you have any serious quality or compliance issues, be sure to ask “5 Why’s” to understand contributing causes. Usually, contributing cause(s) include immaturity in one or more of the MEDICS.}
}
@article{ZHOU2019100300,
title = {Human motion data editing based on a convolutional automatic encoder and manifold learning},
journal = {Entertainment Computing},
volume = {30},
pages = {100300},
year = {2019},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2019.100300},
url = {https://www.sciencedirect.com/science/article/pii/S1875952118300776},
author = {Dongsheng Zhou and Xinzhu Feng and Xin Yang and Qiang Zhang and Xiaopeng Wei and Xiaoyong Fang and Deyun Yang},
keywords = {Human motion data, Convolution automatic encoder, Manifold learning, Motion editing},
abstract = {Technology of human motion capture has been widely used in digital entertainment field. Editing the existing large amount of human motion capture data, correcting and eliminating motion distortion caused by noise and other defects have important value and significance for data reuse. In this paper, data processing is carried out based on convolutional automatic encoder and manifold learning. The popular structure of human motion data was learned by a one-dimensional time domain convolution automatic encoder, in which the hidden unit of the automatic encoder represents motion data. Three constraints were used to overcome the problem that the hidden unit has too much motion editing range. The data to be processed in this paper has no limit on the number of motions. The proposed method can process large data sets in parallel and automatically perform manifold learning without manual labelling and segmentation. In the final, comparative experiments based on a variety of damaged motion data have been carried out. The results showed that the proposed method can effectively reduce the error of the original motion data, and has achieved good results in both objective evaluation and subjective evaluation.}
}
@article{GURSOY2019157,
title = {Consumers acceptance of artificially intelligent (AI) device use in service delivery},
journal = {International Journal of Information Management},
volume = {49},
pages = {157-169},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219301690},
author = {Dogan Gursoy and Oscar Hengxuan Chi and Lu Lu and Robin Nunkoo},
keywords = {Artificial intelligence, AI devices, Technology, Intention, Adoption, Services},
abstract = {This study develops and empirically tests a theoretical model of artificially intelligent (AI) device use acceptance (AIDUA) that aims to explain customers’ willingness to accept AI device use in service encounters. The proposed model incorporates three acceptance generation stages (primary appraisal, secondary appraisal, and outcome stage) and six antecedents (social influence, hedonic motivation, anthropomorphism, performance expectancy, effort expectancy, and emotion). Utilizing data collected from potential customers, the proposed AIDUA model is tested. Findings suggest that customers go through a three-step acceptance generation process in determining whether to accept the use of AI devices during their service interactions. Findings indicate that social influence and hedonic motivation are positively related to performance expectancy while anthropomorphism is positively related to effort expectancy. Both performance and effort expectancy are significant antecedents of customer emotions, which determines customers’ acceptance of AI device use in service encounters. This study provides a conceptual AI device acceptance framework that can be used by other researchers to better investigate AI related topics in the service context.}
}
@incollection{VALLERO2019489,
title = {Chapter 17 - Air pollution decision-making},
editor = {Daniel A. Vallero},
booktitle = {Air Pollution Calculations},
publisher = {Elsevier},
pages = {489-525},
year = {2019},
isbn = {978-0-12-814934-8},
doi = {https://doi.org/10.1016/B978-0-12-814934-8.00017-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012814934800017X},
author = {Daniel A. Vallero},
keywords = {Criteria air pollutants, Hazardous air pollutants, Air toxics, Release-related decisions, Quality-related decisions, Decision space, National Ambient Air Quality Standards (NAAQS), Emissions, Decision force field, Level of detection (LoD), Air pollution ethics, As low as reasonably possible (ALARP), Benefit-to-cost ratio (BCR), Responsible conduct of research (RCR)},
abstract = {This chapter addresses the calculations that are needed to support decisions made by air quality managers, engineers, and scientists. Suggestions are made on how to approach air pollution decision-making that increasingly relies on “other people’s data,” including discussions of metadata, decision space, and data interdependence, and what to do with databases that contain nondetects and other uncertainties. The chapter also addresses ethics, professionalism, and responsible conduct of research.}
}
@incollection{2019375,
title = {Appendix},
editor = {Clara Marina Martínez and Dongpu Cao},
booktitle = {Ihorizon-Enabled Energy Management for Electrified Vehicles},
publisher = {Butterworth-Heinemann},
pages = {375-412},
year = {2019},
isbn = {978-0-12-815010-8},
doi = {https://doi.org/10.1016/B978-0-12-815010-8.10000-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128150108100006}
}
@article{MAEDA2019609,
title = {Temporal patterns of phytoplankton phenology across high latitude lakes unveiled by long-term time series of satellite data},
journal = {Remote Sensing of Environment},
volume = {221},
pages = {609-620},
year = {2019},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0034425718305595},
author = {Eduardo Eiji Maeda and Filipe Lisboa and Laura Kaikkonen and Kari Kallio and Sampsa Koponen and Vanda Brotas and Sakari Kuikka},
keywords = {Chlorophyll , Landsat, Eutrophication, Remote sensing, Finland},
abstract = {Monitoring temporal changes in phytoplankton dynamics in high latitude lakes is particularly timely for understanding the impacts of warming on aquatic ecosystems. In this study, we analyzed 33-years of high resolution (30 m) Landsat (LT) data for reconstructing seasonal patterns of chlorophyll a (chl a) concentration in four lakes across Finland, between 60°N and 64°N. Chl a models based on LT spectral bands were calibrated using 17-years (2000–2016) of field measurements collected across the four lakes. These models were then applied for estimating chl a using the entire LT-5 and 7 archives. Approximately 630 images, from 1984 to 2017, were analyzed for each lake. The chl a seasonal patterns were characterized using phenology metrics, and the time-series of LT-based chl a estimates were used for identifying temporal shifts in the seasonal patterns of chl a concentration. Our results showed an increase in the length of phytoplankton growth season in three of the lakes. The highest increase was observed in Lake Köyliönjärvi, where the length of growth season has increased by 28 days from the baseline period of 1984–1994 to 2007–2017. The increase in the length of season was mainly attributed to an earlier start of phytoplankton blooms. We further analyzed surface temperature (Ts) and precipitation data to verify if climatic factors could explain the shifts in the seasonal patterns of chl a. We found no direct relationship between Ts and chl a seasonal patterns. Similarly, the phenological metrics of Ts, in particular length of season, did not show significant temporal trends. On the other hand, we identify potential links between changes in precipitation patterns and the increase in the phytoplankton season length. We verified a significant increase in the rainfall contribution to the total precipitation during the autumn and winter, accompanied by a decline in snowfall volumes. This could indicate an increasing runoff volume during the beginning of spring, contributing to an earlier onset of the phytoplankton blooms, although further assessments are needed to analyze historical streamflow values and nearby land cover data. Likewise, additional studies are needed to better understand why chl a patterns in some lakes seem to be more resilient than in others.}
}
@article{IMMONEN201946,
title = {Harmonization of pipeline for preclinical multicenter MRI biomarker discovery in a rat model of post-traumatic epileptogenesis},
journal = {Epilepsy Research},
volume = {150},
pages = {46-57},
year = {2019},
issn = {0920-1211},
doi = {https://doi.org/10.1016/j.eplepsyres.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0920121118304765},
author = {Riikka Immonen and Gregory Smith and Rhys D. Brady and David Wright and Leigh Johnston and Neil G. Harris and Eppu Manninen and Raimo Salo and Craig Branch and Dominique Duncan and Ryan Cabeen and Xavier Ekolle Ndode-Ekane and Cesar Santana Gomez and Pablo M. Casillas-Espinosa and Idrish Ali and Sandy R. Shultz and Pedro Andrade and Noora Puhakka and Richard J. Staba and Terence J. O’Brien and Arthur W. Toga and Asla Pitkänen and Olli Gröhn},
keywords = {Common data element, Diffusion tensor imaging, Magnetization transfer imaging, Multi-site harmonization, Post-traumatic epilepsy, Traumatic brain injury},
abstract = {Preclinical imaging studies of posttraumatic epileptogenesis (PTE) have largely been proof-of-concept studies with limited animal numbers, and thus lack the statistical power for biomarker discovery. Epilepsy Bioinformatics Study for Antiepileptogenic Therapy (EpiBioS4Rx) is a pioneering multicenter trial investigating preclinical imaging biomarkers of PTE. EpiBios4Rx faced the issue of harmonizing the magnetic resonance imaging (MRI) procedures and imaging data metrics prior to its execution. We present here the harmonization process between three preclinical MRI facilities at the University of Eastern Finland (UEF), the University of Melbourne (Melbourne), and the University of California, Los Angeles (UCLA), and evaluate the uniformity of the obtained MRI data. Adult, male rats underwent a lateral fluid percussion injury (FPI) and were followed by MRI 2 days, 9 days, 1 month, and 5 months post-injury. Ex vivo scans of fixed brains were conducted 7 months post-injury as an end point follow-up. Four MRI modalities were used: T2-weighted imaging, multi-gradient-echo imaging, diffusion-weighted imaging, and magnetization transfer imaging, and acquisition parameters for each modality were tailored to account for the different field strengths (4.7 T and 7 T) and different MR hardwares used at the three participating centers. Pilot data collection resulted in comparable image quality across sites. In interim analysis (of data obtained by April 30, 2018), the within-site variation of the quantified signal properties was low, while some differences between sites remained. In T2-weighted images the signal-to-noise ratios were high at each site, being 35 at UEF, 48 at Melbourne, and 32 at UCLA (p < 0.05). The contrast-to-noise ratios were similar between the sites (9, 10, and 8, respectively). Magnetization transfer ratio maps had identical white matter/ gray matter contrast between the sites, with white matter showing 15% higher MTR than gray matter despite different absolute MTR values (MTR both in white and gray matter was 3% lower in Melbourne than at UEF, p < 0.05). Diffusion-weighting yielded different degrees of signal attenuation across sites, being 83% at UEF, 76% in Melbourne, and 80% at UCLA (p < 0.05). Fractional anisotropy values differed as well, being 0.81 at UEF, 0.73 in Melbourne, and 0.84 at UCLA (p < 0.05). The obtained values in sham animals showed low variation within each site and no change over time, suggesting high repeatability of the measurements. Quality control scans with phantoms demonstrated stable hardware performance over time. Timing of post-TBI scans was designed to target specific phases of the dynamic pathology, and the execution at different centers was highly accurate. Besides a few outliers, the 2-day scans were done within an hour from the target time point. At day 9, most animals were scanned within an hour from the target time point, and all but 2 outliers within 24 h from the target. The 1-month post-TBI scans were done within 31 ± 3 days. MRI procedures and animal physiology during scans were similar between the sites. Taken together, the 10% inter-site difference in FA and 3% difference in MTR values should be included into analysis as a covariate or balanced out in post-processing in order to detect disease-related effects on brain structure at the same scale. However, for a MRI biomarker for post-traumatic epileptogenesis to have realistic chance of being successfully translated to validation in clinical trials, it would need to be a robust TBI-induced structural change which tolerates the inter-site methodological variability described here.}
}
@incollection{LAH2019133,
title = {Chapter 7 - Sustainable Urban Mobility in Action},
editor = {Oliver Lah},
booktitle = {Sustainable Urban Mobility Pathways},
publisher = {Elsevier},
pages = {133-282},
year = {2019},
isbn = {978-0-12-814897-6},
doi = {https://doi.org/10.1016/B978-0-12-814897-6.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128148976000077},
author = {Oliver Lah},
keywords = {Urban mobility, policy, implementation action},
abstract = {There are various levels where local governments can shape the energy consumption and sustainability of urban transport systems through infrastructure development, provided services, and policy decisions. This chapter will outline key areas for local policy and planning intervention that address the urban transport system in a holistic way, focusing on transport activity, the modal structure, energy intensity and fuels, and energy carriers. The chapter will explore key strategies for sustainable urban mobility, including integrated urban planning, public transport, walking, cycling, pricing measures, urban logistics, intelligent transport systems, and options to boost electric mobility, actively pursuing the need for inclusive cities through accessibility. Short introductions to key measures are provided, followed by factsheets developed as part of the SOLUTIONS project on public transport, urban mobility planning, infrastructure, mobility management, and clean vehicles. (SOLUTIONS project partners contributing to the factsheets included: ICLEI-SA: Ashish Rao Ghorpade, Ranjith Parvathapuram, BASt: Benjamin Schreck, Austria Tech: Florian Kressler, WI: Oliver Lah, Hanna Hüging, EMBARQ India: Pawan Mulukutla, Ranjana Menon, Clean Air Asia: Sameera Anthapur, Alvin Meija, FEHRL: Thierry Goger, Adewole Adesiyun, RC: Bernard Gyergyay, Susanne Böhler-Baedeker, EMBARQ Brazil: Magdala Arioli, Daniela Facchini, Marcelo Cintra do Amaral, IFSTTAR: Leatitia DaBlanc, Christophe Rizet, LNEC: Elisabete Arsenio, EMBARQ Turkey: Pinar Köse, Cigdem Corek, Ece Ömür.)}
}
@incollection{ROMANO20197,
title = {Chapter 2 - Holistic Health Screening},
editor = {Vincent Morelli},
booktitle = {Adolescent Health Screening: an Update in the Age of Big Data},
publisher = {Elsevier},
pages = {7-20},
year = {2019},
isbn = {978-0-323-66130-0},
doi = {https://doi.org/10.1016/B978-0-323-66130-0.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323661300000028},
author = {Mary Romano and Debra Kristen Braun-Courville and Neerav Desai},
keywords = {Acupuncture, Adolescent medicine, Chinese herbal medicine, Complementary and alternative medicine, Meditation, Performance-enhancing substances, Polyunsaturated fatty acids, St. John’s wort, Supplements, Yoga},
abstract = {This chapter will focus on complementary and alternative medicine (CAM), as it relates to adolescents. More than 1 out of every 10 children utilize complementary medicine as part of their healthcare plan, and adolescents are more likely to use CAM than children. Given its frequency of use, it is important that healthcare providers regularly ask about its use in both acute-care and well-care visits. The annual well visit and/or sports physical examination may be an ideal time to identify CAM use and there are resources available to aid providers in how to obtain this information. Medical conditions for which adolescents commonly use CAM include dysmenorrhea, psychiatric disorders (depression, anxiety, attention-deficit/hyperactivity disorder), sleep disorders, headaches, acne, dermatitis, weight management, and performance enhancement. Although for some conditions there is contradictory and/or lacking evidence of efficacy, patients and families use CAM. Including CAM screening in our adolescent population is an opportunity to educate patients and families and separate fact from fiction.}
}
@article{2019S1,
title = {Abstracts of the 55th Congress of the European Societies of Toxicology (EUROTOX 2019) TOXICOLOGY SCIENCE PROVIDING SOLUTIONS},
journal = {Toxicology Letters},
volume = {314},
pages = {S1-S309},
year = {2019},
note = {Abstracts of the 55th Congress of the European Societies of Toxicology (EUROTOX 2019) TOXICOLOGY – SCIENCE PROVIDING SOLUTIONS},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378427419302486}
}
@article{BATES2019101925,
title = {Literature Listing},
journal = {World Patent Information},
volume = {59},
pages = {101925},
year = {2019},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2019.101925},
url = {https://www.sciencedirect.com/science/article/pii/S0172219019301164},
author = {Susan Bates},
keywords = {Patents, Designs, Trade marks, Literature listing, Patent analysis, Current awareness},
abstract = {The quarterly Literature Listing is intended as a current awareness service for readers indicating newly published books, journal and conference articles on: patent search techniques, databases, analysis and classifications; patent searcher certification; patents relating to a) life sciences and pharmaceuticals and b) software; patent policy and strategic issues; trade marks; designs; domain names; and articles reviewing historical aspects of intellectual property or reviewing specific topics/persons. The current Literature Listing was compiled end-August 2019. Key resources used are Scopus, Digital Commons, publishers’ RSS feeds, and serendipity! Please feel free to send the author details of newly published reports/monographs/books for potential inclusion.}
}
@article{ZHU2019730,
title = {Personalized Control for Promoting Sustainable Travel Behaviors},
journal = {Transportation Research Procedia},
volume = {38},
pages = {730-750},
year = {2019},
note = {Journal of Transportation and Traffic Theory},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2019.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S235214651930047X},
author = {Xi Zhu and Feilong Wang and Cynthia Chen and Derek D. Reed},
keywords = {Sustainable travel behaviors, personalized incentives, controls, preference learning, particle filter;},
abstract = {We develop a personalized control system to modify individual travel behaviors by offering personalized incentives. Individual preferences are learned to provide personalized incentives so that the promoted alternative is more likely to be accepted. The work described is based on the integration of two fields (controls and human behavior) that are traditionally separate from each other: we model the travelers’ choice-making behaviors with the random utility theory and responses from the individuals are mined by a particle filter for learning individual preferences to promote sustainable behaviors. The discrete nature of travel behavior naturally leads to limited observability. We overcome this problem by designing a measurement function from which additional information can be solicited. Additionally, the inherent trade-off nature of travel behaviors results in an infinite set of solutions, to which two solutions are proposed: 1) the divide and conquer strategy in which a multi-dimensional conditional probability function is proposed; and 2) use of domain knowledge to restrict that preference values fall in certain ranges and are consistent with certain distributions. The performance of preference learning with these two solutions applied is shown via simulation tests. An online experiment, involving hypothetical scenarios on departure time choices and human subjects, shows that among all the recruited participants, the majority (65% of participants) are more likely to accept the promoted alternatives given personalized incentives. We also show that changes in individual departure time choice can potentially lead to a significant reduction (48%) in total travel time on a simple transportation network.}
}
@article{ABDELHAMEED20191,
title = {Restricted Sensitive Attributes-based Sequential Anonymization (RSA-SA) approach for privacy-preserving data stream publishing},
journal = {Knowledge-Based Systems},
volume = {164},
pages = {1-20},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118304131},
author = {Saad A. Abdelhameed and Sherin M. Moussa and Mohamed E. Khalifa},
keywords = {Data privacy, Data anonymization, Data streams, Sequential anonymization, Privacy disclosure attacks, Privacy-preserving data publishing},
abstract = {Data streams have become a widely-adopted data representation format in many real-world applications. This data streaming may be published for different scientific research, mining, or analysis purposes. However, such streams may contain personal-specific data that could be considered as sensitive about individuals. This makes the privacy preserving of data streams against privacy disclosure attacks, while maintaining their utilization, is a real challenge. Some studies have considered privacy-preserving publishing over data streams with only Single Sensitive Attribute, in which they do not protect the published streams from all possible privacy attacks. In this paper, we propose a novel Restricted Sensitive Attributes-based Sequential Anonymization (RSA-SA) approach for privacy-preserving data stream publishing. Besides, two new privacy restrictions are introduced to restrict the published Sensitive Attributes values: Semantic-diversity and Sensitivity-diversity. RSA-SA can protect the sensitive values of the published data streams against the related privacy attacks, including the attribute disclosure, skewness, similarity, and sensitivity attacks. In addition, RSA-SA handles data streams that have either single or multiple sensitive attributes with minimum information loss and delay time. Thus, the data utility of the published data streams is efficiently maintained to provide more accurate mining and analytical results, where robust invulnerability to privacy attacks is sustained.}
}
@incollection{CARLTON2019509,
title = {Chapter 24 - Service Performance and Analysis},
editor = {J.S. Carlton},
booktitle = {Marine Propellers and Propulsion (Fourth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Fourth Edition},
pages = {509-533},
year = {2019},
isbn = {978-0-08-100366-4},
doi = {https://doi.org/10.1016/B978-0-08-100366-4.00024-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081003664000249},
author = {J.S. Carlton},
keywords = {Ship performance, Deterioration, Weather, Fouling, Hull, Propeller},
abstract = {In general, the performance of a ship in service is different from that obtained on trial. Apart from differences in loading conditions and for which due correction should be made, these differences arise principally from engine deterioration, the weather, fouling, and surface deterioration of the hull and propeller.}
}
@article{WEI2019276,
title = {Vehicle engine classification using normalized tone-pitch indexing and neural computing on short remote vibration sensing data},
journal = {Expert Systems with Applications},
volume = {115},
pages = {276-286},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.07.073},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418304974},
author = {Jie Wei and Chi-Him Liu and Zhigang Zhu and Lindsay R. Cain and Vincent J. Velten},
keywords = {Remote sensing, Fourier transforms, Band-pass filters, Indexing, Classification algorithms, Machine learning, Neural networks},
abstract = {As a non-invasive and remote sensor, a Laser Doppler Vibrometer (LDV) has found a broad spectrum of applications. It is a remote, non-line-of-sight sensor to detect threats more reliably and provide increased security protection, which is of utmost importance to military and law enforcement applications. However, the use of the LDV in situation surveillance, especially in vehicle classification, lacks systematic investigations as to its phenomenological and statistical properties. In this work, we aim to identify vehicles by their engine types within a very short period of time to yield a practical expert and intelligent system to classify vehicle engines remotely using laser sensors. Based on our preliminary success on the use of tone-pitch indexes (TPI) over these data, a new normalized tone-pitch indexing (nTPI) scheme is developed to capture engine periodic vibrations by various engine types with vibration data over a much shorter period (from 1.25 to 0.2 s), which makes it possible to monitor slowly moving vehicles around 15 miles per hour. We also exploit the learning power of neural computing, including artificial neural network (ANN), Deep Belief nets (DBN), Stacked Auto-Encoder (SAE), and Convolutional Neural Networks (CNN). To apply a CNN, a two-dimensional array is formulated by stacking nTPI data in an overlapping manner, which is termed as 2DonTPI. The classification results using the proposed nTPI and 2DonTPI over a standard LDV dataset are promising: with encoding duration significantly smaller than that required by the original TPI, consistently high performance is attained for all four neural computing methods. The new vibration data representation combined with neural computing approaches gives rise to a powerful expert and intelligent system for vehicle engine classification, which can find a great array of applications for civil, law enforcement, and military agencies for Intelligence, Surveillance and Reconnaissance purposes that are of crucial importance to national and international security.}
}
@incollection{2019419,
title = {Index},
editor = {Constantinos Antoniou and Loukas Dimitriou and Francisco Pereira},
booktitle = {Mobility Patterns, Big Data and Transport Analytics},
publisher = {Elsevier},
pages = {419-432},
year = {2019},
isbn = {978-0-12-812970-8},
doi = {https://doi.org/10.1016/B978-0-12-812970-8.09992-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128129708099929}
}
@article{ZUIDERWIJK2019228,
title = {Sharing and re-using open data: A case study of motivations in astrophysics},
journal = {International Journal of Information Management},
volume = {49},
pages = {228-241},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218311836},
author = {Anneke Zuiderwijk and Helen Spiers},
keywords = {Open data, Open science, Motivation, Astrophysics, Interview analysis, Qualitative research},
abstract = {Open data sharing and re-use is currently more common in some academic disciplines than others. Although each discipline has unique challenges and characteristics which can influence data sharing and re-use behavior, it may be possible to gain transferable insight from disciplines where these practices are more common. Several studies of the motivations underlying data sharing and re-use have been conducted, however these studies often remain at a high level of abstraction rather than providing in-depth insight about discipline-specific challenges and opportunities. This study sought to provide in-depth insight about the complex interaction of factors influencing motivations for sharing and re-using open research data within a single discipline, namely astrophysics. We focused on this discipline due to its well-developed tradition of free and open access to research data. Eight factors were found to influence researchers’ motivations for sharing data openly, including the researcher’s background, personal drivers, experience, legislation, regulation and policy, data characteristics, performance expectancy, usability, and collaboration. We identified six factors that influence researchers’ motivations to re-use open research data, including the researcher’s background, facilitating conditions, expected performance, social and affiliation factors, effort and experience. Finally, we discuss how data sharing and re-use can be encouraged within the context of astrophysics research, and we discuss how these insights may be transferred to disciplines with low rates of data sharing and re-use.}
}
@incollection{2019117,
title = {Chapter 4 - Ocean In Situ Sensors Crosscutting Innovations},
editor = {Eric Delory and Jay Pearlman},
booktitle = {Challenges and Innovations in Ocean In Situ Sensors},
publisher = {Elsevier},
pages = {117-171},
year = {2019},
isbn = {978-0-12-809886-8},
doi = {https://doi.org/10.1016/B978-0-12-809886-8.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128098868000041}
}
@article{LIU2019168,
title = {A socio-spatial analysis of neighbour complaints using large-scale administrative data: The case in Brisbane, Australia},
journal = {Cities},
volume = {90},
pages = {168-180},
year = {2019},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2019.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0264275118311314},
author = {Yan Liu and Lynda Cheshire and Siqin Wang and Xuanming Fu},
keywords = {Administrative data, Socio-spatial analysis, GIS, Neighbor problems, Brisbane},
abstract = {Large-scale administrative data collected by municipal government are increasingly being used by researchers to better understand a host of urban phenomena and the way they are patterned over space and time. In this paper, council data are used to explore the incidence of complaints about neighbours across urban neighbourhoods using a GIS-based spatial approach. Through an exploratory and a confirmatory factor analysis of the spatially extracted neighbour complaints data, we identify four types of neighbour complaints – animal related; building construction; property management issues; and health and visual amenity issues – that categorise neighbour problems. GIS technologies are applied to map the spatial distribution of each complaint type across the 218 suburbs, resulting in distinct patterns of neighbour complaints in Brisbane suburbs. Our research demonstrates the utility of naturally occurring administrative data as a means of learning more about the social life of urban areas.}
}
@article{HUANG20191382,
title = {A systematic review of empirical methods for modelling sectoral carbon emissions in China},
journal = {Journal of Cleaner Production},
volume = {215},
pages = {1382-1401},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.01.058},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619300691},
author = {Li Huang and Scott Kelly and Kangjuan Lv and Damien Giurco},
keywords = {Climate change, Carbon emissions, China, Knowledge mapping, Literature review, Modelling},
abstract = {Abstract:
A number of empirical methods have been developed to study China's sectoral carbon emissions (CSCE). Measuring these emissions is important for climate change mitigation. While several articles have reviewed specific methods, few attempts conduct a systematic analysis of all the major research methods. In total 807 papers were published on CSCE research between 1997 and 2017. The primary source of literature for this analysis was taken from the Web of Science database. Based on a bibliometric analysis using knowledge mapping with the software CiteSpace, the review identified five common families of methods: 1) environmentally-extended input-output analysis (EE-IOA), 2) index decomposition analysis (IDA), 3) econometrics, 4) carbon emission control efficiency evaluation and 5) simulation. The research revealed the main trends in each family of methods and has visualized this research into ten research clusters. In addition, the paper provides a direct comparison of all methods. The research results can help scholars quickly identify and compare different methods for addressing specific research questions.}
}
@article{NIEMIERKO2019691,
title = {A D-vine copula quantile regression approach for the prediction of residential heating energy consumption based on historical data},
journal = {Applied Energy},
volume = {233-234},
pages = {691-708},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918315812},
author = {Rochus Niemierko and Jannick Töppel and Timm Tränkler},
keywords = {Data-driven heating energy analysis, Energetic retrofitting, Quantile regression, D-vine copula, Rebound effect, Performance gap},
abstract = {Energetic retrofitting of residential buildings is poised to play an important role in the achievement of ambitious global climate targets. A prerequisite for purposeful policy-making and private investments is the accurate prediction of energy consumption. Building energy models are mostly based on engineering methods quantifying theoretical energy consumption. However, a performance gap between predicted and actual consumption has been identified in literature. Data-driven methods using historical data can potentially overcome this issue. The D-vine copula-based quantile regression model used in this study achieved very good fitting results based on a representative data set comprising 25,000 German households. The findings suggest that quantile regression increases transparency by analyzing the entire distribution of heating energy consumption for individual building characteristics. More specifically, the analyses reveal the following exemplary insights. First, for different levels of energy efficiency, the rebound effect exhibits cyclical behavior and significantly varies across quantiles. Second, very energy-conscious and energy-wasteful households are prone to more extreme rebound effects. Third, with regards to the performance gap, heating energy demand of inefficient buildings is systematically underestimated, while it is overestimated for efficient buildings.}
}
@article{ISLAM2019344,
title = {Material flow analysis (MFA) as a strategic tool in E-waste management: Applications, trends and future directions},
journal = {Journal of Environmental Management},
volume = {244},
pages = {344-361},
year = {2019},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2019.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S0301479719306826},
author = {Md Tasbirul Islam and Nazmul Huda},
keywords = {Electrical and electronic equipment (EEE), Waste electrical and electronic equipment (WEEE), Recycling, Substance flow analysis, Circular economy, Literature review},
abstract = {Material flow analysis (MFA) is one of the most widely accepted and utilized tools in the industrial-ecology discipline, that measures the input-output materials and examines the pathways and flux of each material flow within the whole system. The application of MFA in e-waste management has recently increased and quite a few academic articles have been published on this issue providing decision support at the policy level. However, there is a need to understand the dynamics of MFA methodology, the data requirements (as well as the data sources used in the previous studies) and the lessons learnt from the studies, so that countries where such an E-waste-MFA study has not yet been performed can apply the international experience of such an emerging research technique. This comprehensive review article presents the recent applications, trends, characteristics, research gaps and challenges of the MFA method that may help e-waste management with an overview of the need for a such tool to be applied. A country-wise analysis is presented and MFA models complemented by various associated methods are summarized with national-level, regional-level, product-level, and element-level assessment. The highlighted future research perspectives discussed in this study will help to analyze e-waste management systems more critically, including the hidden and known flows of waste products and associated materials, economic assessment of material recovery and the role of responsible authorities. This invaluable contribution will help future researchers, particularly from the data collection techniques and previously applied MFA models complemented by various associated methods.}
}
@article{DOSSRIBEIRO2019100085,
title = {Overcoming challenges for designing and implementing the One Health approach: A systematic review of the literature},
journal = {One Health},
volume = {7},
pages = {100085},
year = {2019},
issn = {2352-7714},
doi = {https://doi.org/10.1016/j.onehlt.2019.100085},
url = {https://www.sciencedirect.com/science/article/pii/S2352771418300223},
author = {Carolina {dos S. Ribeiro} and Linda H.M. {van de Burgwal} and Barbara J. Regeer},
keywords = {One health, Challenges, Design, Implementation, Interdisciplinary collaboration, Transdisciplinary research, strategic solutions},
abstract = {Collaborative approaches in health, such as One Health (OH), are promising; nevertheless, several authors point at persistent challenges for designing and implementing OH initiatives. Among other challenges, OH practitioners struggle in their efforts to collaborate across disciplines and domains. This paper aims to provide insights into the existing challenges for designing and implementing OH initiatives, their causes and solutions, and points out strategic solutions with the potential to solve practical challenges. A systematic literature search was performed for emerging challenges and proposed solutions in the process of conducting OH initiatives. Next, a thematic and a causal analysis were performed to unravel challenges and their causes. Finally, solutions were discriminated on whether they were only recommended, or implemented as a proof-of-principle. The 56 included papers describe 21 challenges endured by OH initiatives that relate to different themes (policy and funding; education and training; surveillance; multi-actor, multi-domain, and multi-level collaborations; and evidence). These challenges occur in three different phases: the acquisition of sufficient conditions to start an initiative, its execution, and its monitoring and evaluation. The findings indicate that individual challenges share overlapping causes and crosscutting causal relations. Accordingly, solutions for the successful performance of OH initiatives should be implemented to tackle simultaneously different types of challenges occurring in different phases. Still, promoting collaboration between the wide diversity of stakeholders, as a fundamental aspect in the OH approach, is still by far the most challenging factor in performing OH initiatives. Causes for that are the difficulties in promoting meaningful and equal participation from diverse actors. Solutions proposed for this challenge focused on guiding stakeholders to think and collaborate beyond their professional and cultural silos to generate knowledge co-creation and innovative methodologies and frameworks. Finally, the biggest knowledge gap identified, in terms of proposed solutions, was for monitoring and evaluating OH initiatives. This highlights the need for future research on evaluation methods and tools specific for the OH approach, to provide credible evidence on its added value. When considering challenges endured by former OH initiatives and the proposed solutions for these challenges, practitioners should be able to plan and structure such initiatives in a more successful way, through the strategic pre-consideration of solutions or simply by avoiding known barriers.}
}
@article{KHAN2019396,
title = {Landscaping systematic mapping studies in software engineering: A tertiary study},
journal = {Journal of Systems and Software},
volume = {149},
pages = {396-436},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302784},
author = {Muhammad Uzair Khan and Salman Sherin and Muhammad Zohaib Iqbal and Rubab Zahid},
keywords = {Tertiary study, Systematic mapping study, Secondary study, Survey, Software engineering},
abstract = {Context
A number of Systematic Mapping Studies (SMSs) that cover Software Engineering (SE) are reported in literature. Tertiary studies synthesize the secondary studies to provide a holistic view of an area.
Objectives
We synthesize SMSs in SE to provide insights into existing SE areas and to investigate the trends and quality of SMSs.
Methodology
We use Systematic Literature Review protocol to analyze and map the SMSs in SE, till August 2017, to SE Body of Knowledge (SWEBOK).
Results
We analyze 210 SMSs and results show that: (1) Software design and construction are most active areas in SE; (2) Some areas lack SMSs, including mathematical foundations, software configuration management, and SE tools; (3) The quality of SMSs is improving with time; (4) SMSs in journals have higher quality than SMSs in conferences and are cited more often; (5) Low quality in SMSs can be attributed to a lack of quality assessment in SMSs and not reporting information about the primary studies.
Conclusion
There is a potential for more SMSs in some SE areas. A number of SMSs do not provide the required information for an SMS, which leads to a low quality score.}
}
@article{MONEDERO2019417,
title = {Efficient k-anonymous microaggregation of multivariate numerical data via principal component analysis},
journal = {Information Sciences},
volume = {503},
pages = {417-443},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.07.042},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519306474},
author = {David Rebollo Monedero and Ahmad Mohamad Mezher and Xavier Casanova Colomé and Jordi Forné and Miguel Soriano},
keywords = {Data privacy, Statistical disclosure control, -anonymity, Microaggregation, Principal component analysis, Large-scale datasets},
abstract = {k-Anonymous microaggregation is a widespread technique to address the problem of protecting the privacy of the respondents involved beyond the mere suppression of their identifiers, in applications where preserving the utility of the information disclosed is critical. Unfortunately, microaggregation methods with high data utility may impose stringent computational demands when dealing with datasets containing a large number of records and attributes. This work proposes and analyzes various anonymization methods which draw upon the algebraic-statistical technique of principal component analysis (PCA), in order to effective reduce the number of attributes processed, that is, the dimension of the multivariate microaggregation problem at hand. By preserving to a high degree the energy of the numerical dataset and carefully choosing the number of dominant components to process, we manage to achieve remarkable reductions in running time and memory usage with negligible impact in information utility. Our methods are readily applicable to high-utility SDC of large-scale datasets with numerical demographic attributes. © 2019 The Authors. Preprint submitted to Elsevier, Inc.}
}
@article{RODRIGUEZGARCIA2019282,
title = {Utility-preserving privacy protection of nominal data sets via semantic rank swapping},
journal = {Information Fusion},
volume = {45},
pages = {282-295},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2018.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253517304657},
author = {Mercedes Rodriguez-Garcia and Montserrat Batet and David Sánchez},
keywords = {Rank swapping, Nominal data, Semantics, Ontologies},
abstract = {Personal data are of great interest for research but, at the same time, they pose a serious privacy risk. Therefore, appropriate data protection measures should be undertaken by the data controller before making personal data available for secondary use. Also, such data protection should be done in a way that data are still useful for analysis. In the last years, a plethora of data protection mechanisms have been proposed. Among them, rank swapping is considered one of the best with respect to disclosure risk minimization and data utility preservation. Because rank swapping is based on sorting input data to swap values that are close to each other, in principle, it is a method restricted to numerical and ordinal categorical data. However, a significant amount of personal data currently compiled and used in data analysis are nominal, and their utility depends on the semantics they convey. To properly cope with this type of data, in this paper, we present rank swapping methods capable of protecting nominal data from a semantic perspective. Specifically, by exploiting ontologies, our methods are able to protect nominal data while properly preserving their semantics and, thus, their analytical utility. For that, we provide a suitable binary relation to semantically sort nominal data. Our proposal is capable of managing both independent individual attributes and non-independent multivariate data sets, being the latter especially relevant for data analysis. Empirical experiments carried on real clinical records and using a standard medical ontology show that our methods are able to preserve the semantic features of nominal data significantly better than standard permutation mechanisms.}
}
@article{HOWIESON2019299,
title = {Frankenstein's monster or the Birth of Venus? Perceptions of the impact and contributions of Ball and Brown 1968},
journal = {Pacific-Basin Finance Journal},
volume = {55},
pages = {299-328},
year = {2019},
issn = {0927-538X},
doi = {https://doi.org/10.1016/j.pacfin.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0927538X19301842},
author = {Bryan A. Howieson},
keywords = {Accounting practices, Accounting education, Ball and Brown 1968, Finance practices, Normative research, Positive research, Research impact, Research methodology, Research paradigms, Standards-setting},
abstract = {This study explores perceptions of the impact of Ball and Brown (1968) (BB68) upon accounting research, education, standards-setting, and practice. These perceptions are gathered from an extensive literature review and from 27 interviewees drawn from academe, practice and standards-setting from around the globe. The study reports upon the genesis of BB68, it's disruptive effect on accounting research methodology and methods, and its broader impact on accounting education, practice and standards-setting. Reasons are provided for the popularity of BB68 and an exploration is undertaken of both the positive and negative impacts of the research methodology inspired by BB68. The characteristics of the Ball and Brown research team are also discussed and the paper finishes with perceptions regarding the future relevance of BB68. It is concluded that BB68 stands as an exemplar for innovation and for quality in the execution of accounting and finance research but that much of that innovative spirit has been lost as the BB68 methodology became mainstream.}
}
@article{MIKKELSEN2019537,
title = {Big GABA II: Water-referenced edited MR spectroscopy at 25 research sites},
journal = {NeuroImage},
volume = {191},
pages = {537-548},
year = {2019},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.02.059},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919301557},
author = {Mark Mikkelsen and Daniel L. Rimbault and Peter B. Barker and Pallab K. Bhattacharyya and Maiken K. Brix and Pieter F. Buur and Kim M. Cecil and Kimberly L. Chan and David Y.-T. Chen and Alexander R. Craven and Koen Cuypers and Michael Dacko and Niall W. Duncan and Ulrike Dydak and David A. Edmondson and Gabriele Ende and Lars Ersland and Megan A. Forbes and Fei Gao and Ian Greenhouse and Ashley D. Harris and Naying He and Stefanie Heba and Nigel Hoggard and Tun-Wei Hsu and Jacobus F.A. Jansen and Alayar Kangarlu and Thomas Lange and R. Marc Lebel and Yan Li and Chien-Yuan E. Lin and Jy-Kang Liou and Jiing-Feng Lirng and Feng Liu and Joanna R. Long and Ruoyun Ma and Celine Maes and Marta Moreno-Ortega and Scott O. Murray and Sean Noah and Ralph Noeske and Michael D. Noseworthy and Georg Oeltzschner and Eric C. Porges and James J. Prisciandaro and Nicolaas A.J. Puts and Timothy P.L. Roberts and Markus Sack and Napapon Sailasuta and Muhammad G. Saleh and Michael-Paul Schallmo and Nicholas Simard and Diederick Stoffers and Stephan P. Swinnen and Martin Tegenthoff and Peter Truong and Guangbin Wang and Iain D. Wilkinson and Hans-Jörg Wittsack and Adam J. Woods and Hongmin Xu and Fuhua Yan and Chencheng Zhang and Vadim Zipunnikov and Helge J. Zöllner and Richard A.E. Edden},
keywords = {Editing, GABA, MEGA-PRESS, MRS, Quantification, Tissue correction},
abstract = {Accurate and reliable quantification of brain metabolites measured in vivo using 1H magnetic resonance spectroscopy (MRS) is a topic of continued interest. Aside from differences in the basic approach to quantification, the quantification of metabolite data acquired at different sites and on different platforms poses an additional methodological challenge. In this study, spectrally edited γ-aminobutyric acid (GABA) MRS data were analyzed and GABA levels were quantified relative to an internal tissue water reference. Data from 284 volunteers scanned across 25 research sites were collected using GABA+ (GABA + co-edited macromolecules (MM)) and MM-suppressed GABA editing. The unsuppressed water signal from the volume of interest was acquired for concentration referencing. Whole-brain T1-weighted structural images were acquired and segmented to determine gray matter, white matter and cerebrospinal fluid voxel tissue fractions. Water-referenced GABA measurements were fully corrected for tissue-dependent signal relaxation and water visibility effects. The cohort-wide coefficient of variation was 17% for the GABA + data and 29% for the MM-suppressed GABA data. The mean within-site coefficient of variation was 10% for the GABA + data and 19% for the MM-suppressed GABA data. Vendor differences contributed 53% to the total variance in the GABA + data, while the remaining variance was attributed to site- (11%) and participant-level (36%) effects. For the MM-suppressed data, 54% of the variance was attributed to site differences, while the remaining 46% was attributed to participant differences. Results from an exploratory analysis suggested that the vendor differences were related to the unsuppressed water signal acquisition. Discounting the observed vendor-specific effects, water-referenced GABA measurements exhibit similar levels of variance to creatine-referenced GABA measurements. It is concluded that quantification using internal tissue water referencing is a viable and reliable method for the quantification of in vivo GABA levels.}
}
@article{WESSELS2019200,
title = {Hybrid expert ensembles for identifying unreliable data in citizen science},
journal = {Engineering Applications of Artificial Intelligence},
volume = {81},
pages = {200-212},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619300041},
author = {Pieter Wessels and Nick Moran and Alison Johnston and Wenjia Wang},
keywords = {Citizen science, Ensemble, Expert system, BirdTrack, Classification, Diversity},
abstract = {Citizen science utilises public resources for scientific research. BirdTrack is such a project established in 2004 by the British Trust for Ornithology (BTO) for the public to log their bird observations through its web or mobile applications. It has accumulated over 40 million observations. However, the veracity of these observations needs to be checked and the current process involves time-consuming interventions by human experts. This research therefore aims to develop a more efficient system to automatically identify unreliable observations from large volume of records. This paper presents a novel approach — a Hybrid Expert Ensemble System (HEES) that combines an Expert System (ES) and machine induced models to perform the intended task. The ES is built based on human expertise and used as a base member of the ensemble. Other members are decision trees induced from county-based data. The HEES uses accuracy and diversity as criteria to select its members with an aim of improving its accuracy and reliability. The experiments were carried out using the county-based data and the results indicate that (1) the performance of the expert system is reasonable for some counties but varied considerably on others. (2) An HEES is more accurate and reliable than the Expert System and also other individual models, with Sensitivity of 85% for correctly identifying unreliable observations and Specificity of 99% for reliable observations. These results demonstrated that the proposed approach has the ability to be an alternative or additional means to validate the observations in a timely and cost-effective manner and also has a potential to be applied in other citizen science projects where the huge amount of data needs to be checked effectively and efficiently.}
}
@article{LAKS20191207,
title = {Clonal Decomposition and DNA Replication States Defined by Scaled Single-Cell Genome Sequencing},
journal = {Cell},
volume = {179},
number = {5},
pages = {1207-1221.e22},
year = {2019},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2019.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0092867419311766},
author = {Emma Laks and Andrew McPherson and Hans Zahn and Daniel Lai and Adi Steif and Jazmine Brimhall and Justina Biele and Beixi Wang and Tehmina Masud and Jerome Ting and Diljot Grewal and Cydney Nielsen and Samantha Leung and Viktoria Bojilova and Maia Smith and Oleg Golovko and Steven Poon and Peter Eirew and Farhia Kabeer and Teresa {Ruiz de Algara} and So Ra Lee and M. Jafar Taghiyar and Curtis Huebner and Jessica Ngo and Tim Chan and Spencer Vatrt-Watts and Pascale Walters and Nafis Abrar and Sophia Chan and Matt Wiens and Lauren Martin and R. Wilder Scott and T. Michael Underhill and Elizabeth Chavez and Christian Steidl and Daniel {Da Costa} and Yussanne Ma and Robin J.N. Coope and Richard Corbett and Stephen Pleasance and Richard Moore and Andrew J. Mungall and Colin Mar and Fergus Cafferty and Karen Gelmon and Stephen Chia and Gregory J. Hannon and Giorgia Battistoni and Dario Bressan and Ian Cannell and Hannah Casbolt and Cristina Jauset and Tatjana Kovačević and Claire Mulvey and Fiona Nugent and Marta Paez Ribes and Isabella Pearsall and Fatime Qosaj and Kirsty Sawicka and Sophia Wild and Elena Williams and Samuel Aparicio and Emma Laks and Yangguang Li and Ciara O’Flanagan and Austin Smith and Teresa Ruiz and Shankar Balasubramanian and Maximillian Lee and Bernd Bodenmiller and Marcel Burger and Laura Kuett and Sandra Tietscher and Jonas Windager and Edward Boyden and Shahar Alon and Yi Cui and Amauche Emenari and Dan Goodwin and Emmanouil Karagiannis and Anubhav Sinha and Asmamaw T. Wassie and Carlos Caldas and Alejandra Bruna and Maurizio Callari and Wendy Greenwood and Giulia Lerda and Yaniv Lubling and Alastair Marti and Oscar Rueda and Abigail Shea and Owen Harris and Robby Becker and Flaminia Grimaldi and Suvi Harris and Sara Vogl and Johanna A. Joyce and Jean Hausser and Spencer Watson and Sorhab Shah and Andrew McPherson and Ignacio Vázquez-García and Simon Tavaré and Khanh Dinh and Eyal Fisher and Russell Kunes and Nicolas A. Walton and Mohammad {Al Sa’d} and Nick Chornay and Ali Dariush and Eduardo Gonzales Solares and Carlos Gonzalez-Fernandez and Aybuke Kupcu Yoldas and Neil Millar and Xiaowei Zhuang and Jean Fan and Hsuan Lee and Leonardo Sepulveda Duran and Chenglong Xia and Pu Zheng and Marco A. Marra and Carl Hansen and Sohrab P. Shah and Samuel Aparicio},
keywords = {single cell, copy number, aneuploidy, tumor evolution, cancer genomics, DNA sequencing, genomic instability, tumor heterogeneity, cell cycle},
abstract = {Summary
Accurate measurement of clonal genotypes, mutational processes, and replication states from individual tumor-cell genomes will facilitate improved understanding of tumor evolution. We have developed DLP+, a scalable single-cell whole-genome sequencing platform implemented using commodity instruments, image-based object recognition, and open source computational methods. Using DLP+, we have generated a resource of 51,926 single-cell genomes and matched cell images from diverse cell types including cell lines, xenografts, and diagnostic samples with limited material. From this resource we have defined variation in mitotic mis-segregation rates across tissue types and genotypes. Analysis of matched genomic and image measurements revealed correlations between cellular morphology and genome ploidy states. Aggregation of cells sharing copy number profiles allowed for calculation of single-nucleotide resolution clonal genotypes and inference of clonal phylogenies and avoided the limitations of bulk deconvolution. Finally, joint analysis over the above features defined clone-specific chromosomal aneuploidy in polyclonal populations.}
}
@incollection{JAN2019293,
title = {Chapter 6 - Ecological Metabolomics: Challenges and Perspectives},
editor = {Sumira Jan and Parvaiz Ahmad},
booktitle = {Ecometabolomics},
publisher = {Academic Press},
pages = {293-378},
year = {2019},
isbn = {978-0-12-814872-3},
doi = {https://doi.org/10.1016/B978-0-12-814872-3.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128148723000060},
author = {Sumira Jan and Parvaiz Ahmad},
keywords = {Metabolomic databases, Linearization, Metabolome, Data mining, Metabolic flash, Statistical tools, Mega metabolome},
abstract = {Ecometabolomics intends to evaluate metabolomes, metabolic flux, and its modulation against varied environmental responses. This is an emerging field due to the advent of new and advanced technologies, such as 1H NMR spectrometry and hyphenated chromatographic techniques such as gas chromatography GC-MS and LC-MS, coupled with bioinformatics tools. The integration of ecometabolomics with the disciplines of genomics, transcriptomics, and ecological stoichiometry can advance the environmental sciences, thereby advancing our perspective on stress responses, species survival, population dynamics, trophic interaction, nutrient cycling, and global change. Numerous exciting issues remain to be explored using ecometabolomics in field conditions, implicating more than two trophic levels, or integrating the effects of abiotic factors with intra- and interspecific associations. This chapter offers an outline of ecometabolomics research. We begin with a synopsis of metabolomics’ role as an imperative tool in ecology for deciphering organisms and environmental interactions. This will be followed by a summary of how metabolomics is linked with metabolic regulation. The chapter will conclude with the challenges that researchers face when attempting to implement statistical tools for decoding population metabolomes, and in identifying mega metabolomes via multivariate data mining.}
}
@article{OTT2019123,
title = {Improving sustainability and cost efficiency for spare part allocation strategies by utilisation of additive manufacturing technologies},
journal = {Procedia Manufacturing},
volume = {33},
pages = {123-130},
year = {2019},
note = {Sustainable Manufacturing for Global Circular Economy: Proceedings of the 16th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919306237},
author = {Karl Ott and Heimo Pascher and Wilfried Sihn},
keywords = {additive manufacturing, spare parts management, cost model, spare part classification, allocation strategy},
abstract = {Currently, no proper cost models are available to assist managers in selecting part-specific allocation strategies for spare parts under consideration of metal additive manufacturing (AM). Therefore, sustainability aspects and cost efficiency over a product lifecycle show potential for optimisation. The aim of this paper is to propose a two-stage model as a basis for decision support in spare part allocation. The first stage introduces a multi-criteria part classification regarding classical criteria as well as criteria referring to AM. The impacts on different spare part allocation strategies like final stockpiling, conventional spare part production or AM on demand will be focused. Based on the first stage, a conceptual model for a comprehensive activity-based cost assessment will be adopted to assess the arising costs that occur for each of the compared allocation strategies. Evaluating the relevant factors for the specific product, process, warehousing and capital issues, the basis for choosing the best suitable spare part allocation strategy will be presented. The present document is a working paper, where the interim results of the intended concept model are introduced.}
}
@article{SCHMIDT2019615,
title = {Localised gear anomaly detection without historical data for reference density estimation},
journal = {Mechanical Systems and Signal Processing},
volume = {121},
pages = {615-635},
year = {2019},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2018.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S0888327018307696},
author = {Stephan Schmidt and P. Stephan Heyns},
keywords = {Diagnostics, Fluctuating operating conditions, Kullback-Leibler divergence, Bayesian},
abstract = {Performing condition monitoring on rotating machines which operate under fluctuating conditions remains a challenge, with robust diagnostic techniques being required for the condition inference task. Some of the developed diagnostic techniques rely on the availability of historical fault data, which is impractical or even impossible to obtain in many circumstances and therefore novelty detection techniques such as discrepancy analysis are used. However, discrepancy analysis assumes that the condition of the machine is the same throughout the signal in the model optimisation process i.e. no localised damage is present, which can pose problems if the training data unwittingly contain a component with localised damage. In this paper, an automatic procedure is proposed for diagnosing localised gear damage in the presence of fluctuating operating conditions, with no historical data being required to model the data used in the condition inference process. The continuous wavelet transform, principal component analysis and information theory are used to obtain divergence data of the gear under consideration. The divergence data are used with Bayesian data analysis techniques to automatically infer the presence of localised anomalies due to localised gear damage. The proposed technique is validated in two experimental investigations, with promising results being obtained.}
}
@article{HOSSAIN201977,
title = {Alternatives to calorie-based indicators of food security: An application of machine learning methods},
journal = {Food Policy},
volume = {84},
pages = {77-91},
year = {2019},
issn = {0306-9192},
doi = {https://doi.org/10.1016/j.foodpol.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0306919218307309},
author = {Marup Hossain and Conner Mullally and M. Niaz Asadullah},
keywords = {Food security, Poverty, Dietary diversity, Program targeting, Machine learning},
abstract = {Identifying food insecure households in an accurate and cost-effective way is important for targeted food policy interventions. Since predictive accuracy depends partly on which indicators are used to identify food insecure households, it is important to assess the performance of indicators that are relatively easy and inexpensive to collect yet can proxy for the “gold standard” food security indicator, calorie intake. We study the effectiveness of different variable combinations and methods in predicting calorie-based food security among poor households and communities in rural Bangladesh. We use basic household information as a benchmark set for predicting calorie-based food security. We then assess the gain in predictive power obtained by adding subjective food security indicators (e.g., self-reported days without sufficient food), the dietary diversity score (DDS), and the combination of both sets to our model of calorie-based food security. We apply machine learning as well as traditional econometric methods in estimation. We find that the overall predictive accuracy rises from 63% to 69% when we add the subjective and DDS sets to the benchmark set. Our study demonstrates that while alternative indicators and methods are not always accurate in predicting calorie intake, DDS related indicators do improve accuracy compared to a simple benchmark set.}
}
@article{ZHAO2019101052,
title = {User profiling from their use of smartphone applications: A survey},
journal = {Pervasive and Mobile Computing},
volume = {59},
pages = {101052},
year = {2019},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2019.101052},
url = {https://www.sciencedirect.com/science/article/pii/S1574119219300124},
author = {Sha Zhao and Shijian Li and Julian Ramos and Zhiling Luo and Ziwen Jiang and Anind K. Dey and Gang Pan},
keywords = {User profiling, Smartphone applications, Mobile sensing},
abstract = {The number and popularity of smartphone applications is rising dramatically. Users install and use applications depending on their needs and interests. Applications on smartphones convey lots of personal information, providing us a new lens to well profile users. In this paper, we first describe application information for user profiling. Second, we analyze what types of user information can be profiled from smartphone applications. Then, we overview the previous work and summarize the state-of-the-art methods for profiling users from application data. We also describe implications from different perspectives. Finally, the challenges in profiling users from smartphone applications are discussed.}
}
@article{PARA201930,
title = {Analyze, Sense, Preprocess, Predict, Implement, and Deploy (ASPPID): An incremental methodology based on data analytics for cost-efficiently monitoring the industry 4.0},
journal = {Engineering Applications of Artificial Intelligence},
volume = {82},
pages = {30-43},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619300727},
author = {Jesus Para and Javier {Del Ser} and Antonio J. Nebro and Urko Zurutuza and Francisco Herrera},
keywords = {Industry 4.0, Methodological data analytics, Process monitoring, Cost efficiency, Imbalanced learning},
abstract = {Industry 4.0 is revolutionizing decision making processes within the manufacturing industry. Among the technological portfolio enabling this revolution, the late literature has capitalized on the potential of data analytics for improving the production cycle at different stages, from resource provisioning to planning, delivery and storage. However, such a promising role of data analytics has been so far explored without a proper, quantitative inspection of the cost-improvement trade-off, nor has the process of acquiring sensors and extracting valuable information from their captured data formalized in a series of methodological steps. This paper introduces the Analyze, Sense, Preprocess, Predict, Implement and Deploy (ASPPID) methodology, an iterative decision workflow that spans from the acquisition of sensing equipment to the quantitative assessment of the contribution of their captured data to enhance the production step under focus. By placing the data scientist at the core of the workflow, this methodology helps improvement teams make informed decisions about which parts of the process need to be sensed, and how to exploit this information towards a verifiable improvement of the production cycle. The implementation of this methodology is exemplified in a real use case within the automotive industry, where the detection of defects in an annealing process can be modeled as a classification problem over a highly imbalanced dataset. Results obtained after applying the proposed ASPPID methodology show that the scrap ratio is reduced by sensing the correct part of the process at minimal investment costs, thus highlighting the crucial role of the data scientist in the management team of manufacturing plants.}
}
@article{YUAN2019381,
title = {Sweating the assets – The role of instrumentation, control and automation in urban water systems},
journal = {Water Research},
volume = {155},
pages = {381-402},
year = {2019},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2019.02.034},
url = {https://www.sciencedirect.com/science/article/pii/S0043135419301599},
author = {Zhiguo Yuan and Gustaf Olsson and Rachel Cardell-Oliver and Kim {van Schagen} and Angela Marchi and Ana Deletic and Christian Urich and Wolfgang Rauch and Yanchen Liu and Guangming Jiang},
keywords = {Distribution, ICA, IUWM, Sewer, Water supply, WWTP},
abstract = {Instrumentation, control and automation (ICA) are currently applied throughout the urban water system at water treatment plants, in water distribution networks, in sewer networks, and at wastewater treatment plants. However, researchers and practitioners specialising in respective urban water sub-systems do not frequently interact, and in most cases to date the application of ICA has been achieved in silo. Here, we review start-of-the-art ICA throughout these sub-systems, and discuss the benefits achieved in terms of performance improvement, cost reduction, and more importantly, the enhanced capacity of the existing infrastructure to cope with increased service demand caused by population growth and continued urbanisation. We emphasise the importance of integrated control within each of the sub-systems, and also across the entire urban water system. System-wide ICA will have increasing importance with the growing complexity of the urban water environment in cities of the future.}
}
@article{CHIN2019101348,
title = {Inferring fine-grained transport modes from mobile phone cellular signaling data},
journal = {Computers, Environment and Urban Systems},
volume = {77},
pages = {101348},
year = {2019},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2019.101348},
url = {https://www.sciencedirect.com/science/article/pii/S0198971519300638},
author = {Kimberley Chin and Haosheng Huang and Christopher Horn and Ivan Kasanicky and Robert Weibel},
keywords = {Transport mode detection, Mobile phone network data, Cellular signaling data, Rule-based heuristic, Random forest, Fuzzy logic},
abstract = {Due to the ubiquity of mobile phones, mobile phone network data (e.g., Call Detail Records, CDR; and cellular signaling data, CSD), which are collected by mobile telecommunication operators for maintenance purposes, allow us to potentially study travel behaviors of a high percentage of the whole population, with full temporal coverage at a comparatively low cost. However, extracting mobility information such as transport modes from these data is very challenging, due to their low spatial accuracy and infrequent/irregular temporal characteristics. Existing studies relying on mobile phone network data mostly employed simple rule-based methods with geographic data, and focused on easy-to-detect transport modes (e.g., train and subway) or coarse-grained modes (e.g., public versus private transport). Meanwhile, due to the lack of ground truth data, evaluation of these methods was not reported, or only for aggregate data, and it is thus unclear how well the existing methods can detect modes of individual trips. This article proposes two supervised methods - one combining rule-based heuristics (RBH) with random forest (RF), and the other combining RBH with a fuzzy logic system - and a third, unsupervised method with RBH and k-medoids clustering, to detect fine-grained transport modes from CSD, particularly subway, train, tram, bike, car, and walk. Evaluation with a labeled ground truth dataset shows that the best performing method is the hybrid one with RBH and RF, where a classification accuracy of 73% is achieved when differentiating these modes. To our knowledge, this is the first study that distinguishes fine-grained transport modes in CSD and validates results with ground truth data. This study may thus inform future CSD-based applications in areas such as intelligent transport systems, urban/transport planning, and smart cities.}
}
@article{PASICHNYI2019360,
title = {Data-driven building archetypes for urban building energy modelling},
journal = {Energy},
volume = {181},
pages = {360-377},
year = {2019},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.04.197},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219308370},
author = {Oleksii Pasichnyi and Jörgen Wallin and Olga Kordas},
keywords = {Building archetype, Urban building energy modelling, Building retrofitting, Electric heating, Stockholm},
abstract = {This paper presents an approach for using rich datasets to develop different building archetypes depending on the urban energy challenges addressed. Two cases (building retrofitting and electric heating) were analysed using the same city, Stockholm (Sweden), and the same input data, energy performance certificates and heat energy use metering data. The distinctive character of these problems resulted in different modelling workflows and archetypes being developed. The building retrofitting case followed a hybrid approach, integrating statistical and physical perspectives, estimating energy savings for 5532 buildings from seven retrofitting packages. The electric heating case provided an explicitly statistical data-driven view of the problem, estimating potential for improvement of power capacity of the local electric grid at peak electric power of 147 MW. The conclusion was that the growing availability of linked building energy data requires a shift in the urban building energy modelling (UBEM) paradigm from single-logic models to on-request multiple-purpose data intelligence services.}
}
@article{MOHAMMED2019102898,
title = {Casing structural integrity and failure modes in a range of well types - A review},
journal = {Journal of Natural Gas Science and Engineering},
volume = {68},
pages = {102898},
year = {2019},
issn = {1875-5100},
doi = {https://doi.org/10.1016/j.jngse.2019.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S1875510019301428},
author = {Auwalu I. Mohammed and Babs Oyeneyin and Bryan Atchison and James Njuguna},
keywords = {Casing, Failure modes, Well integrity, Logs},
abstract = {This paper focus on factors attributing to casing failure, their failure mechanism and the resulting failure mode. The casing is a critical component in a well and the main mechanical structural barrier element that provide conduits and avenue for oil and gas production over the well lifecycle and beyond. The casings are normally subjected to material degradation, varying local loads, induced stresses during stimulation, natural fractures, slip and shear during their installation and operation leading to different kinds of casing failure modes. The review paper also covers recent developments in casing integrity assessment techniques and their respective limitations. The taxonomy of the major causes and cases of casing failure in different well types is covered. In addition, an overview of casing trend utilisation and failure mix by grades is provided. The trend of casing utilisation in different wells examined show deep-water and shale gas horizontal wells employing higher tensile grades (P110 & Q125) due to their characteristics. Additionally, this review presents casing failure mixed by grades, with P110 recording the highest failure cases owing to its stiffness, high application in injection wells, shale gas, deep-water and high temperature and high temperature (HPHT) - wells with high failure probability. A summary of existing tools used for the assessment of well integrity issues and their respective limitations is provided and conclusions drawn.}
}
@article{ALTURJMAN2019101608,
title = {Smart parking in IoT-enabled cities: A survey},
journal = {Sustainable Cities and Society},
volume = {49},
pages = {101608},
year = {2019},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101608},
url = {https://www.sciencedirect.com/science/article/pii/S2210670718327173},
author = {Fadi Al-Turjman and Arman Malekloo},
keywords = {Smart parking system, ITS, IoT, Smart city},
abstract = {The rapid growth in population has led to substantial traffic bottlenecks in recent transportation systems. This not only causes significant air pollution, and waste in time and energy, but also signifies the issue of the auto-park scarcity. In the age of Internet of Things (IoT) and smart city ecosystems, smart parking and relevant innovative solutions are necessary towards more sustainable future cities. Smart parking with the help of sensors embedded in cars and city infrastructures can alleviate the deadlocks in parking problems and provide the best quality of services and profit to citizens. However, several design aspects should be well investigated and analyzed before implementing such solutions. In this paper, we classify the smart parking systems while considering soft and hard design factors. We overview the enabling technologies and sensors which have been commonly used in the literature. We emphasize the importance of data reliability, security, privacy and other critical design factors in such systems. Emerging parking trends in the ecosystem are investigated, while focusing on data interoperability and exchange. We also outline open research issues in the current state of smart parking systems and recommend a conceptual hybrid-parking model.}
}
@article{CHEN2019835,
title = {Global overview for energy use of the world economy: Household-consumption-based accounting based on the world input-output database (WIOD)},
journal = {Energy Economics},
volume = {81},
pages = {835-847},
year = {2019},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2019.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0140988319301707},
author = {G.Q. Chen and X.D. Wu and Jinlan Guo and Jing Meng and Chaohui Li},
keywords = {World economy, Household-consumption-based MRIO accounting, Energy use, Trade imbalance, Hubs},
abstract = {Globalization has integrated nations into a world economy. Based on the world input-output database (WIOD), this paper explored the energy use of the world economy under a household-consumption-based MRIO (multi-region input-output) accounting scheme. Pertaining to normative economics, the household-consumption-based MRIO accounting scheme corresponds to the value judgement of household consumption being the ultimate driver of the economy, which complements existing accounting methods based on different viewpoints. The energy use associated with the internationally traded products is calculated to be around one-fifth of the global total energy consumption. For China as the largest exporter and also the biggest deficit economy in terms of energy use, its trade imbalance is nearly the summation of that of the United States, the United Kingdom, Japan and Germany. Energy self-sufficiency rates by supply and by demand are respectively proposed. While the United States economy as the largest importer maintains the majority of the energy welfare denoted by the onsite energy use at home, China exports large quantities of energy use abroad. For economies like Germany, South Korea and Taiwan, they could be regarded as hubs that export a considerable amount of energy use abroad and absorb massive energy use from outside simultaneously. For sustainable use of energy resources, economies are suggested to carefully identify their roles in the global trading network of energy use.}
}
@article{MARRELLA201962,
title = {A design-time data-centric maturity model for assessing resilience in multi-party business processes},
journal = {Information Systems},
volume = {86},
pages = {62-78},
year = {2019},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917306063},
author = {Andrea Marrella and Massimo Mecella and Barbara Pernici and Pierluigi Plebani},
keywords = {Business process resilience, Artifact-centric modeling, Resilience maturity model, CMMN — Case Management Model and Notation},
abstract = {Nowadays, every business organization operates in ecosystems and cooperation is mandatory. If, on the one hand, this increases the opportunities for the involved organizations, on the other hand, every business partner is a potential source of failures with impacts on the entire ecosystem. To avoid that these failures, which are local to one of the organizations, would block the whole cooperation, resilience is a feature that multi-party business processes currently support at run-time, to cope with unplanned situations caused by those failures. In this work, we consider awareness of resilience in multi-party business processes during design-time, by focusing on the role of available – as an alternative to unreliable – data as a resource for increasing resiliency, as data exchange usually drives the cooperation among the parties. In fact, a proper analysis of involved data allows the process designer to identify (possible) failures, their impact, and thus improve the process model at the outset. A maturity model for resilience awareness is proposed, based on a modeling notation extending OMG CMMN — Case Management Model and Notation, and it is organized in different resiliency levels, which allow designers (i) to model at an increasing degree of detail how data and milestones should be defined in order to have resilient by-design process models and (ii) to quantify the distance between a process model and the complete achievement of a resiliency level.}
}
@incollection{SHEVAH201951,
title = {Chapter 3 - Impact of Persistent Droughts on the Quality of the Middle East Water Resources},
editor = {Satinder Ahuja},
series = {Separation Science and Technology},
publisher = {Academic Press},
volume = {11},
pages = {51-84},
year = {2019},
booktitle = {Evaluating Water Quality to Prevent Future Disasters},
issn = {1877-1718},
doi = {https://doi.org/10.1016/B978-0-12-815730-5.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012815730500003X},
author = {Y. Shevah},
keywords = {Middle East, Climate change, Water scarcity, Transboundary water resources, Water management, Water reuse, Desalination},
abstract = {In the Middle East, the most water-stressed part of the world, the water resources and water quality are severely affected by the climate change and the global warming, leading to frequent droughts and depleted water reserves. The low rainfall and desertification cause serious political instability, exacerbating the likelihood of failed states in this volatile region. The extreme climate coupled with mismanagement of water resources and inadequate infrastructure, deprive the growing population of safe drinking water, food, shelter, and stability, as reflected by the erupted 2011 “Arab Spring” and the on-going Iraq, Syrian, and Yemen civil wars which enflamed migration and dislocation of population with widespread consequences. Against this background, the region is to promote sustainable management of water resources, incorporating advanced and innovative technologies to support water demand management, adopting good governance, social empowerment, and the protection of aquatic ecosystems. Advanced water conservation, consumption administration, data management and public participation, as already available, supplemented by water reuse, recovery of resources and sea water desalination, expanding on early initiatives by prosperous countries, have yet to be practiced on a large scale, region-wide. For such desirable vision to materialize, cross border and upstream-downstream linkages, between riparian states, have to be addressed improving capacities and sharing experiences, based on water management systems fostered by UN, such as IWRM, 2030 Agenda and NBS, aiming to overcome the climate change and adapting to its consequences. The causes, impacts, and adaptation to climate change and mitigation measures, providing an integrated picture of water management and technological challenges are discussed in this chapter.}
}
@article{WILLMS20192620,
title = {Emerging trends from advanced planning to integrated business planning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {2620-2625},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.602},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319315897},
author = {Philipp Willms and Marcus Brandenburg},
keywords = {supply chain management, advanced planning, integrated business planning, sales, operations planning},
abstract = {For more than 20 years, the conceptualization of SC planning processes has been paralleled by the development of related planning systems. Traditional approaches focused on volume-based planning that matches demand and supply quantities, but more recently, value-based aspects and financial criteria have gained importance for SC planning. This study analyzes recent trends towards integrated business planning and studies the emergence of related system solutions. Related scientific literature is briefly summarized before market-leading software vendors and system providers are assessed based on publicly accessible information.}
}
@incollection{SCHNEIDER2019167,
title = {Chapter 8 - Predicting energy consumption using machine learning},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {167-186},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376000087},
author = {Jens Schneider and Matthias Dziubany and Anke Schmeink and Guido Dartmann and Klaus-Uwe Gollmer and Stefan Naumann},
keywords = {Energy consumption, Machine learning, Statistical learning, Prediction, Linear regression, Artificial neural networks, Learning from data},
abstract = {In the development of a sustainable smart infrastructure, the exact adaptation of energy production to the actual energy demand is of crucial importance. For this purpose, it is necessary to predict future energy requirements as accurately as possible. In this chapter, statistical and machine learning methods are presented that learn from historical consumption data to predict future consumption. In addition, the implementation of such a prognosis is described by using a real-world example, where the challenges encountered are discussed and the results are presented.}
}
@article{AKPOTI2019172,
title = {Review - Agricultural land suitability analysis: State-of-the-art and outlooks for integration of climate change analysis},
journal = {Agricultural Systems},
volume = {173},
pages = {172-208},
year = {2019},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2019.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X18300428},
author = {Komlavi Akpoti and Amos T. Kabo-bah and Sander J. Zwart},
keywords = {Agriculture, Land suitability analysis, Climate change, Multi-criteria evaluation, Machine learning, Predictors},
abstract = {Agricultural land suitability analysis (ALSA) for crop production is one of the key tools for ensuring sustainable agriculture and for attaining the current global food security goal in line with the Sustainability Development Goals (SDGs) of United Nations. Although some review studies addressed land suitability, few of them specifically focused on land suitability analysis for agriculture. Furthermore, previous reviews have not reflected on the impact of climate change on future land suitability and how this can be addressed or integrated into ALSA methods. In the context of global environmental changes and sustainable agriculture debate, we showed from the current review that ALSA is a worldwide land use planning approach. We reported from the reviewed articles 69 frequently used factors in ALSA. These factors were further categorized in climatic conditions (16), nutrients and favorable soils (34 of soil and landscape), water availability in the root zone (8 for hydrology and irrigation) and socio-economic and technical requirements (11). Also, in getting a complete view of crop’s ecosystems and factors that can explain and improve yield, inherent local socio-economic factors should be considered. We showed that this aspect has been often omitted in most of the ALSA modeling with only 38% of the total reviewed article using socio-economic factors. Also, only 30% of the studies included uncertainty and sensitivity analysis in their modeling process. We found limited inclusions of climate change in the application of the ALSA. We emphasize that incorporating current and future climate change projections in ALSA is the way forward for sustainable or optimum agriculture and food security. To this end, qualitative and quantitative approaches must be integrated into a unique ALSA system (Hybrid Land Evaluation System - HLES) to improve the land evaluation approach.}
}
@incollection{RUAN20191,
title = {Chapter 1 - Digital Assets as Economic Goods},
editor = {Keyun Ruan},
booktitle = {Digital Asset Valuation and Cyber Risk Measurement},
publisher = {Academic Press},
pages = {1-28},
year = {2019},
isbn = {978-0-12-812158-0},
doi = {https://doi.org/10.1016/B978-0-12-812158-0.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128121580000016},
author = {Keyun Ruan},
keywords = {Intrinsic digital value, extrinsic digital value, digital asset valuation, digital value matrix, characteristics of digital assets},
abstract = {This chapter first looks at the origins and philosophical concept of value, the different schools of thoughts from the subjective and objective views of value, the differences between intrinsic and extrinsic value, the definition of traditional assets, the definition of economic goods, and current asset valuation methods. Then, five unique characteristics of digital assets distinct from traditional assets are outlined. For example, the most fundamental assumptions of the discipline of economics are that: (1) the amount of resources available for a society is limited; and (2) the market exists as a measure of substitutability of those limited resources. To find optimal distribution of resources by means of free competition under these assumptions has always been thought to be the fundamental purpose of economics. Today, some argue “limitless” computing has arrived, making digital value theory inherently unique from past theories. This chapter then defines intrinsic digital value, extrinsic digital value, and a digital value matrix for categorizing digital assets according to their economic functions, this is, core value versus supporting value, digitized versus digital native, rather than technical functions, for example, software, hardware, etc. Attributes of digital assets that contribute to intrinsic and extrinsic value creation are then discussed along with measurement methods. This chapter closes with a review of current methods used for digital asset valuation.}
}
@incollection{PARKINSON201953,
title = {Chapter 2 - NMR Spectroscopy Methods in Metabolic Phenotyping},
editor = {John C. Lindon and Jeremy K. Nicholson and Elaine Holmes},
booktitle = {The Handbook of Metabolic Phenotyping},
publisher = {Elsevier},
pages = {53-96},
year = {2019},
isbn = {978-0-12-812293-8},
doi = {https://doi.org/10.1016/B978-0-12-812293-8.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122938000025},
author = {John A. Parkinson},
keywords = {NMR spectroscopy, Bioanalysis, TOCSY, HSQC, Diffusion, -resolved, Pure shift, PSYCHE, Nonuniform sampling, Hyperpolarization, Quantitation},
abstract = {NMR spectroscopy is an established, versatile, widely applicable analytical technique forming one of the cornerstone methods for metabolic profiling, metabonomics, metabolomics, and metabolic phenotyping applied to complex biofluid and tissue samples. From sample storage and preparation conditions through basic one-dimensional proton and two-dimensional proton only and proton-carbon correlated NMR techniques, this chapter moves on to consider the quest for ever more advanced NMR methods to improve data resolution, increase analyte detection limits, and robustly quantify components within complex biofluid and tissue matrices. The work is a guide to the established state-of-the-art in bioanalytical NMR methods, and through its course addresses some pitfalls to be aware of as well as providing a window through which the future potential shape of the field may be considered.}
}
@incollection{TYCZYNSKI201989,
title = {Chapter 6 - Real-World Epidemiologic Studies and Patient Registries},
editor = {Thao Doan and Cheryl Renz and Mondira Bhattacharya and Fabio Lievano and Linda Scarazzini},
booktitle = {Pharmacovigilance: A Practical Approach},
publisher = {Elsevier},
pages = {89-100},
year = {2019},
isbn = {978-0-323-58116-5},
doi = {https://doi.org/10.1016/B978-0-323-58116-5.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323581165000067},
author = {Jerzy Edward Tyczynski and Ryan Kilpatrick},
keywords = {Observational studies, Patient registries, Randomized clinical trial, Real-world epidemiologic studies, Risk factors, Risk profile},
abstract = {Real-world epidemiologic studies are an increasingly valuable source of evidence to establish disease burden, identify risk factors, and inform the benefit–risk profile of therapeutics. There are a number of different sources for such studies, including secondary sources, such as administrative claims and health records, as well as primary data collection via registries and surveys. The design of these studies must carefully consider both validity and efficiency. For large secondary data sources, sample size is often large, but capture of confounding factors may be suboptimal. Considering analytic approaches to address sources of bias in this context is important. Lastly, the ability to rely on real-world studies for decision making depends on developing and adhering to the best practices for ethical and transparent study conduct.}
}
@article{RUPNIK2019260,
title = {AgroDSS: A decision support system for agriculture and farming},
journal = {Computers and Electronics in Agriculture},
volume = {161},
pages = {260-271},
year = {2019},
note = {BigData and DSS in Agriculture},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917314205},
author = {Rok Rupnik and Matjaž Kukar and Petar Vračar and Domen Košir and Darko Pevec and Zoran Bosnić},
keywords = {Decision support system, Agriculture, Farming, 68U35, 68T05},
abstract = {Decision support systems, data analysis and data mining have become significant tools for improving business in professional world. The emerging technologies are making the precision agriculture omnipresent and allow potential for enriching it with computer-assisted decision support systems for farm management. In this paper we describe a novel system AgroDSS that bridges the gap between agricultural systems and state-of-the-art decision support methodology. The described system is intended for integration into the existing farm management information systems and provides a cloud-based decision support toolbox, allowing farmers to upload their own data, utilize several data analysis methods and retrieve their outputs. The implemented tools include predictive modeling with explanation, accuracy evaluation, time series clustering and decomposition, and structural change detection. They can help users make predictions for simulated scenarios and better understand the dependencies (interactions) within their domain. We apply the AgroDSS system on a case study of pest population dynamics, illustrating the potential for its use.}
}
@article{LUO2019274,
title = {Updating three-way decisions in incomplete multi-scale information systems},
journal = {Information Sciences},
volume = {476},
pages = {274-289},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518308089},
author = {Chuan Luo and Tianrui Li and Yanyong Huang and Hamido Fujita},
keywords = {Three-way decisions, Decision-theoretic rough sets, Multi-scale decision systems, Incremental learning},
abstract = {Three-way decisions theory has been developed and applied to decision making in uncertain environments. Decision tasks under ambiguity involving both missing values and hierarchical concept are abundant in real-world applications. In this paper, we study the update problem of three-way decisions with dynamic variation of scales in incomplete multi-scale information systems. The updating mechanisms of decision granules induced by the similarity relation are exploited with the cut refinement and coarsening through the attribute value taxonomies, and the dynamic tendencies of conditional probability are hence estimated with evolving granularity structure. Reasonable updating behaviors of probabilistic parameters are characterized in the framework of decision-theoretic rough sets, in order to relate optimism and pessimism decision making on the basis of the variation in attitudes to losses or costs of decisions during the periodic transformation between coarser granular-scale and refiner granular-scale of data. Then the dynamic tendencies of conditional probability with the updating behaviors of probabilistic parameters together are incorporated into the incremental process of updating three-way decisions. Theoretical justifications and semantic interpretations are provided to guarantee the satisfaction of the proposed method for incremental three-way decisions in multi-scale incomplete information systems. Experimental results on several UCI datasets show that the proposed incremental algorithms efficiently update three-way decisions for handling transformation of data scales caused by the cut refinement and coarsening through the attribute value taxonomies.}
}
@incollection{DASH2019159,
title = {Chapter 6 - Hepatic microphysiological systems: Current and future applications in drug discovery and development},
editor = {Jeffrey T. Borenstein and Vishal Tandon and Sarah L. Tao and Joseph L. Charest},
booktitle = {Microfluidic Cell Culture Systems (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {159-186},
year = {2019},
series = {Micro and Nano Technologies},
isbn = {978-0-12-813671-3},
doi = {https://doi.org/10.1016/B978-0-12-813671-3.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128136713000062},
author = {Ajit Dash and William R. Proctor},
keywords = {Liver, Microphysiological, In vitro, Hepatocyte, Chip, Flow, 3-D},
abstract = {In vitro liver models have widespread potential applications in drug discovery and development. The rapid loss of hepatocyte function in traditional cultures spurs the need for advanced microphysiological systems that incorporate in vivo attributes to restore liver cell phenotype. This chapter covers complex and more phenotypically relevant 2-D cultures to “liver-on-a-chip” technologies and describes published examples of the development, the characterization, and the proof-of-concept studies to define each model for applications in drug disposition and toxicology and as disease models. Finally, it outlines the validation required within specific context of use and discusses additional factors that will play a role in their eventual success and widespread adoption of these systems.}
}
@article{WANG2019220,
title = {Generalized reference evapotranspiration models with limited climatic data based on random forest and gene expression programming in Guangxi, China},
journal = {Agricultural Water Management},
volume = {221},
pages = {220-230},
year = {2019},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2019.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0378377419305499},
author = {Sheng Wang and Jinjiao Lian and Yuzhong Peng and Baoqing Hu and Hongsong Chen},
keywords = {Water resources, Climate change impact, Variable importance, Karst region},
abstract = {Accurate estimation of reference evapotranspiration (ET0) is very important in hydrological cycle research, and is essential in agricultural water management and allocation. The application of the standard model (FAO-56 Penman-Monteith) to estimate ET0 is restricted due to the absence of required meteorological data. Although many machine learning algorithms have been applied in modeling ET0 with fewer meteorological variables, most of the models are trained and tested using data from the same station, their performances outside the training station are not evaluated. This study aims to investigate generalization ability of the random forest (RF) algorithm in modeling ET0 with different input combinations (refer to different circumstances in missing data), and compares this algorithm with the gene-expression programming (GEP) method using the data from 24 weather stations in a karst region of southwest China. The ET0 estimated by the FAO-56 Penman-Monteith model was used as a reference to evaluate the derived RF-based and GEP-based models, and the coefficient of determination (R2), Nash-Sutcliffe coefficiency of efficiency (NSCE), root of mean squared error (RMSE), and percent bias (PBIAS) were used as evaluation criteria. The results revealed that the derived RF-based generalization ET0 models are successfully applied in modeling ET0 with complete and incomplete meteorological variables (R2, NSCE, RMSE and PBIAS ranged from 0.637 to 0.987, 0.626 to 0.986, 0.107 to 0.563 mm day−1, and −2.916% to 1.571%, respectively), and seven RF-based models corresponding to different incomplete data circumstances are proposed. The GEP-based generalization ET0 models are also proposed, and they produced promising results (R2, NSCE, RMSE and PBIAS ranged from 0.639 to 0.944, 0.636 to 0.942, 0.222 to 0.555 mm day−1, and −1.98% to 0.248%, respectively). Although the RF-based ET0 models performed slightly better than the GEP-based models, the GEP approach has the ability to give explicit expressions between the dependent and independent variables, which is more convenient for irrigators with minimal computer skills. Therefore, we recommend applying the RF-based models in water balance research, and the GEP-based models in agricultural irrigation practice. Moreover, the models performance decreased with periods due to climate change impact on ET0. At last, both of the two methods have the ability to assess the importance of predictors, the order of the importance of meteorological variables on ET0 in Guangxi is: sunshine duration, air temperature, relative humidity, and wind speed.}
}
@article{BALAMURALIKRISHNA20191,
title = {Possibilistic keys},
journal = {Fuzzy Sets and Systems},
volume = {376},
pages = {1-36},
year = {2019},
note = {Theme: Computer Science},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0165011419300788},
author = {Nishita Balamuralikrishna and Yingnan Jiang and Henning Koehler and Uwe Leck and Sebastian Link and Henri Prade},
keywords = {Armstrong relation, Axiomatization, Constraint maintenance, Database, Extremal combinatorics, Discovery, Implication, Key, Possibility theory, Uncertain data},
abstract = {Possibility theory is applied to introduce and reason about the fundamental notion of a key for uncertain data. Uncertainty is modeled qualitatively by assigning to tuples of data a degree of possibility with which they occur in a relation, and assigning to keys a degree of certainty which says to which tuples the key applies. The associated implication problem is characterized axiomatically and algorithmically. Using extremal combinatorics, we then characterize the families of non-redundant possibilistic keys that attain maximum cardinality. In addition, we show how to compute for any given set of possibilistic keys a possibilistic Armstrong relation, that is, a possibilistic relation that satisfies every key in the given set and violates every possibilistic key not implied by the given set. We also establish an algorithm for the discovery of all possibilistic keys that are satisfied by a given possibilistic relation. It is shown that the computational complexity of computing possibilistic Armstrong relations is precisely exponential in the input, and the decision variant of the discovery problem is NP-complete as well as W[2]-complete in the size of the possibilistic key. Further applications of possibilistic keys in constraint maintenance, data cleaning, and query processing are illustrated by examples. The computation of possibilistic Armstrong relations and discovery of possibilistic keys from possibilistic relations have been implemented as prototypes. Extensive experiments with these prototypes provide insight into the size of possibilistic Armstrong relations and the time to compute them, as well as the time it takes to compute a cover of the possibilistic keys that hold on a possibilistic relation, and the time it takes to remove any redundant possibilistic keys from this cover.}
}
@article{ANDRACHUK2019430,
title = {Smartphone technologies supporting community-based environmental monitoring and implementation: a systematic scoping review},
journal = {Biological Conservation},
volume = {237},
pages = {430-442},
year = {2019},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2019.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0006320719305816},
author = {Mark Andrachuk and Melissa Marschke and Charlotte Hings and Derek Armitage},
keywords = {Community-based monitoring, Environmental monitoring, Citizen science, Smartphones, Mobile technologies, Conservation},
abstract = {The prospect of leveraging new technologies for community-based environmental monitoring has captured the imagination of many scientists, policy makers, and conservation professionals. This systematic review examines the state of knowledge and trends in the peer-reviewed literature related to the use of smartphone technologies for community and citizen science environmental monitoring. We organize our findings in relation to data collection and data handling, the process of developing smartphone applications, and the ways that outcomes are reported. While the literature is nascent and technological advances are continually opening new opportunities, it is notable that there is limited scholarship that explicitly connects the monitoring function of smartphones to tangible conservation action (e.g., only 10 percent of the papers analysed data collected by smartphones, let alone making connections to required actions or policy). We discuss two central implications in terms of research-implementation spaces for environmental monitoring with smartphones: (1) what we identify as the cost paradox, the lack of recognition of actual costs of app development, monitoring, and implementation; and (2) the need to center the role of people and partnerships in order to ask more precise questions about outcomes for app users and conservation impacts from data collection. We conclude with a call for more research on costs and actual impacts, documentation of factors that lead to successes and failures, and how digital divides influence conservation outcomes. Our intent is not to call into question the potential impacts of smartphone technologies, but to encourage further understanding of how and when they can be most useful.}
}
@incollection{20191001,
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {1001-1077},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.09001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338090014}
}
@incollection{WALTER2019145,
title = {Chapter 7 - AI-based sensor platforms for the IoT in smart cities},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {145-166},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.00007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376000075},
author = {Klaus-Dieter Walter},
keywords = {IoT sensor, Wireless sensor network, Embedded system, Artificial intelligence algorithm, Decentralized intelligence},
abstract = {More and more people live in big cities, today more than 50% of the world population. All these people have to deal with many things of daily life including food, water, energy, communication, and entertainment. Also, the disposal of garbage, sewage, or the growing traffic must be managed and organized while minimizing energy consumption and protect the environment. These activities require in the future a new kind of Internet of things sensor concepts and a stronger use of modern IT and communication technologies. This should lead to smart cities with innovative types of sensor data collection and processing systems which are able to make intelligent real-time decisions to manage assets and resources efficiently. There are many new tasks for state-of-the-art embedded systems within future sensors and measuring technology. Furthermore, we need a more and more obvious paradigm shift from centralized to decentralized, autonomous control with the goal of highly flexible environments and localized artificial intelligence.}
}
@article{YIGITCANLAR2019104187,
title = {The making of smart cities: Are Songdo, Masdar, Amsterdam, San Francisco and Brisbane the best we could build?},
journal = {Land Use Policy},
volume = {88},
pages = {104187},
year = {2019},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2019.104187},
url = {https://www.sciencedirect.com/science/article/pii/S0264837719309093},
author = {Tan Yigitcanlar and Hoon Han and Md. Kamruzzaman and Giuseppe Ioppolo and Jamile Sabatini-Marques},
keywords = {Smart cities, Smart urbanism, Smart urban technology, Sustainable urban development, Sustainable urbanism, Urban policymaking, Climate emergency},
abstract = {Transforming urban areas into prosperous, liveable, and sustainable settlements is a longstanding goal for local governments. Today, countless urban settlements across the globe have jumped into the so-called ‘smart city’ bandwagon to achieve this goal. Under the smart city agenda, presently, many government agencies are attempting to engineer an urban transformation to tackle urban prosperity, liveability, and sustainability issues mostly through the means of technology solutions. Nonetheless, the notion of smart cities is ambiguous, and there are limited conceptual frameworks to assist cities and their administrations in understanding the big picture view of this urban development paradigm. The aim of this paper is to generate a clear understanding on the making of successful smart city practices. This is done by elaborating the smart cities notion through a multidimensional conceptual framework, examining smart city best practices across the globe—i.e., Songdo, Masdar, Amsterdam, San Francisco, Brisbane—, and providing insights of smart city approaches from these cases. The findings of the study disclose the need for a comprehensive smart city conceptualisation to inform policymaking and consequently the practice. This will help in the formation of a much-needed smart urbanism model for the resilient settlements of the climate emergency era.}
}
@article{ISLAM2019117787,
title = {E-waste in Australia: Generation estimation and untapped material recovery and revenue potential},
journal = {Journal of Cleaner Production},
volume = {237},
pages = {117787},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.117787},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619326472},
author = {Md Tasbirul Islam and Nazmul Huda},
keywords = {Waste electrical and electronic equipment (WEEE), Circular economy, Estimation and generation, Material, NTCRS, Revenue, Australia},
abstract = {This study presents an estimation of electrical and electronic equipment (EEE) products put-on-market (PoM), electronic-waste (e-waste) generation and the stock of EEE products in Australia from the year 2000–2047, considering seven categories that encompass 51 different types of product. Holt's double-exponential smoothing and dynamic lifespans (using the Weibull distribution function) are applied to compute past and future PoM and in e-waste generation, respectively. With the estimation, it is found that EEE PoM was increased from 470 kilo tons (kt) to 2135 kt in a timeframe of 2000–2015. On the other hand, e-waste generation was 115 kt in the year 2000, which then increased to 485 kt in the year 2010. For the projected period (2018–2047), the annual average growth of e-waste generation will be around 3%. E-waste generation will increase, particularly for large household appliances (LHA), small household appliances (SHA) and consumer equipment (CE), in terms of weight. This study evaluates the potentially recoverable material and revenue potential of regulated products (e.g., computer, televisions and other IT peripherals currently considered under the national television and computer recycling scheme (NTCRS) and mobile phones) and e-waste generated outside of the current product coverage. The gap between revenue generation from regulated and non-regulated products is quantitatively assessed for the first time and reported in this paper. Sensitivity and uncertainty analysis performed by Monte Carlo simulation showed the robustness and accuracy of this study. This quantification will provide invaluable insights to policymakers, including products in future legislative reform as well as the development of the recycling industry in Australia. Furthermore, this study presents a transparent process of calculation for time-series data that can be used for e-waste generation estimation for other countries, as well.}
}
@article{XIAO2019214,
title = {Exploring the moderators and causal process of trust transfer in online-to-offline commerce},
journal = {Journal of Business Research},
volume = {98},
pages = {214-226},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.01.069},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319300888},
author = {Lin Xiao and Yucheng Zhang and Bin Fu},
keywords = {Perceived effectiveness of dispute resolution, Moderators of trust transfer, Online-to-offline commerce, Perceived effectiveness of feedback mechanisms},
abstract = {This study attempted to explore the boundary conditions of trust transfer in the online-to-offline commerce context, which is overlooked in prior research. In Study 1, cross-sectional data were collected from 417 consumers to examine the research model. In Study 2, to confirm causality of trust transfer, longitudinal data were collected and analyzed using a cross-lagged panel model. Results indicated that trust in the intermediary platform positively influences trust in the user community, which further positively influences trust in the focal merchant. Perceived effectiveness of dispute resolution strengthens the impact of trust in the intermediary platform on trust in the focal merchant, while perceived effectiveness of the feedback mechanisms strengthens the impact of trust in the user community on trust in the focal merchant. From a theoretical perspective, this study extends insights into trust transfer theory by identifying the boundary conditions of trust transfer. From a practical perspective, it informs intermediary platforms on how to manage dispute resolution and feedback mechanisms effectively to succeed in online-to-offline commerce. It also helps merchants in selecting the most effective intermediary platforms with which to cooperate.}
}
@article{ZHANG2019354,
title = {Radiological images and machine learning: Trends, perspectives, and prospects},
journal = {Computers in Biology and Medicine},
volume = {108},
pages = {354-370},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519300642},
author = {Zhenwei Zhang and Ervin Sejdić},
keywords = {Deep learning, Machine learning, Imaging modalities, Deep neural network},
abstract = {The application of machine learning to radiological images is an increasingly active research area that is expected to grow in the next five to ten years. Recent advances in machine learning have the potential to recognize and classify complex patterns from different radiological imaging modalities such as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography imaging. In many applications, machine learning based systems have shown comparable performance to human decision-making. The applications of machine learning are the key ingredients of future clinical decision making and monitoring systems. This review covers the fundamental concepts behind various machine learning techniques and their applications in several radiological imaging areas, such as medical image segmentation, brain function studies and neurological disease diagnosis, as well as computer-aided systems, image registration, and content-based image retrieval systems. Synchronistically, we will briefly discuss current challenges and future directions regarding the application of machine learning in radiological imaging. By giving insight on how take advantage of machine learning powered applications, we expect that clinicians can prevent and diagnose diseases more accurately and efficiently.}
}
@article{MARTIN201939,
title = {False confidence, non-additive beliefs, and valid statistical inference},
journal = {International Journal of Approximate Reasoning},
volume = {113},
pages = {39-73},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X19300696},
author = {Ryan Martin},
keywords = {Bayes, Fiducial, Inferential model, p-Value, Plausibility function, Random set},
abstract = {Statistics has made tremendous advances since the times of Fisher, Neyman, Jeffreys, and others, but the fundamental and practically relevant questions about probability and inference that puzzled our founding fathers remain unanswered. To bridge this gap, I propose to look beyond the two dominating schools of thought and ask the following three questions: what do scientists need out of statistics, do the existing frameworks meet these needs, and, if not, how to fill the void? To the first question, I contend that scientists seek to convert their data, posited statistical model, etc., into calibrated degrees of belief about quantities of interest. To the second question, I argue that any framework that returns additive beliefs, i.e., probabilities, necessarily suffers from false confidence—certain false hypotheses tend to be assigned high probability—and, therefore, risks systematic bias. This reveals the fundamental importance of non-additive beliefs in the context of statistical inference. But non-additivity alone is not enough so, to the third question, I offer a sufficient condition, called validity, for avoiding false confidence, and present a framework, based on random sets and belief functions, that provably meets this condition. Finally, I discuss characterizations of p-values and confidence intervals in terms of valid non-additive beliefs, which imply that users of these classical procedures are already following the proposed framework without knowing it.}
}
@article{SI20191028,
title = {IoT information sharing security mechanism based on blockchain technology},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1028-1040},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19312725},
author = {Haiping Si and Changxia Sun and Yanling Li and Hongbo Qiao and Lei Shi},
keywords = {Blockchain, IoT, Location information, Storage},
abstract = {The blockchain is the first distributed recording system with its own trust mechanism. It builds a reliable architecture for decentralized control through multi-node information redundancy. Based on this, this paper proposes lightweight IoT information sharing security framework based on blockchain technology. The framework adopts a double-chain model combining data blockchain and transaction blockchain: distributed storage and tamper-proof of data in the data blockchain, and improved by the improved practical Byzantine fault-tolerant (PBFT) mechanism consensus algorithm. Data registration efficiency; resource and data transactions in the transaction blockchain is improved transaction efficiency and privacy protection through improved algorithms based on partial blind signature algorithms. A dynamic game method of node cooperation is proposed to prevent malicious behavior of local dominance. The state of the unknown node is estimated by reporting the node’s institutional reputation value; the high-trust reference report is used to correct the weight of the malicious node in the overall report and node merging, and finally reach the Bayesian equilibrium. Finally, the simulation experiment department carried out verification analysis on the anti-attack capability, double-strand processing capability and delay. The results show that the framework is safe, effective and feasible, and it is feasible to verify the location information of the system for secure storage devices.}
}
@article{MONTELLA2019103,
title = {Workflow-based automatic processing for Internet of Floating Things crowdsourced data},
journal = {Future Generation Computer Systems},
volume = {94},
pages = {103-119},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18307672},
author = {Raffaele Montella and Diana {Di Luccio} and Livia Marcellino and Ardelio Galletti and Sokol Kosta and Giulio Giunta and Ian Foster},
keywords = {Workflows, Data crowd sourcing, Mobile computing, Cloud computing, GPGPU virtualization, Internet of Things, Bathymetry interpolation},
abstract = {Data from sensors incorporated into mobile devices, such as networked navigational sensors, can be used to capture detailed environmental information. We describe here a workflow and framework for using sensors on boats to construct unique new datasets of underwater topography (bathymetry). Starting with a large number of measurements of position, depth, etc., obtained from such an Internet of Floating Things, we illustrate how, with a specialized protocol, data can be communicated to cloud resources, even when using delayed, intermittent, or disconnected networks. We then propose a method for automatic sensor calibration based on a novel reputation approach. Sampled depth data are interpolated efficiently on a cloud computing platform in order to provide a continuously updated bathymetric database. Our prototype implementation uses the FACE-IT Galaxy workflow engine to manage network communication and exploits the computational power of GPGPUs in a virtualized cloud environment, working with a CUDA-parallel algorithm, for efficient data processing. We report on an initial evaluation involving data from a sailing vessel in Italian coastal waters.}
}
@incollection{MARTENS201913,
title = {2 - Measuring transport equity: Key components, framings and metrics},
editor = {Karen Lucas and Karel Martens and Floridea {Di Ciommo} and Ariane Dupont-Kieffer},
booktitle = {Measuring Transport Equity},
publisher = {Elsevier},
pages = {13-36},
year = {2019},
isbn = {978-0-12-814818-1},
doi = {https://doi.org/10.1016/B978-0-12-814818-1.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128148181000020},
author = {Karel Martens and Jeroen Bastiaanssen and Karen Lucas},
keywords = {Indicator, Metrics, Equity, Distribution, Equality, Equalization, Need, Proportionality, Benefits, Burdens, Accessibility, Mobility, Externalities, Air pollution, Traffic safety, Health, Active travel},
abstract = {In this chapter, we recommend that the development of equity indicators should account for three components: (i) the benefits and burdens of interest, (ii) the population groups over which they are distributed; and (iii) a clear conception of what a “morally proper distribution” of benefit or burden should be. We present a simple framework of how to move from a general conceptualization of a benefit or burden to a more precise definition of suitable individual variables that can be subjected to an equity analysis. The assessment of equity also hinges on the identification of different population groups, along multiple dimensions that can represent advantage or disadvantage: income, gender, age, ethnicity, ability, and residential location. We end the chapter with an overview of the linchpin of an equity indicator: an explicit normative standard specifying what is desirable and what is not. Taken together, these three components are the ingredients of the equity indicators presented in the remaining chapters of this book.}
}
@incollection{HALIM2019241,
title = {Chapter 11 - Road map to best practices},
editor = {Abdel B. Halim},
booktitle = {Biomarkers, Diagnostics and Precision Medicine in the Drug Industry},
publisher = {Academic Press},
pages = {241-273},
year = {2019},
isbn = {978-0-12-816121-0},
doi = {https://doi.org/10.1016/B978-0-12-816121-0.00011-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128161210000118},
author = {Abdel B. Halim},
keywords = {Mitigate challenges, unsustainable practice, diagnostic evil, opportunity and strategy, effective partnership, LDT versus universal IVD, illusive lab claims, basis of cutoffs, PD-L1 dilemma, companion versus complementary},
abstract = {Fueled by the extremely wide gap between diagnostic and pharmaceutical industries, biomarkers and precision medicine have been faced by a significant number of critical challenges which put most of efforts and spend in vain. The challenges and limitations are unintentionally or, likely, intentionally ignored, overlooked, or underestimated—scientific buzz and attractive claims, but not bad news, always find audience and supporters. Unfortunately, there is no foreseen universal fix for the issues which propagate, spread, and get worse over time, and each pharma/biotech company has to work vigilantly to protect its assets. Recognizing the problem is the first step, and building strategy and infrastructure with the implementation of the right tools and mindsets are the cornerstones for best practices. In addition to the tips which were given throughout this book, this chapter will provide practical procedures to establish an efficient partnership with lab and diagnostic partners, and mitigate the biological, preanalytical, analytical, and clinical challenges.}
}
@article{ECKHARDT2019398,
title = {Systematic literature review of methodologies for assessing the costs of disasters},
journal = {International Journal of Disaster Risk Reduction},
volume = {33},
pages = {398-416},
year = {2019},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212420918306174},
author = {Daniel Eckhardt and Adriana Leiras and Antônio Márcio Tavares Thomé},
keywords = {Cost assessment, Methodology, Disaster, Natural hazards, Economic assessment, Framework},
abstract = {The importance of this study lies with the significant number of disasters reported over the past decades, the financial volume traded, and due the associated damages and losses. Despite its importance, finding an economic cost assessment methodology to address disasters of small and large proportions as well as to cover all disaster cost types is a challenge. This study, through a systematic literature review (SLR), offers a taxonomy, a framework and a research agenda for assessing the economic costs of disasters. To meet these goals, 1446 peer-reviewed papers and 2825 grey documents were retrieved and analyzed. The results highlight (i) the use of new technologies (such as social media and crowdsourcing) and pre-defined economic costs indicators for improving the disaster assessment response time; (ii) that there is a lack of usage of economic methods (e.g., econometrics, input-output) within the existing methodologies; (iii) and that there are few publications addressing the economic costs of slow-onset disasters. Also, the proposed framework aims to unify, through a critical analysis of the selected documents, common patterns, best practices, functionalities and actions to obtain an economic assessment independent of type and size of the disaster to be assessed. Finally, this study shows the importance of including grey literature in systematic reviews, as 50% of the selected disaster assessment methodologies came from this source.}
}
@article{MARTINEZHERNANDEZ201966,
title = {Robust depth-based estimation of the functional autoregressive model},
journal = {Computational Statistics & Data Analysis},
volume = {131},
pages = {66-79},
year = {2019},
note = {High-dimensional and functional data analysis},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2018.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167947318301415},
author = {Israel Martínez-Hernández and Marc G. Genton and Graciela González-Farías},
keywords = {Functional autoregression model, Functional data analysis, Functional regression model, Functional time series, Influence function, Robust estimator},
abstract = {A robust estimator for functional autoregressive models is proposed, the Depth-based Least Squares (DLS) estimator. The DLS estimator down-weights the influence of outliers by using the functional directional outlyingness as a centrality measure. It consists of two steps: identifying the outliers with a two-stage functional boxplot, then down-weighting the outliers using the functional directional outlyingness. Theoretical properties of the DLS estimator are investigated such as consistency and boundedness of its influence function. Through a Monte Carlo study, it is shown that the DLS estimator performs better than estimators based on Principal Component Analysis (PCA) and robust PCA, which are the most commonly used. To illustrate a practical application, the DLS estimator is used to analyze a dataset of ambient CO2 concentrations in California.}
}
@article{20191,
title = {Abstracts of the 2019 American Dairy Science Association® Annual Meeting, June 23–26, 2019, Cincinnati, Ohio},
journal = {Journal of Dairy Science},
volume = {102},
pages = {1-472},
year = {2019},
issn = {0022-0302},
doi = {https://doi.org/10.1016/S0022-0302(20)30815-8},
url = {https://www.sciencedirect.com/science/article/pii/S0022030220308158}
}
@incollection{2019585,
title = {Index},
editor = {John C. Lindon and Jeremy K. Nicholson and Elaine Holmes},
booktitle = {The Handbook of Metabolic Phenotyping},
publisher = {Elsevier},
pages = {585-597},
year = {2019},
isbn = {978-0-12-812293-8},
doi = {https://doi.org/10.1016/B978-0-12-812293-8.09989-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122938099898}
}
@article{KOUROUBALI2019103166,
title = {The new European interoperability framework as a facilitator of digital transformation for citizen empowerment},
journal = {Journal of Biomedical Informatics},
volume = {94},
pages = {103166},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103166},
url = {https://www.sciencedirect.com/science/article/pii/S153204641930084X},
author = {Angelina Kouroubali and Dimitrios G. Katehakis},
keywords = {European interoperability framework, Coordinated care, Electronic health record, National infrastructures, Cross-border healthcare, Personal health record},
abstract = {Healthcare is a highly regulated domain. Seamless, online access to integrated electronic health records for citizens is still far from becoming a reality. The implementation of personally managed health data systems still needs to overcome several interoperability, usability, ethics, security, and regulatory issues to deliver the envisioned benefits. This paper offers a policy viewpoint on how the new European Interoperability Framework (EIF) may benefit the implementation of eHealth systems for the management of personal health information for citizens. Interoperability facilitates sharing of health and illness experiences, coordinated care and research for citizen empowerment and improved health outcomes. The adoption of principles relevant to core interoperability and generic user needs and expectations, as described in the new EIF, in line with European and national regulations are quite essential for the development of safe and secure patient access services to support mobility. An interoperability framework facilitates the creation of the appropriate context in which personal health record applications can be designed and implemented in support of disease specific solutions, such as chronic non-malignant pain, diabetes and cancer. It is evident that no solution will fit all circumstances. However, the new EIF, when adapted for personally managed health data, provides a useful and relevant framework to facilitate implementation and adoption of personal health record systems within a coordinated care environment. Practical implications of this work relate to the need of multi-disciplinary cooperation and European level compatibility and sustainability of the underlying infrastructures required to support reliable and secure access to and sharing of medical data, as well as the readiness to address continuously evolving functional and non-functional requirements for regional, national, and cross-border settings.}
}
@article{YANIKOGLU2019799,
title = {A survey of adjustable robust optimization},
journal = {European Journal of Operational Research},
volume = {277},
number = {3},
pages = {799-813},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2018.08.031},
url = {https://www.sciencedirect.com/science/article/pii/S0377221718307264},
author = {İhsan Yanıkoğlu and Bram L. Gorissen and Dick {den Hertog}},
keywords = {Semi-infinite programming, Robust optimization, Adjustable robust optimization, Multistage decision making},
abstract = {Static robust optimization (RO) is a methodology to solve mathematical optimization problems with uncertain data. The objective of static RO is to find solutions that are immune to all perturbations of the data in a so-called uncertainty set. RO is popular because it is a computationally tractable methodology and has a wide range of applications in practice. Adjustable robust optimization (ARO), on the other hand, is a branch of RO where some of the decision variables can be adjusted after some portion of the uncertain data reveals itself. ARO generally yields a better objective function value than that in static robust optimization because it gives rise to more flexible adjustable (or wait-and-see) decisions. Additionally, ARO also has many real life applications and is a computationally tractable methodology for many parameterized adjustable decision variables and uncertainty sets. This paper surveys the state-of-the-art literature on applications and theoretical/methodological aspects of ARO. Moreover, it provides a tutorial and a road map to guide researchers and practitioners on how to apply ARO methods, as well as, the advantages and limitations of the associated methods.}
}
@article{LI2019735,
title = {Predicting ground-level PM2.5 concentrations in the Beijing-Tianjin-Hebei region: A hybrid remote sensing and machine learning approach},
journal = {Environmental Pollution},
volume = {249},
pages = {735-749},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.03.068},
url = {https://www.sciencedirect.com/science/article/pii/S0269749118353387},
author = {Xintong Li and Xiaodong Zhang},
keywords = {Remote sensing, Aerosol optical depth, Machine learning, PM, Random forest},
abstract = {An accurate estimation of PM2.5 (fine particulate matters with diameters ≤ 2.5 μm) concentration is critical for health risk assessment and generating air pollution control strategies. In this study, a hybrid remote sensing and machine learning approach, named RSRF model is proposed to estimate daily ground-level PM2.5 concentrations, which integrates Random Forest (RF), one of machine learning (ML) models, and aerosol optical depth (AOD), one of remote sensing (RS) products. The proposed RSRF model provides an opportunity for an adequate characterization of real-time spatiotemporal PM2.5 distributions at uninhabited places and complex surfaces. It also offers advantages in handling complicated non-linear relationships among a large number of meteorological, environmental and air pollutant factors, as well as ever-increasing environmental data sets. The applicability of the proposed RSRF model is tested in the Beijing-Tianjin-Hebei region (BTH region) during 2015–2017. Deep Blue (DB) AOD from Aqua-retrieved Collection 6.1 (C_61) aerosol products of Moderate Resolution Imaging Spectroradiometer (MODIS) is validated with Aerosol Robotic Network. The validation results indicate C_61 DB AOD has a high correlation with ground based AOD in the BTH region. The proposed RSRF model performed well in characterizing spatiotemporal variations of annual and seasonal PM2.5 concentrations. It not only is useful to quantify the relationships between PM2.5 and relevant factors such as DB AOD, meteorological and air pollutant variables, but also can provide decision support for air pollution control at a regional environment during haze periods.}
}
@article{BOURDEAU2019101533,
title = {Modeling and forecasting building energy consumption: A review of data-driven techniques},
journal = {Sustainable Cities and Society},
volume = {48},
pages = {101533},
year = {2019},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101533},
url = {https://www.sciencedirect.com/science/article/pii/S2210670718323862},
author = {Mathieu Bourdeau and Xiao qiang Zhai and Elyes Nefzaoui and Xiaofeng Guo and Patrice Chatellier},
keywords = {Building energy consumption, Building load forecasting, Data-driven techniques, Machine learning},
abstract = {Building energy consumption modeling and forecasting is essential to address buildings energy efficiency problems and take up current challenges of human comfort, urbanization growth and the consequent energy consumption increase. In a context of integrated smart infrastructures, data-driven techniques rely on data analysis and machine learning to provide flexible methods for building energy prediction. The present paper offers a review of studies developing data-driven models for building scale applications. The prevalent methods are introduced with a focus on the input data characteristics and data pre-processing methods, the building typologies considered, the targeted energy end-uses and forecasting horizons, and accuracy assessment. A special attention is also given to different machine learning approaches. Based on the results of this review, the latest technical improvements and research efforts are synthesized. The key role of occupants’ behavior integration in data-driven modeling is discussed. Limitations and research gaps are highlighted. Future research opportunities are also identified.}
}
@article{GUO2019185,
title = {An improved intuitionistic fuzzy interval two-stage stochastic programming for resources planning management integrating recourse penalty from resources scarcity and surplus},
journal = {Journal of Cleaner Production},
volume = {234},
pages = {185-199},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.06.183},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619321456},
author = {Shanshan Guo and Fan Zhang and Chenglong Zhang and Youzhi Wang and Ping Guo},
keywords = {Interval two-stage programming, Production frontier estimation, Shadow price, Intuitionistic fuzzy sets, Water resources planning},
abstract = {Two-stage programming (TSP) is popular in resources planning management, especially for limited and precious resources. Remarkable study has been done to improve the model performance. However, one of the biggest obstacle is lack of objectivity when it comes to penalty quantification derived from recourse behavior. Besides, much attention has been paid in the resources deficiency penalty but little in resources residual, which may lead to wasting. In order to clarify the physical meaning of mathematical equation for recourse penalty from both resources scarcity and surplus, the production frontier was estimated and the technical efficiency and shadow prices of resources were introduced into TSP to characterize the resources deficiency and residual penalty, respectively. Then, an intuitionist fuzzy interval two-stage stochastic programming (IFITSP) was generated integrating the uncertainty of fuzzy membership and traditional TSP. An integrated solving approach was proposed coupling several previous uncertain programming methods and an improved robust interval TSP method. A case study was conducted in an arid area of northwest China to schedule agricultural cultivation scale based on limited water resources. The inefficiencies were [0.26, 0.49], [0.14, 0.37] and [0, 0.03] for GZ, LZ, and GT. The shadow prices of GZ, LZ, and GT in 2015 were 12.94, 2.61, 2.67 Yuan/m3 respectively, indicating the sever water crisis of GZ. The relatively unbiased and abundant decision could be generated by the developed IFITSP to help decision makers with various preferences make tradeoff between benefits and basic crop production requirement as well as balance resources deficiency and surplus. The results also show that the developed model could unveil the uncertainty influence of model inputs on decision strategies and trigger managers to deeply analyze subjective effect and associated risk. By comparison, the proposed methodology can not only clarify the physical meaning of penalty but deal with more complex uncertainty than previous methods. Therefore, the established model can provide reliable and scientific support for resources planning with recourse.}
}
@article{BILLINGS2019602,
title = {Advancing the profession: An updated future-oriented competency model for professional development in infection prevention and control},
journal = {American Journal of Infection Control},
volume = {47},
number = {6},
pages = {602-614},
year = {2019},
issn = {0196-6553},
doi = {https://doi.org/10.1016/j.ajic.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0196655319302317},
author = {Corrianne Billings and Heather Bernard and Lisa Caffery and Susan A. Dolan and John Donaldson and Ericka Kalp and Angel Mueller},
keywords = {APIC MegaSurvey, certification, career stage, leadership, professional stewardship, professional and practice standards}
}
@article{GHODDUSI2019709,
title = {Machine learning in energy economics and finance: A review},
journal = {Energy Economics},
volume = {81},
pages = {709-727},
year = {2019},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0140988319301513},
author = {Hamed Ghoddusi and Germán G. Creamer and Nima Rafizadeh},
keywords = {Machine learning, Energy markets, Energy finance, Support Vector Machine, Artificial Neural Network, Forecasting, Crude oil, Electricity price},
abstract = {Machine learning (ML) is generating new opportunities for innovative research in energy economics and finance. We critically review the burgeoning literature dedicated to Energy Economics/Finance applications of ML. Our review identifies applications in areas such as predicting energy prices (e.g. crude oil, natural gas, and power), demand forecasting, risk management, trading strategies, data processing, and analyzing macro/energy trends. We critically review the content (methods and findings) of more than 130 articles published between 2005 and 2018. Our analysis suggests that Support Vector Machine (SVM), Artificial Neural Network (ANN), and Genetic Algorithms (GAs) are among the most popular techniques used in energy economics papers. We discuss the achievements and limitations of existing literature. The survey concludes by identifying current gaps and offering some suggestions for future research.}
}
@article{GROSSI2019572,
title = {A sensor-centric survey on the development of smartphone measurement and sensing systems},
journal = {Measurement},
volume = {135},
pages = {572-592},
year = {2019},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2018.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0263224118311576},
author = {Marco Grossi},
keywords = {Smartphone, Measurement, Sensors, Embedded system, Wireless communication, Pervasive computing},
abstract = {Modern mobile phones, featuring high performance microprocessors, rich set of sensors and internet connectivity are largely diffused all over the world and are ideal devices for the development of low-cost sensing systems, in particular for low-income developing countries and rural areas that lack the access to diagnostic laboratories and expensive instrumentation. In the design of a smartphone based sensing system different elements must be taken in consideration such as sensors performance, acquisition rate and privacy preserving strategies when personal data must be shared in the cloud. In this paper a sensor-centric survey on smartphone based sensing systems is presented, covering different fields of application. Two different development approaches will be discussed: 1) the exploitation of the large number of sensors embedded in modern smartphones (high-resolution camera, microphone, accelerometer, gyroscope, magnetometer, GPS); 2) the interfacing with external sensors that communicate with the smartphone by the embedded wireless or wired communication technology.}
}
@incollection{2019173,
title = {Chapter 5 - Innovative Sensor Carriers for Cost-Effective Global Ocean Sampling},
editor = {Eric Delory and Jay Pearlman},
booktitle = {Challenges and Innovations in Ocean In Situ Sensors},
publisher = {Elsevier},
pages = {173-288},
year = {2019},
isbn = {978-0-12-809886-8},
doi = {https://doi.org/10.1016/B978-0-12-809886-8.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128098868000053}
}
@article{CAO201923,
title = {The design of an IoT-GIS platform for performing automated analytical tasks},
journal = {Computers, Environment and Urban Systems},
volume = {74},
pages = {23-40},
year = {2019},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518303284},
author = {Hung Cao and Monica Wachowicz},
keywords = {Internet of things, Automated analytical tasks, Mobility context, Smart transit},
abstract = {Society has a very ambitious vision of building smart interconnected cities through the Internet of Things (IoT). Billions of data streams will be generated by devices using different networking infrastructures of smart cities, enabling the automation of how the data that are being collected can be analysed for. However, significant scientific and technological challenges need to be overcome before IoT-GIS platforms can be widely used. This paper is a first step towards designing an IoT-GIS platform for performing automated analytical tasks that are able to retrieve, integrate and contextualize data streams with the purpose of adding value to the provision of transit services. Three automated tasks are used to describe our platform: (1) data ingestion for retrieving data streams; (2) data cleaning for handling missing and redundant data streams; and (3) data contextualization for representing the mobility context of transit driving behaviour. The Codiac Transit System of the Greater Moncton area, NB, Canada was used for building a mobility context and evaluating the cloud architecture that was used to implement our IoT-GIS platform. From the experimental results, the need for cloud computing for achieving scalability and high performance of our IoT-GIS platform is validated. Suggestions for the operational management of routes to improve service quality are proposed based on the analytical outcomes.}
}
@article{ZHAO201952,
title = {Real-time resource tracking for analyzing value-adding time in construction},
journal = {Automation in Construction},
volume = {104},
pages = {52-65},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518306496},
author = {Jianyu Zhao and Olli Seppänen and Antti Peltokorpi and Behnam Badihi and Hylton Olivieri},
keywords = {Real-time tracking, Production control, Construction, Data analysis, Value-adding time},
abstract = {Improving the effectiveness of production control has attracted the interest of researchers and lean construction practitioners over recent years, through techniques such as Last Planner System (LPS) and Location-based Management System (LBMS). However, in these techniques, data collection and analysis still remain manual. Remotely locating workers on site has been suggested as a potential technology to collect crucial data required for production control. The purpose of this study is to test the applicability of a real-time tracking system for collecting data for production control in different types of construction projects. We applied Bluetooth Low Energy (BLE) technology in real-time tracking of workers in three case projects, including residential, office building, and plumbing renovation. We compared various tracking device placement strategies and analyzed the share of uninterrupted presence of workers in work locations based on the collected data. The findings show that both location-based and time-based information of workers can be obtained in real time from the proposed system, but issues of accuracy and coverage need to be considered when defining the data collection plan for each project. Accuracy and coverage issues can be resolved to a significant degree by applying heuristics in data analysis rather than investing in a more sophisticated tracking technology. The conclusion is that real-time tracking technologies are ready for implementation when certain heuristics and guidelines for installation are followed. It is possible to calculate a real-time presence index on a construction site. These data could be used to evaluate the impact of construction management interventions on waste on-site.}
}
@article{ZHANG2019462,
title = {China's new regulatory regime tailored for the sharing economy: The case of Uber under Chinese local government regulation in comparison to the EU, US, and the UK},
journal = {Computer Law & Security Review},
volume = {35},
number = {4},
pages = {462-475},
year = {2019},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0267364919300846},
author = {Chenguo Zhang},
keywords = {Uber, Sharing economy, China, Local regulation, Fair competition, Personal data protection, Consumer protection},
abstract = {Although Uber's arrival in China has resulted in disruptive competition for incumbent taxi companies, it offers an attractive alternative in China's supply-demand-imbalanced urban passenger transport system. China's regulatory regime for Uber has evolved in three stages: from the regulatory vacuum prior to 2015 to its official legalization in 2015–2016, and the enactment of numerous local regulations in 2016, with specific and more demanding requirements for Uber. This policy is a part of the Chinese approach to the gradual liberalization of the urban passenger transport market. Policymakers should consider ‘fair competition’ as the guiding principle to balance the interests of sharing firms and incumbent service providers, as well as between different sharing firms. The core value of this principle lies in the benefits it provides for consumers and the way it engenders a pro-competitive market environment. The labor protection arrangements for sharing firms’ laborers should be more flexible and diversified. In order to recognize whether an Uber-Driver is an employee or independent contractor, a new standard taking into account a range of factors should be established through collective negotiations between the participants of the sharing economy, and dialogues between members of the judiciary, academics, and the policymakers. Further, consumer protection law and personal data protection provisions should apply when sharing firms misuse their distinctive algorithmic management model to compete unfairly to the detriment of consumers and other users. Ex ante regulatory measures designed to protect the personal data of users should be introduced for deployment in the context of the sharing economy. When enforcing these rules, a balance should be struck ensuring free data flow that is essential to sharing firms’ innovation and competition, and the need to ensure the level of data security required to underpin a well-functioning sharing society.}
}
@incollection{WELKE201924,
title = {4 - How to Interpret and Use Outcome Data},
editor = {Ross M. Ungerleider and Jon N. Meliones and Kristen {Nelson McMillan} and David S. Cooper and Jeffrey P. Jacobs},
booktitle = {Critical Heart Disease in Infants and Children (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Philadelphia},
pages = {24-34.e2},
year = {2019},
isbn = {978-1-4557-0760-7},
doi = {https://doi.org/10.1016/B978-1-4557-0760-7.00004-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781455707607000048},
author = {Karl Welke and Tara Karamlou and Ross M. Ungerleider and Jeffrey P. Jacobs},
keywords = {Outcomes, Databases, STS Congenital Heart Surgery Database, Results, Mortality rates, Morbidity, Public Reporting},
abstract = {Abstract:
The results of the treatment of congenital heart disease are influenced by numerous factors, including the structure of the treatment center, the processes in place to administer treatment, and the unique patient- and disease-specific variables that distinguish each congenital heart lesion as unique. Numerous metrics derived from a variety of data sources are used to define outcomes. This chapter discusses how to evaluate and understand this information.}
}
@article{WOOD2019347,
title = {Lithofacies and stratigraphy prediction methodology exploiting an optimized nearest-neighbour algorithm to mine well-log data},
journal = {Marine and Petroleum Geology},
volume = {110},
pages = {347-367},
year = {2019},
issn = {0264-8172},
doi = {https://doi.org/10.1016/j.marpetgeo.2019.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0264817219303368},
author = {David A. Wood},
keywords = {Lithofacies/stratigraphy prediction from well logs, Data-matching machine learning, Standardized well-log interval representation, Transparent data mining, Prediction error analysis},
abstract = {A novel lithofacies and stratigraphic, supervised machine-learning prediction methodology, coupling a standardized well-log representation with an optimized nearest-neighbour algorithm, is introduced and its broader data mining capabilities described. This approach can generate highly accurate predictions and, because of its transparency and standardized representation, can provide unprecedented insight to the predictions it makes. It also provides alternative predictions that help to rectify at least some of the few prediction errors it generates. A detailed case study applying the methodology to published well-log data (8 metrics), plus lithology and stratigraphic information, for the Triassic reservoir section of the giant Hassi R'Mel gas field (Algeria) demonstrates the insight this methodology can achieve. For its lithofacies index, optimized solutions for a 9-independent-variable model generate only about 30 prediction errors from a 1000 interval configured well-log network (RMSE~0.2; R2 ~0.98). A 6-independent-variable lithofacies model achieves only slightly inferior prediction accuracies. For its stratigraphic index, optimized solutions for a 9-independent-variable model generate only about 40 prediction errors from a 1000 interval configured well-log network (RMSE~0.23; R2 ~0.97). Further data mining of these models provides understanding of causes of the few data prediction errors it generates. The lithofacies prediction errors are more easily rectified than the stratigraphic prediction errors. The level of detail provided for each prediction in its two-stage generation process significantly exceeds that provided by regression-based and less-transparent neural-network-based lithofacies prediction algorithms.}
}
@incollection{KVAMME2019287,
title = {Chapter 9 - Putting it all together: Geophysical data integration},
editor = {Raffaele Persico and Salvatore Piro and Neil Linford},
booktitle = {Innovation in Near-Surface Geophysics},
publisher = {Elsevier},
pages = {287-339},
year = {2019},
isbn = {978-0-12-812429-1},
doi = {https://doi.org/10.1016/B978-0-12-812429-1.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812429100009X},
author = {Kenneth L. Kvamme and Eileen G. Ernenwein and Jeremy G. Menzer},
keywords = {Geophysics, Aerial and satellite remote sensing, Data integration, Fusion methods, Multimethod surveys, Data compositing},
abstract = {The integration of information from multiple geophysical and other prospection surveys of archaeological sites and regions leads to a richer and more complete understanding of subsurface content, structure, and physical relationships. Such fusions of information occur within a single geophysical data set or between two or more geophysical and other prospection sources in one, two, or three dimensions. An absolute requirement is the accurate coregistration of all information to the same coordinate space. Data integrations occur at two levels. At the feature level, discrete objects that denote archaeological features are defined, usually subjectively, through the manual digitization of features interpreted in the data, although there is growing interest in automated feature identification and extraction. At the pixel level, distributional issues of skewness and outliers, high levels of noise that obfuscate targets of interest, and a lack of correlation between largely independent dimensions must be confronted. Nevertheless, successful fusions occur using computer graphic methods, simple arithmetic combinations, and advanced multivariate methods, including principal components analysis and supervised and unsupervised classifications. Four case studies are presented that illustrate some of these approaches and offer advancement into new domains.}
}
@article{LING2019212,
title = {Residential mobility in early childhood and the impact on misclassification in pesticide exposures},
journal = {Environmental Research},
volume = {173},
pages = {212-220},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119301677},
author = {Chenxiao Ling and Julia E. Heck and Myles Cockburn and Zeyan Liew and Erin Marcotte and Beate Ritz},
keywords = {Pesticide exposure, Residential mobility, Early childhood, Exposure misclassification, Childhood cancers},
abstract = {Studies of environmental exposures and childhood cancers that rely on records often only use maternal address at birth or address at cancer diagnosis to assess exposures in early childhood, possibly leading to exposure misclassification and questionable validity due to residential mobility during early childhood. Our objective was to assess patterns and identify factors that may predict residential mobility in early childhood, and examine the impact of mobility on early childhood exposure assessment for agriculturally applied pesticides and childhood cancers in California. We obtained the addresses at diagnosis of all childhood cancer cases born in 1998–2011 and diagnosed at 0–5 years of age (n = 6478) from the California Cancer Registry (CCR), and their birth addresses from linked birth certificates. Controls were randomly selected from California birth records and frequency matched (20:1) to all cases by year of birth. We obtained residential histories from a public-record database LexisNexis for both case (n = 3877 with age at diagnosis 1–5 years) and control (n = 99,262) families. Logistic regression analyses were conducted to assess the socio-demographic factors in relation to residential mobility in early childhood. We employed a Geographic Information System (GIS)-based system to estimate children's first year of life exposures to agriculturally applied pesticides based on birth vs diagnosis address or residential histories based upon Lexis-Nexis Public Records and assessed agreement between exposure measures using Spearman correlations and kappa statistics. Over 20% of case and control children moved in their first year of life, and 55% of children with cancer moved between birth and diagnosis. Older age at diagnosis, younger maternal age, lower maternal education, not having a Hispanic ethnic background, use of public health insurance, and non-metropolitan residence at birth were predictors of higher residential mobility. There was moderate to strong correlation (Spearman correlation = 0.76–0.83) and good agreement (kappa = 0.75–0.81) between the first year of life exposure estimates for agricultural pesticides applied within 2 km of a residence relying on an address at birth or at diagnosis or LexisNexis addresses; this did not differ by outcome status, but agreement decreased with decreasing buffer size, and increasing distance moved or age at diagnosis. These findings suggest that residential addresses collected at one point in time may represent residential history in early childhood to a reasonable extent; nevertheless, they exposure misclassification in the first year of life remains an issue. Also, the highest proportion of women not captured by LexisNexis were Hispanic women born in Mexico and those living in the lowest SES neighborhoods, i.e. possibly those with the higher environmental exposures, as well as younger women and those with less than high school education. Though LexisNexis only captures a sub-population, its data may be useful for augmenting address information and assessing the extent of exposure misclassification when estimating environmental exposures in large record linkage studies. Future research should investigate how to correct for exposure misclassification introduced by residential mobility that is not being captured by records.}
}
@article{BACHMANN2019106,
title = {Multivariate geochemical classification of chromitite layers in the Bushveld Complex, South Africa},
journal = {Applied Geochemistry},
volume = {103},
pages = {106-117},
year = {2019},
issn = {0883-2927},
doi = {https://doi.org/10.1016/j.apgeochem.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0883292719300435},
author = {Kai Bachmann and Peter Menzel and Raimon Tolosana-Delgado and Christopher Schmidt and Moritz Hill and Jens Gutzmer},
keywords = {Linear discriminant analysis, PGE, Thaba Mine, Lower group chromitites, Middle group chromitites, Compositional data analysis},
abstract = {The Bushveld Complex, the largest layered mafic-ultramafic intrusion worldwide, is host of numerous, laterally continuous and chemically similar chromitite layers. Based on their stratigraphic position the layers are subdivided into a lower, middle and upper group (LG, MG and UG). Within these groups the layers are numbered successively – from the base to the top of each group. Attempts of discriminating between single layers based on their composition have failed – mainly due to the significant overlap of compositional fields, e.g. of chromitite mineral assemblages and chromite mineral chemistry between (neighboured) layers. In this contribution a tailored and easy to use multivariate classification scheme for the chromitite layers is proposed, based on a comprehensive classification routine for the LG and MG chromitites. This routine allows a clear attribution with known uncertainty of eight distinct chromitite layers. The study was carried out at the Thaba Mine, a chromite mine located on the western limb of the Bushveld Complex. The classification is based on a large geochemical database (N = 1205) from Thaba Mine. It comprises of a hierarchical discrimination approach relying on linear discriminant analysis and involves five distinct steps. Using default homogeneous prior probabilities, classification results are excellent for the first discrimination steps (LGs vs. MGs, 97 %; LG-6 vs. LG-6A, 94 %) and very good for the following steps (MG-1/2 vs. MG-3/4, 86 %; MG-1 vs. MG-2, 92 %; MG-3 vs. MG-4, 93 %; MG-4 vs. MG-4Z, 97 %; MG-4 vs. MG-4A, 88 %). The classification scheme was tested using the same sample set as a training set with unknown composition. Overall classification results for unknown samples belonging to one of the layers are 81 %. Hence, the classification scheme is at least valid for the Thaba mine. The approach may, however, be extended across the entire Bushveld, provided that an appropriate geochemical data set is available.}
}
@article{SAMMAKNEJAD2019123,
title = {A review of the Expectation Maximization algorithm in data-driven process identification},
journal = {Journal of Process Control},
volume = {73},
pages = {123-136},
year = {2019},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418305614},
author = {Nima Sammaknejad and Yujia Zhao and Biao Huang},
keywords = {Expectation Maximization algorithm, Data-driven process identification, Multiple models, Switching, State space, Time delay, Hidden Markov Models, Latent variable models, Outlier treatment, Missing data},
abstract = {The Expectation Maximization (EM) algorithm has been widely used for parameter estimation in data-driven process identification. EM is an algorithm for maximum likelihood estimation of parameters and ensures convergence of the likelihood function. In presence of missing variables and in ill conditioned problems, EM algorithm greatly assists the design of more robust identification algorithms. Such situations frequently occur in industrial environments. Missing observations due to sensor malfunctions, multiple process operating conditions and unknown time delay information are some of the examples that can resort to the EM algorithm. In this article, a review on applications of the EM algorithm to address such issues is provided. Future applications of EM algorithm as well as some open problems are also provided.}
}
@article{ZHANG20191011,
title = {Estimation of saturated hydraulic conductivity with pedotransfer functions: A review},
journal = {Journal of Hydrology},
volume = {575},
pages = {1011-1030},
year = {2019},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2019.05.058},
url = {https://www.sciencedirect.com/science/article/pii/S0022169419305049},
author = {Yonggen Zhang and Marcel G. Schaap},
keywords = {Pedotransfer function, Saturated hydraulic conductivity, Permeability, Kozeny-Carman, Vadose zone, Soil},
abstract = {Saturated hydraulic conductivity (Ks) is a singular parameter in earth system science. Ks not only governs the rate of flow of water under a hydraulic gradient as specified by the Darcy equation for saturated conditions, but also acts as a scaling factor in many unsaturated flow and transport applications that involve pore-size distribution models. Without knowledge of saturated hydraulic conductivity, it would be difficult to accurately describe the transport of water and dissolved or suspended constituents in soils and sediments, or calculate groundwater transport and recharge, and quantify the exchange between soils and the atmosphere. While the determination of Ks is not especially difficult, it is expensive and (in many cases) infeasible to carry out field or lab experiments for large-scale applications. Pedotransfer functions (PTFs) are a class of largely data-driven empirical models that aim to estimate Ks (and often other hydraulic quantities such as water retention characteristics) from easily available data. In this review, we first briefly discuss the history of the development of the concept of saturated hydraulic conductivity and its relation to the Kozeny-Carman (KC) equation. The KC equation serves as a central point in this review because it determines which soil variables affect saturated flow at the pore-scale, a domain which now can also be visited by computational fluid dynamics models. The KC equation also provides us with a structure in which we can classify the large number of PTFs that have been developed for estimating Ks. Datasets and statistical techniques available for PTF development are discussed, and we also describe common metrics used to assess the accuracy and reliability of PTF estimates. The mutual agreement of two main classes (i.e., an effective porosity KC-based and soil texture-based) of PTFs is analyzed using a number of global maps of predicted Ks. Finally, we discuss challenges and perspectives that might lead to PTFs with improved estimates of Ks. In particular, we suggest establishing and utilizing large and completely independent databases to assess the accuracy and reliability of PTFs for global use, while also drawing in information from pedological and remote sensing sources.}
}
@article{BRAGGE2019141,
title = {Unveiling the intellectual structure and evolution of external resource management research: Insights from a bibliometric study},
journal = {Journal of Business Research},
volume = {97},
pages = {141-159},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.12.050},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318306696},
author = {Johanna Bragge and Katri Kauppi and Tuomas Ahola and Anna Aminoff and Riikka Kaipia and Kari Tanskanen},
keywords = {Bibliometric analysis, Cross-disciplinary research, External resource management, Intellectual structure, Text-mining, Visualization},
abstract = {In the current hyper-competitive economy, it is increasingly important to understand how firms can and should access and leverage external resources, such as customer knowledge or supply-chain partners' capabilities. In this paper, we report the results of bibliometric analyses on external resource management (ERM) research in nine representative journals, and elaborate the underlying patterns and dynamics in this relatively young research area. A total of 1290 articles ranging from year 2000 to 2015 were analyzed with text-mining and visualization methods. We found that the annual number of ERM publications is steadily increasing, and identified and described four distinct research clusters focusing on integration & operational effectiveness, innovation & value creation, inter-organizational relationships, and knowledge transfer & learning. The identification of research clusters and key works and authors in this multidisciplinary research field can assist future research in better positioning their studies and finding the key references across disciplinary silos.}
}
@article{WURM201959,
title = {Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {150},
pages = {59-69},
year = {2019},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271619300383},
author = {Michael Wurm and Thomas Stark and Xiao Xiang Zhu and Matthias Weigand and Hannes Taubenböck},
keywords = {Slums, FCN, Convolutional neural networks, Deep learning, Transfer learning},
abstract = {Unprecedented urbanization in particular in countries of the global south result in informal urban development processes, especially in mega cities. With an estimated 1 billion slum dwellers globally, the United Nations have made the fight against poverty the number one sustainable development goal. To provide better infrastructure and thus a better life to slum dwellers, detailed information on the spatial location and size of slums is of crucial importance. In the past, remote sensing has proven to be an extremely valuable and effective tool for mapping slums. The nature of used mapping approaches by machine learning, however, made it necessary to invest a lot of effort in training the models. Recent advances in deep learning allow for transferring trained fully convolutional networks (FCN) from one data set to another. Thus, in our study we aim at analyzing transfer learning capabilities of FCNs to slum mapping in various satellite images. A model trained on very high resolution optical satellite imagery from QuickBird is transferred to Sentinel-2 and TerraSAR-X data. While free-of-charge Sentinel-2 data is widely available, its comparably lower resolution makes slum mapping a challenging task. TerraSAR-X data on the other hand, has a higher resolution and is considered a powerful data source for intra-urban structure analysis. Due to the different image characteristics of SAR compared to optical data, however, transferring the model could not improve the performance of semantic segmentation but we observe very high accuracies for mapped slums in the optical data: QuickBird image obtains 86–88% (positive prediction value and sensitivity) and a significant increase for Sentinel-2 applying transfer learning can be observed (from 38 to 55% and from 79 to 85% for PPV and sensitivity, respectively). Using transfer learning proofs extremely valuable in retrieving information on small-scaled urban structures such as slum patches even in satellite images of decametric resolution.}
}
@incollection{2019263,
title = {Index},
editor = {Zaheer-Ud-Din Babar},
booktitle = {Encyclopedia of Pharmacy Practice and Clinical Pharmacy},
publisher = {Elsevier},
address = {Oxford},
pages = {263-309},
year = {2019},
isbn = {978-0-12-812736-0},
doi = {https://doi.org/10.1016/B978-0-12-812735-3.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128127353000212}
}
@incollection{HENRICKSON201973,
title = {Chapter 5 - Data Preparation},
editor = {Constantinos Antoniou and Loukas Dimitriou and Francisco Pereira},
booktitle = {Mobility Patterns, Big Data and Transport Analytics},
publisher = {Elsevier},
pages = {73-106},
year = {2019},
isbn = {978-0-12-812970-8},
doi = {https://doi.org/10.1016/B978-0-12-812970-8.00005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128129708000051},
author = {Kristian Henrickson and Filipe Rodrigues and Francisco Camara Pereira},
keywords = {Scripting, Statistical analysis, Web data, Protocols, Context data, Data collection, Probe vehicle data},
abstract = {This chapter is designed as an introduction to data preparation, with a focus on nonconventional data sources such as probe vehicles, internet, and web services. These nonconventional data promise to support new analysis methods which can provide a better understanding of our transport systems, and they are rapidly making their way into mainstream transport engineering, analysis, and decision making. However, such data come with a number of new challenges associated with their size and/or complexity, new acquisition channels, and added quality control considerations.}
}
@article{AYRE2019100302,
title = {Supporting and practising digital innovation with advisers in smart farming},
journal = {NJAS - Wageningen Journal of Life Sciences},
volume = {90-91},
pages = {100302},
year = {2019},
issn = {1573-5214},
doi = {https://doi.org/10.1016/j.njas.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1573521418302355},
author = {Margaret Ayre and Vivienne {Mc Collum} and Warwick Waters and Peter Samson and Anthony Curro and Ruth Nettle and Jana-Axinja Paschen and Barbara King and Nicole Reichelt},
keywords = {Digital innovation, Smart farming, Farm advisers, Digiware},
abstract = {The promise of technology development in agriculture is well publicised with some claiming that digital disruption will transform the way farming and food production is done in the future. For farm advisers, engaging in smart farming involves managing the proliferation of new forms of information, new knowledge and networks and new technical devices that produce digitised representations of farm performance. The nature and effects of digital practices in particular poses challenges for farm advisers as they seek to understand how digital tools and services can be integrated into their service delivery for improved farm decision making. In this paper we present insights from a co-design process with private farm advisers and ask: What enables farm advisers to engage with digital innovation? And, how can digital innovation be supported and practiced in smart farming contexts? Digital innovation presents challenges for farmers and advisers due to the new relationships, skills, arrangements, techniques and devices required to realise value for farm production and profitability from digital tools and services. We show how a co-design process supported farm advisers to adapt their routine advisory practices through recognising and engaging with the social, material and symbolic practices of digiware in smart farming. We demonstrate the need to recognise ‘digiware as constituted in and by heterogeneous practices from which possibilities for digital innovation emerge. These possibilities include the increased capacity of farm advisers to identify the value proposition of smart farming tools and services for theirs and their clients’ businesses, and the adaptation of advisory services in ways that harnass and mobilise diverse skills, knowledge/s, materials and representations for translating digital data, digital infrastructure and digital capacities into better decisions for farm management.}
}
@article{RUAN2019101268,
title = {Soft computing model based financial aware spatiotemporal social network analysis and visualization for smart cities},
journal = {Computers, Environment and Urban Systems},
volume = {77},
pages = {101268},
year = {2019},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517304829},
author = {Lei Ruan and Chunyan Li and Yan Zhang and Haoxiang Wang},
keywords = {Soft computing model, Financial, Spatiotemporal, Social network analysis, Data visualization, Smart cities, Computing paradigms},
abstract = {The era of intelligence is the development of human science and technology at a higher level, bringing a new layout for the financial market. Then, how to realize the good layout of the financial market in the era of intelligence is an important problem facing all the countries in the world. We in the financial industry as the origin and change as the clue, analyzes the physical outlets as the representative of the financial institutions, banking as standard electronic banking and mobile phone to the bank as the representative of the mobile financial development of three major formats. On this basis, we analyze the current mobile phone terminal model of mobile banking three shortcomings, and propose the new mobile financial formats. This new format is tentatively known as “smart financial format”, it has the equipment personality, wearable, low carbon environmental protection, offline interaction, security, privacy and intelligence, efficiency, and other five major characteristics. In the future, the financial format is likely to be achieved by mobile financial formats to the upgrading of intelligent financial formats, to meet the needs of customers wider and deeper levels. The artificial intelligence method has its advantages in dealing with the problems of the economic system. Therefore, the introduction of artificial intelligence methods into the economic control will become a trend. The proposed model is validated through the public databases to verify the effectiveness.}
}
@article{NARAYAN201970,
title = {New Concepts in Sudden Cardiac Arrest to Address an Intractable Epidemic: JACC State-of-the-Art Review},
journal = {Journal of the American College of Cardiology},
volume = {73},
number = {1},
pages = {70-88},
year = {2019},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2018.09.083},
url = {https://www.sciencedirect.com/science/article/pii/S0735109718389836},
author = {Sanjiv M. Narayan and Paul J. Wang and James P. Daubert},
keywords = {acute coronary syndrome, cardiopulmonary resuscitation, ECG, heart failure, informatics, machine learning, sudden cardiac arrest},
abstract = {Sudden cardiac arrest (SCA) is one of the largest causes of mortality globally, with an out-of-hospital survival below 10% despite intense research. This document outlines challenges in addressing the epidemic of SCA, along the framework of respond, understand and predict, and prevent. Response could be improved by technology-assisted orchestration of community responder systems, access to automated external defibrillators, and innovations to match resuscitation resources to victims in place and time. Efforts to understand and predict SCA may be enhanced by refining taxonomy along phenotypical and pathophysiological “axes of risk,” extending beyond cardiovascular pathology to identify less heterogeneous cohorts, facilitated by open-data platforms and analytics including machine learning to integrate discoveries across disciplines. Prevention of SCA must integrate these concepts, recognizing that all members of society are stakeholders. Ultimately, solutions to the public health challenge of SCA will require greater awareness, societal debate and focused public policy.}
}
@article{FAN2019186,
title = {Empirical and machine learning models for predicting daily global solar radiation from sunshine duration: A review and case study in China},
journal = {Renewable and Sustainable Energy Reviews},
volume = {100},
pages = {186-212},
year = {2019},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2018.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S1364032118307226},
author = {Junliang Fan and Lifeng Wu and Fucang Zhang and Huanjie Cai and Wenzhi Zeng and Xiukang Wang and Haiyang Zou},
keywords = {Solar radiation, Machine learning, Model comparison, Global performance index (GPI), Computational time, Memory usage},
abstract = {Accurate estimation of global solar radiation (Rs) is essential to the design and assessment of solar energy utilization systems. Existing empirical and machine learning models for estimating Rs from sunshine duration were comprehensively reviewed. The performances of 12 empirical model forms and 12 machine learning algorithms for estimating daily Rs were further evaluated in different climatic zones of China as a case study, i.e. the temperate continental zone (TCZ), temperate monsoon zone (TMZ), mountain plateau zone (MPZ) and (sub)tropical monsoon zone (SMZ). The best-performing model at each station and the overall best model for each climatic zone were selected based on six statistical indictors, a global performance index (GPI) and computational costs (computational time and memory usage). The results revealed that the machine learning models (RMSE: 2.055–2.751 MJ m−2 d−1; NRMSE: 12.8–21.3%; R2: 0.839–0.936) generally outperformed the empirical models (RMSE: 2.118–3.540 MJ m−2 d−1; NRMSE: 12.1–27.5%; R2: 0.834–0.935) in terms of prediction accuracy. The cubic model (M3), modified linear-logarithmic model (M5) and power model (M10) attained generally better ranks among empirical models based on GPI. M3 was the top-ranked model in TMZ and MPZ, while general best performance was obtained by M5 and M2 in SMZ and TCZ, respectively. ANFIS, ELM, LSSVM and MARS obtained generally better performance among machine learning models, with the overall best ranking by ANFIS in TCZ and SMZ and by ELM in MPZ and SMZ. XGBoost (8.1 s and 74.2 MB), M5Tree (11.3 s and 29.7 MB), GRNN (12.3 s and 295.3 MB), MARS (14.4 s and 42.6 MB), MLP (22.4 s and 41.3 MB) and ANFIS (29.8 s and 23.1 MB) showed relatively small computational time and memory usage. Comprehensively considering both the prediction accuracy and computational costs, ANFIS is highly recommended, while MARS and XGBoost are also promising models for daily Rs estimation.}
}