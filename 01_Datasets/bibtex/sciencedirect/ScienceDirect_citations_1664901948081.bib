@article{FLECHSIG2022100718,
title = {Robotic Process Automation in purchasing and supply management: A multiple case study on potentials, barriers, and implementation},
journal = {Journal of Purchasing and Supply Management},
volume = {28},
number = {1},
pages = {100718},
year = {2022},
issn = {1478-4092},
doi = {https://doi.org/10.1016/j.pursup.2021.100718},
url = {https://www.sciencedirect.com/science/article/pii/S1478409221000522},
author = {Christian Flechsig and Franziska Anslinger and Rainer Lasch},
keywords = {Robotic process automation, Digital procurement, Implementation, Barriers, Digital readiness, Public sector},
abstract = {Robotic Process Automation (RPA) has received growing attention within the digital transformation as this cutting-edge technology automates human behavior and promises high potentials. However, the adoption in purchasing and supply management (PSM) is still in its infancy and has hardly been explored, particularly in the public sector. Based on a multiple case study including 19 organizations of the public and private sector, this paper narrows that gap and presents comprehensive insights into potentials, barriers, suitable processes, and best practices and components for RPA implementation. The findings indicate that adoption depends on the organizations’ digital procurement readiness and maturity. Application areas of RPA enlarge with increasing experience and range from transactional and operative tasks within the procure-to-pay process to more strategic use cases in sourcing and supply relationship management. Potentials mainly comprise employee reliefs, cost savings, and increased operational efficiency and quality. We uncover multiple technical, organizational, and environmental barriers related to IT infrastructure and human resources, internal communication, financial resources, top management support, organizational structures, supplier-related issues, and government regulations. Furthermore, our study indicates several differences between the private and public sectors for RPA implementation. We outline implications for the emerging research on RPA and pivotal directions for organizational practice.}
}
@article{VANDINTER2022107008,
title = {Predictive maintenance using digital twins: A systematic literature review},
journal = {Information and Software Technology},
volume = {151},
pages = {107008},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001331},
author = {Raymon {van Dinter} and Bedir Tekinerdogan and Cagatay Catal},
keywords = {Systematic literature review, Active learning, Digital twin, Predictive maintenance},
abstract = {Context
Predictive maintenance is a technique for creating a more sustainable, safe, and profitable industry. One of the key challenges for creating predictive maintenance systems is the lack of failure data, as the machine is frequently repaired before failure. Digital Twins provide a real-time representation of the physical machine and generate data, such as asset degradation, which the predictive maintenance algorithm can use. Since 2018, scientific literature on the utilization of Digital Twins for predictive maintenance has accelerated, indicating the need for a thorough review.
Objective
This research aims to gather and synthesize the studies that focus on predictive maintenance using Digital Twins to pave the way for further research.
Method
A systematic literature review (SLR) using an active learning tool is conducted on published primary studies on predictive maintenance using Digital Twins, in which 42 primary studies have been analyzed.
Results
This SLR identifies several aspects of predictive maintenance using Digital Twins, including the objectives, application domains, Digital Twin platforms, Digital Twin representation types, approaches, abstraction levels, design patterns, communication protocols, twinning parameters, and challenges and solution directions. These results contribute to a Software Engineering approach for developing predictive maintenance using Digital Twins in academics and the industry.
Conclusion
This study is the first SLR in predictive maintenance using Digital Twins. We answer key questions for designing a successful predictive maintenance model leveraging Digital Twins. We found that to this day, computational burden, data variety, and complexity of models, assets, or components are the key challenges in designing these models.}
}
@article{LIN2022154640,
title = {Does environmental decentralization aggravate pollution emissions? Microscopic evidence from Chinese industrial enterprises},
journal = {Science of The Total Environment},
volume = {829},
pages = {154640},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.154640},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722017338},
author = {Boqiang Lin and Chongchong Xu},
keywords = {Environmental decentralization, Pollution emissions, Micro mechanism, Race to the bottom},
abstract = {Rational division of environmental management power among governments is a necessary institutional support for speeding up the realization of green development goals. Based on the combined microdata of China Industrial Enterprise Database and China Enterprise Pollution Database from 2000 to 2012, the effect of environmental decentralization on enterprise pollution emission is empirically examined in this research. Results show that Chinese-style environmental decentralization, especially environmental supervision decentralization and environmental monitoring decentralization, significantly aggravates the pollution emissions of enterprises. Moreover, the impact of environmental decentralization on enterprise pollution emissions has regional and enterprise ownership heterogeneity. The mechanism test results denote that the production scale effect, energy structure effect and pollution control effect are the micro mechanisms of environmental decentralization aggravating the pollution emission of enterprises. This research confirms the existence of “race to the bottom” among local governments in China and provides evidence support and beneficial enlightenment for the vertical reform of the environmental management system.}
}
@article{SERRANO2022100100,
title = {Verification and Validation for data marketplaces via a blockchain and smart contracts},
journal = {Blockchain: Research and Applications},
pages = {100100},
year = {2022},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2022.100100},
url = {https://www.sciencedirect.com/science/article/pii/S2096720922000410},
author = {Will Serrano},
keywords = {Data marketplace, Project information model, Smart cities, Smart buildings, Real estate, Distributed ledger technology, Blockchain, Smart contracts, Artificial intelligence},
abstract = {Actual challenges with data in physical infrastructure include 1) the adversity of its velocity based on access and retrieval, thus integration; 2) its value as its intrinsic quality; 3) its extensive volume with a limited variety in terms of systems and finally, 4) its veracity, as data can be modified to obtain an economical advantage. Physical infrastructure design based on agile project management and minimum viable products provides benefits against the traditional waterfall method. Agile supports an early return of investment that promotes circular re-investing while making the product more adaptable to variable social-economical environments. However, Agile also presents inherent issues due to its iterative approach. Furthermore, project information requires an efficient record of the aims, requirements, and governance not only for the investors, owners, or users, but also to keep evidence in future health & safety and other statutory compliance. In order to address these issues, this article presents a Validation and Verification (V&V) model for Data Marketplaces with a hierarchical process; each data V&V stage provides a layer of data abstraction, value-added services and authenticity based on Artificial Intelligence (AI). In addition, this proposed solution applies a Distributed Ledger Technology (DTL) for a decentralised approach where each user keeps and maintains the data within a ledger. The presented model is validated in real Data Marketplace applications: 1) live data for Newcastle urban observatory smart city project where data is collected from sensors embedded within the Smart city via APIs. 2) static data for University College London (UCL) – Real Estate – PEARL Project where different project users and stakeholders introduce data into a (Project Information Model) PIM.}
}
@article{XU2022114241,
title = {China Sponge City database development and urban runoff source control facility configuration comparison between China and the US},
journal = {Journal of Environmental Management},
volume = {304},
pages = {114241},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.114241},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721023033},
author = {Changqing Xu and Xinmei Shi and Mingyi Jia and Yu Han and Rongrong Zhang and Shakeel Ahmad and Haifeng Jia},
keywords = {Sponge City, Urban runoff source control facility, Database, Demand analysis, Comparison dimension},
abstract = {Urban runoff source control facilities (URSCFs) are important parts of Sponge City (SC) by controlling urban flooding, restoring eco-balance, and enhancing city resilience. To evaluate the performance of URSCF, one needs to summarize and analyze the past SC construction and operation data. Previous studies however are predominately engineering practice studies. There lacks localized reference datasets to quantitatively evaluate the performance and guide public policy development for SC. Therefore, it is imperative to develop a database, which would summarize data obtained through the already completed pilot sponge cities, and provide a reference for future URSCFs planning and construction. This study makes a zero to one breakthrough by establishing a SC database using New Orleans method. Then statistical results of facility type, size, and costs information for 30 pilot sponge cities have been summarized and analyzed. The URSCFs type distribution statistical results show that bioretention, permeable pavement, detention cell, grassed swale and constructed wetland are the top five most constructed facilities in China. The cost statistical results display that the range of facility cost collected is usually larger than the range given by the reference value, which may attribute to the variation in material cost, labor cost and design parameters in different cities. To check the similarities and differences of URSCFs parameters between China and the US. A configuration parameters comparison of URSCFs has been conducted. Bioretention is taken as an exampl. Comparison results show that factors such as climate type, geographical environment, and socio-economic conditions will affect the configuration parameters of URSCFs. The groundwater depth and designed rainfall intensity are mainly influenced by local climate and geographical conditions. Surface area is influenced by local socio-economic conditions. The thickness of the covering layer and drainage layer are not affected by geographic location. The service area ratio, water storage depth and planting soil layer thickness are significantly different between China and the US.}
}
@article{AHMAD2022334,
title = {Energetics Systems and artificial intelligence: Applications of industry 4.0},
journal = {Energy Reports},
volume = {8},
pages = {334-361},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.11.256},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721014037},
author = {Tanveer Ahmad and Hongyu Zhu and Dongdong Zhang and Rasikh Tariq and A. Bassam and Fasee Ullah and Ahmed S AlGhamdi and Sultan S. Alshamrani},
keywords = {Applications of industry 4.0, Power sector, Artificial intelligence, Energetics systems, Energy storage, Sustainability},
abstract = {Industrial development with the growth, strengthening, stability, technical advancement, reliability, selection, and dynamic response of the power system is essential. Governments and companies invest billions of dollars in technologies to convert, harvest, rising demand, changing demand and supply patterns, efficiency, lack of analytics required for optimal energy planning, and store energy. In this scenario, artificial intelligence (AI) is starting to play a major role in the energy market. Recognizing the importance of AI, this study was conducted on seven different energetics systems and their variety of applications, including: i) electricity production; ii) power delivery; iii) electric distribution networks; iv) energy storage; v) energy saving, new energy materials, and devices; vi) energy efficiency and nanotechnology; and vii) energy policy, and economics. The main drivers are the four key techniques used in current AI technologies, including: i) fuzzy logic systems; ii) artificial neural networks; iii) genetic algorithms; and iv) expert systems. In developed countries, the power industry has started using AI to connect with smart meters, smart grids, and the Internet of Things devices. These AI technologies will lead to the improvement of efficiency, energy management, transparency, and the usage of renewable energies. In recent decades/years, new AI technology has brought significant improvements to how power system devices monitor data, communicate with the system, analyze input–output, and display data in unprecedented ways. New applications in the energy system become feasible when these new AI developments are incorporated into the energy industry. But on the contrary, much more investment is needed in global research into AI and data-driven models. In terms of power supply, AI can help utilities provide customers with renewable and affordable electricity from complex sources in a secure manner, while at the same time providing these customers with the opportunity to use their own energy more efficiently. Moreover, policy recommendations, research opportunities, and how industry 4.0 will improve sustainability have been briefly described.}
}
@article{GE2022109054,
title = {Contrasting trends between peak photosynthesis timing and peak greenness timing across seven typical biomes in Northern Hemisphere mid-latitudes},
journal = {Agricultural and Forest Meteorology},
volume = {323},
pages = {109054},
year = {2022},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2022.109054},
url = {https://www.sciencedirect.com/science/article/pii/S016819232200243X},
author = {Zhongxi Ge and Jing Huang and Xufeng Wang and Xuguang Tang and Lei Fan and Yinjun Zhao and Mingguo Ma},
keywords = {Phenology, Climate change, FLUXNET2015, Potential PPT, Actual PPT},
abstract = {The peak photosynthesis timing (PPT) is a key factor that affects the seasonality of the terrestrial carbon uptake. Carbon phenology derived from gross primary production (GPP) has been used to validate the peak greenness timing (PGT) from satellite-based vegetation indices (VIs) in phenology research. However, PPT, derived from GPP, has not been comprehensively analyzed, especially taking different GPP estimates, fitting methods, and biomes into account. Moreover, whether or not the PPT trend is consistent with the reported PGT trend still unclear. We explored the above questions at widely used flux sites in Northern Hemisphere mid-latitudes and found that no significant differences in PPT derived from GPP using different carbon flux partitioning methods. Moreover, fitting methods performed well in grassland, cropland, wetland, and wood savannas compared with evergreen needleleaf forest, deciduous broadleaf forest, and mixed forest. Unexpectedly, we did not find an advancing trend in PPT derived from GPP compared with PGT from SPOT-VGT normalized difference vegetation index (NDVI). Our study suggests that the principle of the fitting method and physiological property of the biome should be taken into account when predicting PPT. More importantly, PGT is not a good proxy of the PPT. Therefore, PPT trends based on VIs should be viewed with caution. In general, this study is meaningful for better understanding photosynthesis and carbon cycling in the context of changing climate.}
}
@article{PANOVSKAGRIFFITHS2022126050,
title = {Modelling the impact of reopening schools in the UK in early 2021 in the presence of the alpha variant and with roll-out of vaccination against SARS-CoV-2},
journal = {Journal of Mathematical Analysis and Applications},
volume = {514},
number = {2},
pages = {126050},
year = {2022},
issn = {0022-247X},
doi = {https://doi.org/10.1016/j.jmaa.2022.126050},
url = {https://www.sciencedirect.com/science/article/pii/S0022247X22000646},
author = {J. Panovska-Griffiths and R.M. Stuart and C.C. Kerr and K. Rosenfield and D. Mistry and W. Waites and D.J. Klein and C. Bonell and R.M. Viner},
keywords = {COVID-19, National lockdown, Reopening schools and society, Mathematical modelling},
abstract = {Following the resurgence of the COVID-19 epidemic in the UK in late 2020 and the emergence of the alpha (also known as B117) variant of the SARS-CoV-2 virus, a third national lockdown was imposed from January 4, 2021. Following the decline of COVID-19 cases over the remainder of January 2021, the question of when and how to reopen schools became an increasingly pressing one in early 2021. This study models the impact of a partial national lockdown with social distancing measures enacted in communities and workplaces under different strategies of reopening schools from March 8, 2021 and compares it to the impact of continual full national lockdown remaining until April 19, 2021. We used our previously published agent-based model, Covasim, to model the emergence of the alpha variant over September 1, 2020 to January 31, 2021 in presence of Test, Trace and Isolate (TTI) strategies. We extended the model to incorporate the impacts of the roll-out of a two-dose vaccine against COVID-19, with 200,000 daily vaccine doses prioritised by age starting with people 75 years or older, assuming vaccination offers a 95% reduction in disease acquisition risk and a 30% reduction in transmission risk. We used the model, calibrated until January 25, 2021, to simulate the impact of a full national lockdown (FNL) with schools closed until April 19, 2021 versus four different partial national lockdown (PNL) scenarios with different elements of schooling open: 1) staggered PNL with primary schools and exam-entry years (years 11 and 13) returning on March 8, 2021 and the rest of the schools years on March 15, 2020; 2) full-return PNL with both primary and secondary schools returning on March 8, 2021; 3) primary-only PNL with primary schools and exam critical years (years 11 and 13) going back only on March 8, 2021 with the rest of the secondary schools back on April 19, 2021 and 4) part-rota PNL with both primary and secondary schools returning on March 8, 2021 with primary schools remaining open continuously but secondary schools on a two-weekly rota-system with years alternating between a fortnight of face-to-face and remote learning until April 19, 2021. Across all scenarios, we projected the number of new daily cases, cumulative deaths and effective reproduction number R until April 30, 2021. Our calibration across different scenarios is consistent with alpha variant being around 60% more transmissible than the wild type. We find that strict social distancing measures, i.e. national lockdowns, were essential in containing the spread of the virus and controlling hospitalisations and deaths during January and February 2021. We estimated that a national lockdown over January and February 2021 would reduce the number of cases by early March to levels similar to those seen in October 2020, with R also falling and remaining below 1 over this period. We estimated that infections would start to increase when schools reopened, but found that if other parts of society remain closed, this resurgence would not be sufficient to bring R above 1. Reopening primary schools and exam critical years only or having primary schools open continuously with secondary schools on rotas was estimated to lead to lower increases in cases and R than if all schools opened. Without an increase in vaccination above the levels seen in January and February, we estimate that R could have increased above 1 following the reopening of society, simulated here from April 19, 2021. Our findings suggest that stringent measures were integral in mitigating the increase in cases and bringing R below 1 over January and February 2021. We found that it was plausible that a PNL with schools partially open from March 8, 2021 and the rest of the society remaining closed until April 19, 2021 would keep R below 1, with some increase evident in infections compared to continual FNL until April 19, 2021. Reopening society in mid-April, without an increase in vaccination levels, could push R above 1 and induce a surge in infections, but the effect of vaccination may be able to control this in future depending on the transmission blocking properties of the vaccines.}
}
@article{DAVIDSON2022126,
title = {The crossroads of digital phenotyping},
journal = {General Hospital Psychiatry},
volume = {74},
pages = {126-132},
year = {2022},
issn = {0163-8343},
doi = {https://doi.org/10.1016/j.genhosppsych.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0163834320301614},
author = {Brittany I. Davidson},
abstract = {The term ‘Digital Phenotyping’ has started to appear with increasing regularity in medical research, especially within psychiatry. This aims to bring together digital traces (e.g., from smartphones), medical data (e.g., electronic health records), and lived experiences (e.g., daily activity, location, social contact), to better monitor, intervene, and diagnose various psychiatric conditions. However, is this notion any different from digital traces or the quantified self? While digital phenotyping has the potential to transform and revolutionize medicine as we know it; there are a number of challenges that must be addressed if research is to blossom. At present, these issues include; (1) methodological issues, for example, the lack of clear theoretical links between digital markers (e.g., battery life, interactions with smartphones) and condition relapses, (2) the current tools being employed, where they typically have a number of security or privacy issues, and are invasive by nature, (3) analytical methods and approaches, where I question whether research should start in larger-scale epidemiological scale or in smaller (and potentially highly vulnerable) patient populations as is the current norm, (4) the current lack of security and privacy regulation adherence of apps used, and finally, (5) how do such technologies become integrated into various healthcare systems? This aims to provide deep insight into how the Digital Phenotyping could provide huge promise if we critically reflect now and gather clinical insights with a number of other disciplines such as epidemiology, computer- and the social sciences to move forward.}
}
@article{YOHANANDHAN2022107718,
title = {A holistic review on Cyber-Physical Power System (CPPS) testbeds for secure and sustainable electric power grid – Part – I: Background on CPPS and necessity of CPPS testbeds},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {136},
pages = {107718},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107718},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521009443},
author = {Rajaa Vikhram Yohanandhan and Rajvikram Madurai Elavarasan and Rishi Pugazhendhi and Manoharan Premkumar and Lucian Mihet-Popa and Vladimir Terzija},
keywords = {Critical infrastructure protection, Cyber attack, Cyber-physical power system testbed, Cyber security, Real-time testbed, Smart grid, Sustainability},
abstract = {The integration of advanced Information and Communication Technologies (ICTs) in the conventional electric power grid is evolving into a Cyber-Physical Power System (CPPS). The seamless integration of control, communication, and computing operations allow CPPS to be fully monitored and controlled. Threats, vulnerabilities, and catastrophic attacks will intrude as CPPS monitoring, protection, and control functions advance. For the CPPS to operate safely, securely, and efficiently, sustainable cybersecurity solutions must be developed, and the power grid's reliability and resilience must be maintained even when exposed to unfavorable network conditions. While more distributed and renewable energy sources are connected to the grid, cybersecurity helps to ensure the supply of electricity is sustainable and of high quality. The validation of such sustainable cybersecurity analysis goes beyond the traditional power grid network analysis, which means that the test should integrate the physical and cyber system behavior and respond to the network attacks. To analyze the cybersecurity and cyberattacks in the practical CPPS, comprehensive and realistic CPPS testbeds are needed. The heterogeneous nature of the CPPS concept urges multidisciplinary testbeds with various functions and capabilities to evaluate the new cyber-attacks, vulnerabilities, and threats in CPPS. Using this CPPS testbed framework, the different types of cyber-attack can be detected, and detection algorithm can be evaluated, and helps to enhance the development of sustainable cybersecurity defenses for realistic CPPS environment with the increasingly dense integration of distributed energy resources (DERs). This Part-I paper review the CPPS testbeds in the view of the physical power system layer, sensing layer, communication layer, control layer, application layer, test platforms, and research goals with the fusion of cyber and physical systems. In addition, this review presented an overview, structure, and application-based evaluation of existing testbeds from the industry and institutions. The various research areas in CPPS are reviewed first to show the research trends on different aspects of CPPS which have gained significant attention in the past decade. The necessity of testbeds for cyberattacks and sustainable cybersecurity analysis in CPPS are then described. Finally, the NIST framework for CPS, CPPS domains, and research areas are presented. Further the Part-II paper will review the classification, overview, and assessment of CPPS testbeds.}
}
@article{ALANNE2022103445,
title = {An overview of machine learning applications for smart buildings},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103445},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103445},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721007186},
author = {Kari Alanne and Seppo Sierla},
keywords = {Smart building, Intelligent building, Learning, HVAC, Reinforcement learning, Energy efficiency},
abstract = {The efficiency, flexibility, and resilience of building-integrated energy systems are challenged by unpredicted changes in operational environments due to climate change and its consequences. On the other hand, the rapid evolution of artificial intelligence (AI) and machine learning (ML) has equipped buildings with an ability to learn. A lot of research has been dedicated to specific machine learning applications for specific phases of a building's life-cycle. The reviews commonly take a specific, technological perspective without a vision for the integration of smart technologies at the level of the whole system. Especially, there is a lack of discussion on the roles of autonomous AI agents and training environments for boosting the learning process in complex and abruptly changing operational environments. This review article discusses the learning ability of buildings with a system-level perspective and presents an overview of autonomous machine learning applications that make independent decisions for building energy management. We conclude that the buildings’ adaptability to unpredicted changes can be enhanced at the system level through AI-initiated learning processes and by using digital twins as training environments. The greatest potential for energy efficiency improvement is achieved by integrating adaptability solutions at the timescales of HVAC control and electricity market participation.}
}
@article{ZHAO2022101786,
title = {Investment incentives and the relative demand for skilled labor: Evidence from accelerated depreciation policies in China},
journal = {China Economic Review},
volume = {73},
pages = {101786},
year = {2022},
issn = {1043-951X},
doi = {https://doi.org/10.1016/j.chieco.2022.101786},
url = {https://www.sciencedirect.com/science/article/pii/S1043951X2200044X},
author = {Lexin Zhao and Hongsheng Fang},
keywords = {Tax incentives, Accelerated depreciation, Relative demand for skilled labor, Financing constraints, Tax compliance},
abstract = {This study evaluates the effects of China's 2014 and 2015 accelerated depreciation policies on the relative demand of firms for skilled labor. We develop a simple model to explore how the policies affect the relative demand of firms for skilled labor and illustrate the roles of financing constraints and tax compliance in mediating the policy effects. We then employ a firm-level dataset from China's A-share listed companies and use a quasi-experimental design to examine the model predictions. We find that the policies significantly increase the relative demand of firms for skilled labor. The channels underlying the policy effects are that the policies generate additional cash flow for firms, stimulate investment and, thus, raise the demand of firms for skilled labor with the presence of capital–skill complementarity. We also find that the positive effects of the policies on the relative demand for skilled labor are primarily significant for firms with strong financing constraints and high tax compliance. Moreover, we document the positive effects of the policies on R&D investment, firm value added, productivity, workers' benefits, and corporate social responsibility performance, which further corroborate our main results.}
}
@article{THONGTHAMMACHART2022105447,
title = {Incorporating Light Gradient Boosting Machine to land use regression model for estimating NO2 and PM2.5 levels in Kansai region, Japan},
journal = {Environmental Modelling & Software},
volume = {155},
pages = {105447},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105447},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001530},
author = {Tin Thongthammachart and Shin Araki and Hikari Shimadera and Tomohito Matsuo and Akira Kondo},
keywords = {Light gradient boosting machine, Extreme gradient boosting, Random forests, Community multiscale air quality model, Land use regression, Air quality forecasting model},
abstract = {This study incorporates Light Gradient Boosting Machine (LightGBM) to a land use regression (LUR) model for estimating NO2 and PM2.5 levels. The predictions were compared with LUR-based machine learnings models of Extreme Gradient Boosting (XGBoost) and Random Forests (RF). Weather Research and Forecasting (WRF) model-simulated meteorological parameters, Community Multiscale Air Quality modeling system (CMAQ)-simulated NO2/PM2.5 concentrations, land use variables, and population data were used as predictor variables. The model performances were evaluated through spatial and temporal cross-validations (CV). The CV results indicated that the LightGBM model was moderately superior in NO2 and PM2.5 predictions compared to the RF and XGBoost models. Moreover, the LightGBM model had high performance in NO2 and PM2.5 predictions at high concentrations, which is essential for risk assessment. Our findings demonstrate that LightGBM can greatly improve the accuracy of NO2 and PM2.5 estimates.}
}
@article{BENSLAMA2022101504,
title = {Prosumer in smart grids based on intelligent edge computing: A review on Artificial Intelligence Scheduling Techniques},
journal = {Ain Shams Engineering Journal},
volume = {13},
number = {1},
pages = {101504},
year = {2022},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2021.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S2090447921002409},
author = {Sami {Ben Slama}},
keywords = {Smart grid, Edge computing, Prosumer, Internet of things, Artificial Intelligence, Machine learning},
abstract = {Smart Grid technology has been considered an attractive research issue due to its efficiency in solving energy demand, storage, and power transmission. The integration of IoT technology in the Smart Grids is a critical way to accelerate the digitization of the power grid and is useful for the efficient performance of the energy grid infrastructure. For efficient real-time data analysis and decision-making, the Internet of Things will incorporate various communication systems seamlessly. To achieve efficient communication between all Internet of Things, devices are expected to use multiple means, including smart sensors, cable and wireless communication. Improved Internet of Things sensor technologies and connectivity could theoretically prevent or minimize the potential to natural disaster transmission lines, improve transmission power capacity and reduce economic losses. A smart grid is a variety of sensors, devices, and data sets that continuously capture high-resolution data equal to individual IoT conditions. A vast amount of data is one of the biggest challenges on the Internet of Things. Edge Computing is trying to process data close to linked sensors to address this problem, where the data is gathered and processed. This paper aims to investigate the edge computing solutions for the smart grid. A comprehensive review of both emerging issues and edge computing in the Smart Grid environment is discussed and explained. There are two primary components to the energy sharing process among Prosumers: information/digital technologies and Artificial Intelligence Scheduling Techniques. Each of them is mentioned in detail to discuss the Prosumer smart Grid. Furthermore, Edge Computing and classifications (cloudlet, Fog computing and Multi-Access) are among the suitable network methods mentioned in this paper. Some techniques and methodologies have been extensively covered to improve reader awareness of the Prosumer smart grid system.}
}
@article{SUN2022525,
title = {Effects of spatial scale of atmospheric reanalysis data on clear-sky surface radiation modeling in tropical climates: A case study for Singapore},
journal = {Solar Energy},
volume = {241},
pages = {525-537},
year = {2022},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X2200411X},
author = {Xixi Sun and Dazhi Yang and Christian A. Gueymard and Jamie M. Bright and Peng Wang},
keywords = {MERRA-2, Aerosol optical depth, Water vapor, Clear-sky radiation model, Inter-comparison, Spatial scale mismatch},
abstract = {Solar resource assessments most generally require atmospheric information, which is customarily acquired from gridded datasets. The spatial scale mismatch problem, i.e., the difference in spatial representativeness of gridded data and in situ measurements, therefore becomes relevant. This study examines how the gridded data used as inputs to clear-sky radiation models can affect their performance at urban scale. The tropical island of Singapore is selected for the case study. Aerosol optical depth at 550 nm (AOD550), Å ngström exponent (AE), and precipitable water (PW) from both the MERRA-2 reanalysis and ground-based stations (AERONET and SuomiNet) are collected between 2013–2020. Firstly, it is found that, relatively to the AERONET ground truth, the bias in MERRA-2’s AOD550 is more prominent than that in AE or PW. Next, the bias propagation from the gridded inputs (AOD550, AE, and PW) to clear-sky radiation predictions is explored using various models. The estimated clear-sky direct normal irradiance (DNIcs) is more sensitive to AOD550 variation than the clear-sky global horizontal irradiance (GHIcs). Six clear-sky radiation models, five of which accept MERRA-2 gridded inputs, are compared with each other, and with the in situ irradiance measurements recorded at 9 sites. The inter-model difference across Singapore is remarkably consistent because the whole island fits inside a single MERRA-2 grid cell. Under high-AOD550 situations, however, the inter-model deviation becomes large for both GHIcs and DNIcs. The conventional model-versus-measurement comparison shows that each model achieves very different site-to-site performance, largely because the spatially-averaged inputs cannot fully represent the micro-climatic variability. Relatively speaking, no clear-sky radiation model significantly outperforms its peers. The simple MAC2 model and the empirical (locally derived) Yang GHIcs-only model are recommended for Singapore.}
}
@article{CHOWDHURY2022100899,
title = {Unlocking the value of artificial intelligence in human resource management through AI capability framework},
journal = {Human Resource Management Review},
pages = {100899},
year = {2022},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2022.100899},
url = {https://www.sciencedirect.com/science/article/pii/S1053482222000079},
author = {Soumyadeb Chowdhury and Prasanta Dey and Sian Joel-Edgar and Sudeshna Bhattacharya and Oscar Rodriguez-Espindola and Amelie Abadie and Linh Truong},
keywords = {Artificial intelligence, Organisational resources, AI capability, Human resource management, Systematic review, AI-employee collaboration},
abstract = {Artificial Intelligence (AI) is increasingly adopted within Human Resource management (HRM) due to its potential to create value for consumers, employees, and organisations. However, recent studies have found that organisations are yet to experience the anticipated benefits from AI adoption, despite investing time, effort, and resources. The existing studies in HRM have examined the applications of AI, anticipated benefits, and its impact on human workforce and organisations. The aim of this paper is to systematically review the multi-disciplinary literature stemming from International Business, Information Management, Operations Management, General Management and HRM to provide a comprehensive and objective understanding of the organisational resources required to develop AI capability in HRM. Our findings show that organisations need to look beyond technical resources, and put their emphasis on developing non-technical ones such as human skills and competencies, leadership, team co-ordination, organisational culture and innovation mindset, governance strategy, and AI-employee integration strategies, to benefit from AI adoption. Based on these findings, we contribute five research propositions to advance AI scholarship in HRM. Theoretically, we identify the organisational resources necessary to achieve business benefits by proposing the AI capability framework, integrating resource-based view and knowledge-based view theories. From a practitioner’s standpoint, our framework offers a systematic way for the managers to objectively self-assess organisational readiness and develop strategies to adopt and implement AI-enabled practices and processes in HRM.}
}
@article{BJERREGAARD2022,
title = {Framing strategy under high complexity: Processes and practices of ongoing reframing in the becoming of strategy},
journal = {European Management Journal},
year = {2022},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2022.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0263237322000536},
author = {Toke Bjerregaard and Frederik Jeppesen},
keywords = {Framing, Strategy as practice, Complex context, Digital transformation, Ethnography},
abstract = {Framing is a key concept in research on how strategists legitimize and win support for strategic change by establishing a frame of reference for that change. This article advances research on strategy framing by showing how, under conditions of high complexity and uncertainty, strategists continuously reframe strategy in relation to shifting constellations of stakeholders. It presents the findings of an ethnographic study of strategizing in the highly complex context of the digital transformation journey of a global manufacturing firm. It shows how (re)framing practices are combined to iteratively shape strategy formation in ways that sustain strategic influence in the face of constant threats to legitimacy. By accounting for how (re)framing practices reach back and forth in time, the ethnographic findings refine the conventional understanding of how framing resources of past strategizing enter and reworked in present strategy work. Finally, the article contributes empirical insights into how information systems specialists, often marginalized as strategic actors, frame and pitch strategic projects to gain and exert influence in strategy formation processes.}
}
@article{K2022104724,
title = {A review of preserving privacy in data collected from buildings with differential privacy},
journal = {Journal of Building Engineering},
volume = {56},
pages = {104724},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.104724},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222007379},
author = {Janghyun K and Barry H and Tianzhen H and Marc A. P},
keywords = {Differential privacy, Building, Meter, Data},
abstract = {Significant amounts of data are collected in buildings. While these data have great potential for maximizing the energy efficiency of buildings in general, only a small portion of the data are accessible to researchers, government, and industry for analyses. Concerns about privacy are one of the major barriers prohibiting access to these data. Privacy preservation techniques are generally applied to this problem not only to preserve underlying privacy but also to improve the usefulness of data. Among various privacy preserving techniques, differential privacy has become one of the more popular solutions since its introduction in 2006. Differential privacy is a mathematical measure for protecting privacy so that one's privacy cannot be incurred by participating in a database. Although significant research improvements have been made for more than a decade, applying differential privacy to data collected in buildings is still an immature field of study. Because implementing differential privacy on a certain use case is not straightforward and can be achieved with various configurations, it is important to understand variation of configurations with different use cases around data collected from buildings. This literature review aims to introduce what has been done to implement differential privacy in data collected in buildings, and to discuss associated challenges and potential future research opportunities.}
}
@article{WANG202259,
title = {Explainable AI techniques with application to NBA gameplay prediction},
journal = {Neurocomputing},
volume = {483},
pages = {59-71},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222001333},
author = {Yuanchen Wang and Weibo Liu and Xiaohui Liu},
keywords = {Data science, Explainable artificial intelligence, NBA, Clustering, Regression},
abstract = {In this paper, an explainable artificial intelligence (AI) technique is employed to analyze the match style and gameplay of the national basketball association (NBA). A descriptive analysis on the evolution of the NBA gameplay is conducted by using clustering and principal component analysis. Supervised-learning based AI models (including the random forest and the feed-forward neural network) are applied to produce accurate predictions on NBA outcomes at a season-by-season and a month-by-month basis. To evaluate the interpretability of the established AI models, an explainable AI algorithm is utilized to deduce and assess the precise reasoning behind the model prediction based on the local interpretable model-agnostic explanation method. To illustrate its application potential, the method is applied to the open-source NBA data from 1980 to 2019. Experimental results demonstrate the effectiveness of the introduced explainable AI algorithm on predicting NBA outcomes with interpretation.}
}
@article{SONI2022103419,
title = {Machine learning techniques in emerging cloud computing integrated paradigms: A survey and taxonomy},
journal = {Journal of Network and Computer Applications},
volume = {205},
pages = {103419},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103419},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000765},
author = {Dinesh Soni and Neetesh Kumar},
keywords = {Cloud computing, Edge computing, Fog computing, Mist computing, IoT, SDN, Cybertwin, Industry 4.0, Digitaltwin, IIoT, Machine learning (ML), Integrated computing paradigm},
abstract = {Cloud computing offers Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) to provide compute, network, and storage capabilities to the clients utilizing the pay-per-use model. On the other hand, Machine Learning (ML) based techniques are playing a major role in effective utilization of the computing resources and offering Quality of Service (QoS). Based on the customer’s application requirements, several cloud computing-based paradigms i.e., edge computing, fog computing, mist computing, Internet of Things (IoT), Software-Defined Networking (SDN), cybertwin, and industry 4.0 have been evolved. These paradigms collaborate to offer customer-centric services with the backend of cloud server/data center. In brief, cloud computing has been emerged with respect to the above-mentioned paradigms to enhance the Quality of Experience (QoE) for the users. In particular, ML techniques are the motivating factor to backend the cloud for emerging paradigms, and ML techniques are essentially enhancing the usages of these paradigms by solving several problems of scheduling, resource provisioning, resource allocation, load balancing, Virtual Machine (VM) migration, offloading, VM mapping, energy optimization, workload prediction, device monitoring, etc. However, a comprehensive survey focusing on multi-paradigm integrated architectures, technical and analytical aspects of these paradigms, and the role of ML techniques in emerging cloud computing paradigms are still missing, and this domain needs to be explored. To the best of the authors’ knowledge, this is the first survey that investigates the emerging cloud computing paradigms integration considering the most dominating problem-solving technology i.e., ML. This survey article provides a comprehensive summary and structured layout for the vast research on ML techniques in the emerging cloud computing paradigm. This research presents a detailed literature review of emerging cloud computing paradigms: cloud, edge, fog, mist, IoT, SDN, cybertwin, and industry 4.0 (IIoT) along with their integration using ML. To carry out this study, majorly, the last five years (2017-21) articles are explored and analyzed thoroughly to understand the emerging integrated architectures, the comparative study on several attributes, and recent trends. Based on this, research gaps, challenges, and future trends are revealed.}
}
@article{ZOLBANIN2022103282,
title = {Data analytics for the sustainable use of resources in hospitals: Predicting the length of stay for patients with chronic diseases},
journal = {Information & Management},
volume = {59},
number = {5},
pages = {103282},
year = {2022},
note = {Big Data Analytics for Sustainability},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103282},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619301594},
author = {Hamed M. Zolbanin and Behrooz Davazdahemami and Dursun Delen and Amir Hassan Zadeh},
keywords = {Data analytics, Length of hospital stay, Deep learning, Temporal evaluation, Sustainability},
abstract = {Various factors are behind the forces that drive hospitals toward more sustainable operations. Hospitals contracting with Medicare, for instance, are reimbursed for the procedures performed, regardless of the number of days that patients stay in the hospital. This reimbursement structure has incentivized hospitals to use their resources (such as their beds) more efficiently to maximize revenues. One way hospitals can improve bed utilization is by predicting patients’ length of stay (LOS) at the time of admission, the benefits of which extend to employees, communities, and the patients themselves. In this paper, we employ a data analytics approach to develop and test a deep learning neural network to predict LOS for patients with chronic obstructive pulmonary disease (COPD) and pneumonia. The theoretical contribution of our effort is that it identifies variables related to patients’ prior admissions as important factors in the prediction of LOS in hospitals, thereby revising the current paradigm in which patients’ medical histories are rarely considered for the prediction of LOS. The methodological contributions of our work include the development of a data engineering methodology to augment the data sets, prediction of LOS as a numerical (rather than a binary) variable, temporal evaluation of the training and validation data sets, and a significant improvement in the accuracy of predicting LOS for COPD and pneumonia inpatients. Our evaluations show that variables related to patients’ previous admissions are the main driver of the deep network’s superior performance in predicting the LOS as a numerical variable. Using the assessment criteria introduced in prior studies (i.e., ±2 days and ±3 days tolerance), our models are able to predict the length of hospital stay with 86 % and 91 % accuracy for the COPD data set, and with 74 % and 85 % accuracy for the pneumonia data set. Hence, our effort could help hospitals serve a larger number of patients with a fixed amount of resources, thereby reducing their environmental footprint while increasing their revenue, as well as their patients’ satisfaction.}
}
@article{GAUTRON2022107182,
title = {Reinforcement learning for crop management support: Review, prospects and challenges},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107182},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107182},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922004999},
author = {Romain Gautron and Odalric-Ambrym Maillard and Philippe Preux and Marc Corbeels and Régis Sabbadin},
keywords = {reinforcement learning, multi-armed bandit, machine learning, decision support system, crop management},
abstract = {Reinforcement learning (RL), including multi-armed bandits, is a branch of machine learning that deals with the problem of sequential decision-making in uncertain and unknown environments through learning by practice. While best known for being the core of the artificial intelligence (AI) world’s best Go game player, RL has a vast range of potential applications. RL may help to address some of the criticisms leveled against crop management decision support systems (DSS): it is an interactive, geared towards action, contextual tool to evaluate series of crop operations faced with uncertainties. A review of RL use for crop management DSS reveals a limited number of contributions. We profile key prospects for a human-centered, real-world, interactive RL-based system to face tomorrow’s agricultural decisions, and theoretical and ongoing practical challenges that may explain its current low uptake. We argue that a joint research effort from the RL and agronomy communities is necessary to explore RL’s full potential.}
}
@article{ABDELDAYEM2022149834,
title = {Viral outbreaks detection and surveillance using wastewater-based epidemiology, viral air sampling, and machine learning techniques: A comprehensive review and outlook},
journal = {Science of The Total Environment},
volume = {803},
pages = {149834},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.149834},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721049093},
author = {Omar M. Abdeldayem and Areeg M. Dabbish and Mahmoud M. Habashy and Mohamed K. Mostafa and Mohamed Elhefnawy and Lobna Amin and Eslam G. Al-Sakkari and Ahmed Ragab and Eldon R. Rene},
keywords = {SARS-CoV-2, COVID-19, Wastewater based-epidemiology, Viral air surveillance, Artificial intelligence, Artificial neural networks, Machine learning, Deep learning, Reinforcement Learning},
abstract = {A viral outbreak is a global challenge that affects public health and safety. The coronavirus disease 2019 (COVID-19) has been spreading globally, affecting millions of people worldwide, and led to significant loss of lives and deterioration of the global economy. The current adverse effects caused by the COVID-19 pandemic demands finding new detection methods for future viral outbreaks. The environment's transmission pathways include and are not limited to air, surface water, and wastewater environments. The wastewater surveillance, known as wastewater-based epidemiology (WBE), can potentially monitor viral outbreaks and provide a complementary clinical testing method. Another investigated outbreak surveillance technique that has not been yet implemented in a sufficient number of studies is the surveillance of Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2) in the air. Artificial intelligence (AI) and its related machine learning (ML) and deep learning (DL) technologies are currently emerging techniques for detecting viral outbreaks using global data. To date, there are no reports that illustrate the potential of using WBE with AI to detect viral outbreaks. This study investigates the transmission pathways of SARS-CoV-2 in the environment and provides current updates on the surveillance of viral outbreaks using WBE, viral air sampling, and AI. It also proposes a novel framework based on an ensemble of ML and DL algorithms to provide a beneficial supportive tool for decision-makers. The framework exploits available data from reliable sources to discover meaningful insights and knowledge that allows researchers and practitioners to build efficient methods and protocols that accurately monitor and detect viral outbreaks. The proposed framework could provide early detection of viruses, forecast risk maps and vulnerable areas, and estimate the number of infected citizens.}
}
@article{CHENG2022960,
title = {Evaluation of opaque deep-learning solar power forecast models towards power-grid applications},
journal = {Renewable Energy},
volume = {198},
pages = {960-972},
year = {2022},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2022.08.054},
url = {https://www.sciencedirect.com/science/article/pii/S0960148122012204},
author = {Lilin Cheng and Haixiang Zang and Zhinong Wei and Fengchun Zhang and Guoqiang Sun},
keywords = {Solar power forecasting, Deep learning, Forecasting interpretability, Model evaluation, Solar photovoltaic, Integrated solar power system},
abstract = {Solar photovoltaic power plays a vital role in global renewable energy power generation, and an accurate solar power forecast can further promote applications in integrated power systems. Due to advanced artificial intelligence technologies, various deep-learning models have been developed with the benefits of improved prediction precision, but these models inevitably sacrifice their interpretability compared to linear methods. Since a 100% accurate forecast is impossible to achieve, an opaque black-box model will always raise doubts for the operators of renewable power-grids, especially when the prediction deviation may produce higher economic costs and even a system turbulence. Motivated by this, the present study summarizes the requirements of deep-learning solar power forecast models from the power-grid application perspective. Post-hoc evaluation and discussion are conducted to analyze the performances of a typical deep-learning benchmark model based on open-access dataset for solar forecasting. Based on the results, the aim of this study is to increase confidence of deep-learning-based intelligent models into the practical engineering utilization of solar power forecasting. The case studies indicate that some simple evaluation procedures can aid a better understanding of the factors that influence the performances of opaque models, and these procedures can help in the design methods for model modifications.}
}
@article{PILLERON2022346,
title = {International trends in cancer incidence in middle-aged and older adults in 44 countries},
journal = {Journal of Geriatric Oncology},
volume = {13},
number = {3},
pages = {346-355},
year = {2022},
issn = {1879-4068},
doi = {https://doi.org/10.1016/j.jgo.2021.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1879406821002575},
author = {Sophie Pilleron and Naser Alqurini and Jacques Ferlay and Kristen R. Haase and Michelle Hannan and Maryska Janssen-Heijnen and Kumud Kantilal and Kota Katanoda and Cindy Kenis and Grace Lu-Yao and Tomohiro Matsuda and Erna Navarrete and Nikita Nikita and Martine Puts and Fay J. Strohschein and Eva J.A. Morris},
keywords = {Older adults, Neoplasms, Incidence, Trends, Epidemiology, Population-based cancer registries},
abstract = {Objective
We examine international incidence trends of lung, colorectal, prostate, and breast cancers, as well as all cancers combined excluding non-melanoma skin cancer (NMSC) in adults aged 50 and older, over a fifteen-year period using data from 113 high quality population-based cancer registries included in the Cancer in Five Continents (CI5) series and NORDCAN.
Materials and methods
We calculated annual incidence rates between 1998 and 2012 for ages 50–64, 65–74, and 75+, by sex and both sexes combined. We estimated average annual percentage change (AAPC) in rates using quasi-Poisson regression models.
Results
From 1998 to 2012, incidence trends for all cancers (excluding NMSC) have increased in most countries across all age groups, with the greatest increase observed in adults aged 75+ in Ecuador (AAPC = +3%). Colorectal cancer incidence rates increased in the majority of countries, across all age groups. Lung cancer rates among females have increased but decreased for males. Prostate cancer rates have sharply increased in men aged 50–64 with AAPC between 5% and 15% in 24 countries, while decreasing in the 75+ age group in 21 countries, by up to −7% in Bahrain. Female breast cancer rates have increased across all age groups in most countries, especially in the 65–74 age group and in Asia with AAPC increasing to 7% in the Republic of Korea.
Conclusions
These findings assist with anticipating changing patterns and needs internationally. Due to the specific needs of older patients, it is urgent that cancer systems adapt to address their growing number.}
}
@article{ARIENTI2022209,
title = {The methodology of a “living” COVID-19 registry development in a clinical context},
journal = {Journal of Clinical Epidemiology},
volume = {142},
pages = {209-217},
year = {2022},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2021.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0895435621003747},
author = {Chiara Arienti and Silvia Campagnini and Lorenzo Brambilla and Chiara Fanciullacci and Stefano Giuseppe Lazzarini and Andrea Mannini and Michele Patrini and Maria Chiara Carrozza},
keywords = {COVID-19, Registry, Living systematic review, Real World Data, methodological study, rehabilitation},
abstract = {Objective
The aim of this study was to describe an innovative methodology of a registry development, constantly updated for the scientific assessment and analysis of the health status of the population with COVID-19.
Study Design and Setting
A methodological study design to develop a multi-site, Living COVID-19 Registry of COVID-19 patients admitted in Fondazione Don Gnocchi centres started in March 2020.
Results
The integration of the living systematic reviews and focus group methodologies led to a development of a registry which includes 520 fields filled in for 748 COVID-19 patients recruited from 17 Fondazione Don Gnocchi centres. The result is an evidence and experience-based registry, according to the evolution of a new pathology which was not known before outbreak of March 2020 and with the aim of building knowledge to provide a better quality of care for COVID-19 patients.
Conclusion
A Living COVID-19 Registry is an open, living and up to date access to large-scale patient-level data sets that could help identifying important factors and modulating variable for recognising risk profiles and predicting treatment success in COVID-19 patients hospitalized. This innovative methodology might be used for other registries, to be sure which the data collected is an appropriate means of accomplishing the scientific objectives planned.
Clinical trial registration number
not applicable}
}
@article{WANG2022109206,
title = {Privacy protection federated learning system based on blockchain and edge computing in mobile crowdsourcing},
journal = {Computer Networks},
volume = {215},
pages = {109206},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109206},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002936},
author = {Weilong Wang and Yingjie Wang and Yan Huang and Chunxiao Mu and Zice Sun and Xiangrong Tong and Zhipeng Cai},
keywords = {Mobile crowdsourcing, Privacy protection, Blockchain, Edge computing, Federated learning, Localized Differential Privacy},
abstract = {With the rapid popularization and development of the Internet of Things (IoT) and 5G networks, mobile crowdsourcing (MCS) has become an indispensable part in today’s society. However, when task participants submit tasks, they are likely to expose their data privacy and location privacy. These privacy will be maliciously attacked and exploited by attackers (external attackers and untrusted third party). With the rapid increase of MCS data throughput, traditional cloud platforms can no longer meet the huge data processing needs. To solve these problems, this paper proposes an MCS federated learning system based on Blockchain and edge computing. This paper uses federated learning as the framework of the MCS system. The system protects data privacy and location privacy by using the Double local disturbance Localized Differential Privacy (DLD-LDP) proposed in this paper. Because the sensed data exists in multiple modalities (text, video, audio, etc.), this paper uses the Multi-modal Transformer (MulT) method to merge the multi-modal data before subsequent operations. To solve the problem that the third party is untrusted, we utilize Blockchain to distribute tasks and collect models in a distributed way. A reputation calculation method (Sig-RCU) is proposed to calculate the real-time reputation of task participants. Through conducting experiments on real data sets, the effectiveness and adaptation of the proposed DLD-LDP algorithm and Sig-RCU algorithm are verified.}
}
@article{JOHNSON2022105743,
title = {Metacognition for artificial intelligence system safety – An approach to safe and desired behavior},
journal = {Safety Science},
volume = {151},
pages = {105743},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105743},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522000832},
author = {Bonnie Johnson},
keywords = {Metacognition, Artificial intelligence systems, Machine learning, System safety, Complexity},
abstract = {Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human–machine teaming operations. In parallel, the increasing volume, velocity, variety, veracity, value, and variability of data is confounding the complexity of these new systems – creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.}
}
@article{PUTHAL2022107754,
title = {Decision tree based user-centric security solution for critical IoT infrastructure},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107754},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107754},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000623},
author = {Deepak Puthal and Stanly Wilson and Ashish Nanda and Ming Liu and Srinibas Swain and Biswa P.S. Sahoo and Kumar Yelamarthi and Prashant Pillai and Hesham El-Sayed and Mukesh Prasad},
keywords = {Internet of things, Cryptography, Security, Decision tree, Software defined perimeter},
abstract = {Data processing in real-time brings better business modeling and an intuitive plan of action. Internet of things (IoT), being a source of sensitive data collected and communicated through either public or private networks, requires better security from end to end to uphold integrity, quality, and acceptability of data. Designing an adaptive solution plays a vital role where IoT is deployed for the sensing-as-a-services in the critical infrastructure and near real-time decision making by deploying data analysis in the edge datacenters. Again, securing the system with user’s demand and device specifications is a challenging and open research problem. This paper proposed a decision tree based user-centric security approach named DecisionTSec that provides a secure channel for communication in IoT networks, combining edge datacenters in the network edges. Further, the proposed DecisionTSec is validated by experimenting with the real-time testbed for the system performance along with the theoretical security validation.}
}
@article{SAVIC202235,
title = {Digital Water Developments and Lessons Learned from Automation in the Car and Aircraft Industries},
journal = {Engineering},
volume = {9},
pages = {35-41},
year = {2022},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S2095809921002794},
author = {Dragan Savić},
keywords = {Digitalization, Automation, Water sector, Potential risks, Lessons},
abstract = {The provision of water and sanitation services is a key challenge worldwide. The size, complexity, and critical nature of the water and wastewater infrastructure providing such services make the planning and management of these systems extremely difficult. Following the digital revolution in many areas of our lives, the water sector has begun to benefit from digital transformation. Effective utilization of remotely sensed weather and soil moisture data for more efficient irrigation (i.e., for food production), better detection of anomalies and faults in pipe networks using artificial intelligence, the use of nature-inspired optimization to improve the management and planning of systems, and greater use of digital twins and robotics all exhibit great potential to change and improve the ways in which complex water systems are managed. However, there are additional risks associated with these developments, including—but not limited to—cybersecurity, incorrect use, and overconfidence in the capability and accuracy of digital solutions and automation. This paper identifies key advances in digital technology that have found application in the water sector, and applies forensic engineering principles to failures that have been experienced in industries further ahead with automation and digital transformation. By identifying what went wrong with new digital technologies that might have contributed to high-profile accidents in the car and aircraft industries (e.g., Tesla self-driving cars and the Boeing 737 MAX), it is possible to identify similar risks in the water sector, learn from them, and prevent future failures. The key findings show that: ① Automation will require “humans in the loop”; ② human operators must be fully aware of the technology and trained to use it; ③ fallback manual intervention should be available in case of technology malfunctioning; ④ while redundant sensors may be costly, they reduce the risks due to erroneous sensor readings; ⑤ cybersecurity risks must be considered; and ⑥ ethics issues have to be considered, given the increasing automation and interconnectedness of water systems. These findings also point to major research areas related to digital transformation in the water sector.}
}
@article{QIAN2022109394,
title = {Research on deterioration evolution trend of primary loop piping in nuclear power plant based on fusion health index},
journal = {Annals of Nuclear Energy},
volume = {179},
pages = {109394},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109394},
url = {https://www.sciencedirect.com/science/article/pii/S030645492200425X},
author = {Hong Qian and Bangzhi Xu and Jun Zhang},
keywords = {Nuclear power plant, Piping deterioration, Health index, Improved Mahalanobis distance, Convolutional neural networks, Long short-term memory neural networks},
abstract = {The perennial operation of nuclear power primary loop piping leads to deterioration of the piping and the potential risk of leakage. At present, there is no reliable technology to detect the leakage directly. This paper proposes a method to obtain the piping deterioration evolution trend based on the analysis of sets of data that can reflect real state of the piping. The specific method is to quantitatively construct a fusion health index model based on the improved Mahalanobis distance, which integrates the analytic hierarchy process and the entropy weight method. A prediction model combining convolutional neural networks and long short-term memory neural networks is established to make the trend analysis and prediction. The experimental results show that the method can better reflect the actual health state of the piping and effectively predict the deterioration evolution trend, which provides a specific reference value for ensuring the safe and stable operation of equipment.}
}
@article{SHI2022106836,
title = {Real-time driving risk assessment using deep learning with XGBoost},
journal = {Accident Analysis & Prevention},
volume = {178},
pages = {106836},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2022.106836},
url = {https://www.sciencedirect.com/science/article/pii/S0001457522002718},
author = {Liang Shi and Chen Qian and Feng Guo},
keywords = {Crash prediction, High frequency kinematic driving data, Deep learning, Convolutional neural network, Gated recurrent unit, XGBoost, Naturalistic driving study},
abstract = {Traffic crashes typically occur in a few seconds and real-time prediction can significantly benefit traffic safety management and the development of safety countermeasures. This paper presents a novel deep learning model for crash identification based on high-frequency, high-resolution continuous driving data. The method consists of feature engineering based on Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU) and classification based on Extreme Gradient Boosting (XGBoost). The CNN-GRU architecture captures the time series characteristics of driving kinematics data. Compared to normal driving segments, safety-critical events (SCEs)—i.e., crashes and near-crashes (CNC)—are rare. The weighted categorical cross-entropy loss and oversampling methods are utilized to address this imbalance issue. An XGBoost classifier is utilized instead of the multi-layer perceptron (MLP) to achieve a high precision and recall rate. The proposed approach is applied to the Second Strategic Highway Research Program Naturalistic Driving Study (SHRP 2 NDS) data with 1,820 crashes, 6,848 near-crashes, and 59,997 normal driving segments. The results show that in a 3-class classification system (crash, near-crash, normal driving segments), the accuracy for the overall model is 97.5%, and the precision and recall for crashes are 84.7%, and 71.3% respectively, which is substantially better than benchmarks models. Furthermore, the recall of the most severe crashes is 98.0%. The proposed crash identification approach provides an accurate, highly efficient, and scalable way to identify crashes based on high frequency, high-resolution continuous driving data and has broad application prospects in traffic safety applications.}
}
@article{WIBOWO2022484,
title = {Problem identification and intervention in the higher education data synchronization system in Indonesia},
journal = {Procedia Computer Science},
volume = {197},
pages = {484-494},
year = {2022},
note = {Sixth Information Systems International Conference (ISICO 2021)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.165},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921023899},
author = {Radityo Prasetianto Wibowo and Ika Nurkasanah and Rully Agus Hendrawan and Umi Laili Yuhana and Arif Wibisono and Nur Aini Lestari and Siti Aminatus Zehroh},
keywords = {PDDikti feeder, AKM completeness, pareto analysis, application of metadata management, data governance},
abstract = {IT Department identified problems in reporting student study activities (AKM) by interviewing stakeholders. Then the priorities of the problems are ineffective business process and technical matters determined by using Pareto Analysis. This study proposed two solutions, application of data governance to achieve an effective business process and application of metadata management to solve the technical matters. Both solutions were socialized, tested and applied among all stakeholders. The result was that the percentage of AKM completeness increased significantly from 84% to 100% and Data Reporting Team now can send data faster and reduce data transmission errors which decrease technical debt problem.}
}
@article{DIESTE2022108532,
title = {Organizational tensions in industry 4.0 implementation: A paradox theory approach},
journal = {International Journal of Production Economics},
volume = {251},
pages = {108532},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108532},
url = {https://www.sciencedirect.com/science/article/pii/S0925527322001256},
author = {Marcos Dieste and Philipp C. Sauer and Guido Orzes},
keywords = {Managerial challenges, Tensions, Paradox theory, Fourth industrial revolution, Digitalization, Smart manufacturing},
abstract = {While implementing Industry 4.0, organizational environments become more global, dynamic, and competitive thereby intensifying contradictory demands. In light of the resource and knowledge intensiveness of this often multi-year process, this study draws on paradox theory to identify the main organizational tensions emerging and persisting during the Industry 4.0 transformation of companies in a two-step approach. First, a systematic review of 73 academic papers on organizational challenges in Industry 4.0 adoption is conducted that summarizes 35 key challenges. Second, from these challenges a conceptual framework is built that illustrates the main tensions in Industry 4.0 implementation. The identified organizational tensions are categorized according to the learning, organizing, belonging, and performing categories proposed by the paradox theory. Moreover, resolution strategies to address these tensions have been drawn from the reviewed literature. These strategies are presented and linked to the individual tensions. Finally, tensions and resolution strategies were preliminary validated by a group of Industry 4.0 professionals during a workshop. As a result, the findings provide 23 tensions and 18 related resolution strategies illustrating how organizations can address to competing demands simultaneously when implementing Industry 4.0 technologies and thus raise their competitiveness and performance. Based on these results, the article discusses implications for operations management practitioners that can use the proposed framework to inform their strategies and decisions in Industry 4.0 implementation. Moreover, policymakers can adopt the results to develop focussed support actions for driving the Industry 4.0 transition. Finally, four main avenues for future research and implications for operations management scholars are provided.}
}
@article{BADICA202242,
title = {Exploring the Usability of Process Mining in Smart City},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {11},
pages = {42-47},
year = {2022},
note = {IFAC Workshop on Control for Smart Cities CSC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322011351},
author = {Amelia Bădică and Costin Bădică and Ion Buligiu and Liviu-Ion Ciora},
keywords = {Optimization, Decision Making in Smart City Control, System Theory in Smart City Control, Cyber-Physical Systems},
abstract = {The aim of this paper is to explore the usability of Process Mining approaches in Smart City applications. Our research was triggered by the following three research questions, concerning the initial investigation of: i) the most researched Smart City problems using the methods of Process Mining; ii) the most utilized Process Mining methods in Smart City applications; iii) the most popular Smart City topics that were not approached yet from the perspective of Process Mining. This analysis will result in a set of challenges and opportunities of utilizing the most innovative Process Mining methods for solving emergent Smart City problems.}
}
@article{FOSHAMMER2022108573,
title = {Identification of aftermarket and legacy parts suitable for additive manufacturing: A knowledge management-based approach},
journal = {International Journal of Production Economics},
volume = {253},
pages = {108573},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108573},
url = {https://www.sciencedirect.com/science/article/pii/S0925527322001657},
author = {Jeppe Foshammer and Peder Veng Søberg and Petri Helo and Iñigo Flores Ituarte},
keywords = {Additive manufacturing, 3D print, Part identification, Aftermarket parts, Legacy parts, Knowledge management},
abstract = {A research stream identifying aftermarket and legacy parts suitable for additive manufacturing (AM) has emerged in recent years. However, existing research reveals no golden standard for identifying suitable part candidates for AM and mainly combines preexisting methods that lack conceptual underpinnings. As a result, the identification approaches are not adjusted to organizations and are not completely operationalizable. Our first contribution is to investigate and map the existing literature from the perspective of knowledge management (KM). The second contribution is to develop and empirically investigate a combined part-identification approach in a defense sector case study. The part identification entailed an analytical hierarchy process (AHP), semi-structured interviews, and workshops. In the first run, we screened 35,000 existing aftermarket and legacy parts. Similar to previous research, the approach was not in sync with the organization. However, in contrast to previous research, we infuse part identification with KM theory by developing and testing a “Phase 0” assessment that ensures an operational fit between the approach and the organization. We tested Phase 0 and the knowledge management-based approach in a second run, which is the main contribution of this study. This paper contributes empirical research that moves beyond previous research by demonstrating how to overcome the present challenges of part identification and outlines how knowledge management-based part identification integrates with current operations and supply chains. The paper suggests avenues for future research related to AM; however, it also concerns Industry 4.0, lean improvement, and beyond, particularly from the perspective of KM.}
}
@article{BOOBALAN2022109048,
title = {Fusion of Federated Learning and Industrial Internet of Things: A survey},
journal = {Computer Networks},
volume = {212},
pages = {109048},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109048},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622001955},
author = {Parimala Boobalan and Swarna Priya Ramu and Quoc-Viet Pham and Kapal Dev and Sharnil Pandya and Praveen Kumar Reddy Maddikunta and Thippa Reddy Gadekallu and Thien Huynh-The},
keywords = {Data storage, IIoT, Federated Learning, Data privacy, Data sharing, Resource management},
abstract = {Industrial Internet of Things (IIoT) lays a new paradigm for the concept of Industry 4.0 and paves an insight for new industrial era. Nowadays smart machines and smart factories use machine learning/deep learning based models for incurring intelligence. However, storing and communicating the data to the cloud and end device leads to issues in preserving privacy. In order to address this issue, Federated Learning (FL) technology is implemented in IIoT by the researchers nowadays to provide safe, accurate, robust and unbiased models. Integrating FL in IIoT ensures that no local sensitive data is exchanged, as the distribution of learning models over the edge devices has become more common with FL. Therefore, only the encrypted notifications and parameters are communicated to the central server. In this paper, we provide a thorough overview on integrating FL with IIoT in terms of privacy, resource and data management. The survey starts by articulating IIoT characteristics and fundamentals of distributed machine learning and FL. The motivation behind integrating IIoT and FL for achieving data privacy preservation and on-device learning are summarized. Then we discuss the potential of using machine learning (ML), deep learning (DL) and blockchain techniques for FL in secure IIoT. Further we analyze and summarize several ways to handle the heterogeneous and huge data. Comprehensive background on data and resource management are then presented, followed by applications of IIoT with FL in automotive, robotics, agriculture, energy, and healthcare industries. Finally, we shed light on challenges, some possible solutions and potential directions for future research.}
}
@article{DIFRANCO2022105076,
title = {Increasing the interoperability of snow/ice hyperspectral observations},
journal = {Computers & Geosciences},
volume = {162},
pages = {105076},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105076},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422000401},
author = {Sabina {Di Franco} and Roberto Salzano and Enrico Boldrini and Rosamaria Salvatori},
keywords = {Snow, Field spectroscopy, Metadata, Reflectance, Data model, Interoperability},
abstract = {This study aims to set up a metadata profile useful for preparing an interoperable dataset containing snow and ice hyperspectral measurements. The proposed Snow and Ice Spectral Library (SISpec) scheme was prepared for sharing a data collection focused on Antarctica, including 70 observations. Following the perspective to grant “open access” to such a dataset, we found a compromise between the ERC (European Research Council) guidelines, the FAIR (Findability, Accessibility, Interoperability, and Reuse) Data principles defined by the RDA (Research Data Alliance), and the GEO (Group on Earth Observation) Data Sharing Principles. The ISO (International Organization for Standardization) standard 19115 was chosen as the standard framework for describing SISpec. When the available metadata scheme was not sufficient or suitable, metadata extensions or new detailed metadata components were created to be compliant with the ISO 19115 standard. We also considered the INSPIRE (Infrastructure for Spatial Information in Europe) requirements and the result is a metadata model that can be useful to share SISpec metadata both in the European and international contexts. Particularly detailed metadata sections and elements were created for describing spectral signatures and microphysical snow parameters.}
}
@article{CARDONE202260,
title = {A fuzzy partition-based method to classify social messages assessing their emotional relevance},
journal = {Information Sciences},
volume = {594},
pages = {60-75},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S002002552200161X},
author = {Barbara Cardone and Ferdinando {Di Martino} and Sabrina Senatore},
keywords = {Fuzzy partition, Emotional categories, Classification, TF-IDF, Fuzzy linguistic labels},
abstract = {With the surge of the large volume of data availability, Machine Learning and mainly Deep Learning techniques are the leading solutions in classification and predictive tasks, targeted at data-efficient learning. These models learn by training on many diversified samples in a process that is computationally expensive or time-consuming. Moreover, in many real-world scenarios, the amount of available data for training is unsuitable, because it is unlabeled or covers only portions of the whole reference domain cases. This paper proposes an alternative approach for document classification that leverages the distribution of the data projected in the multi-dimensional feature space to assess the weight of features in the final classification. The approach does not rely on traditional iterative methods for classification but builds a relevance measure to assess the relevance/importance of the features describing the domain of interest. The idea is to harness this metric to select relevant features and then express the values calculated by these metrics in natural language by exploiting fuzzy variables and linguistic labels to make human comprehension more immediate. The approach has been employed for emotion extraction from social media messages. The novelty of this approach is twofold: first, the well-known TF-IDF measure was reinterpreted as a relevance measure of emotions discovered in text content. Then, the discovered emotion relevance was described by fuzzy linguistic labels, defined on an ad-hoc-designed fuzzy partition, to express the data classification in natural language, more suitable to human understanding.}
}
@article{BATCHU2022109269,
title = {An integrated approach explaining the detection of distributed denial of service attacks},
journal = {Computer Networks},
volume = {216},
pages = {109269},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109269},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622003334},
author = {Raj Kumar Batchu and Hari Seetha},
keywords = {DDoS attacks, Data preprocessing, Feature selection, CICDDoS2019 dataset, SHAP, LIME},
abstract = {In recent years, several machine learning and deep learning models have been designed to detect various DDoS attacks, but the presence of irrelevant features, lack of transparency and class imbalance make these models less efficient. In this paper, we developed a novel efficient model to address these issues in detecting DDOS attacks. To begin with, data preprocessing is performed to improve the quality of the training data. The minority class samples are then generated using the Adaptive Synthetic oversampling technique to overcome the class imbalance. Following that, feature selection is performed by embedding SHAP feature importance within recursive feature elimination with five base classifiers. In addition, the hyperparameter of these classifiers is tuned to determine the most contributed features. Furthermore, global and local explanations for extracted features are provided to ensure transparency. Finally, these features are fed to the dynamic ensemble selection techniques such as KNORA-E and KNORA-U for classification by varying k values. These evaluations are analyzed using the CICDDoS2019 dataset. The evaluations are carried out in balanced and imbalanced data scenarios. The results indicate that the balanced data scenario outperformed the imbalanced data scenario as well as existing approaches. An accuracy of 99.9878% using KNORA-E and 99.9886% using KNORA-U is obtained utilizing the five most contributed features.}
}
@article{AKTER202285,
title = {The future of marketing analytics in the sharing economy},
journal = {Industrial Marketing Management},
volume = {104},
pages = {85-100},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122000815},
author = {Shahriar Akter and Umme Hani and Yogesh K. Dwivedi and Anuj Sharma},
keywords = {Marketing analytics capability, Sharing economy, Marketing agility, Marketing effectiveness, Market turbulence},
abstract = {The rise of sharing economy has accelerated the growth of marketing analytics to match demand and supply in industrial markets. However, the conceptualization of marketing analytics remains unclear in the sharing economy. Theorizing market turbulence as the dark side of the sharing economy, this study presents a marketing analytics capability model using dynamic capabilities and contingency theories to advance thought and practice in industrial marketing research. Using a thematic analysis and a survey-based empirical study on B2B cloud sharing platforms (n = 252), the findings present pattern identification, real-time solutions and data governance as the antecedents of marketing analytics capability with its holistic effects on marketing agility and marketing effectiveness. The empirical findings further support the mediating role of marketing agility and the moderating impact of market turbulence on marketing analytics-effectiveness and marketing agility-effectiveness chain. Overall, our results contribute toward a more nuanced understanding of the dark side of market turbulence on marketing analytics capability dynamics in the sharing economy.}
}
@article{LEE2022106595,
title = {SEMRES - A Triple Security Protected Blockchain Based Medical Record Exchange Structure},
journal = {Computer Methods and Programs in Biomedicine},
volume = {215},
pages = {106595},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106595},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721006696},
author = {Yen-Liang Lee and Hsiu-An Lee and Chien-Yeh Hsu and Hsin-Hua Kung and Hung-Wen Chiu},
keywords = {Blockchain, Security, EMR Protection, Encryption, Data exchange},
abstract = {Background and Objective
COVID-19, a serious infectious disease outbreak started in the end of 2019, has caused a strong impact on the overall medical system, which reflects the gap in the volume and capacity of medical services and highlights the importance of clinical data ex-change and application. The most important concerns of medical records in the medical field include data privacy, data correctness, and data security. By realizing these three goals, medical records can be made available to different hospital information systems to achieve the most complete medical care services. The privacy and protection of health data require detailed specification and usage requirements, which is particularly important for cross-agency data exchange.
Methods
This research is composed of three main modules. "Combined Encryption and Decryption Architecture", which includes the hybrid double encryption mechanism of AES and RSA, and encrypts medical records to produce "Secured Encrypted Medical Record". "Decentralize EMR Repository", which includes data decryption and an exchange mechanism. After a data transmission is completed, the content verification and data decryption process will be launched to confirm the correctness of the data and obtain the data. A blockchain architecture is used to store the hash value of the encrypted EMR, and completes the correctness verification of the EMR after transmission through the hash value.
Results
The results of this study provide an efficient triple encryption mechanism for electronic medical records. SEMRES ensures the correctness of data through the non-repudiation feature of a blockchain open ledger, and complete integrated information security protection and data verification architecture, in order that medical data can be exchanged, verified, and applied in different locations. After the patient receives medical services, the medical record is re-encrypted and verified and stored in the patient's medical record. The blockchain architecture is used to ensure the verification of non-repudiation of medical service, and finally to complete the payment for medical services.
Conclusions
The main aim of this study was to complete a security architecture for medical data, and develop a triple encryption authentication architecture to help data owners easily and securely share personal medical records with medical service personnel.}
}
@article{PENG2022101488,
title = {A collaborative design platform for new alloy material development},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101488},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101488},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002378},
author = {Gongzhuang Peng and Youzhao Sun and Qian Zhang and Quan Yang and Weiming Shen},
keywords = {New material development, Collaborative design platform, Mechanical performance prediction, Mamdani-type fuzzy modeling, Industrial internet of things (IIoT)},
abstract = {To overcome the shortcomings of the conventional trial and error mode for new material development, a full-process collaborative design platform for steel rolling is developed based on an industrial internet of things (IIoT) system in this study. Equipment, process and product entities are modeled in both the physical domain and the cyber domain. A systematic data-driven Mamdani-type fuzzy modeling methodology is proposed to map the relationship between material chemical compositions, organizational structures, process parameters and mechanical performances. The proposed methodology employs a random forest (RF) algorithm to select important parameters from mechanism models, simulation models and production process variables, utilizes a K-means algorithm to merge diverse steel grades into sub-clusters, and implements a multi-objective particle swarm optimization (MOPSO) algorithm to further improve the fuzzy model in terms of both the structure and the membership function parameters. A dataset of 3500 steel coils collected by the prototype platform built in a large hot rolling mill is used to evaluate the performance of the proposed approach. Experiment results show that the proposed methodology performs well in predicting the yield strength, tensile strength and elongation, with the coverage probability over 90% under 10% deviation and about 70% under 5% deviation on average.}
}
@article{SU2022730,
title = {Understanding the relationships between the development of the construction sector, carbon emissions, and economic growth in China: Supply-chain level analysis based on the structural production layer difference approach},
journal = {Sustainable Production and Consumption},
volume = {29},
pages = {730-743},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2021.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S2352550921003316},
author = {Yuqi Su and Zijian Zou and Xiaoming Ma and Junping Ji},
keywords = {Construction sector, Input-output model, SDA, SPLD, Supply chain, CO emissions, Economic growth},
abstract = {Identifying the inter-sectoral relationships between the construction sector and other related sectors is crucial to reducing carbon emissions and promoting economic development. Non-competitive input–output models at comparable prices in 1992, 1997, 2002, 2007, 2012, and 2017 were used to conduct this study. The structural production layer difference (SPLD) method was employed in this study and complemented with the results of structural decomposition analysis to identify significant sectors affecting economic growth and carbon emissions in the construction sector. Please check editor name in valid PIT's and styled as title-footnote or misc-text according to JSS Overall, the carbon emissions and economic growth of China's construction sector in 2017 increased by 2.4 billion tonnes and 10,512.4 billion yuan, respectively, compared to 1992 at the 2000 price level. Important sectors influencing economic growth and carbon emissions in the construction sector are identified. Wholesale and retail trade sector (s31) and finance sector (s33) increased the economic growth of the construction sector without the additional emission of CO2, which is economically and environmentally sustainable. Smelting and processing of the metals (s15) and transport and storage (s29) sectors simultaneously restrained economic growth and inhibited carbon emissions in the construction sector. The production and distribution of the electric and heat power sector (s25) stimulates the growth of carbon emissions in the sector of smelting and processing of the metals (s15), reducing the carbon reduction effect of smelting and processing of the metals (s15) on the construction sector. The SPLD results revealed that the direct impact of other services sector (s35) and transport and storage (s29) on carbon emissions in the construction sector has not been highlighted by the structural decomposition analysis (SDA). In terms of economic added value, processing of petroleum coking and the nuclear fuel sector (s12) is a pivotal sector in the supply chain influencing the added value from the extraction of petroleum and natural gas sector (s3) to the construction sector according to the results of SPLD.}
}
@article{WU2022181,
title = {An ensemble of random decision trees with local differential privacy in edge computing},
journal = {Neurocomputing},
volume = {485},
pages = {181-195},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.145},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016313},
author = {Xiaotong Wu and Lianyong Qi and Jiaquan Gao and Genlin Ji and Xiaolong Xu},
keywords = {random decision tree, privacy preservation, data mining, local differential privacy, edge computing},
abstract = {Edge computing is an emerging computing paradigm, which offers a great opportunity to implement data mining-based services and applications for a large number of devices and sensors in Internet of Things. However, the new paradigm is faced with security and privacy challenges due to the diversity and the limited capability of edge components. In particular, data privacy is one of the most concerned problems for all the participants. In this paper, we propose a framework of privacy-preserving data mining based on private random decision trees in edge computing, which not only gives the strong privacy guarantee, but also provides a certain amount of data utility. Firstly, we design a preservation framework to implement private random decision trees satisfying local differential privacy. Secondly, we present the concrete implementations of algorithms and the corresponding task that each participant needs to undertake. Thirdly, we analyze the key factors to influence privacy and utility, including the allocation of data and privacy budget. Fourthly, we give the improved algorithms to further increase the utility with strong privacy preservation. Finally, extensive experiments demonstrate the good performance of our designed framework.}
}
@article{LI2022103456,
title = {Multi-level video quality services and security guarantees based on compressive sensing in sensor-cloud system},
journal = {Journal of Network and Computer Applications},
volume = {205},
pages = {103456},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103456},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001072},
author = {Min Li and Di Xiao and Hui Huang and Bo Zhang},
keywords = {Compressive video sensing, Multi-level quality services, Multi-level security guarantees, Effective space utilization, The sensor-cloud system},
abstract = {The booming development of the Internet of Things has led to the emergence of novel application systems such as the sensor-cloud system. Numerous data (especially images and videos) are collected, processed, transmitted and stored via the cooperation between the sensor networks and the cloud computing every day, so there exist three main issues to be solved imminently in the sensor-cloud system. (1) Data security: how to ensure the security of required data from the perspective of cloud service users. (2) Effective space utilization: how to economize the cloud storage space as much as possible from the view of cloud service providers. (3) Multi-level quality services and security guarantees: different quality services and security guarantees should be considered for different levels of users, while maximizing financial benefits of sensor network providers and cloud service providers. In this paper, a novel scheme based on compressive sensing with the usage of the private cloud is proposed for three diverse levels of cloud service users to enjoy completely diverse video quality services and security guarantees in the sensor-cloud system. Theoretical analyses and experimental simulations show that the proposed scheme can balance the relationship among sensor network providers, cloud service providers and cloud service users well.}
}
@article{ZHAO2022255,
title = {Perspectives on nonstationary process monitoring in the era of industrial artificial intelligence},
journal = {Journal of Process Control},
volume = {116},
pages = {255-272},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422001184},
author = {Chunhui Zhao},
keywords = {Industrial artificial intelligence, Nonstationary process monitoring, Machine learning, Cointegration analysis, Condition-driven, Semantic knowledge, Cloud–edge collaboration},
abstract = {The development of the Internet of Things, cloud computing, and artificial intelligence has given birth to industrial artificial intelligence (IAI) technology, which enables us to obtain fine perception and in-depth understanding capabilities for the operating conditions of industrial processes, and promotes the intelligent transformation of modern industrial production processes. At the same time, modern industry is facing diversified market demand instead of ultra-large-scale demand, resulting in typical variable conditions, which enhances the nonstationary characteristics of modern industry, and brings great challenges to the monitoring of industrial processes. In this regard, this paper analyzes the complex characteristics of nonstationary industrial operation, reveals the effects on operating condition monitoring, and summarizes the difficulties faced by varying condition monitoring. Furthermore, by reviewing the recent 30 years of development of data-driven methods for industrial process monitoring, we sorted out the evolution of nonstationary monitoring methods, and analyzed the features, advantages and disadvantages of the methods at different stages. In addition, by summarizing the existing related research methods by category, we hope to provide reference for monitoring methods of nonstationary process. Finally, combined with the development trend of industrial artificial intelligence technologies, some promising research directions are given in the field of nonstationary process monitoring.}
}
@article{DAI2022354,
title = {Online quality inspection of resistance spot welding for automotive production lines},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {354-369},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000589},
author = {Wei Dai and Dayong Li and Yongjia Zheng and Dong Wang and Ding Tang and Huamiao Wang and Yinghong Peng},
keywords = {Quality inspection, Process stability, Deep learning, Low-rank and sparse decomposition, Channel attention mechanism},
abstract = {Reliable quality control of resistance spot welding (RSW) is a long-standing challenge, due to random disturbance on automotive production lines. In this paper, a quality evaluation framework is proposed based on dynamic resistance (DR) signals, aiming to accurately predict welding quality. The proposed framework integrates welding process stability with deep learning models. Given the uniform variation pattern of each weld with the same schedule, process stability can be determined based on the reference curve constructed by the low-rank and sparse decomposition method. Subsequently, a one-dimensional convolutional neural network (1DCNN) with channel attention mechanism is developed to further predict welding quality, which can perform channel-wise feature recalibration to enhance the classification performance. Extensive experiments substantiate that the proposed network yields a remarkable classification performance compared with typical algorithms on several RSW datasets collected on an actual production line. This study provides a valuable reference to achieve an intelligent online quality inspection system in the automotive manufacturing industry.}
}
@article{ALEO2022101846,
title = {SNAD transient miner: Finding missed transient events in ZTF DR4 using k-D trees},
journal = {New Astronomy},
volume = {96},
pages = {101846},
year = {2022},
issn = {1384-1076},
doi = {https://doi.org/10.1016/j.newast.2022.101846},
url = {https://www.sciencedirect.com/science/article/pii/S1384107622000574},
author = {P.D. Aleo and K.L. Malanchev and M.V. Pruzhinskaya and E.E.O. Ishida and E. Russeil and M.V. Kornilov and V.S. Korolev and S. Sreejith and A.A. Volnova and G.S. Narayan},
keywords = {Transient sources, Time domain astronomy, Supernovae, Active galactic nuclei},
abstract = {We report the automatic detection of 11 transients (7 possible supernovae and 4 active galactic nuclei candidates) within the Zwicky Transient Facility fourth data release (ZTF DR4), all of them observed in 2018 and absent from public catalogs. Among these, three were not part of the ZTF alert stream. Our transient mining strategy employs 41 physically motivated features extracted from both real light curves and four simulated light curve models (SN Ia, SN II, TDE, SLSN-I). These features are input to a k-D tree algorithm, from which we calculate the 15 nearest neighbors. After pre-processing and selection cuts, our dataset contained approximately a million objects among which we visually inspected the 105 closest neighbors from seven of our brightest, most well-sampled simulations, comprising 89 unique ZTF DR4 sources. Our result illustrates the potential of coherently incorporating domain knowledge and automatic learning algorithms, which is one of the guiding principles directing the SNAD team. It also demonstrates that the ZTF DR is a suitable testing ground for data mining algorithms aiming to prepare for the next generation of astronomical data.}
}
@article{WANG2022102572,
title = {Urban neighborhood socioeconomic status (SES) inference: A machine learning approach based on semantic and sentimental analysis of online housing advertisements},
journal = {Habitat International},
volume = {124},
pages = {102572},
year = {2022},
issn = {0197-3975},
doi = {https://doi.org/10.1016/j.habitatint.2022.102572},
url = {https://www.sciencedirect.com/science/article/pii/S0197397522000698},
author = {Lingqi Wang and Shenjing He and Shiliang Su and Yu Li and Lirong Hu and Guie Li},
keywords = {Neighborhood socioeconomic status, Area deprivation, Machine learning, Open data, Social inequalities, Online housing listings},
abstract = {Understanding the dynamic distribution of residents' socioeconomic status (SES) across neighborhoods within cities is essential for urban planning and policy-making aligning to the Sustainable Development Goals 2030. Whereas the promise in explicitly linking geographical features to SES has been highlighted fairly clear in previous works, scholars hold an eclectic attitude in their outlook, given the absence of theoretical ground, the heavy reliance on nontransparent proprietary data sources and the relatively coarse resolution predictions. Drawing on a case study of Hangzhou metropolitan in China, this paper aims to address these problems by demonstrating a novel approach to neighborhood SES inference based on online housing advertisements. We first revisit the theoretical debates on the linkage between neighborhood SES and online housing advertisements. Then, the Naïve Bayes classifier is employed to semantically identify the topics from online housing advertisements and the associated sentiments are quantified using the lexicon-based approach. Following that, seven commonly used machine learning algorithms are compared and utilized to infer the fine-grained neighborhood SES at residential quarters scale based on the housing attributes and extracted topics from online housing advertisements. Results show that machine learning algorithms vary with predictive ability and the tree-based algorithms are much more powerful in inferring neighborhood SES. More specifically, we distinguish 8 reliable features which not only present relative high importance estimated by all the machine learning algorithms but also exhibit great robustness in inferring neighborhood SES and show promising potential to being applied for unraveling social inequalities. We also observe noteworthy spatial heterogeneity in neighborhood SES across the research site. The demonstrated approach not only enables the policymakers to take stock of deprived neighborhoods in a timely manner, but also lays firm ground for framing contextualized strategies of urban governance. This study is among the first attempts to bridge the theoretical interpretation of housing attributes with the proxy indicator -based approach for fine-grained neighborhood SES measurement.}
}
@article{YANG2022102751,
title = {Social media data analytics for business decision making system to competitive analysis},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102751},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102751},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002326},
author = {Jie Yang and Pishi Xiu and Lipeng Sun and Limeng Ying and Blaanand Muthu},
keywords = {Social media audit, Business intelligence, Business decision making, Competitive analysis, Data analytics},
abstract = {For the past few years, business intelligence has been a major field that uses data analysis to produce key information as part of business decision-making. Data collected from social media sites and blogs are analyzed to make business decisions, a process called social media analytics (SMA). This method, which goes beyond ordinary monitoring or a basic analysis of retweets, develops an in-depth insight into the social consumer. After reading the whole report, add the pertinent figures to the table. Add pertinent data from the Brand24 report to the table. During a social media audit, any followers, impressions, engagement, copy/traffic, and brand mentions are key parameters to analyze. For companies and research institutions, the great interest is to analyse and gain knowledge from user-produced data. These data contain useful knowledge, including customer perceptions feedback and product/service suggestions. Due to content saturation, social media's true meaning regarding business data is hardly ever found. Therefore, in this paper, the business decision making system (BDMS) has been proposed to develop business using social media data analytics. BDMS provides a clear understanding of the key principles, issues and functionality, and big social data developments. Besides, BDMS concentrates on marketing and describes an operational approach for obtaining valuable information from social data. BDMS performs a short and precise description of current use scenarios from the evidence, as per the help of decisions and investment opportunities companies get when using social data analytics. The experimental result shows that BDMS achieves the highest competitive results. With greater accuracy, system dependability, F-1 measurement, and deviation rate of 85.5%, the BDMS system guarantees 93.7%, 86.8%, and 7.0%.}
}
@article{PAN2022100924,
title = {An interdisciplinary review of AI and HRM: Challenges and future directions},
journal = {Human Resource Management Review},
pages = {100924},
year = {2022},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2022.100924},
url = {https://www.sciencedirect.com/science/article/pii/S1053482222000420},
author = {Yuan Pan and Fabian J. Froese},
keywords = {Artificial intelligence (AI), Systematic review, Theory, Method, Human resource management (HRM)},
abstract = {Artificial intelligence (AI) has the potential to change the future of human resource management (HRM). Scholars from different disciplines have contributed to the field of AI in HRM but with rather insufficient cross-fertilization, thus leading to a fragmented body of knowledge. In response, we conducted a systematic, interdisciplinary review of 184 articles to provide a comprehensive overview. We grouped prior research into four categories based on discipline: management and economics, computer science, engineering and operations, and others. The findings reveal that studies in different disciplines had different research foci and utilized different methods. While studies in the technical disciplines tended to focus on the development of AI for specific HRM functions, studies from the other disciplines tended to focus on the consequences of AI on HRM, jobs, and labor markets. Most studies in all categories were relatively weak in theoretical development. We therefore offer recommendations for interdisciplinary collaborations, propose a unified definition of AI, and provide implications for research and practice.}
}
@article{SAHA2022121768,
title = {The interplay of emerging technologies in pharmaceutical supply chain performance: An empirical investigation for the rise of Pharma 4.0},
journal = {Technological Forecasting and Social Change},
volume = {181},
pages = {121768},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121768},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522002931},
author = {Esha Saha and Pradeep Rathore and Ratri Parida and Nripendra P. Rana},
keywords = {Pharma 4.0, Pharmaceutical supply chain, Emerging technologies, Empirical research},
abstract = {The impact and the relevance of the emerging technologies on the supply chains have attracted researchers and practitioners alike worldwide. Based on the resource-based view and organizational information processing theory, this study attempts to investigate how emerging technologies influence supply chain performance, particularly pharmaceutical supply chain in regards to the rise of Pharma 4.0. The pharmaceutical supply chain processes are considered as the mediators and the emerging technology adoption barriers are proposed as the moderators. The study is evaluated using a survey of pharmaceutical companies in India. The findings indicate that manufacturing, distribution and consumption processes in the supply chain mediate the effects of emerging technologies on supply chain performance; however, mediating effects are weakened due to the presence of intricate barriers. This study thereby empirically investigates the interplay between emerging technologies in pharmaceutical supply chain performance and provides managerial insights with a proposed research framework on how to incorporate and encourage Pharma 4.0 for achieving sustainability in the supply chains.}
}
@article{HAMWI2022101004,
title = {Development and integration of VGG and dense transfer-learning systems supported with diverse lung images for discovery of the Coronavirus identity},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101004},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101004},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001472},
author = {Wael Abdulsalam Hamwi and Muhammad Mazen Almustafa},
keywords = {CXR&CT chest COVID-19 images integration of three pre-trained CNN models Fine-tuning, Image processing, Performance evaluation, Artificial intelligence},
abstract = {The contagious SARS-CoV-2 has had a tremendous impact on the life and health of many communities. It was first rampant in early 2019 and so far, 539 million cases of COVID-19 have been reported worldwide. This is reminiscent of the 1918 influenza pandemic. However, we can detect the infected cases of COVID-19 by analysing either X-rays or CT, which are presumably considered the least expensive methods. In the existence of state-of-the-art convolutional neural networks (CNNs), which integrate image pre-processing techniques with fully connected layers, we can develop a sophisticated AI system contingent on various pre-trained models. Each pre-trained model we involved in our study assumed its role in extracting some specific features from different chest image datasets in many verified sources, such as (Mendeley, Kaggle, and GitHub). First, for CXR datasets associated with the CNN trained model from the beginning, whereby is comprised of four layers beginning with the Conv2D layer, which comprises 32 filters, followed by the MaxPooling and afterwards, we reiterated similarly. We used two techniques to avoid overgeneralization, the early stopping and the Dropout techniques. After all, the output was one neuron to classify both cases of 0 or 1, followed by a sigmoid function; in addition, we used the Adam optimizer owing to the more improved outcomes than what other optimizers conducted; ultimately, we referred to our findings by using a confusion matrix, classification report (Recall & Precision), sensitivity and specificity; in this approach, we achieved a classification accuracy of 96%. Our three integrated pre-trained models (VGG16, DenseNet201, and DenseNet121) yielded a remarkable test accuracy of 98.81%. Besides, our merged models (VGG16, DenseNet201) trained on CT images with the utmost effort; this model held an accurate test of 99.73% for binary classification with the (Normal/Covid-19) scenario. Comparing our results with related studies shows that our proposed models were superior to the previous CNN machine learning models in terms of various performance metrics. Our pre-trained model associated with the CT dataset achieved 100% of the F1score and the loss value was approximately 0.00268.}
}
@article{SICARI2022102822,
title = {Insights into security and privacy towards fog computing evolution},
journal = {Computers & Security},
volume = {120},
pages = {102822},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102822},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002164},
author = {Sabrina Sicari and Alessandra Rizzardi and Alberto Coen-Porisini},
keywords = {Fog computing, Internet of things, Security, Privacy, Cloud computing, Fog networking},
abstract = {The incremental diffusion of the Internet of Things (IoT) technologies and applications represents the outcome of a world ever more connected by means of heterogeneous and mobile devices. IoT scenarios imply the presence of multiple data producers (e.g., sensors, actuators, RFID, NFC) and consumers (e.g., end-user devices, such as smartphones, tablets, and PCs). A variety of standards and protocols must cooperate to efficiently gather, process, and share the information. The fog computing paradigm, due to its distributed nature, represents a viable solution to cope with interoperability, scalability, security, and privacy issues, which naturally emerge, since it operates as an intermediate layer between data consumers/producers and traditional cloud systems. This paper analyzes the evolution in the modeling of new methodologies, related to fog computing and IoT, showing how moving security and privacy tasks toward the edge of the network provide both advantages and new challenges to be faced in this research field. The proposed discussion provides an overview of requirements for the realization of secure and privacy-aware IoT-based fog computing infrastructures.}
}
@article{BRACCONI2022109148,
title = {Intensification of catalytic reactors: A synergic effort of Multiscale Modeling, Machine Learning and Additive Manufacturing},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {181},
pages = {109148},
year = {2022},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2022.109148},
url = {https://www.sciencedirect.com/science/article/pii/S025527012200352X},
author = {Mauro Bracconi},
keywords = {Catalytic reactors, Process intensification, Multiscale modeling, Additive manufacturing, Machine learning},
abstract = {The intensification of catalytic reactors is expected to play a crucial role to address the challenges that the chemical industry is facing in the transition to more sustainable productions. An advanced design paradigm is necessary to develop customized and process-tailored reactor solutions able to provide the optimal operating conditions, transport properties and geometry. This can be achieved by a detailed understanding of the catalyst functionality in the reactive environment. Multiscale Modeling provides such in-depth insights into the complex physical-chemical phenomena enabling to achieve a first-principles-based understanding and design of the most suitable reactor geometry and configuration. To overcome the intrinsic complexity of the approach, Machine Learning can be synergically employed to reduce the computational cost fostering the inclusion of detailed numerical simulations since the early stage of the design process. Moreover, hybrid machine learning models trained with the data and enforced by the physics are envisioned to assist the work of designers facilitating the development of disruptive intensified solutions. The manufacturing of these unconventional systems requires adequate techniques. Additive Manufacturing is showing enormous potential in this direction and their future developments are expected to make it possible to routinely fabricate intensified reactors.}
}
@article{DONG2022300,
title = {The market effectiveness of regulatory certification for sustainable food supply: A conjoint analysis approach},
journal = {Sustainable Production and Consumption},
volume = {34},
pages = {300-309},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2022.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S2352550922002603},
author = {Xuemei Dong and Baichen Jiang},
keywords = {Sustainable food supply, Intermediary, Policy integration, Consumer decision, Heuristic-systematic model},
abstract = {The commitment to sustainability follows the consensus of adopting integrated governance to mitigate uncertainty about the whole food system. Thus, a coherent intermediary framework emerges in response to the food system crisis, pledging a “farm to fork” regulatory certification with enhanced credibility and reduced search cost compared with the fragmented intermediation. Yet, the institutional practice is scarce, and the market effectiveness remains underexamined. Drawing upon the heuristic-systematic model from the dual-process theory, this research builds the view of sustainable choice as a hierarchical decision process. The regulatory certification enacts an easy but reliable heuristic judgment, establishing an adaptive base for further systematic decisions. Specifically, common barriers (such as the status quo bias and price sensitivity) to sustainable action are not firmly established as typically assumed; instead, consumer reluctance to action arises from the lacking of a dominant heuristic. Consumers with an available regulatory certification experience less difficult trade-offs and have stronger preferences for technology innovation, such as digital production for food industry 4.0. Potentially, the segment that values regulatory certification as the crucial heuristic accounts for approximately 50%. This research provides insightful implications on how governance reforms spark a shift to sustainable food supply and the market effectiveness of inviting consumers to join the initiatives.}
}
@article{BAS2022,
title = {An interpretable machine learning approach to understanding the impacts of attitudinal and ridesourcing factors on electric vehicle adoption},
journal = {Transportation Letters},
year = {2022},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2021.2009098},
url = {https://www.sciencedirect.com/science/article/pii/S1942786722004623},
author = {Javier Bas and Zhenpeng Zou and Cinzia Cirillo},
keywords = {Electric vehicles, attitudes, ridesourcing, machine learning, local interpretable model-agnostic explanations (lime)},
abstract = {ABSTRACT
The global electric vehicle (EV) market has been experiencing an impressive growth in recent times. Understanding consumer preferences on this cleaner, more eco-friendly mobility option could help guide public policy toward accelerating EV adoption and sustainable transportation systems. Previous studies suggest the strong influence of individual and external factors on EV adoption decisions. In this study, we apply machine learning techniques on EV stated preference survey data to predict EV adoption using attitudinal factors, ridesourcing factors (e.g., frequency of Uber/Lyft rides), as well as underlying sociodemographic and vehicle factors. To overcome machine learning models’ low interpretability, we adopt the innovative Local Interpretable Model-Agnostic Explanations (LIME) method to elaborate each factor’s contribution to the predicting outcomes. Besides what was found in previous EV preference literature, we find that the frequent usage of ridesourcing, knowledge about EVs, and awareness of environmental protection are important factors in explaining high willingness of adopting EVs.}
}
@article{ZHOU2022122028,
title = {Can digital transformation alleviate corporate tax stickiness: The mediation effect of tax avoidance},
journal = {Technological Forecasting and Social Change},
volume = {184},
pages = {122028},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.122028},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522005492},
author = {Shuya Zhou and Peiyan Zhou and Hannah Ji},
keywords = {Digital transformation, Tax stickiness, Tax avoidance, Internal control, Tax enforcement, Environmental uncertainty},
abstract = {As the salient pain during the economic downturn, tax stickiness has been deeply troubling enterprises. Given the burgeoning academic interest in the role of digital transformation in corporate decision-making, we focus on its implications for tax management decisions. This study aims to test the association between digital transformation and tax stickiness and the mediating effect of tax avoidance. Theoretically, we use an information processing view to incorporate digital transformation into tax research. Empirically, we use the fixed effects model to verify our hypotheses based on the fine-grained panel data of Chinese publicly listed enterprises from 2007 to 2019. Results show that (a) digital transformation significantly reduces tax stickiness, (b) digital transformation alleviates tax stickiness by enhancing tax avoidance, and (c) the alleviating effect of digital transformation on tax stickiness is more prominent in companies with weak internal control and regions with low tax enforcement and high environmental uncertainty. Our findings hold after a range of robustness tests. Our results confirm the importance of digital transformation for tax management and provide new insights for enterprises to reduce tax stickiness through digital transformation. Our study also has important practical implications for business managers and policymakers.}
}
@article{HUANG2022107813,
title = {Cross-knowledge-graph entity alignment via relation prediction},
journal = {Knowledge-Based Systems},
volume = {240},
pages = {107813},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107813},
url = {https://www.sciencedirect.com/science/article/pii/S095070512101011X},
author = {Hongren Huang and Chen Li and Xutan Peng and Lifang He and Shu Guo and Hao Peng and Lihong Wang and Jianxin Li},
keywords = {Knowledge alignment, Anchor relation, Self-training, Data augmentation, Relation prediction},
abstract = {The entity alignment task aims to align entities corresponding to the same object in different KGs. The recent work focuses on applying knowledge embedding or graph neural networks to obtain entity embedding for entity alignment. However, there are two challenges encountered by these models: one is some models need to design hyper-parameter to balance embedding loss and alignment loss, the other is the limited training data size. In this paper, we propose a novel entity alignment framework named RpAlign (Relation prediction based cross-knowledge-graph entity Alignment) to address these two issues. Specifically, RpAlign transforms the entity alignment task to the KG completion task to solve and does not need to design any extra alignment component. Unlike the existing models that predict aligned entities by using entity vector distance, the RpAlign defines a new relation called ‘anchor’ for aligned entities, and it predicts new aligned entities based on the relational predictions between the entities. RpAlign employs several data augmentation and improved self-training techniques to mitigate the impact of the data limitation. We conduct experiments on two datasets, and the experimental results show that the RpAlign model significantly outperforms the current state-of-the-art models.}
}
@article{WANG202268,
title = {Evaluation of survey and remote sensing data products used to estimate land use change in the United States: Evolving issues and emerging opportunities},
journal = {Environmental Science & Policy},
volume = {129},
pages = {68-78},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121003762},
author = {Minzi Wang and Michelle Wander and Steffen Mueller and Nico Martin and Jennifer B. Dunn},
keywords = {Land use change, Survey data, Thematic maps, Remote sensing, Land use classification, Error, Uncertainty, Biofuel policy},
abstract = {Transparent, consistent, and statistically reliable land use/ land cover area estimates are needed to assess land use change and greenhouse gas emissions associated with biofuel production and other land uses that are influenced by policy. As relevant studies have increased rapidly during past decades, the methods used to combine data extracted from land use land cover (LULC) surveys and remote sensing-based products and track or report sources of uncertainty vary notably. This paper reviews six data sources that are most commonly used to investigate LULC and change in the contiguous U.S. by highlighting the main characteristics, strengths and weaknesses and considering how uncertainty is assessed by the June Area Survey (JAS), the Census of Agriculture (COA), the Farm Survey Agency (FSA) acreage, the National Resources Inventory (NRI), the National Wetlands Inventory (NWI), and the Forest Inventory and Analysis (FIA); and two remote sensing-based data products, the Cropland Data Layer (CDL) and the National Land Cover Database (NLCD). The summary and conclusion identify important research gaps or challenges limiting current land use/land cover and change studies (e.g., lack of high-quality reference data and uncertainty quantification, etc.) and opportunities and emerging techniques (data fusion and machine learning) that will improve reliability of land use/land cover assessments and associated policies. Blended approaches that marry high quality ground truth data that are more finely resolved than data supplied by government surveys with multitemporal imagery are needed track use of non-agricultural lands vulnerable to agricultural expansion. These considerations are notably important as the U.S. considers the renewal and possibly revision of its Renewable Fuel Standard, which includes provisions that require monitoring of agricultural land expansion.}
}
@article{RICHTER2022112459,
title = {Artificial Intelligence for Electricity Supply Chain automation},
journal = {Renewable and Sustainable Energy Reviews},
volume = {163},
pages = {112459},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112459},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122003653},
author = {Lucas Richter and Malte Lehna and Sophie Marchand and Christoph Scholz and Alexander Dreher and Stefan Klaiber and Steve Lenk},
keywords = {Electricity supply chain, Energy management, Energy transition, Artificial intelligence automation data processing forecasting optimization autonomous trading.interaction},
abstract = {The Electricity Supply Chain is a system of enabling procedures to optimize processes ranging from production to transportation and consumption of electricity. The proportion of distributed energy sources within the electricity system increases steadily, which necessitates an improved monitoring capability to ensure the overall reliability and quality of the Electricity Supply Chain. Automation is strongly required to process the growing amount of data. Thus, it is inevitable to handle large amounts of heterogeneous data and process the information using forecasting and optimization techniques. Artificial Intelligence techniques are crucial for extending human cognitive abilities in these tasks. In our work, we synthesize the main impacts of the Artificial Intelligence paradigm on the automation of the Electricity Supply Chain. We describe the emerging automation through Artificial Intelligence in every layer of the Smart Grid Architecture Model and highlight state-of-the-art approaches. In the review, we focus on the following Electricity Supply Chain functionalities: generation, maintenance, pre-processing, analysis, forecasting, optimization, and trading within energy systems. After investigating the individual perspectives, we examine the potential implementation of a fully automated Electricity Supply Chain. Lastly, we discuss perspectives and limitations for the transformation from conventional to automated Electricity Supply Chains, specifically in terms of human interaction, Artificial Intelligence adaptation, energy transition, and sustainability.}
}
@article{CHEN2022108046,
title = {Multi-sourced sensing and support vector machine classification for effective detection of fire hazard in early stage},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {108046},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108046},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200307X},
author = {Siyuan Chen and Jinchang Ren and Yijun Yan and Meijun Sun and Fuyuan Hu and Huimin Zhao},
keywords = {Fire incident detection, Sensor fusion, Machine learning, Alarm systems, Fire safety},
abstract = {Accurate detection and early warning of fire hazard are crucial for reducing the associated damages. Due to the limitations of smoke-based detection mechanism, most commercial detectors fail to distinguish the smoke from dust and steam, leading to frequent false alarms and costly evacuation unnecessarily. To tackle this issue, we propose a fast and cost-effective indoor fire alarm system for real-time early fire detection under various scenarios, whilst significantly reducing the false alarms. Multimodal sensors are integrated to acquire the data of carbon monoxide, smoke, temperature and humidity, followed by effective data analysis and classification. For ease of embedded implementation, the support vector machine (SVM) is found to outperform the Random Forests (RF), K-means, and Artificial Neural Networks (ANN). On a public dataset and our own dataset, the proposed system performs promising, with the values of the precision, recall, and F1 of 99.8%, 99.6%, and 99.7%, respectively.}
}
@article{AHMED20226505,
title = {Arabic Knowledge Graph Construction: A close look in the present and into the future},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {6505-6523},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822001240},
author = {Ibrahim A. Ahmed and Fatima N. AL-Aswadi and Khaled M.G. Noaman and Wafa' Za'al Alma'aitah},
keywords = {Arabic Knowledge Graph, Knowledge Graph Construction, Knowledge Representation, Ontology},
abstract = {With the widespread growth of data on the Web, the need for efficient methods to get and arrange valuable information from these big noisy data is increased. The knowledge graph (KG) is a way to represent and organize the data in a more efficient and easy way to modify, use, and understand. Recently, KG has become a new hotspot topic in academic and business research; it is used in many applications such as intelligent question-answering (QA), recommender systems, map navigation, etc. There has been a tendency to construct KG in different languages such as English, Chinese, Persian, or Arabic. Constructing KG faces many challenges and obstacles, especially constructing Arabic Knowledge Graph (AKG) due to the sparse Arabic data in online encyclopedias and academic research, as well as the lack of tools that can process the proprietary nature of the Arabic language effectively, besides other challenges. This research aims to review and discuss KG construction best practices (systems, phases, problems, and challenges) with highlighting the Arabic perspective. Besides, it elaborates a classification of the AKG challenges and investigates the potential solutions and opportunities that might define the key future research directions of constructing AKG.}
}
@article{YANG2022100061,
title = {A survey on multisource heterogeneous urban sensor access and data management technologies},
journal = {Measurement: Sensors},
volume = {19},
pages = {100061},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100061},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421000246},
author = {Fei Yang and Yixin Hua and Xiang Li and Zhenkai Yang and Xinkai Yu and Teng Fei},
keywords = {Smart city, Urban sensor, Access, Data management, Computational burden, Energy consumption, Cybersecurity},
abstract = {Urban sensors are an important part of urban infrastructures and are usually heterogeneous. Urban sensors with different uses vary greatly in hardware structure, communication protocols, data formats, interaction modes, sampling frequencies, data accuracy and service quality, thus posing an enormous challenge to the unified integration and sharing of massive sensor information resources. Consequently, access and data management methods for these multisource heterogeneous urban sensors are extremely important. Additionally, multisource heterogeneous urban sensor access and data management technologies provide strong support for intelligent perception and scientific management at the city scale and can accelerate the construction of smart cities or digital twin cities with virtual reality features. We systematically summarize the related research on these technologies. First, we present a summary of the concepts and applications of urban sensors. Then, the research progress on multisource heterogeneous urban sensor access technologies is analysed in relation to communication protocols, data transmission formats, access standards, access technologies and data transmission technologies. Subsequently, the data management technologies for urban sensors are reviewed from the perspectives of data cleaning, data compression, data storage, data indexing and data querying. In addition, the challenges faced by the technologies above and corresponding feasible solutions are discussed from three aspects, namely, the integration of massive Internet of Things (IoT), computational burden and energy consumption and cybersecurity. Finally, a summary of this paper is given, and possible future development directions are analysed and discussed.}
}
@article{LI2022108487,
title = {A perspective survey on deep transfer learning for fault diagnosis in industrial scenarios: Theories, applications and challenges},
journal = {Mechanical Systems and Signal Processing},
volume = {167},
pages = {108487},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108487},
url = {https://www.sciencedirect.com/science/article/pii/S088832702100830X},
author = {Weihua Li and Ruyi Huang and Jipu Li and Yixiao Liao and Zhuyun Chen and Guolin He and Ruqiang Yan and Konstantinos Gryllias},
keywords = {Fault diagnosis, Deep learning, Transfer learning, Domain adaptation, Deep transfer learning},
abstract = {Deep Transfer Learning (DTL) is a new paradigm of machine learning, which can not only leverage the advantages of Deep Learning (DL) in feature representation, but also benefit from the superiority of Transfer Learning (TL) in knowledge transfer. As a result, DTL techniques can make DL-based fault diagnosis methods more reliable, robust and applicable, and they have been widely developed and investigated in the field of Intelligent Fault Diagnosis (IFD). Although several systematic and valuable review articles have been published on the topic of IFD, they summarized relevant research only from an algorithm perspective and overlooked practical applications in industry scenarios. Furthermore, a comprehensive review on DTL-based IFD methods is still lacking. From this insight, it is particularly important and more necessary to comprehensively survey the relevant publications of DTL-based IFD, which will help readers to conveniently understand the current state-of-the-art techniques and to quickly design an effective solution for solving IFD problems in practice. First, theoretical backgrounds of DTL are briefly introduced to present how the transfer learning techniques can be integrated with deep learning models. Then, major applications of DTL and their recent developments in the field of IFD are detailed and discussed. More importantly, suggestions on how to select DTL algorithms in practical applications, and some future challenges are shared. Finally, conclusions of this survey are given. At last, we have reason to believe that the works done in this article can provide convenience and inspiration for the researchers who want to devote their efforts in the progress and advance of IFD.}
}
@article{BERTL2022117464,
title = {A survey on AI and decision support systems in psychiatry – Uncovering a dilemma},
journal = {Expert Systems with Applications},
volume = {202},
pages = {117464},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117464},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422007965},
author = {Markus Bertl and Peeter Ross and Dirk Draheim},
keywords = {Medical information policy, Medical technology, Digital decision support system (DDSS), Clinical Decision Support Systems (CDSS), Artificial Intelligence (AI), Machine Learning (ML), Psychiatry},
abstract = {Every year, healthcare specialists collect more and more data about patients but struggle to use it to optimize disease prevention, diagnosis, or treatment processes. While a manual use of this medical data is virtually impossible considering the vast growth rate, automation with artificial intelligence (AI) and digital decision support systems (DDSSs) has still not yielded any large-scale success in healthcare. We aim to investigate possible obstacles, the trustworthiness based on potential biases, and the adoption of new technology by AI and DDSSs in psychiatry based on a systematic literature review. We screened 520 papers about AI or DDSSs in psychiatry. We added results from a literature screening of 65 articles about AI or DDSSs for post-traumatic stress disorder as one specific psychiatric disease to our research, given that literature possibly deviates from general decision support systems for psychiatry. Out of 80 articles, we extract algorithms, data collection method and sample size of the used training data, and testing process including accuracy metrics. The results show that sample sizes are small (median of 151.5), a focus on algorithm development without real-world interaction, and methodological shortcomings when it comes to the evaluation of DDSSs. Our survey concludes that DDSSs in psychiatry are not ready for the often-promised “AI revolution in healthcare”.}
}
@article{LI2022103882,
title = {Systematic review on tool breakage monitoring techniques in machining operations},
journal = {International Journal of Machine Tools and Manufacture},
volume = {176},
pages = {103882},
year = {2022},
issn = {0890-6955},
doi = {https://doi.org/10.1016/j.ijmachtools.2022.103882},
url = {https://www.sciencedirect.com/science/article/pii/S0890695522000335},
author = {Xuebing Li and Xianli Liu and Caixu Yue and Steven Y. Liang and Lihui Wang},
keywords = {Cutting tools, Tool breakage monitoring, Signal acquisition, Feature extraction, Intelligent decision-making, Imbalanced data},
abstract = {Tool condition monitoring (TCM) in machining operations is crucial to maximise the useful tool life while reducing the risks associated with tool breakage. Unlike progressive tool wear, tool breakage occurs randomly, with more severe implications for workpiece quality, machining system stiffness, and even operator safety. Existing literature reviews on TCM focus on tool wear monitoring, including wear state recognition and remaining useful life prediction. However, a comprehensive review of tool breakage monitoring (TBM) techniques is lacking. Generic signal processing and intelligent decision-making methods cannot fully satisfy the practical requirements of the TBM. In addition, developing and evaluating TBM models using imbalanced data is more challenging. Herein, we present the first systematic review on TBM to bridge these limitations, and provide adequate guidance for avoiding catastrophic tool failures during cutting processes. Signal acquisition, feature extraction, and decision-making methodologies for the TBM are outlined and compared with related techniques for tool wear monitoring. The effects of data imbalance on TBM models are considered, and feasible solutions are provided at the data and algorithm levels. Finally, the challenges faced by the TBM are discussed, and potential research directions are suggested. The research and application of TBM techniques will certainly better empower various machining operations in response to intelligent manufacturing demands.}
}
@article{GE20221318,
title = {“Beyond MELD” – Emerging strategies and technologies for improving mortality prediction, organ allocation and outcomes in liver transplantation},
journal = {Journal of Hepatology},
volume = {76},
number = {6},
pages = {1318-1329},
year = {2022},
note = {Breakthroughs in Hepatology},
issn = {0168-8278},
doi = {https://doi.org/10.1016/j.jhep.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0168827822001416},
author = {Jin Ge and W. Ray Kim and Jennifer C. Lai and Allison J. Kwong},
keywords = {MELD, Prognostication, Allocation, Frailty, Sarcopenia, EHR, OMOP, Clinical Decision Support},
abstract = {Summary
In this review article, we discuss the model for end-stage liver disease (MELD) score and its dual purpose in general and transplant hepatology. As the landscape of liver disease and transplantation has evolved considerably since the advent of the MELD score, we summarise emerging concepts, methodologies, and technologies that may improve mortality prognostication in the future. Finally, we explore how these novel concepts and technologies may be incorporated into clinical practice.}
}
@article{ARMENIA2022102936,
title = {Anticipating human resilience and vulnerability on the path to 2030: What can we learn from COVID-19?},
journal = {Futures},
volume = {139},
pages = {102936},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102936},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000362},
author = {Stefano Armenia and Steven Arquitt and Matteo Pedercini and Alessandro Pompei},
keywords = {Misperception of feedback and delays, Behavioural pattern awareness, Systems thinking, system dynamics, COVID-19, Climate change},
abstract = {The COVID-19 pandemic is causing unprecedented damage to our society and economy, globally impacting progress towards the SDGs. The integrated perspective that Agenda 2030 calls for is ever more important for understanding the vulnerability of our eco-socio-economic systems and for designing policies for enhanced resilience. Since the emergence of COVID-19, countries and international institutions have strengthened their monitoring systems to produce timely data on infections, fostering data-driven decision-making often without the support of systemic-based simulation models. Evidence from the initial phases of the pandemic indicates that countries that were able to implement effective policies before the number of cases grew large (e.g. Australia) managed to contain COVID-19 to a much greater extent than others. We argue that prior systemic knowledge of a phenomenon provides the essential information to correctly interpret data, develop a better understanding of the emerging behavioural patterns and potentially develop early qualitative awareness of how to react promptly in the early phases of destructive phenomena, eventually providing the ground for building more effective simulation models capable of better anticipating the effects of policies. This is even more important as, on its path to 2030, humanity will face other challenges of similar dynamic nature. Chief among these is Climate Change. In this paper, we show how a Systems Thinking and System Dynamics modelling approach is useful for developing a better understanding of these and other issues, and how systemic lessons learned from the COVID-19 case can help decision makers anticipate the destructive dynamics of Climate Change by improving perceptions of the potential impacts of reinforcing feedback and delays, ultimately leading to more timely interventions to achieve the SDGs and mitigate Climate Change risks.}
}
@article{ZHANG2022124919,
title = {Data augmentation for improving heating load prediction of heating substation based on TimeGAN},
journal = {Energy},
volume = {260},
pages = {124919},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124919},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222018205},
author = {Yunfei Zhang and Zhihua Zhou and Junwei Liu and Jianjuan Yuan},
keywords = {Heating load, Prediction model, High-quality data, TimeGAN},
abstract = {Heating load predictions serve as one of the fundamental tasks in heating operation management. Many studies have used data-driven methods to build prediction models, and the quantity and quality of training data are key factors affecting the model performance. However, for some special cases, such as new heating substation and the end period of heating with different load characteristics, sufficient and high-quality data cannot be provided for model training, resulting in low accuracy of the model. In this paper, TimeGAN is applied in the heating field for the first time to augment the data and improve the prediction accuracy of the model. Results show that the prediction error reduces by 50% and CV-RMSE can reach 0.0405 after using TimeGAN in the early period of heating, and the accuracy is highest when the synthetic data are three times of original data. For the mid and end period of heating, the prediction errors can also be reduced by 3%–8% compared with training on original data, and the data amount reaches 15,000–30000, the performance of the model reaches the best.}
}
@article{ZIMMERMANN202252,
title = {Identifying Sales-Influencing Touchpoints along the Omnichannel Customer Journey},
journal = {Procedia Computer Science},
volume = {196},
pages = {52-60},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.11.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022110},
author = {Robert Zimmermann and Wolfgang Weitzl and Andreas Auinger},
keywords = {Omnichannel, Touchpoints, Customer Journey, Sales},
abstract = {Retailers have started to integrate their online and offline channels in order to increase revenue by creating a superior customer experience. However, they lack an instrument with which to identify the touchpoints that are most influential for customer decision making. Therefore, this study introduces a novel, multi-method approach that utilizes combined data-collection and data-analyses procedures that help retailers to identify and meaningfully cluster relevant touchpoints along the customer journey. Results indicate, among others, that retailers can benefit from abandoning the classic, within-company perspective and cluster their touchpoints according to the customers’ perspective. Furthermore, our approach enables retailers to infer the most important sales-influencing touchpoints. Here, findings indicate that retailers should be selective in providing the right touchpoints for their customers, as some of them can have a direct or indirect negative impact on sales. Retailers can use these insights to support their touchpoint-selection and thus decision-making process through thought provoking impulses.}
}
@article{WANG2022,
title = {Methodology of network pharmacology for research on Chinese herbal medicine against COVID-19: A review},
journal = {Journal of Integrative Medicine},
year = {2022},
issn = {2095-4964},
doi = {https://doi.org/10.1016/j.joim.2022.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095496422000966},
author = {Yi-xuan Wang and Zhen Yang and Wen-xiao Wang and Yu-xi Huang and Qiao Zhang and Jia-jia Li and Yu-ping Tang and Shi-jun Yue},
keywords = {Chinese traditional medicine, Herbal medicine, Network pharmacology, Compound identification, COVID-19},
abstract = {Traditional Chinese medicine, as a complementary and alternative medicine, has been practiced for thousands of years in China and possesses remarkable clinical efficacy. Thus, systematic analysis and examination of the mechanistic links between Chinese herbal medicine (CHM) and the complex human body can benefit contemporary understandings by carrying out qualitative and quantitative analysis. With increasing attention, the approach of network pharmacology has begun to unveil the mystery of CHM by constructing the heterogeneous network relationship of “herb-compound-target-pathway,” which corresponds to the holistic mechanisms of CHM. By integrating computational techniques into network pharmacology, the efficiency and accuracy of active compound screening and target fishing have been improved at an unprecedented pace. This review dissects the core innovations to the network pharmacology approach that were developed in the years since 2015 and highlights how this tool has been applied to understanding the coronavirus disease 2019 and refining the clinical use of CHM to combat it.}
}
@article{LOPEZGUAJARDO2022108671,
title = {Process intensification 4.0: A new approach for attaining new, sustainable and circular processes enabled by machine learning},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {180},
pages = {108671},
year = {2022},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2021.108671},
url = {https://www.sciencedirect.com/science/article/pii/S0255270121003597},
author = {Enrique A. López-Guajardo and Fernando Delgado-Licona and Alejandro J. Álvarez and Krishna D.P. Nigam and Alejandro Montesinos-Castellanos and Ruben Morales-Menendez},
keywords = {Process intensification, Industry 4.0, Process intensification 4.0, Machine learning, Circular Chemistry, Artificial Intelligence},
abstract = {This paper reviews system-level transformations converging into the next generation of Process Intensification strategies defined as PI4.0. Process Intensification 4.0 uses data-driven algorithms to understand other physical and chemical processes that improve equipment design, predictive control, and optimization. Following this, an overview of the use of Artificial Intelligence techniques, particularly Machine Learning for the acceleration of equipment design, process optimization, and streamlining, is presented. This work will highlight and discuss the emerging framework of the integration between Circular Chemistry, Industry 4.0, and Process Intensification and how the data obtained from this integration is at the core of the next generation of Process Intensification strategies. This is supported by a discussion of different cases that apply data-driven models enabled by Machine Learning as a mean to enhance an intensified system (product synthesis, equipment or methods).}
}
@article{ZHU2022107,
title = {A review of the application of machine learning in water quality evaluation},
journal = {Eco-Environment & Health},
volume = {1},
number = {2},
pages = {107-116},
year = {2022},
issn = {2772-9850},
doi = {https://doi.org/10.1016/j.eehl.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2772985022000163},
author = {Mengyuan Zhu and Jiawei Wang and Xiao Yang and Yu Zhang and Linyu Zhang and Hongqiang Ren and Bing Wu and Lin Ye},
keywords = {Machine learning, Water quality, Evaluation, Prediction},
abstract = {With the rapid increase in the volume of data on the aquatic environment, machine learning has become an important tool for data analysis, classification, and prediction. Unlike traditional models used in water-related research, data-driven models based on machine learning can efficiently solve more complex nonlinear problems. In water environment research, models and conclusions derived from machine learning have been applied to the construction, monitoring, simulation, evaluation, and optimization of various water treatment and management systems. Additionally, machine learning can provide solutions for water pollution control, water quality improvement, and watershed ecosystem security management. In this review, we describe the cases in which machine learning algorithms have been applied to evaluate the water quality in different water environments, such as surface water, groundwater, drinking water, sewage, and seawater. Furthermore, we propose possible future applications of machine learning approaches to water environments.}
}
@article{MUNOZGAMA2022103994,
title = {Process mining for healthcare: Characteristics and challenges},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {103994},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.103994},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000107},
author = {Jorge Munoz-Gama and Niels Martin and Carlos Fernandez-Llatas and Owen A. Johnson and Marcos Sepúlveda and Emmanuel Helm and Victor Galvez-Yanjari and Eric Rojas and Antonio Martinez-Millana and Davide Aloini and Ilaria Angela Amantea and Robert Andrews and Michael Arias and Iris Beerepoot and Elisabetta Benevento and Andrea Burattin and Daniel Capurro and Josep Carmona and Marco Comuzzi and Benjamin Dalmas and Rene {de la Fuente} and Chiara {Di Francescomarino} and Claudio {Di Ciccio} and Roberto Gatta and Chiara Ghidini and Fernanda Gonzalez-Lopez and Gema Ibanez-Sanchez and Hilda B. Klasky and Angelina {Prima Kurniati} and Xixi Lu and Felix Mannhardt and Ronny Mans and Mar Marcos and Renata {Medeiros de Carvalho} and Marco Pegoraro and Simon K. Poon and Luise Pufahl and Hajo A. Reijers and Simon Remy and Stefanie Rinderle-Ma and Lucia Sacchi and Fernando Seoane and Minseok Song and Alessandro Stefanini and Emilio Sulis and Arthur H.M. {ter Hofstede} and Pieter J. Toussaint and Vicente Traver and Zoe Valero-Ramon and Inge van de Weerd and Wil M.P. {van der Aalst} and Rob Vanwersch and Mathias Weske and Moe Thandar Wynn and Francesca Zerbato},
keywords = {Process mining, Healthcare},
abstract = {Process mining techniques can be used to analyse business processes using the data logged during their execution. These techniques are leveraged in a wide range of domains, including healthcare, where it focuses mainly on the analysis of diagnostic, treatment, and organisational processes. Despite the huge amount of data generated in hospitals by staff and machinery involved in healthcare processes, there is no evidence of a systematic uptake of process mining beyond targeted case studies in a research context. When developing and using process mining in healthcare, distinguishing characteristics of healthcare processes such as their variability and patient-centred focus require targeted attention. Against this background, the Process-Oriented Data Science in Healthcare Alliance has been established to propagate the research and application of techniques targeting the data-driven improvement of healthcare processes. This paper, an initiative of the alliance, presents the distinguishing characteristics of the healthcare domain that need to be considered to successfully use process mining, as well as open challenges that need to be addressed by the community in the future.}
}
@article{ZHANG2022247,
title = {DeepBindBC: A practical deep learning method for identifying native-like protein-ligand complexes in virtual screening},
journal = {Methods},
volume = {205},
pages = {247-262},
year = {2022},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2022.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1046202322001633},
author = {Haiping Zhang and Tingting Zhang and Konda Mani Saravanan and Linbu Liao and Hao Wu and Haishan Zhang and Huiling Zhang and Yi Pan and Xuli Wu and Yanjie Wei},
keywords = {Native like protein-ligand, Drug virtual screening, ResNet, Deep learning, Human pancreatic alpha amylase inhibitor},
abstract = {Identifying native-like protein–ligand complexes (PLCs) from an abundance of docking decoys is critical for large-scale virtual drug screening in early-stage drug discovery lead searching efforts. Providing reliable prediction is still a challenge for most current affinity predicting models because of a lack of non-binding data during model training, lost critical physical–chemical features, and difficulties in learning abstract information with limited neural layers. In this work, we proposed a deep learning model, DeepBindBC, for classifying putative ligands as binding or non-binding. Our model incorporates information on non-binding interactions, making it more suitable for real applications. ResNet model architecture and more detailed atom type representation guarantee implicit features can be learned more accurately. Here, we show that DeepBindBC outperforms Autodock Vina, Pafnucy, and DLSCORE for three DUD.E testing sets. Moreover, DeepBindBC identified a novel human pancreatic α-amylase binder validated by a fluorescence spectral experiment (Ka = 1.0 × 105 M). Furthermore, DeepBindBC can be used as a core component of a hybrid virtual screening pipeline that incorporating many other complementary methods, such as DFCNN, Autodock Vina docking, and pocket molecular dynamics simulation. Additionally, an online web server based on the model is available at http://cbblab.siat.ac.cn/DeepBindBC/index.php for the user’s convenience. Our model and the web server provide alternative tools in the early steps of drug discovery by providing accurate identification of native-like PLCs.}
}
@article{AZHIN202279,
title = {Application of Multivariable Data Analysis in Mineral Processing},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {21},
pages = {79-84},
year = {2022},
note = {19th IFAC Symposium on Control, Optimization and Automation in Mining, Mineral and Metal Processing MMM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.247},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014793},
author = {Maryam Azhin and Robert Lopetinsky and John Stiksma and Faraz Amjad and Bardia Hassanzadeh and Siddhartha Tirumalaraju and Chowdary Meenavilli},
keywords = {Artificial Intelligence, Soft Sensor, Machine Learning, Hydrometalurgy, Metallurgy, Process Monitoring, Copper Boil},
abstract = {Data analysis and application of machine learning (ML) have demonstrated successful performance in various data rich industrial applications. Mineral processing and metallurgical operations are considered suitable for implementation of novel ML-based algorithms. The key operating performance and product outputs are usually obtained from the lab measurements and analyses that can be expensive, complex, and time consuming. Therefore, the development and application of a soft sensor and/or a state observer is a useful option to be considered due to their ability to provide the distribution of desired outputs in a continuous manner. In addition, the motivation to apply a soft sensor (a data-based model) is to provide guidance and/or information feedback to the operator in charge of making operational decisions. The soft sensor was developed at Sherritt's Metal Plant in Fort Saskatchewan as a nonlinear neural network model and it was based on two years of plant historical data. The model was also validated based on historical data, live testing, and additional sampling of process streams during simultaneous sampling campaign.}
}
@article{TOP2022106909,
title = {Cultivating FAIR principles for agri-food data},
journal = {Computers and Electronics in Agriculture},
volume = {196},
pages = {106909},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106909},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922002265},
author = {Jan Top and Sander Janssen and Hendrik Boogaard and Rob Knapen and Görkem Şimşek-Şenel},
keywords = {Agriculture, Food supply chain, Data sharing, FAIR principles, Ontology, Controlled vocabulary},
abstract = {Data generated by the global food system is crucial in the transformation towards sustainable, resilient, and high-quality food production. Although the amount of potentially useful data is growing rapidly, its (re)use is still limited. The FAIR-principles have been developed for making data findable, accessible, interoperable, and reusable both by humans and machines. This paper explores the further operationalization of the FAIR principles in agriculture and food. Experience shows that several conditions must be fulfilled before data can be effectively shared and reused. First, automated tools must be available for data providers and users. Secondly, we need a community-based approach in developing tools and vocabularies. Thirdly, data cannot be shared by an open-by-default policy only. Finally, scientific insight is needed in how data is actually (re)used in scientific communities. We conclude that bringing the FAIR-principles to full maturity requires a fair balance of efforts within the agri-food communities, supported by a proper infrastructure.}
}
@article{LI2022105720,
title = {Imbalanced nitrogen–phosphorus input alters soil organic carbon storage and mineralisation in a salt marsh},
journal = {CATENA},
volume = {208},
pages = {105720},
year = {2022},
issn = {0341-8162},
doi = {https://doi.org/10.1016/j.catena.2021.105720},
url = {https://www.sciencedirect.com/science/article/pii/S0341816221005786},
author = {Juanyong Li and Guangxuan Han and Guangmei Wang and Xiaoling Liu and Qiqi Zhang and Yawen Chen and Weimin Song and Wendi Qu and Xiaojing Chu and Peiguang Li},
keywords = {Imbalanced N and P input, SOC cycling, Microbial community structure, Salt marsh},
abstract = {A large imbalance in soil nitrogen (N) and phosphorus (P) inputs induced by anthropogenic activities is anticipated to profoundly influence soil carbon (C) budgets in salt marshes. In this study, we hypothesized that imbalances in the nitrogen–phosphorus (N–P) input would result in the nonlinear response of soil organic carbon (SOC) content, fractions and mineralization to the N–P input ratio. We applied three N–P input ratios (low (5:1), medium (15:1), high (45:1)) in a salt marsh of the Yellow River Delta (YRD) for four years (in which N added increased from 8.67 to 26.01 g N m−2 y−1 and P added decreased from 1.73 to 0.58 g P m−2 y−1) and quantified their impacts on SOC fractions and SOC mineralisation. The control treatment did not receive fertilization. The results showed that the N and P input led to overall increases in the availability of soil nutrients (i.e., inorganic N (IN) and available P (AP)), stimulation of plant biomass and changes of microbial community structure (i.e., γ- and δ-Proteobacteria and Acidobacteria). N and P input increased soil dissolved organic carbon (DOC) and decreased aromatic DOC components through improving N availability and stimulating plant growth. Notably, though, there may be a threshold N–P input ratio between 15:1 and 45:1 that, once crossed, triggers the loss of SOC. Appropriate increase in N availability induced by low and medium N-P input ratios would stimulate the SOC mineralization. However, excessive N-P input ratio would reduce SOC mineralization. Path analysis indicated that N–P input ratios dominantly regulate SOC mineralisation by changing soil DOC and microbial biomass (MBC)contents and microbial community structure. Thus, we speculate that the continuous increase in N input causes a growing N–P imbalance that reduces SOC stocks, despite a reduction in SOC mineralisation.}
}
@article{SALIM2022102786,
title = {Data analytics of social media 3.0: Privacy protection perspectives for integrating social media and Internet of Things (SM-IoT) systems},
journal = {Ad Hoc Networks},
volume = {128},
pages = {102786},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522000051},
author = {Sara Salim and Benjamin Turnbull and Nour Moustafa},
keywords = {Social media, Internet of Things (ioT), Data analytics, Privacy preservation},
abstract = {With the rapid evolution of web technologies, Web 3.0 aims to expand on current and emerging social media platforms such as Facebook, Twitter, and TikTok, and integrate emerging computing paradigms, including the Internet of Things (IoT), named social media 3.0. The combinations of these platforms in Web 3.0 promises consumers greater integration, interaction, and more seamless movement between physical spaces. However, ensuring the privacy of data across such systems is a potential challenge in this space. In this study, we propose a new privacy-preserving social media 3.0 framework that illustrates the interaction of SM and IoT services and estimates how this interaction could impact users’ behaviors. The framework consists of three main components. First, a new relational dataset, named SM-IoT, is designed to dynamically connect users with their IoT services and assist in processing data heterogeneity. Second, a data pre-processing module is employed for filtering heterogeneous data and providing a certain level of privacy preservation on the data. Third, data analytics using different statistical and machine/deep learning methods are applied to examine data complexity and identify users’ behaviors. The results revealed that our proposed framework can efficiently identify users’ behaviors from social media 3.0 data sources. The outcomes of comparing our SM-IoT dataset with two other well-known SM datasets, namely Pokec and Renren, as well as Activity Recognition with Ambient Sensing (ARAS) IoT dataset reveals the fidelity of our dataset to be used for future evaluations of privacy-preserving and machine learning-based decision-making techniques.}
}
@article{MIASAYEDAVA2022114283,
title = {Automated environmental compliance monitoring of rivers with IoT and open government data},
journal = {Journal of Environmental Management},
volume = {303},
pages = {114283},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.114283},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721023458},
author = {Lizaveta Miasayedava and Keegan McBride and Jeffrey Andrew Tuhtan},
keywords = {Environmental compliance monitoring, Environmental flows, Internet of things, Open government data},
abstract = {Environmental monitoring of rivers is a cornerstone of the European Union's Water Framework Directive. It requires the estimation and reporting of environmental flows in rivers whose characteristics vary widely across the EU member states. This variability has resulted in a fragmentation of estimation and reporting methods for environmental flows and is exhibited by the myriad of regulatory guidelines and estimation procedures. To standardise and systematically evaluate environmental flows at the pan-European scale, we propose to formalise the estimation procedures through automation by reusing existing river monitoring resources. In this work, we explore how sensor-generated hydrological open government data can be repurposed to automate the estimation and monitoring of river environmental flows. In contrast to existing environmental flows estimation methods, we propose a scalable IoT-based architecture and implement its cloud-layer web service. The major contribution of this work is the demonstration of an automated environmental flows system based on open river monitoring data routinely collected by national authorities. Moreover, the proposed system adds value to existing environmental monitoring data, reduces development and operational costs, facilitates streamlining of environmental compliance and allows for any authority with similar data to reuse or scale it with new data and methods. We critically discuss the opportunities and challenges associated with open government data, including its quality. Finally, we demonstrate the proposed system using the Estonian national river monitoring network and define further research directions.}
}
@article{GAO2022185,
title = {What can we learn from telematics car driving data: A survey},
journal = {Insurance: Mathematics and Economics},
volume = {104},
pages = {185-199},
year = {2022},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167668722000233},
author = {Guangyuan Gao and Shengwang Meng and Mario V. Wüthrich},
keywords = {Telematics car driving data, Heatmaps, Poisson regression models, Convolutional neural networks, Limited fluctuation credibility model},
abstract = {We give a survey on the field of telematics car driving data research in actuarial science. We describe and discuss telematics car driving data, we illustrate the difficulties of telematics data cleaning, and we highlight the transparency issue of telematics car driving data resulting in associated privacy concerns. Transparency of telematics data is demonstrated by aiming at correctly allocating different car driving trips to the right drivers. This is achieved rather successfully by a convolutional neural network that manages to discriminate different car drivers by their driving styles. In a last step, we describe two approaches of using telematics data for improving claims frequency prediction, one is based on telematics heatmaps and the other one on time series of individual trips, respectively.}
}
@article{YANG2022109092,
title = {A noise-aware fuzzy rough set approach for feature selection},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {109092},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109092},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200538X},
author = {Xiaoling Yang and Hongmei Chen and Tianrui Li and Chuan Luo},
keywords = {Fuzzy rough set, Robust feature selection, Noisy data, Density distribution, Dependency function},
abstract = {Feature selection has aroused extensive attention and aims at selecting features that are highly relevant to classification from raw datasets to improve the performance of a learning model. Fuzzy rough set theory is a powerful mathematical method for feature selection. The classical fuzzy rough set model is very sensitive to the noise while the noise samples in classification data often appear. In addition, fuzzy rough set theory does not fit well when the density distribution of the samples in the dataset varies greatly. Thus, it is of great significance to improve the robustness of fuzzy rough set models and its adaptability to data for feature selection. Inspired by these issues, we focus on the robust fuzzy rough set approach for feature selection. We first propose a robust fuzzy rough set model based on data distribution to achieve the purpose of anti-noise i.e., Noise-aware Fuzzy Rough Sets (NFRS) model. This model proposes a novel search mechanism, which weakens the sensitivity of the approximation operator to noise by considering the distribution of samples in the decision classes to weight the samples, further obtains three kinds of samples, i.e., intra-class samples, boundary samples, and outlier samples. Then, the degrees of relevance of the feature for class is defined by the dependency function based on the NFRS model to evaluate the significance of the feature subset. On this basis, an evaluation function about feature significance is constructed, which simultaneously considers the relevance and redundancy of a candidate feature provided for the selected subset and the remaining feature subset. A novel forward greedy search algorithm is presented to select a feature sequence. The selected features are subsequently evaluated with downstream classification tasks. Experimental using real-world datasets demonstrate the effectiveness of the proposed model and its superiority against comparison baseline methods.}
}
@article{JAMSHIDI2022101672,
title = {Detecting outliers in a univariate time series dataset using unsupervised combined statistical methods: A case study on surface water temperature},
journal = {Ecological Informatics},
volume = {69},
pages = {101672},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101672},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001224},
author = {Ehsan Jolous Jamshidi and Yusri Yusup and John Stephen Kayode and Mohamad Anuar Kamaruddin},
keywords = {Outlier detection, Ocean temperature, Modified -Score, Exponential moving average method, Univariate data},
abstract = {The surface water temperature is a vital ecological and climate variable, and its monitoring is critical. An extensive sensor network measures the ocean, but outliers pervade the monitoring data due to the sudden change in the water surface level. No single algorithm can identify the outlier efficiently. Hence, this work aims to propose and evaluate the performance of three statistical-based outlier detection algorithms for the water surface temperature: 1) the Standard Z-Score method, 2) the Modified Z-Score coupled with decomposition, and 3) the Exponential Moving Average with the Coupled Modified Z-Score and decomposition. A threshold was set to flag the outlier values. The models' performance was evaluated using the F-score method. Results showed that an increase in outlier detection might reduce the precision of identifying the actual outlier. Based on the results, the Exponential Moving Average with the Modified Z-Score gave the highest F-score value (= 0.83) compared to the other two individual methods. Therefore, this proposed algorithm is recommended to detect outliers efficiently in large surface water temperature datasets.}
}
@article{KIM2022101547,
title = {Short-term prediction of particulate matter (PM10 and PM2.5) in Seoul, South Korea using tree-based machine learning algorithms},
journal = {Atmospheric Pollution Research},
volume = {13},
number = {10},
pages = {101547},
year = {2022},
issn = {1309-1042},
doi = {https://doi.org/10.1016/j.apr.2022.101547},
url = {https://www.sciencedirect.com/science/article/pii/S1309104222002288},
author = {Bu-Yo Kim and Yun-Kyu Lim and Joo Wan Cha},
keywords = {Particulate matter prediction, PM, PM, Tree-based machine learning, Air quality monitoring, Light gradient boosting algorithm},
abstract = {In this study, highly accurate particulate matter (PM10 and PM2.5) predictions were obtained using meteorological prediction data from the local data assimilation and prediction system (LDAPS) and tree-based machine learning (ML). The study area was Seoul, South Korea, and data from July 2018 to June 2021 as well as LDAPS 36-h predictions with 1-h intervals 4 times a day were used. The predicted PM values were then compared with the observed PM measurements to evaluate the prediction accuracy. The PM prediction performance of the Community Multi-Scale Air Quality (CMAQ)-based chemical transport model (CTM) was compared with that reported by this study. The experimental results report that, among tree-based ML algorithms, light gradient boosting (LGB) is the most suitable for PM prediction. The PM prediction results of the LGB algorithm for the hourly test data were: bias = −0.10 μg/m3, root mean square error (RMSE) = 13.15 μg/m3, and R2 = 0.86 for PM10 and bias = −0.02 μg/m3, RMSE = 7.48 μg/m3, and R2 = 0.83 for PM2.5, and for daily mean were: RMSE ≤1.16 μg/m3 and R2 = 0.996. The relative RMSE (%RMSE) is 21% lower than the results of the CTM model, and R2 is 0.20 higher. Even in the high PM concentration case prediction results, the algorithm showed good predictive performance with %RMSE = 8.91%–20.43% and R2 = 0.89–0.97. Therefore, in addition to the CTM, high-accuracy PM prediction results using ML can also be used for air quality monitoring and improvement.}
}
@article{NAGITTA20221084,
title = {Human-centered artificial intelligence for the public sector: The gate keeping role of the public procurement professional},
journal = {Procedia Computer Science},
volume = {200},
pages = {1084-1092},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.308},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003179},
author = {Pross Oluka Nagitta and Godfrey Mugurusi and Peter Adoko Obicci and Emmanuel Awuor},
keywords = {Human-centered artificial intelligence (AI), Explainable AI(XAI), public procurement, Ethical AI, Responsible AI, developing countries},
abstract = {The increasing deployment of artificial intelligence (AI) powered solutions for the public sector is hoped to change how developing countries deliver services in key sectors such as agriculture, healthcare, education, and social sectors. And yet AI has a high potential for abuse and creates risks, which if not managed and monitored will jeopardize respect and dignity of the most vulnerable in society. In this study, we argue for delineating public procurements’ role in the human-centred AI (HCAI) discourses, focusing on the developing countries. The study is based on an exploratory inquiry and gathered data among procurement practitioners in Uganda and Kenya, which have similar country procurement regimes: where traditional forms of competition in procurement apply compared to more recent pre-commercial procurement mechanisms that suit AI procurement. We found limited customization in AI technologies, a lack of developed governance frameworks, and little knowledge and distinction between AI procurement and other typical technology procurement processes. We proposed a framework, which in absence of good legal frameworks can allow procurement professionals to embed HCAI principles in AI procurement processes.}
}
@article{YANG2022113813,
title = {Social influence-based contrast language analysis framework for clinical decision support systems},
journal = {Decision Support Systems},
volume = {159},
pages = {113813},
year = {2022},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113813},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622000847},
author = {Xingwei Yang and Alexandra Joukova and Anteneh Ayanso and Morteza Zihayat},
keywords = {Contrast language analysis, Clinical Decision Support System (CDSS), Depression detection, Social network},
abstract = {Depression is a leading mental health problem affecting 300 million people globally. Recent studies show that social networks provide a tremendous potential for mental health professionals as a source of supplemental information about their patients. This study presents a methodological framework for clinical decision support systems (CDSSs) through analysis of social network data to distinguish the language usage of individuals with early signs of depression (i.e., contrast language analysis). By analyzing the contrast language patterns of different user groups, we are able to uncover constructive and actionable insights into the pain points and characteristics of users with signs of depression as decision support mechanisms for clinicians during intervention, (early) diagnosis and treatment plans. First, we discover terms that represent contrasting language by analyzing the percentage difference of terms in two user groups, labeled as”depressed” and”non-depressed” for ease of reference. Second, by building topic models based on social network contents, the topic-level contrast features are discovered. Finally, we consider the structure of the social network to discover the network-level contrast features. To illustrate the effectiveness of the proposed framework, we present a case study on early depression detection using a real-world dataset. The proposed framework has methodological contributions in enhancing the features and functionalities of CDSS for clinicians. It also contributes to evidence-based health research through cost-effective data and analytical insights that can supplement or improve the traditional survey and time-consuming interview methods.}
}
@article{MAHARANA202291,
title = {A review: Data pre-processing and data augmentation techniques},
journal = {Global Transitions Proceedings},
volume = {3},
number = {1},
pages = {91-99},
year = {2022},
note = {International Conference on Intelligent Engineering Approach(ICIEA-2022)},
issn = {2666-285X},
doi = {https://doi.org/10.1016/j.gltp.2022.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S2666285X22000565},
author = {Kiran Maharana and Surajit Mondal and Bhushankumar Nemade},
keywords = {Data augmentation, Data cleaning, Data oversampling, Data pre-processing, Data wraping},
abstract = {This review paper provides an overview of data pre-processing in Machine learning, focusing on all types of problems while building the machine learning problems. It deals with two significant issues in the pre-processing process (i). issues with data and (ii). Steps to follow to do data analysis with its best approach. As raw data are vulnerable to noise, corruption, missing, and inconsistent data, it is necessary to perform pre-processing steps, which is done using classification, clustering, and association and many other pre-processing techniques available. Poor data can primarily affect the accuracy and lead to false prediction, so it is necessary to improve the dataset's quality. So, data pre-processing is the best way to deal with such problems. It makes the knowledge extraction from the data set much easier with cleaning, Integration, transformation, and reduction methods. The issue with Data missing and significant differences in the variety of data always exists as the information is collected through multiple sources and from a real-world application. So, the data augmentation approach generates data for machine learning models. To decrease the dependency on training data and to improve the performance of the machine learning model. This paper discusses flipping, rotating with slight degrees and others to augment the image data and shows how to perform data augmentation methods without distorting the original data.}
}
@article{NI20221020,
title = {Socioeconomic inequalities in cancer incidence and access to health services among children and adolescents in China: a cross-sectional study},
journal = {The Lancet},
volume = {400},
number = {10357},
pages = {1020-1032},
year = {2022},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(22)01541-0},
url = {https://www.sciencedirect.com/science/article/pii/S0140673622015410},
author = {Xin Ni and Zhe Li and Xinping Li and Xiao Zhang and Guoliang Bai and Yingying Liu and Rongshou Zheng and Yawei Zhang and Xin Xu and Yuanhu Liu and Chenguang Jia and Huanmin Wang and Xiaoli Ma and Huyong Zheng and Yan Su and Ming Ge and Qi Zeng and Shengcai Wang and Junyang Zhao and Yueping Zeng and Guoshuang Feng and Yue Xi and Zhuo Deng and Yongli Guo and Zhuoyu Yang and Jinzhe Zhang},
abstract = {Summary
Background
Despite the substantial burden caused by childhood cancer globally, childhood cancer incidence obtained in a nationwide childhood cancer registry and the accessibility of relevant health services are still unknown in China. We comprehensively assessed the most up-to-date cancer incidence in Chinese children and adolescents, nationally, regionally, and in specific population subgroups, and also examined the association between cancer incidence and socioeconomic inequality in access to health services.
Methods
In this national cross-sectional study, we used data from the National Center for Pediatric Cancer Surveillance, the nationwide Hospital Quality Monitoring System, and public databases to cover 31 provinces, autonomous regions, and municipalities in mainland China. We estimated the incidence of cancer among children (aged 0–14 years) and adolescents (aged 15–19 years) in China through stratified proportional estimation. We classified regions by socioeconomic status using the human development index (HDI). Incidence rates of 12 main groups, 47 subgroups, and 81 subtypes of cancer were reported and compared by sex, age, and socioeconomic status, according to the third edition of the International Classification of Childhood Cancer. We also quantified the geographical and population density of paediatric oncologists, pathology workforce, diagnoses and treatment institutions of paediatric cancer, and paediatric beds. We used the Gini coefficient to assess equality in access to these four health service indicators. We also calculated the proportions of cross-regional patients among new cases in our surveillance system.
Findings
We estimated the incidence of cancer among children (aged 0–14 years) and adolescents (aged 15–19 years) in China from Jan 1, 2018, to Dec 31, 2020. An estimated 121 145 cancer cases were diagnosed among children and adolescents in China between 2018 and 2020, with world standard age-standardised incidence rates of 122·86 (95% CI 121·70–124·02) per million for children and 137·64 (136·08–139·20) per million for adolescents. Boys had a higher incidence rate of childhood cancer (133·18 for boys vs 111·21 for girls per million) but a lower incidence of adolescent cancer (133·92 for boys vs 141·79 for girls per million) than girls. Leukaemias (42·33 per million) were the most common cancer group in children, whereas malignant epithelial tumours and melanomas (30·39 per million) surpassed leukaemias (30·08 per million) in adolescents as the cancer with the highest incidence. The overall incidence rates ranged from 101·60 (100·67–102·51) per million in very low HDI regions to 138·21 (137·14–139·29) per million in high HDI regions, indicating a significant positive association between the incidence of childhood and adolescent cancer and regional socioeconomic status (p<0·0001). The incidence in girls showed larger variation (48·45% from the lowest to the highest) than boys (36·71% from lowest to highest) in different socioeconomic regions. The population and geographical densities of most health services also showed a significant positive correlation with HDI levels. In particular, the geographical density distribution (Gini coefficients of 0·32–0·47) had higher inequalities than population density distribution (Gini coefficients of 0·05–0·19). The overall proportion of cross-regional patients of childhood and adolescent cancer was 22·16%, and the highest proportion occurred in retinoblastoma (56·54%) and in low HDI regions (35·14%).
Interpretation
Our study showed that the burden of cancer in children and adolescents in China is much higher than previously nationally reported from 2000 to 2015. The distribution of the accessibility of health services, as a social determinant of health, might have a notable role in the socioeconomic inequalities in cancer incidence among Chinese children and adolescents. With regards to achieving the Sustainable Development Goals, policy approaches should prioritise increasing the accessibility of health services for early diagnosis to improve outcomes and subsequently reduce disease burdens, as well as narrowing the socioeconomic inequalities of childhood and adolescent cancer.
Funding
National Major Science and Technology Projects of China, National Natural Science Foundation of China, Chinese Academy of Engineering Consulting Research Project, Wu Jieping Medical Foundation, Beijing Municipal Administration of Hospitals Incubating Program.}
}
@article{BARCELLOS2022101961,
title = {Towards defining data interpretability in open data portals: Challenges and research opportunities},
journal = {Information Systems},
volume = {106},
pages = {101961},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101961},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001538},
author = {Raissa Barcellos and Flavia Bernardini and José Viterbo},
keywords = {Data interpretability, Open data portals},
abstract = {Open data portals are growing in scope, and the development of this initiative remains one of the main ways to help create new value for society and the economy. Citizens can use the open data made available on these portals to participate more effectively in democratic processes. For that, they have to be able to access, manipulate and interpret such data. Different authors present different definitions and perceptions about the meaning of data interpretability. Today there is no formal consensus on the concept of data interpretability. The goal of this work is to conceptualize what data interpretability is formal. For this, we carried out literature research to identify the definitions of data interpretability. In addition, we studied the information quality literature to identify the Non-Function Requirements that shape the concept of information quality. So, we aligned the interpretability characteristics with the NFR Framework characteristics to find a unique definition. We also conduct a qualitative analysis with experts in data analysis, e-government, and transparency to identify what these experts understand by interpretability. Based on these two studies, we defined interpretability through a model composed of 8 dimensions, each consisting of different characteristics, which must be guaranteed in the data interpretability process to interpret the data correctly. We understand that, for such characteristics to being guaranteed in the interpretability of open government data, it is necessary to have computational tools to support the user. Thus, we also surveyed which technologies and methods ensure each of the interpretability characteristics and pointed out which computational tools implement such technologies and methods. Finally, we analyzed three large open data portals to identify which characteristics are present in these portals, and we note that there are still several challenges to be handled in open government data portals.}
}
@article{SYU2022835,
title = {Usability and Usefulness of Circularity Indicators for Manufacturing Performance Management},
journal = {Procedia CIRP},
volume = {105},
pages = {835-840},
year = {2022},
note = {The 29th CIRP Conference on Life Cycle Engineering, April 4 – 6, 2022, Leuven, Belgium.},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.138},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122001391},
author = {Fu-Siang Syu and Adarsh Vasudevan and Mélanie Despeisse and Arpita Chari and Ebru Turanoglu Bekar and Maria M. Gonçalves and Marco A. Estrela},
keywords = {Circular economy, Circularity indicators, Sustainable manufacturing},
abstract = {Advances in industrial digitalization present many opportunities for process and product data exploitation in manufacturing, unlocking new systemic measures of performance beyond a single machine, process, facility area and even beyond the factory gates. However, existing data models and manufacturing systems’ performance measures are still focused on productivity, quality and delivery time, which could potentially lead to an accelerated linear economy. To shift to more circular industrial systems, we need to identify and assess circularity opportunities in ways that align the goals of sustainable and industrial development. In this study, micro-level circular indicators were reviewed, selected, analysed and tested in a manufacturing company to evaluate their usability and usefulness to guide process improvements. The aim is to enable circular and eco-efficient solutions towards sustainable production systems. Usability and usefulness of the indicators are essential to their integration into established environmental and operations management systems. The main contribution of this study is in the identification of key features making circularity indicators usable and useful from a manufacturer’s perspective. The conclusion also suggests directions for further research on tools and methods to support circular manufacturing.}
}
@article{DASILVA2022181,
title = {An ontological approach for modelling evolutionary knowledge of prognostic method selection},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {181-186},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.190},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001914},
author = {Márcio J. {da Silva} and Lynceo F. Braghirolli and Eike Broda and Hendrik Engbers and Enzo M. Frazzon and Carlos E. Pereira},
keywords = {Domain Ontology, Evolutionary Knowledge, Semantics Mapping, Industry 4.0 (I4.0)},
abstract = {The selection of appropriate predictive maintenance methods based on the current state of a manufacturing system, its machines, and its components is not an easy task due to the multitude of physical and virtual resources available. Moreover, the value of the prognostic information provided by a prognostic method, when applied to a given machine, depends on the system structure and the production and maintenance planning process. Therefore, it is necessary to consider the impact of such information on the system’s key performance indicators to assess the real benefits of each prognostic method. Based on knowledge from predictive maintenance approaches, manufacturing system simulation, and production and maintenance planning, an appropriate semantic model allows establishing a shared vocabulary and understanding among all these fields, along with a description of their relationship. Thus, this paper proposes an ontology of domain termed Ontological MetaMaintain (OntoMM), which specifies the description of concepts and their relationships to provide evolutionary knowledge on domain structure which further specializes in information flows. It is a novel semantic architecture in an ontology network with three modules that address automated prognostic method selection in an industrial environment.}
}
@article{HASHIGUCHI2022368,
title = {Fulfilling the Promise of Artificial Intelligence in the Health Sector: Let’s Get Real},
journal = {Value in Health},
volume = {25},
number = {3},
pages = {368-373},
year = {2022},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2021.11.1369},
url = {https://www.sciencedirect.com/science/article/pii/S1098301521032253},
author = {Tiago Cravo Oliveira Hashiguchi and Jillian Oderkirk and Luke Slawomirski},
keywords = {artificial intelligence, governance, machine learning, policy},
abstract = {Objectives
This study aimed to showcase the potential and key concerns and risks of artificial intelligence (AI) in the health sector, illustrating its application with current examples, and to provide policy guidance for the development, assessment, and adoption of AI technologies to advance policy objectives.
Methods
Nonsystematic scan and analysis of peer-reviewed and gray literature on AI in the health sector, focusing on key insights for policy and governance.
Results
The application of AI in the health sector is currently in the early stages. Most applications have not been scaled beyond the research setting. The use in real-world clinical settings is especially nascent, with more evidence in public health, biomedical research, and “back office” administration. Deploying AI in the health sector carries risks and hazards that must be managed proactively by policy makers. For AI to produce positive health and policy outcomes, 5 key areas for policy are proposed, including health data governance, operationalizing AI principles, flexible regulation, skills among health workers and patients, and strategic public investment.
Conclusions
AI is not a panacea, but a tool to address specific problems. Its successful development and adoption require data governance that ensures high-quality data are available and secure; relevant actors can access technical infrastructure and resources; regulatory frameworks promote trustworthy AI products; and health workers and patients have the information and skills to use AI products and services safely, effectively, and efficiently. All of this requires considerable investment and international collaboration.}
}
@article{BERGHOUT2022100547,
title = {Machine learning for cybersecurity in smart grids: A comprehensive review-based study on methods, solutions, and prospects},
journal = {International Journal of Critical Infrastructure Protection},
volume = {38},
pages = {100547},
year = {2022},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2022.100547},
url = {https://www.sciencedirect.com/science/article/pii/S1874548222000348},
author = {Tarek Berghout and Mohamed Benbouzid and S.M. Muyeen},
keywords = {Cybersecurity, Cyberattacks, Machine learning, Model selection, Smart grids},
abstract = {In modern Smart Grids (SGs) ruled by advanced computing and networking technologies, condition monitoring relies on secure cyberphysical connectivity. Due to this connection, a portion of transported data, containing confidential information, must be protected as it is vulnerable and subject to several cyber threats. SG cyberspace adversaries attempt to gain access through networking platforms to commit several criminal activities such as disrupting or malicious manipulation of whole electricity delivery process including generation, distribution, and even customer services such as billing, leading to serious damage, including financial losses and loss of reputation. Therefore, human awareness training and software technologies are necessary precautions to ensure the reliability of data traffic and power transmission. By exploring the available literature, it is undeniable that Machine Learning (ML) has become the latest in the timeline and one of the leading artificial intelligence technologies capable of detecting, identifying, and responding by mitigating adversary attacks in SGs. In this context, the main objective of this paper is to review different ML tools used in recent years for cyberattacks analysis in SGs. It also provides important guidelines on ML model selection as a global solution when building an attack predictive model. A detailed classification is therefore developed with respect to data security triad, i.e., Confidentiality, Integrity, and Availability (CIA) within different types of cyber threats, systems, and datasets. Furthermore, this review highlights the various encountered challenges, drawbacks, and possible solutions as future prospects for ML cybersecurity applications in SGs.}
}
@article{PETROPOULOS2022705,
title = {Forecasting: theory and practice},
journal = {International Journal of Forecasting},
volume = {38},
number = {3},
pages = {705-871},
year = {2022},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169207021001758},
author = {Fotios Petropoulos and Daniele Apiletti and Vassilios Assimakopoulos and Mohamed Zied Babai and Devon K. Barrow and Souhaib {Ben Taieb} and Christoph Bergmeir and Ricardo J. Bessa and Jakub Bijak and John E. Boylan and Jethro Browell and Claudio Carnevale and Jennifer L. Castle and Pasquale Cirillo and Michael P. Clements and Clara Cordeiro and Fernando Luiz {Cyrino Oliveira} and Shari {De Baets} and Alexander Dokumentov and Joanne Ellison and Piotr Fiszeder and Philip Hans Franses and David T. Frazier and Michael Gilliland and M. Sinan Gönül and Paul Goodwin and Luigi Grossi and Yael Grushka-Cockayne and Mariangela Guidolin and Massimo Guidolin and Ulrich Gunter and Xiaojia Guo and Renato Guseo and Nigel Harvey and David F. Hendry and Ross Hollyman and Tim Januschowski and Jooyoung Jeon and Victor Richmond R. Jose and Yanfei Kang and Anne B. Koehler and Stephan Kolassa and Nikolaos Kourentzes and Sonia Leva and Feng Li and Konstantia Litsiou and Spyros Makridakis and Gael M. Martin and Andrew B. Martinez and Sheik Meeran and Theodore Modis and Konstantinos Nikolopoulos and Dilek Önkal and Alessia Paccagnini and Anastasios Panagiotelis and Ioannis Panapakidis and Jose M. Pavía and Manuela Pedio and Diego J. Pedregal and Pierre Pinson and Patrícia Ramos and David E. Rapach and J. James Reade and Bahman Rostami-Tabar and Michał Rubaszek and Georgios Sermpinis and Han Lin Shang and Evangelos Spiliotis and Aris A. Syntetos and Priyanga Dilini Talagala and Thiyanga S. Talagala and Len Tashman and Dimitrios Thomakos and Thordis Thorarinsdottir and Ezio Todini and Juan Ramón {Trapero Arenas} and Xiaoqian Wang and Robert L. Winkler and Alisa Yusupova and Florian Ziel},
keywords = {Review, Encyclopedia, Methods, Applications, Principles, Time series, Prediction},
abstract = {Forecasting has always been at the forefront of decision making and planning. The uncertainty that surrounds the future is both exciting and challenging, with individuals and organisations seeking to minimise risks and maximise utilities. The large number of forecasting applications calls for a diverse set of forecasting methods to tackle real-life challenges. This article provides a non-systematic review of the theory and the practice of forecasting. We provide an overview of a wide range of theoretical, state-of-the-art models, methods, principles, and approaches to prepare, produce, organise, and evaluate forecasts. We then demonstrate how such theoretical concepts are applied in a variety of real-life contexts. We do not claim that this review is an exhaustive list of methods and applications. However, we wish that our encyclopedic presentation will offer a point of reference for the rich work that has been undertaken over the last decades, with some key insights for the future of forecasting theory and practice. Given its encyclopedic nature, the intended mode of reading is non-linear. We offer cross-references to allow the readers to navigate through the various topics. We complement the theoretical concepts and applications covered by large lists of free or open-source software implementations and publicly-available databases.}
}
@article{LI2022108008,
title = {Artificial intelligence in the analysis of glycosylation data},
journal = {Biotechnology Advances},
volume = {60},
pages = {108008},
year = {2022},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2022.108008},
url = {https://www.sciencedirect.com/science/article/pii/S0734975022001045},
author = {Haining Li and Austin W.T. Chiang and Nathan E. Lewis},
keywords = {Glycosylation machinery, Artificial intelligence, Multi-omics integration, Interpretable models},
abstract = {Glycans are complex, yet ubiquitous across biological systems. They are involved in diverse essential organismal functions. Aberrant glycosylation may lead to disease development, such as cancer, autoimmune diseases, and inflammatory diseases. Glycans, both normal and aberrant, are synthesized using extensive glycosylation machinery, and understanding this machinery can provide invaluable insights for diagnosis, prognosis, and treatment of various diseases. Increasing amounts of glycomics data are being generated thanks to advances in glycoanalytics technologies, but to maximize the value of such data, innovations are needed for analyzing and interpreting large-scale glycomics data. Artificial intelligence (AI) provides a powerful analysis toolbox in many scientific fields, and here we review state-of-the-art AI approaches on glycosylation analysis. We further discuss how models can be analyzed to gain mechanistic insights into glycosylation machinery and how the machinery shapes glycans under different scenarios. Finally, we propose how to leverage the gained knowledge for developing predictive AI-based models of glycosylation. Thus, guiding future research of AI-based glycosylation model development will provide valuable insights into glycosylation and glycan machinery.}
}
@article{WANG202255,
title = {A Knowledge-enriched Framework for Life Cycle Assessment in Manufacturing},
journal = {Procedia CIRP},
volume = {105},
pages = {55-60},
year = {2022},
note = {The 29th CIRP Conference on Life Cycle Engineering, April 4 – 6, 2022, Leuven, Belgium.},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122000105},
author = {Yanan Wang and Jiaqi Tao and Weipeng Liu and Tao Peng and Renzhong Tang and Qi Wu},
keywords = {knowledge graph, life cycle assessment, process, flow, life cycle inventory analysis, aluminum die casting},
abstract = {Even though carbon neutralization is promised by several countries, many developed tools for life cycle assessment (LCA) or sustainability performance is still not widely and easily used in industrial practices. A big gap exists between having the tools and using the tools, which is currently facilitated by human expertise. LCA practitioners should embrace and take advantage the rapid development of knowledge graph (KG) technology. In this paper, the important position and key methodologies of KG are presented in the context of LCA, latest advances are reviewed and presented. A framework of knowledge-enriched LCA is proposed which was illustrated with a LCA-oriented process-flow knowledge graph (PFKG) for inventory analysis. The knowledge acquisition and construction of PFKG was demonstrated with aluminum die casting, and on top of which a recommendation application of PFKG was discussed in a case of five-star feet of office chair. PFKG can facilitate non-experts in identifying the complex structured relationship between process and flow, and recommend possible life cycle inventory data. It is envisioned that merging with KG will boost engineering sustainability in a more practical manner.}
}
@article{CARUCCIO20221,
title = {A decision-support framework for data anonymization with application to machine learning processes},
journal = {Information Sciences},
volume = {613},
pages = {1-32},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522010490},
author = {Loredana Caruccio and Domenico Desiato and Giuseppe Polese and Genoveffa Tortora and Nicola Zannone},
keywords = {Privacy preserving machine learning, k-anonymity, Relaxed functional dependencies, Generalization strategies},
abstract = {The application of machine learning techniques to large and distributed data archives might result in the disclosure of sensitive information about the data subjects. Data often contain sensitive identifiable information, and even if these are protected, the excessive processing capabilities of current machine learning techniques might facilitate the identification of individuals, raising privacy concerns. To this end, we propose a decision-support framework for data anonymization, which relies on a novel approach that exploits data correlations, expressed in terms of relaxed functional dependencies (rfds) to identify data anonymization strategies providing suitable trade-offs between privacy and data utility. Moreover, we investigate how to generate anonymization strategies that leverage multiple data correlations simultaneously to increase the utility of anonymized datasets. In addition, our framework provides support in the selection of the anonymization strategy to apply by enabling an understanding of the trade-offs between privacy and data utility offered by the obtained strategies. Experiments on real-life datasets show that our approach achieves promising results in terms of data utility while guaranteeing the desired privacy level, and it allows data owners to select anonymization strategies balancing their privacy and data utility requirements.}
}
@article{VAGHARI2022119344,
title = {A multi-site, multi-participant magnetoencephalography resting-state dataset to study dementia: The BioFIND dataset},
journal = {NeuroImage},
volume = {258},
pages = {119344},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119344},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922004633},
author = {Delshad Vaghari and Ricardo Bruna and Laura E. Hughes and David Nesbitt and Roni Tibon and James B. Rowe and Fernando Maestu and Richard N. Henson},
abstract = {Early detection of Alzheimer's Disease (AD) is vital to reduce the burden of dementia and for developing effective treatments. Neuroimaging can detect early brain changes, such as hippocampal atrophy in Mild Cognitive Impairment (MCI), a prodromal state of AD. However, selecting the most informative imaging features by machine-learning requires many cases. While large publically-available datasets of people with dementia or prodromal disease exist for Magnetic Resonance Imaging (MRI), comparable datasets are missing for Magnetoencephalography (MEG). MEG offers advantages in its millisecond resolution, revealing physiological changes in brain oscillations or connectivity before structural changes are evident with MRI. We introduce a MEG dataset with 324 individuals: patients with MCI and healthy controls. Their brain activity was recorded while resting with eyes closed, using a 306-channel MEG scanner at one of two sites (Madrid or Cambridge), enabling tests of generalization across sites. A T1-weighted MRI is provided to assist source localisation. The MEG and MRI data are formatted according to international BIDS standards and analysed freely on the DPUK platform (https://portal.dementiasplatform.uk/Apply). Here, we describe this dataset in detail, report some example (benchmark) analyses, and consider its limitations and future directions.}
}