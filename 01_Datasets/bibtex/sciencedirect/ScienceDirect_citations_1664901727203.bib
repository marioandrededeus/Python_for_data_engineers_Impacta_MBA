@article{SEMLALI2021107257,
title = {SAT-CEP-monitor: An air quality monitoring software architecture combining complex event processing with satellite remote sensing},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107257},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107257},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002421},
author = {Badr-Eddine Boudriki Semlali and Chaker El Amrani and Guadalupe Ortiz and Juan Boubeta-Puig and Alfonso Garcia-de-Prado},
keywords = {Remote sensing, Satellite sensors, Air quality, Complex event processing, Big data, Decision-making},
abstract = {Air pollution is a major problem today that causes serious damage to human health. Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions. Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches. We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers. We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT. This study also validates the air quality measured by ground stations and satellite sensor data.}
}
@article{CAUDAI20215762,
title = {AI applications in functional genomics},
journal = {Computational and Structural Biotechnology Journal},
volume = {19},
pages = {5762-5790},
year = {2021},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2021.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2001037021004311},
author = {Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo},
keywords = {Artificial intelligence, Functional genomics, Genomics, Proteomics, Epigenomics, Transcriptomics, Epitranscriptomics, Metabolomics, Machine learning, Deep learning},
abstract = {We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by “deep learning”, along with a burst of “big data” that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.}
}
@article{SHIROSAKIMARCALDESOUZA2021160,
title = {Evaluating and ranking secondary data sources to be used in the Brazilian LCA database – “SICV Brasil”},
journal = {Sustainable Production and Consumption},
volume = {26},
pages = {160-171},
year = {2021},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2020.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S2352550920303286},
author = {Luri {Shirosaki Marçal de Souza} and Andréa Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva}},
keywords = {Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors},
abstract = {The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System – the “SICV Brasil” launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.}
}
@article{PAGGI2021106907,
title = {Towards the definition of an information quality metric for information fusion models},
journal = {Computers & Electrical Engineering},
volume = {89},
pages = {106907},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106907},
url = {https://www.sciencedirect.com/science/article/pii/S004579062030759X},
author = {Horacio Paggi and Javier Soriano and Juan A. Lara and Ernesto Damiani},
keywords = {Adaptive Peer-to-Peer systems, Information fusion, Uncertain information handling, Information quality metric},
abstract = {Managing information quality has become important in cyber-physical systems dealing with big data. In this regard, different models have been proposed, mainly in flat peer-to-peer networks, in which exchanging information efficiently is a key aspect due to scarce resources. However, little research has been conducted on information quality metrics for cyber-physical scenarios. In this paper, we propose an information quality metric and show its application to an information fusion model. It is a “model-oriented quality metric” since it allows non-predefined variants on its configuration depending on the application domain. The model was tested on several simulations using open datasets. The results obtained in the performance of the model confirm the validity of the information quality metric, proposed in this paper, on which the model is based. The model may have a wide variety of applications such as mobile recommendation or decision making in critical environments (emergencies, war, and so on).}
}
@article{D2021,
title = {A study on artificial intelligence for monitoring smart environments},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321043911},
author = {Karthika D.},
keywords = {Big data analytics, Machine learning, Internet of things, Smart cities, Wireless technologies},
abstract = {Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.}
}
@incollection{MCGILVRAY2021253,
title = {Chapter 5 - Structuring Your Project},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {253-267},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000013},
author = {Danette McGilvray},
keywords = {Solution Development Life Cycle (SDLC), Agile, Scrum, sequential, waterfall, project objectives, project team, project roles, project timing, project approach},
abstract = {It is essential that those using the Ten Steps do a good job of organizing their work. This chapter guides readers’ choices when setting up their projects and assembling a project team. Three general types of projects are detailed: 1) Focused data quality improvement project, 2) Data quality activities in another project, such as application development, data migration or integration of any kind, and 3) Ad hoc use of data quality steps, activities, or techniques from the Ten Steps. Additional information is given for incorporating data quality activities into another project using various SDLCs (solution/system/software life cycles). Relevant data quality activities from the Ten Steps can be incorporated into any SDLC that is the basis for the larger project (Agile, sequential, hybrid, etc.). To that end, several tables list data governance, stewardship, data quality and readiness activities and where they would take place in typical SDLC phases. A table with Agile Scrum activities are cross-referenced to the same SDLC phases. The chapter concludes with general tips for project timing, communication, and engagement.}
}
@article{OYEDELE2021100158,
title = {Machine learning predictions for lost time injuries in power transmission and distribution projects},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100158},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100158},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000797},
author = {Ahmed O. Oyedele and Anuoluwapo O. Ajayi and Lukumon O. Oyedele},
keywords = {Hazard assessment, Deep learning, Big data predictive analytics, Power infrastructure, Zero count data},
abstract = {Although advanced machine learning algorithms are predominantly used for predicting outcomes in many fields, their utilisation in predicting incident outcome in construction safety is still relatively new. This study harnesses Big Data with Deep Learning to develop a robust safety management system by analysing unstructured incident datasets consisting of 168,574 data points from power transmission and distribution projects delivered across the UK from 2004 to 2016. This study compared Deep Learning performance with popular machine learning algorithms (support vector machine, random forests, multivariate adaptive regression splines, generalised linear model, and their ensembles) concerning lost time injury and risk assessment in power utility projects. Deep Learning gave the best prediction for safety outcomes with high skills (AUC = 0.95, R2 = 0.88, and multi-class ROC = 0.93), thus outperforming the other algorithms. The results from this study also highlight the significance of quantitative analysis of empirical data in safety science and contribute to an enhanced understanding of injury patterns using predictive analytics in conjunction with safety experts’ perspectives. Additionally, the results will enhance the skills of safety managers in the power utility domain to advance safety intervention efforts.}
}
@article{LI202156,
title = {Construction of an artificial intelligence system in dermatology: effectiveness and consideration of Chinese Skin Image Database (CSID)},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {56-60},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S266710262100005X},
author = {Chengxu Li and Wenmin Fei and Yang Han and Xiaoli Ning and Ziyi Wang and Keke Li and Ke Xue and Jingkai Xu and Ruixing Yu and Rusong Meng and Feng Xu and Weimin Ma and Yong Cui},
keywords = {Artificial intelligence, Dermatology, Skin image, Chinese Skin Image Database},
abstract = {After more than 60 years of development, artificial intelligence (AI) has been widely used in various fields. Especially in recent years, with the development of deep learning, AI has made many remarkable achievements in the medical field. Dermatology, as a clinical discipline with morphology as its main feature, is particularly suitable for the development of AI. The rapid development of skin imaging technology has helped dermatologists to assist in the diagnosis of diseases and has greatly improved the accuracy of diagnosis. Skin imaging data have natural big data attributes, which is important for AI research. The establishment of the Chinese Skin Image Database (CSID) has solved many problems such as isolated data islands and inconsistent data quality. Based on the CSID, many pioneering achievements have been made in the research and development of AI-assisted decision-making software, the establishment of expert organizations, personnel training, scientific research, and so on. At present, there are still many problems with AI in the field of dermatology, such as clinical validation, medical device licensing, interdisciplinary, and standard formulation, which urgently need to be solved by joint efforts of all parties.}
}
@article{ZHOU2021103342,
title = {Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation},
journal = {Journal of Energy Storage},
volume = {44},
pages = {103342},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.103342},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21010331},
author = {Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng},
keywords = {Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction},
abstract = {This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2σ range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.}
}
@article{JIANG2021172,
title = {Data consistency method of heterogeneous power IOT based on hybrid model},
journal = {ISA Transactions},
volume = {117},
pages = {172-179},
year = {2021},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2021.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S0019057821000665},
author = {Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li},
keywords = {Power IOT system, Hybrid model, Heterogeneous data consistency, Machine learning combination method},
abstract = {The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.}
}
@article{PATONAI2021e00203,
title = {Integrating trophic data from the literature: The Danube River food web},
journal = {Food Webs},
volume = {28},
pages = {e00203},
year = {2021},
issn = {2352-2496},
doi = {https://doi.org/10.1016/j.fooweb.2021.e00203},
url = {https://www.sciencedirect.com/science/article/pii/S2352249621000161},
author = {Katalin Patonai and Ferenc Jordán},
keywords = {Aggregation, Danube River, Food web, Incomplete data, Taxonomy},
abstract = {In the era of bioinformatics and big data, ecological research depends on large and easily accessible databases that make it possible to construct complex system models. Open-access data repositories for food webs via publications and ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the trophic connections (predator-prey relationships) for the Danube River ecosystem as gathered from globally available literature data. Data are analyzed by Danube regions separately (Upper, Middle, Lower Danube) as well as an integrated master network version. The master version has been aggregated into larger taxonomic categories. Local and global metrics were used to analyze and compare each network. We find disparity between regions (the Middle Danube having most nodes, but still quite heterogenous), we identify the most important trophic groups, and explain ways on evaluating missing data using each aggregation stage. This data-driven approach, summarizing our presently documented knowledge, can be used for preparing preliminary models and to further refine the Danube River food web in the future.}
}
@article{SHET2021311,
title = {Examining the determinants of successful adoption of data analytics in human resource management – A framework for implications},
journal = {Journal of Business Research},
volume = {131},
pages = {311-326},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.03.054},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002174},
author = {Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi},
keywords = {Human resource analytics, HRM analytics, People analytics, Adoption of HR analytics, Challenges, Implementation of HR analytics, Big data, Data analytics, Framework synthesis},
abstract = {Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.}
}
@article{AYVAZ2021114598,
title = {Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time},
journal = {Expert Systems with Applications},
volume = {173},
pages = {114598},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114598},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421000397},
author = {Serkan Ayvaz and Koray Alpay},
keywords = {Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data},
abstract = {In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.}
}
@article{BUI2021109392,
title = {Advanced data analytics for ship performance monitoring under localized operational conditions},
journal = {Ocean Engineering},
volume = {235},
pages = {109392},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109392},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821008040},
author = {Khanh Q. Bui and Lokukaluge P. Perera},
keywords = {Big data analytics, Machine learning, Ship performance monitoring, Energy efficiency, Emission control, Data anomaly detection},
abstract = {Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship’s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship’s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship’s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.}
}
@article{FERREIRA2021757,
title = {How do data scientists and managers influence machine learning value creation?},
journal = {Procedia Computer Science},
volume = {181},
pages = {757-764},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.228},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002714},
author = {Humberto Ferreira and Pedro Ruivo and Carolina Reis},
keywords = {machine learning, business value, data scientists, managers, people factor},
abstract = {Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.}
}
@article{LEAL2021100172,
title = {Smart Pharmaceutical Manufacturing: Ensuring End-to-End Traceability and Data Integrity in Medicine Production},
journal = {Big Data Research},
volume = {24},
pages = {100172},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100172},
url = {https://www.sciencedirect.com/science/article/pii/S221457962030040X},
author = {Fátima Leal and Adriana E. Chis and Simon Caton and Horacio González–Vélez and Juan M. García–Gómez and Marta Durá and Angel Sánchez–García and Carlos Sáez and Anthony Karageorgos and Vassilis C. Gerogiannis and Apostolos Xenakis and Efthymios Lallas and Theodoros Ntounas and Eleni Vasileiou and Georgios Mountzouris and Barbara Otti and Penelope Pucci and Rossano Papini and David Cerrai and Mariola Mier},
keywords = {ALCOA, Blockchain, Data anaytics, Data quality, Intelligent agents, Smart contracts},
abstract = {Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.}
}
@article{YE2021102513,
title = {A feasible framework to downscale NPP-VIIRS nighttime light imagery using multi-source spatial variables and geographically weighted regression},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {104},
pages = {102513},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102513},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421002208},
author = {Yang Ye and Linyan Huang and Qiming Zheng and Chenxin Liang and Baiyu Dong and Jinsong Deng and Xiuzhen Han},
keywords = {Nighttime light (NTL), Downscaling, Geographically weighted regression (GWR), Impervious surface detection},
abstract = {The cloud-free monthly composite of global nighttime light (NTL) data of the Suomi National Polar-orbiting Partnership with the Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) day/night band (DNB) provides indispensable indications of human activities and settlements. However, the coarse spatial resolution (15 arc sec) of NTL imagery greatly restricts its application potential. This study proposes a feasible framework to downscale NPP-VIIRS NTL using muti-source spatial variables and geographically weighted regression (GWR) method. High-resolution auxiliary variables were acquired from the Landsat 8 OLI/ TIRS and social media platforms. GWR-based downscaling procedures were consequently implemented to obtain NTL at a 100-m resolution. The downscaled NTL data were validated against Loujia1-01 imagery based on the coefficient of determination (R2) and root-mean-square error (RMSE). The results suggest that the data quality was suitably improved after downscaling, yielding higher R2 (0.604 vs. 0.568) and lower RMSE (8.828 vs. 9.870 nW/cm2/sr) values than those of the original NTL data. Finally, the NTL was extendedly applied to detect impervious surfaces, and the downscaled NTL had higher accuracy than the original NTL. Therefore, this study facilitates data quality improvement of NPP-VIIRS NTL imagery by downscaling, thus enabling more accurate applications.}
}
@article{AMEEN2021106761,
title = {Consumer interaction with cutting-edge technologies: Implications for future research},
journal = {Computers in Human Behavior},
volume = {120},
pages = {106761},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106761},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221000832},
author = {Nisreen Ameen and Sameer Hosany and Ali Tarhini},
keywords = {Consumer interaction, Cutting-edge technologies, Artificial intelligence, Virtual reality and augmented reality, Robotics, Wearable technology, Big data analytics},
abstract = {This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.}
}
@article{JUNG202115,
title = {The potential of remote sensing and artificial intelligence as tools to improve the resilience of agriculture production systems},
journal = {Current Opinion in Biotechnology},
volume = {70},
pages = {15-22},
year = {2021},
note = {Food Biotechnology ● Plant Biotechnology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0958166920301257},
author = {Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles},
abstract = {Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade’s agricultural and human nutrition challenges.}
}
@article{DELRIOCASTRO2021122204,
title = {Unleashing the convergence amid digitalization and sustainability towards pursuing the Sustainable Development Goals (SDGs): A holistic review},
journal = {Journal of Cleaner Production},
volume = {280},
pages = {122204},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122204},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620322514},
author = {Gema {Del Río Castro} and María Camino {González Fernández} and Ángel {Uruburu Colsa}},
keywords = {Sustainability, Sustainable development goals (SDGs), Digitalization, ICT, Big data, Artificial intelligence},
abstract = {The Sustainable Development Goals (SDGs) within the United Nations 2030 Agenda emerged in 2015, becoming an unprecedented global compass for navigating extant sustainability challenges. Nevertheless, it still represents a nascent field enduring uncertainties and complexities. In this regard, the interplay between digitalization and sustainability unfolds bright opportunities for shaping a greener economy and society, paving the way towards the SDGs. However, little evidence exists so far, about a genuine contribution of digital paradigms to sustainability. Besides, their role to tackle the SDGs research gaps remains unexplored. Thus, a holistic characterization of the aforementioned topics has not been fully explored in the emerging literature, deserving further research. The article endeavors a twofold purpose: (1) categorizing the main SDGs research gaps; (2) coupled with a critical exploration of the potential contribution of digital paradigms, particularly Big Data and Artificial Intelligence, towards overcoming the aforesaid caveats and pursuing the 2030 Agenda. Ultimately, the study seeks to bridge literature gaps by providing a first-of-its-kind overview on the SDGs and their nexus with digitalization, while unraveling policy implications and future research directions. The methodology has consisted of a systematic holistic review and in-depth qualitative analysis of the literature on the realms of the SDGs and digitalization. Our findings evidence that the SDGs present several research gaps, namely: flawed understanding of complexities and interlinkages; design shortcomings and imbalances; implementation and governance hurdles; unsuitable indicators and assessment methodologies; truncated adoption and off-target progress; unclear responsibilities and lacking coordination; untapped role of technological innovation and knowledge management. Moreover, our results show growing expectations about the added value brought by digitalization for pursuing the SDGs, through novel data sources, enhanced analytical capacities and collaborative digital ecosystems. However, current research and practice remains in early-stage, pointing to ethical, social and environmental controversies, along with policy caveats, which merit additional research. In light of the findings, the authors suggest a first-approach exploration of research and policy implications. Results suggest that further multidisciplinary research, dialogue and concerted efforts for transformation are required. Reframing the Agenda, while aligning the sustainable development and digitalization policies, seems advisable to ensure a holistic sustainability. The findings aim at guiding and stimulating further research and science-policy dialogue on the promising nexus amid the SDGs and digitalization.}
}
@article{XU2021100860,
title = {Comparing differences in the spatiotemporal patterns between resident tourists and non-resident tourists using hotel check-in registers},
journal = {Tourism Management Perspectives},
volume = {39},
pages = {100860},
year = {2021},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2021.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2211973621000738},
author = {Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang},
keywords = {Big data, Spatiotemporal patterns, Resident tourists and non-resident tourists, Multiple city travel, Hotel check-in registers},
abstract = {Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.}
}
@article{WANG2021189,
title = {Safety intelligence as an essential perspective for safety management in the era of Safety 4.0: From a theoretical to a practical framework},
journal = {Process Safety and Environmental Protection},
volume = {148},
pages = {189-199},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2020.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0957582020318000},
author = {Bing Wang},
keywords = {Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making},
abstract = {In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science—a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.}
}
@article{SCHOCK2021636,
title = {Data Acquisition and Preparation – Enabling Data Analytics Projects within Production},
journal = {Procedia CIRP},
volume = {104},
pages = {636-640},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.107},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010052},
author = {Christoph Schock and Jonas Dumler and Frank Doepper},
keywords = {Data Analytics, CRISP-DM, Data Acquisition, Data Preparation, Feature Engineering, Process Monitoring, Condition Monitoring},
abstract = {The increasing amount of available data in production systems is associated with great potential for process optimization. Due to lack of a data analytics methodology and low data quality within production these potentials often remain unused. Therefore, in this paper we present a model for data acquisition and data preparation including feature engineering for characteristic sensor signals of production machines. The model allows the extraction of relevant process information from the signal, which can be used for monitoring, KPI tracking, trend analysis and anomaly detection. The approach is evaluated on an industrial turning process.}
}
@article{EHRING2021163,
title = {SMART standards - concept for the automated transfer of standard contents into a machine-actionable form},
journal = {Procedia CIRP},
volume = {100},
pages = {163-168},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004868},
author = {Dominik Ehring and Janosch Luttmer and Robin Pluhnau and Arun Nagarajah},
keywords = {SMART Standards, knowledge representation, automatic extraction, transfer, 3M model of Duisburg},
abstract = {Standards are - not directly visible to everyone – omnipresent in nearly every development process. In times of digitalization, where buzzwords such as "connectivity of machines", "artificial intelligence", “big data”, “cloud computing” or “smart factories” are often used, companies are still confronted with problems in handling standards throughout the entire product lifecycle. Today’s way of working with standards is characterized by manual viewing of documents, whereby a user searches for relevant information, such as formulas, and has to transfer this information to his process, method or tool. This manual process results in an increased time, loss of quality due to faulty manual transmission of information, a high adjustment effort for updates of standards and no guarantee for traceability. In order to reduce and minimize errors and needed time for work with information stored within standards, there is a need for a new form of knowledge representation for standards with sufficient data quality to ensure standard-compliant development activities. Consequently, there is a need for machine-actionable standards to ensure autonomous and efficient processes, whereby the effort for preparation is less than the benefit. The question arises how classified standards content can be represented in a machine-actionable way without loss of information. This paper shows a concept for the automatic extraction of standards content and their transfer into a machine-actionable knowledge representation. The concept, which is based on the “3M Framework of Duisburg” and thus answers questions of modularization, modeling and management, consists of six steps "extraction", "modeling", “modification”, "fusion and storage", "provision" and "application", to digitalize existing content, is presented and discussed.}
}
@article{RAJ2021103107,
title = {A survey on the role of Internet of Things for adopting and promoting Agriculture 4.0},
journal = {Journal of Network and Computer Applications},
volume = {187},
pages = {103107},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103107},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001284},
author = {Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato},
abstract = {There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.}
}
@article{FRANCIA2021299,
title = {Making data platforms smarter with MOSES},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {299-313},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002260},
author = {Matteo Francia and Enrico Gallinucci and Matteo Golfarelli and Anna Giulia Leoni and Stefano Rizzi and Nicola Santolini},
keywords = {Data lake, Metadata, Big data, Data platform},
abstract = {The rise of data platforms has enabled the collection and processing of huge volumes of data, but has opened to the risk of losing their control. Collecting proper metadata about raw data and transformations can significantly reduce this risk. In this paper we propose MOSES, a technology-agnostic, extensible, and customizable framework for metadata handling in big data platforms. The framework hinges on a metadata repository that stores information about the objects in the big data platform and the processes that transform them. MOSES provides a wide range of functionalities to different types of users of the platform. Differently from previous high-level proposals, MOSES is fully implemented and it was not conceived for a specific technology. Besides discussing the rationale and the features of MOSES, in this paper we describe its implementation and we test it on a real case study. The ultimate goal is to take a significant step forward towards proving that metadata handling in big data platforms is feasible and beneficial.}
}
@article{LEPRINCE2021111195,
title = {Data mining cubes for buildings, a generic framework for multidimensional analytics of building performance data},
journal = {Energy and Buildings},
volume = {248},
pages = {111195},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111195},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821004795},
author = {Julien Leprince and Clayton Miller and Wim Zeiler},
keywords = {Data mining, Data cube, Generic method, Multidimensional analytics, Machine learning, Building data},
abstract = {Over the last decade, collecting massive volumes of data has been made all the more accessible, pushing the building sector to embrace data mining as a powerful tool for harvesting the potential of big data analytics. However repetitive challenges still persist emerging from the need for a common analytical frame, effective application- and insight-driven targeted data selection, as well as benchmarked-supported claims. This study addresses these concerns by putting forward a generic stepwise multidimensional data mining framework tailored to building data, leveraging the dimensional-structures of data cubes. Using the open Building Data Genome Project 2 set, composed of 3053 energy meters from 1636 buildings, we provide an online, open access, implementation illustration of our method applied to automated pattern identification. We define a 3-dimensional building cube echoing typical analytical frames of interest, namely, bottom-up, top-down and temporal drill-in approaches. Our results highlight the importance of application and insight driven mining for effective dimensional-frame targeting. Impactful visualizations were developed allowing practical human inspection, paving the path towards more interpretable analytics.}
}
@article{LI2021692,
title = {Data science skills and domain knowledge requirements in the manufacturing industry: A gap analysis},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {692-706},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001448},
author = {Guoyan Li and Chenxi Yuan and Sagar Kamarthi and Mohsen Moghaddam and Xiaoning Jin},
keywords = {Industry 4.0, Labor market analysis, Skills gap, Data science},
abstract = {Manufacturing has adopted technologies such as automation, robotics, industrial Internet of Things (IoT), and big data analytics to improve productivity, efficiency, and capabilities in the production environment. Modern manufacturing workers not only need to be adept at the traditional manufacturing technologies but also ought to be trained in the advanced data-rich computer-automated technologies. This study analyzes the data science and analytics (DSA) skills gap in today's manufacturing workforce to identify the critical technical skills and domain knowledge required for data science and intelligent manufacturing-related jobs that are highly in-demand in today's manufacturing industry. The gap analysis conducted in this paper on Emsi job posting and profile data provides insights into the trends in manufacturing jobs that leverage data science, automation, cyber, and sensor technologies. These insights will be helpful for educators and industry to train the next generation manufacturing workforce. The main contribution of this paper includes (1) presenting the overall trend in manufacturing job postings in the U.S., (2) summarizing the critical skills and domain knowledge in demand in the manufacturing sector, (3) summarizing skills and domain knowledge reported by manufacturing job seekers, (4) identifying the gaps between demand and supply of skills and domain knowledge, and (5) recognize opportunities for training and upskilling workforce to address the widening skills and knowledge gap.}
}
@article{WANG202151,
title = {The national multi-center artificial intelligent myopia prevention and control project},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {51-55},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000085},
author = {Xun Wang and Yahan Yang and Yuxuan Wu and Wenbin Wei and Li Dong and Yang Li and Xingping Tan and Hankun Cao and Hong Zhang and Xiaodan Ma and Qin Jiang and Yunfan Zhou and Weihua Yang and Chaoyu Li and Yu Gu and Lin Ding and Yanli Qin and Qi Chen and Lili Li and Mingyue Lian and Jin Ma and Dongmei Cui and Yuanzhou Huang and Wenyan Liu and Xiao Yang and Shuiming Yu and Jingjing Chen and Dongni Wang and Zhenzhe Lin and Pisong Yan and Haotian Lin},
keywords = {Myopia prevention and control, Artificial intelligent, National multicenter project},
abstract = {In recent years, the incidence of myopia has increased at an alarming rate among children and adolescents in China. The exploration of an effective prevention and control method for myopia is in urgent need. With the development of information technology in the past decade, artificial intelligence with the Internet of Things technology (AIoT) is characterized by strong computing power, advanced algorithm, continuous monitoring, and accurate prediction of long-term progression. Therefore, big data and artificial intelligence technology have the potential to be applied to data mining of myopia etiology and prediction of myopia occurrence and development. More recently, there has been a growing recognition that myopia study involving AIoT needs to undergo a rigorous evaluation to demonstrate robust results.}
}
@article{LAIFA2021981,
title = {Train delay prediction in Tunisian railway through LightGBM model},
journal = {Procedia Computer Science},
volume = {192},
pages = {981-990},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015891},
author = {Hassiba Laifa and Raoudha khcherif and Henda Hajjami {Ben Ghezalaa}},
keywords = {Delay prediction, Data Analysis, Machine learning, LightGBM},
abstract = {Train delays are one of the most important problems in the railway systems across the world, which urges the development of predictive analysis-based approaches to estimate it. In fact, with the advanced big data analysis and machine learning tools and technologies, the train delay-prediction systems can process and extract useful information from the large historical train movement data collected by the railway information system. Besides, accurate prediction of train delays can help train dispatchers make decisions through timetable rescheduling and service reliability improving. We propose, in this manuscript, a machine-learning model that captures the relationship between the arrival delay of passenger trains and the various characteristics of the railway system. We also apply, for the first time, lightGBM regressor based on optimal hyper-parameters to predict train delays. To evaluate the introduced model performance, the latter is compared with that of some other widely used existing models. Its R-squared, RMSE and RME were also compared with those of Support Vector Machine, Random Forest, XGBboost and Artificial Neural Network models. Statistical comparison indicates that the LightGBM outperforms the other models and is the fastest.}
}
@article{KOTSIOPOULOS2021100341,
title = {Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm},
journal = {Computer Science Review},
volume = {40},
pages = {100341},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100341},
url = {https://www.sciencedirect.com/science/article/pii/S157401372030441X},
author = {Thanasis Kotsiopoulos and Panagiotis Sarigiannidis and Dimosthenis Ioannidis and Dimitrios Tzovaras},
keywords = {Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid},
abstract = {Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.}
}
@article{YE2021107036,
title = {True mean value discovery over multiple data sources with unknown reliability degrees},
journal = {Knowledge-Based Systems},
volume = {223},
pages = {107036},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107036},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121002999},
author = {Songtao Ye and Junjie Wang and Hongjie Fan and Zhiqiang Zhang},
keywords = {Multiple data sources, Source reliabilities, True mean value discovery, Confidence interval},
abstract = {In the era of big data, we are committed to obtaining the observations of target objects from a wider range of data sources. As the number of data sources increases, we expect that more trustworthy statistical parameters can be estimated from the multi-source observations, for example, the population mean. However, the reliability of data sources rarely attracts our attention, because the hypothesis testing seems to be an effective tool for determining whether a given estimate is acceptable. In practice, the noisy observations from different unreliable data sources may have different statistical characteristic parameters, and these parameters are unknown. It makes the condition that observations should be identically distributed in hypothesis testing no longer tenable. Therefore, a poor estimate of the population mean may be accepted, as the hypothesis testing is performed over the multi-source observations. To address this issue, in this paper, we propose a true mean value discovery algorithm in which we can use multi-source observations to determine whether an estimated population mean should be rejected. Additionally, the reliability degree of each data source can be estimated using the proposed algorithm. By removing incorrect observations provided by unreliable sources, we can obtain more reliable estimates of true population means. Experiments on three real-world tasks demonstrate that the proposed method outperforms state-of-the-art approaches.}
}
@article{NEETHIRAJAN2021100408,
title = {Digital Livestock Farming},
journal = {Sensing and Bio-Sensing Research},
volume = {32},
pages = {100408},
year = {2021},
issn = {2214-1804},
doi = {https://doi.org/10.1016/j.sbsr.2021.100408},
url = {https://www.sciencedirect.com/science/article/pii/S2214180421000131},
author = {Suresh Neethirajan and Bas Kemp},
keywords = {Precision Livestock Farming, digitalization, Digital Technologies in Livestock Systems, sensor technology, big data, blockchain, data models, livestock agriculture},
abstract = {As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal’s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.}
}
@article{HARRIGAN2021102246,
title = {Identifying influencers on social media},
journal = {International Journal of Information Management},
volume = {56},
pages = {102246},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102246},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220314456},
author = {Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers},
keywords = {Influencers, Market mavens, Big data, Social media, Twitter},
abstract = {The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.}
}
@article{SMIDT20211018,
title = {The challenge of privacy and security when using technology to track people in times of COVID-19 pandemic},
journal = {Procedia Computer Science},
volume = {181},
pages = {1018-1026},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003306},
author = {Hermanus J Smidt and Osden Jokonya},
keywords = {COVID 19, tracking, society, technology, privacy},
abstract = {Since the start of the Coronavirus disease 2019 (COVID-19) governments and health authorities across the world have find it very difficult in controlling infections. Digital technologies such as artificial intelligence (AI), big data, cloud computing, blockchain and 5G have effectively improved the efficiency of efforts in epidemic monitoring, virus tracking, prevention, control and treatment. Surveillance to halt COVID-19 has raised privacy concerns, as many governments are willing to overlook privacy implications to save lives. The purpose of this paper is to conduct a focused Systematic Literature Review (SLR), to explore the potential benefits and implications of using digital technologies such as AI, big data and cloud to track COVID-19 amongst people in different societies. The aim is to highlight the risks of security and privacy to personal data when using technology to track COVID-19 in societies and identify ways to govern these risks. The paper uses the SLR approach to examine 40 articles published during 2020, ultimately down selecting to the most relevant 24 studies. In this SLR approach we adopted the following steps; formulated the problem, searched the literature, gathered information from studies, evaluated the quality of studies, analysed and integrated the outcomes of studies while concluding by interpreting the evidence and presenting the results. Papers were classified into different categories such as technology use, impact on society and governance. The study highlighted the challenge for government to balance the need of what is good for public health versus individual privacy and freedoms. The findings revealed that although the use of technology help governments and health agencies reduce the spread of the COVID-19 virus, government surveillance to halt has sparked privacy concerns. We suggest some requirements for government policy to be ethical and capable of commanding the trust of the public and present some research questions for future research.}
}
@article{STOGER2021105587,
title = {Legal aspects of data cleansing in medical AI},
journal = {Computer Law & Security Review},
volume = {42},
pages = {105587},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105587},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000601},
author = {Karl Stöger and David Schneeberger and Peter Kieseberg and Andreas Holzinger},
keywords = {Data cleansing, Data quality, Medical AI, Medical devices},
abstract = {Data quality is of paramount importance for the smooth functioning of modern data-driven AI applications with machine learning as a core technology. This is also true for medical AI, where malfunctions due to "dirty data" can have particularly dramatic harmful implications. Consequently, data cleansing is an important part in improving the usability of (Big) Data for medical AI systems. However, it should not be overlooked that data cleansing can also have negative effects on data quality if not performed carefully. This paper takes an interdisciplinary look at some of the technical and legal challenges of data cleansing against the background of European medical device law, with the key message that technical and legal aspects must always be considered together in such a sensitive context.}
}
@article{FAN2021123651,
title = {The future of Internet of Things in agriculture: Plant high-throughput phenotypic platform},
journal = {Journal of Cleaner Production},
volume = {280},
pages = {123651},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123651},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620336969},
author = {Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo},
keywords = {Internet of things in agriculture, Big data, High-throughput phenotype, Data mining},
abstract = {With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype–phenotype–envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.}
}
@article{AHMAD2021125834,
title = {Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities},
journal = {Journal of Cleaner Production},
volume = {289},
pages = {125834},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.125834},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621000548},
author = {Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen},
keywords = {Artificial intelligence, Renewable energy, Energy demand, Decision making, Big data, Energy digitization},
abstract = {The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.}
}
@article{NILASHI2021102630,
title = {Big social data and customer decision making in vegetarian restaurants: A combined machine learning method},
journal = {Journal of Retailing and Consumer Services},
volume = {62},
pages = {102630},
year = {2021},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2021.102630},
url = {https://www.sciencedirect.com/science/article/pii/S096969892100196X},
author = {Mehrbakhsh Nilashi and Hossein Ahmadi and Goli Arji and Khalaf Okab Alsalem and Sarminah Samad and Fahad Ghabban and Ahmed Omar Alzahrani and Ali Ahani and Ala Abdulsalam Alarood},
keywords = {Online reviews, Food quality, Vegetarian friendly restaurants, Text mining, Segmentation},
abstract = {Customers increasingly use various social media to share their opinion about restaurants service quality. Big data collected from social media provides a data platform to improve the service quality of restaurants through customers' online reviews, where online reviews are a trustworthy and reliable source that helps consumers to evaluate food quality. Developing methods for effective evaluation of customer-generated reviews of restaurant services is important. This study develops a new method through effective learning techniques for customer segmentation and their preferences prediction in vegetarian friendly restaurants. The method is developed through text mining (Latent Dirichlet Allocation), cluster analysis (Self Organizing Map) and predictive learning technique (Classification and Regression Trees) to reveal the customer’ satisfaction levels from the service quality in vegetarian friendly restaurants. Based on the obtained results of our experiments on the data vegetarian friendly restaurants in Bangkok, the models constructed by Classification and Regression Trees were able to give an accurate prediction of customers' preferences on the basis of restaurants' quality factors. The results showed that customers’ online reviews analysis can be an effective way for customers segmentation to predict their preferences and help the restaurant managers to set priority instructions for service quality improvements.}
}
@article{ZHANG2021101336,
title = {A framework of energy-consumption driven discrete manufacturing system},
journal = {Sustainable Energy Technologies and Assessments},
volume = {47},
pages = {101336},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101336},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821003465},
author = {Tao Zhang and Weixi Ji and Yongtao Qiu},
keywords = {Energy-efficient optimization, Discrete manufacturing system, Data preprocessing, Data mining},
abstract = {Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.}
}
@incollection{WANG2021295,
title = {Chapter 13 - Artificial Intelligence for Flood Observation},
editor = {Guy J-P. Schumann},
booktitle = {Earth Observation for Flood Applications},
publisher = {Elsevier},
pages = {295-304},
year = {2021},
series = {Earth Observation},
isbn = {978-0-12-819412-6},
doi = {https://doi.org/10.1016/B978-0-12-819412-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194126000134},
author = {Ruo-Qian Wang},
keywords = {artificial intelligence, natural language processing, computer vision, machine learning, Big Data, Internet of Things, crowdsourcing, citizen science, surveillance video},
abstract = {Artificial intelligence (AI) is fundamentally changing our society, benefiting from the big data revolution and dramatical  declination in the Internet of Things (IoT) costs. Flood research and applications will progress with this emerging technology, as AI is creating new flood data sources, enhancing our capability to analyze the data, and improving our accuracy of flood predictions. This chapter introduces the basic concepts of AI and its technical frontier. Using the method of “review of the reviews” with example highlight the emerging AI applications in the field of flood hazards is summarized in terms of the data sources, including crowdsourcing and surveillance camera videos. The use of the AI-enabled big data is also discussed. The opportunities and barriers of this new technology are summarized. At the end of the chapter, the trend and the research gaps are identified in this field.}
}
@article{YANG2021,
title = {Standardization of collection, storage, annotation, and management of data related to medical artificial intelligence},
journal = {Intelligent Medicine},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621001200},
author = {Yahan Yang and Ruiyang Li and Yifan Xiang and Duoru Lin and Anqi Yan and Wenben Chen and Zhongwen Li and Weiyi Lai and Xiaohang Wu and Cheng Wan and Wei Bai and Xiucheng Huang and Qiang Li and Wenrui Deng and Xiyang Liu and Yucong Lin and Pisong Yan and Haotian Lin},
keywords = {Artificial intelligence, Big data, Intelligent medicine, Data collection, Data storage, Data annotation, Data management},
abstract = {Medical artificial intelligence (AI) and big data technology have rapidly advanced in recent years, and they are now routinely used for image-based diagnosis. China has a massive amount of medical data. However, a uniform criteria for medical data quality have yet to be established. Therefore, this review aimed to develop a standardized and detailed set of quality criteria for medical data collection, storage, annotation, and management related to medical AI. This will greatly improve the process of medical data resource sharing and the use of AI in clinical medicine.}
}
@article{CHAKRABORTY2021662,
title = {The role of surrogate models in the development of digital twins of dynamic systems},
journal = {Applied Mathematical Modelling},
volume = {90},
pages = {662-681},
year = {2021},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2020.09.037},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X20305588},
author = {S. Chakraborty and S. Adhikari and R. Ganguli},
keywords = {Digital twin, Vibration, Response, Frequency, Surrogate},
abstract = {Digital twin technology has significant promise, relevance and potential of widespread applicability in various industrial sectors such as aerospace, infrastructure and automotive. However, the adoption of this technology has been slower due to the lack of clarity for specific applications. A discrete damped dynamic system is used in this paper to explore the concept of a digital twin. As digital twins are also expected to exploit data and computational methods, there is a compelling case for the use of surrogate models in this context. Motivated by this synergy, we have explored the possibility of using surrogate models within the digital twin technology. In particular, the use of Gaussian process (GP) emulator within the digital twin technology is explored. GP has the inherent capability of addressing noisy and sparse data and hence, makes a compelling case to be used within the digital twin framework. Cases involving stiffness variation and mass variation are considered, individually and jointly, along with different levels of noise and sparsity in data. Our numerical simulation results clearly demonstrate that surrogate models, such as GP emulators, have the potential to be an effective tool for the development of digital twins. Aspects related to data quality and sampling rate are analysed. Key concepts introduced in this paper are summarised and ideas for urgent future research needs are proposed.}
}
@incollection{SVAB2021274,
title = {Complexity of Patient Data in Primary Care Practice},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {274-282},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11590-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383115909},
author = {Igor Švab},
keywords = {Big data, Digital health, Family medicine, Genomics, Primary care},
abstract = {The chapter deals with the practical implications of big data in clinical practice, especially primary care. Family medicine has always advocated individualized approach to patient care. Medicine is changing rapidly for different reasons. One of the reasons is the development of new technologies which is going to radically change medical practice in the future. One of the key changes will involve the importance and practice of data management. The traditional data management that was based on paper records is being changed to the electronic medical record which offers great potential for patient management. This transition will also give rise to new challenges to the practising physician. We are facing the challenge of new sources of data, their increase and variety. Currently, all these data is stored in different locations and there is no consensus whether one single profession is going to take responsibility for managing the data of the patient. If this is decided, the primary care practice would seem a logical solution. In order to do this would involve challenges to healthcare policy and infrastructure. The big data approach to medical care gives rise to new ethical challenges that we would have to address. Existing and future physicians will have to be educated in order to address all these issues for the benefit of their patients. Nevertheless, the physicians should still remember that even with the vast development of precision medicine, the patient will still be more than just a collection of data.}
}
@article{CHENG2021102938,
title = {Construction of a service quality scale for the online food delivery industry},
journal = {International Journal of Hospitality Management},
volume = {95},
pages = {102938},
year = {2021},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2021.102938},
url = {https://www.sciencedirect.com/science/article/pii/S0278431921000815},
author = {Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen},
keywords = {Online food delivery, Service quality, Big data analytic, OFD service quality scale},
abstract = {The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.}
}
@article{CHEN20211,
title = {Predicting weather-induced delays of high-speed rail and aviation in China},
journal = {Transport Policy},
volume = {101},
pages = {1-13},
year = {2021},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2020.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X20309252},
author = {Zhenhua Chen and Yuxuan Wang and Lei Zhou},
keywords = {High-speed rail, Aviation, Extreme weather event, Machine learning, Big data},
abstract = {High-speed rail (HSR) has become a competitive mode with aviation for medium-distance intercity travel, given the massive deployment of the HSR infrastructure network in China. While the travel experience with both HSR and air has become more convenient, the systems’ operational reliability in terms of punctuality remains a key concern, especially during disruptive events, such as under severe weather conditions. Although previous studies have attempted to investigate the impact of severe weather events on the operational performance of transportation systems, there is still a lack of ability to forecast to what extent the performance of different transportation systems may vary under various conditions. This study develops an integrated modeling framework that allows us to predict the performance of weather-induced delays of different transportation systems, including HSR and aviation. By applying machine-learning methods to real-world transportation performance data, the study examines the robustness of the method, variations of data characteristics and the different applications of the predictive modeling system. Overall, the concept and modeling framework provide important implications for the improvement of transportation system resilience to various severe weather-related disruptions through the understanding of the impact and its predictability of the system performance.}
}
@article{ZARKOWSKY2021260,
title = {Artificial intelligence's role in vascular surgery decision-making},
journal = {Seminars in Vascular Surgery},
volume = {34},
number = {4},
pages = {260-267},
year = {2021},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0895796721000624},
author = {Devin S. Zarkowsky and David P. Stonko},
abstract = {ABSTRACT
Artificial intelligence (AI) is the next great advance informing medical science. Several disciplines, including vascular surgery, use AI-based decision-making tools to improve clinical performance. Although applied widely, AI functions best when confronted with voluminous, accurate data. Consistent, predictable analytic technique selection also challenges researchers. This article contextualizes AI analyses within evidence-based medicine, focusing on “big data” and health services research, as well as discussing opportunities to improve data collection and realize AI's promise.}
}
@article{THOMAS2021101994,
title = {Advances in monitoring and evaluation in low- and middle-income countries},
journal = {Evaluation and Program Planning},
volume = {89},
pages = {101994},
year = {2021},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2021.101994},
url = {https://www.sciencedirect.com/science/article/pii/S0149718921000896},
author = {James C. Thomas and Kathy Doherty and Stephanie Watson-Grant and Manish Kumar},
keywords = {Monitoring and evaluation, Health information systems, Developing countries},
abstract = {Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected “big data” from electronic health records and social media.}
}
@incollection{MCGILVRAY202173,
title = {Chapter 4 - The Ten Steps Process},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {73-252},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000062},
author = {Danette McGilvray},
keywords = {business needs, information environment, information life cycle, data quality dimensions, business impact techniques, root causes, improvement, correction, prevention, controls, monitor, communicate, ethics, change management},
abstract = {This chapter contains the step-by-step guide for creating, assessing, improving, sustaining, and managing information and data quality. Concrete instructions, sample output and templates, and practical advice for executing every step of the Ten Steps Process are provided. A step summary table gives an at-a-glance overview of objectives, purpose, inputs and outputs, techniques and tools, communication, and checkpoints for each step. The Ten Steps Process was designed to be flexible. Suggestions are given to help the reader select and adjust the Ten Steps to various situations, business needs, and data quality issues. The layout allows for quick reference with an easy-to-use format highlighting key concepts and definitions, important checkpoints, communication activities, best practices, and warnings. The experience of actual clients and users of the Ten Steps are highlighted in callout boxes called Ten Steps in Action.}
}
@article{LIOUTAS2021103023,
title = {Enhancing the ability of agriculture to cope with major crises or disasters: What the experience of COVID-19 teaches us},
journal = {Agricultural Systems},
volume = {187},
pages = {103023},
year = {2021},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2020.103023},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X20308842},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari},
keywords = {Agriculture, COVID-19, Major crises, Smart technology, Community marketing, Resilience},
abstract = {The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.}
}
@article{LIU202131,
title = {Objects detection toward complicated high remote basketball sports by leveraging deep CNN architecture},
journal = {Future Generation Computer Systems},
volume = {119},
pages = {31-36},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000303},
author = {Long Liu},
keywords = {Object detection, Sport action recognition, Image recognition, Basketball recognition},
abstract = {The analysis of high-difficulty action recognition technology in basketball is mainly to identify and analyze the physical behavior of basketball players in the video to complete the technical action. The purpose of video recognition is to provide an important guarantee for improving the level of basketball training. The current target recognition technology has achieved some results. It shows that the application of target detection technology in basketball sports scene is of great significance and can improve the effect of sports training. However, traditional sports target recognition is limited by technology and injury, and the analysis of difficult sports skills is limited by the scene, dynamic background and technology, and cannot achieve the desired effect. This is not conducive to the improvement of athletes’ skills. Therefore, this article aims to develop a big data motion target detection system based on deep convolutional neural network for sports difficult motion image recognition. More specifically, we use the high discriminative power of the convolutional neural network to extract images to perform computational preprocessing for the recognition of each human motion image in the video stream. Then, the skeleton recognition algorithm based on LSTM is used to detect the key points of the human body, which is of great significance for modeling different movements. Finally, we developed an object detection system to reconstruct each movement. By selecting five groups of highly difficult actions that are likely to cause sports injuries to conduct experimental research, the results prove the effectiveness of the target detection system we proposed.}
}
@article{MOHDFAIZAL2021106190,
title = {A review of risk prediction models in cardiovascular disease: conventional approach vs. artificial intelligent approach},
journal = {Computer Methods and Programs in Biomedicine},
volume = {207},
pages = {106190},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106190},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721002649},
author = {Aizatul Shafiqah {Mohd Faizal} and T. Malathi Thevarajah and Sook Mei Khor and Siow-Wee Chang},
keywords = {Cardiovascular diseases, Risk prediction, Artificial intelligence, Machine learning, Deep learning},
abstract = {Cardiovascular disease (CVD) is the leading cause of death worldwide and is a global health issue. Traditionally, statistical models are used commonly in the risk prediction and assessment of CVD. However, the adoption of artificial intelligent (AI) approach is rapidly taking hold in the current era of technology to evaluate patient risks and predict the outcome of CVD. In this review, we outline various conventional risk scores and prediction models and do a comparison with the AI approach. The strengths and limitations of both conventional and AI approaches are discussed. Besides that, biomarker discovery related to CVD are also elucidated as the biomarkers can be used in the risk stratification as well as early detection of the disease. Moreover, problems and challenges involved in current CVD studies are explored. Lastly, future prospects of CVD risk prediction and assessment in the multi-modality of big data integrative approaches are proposed.}
}
@article{VALENCA2021100008,
title = {Main challenges and opportunities to dynamic road space allocation: From static to dynamic urban designs},
journal = {Journal of Urban Mobility},
volume = {1},
pages = {100008},
year = {2021},
issn = {2667-0917},
doi = {https://doi.org/10.1016/j.urbmob.2021.100008},
url = {https://www.sciencedirect.com/science/article/pii/S266709172100008X},
author = {Gabriel Valença and Filipe Moura and Ana {Morais de Sá}},
keywords = {Dynamic road space allocation, Big data, Smart cities, Intelligent transportation systems, Information and communication technology, Urban planning},
abstract = {Urban planning has focused on reallocating road space from automobile to more sustainable transport modes in many cities worldwide. Mostly in urban areas, road space (from façade to façade) is highly disputed by different urban activities and functions. Nonetheless, there are varying demand periods during the day in which road space is underutilized due to its static design. Underutilized spaces could be used for other mobility or access purposes to improve efficiency. Sensing road space, using big data and transport demand management tools, may characterize different demand patterns, adapt the road space dynamically and, ultimately, promote efficiency in using a scarce resource, such as urban road space. This approach also reinforces short-term flexibility in urban planning, allowing for better responses to unpredictable events. This paper defines the concept of dynamic road space allocation by discussing the previous literature on dynamic allocation of space. We propose a methodological framework and discuss the technological solutions as well as the many challenges of implementing dynamic road space allocation.}
}
@article{MONDEJAR2021148539,
title = {Digitalization to achieve sustainable development goals: Steps towards a Smart Green Planet},
journal = {Science of The Total Environment},
volume = {794},
pages = {148539},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.148539},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721036111},
author = {Maria E. Mondejar and Ram Avtar and Heyker Lellani Baños Diaz and Rama Kant Dubey and Jesús Esteban and Abigail Gómez-Morales and Brett Hallam and Nsilulu Tresor Mbungu and Chukwuebuka Christopher Okolo and Kumar Arun Prasad and Qianhong She and Sergi Garcia-Segura},
keywords = {Digitalization, Food-water-energy nexus, Internet of things, Geographic information system (GIS), Sustainable development},
abstract = {Digitalization provides access to an integrated network of unexploited big data with potential benefits for society and the environment. The development of smart systems connected to the internet of things can generate unique opportunities to strategically address challenges associated with the United Nations Sustainable Development Goals (SDGs) to ensure an equitable, environmentally sustainable, and healthy society. This perspective describes the opportunities that digitalization can provide towards building the sustainable society of the future. Smart technologies are envisioned as game-changing tools, whereby their integration will benefit the three essential elements of the food-water-energy nexus: (i) sustainable food production; (ii) access to clean and safe potable water; and (iii) green energy generation and usage. It then discusses the benefits of digitalization to catalyze the transition towards sustainable manufacturing practices and enhance citizens' health wellbeing by providing digital access to care, particularly for the underserved communities. Finally, the perspective englobes digitalization benefits by providing a holistic view on how it can contribute to address the serious challenges of endangered planet biodiversity and climate change.}
}
@article{ZHANG2021118910,
title = {Deep spatial representation learning of polyamide nanofiltration membranes},
journal = {Journal of Membrane Science},
volume = {620},
pages = {118910},
year = {2021},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2020.118910},
url = {https://www.sciencedirect.com/science/article/pii/S037673882031485X},
author = {Ziyang Zhang and Yingtao Luo and Huawen Peng and Yu Chen and Rong-Zhen Liao and Qiang Zhao},
keywords = {Nanofiltration, Thin film composite membranes, Feature engineering, Machine learning, Data augmentation, Molecular vibration},
abstract = {Machine learning overfitting caused by data scarcity greatly limits the application of chemical artificial intelligence in membrane materials. As the original data for thin film polyamide nanofiltration membranes is limited, here we propose to extract the natural features of monomer molecular structures and rationally distort them to augment the data availability. This few-shot learning method allows a chemical engineering project to leverage the powerful fit of deep learning without big data at the outset, which is advantageous over traditional machine learning models. The rejection and flux predictions of polyamide nanofiltration membranes are practiced by the molecular augmentation in deep learning. Convergence of loss function indicates that the model is effectively optimized. Correlation coefficients over 0.80 and the mean relative error below 5% prove an accurate prediction of nanofiltration performance. The success of predicting nanofiltration membrane performances is widely instructive for molecule and material science.}
}
@article{ESCOBAR2021748,
title = {Quality 4.0 — Green, Black and Master Black Belt Curricula},
journal = {Procedia Manufacturing},
volume = {53},
pages = {748-759},
year = {2021},
note = {49th SME North American Manufacturing Research Conference (NAMRC 49, 2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.06.085},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001086},
author = {Carlos A. Escobar and Debejyo Chakraborty and Megan McGovern and Daniela Macias and Ruben Morales-Menendez},
keywords = {Quality 4.0, Certification, Smart manufacturing, Artificial intelligence, Big data},
abstract = {Industrial Big Data (IBD) and Artificial Intelligence (AI) are propelling the new era of manufacturing - smart manufacturing. Manufacturing companies can competitively position themselves amongst the most advanced and influential companies by successfully implementing Quality 4.0 practices. Despite the global impact of COVID-19 and the low deployment success rate, industrialization of the AI mega-trend has dominated the business landscape in 2020. Although these technologies have the potential to advance quality standards, it is not a trivial task. A significant portion of quality leaders do not yet have a clear deployment strategy and universally cite difficulty in harnessing such technologies. The lack of people power is one of the biggest challenges. From a career development standpoint, the higher-educated employees (such as engineers) are the most exposed to, and thus affected by, these new technologies. 79% of young professionals have reported receiving training outside of formal schooling to acquire the necessary skills for Industry 4.0. Strategically investing in training is thus important for manufacturing companies to generate value from IBD and AI. Following the path traced by Six Sigma, this article presents a certification curricula for Green, Black, and Master Black Belts. The proposed curriculum combines six areas of knowledge: statistics, quality, manufacturing, programming, learning, and optimization. These areas, along with an ad hoc 7-step problem solving strategy, must be mastered to obtain a certification. Certified professionals will be well positioned to deploy Quality 4.0 technologies and strategies. They will have the capacity to identify engineering intractable problems that can be formulated as machine learning problems and successfully solve them. These certifications are an efficient and effective way for professionals to advance in their career and thrive in Industry 4.0.}
}
@article{TSENG2021108244,
title = {Smart product service system hierarchical model in banking industry under uncertainties},
journal = {International Journal of Production Economics},
volume = {240},
pages = {108244},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108244},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321002206},
author = {Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud},
keywords = {Smart product-service systems, Digital technology, Sustainable innovation, Fuzzy delphi method, Decision-making trial and evaluation laboratory (DEMATEL), Diffusion of innovation theory},
abstract = {This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.}
}
@incollection{BRAVOMERODIO2021191,
title = {Chapter Four - Translational biomarkers in the era of precision medicine},
editor = {Gregory S. Makowski},
series = {Advances in Clinical Chemistry},
publisher = {Elsevier},
volume = {102},
pages = {191-232},
year = {2021},
issn = {0065-2423},
doi = {https://doi.org/10.1016/bs.acc.2020.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065242320300913},
author = {Laura Bravo-Merodio and Animesh Acharjee and Dominic Russ and Vartika Bisht and John A. Williams and Loukia G. Tsaprouni and Georgios V. Gkoutos},
keywords = {Translational biomarkers, Omics, Big data, Artificial intelligence, Clinical trials},
abstract = {In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.}
}
@article{KREGEL2021107083,
title = {Process Mining for Six Sigma: Utilising Digital Traces},
journal = {Computers & Industrial Engineering},
volume = {153},
pages = {107083},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.107083},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220307531},
author = {I. Kregel and D. Stemann and J. Koch and A. Coners},
keywords = {process mining, six sigma, DMAIC, big data analytics, data science, project management},
abstract = {Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma’s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.}
}
@article{RAGUSEO2021103451,
title = {Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness},
journal = {Information & Management},
volume = {58},
number = {4},
pages = {103451},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103451},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000252},
author = {Elisabetta Raguseo and Federico Pigni and Claudio Vitari},
keywords = {Streams of big data, Process efficiency, Product effectiveness, Competitive advantage},
abstract = {Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.}
}
@article{KRISTOFFERSEN2021120957,
title = {Towards a business analytics capability for the circular economy},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120957},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120957},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521003899},
author = {Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li},
keywords = {Digital circular economy, Sustainability, Big data analytics, Competitive advantage, Resource-based view, Expert interviews},
abstract = {Digital technologies are growing in importance for accelerating firms’ circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms’ circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.}
}
@article{FANTKE20212866,
title = {Transition to sustainable chemistry through digitalization},
journal = {Chem},
volume = {7},
number = {11},
pages = {2866-2882},
year = {2021},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2021.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S2451929421004745},
author = {Peter Fantke and Claudio Cinquemani and Polina Yaseneva and Jonathas {De Mello} and Henning Schwabe and Bjoern Ebeling and Alexei A. Lapkin},
keywords = {safe and sustainable by design, artificial intelligence, big data, green transition, sustainable development, machine learning},
abstract = {Summary
Modern chemistry is the backbone of our society, but it is also a major contributor to global environmental pollution and the ongoing climate crisis. The transition toward a sustainable future requires a radical transformation of how chemistry is designed, developed, and used. This represents a “break it or make it” challenge for the chemical industry with significant technology lock-in and high entry barriers to radical innovations. We propose that urgently required systemic changes in chemical industry, research and development (R&D), chemicals assessment and management, and education to advance sustainable chemistry are attainable through increased and more rapid adoption of digitalization and new digital tools. This will enable flexible data exchange, increased transparency of information flows along cross-country chemical, material, and product life cycles, and chemistries that are safe and sustainable by design, addressing the complexity of chemicals-environment-health interactions and lowering the costs of entry into chemical R&D and manufacture, and new, more sustainable and collaborative business models.}
}
@article{BENJELLOUN20211177,
title = {Improving outliers detection in data streams using LiCS and voting},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {10},
pages = {1177-1185},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819301454},
author = {Fatima-Zahra Benjelloun and Ahmed Oussous and Amine Bennani and Samir Belfkih and Ayoub {Ait Lahcen}},
keywords = {Data streams, Outlier detection, High-dimensional data, Big data mining, Intrusion detection},
abstract = {Detecting outliers in real-time is increasingly important for many real-world applications such as detecting abnormal heart activity, intrusions to systems, spams or abnormal credit card transactions. However, detecting outliers in data streams rises many challenges such as high-dimensionality, dynamic data distribution and unpredictable relationships. Our simulations demonstrate that some advanced solutions still show drawbacks. In this paper, first, we improve the capacity to detect outliers of both micro-clusters based algorithms (MCOD) and distance-based algorithms (Abstract-C and Exact-Storm) known for their performance. This is by adding a layer called LiCS that classifies online the K-nearest-neighbors (Knn) of each node based on their evolutionary status. This layer aggregates the results and uses a count threshold to better classify nodes. Experiments on SpamBase datasets confirmed that our technique enhances the accuracy and the precision of such algorithm and helps to reduce the unclassified nodes.Second, we propose a hybrid solution based on iterative majority voting and our LiCS. Experiments on real data proves that it outperforms discussed algorithms in terms of accuracy, precision and sensitivity in detecting outliers. It also minimizes the issue of unclassified instances and consolidate the different outputs of algorithms.}
}
@article{BAHLO2021106365,
title = {Livestock data – Is it there and is it FAIR? A systematic review of livestock farming datasets in Australia},
journal = {Computers and Electronics in Agriculture},
volume = {188},
pages = {106365},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106365},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921003823},
author = {Christiane Bahlo and Peter Dahlhaus},
keywords = {Livestock data quality, Systematic data review, FAIR data, FAIR assessment, Precision livestock farming, Extensive livestock farming},
abstract = {The global adoption of the FAIR principles for scientific data: findable, accessible, interoperable and reusable, has been relatively slow in agriculture, compared to other disciplines. A recent review of the literature showed that the use of precision farming technologies and the development and adoption of open data standards was particularly low in extensive livestock farming. However, a plethora of public datasets exist that have the potential to be used to inform precision farming decision tools. Using extensive livestock farming in Australia as example, we investigate the quantity and quality of datasets available via a systematic dataset review. This systematic review of datasets begins with a search of open data catalogues and querying these to find datasets. Software scripts are developed and used to query the Application Programming Interfaces (APIs) of many of the large data catalogues in Australia, while catalogues without public APIs are queried manually via available web portals. Following the systematic search, a combined list of all datasets is collated and tested for FAIRness and other quality metrics. The contribution of this work is the resulting overview of the state of open datasets within the livestock farming domain on the one hand, but also the development of a systematic dataset search strategy, reusable methods and software scripts.}
}
@article{KUMAR202185,
title = {Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: an ISM Approach},
journal = {Procedia CIRP},
volume = {98},
pages = {85-90},
year = {2021},
note = {The 28th CIRP Conference on Life Cycle Engineering, March 10 – 12, 2021, Jaipur, India},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121000330},
author = {Pramod Kumar and Jaiprakash Bhamu and Kuldip Singh Sangwan},
keywords = {Industry 4.0, interpretive structural modeling, digital manufacturing, barriers, MICMAC analysis},
abstract = {Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.}
}
@article{JIN2021202,
title = {Lidar sheds new light on plant phenomics for plant breeding and management: Recent advances and future prospects},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {171},
pages = {202-223},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620303130},
author = {Shichao Jin and Xiliang Sun and Fangfang Wu and Yanjun Su and Yumei Li and Shiling Song and Kexin Xu and Qin Ma and Frédéric Baret and Dong Jiang and Yanfeng Ding and Qinghua Guo},
keywords = {Lidar, Traits, Phenomics, Breeding, Management, Multi-omics},
abstract = {Plant phenomics is a new avenue for linking plant genomics and environmental studies, thereby improving plant breeding and management. Remote sensing techniques have improved high-throughput plant phenotyping. However, the accuracy, efficiency, and applicability of three-dimensional (3D) phenotyping are still challenging, especially in field environments. Light detection and ranging (lidar) provides a powerful new tool for 3D phenotyping with the rapid development of facilities and algorithms. Numerous efforts have been devoted to studying static and dynamic changes of structural and functional phenotypes using lidar in agriculture. These progresses also improve 3D plant modeling across different spatial–temporal scales and disciplines, providing easier and less expensive association with genes and analysis of environmental practices and affords new insights into breeding and management. Beyond agriculture phenotyping, lidar shows great potential in forestry, horticultural, and grass phenotyping. Although lidar has resulted in remarkable improvements in plant phenotyping and modeling, the synthetization of lidar-based phenotyping for breeding and management has not been fully explored. We identify three main challenges in lidar-based phenotyping development: 1) developing low cost, high spatial–temporal, and hyperspectral lidar facilities, 2) moving into multi-dimensional phenotyping with an endeavor to generate new algorithms and models, and 3) embracing open source and big data.}
}
@article{PANG2021104454,
title = {Prediction of early childhood obesity with machine learning and electronic health record data},
journal = {International Journal of Medical Informatics},
volume = {150},
pages = {104454},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104454},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621000800},
author = {Xueqin Pang and Christopher B. Forrest and Félice Lê-Scherban and Aaron J. Masino},
keywords = {Data quality control, Early childhood obesity, Electronic health record, Machine learning, Prediction},
abstract = {Objective
This study compares seven machine learning models developed to predict childhood obesity from age > 2 to ≤ 7 years using Electronic Healthcare Record (EHR) data up to age 2 years.
Materials and methods
EHR data from of 860,510 patients with 11,194,579 healthcare encounters were obtained from the Children’s Hospital of Philadelphia. After applying stringent quality control to remove implausible growth values and including only individuals with all recommended wellness visits by age 7 years, 27,203 (50.78 % male) patients remained for model development. Seven machine learning models were developed to predict obesity incidence as defined by the Centers for Disease Control and Prevention (age/sex adjusted BMI>95th percentile). Model performance was evaluated by multiple standard classifier metrics and the differences among seven models were compared using the Cochran's Q test and post-hoc pairwise testing.
Results
XGBoost yielded 0.81 (0.001) AUC, which outperformed all other models. It also achieved statistically significant better performance than all other models on standard classifier metrics (sensitivity fixed at 80 %): precision 30.90 % (0.22 %), F1-socre 44.60 % (0.26 %), accuracy 66.14 % (0.41 %), and specificity 63.27 % (0.41 %).
Discussion and conclusion
Early childhood obesity prediction models were developed from the largest cohort reported to date. Relative to prior research, our models generalize to include males and females in a single model and extend the time frame for obesity incidence prediction to 7 years of age. The presented machine learning model development workflow can be adapted to various EHR-based studies and may be valuable for developing other clinical prediction models.}
}
@article{VISSER2021623,
title = {Imprecision farming? Examining the (in)accuracy and risks of digital agriculture},
journal = {Journal of Rural Studies},
volume = {86},
pages = {623-632},
year = {2021},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2021.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0743016721002217},
author = {Oane Visser and Sarah Ruth Sippel and Louis Thiemann},
keywords = {Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data},
abstract = {The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows ‘doing more with less’ through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often ‘precisely inaccurate’, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of ‘ultra-precise’ digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a ‘precision trap’. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of ‘precision traps’ increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging ‘precision divide’: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like ‘imprecision farming’.}
}
@article{KRIEGER2021100511,
title = {Explaining the (non-) adoption of advanced data analytics in auditing: A process theory},
journal = {International Journal of Accounting Information Systems},
volume = {41},
pages = {100511},
year = {2021},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2021.100511},
url = {https://www.sciencedirect.com/science/article/pii/S1467089521000130},
author = {Felix Krieger and Paul Drews and Patrick Velte},
keywords = {Audit digitization, Audit data analytics, Big data, Machine learning, Advanced data analytics in auditing, Audit innovation},
abstract = {Audit firms are increasingly engaging with advanced data analytics to improve the efficiency and effectiveness of external audits through the automation of audit work and obtaining a better understanding of the client’s business risk and thus their own audit risk. This paper examines the process by which audit firms adopt advanced data analytics, which has been left unaddressed by previous research. We derive a process theory from expert interviews which describes the activities within the process and the organizational units involved. It further describes how the adoption process is affected by technological, organizational and environmental contextual factors. Our work contributes to the extent body of research on technology adoption in auditing by using a previously unused theoretical perspective, and contextualizing known factors of technology adoption. The findings presented in this paper emphasize the importance of technological capabilities of audit firms for the adoption of advanced data analytics; technological capabilities within audit teams can be leveraged to support both the ideation of possible use cases for advanced data analytics, as well as the diffusion of solutions into practice.}
}
@article{SCHAEFER2021156,
title = {Framework of Data Analytics and Integrating Knowledge Management},
journal = {International Journal of Intelligent Networks},
volume = {2},
pages = {156-165},
year = {2021},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666603021000208},
author = {Camilla Schaefer and Ana Makatsaria},
keywords = {Data analytics, Knowledge management, Big data, Business intelligence, Data discovery},
abstract = {Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users’ process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.}
}
@article{ALASHHAB2021100059,
title = {Impact of coronavirus pandemic crisis on technologies and cloud computing applications},
journal = {Journal of Electronic Science and Technology},
volume = {19},
number = {1},
pages = {100059},
year = {2021},
note = {Special Section on In Silico Research on Microbiology and Public Health},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2020.100059},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X20300665},
author = {Ziyad R. Alashhab and Mohammed Anbar and Manmeet Mahinderjit Singh and Yu-Beng Leau and Zaher Ali Al-Sai and Sami {Abu Alhayja’a}},
keywords = {Big data privacy, Cloud computing (CC) applications, COVID-19, Digital transformation, Security challenge, Work from home},
abstract = {In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.}
}
@article{XIONG2021386,
title = {Anti-collusion data auction mechanism based on smart contract},
journal = {Information Sciences},
volume = {555},
pages = {386-409},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.10.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520310458},
author = {Wei Xiong and Li Xiong},
keywords = {Data auction mechanism, Anti-collusion, Smart contract, Blockchain, Ethereum},
abstract = {Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.}
}
@incollection{TAT2021395,
title = {Chapter 17 - Ethical and legal challenges},
editor = {Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas},
booktitle = {Machine Learning in Cardiovascular Medicine},
publisher = {Academic Press},
pages = {395-410},
year = {2021},
isbn = {978-0-12-820273-9},
doi = {https://doi.org/10.1016/B978-0-12-820273-9.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202739000178},
author = {Emily Tat and Mark Rabbat},
keywords = {Artificial intelligence, Autonomy, Big data, Black box, Ethics, Informed consent, Liability, Privacy, Safety},
abstract = {As the technology of artificial intelligence (AI) grows in cardiovascular medicine, so do the ethical and legal challenges that come with it. Currently, the medical community is ill-informed of what these challenges entail, and policy and ethical guidelines are lacking. Physicians and policy makers should be informed of these issues to minimize harm and promote patient care. Three overarching themes relating to the data, the algorithms, and the results comprise the foundation of these challenges and will be discussed in this chapter. The introduction of big data raises concern for patient privacy and security, with issues of data quality and inconsistent medical records. There is also risk for biases in the algorithms that could worsen health disparities or skew results for financial gain. Finally, the archetypal “black box” algorithm, questions of legal liability, and what happens when humans and machine disagree are discussed in depth. Ultimately, a code of ethics in the coming integration of AI is needed to ensure the preservation of human rights.}
}
@article{OIEN20211334,
title = {An approach to data structuring and predictive analysis in discrete manufacturing},
journal = {Procedia CIRP},
volume = {104},
pages = {1334-1338},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.224},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011227},
author = {Christian Dalheim Øien and Sebastian Dransfeld},
keywords = {Anomaly Detection, Predictive Maintenance, Discrete Manufacturing, Big Data Analytics, Adaptive Self-learning Systems},
abstract = {In discrete manufacturing the variation in process parameters and duration is often large. Common data storage and analytics systems primarily store data in univariate time series, and when analysing machine components of strongly varying lifetime and behaviour this causes a challenge. This paper presents a data structure and an analysis method for outlier detection which intends to deal with this challenge, as an alternative to predictive maintenance which often requires more data with higher quality than what is available. A case study in aluminium extrusion billet manufacturing is used to demonstrate the approach, predominantly detecting anomalies at the end of a critical component’s lifetime.}
}
@article{LUO2021197,
title = {Towards high quality mobile crowdsensing: Incentive mechanism design based on fine-grained ability reputation},
journal = {Computer Communications},
volume = {180},
pages = {197-209},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421003637},
author = {Zhuangye Luo and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu and Jian Luo},
keywords = {Mobile crowdsensing, Incentive mechanism, Quality-aware, Reputation system},
abstract = {Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers’ fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.}
}
@article{GUO2021107627,
title = {Automated pressure transient analysis: A cloud-based approach},
journal = {Journal of Petroleum Science and Engineering},
volume = {196},
pages = {107627},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.107627},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520306951},
author = {Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed},
keywords = {Pressure transient analysis, Cloud computing, ISIP, Flow regime, G-function, Web-based application},
abstract = {Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.}
}
@article{ZHANG202124,
title = {Thinking on the informatization development of China's healthcare system in the post-COVID-19 era},
journal = {Intelligent Medicine},
volume = {1},
number = {1},
pages = {24-28},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000097},
author = {Ming Zhang and Danyun Dai and Siliang Hou and Wei Liu and Feng Gao and Dong Xu and Yu Hu},
keywords = {Coronavirus disease 2019, Healthcare system, Informatization},
abstract = {With the application of Internet of Things, big data, cloud computing, artificial intelligence, and other cutting-edge technologies, China's medical informatization is developing rapidly. In this paper, we summaried the role of information technology in healthcare sector's battle against the Coronavirus disease 2019 (COVID-19) from the perspectives of early warning and monitoring, screening and diagnosis, medical treatment and scientific research, analyzes the bottlenecks of the development of information technology in the post-COVID-19 era, and puts forward feasible suggestions for further promoting the construction of medical informatization from the perspectives of sharing, convenience, and safety.}
}
@article{VERBOVEN2021113422,
title = {Autoencoders for strategic decision support},
journal = {Decision Support Systems},
volume = {150},
pages = {113422},
year = {2021},
note = {Interpretable Data Science For Decision Making},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113422},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301779},
author = {Sam Verboven and Jeroen Berrevoets and Chris Wuytens and Bart Baesens and Wouter Verbeke},
keywords = {Unsupervised learning, Strategic decision support, Outlier detection},
abstract = {In the majority of executive domains, a notion of normality is involved in most strategic decisions. However, few data-driven tools that support strategic decision-making are available. We introduce and extend the use of autoencoders to provide strategically relevant granular feedback. A first experiment indicates that experts are inconsistent in their decision making, highlighting the need for strategic decision support. Furthermore, using two large industry-provided human resources datasets, the proposed solution is evaluated in terms of ranking accuracy, synergy with human experts, and dimension-level feedback. This three-point scheme is validated using (a) synthetic data, (b) the perspective of data quality, (c) blind expert validation, and (d) transparent expert evaluation. Our study confirms several principal weaknesses of human decision-making and stresses the importance of synergy between a model and humans. Moreover, unsupervised learning and in particular the autoencoder are shown to be valuable tools for strategic decision-making.}
}
@article{MAO2021103052,
title = {Comprehensive strategies of machine-learning-based quantitative structure-activity relationship models},
journal = {iScience},
volume = {24},
number = {9},
pages = {103052},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.103052},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221010208},
author = {Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang},
keywords = {Data analysis in structural biology, Machine learning, Structural biology},
abstract = {Summary
Early quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.}
}
@article{LIU2021589,
title = {Toward intelligent wireless communications: Deep learning - based physical layer technologies},
journal = {Digital Communications and Networks},
volume = {7},
number = {4},
pages = {589-597},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S2352864821000742},
author = {Siqi Liu and Tianyu Wang and Shaowei Wang},
keywords = {Data-driven, Deep learning, Physical layer, Wireless communications},
abstract = {Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.}
}
@article{XIA2021100055,
title = {Aiding pro-environmental behavior measurement by Internet of Things},
journal = {Current Research in Behavioral Sciences},
volume = {2},
pages = {100055},
year = {2021},
issn = {2666-5182},
doi = {https://doi.org/10.1016/j.crbeha.2021.100055},
url = {https://www.sciencedirect.com/science/article/pii/S2666518221000425},
author = {Ziqian Xia and Yurong Liu},
keywords = {Pro-environmental behavior, Internet of Things, Measurement, Big data, Environmental psychology},
abstract = {Promoting pro-environmental behavior is an effective means of reducing carbon emissions at the individual end, but the measurement of behavior has long been a problem for scholars. Especially in environmental psychology community, the complexity of social policies and habitat implies greater difficulty in measuring. Due to the limitations of traditional questionnaire, laboratory, and naturalistic observation methods, environmental psychologists need more realistic, accurate, and cost-effective ways to measure behavior. The rapid development of IoT technology lights up the hope for achieving this goal, and its large-scale popularization will bring great changes to the research community. This paper reviews the current methods and their limitations, proposes a framework for measuring behavior using IoT devices, and points out its future research directions.}
}
@article{HE2021102867,
title = {State-of-health estimation based on real data of electric vehicles concerning user behavior},
journal = {Journal of Energy Storage},
volume = {41},
pages = {102867},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.102867},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21005892},
author = {Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan},
keywords = {Electric vehicles, SOH, User behavior, LWLR, LSTM},
abstract = {State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.}
}
@incollection{FORTSON2021185,
title = {Chapter 10 - From Green Peas to STEVE: Citizen Science Engagement in Space Science},
editor = {Amy Paige Kaminski},
booktitle = {Space Science and Public Engagement},
publisher = {Elsevier},
pages = {185-219},
year = {2021},
isbn = {978-0-12-817390-9},
doi = {https://doi.org/10.1016/B978-0-12-817390-9.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173909000099},
author = {Lucy Fortson},
keywords = {Citizen Science, Crowdsourcing, Machine learning, Volunteer engagement},
abstract = {The past two decades has seen a tremendous rise in citizen science and crowdsourcing techniques as a means to carry out ground-breaking research while at the same time engage the general public in the wonders of space science. This article reviews some of the recent advances made in this realm as well as lessons learned from the unique perspective of the author’s role as a cofounder of the Zooniverse citizen science platform and practicing astrophysics researcher. I briefly describe the factors that led to the recent rise of citizen science including the formation of governance bodies at national and international levels, and the adoption by Federal Agencies within the United States government. I address concerns raised by research colleagues on the validity of citizen science as a research methodology, and then describe several key metrics for the success of citizen science including the link between data quality and publications, and the critical role that motivation and engagement of volunteer participants play in project success. I use the Green Pea galaxies discovered by Galaxy Zoo volunteers and an aurora-like phenomenon known as STEVE discovered by Aurorasaurus volunteers as examples of how, with the right tools and support, non-professional volunteers can make key contributions to space science. I then describe the role that machine learning can play when judiciously teamed with citizen scientists to tackle the ever-growing challenge of big data and close with some reflections on what it takes to support and manage a large platform like the Zooniverse.}
}
@article{ZHANG2021103691,
title = {Perspectives of big experimental database and artificial intelligence in tunnel fire research},
journal = {Tunnelling and Underground Space Technology},
volume = {108},
pages = {103691},
year = {2021},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103691},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820306453},
author = {Xiaoning Zhang and Xiqiang Wu and Younggi Park and Tianhang Zhang and Xinyan Huang and Fu Xiao and Asif Usmani},
keywords = {Big data, Empirical model, Deep learning, Critical event, Smart firefighting},
abstract = {Tunnel fire is one of the most severe global fire hazards and causes a significant amount of economic losses and casualties every year. Over the last 50 years, numerous full-scale and reduced-scale tunnel fire tests, as well as numerical simulations have been conducted to quantify the critical fire events and key parameters to guide the fire safety design of the tunnel. In light of the recent advances in big data and artificial intelligence, this paper aims to establish a database that contains all existing experimental data of tunnel fire, based on an extensive literature review on tunnel fire tests. This tunnel-fire database summarizes seven key parameters of flame, ventilation, and smoke in that is open access at a GitHub site: https://github.com/PolyUFire/Tunnel_Fire_Database. The test conditions, experimental phenomena, and data of each literature work were organized and categorized in a standard format that could be conveniently accessed and continuously updated. Based on this database, machine learning is applied to predict the critical ventilation velocity of a tunnel fire as a demonstration. The review of the current database not only reveals more valuable information and hidden problems in the conventional collection of test data, but also provides new directions in future tunnel fire research. The established database and methodology help promote the application of artificial intelligence and smart firefighting in tunnel fire safety.}
}
@article{MIGLANI202137,
title = {Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review},
journal = {Computer Communications},
volume = {178},
pages = {37-63},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002632},
author = {Arzoo Miglani and Neeraj Kumar},
keywords = {Blockchain, Machine learning, Federated learning, Internet of Things, Deep learning, 5G, 6G},
abstract = {Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.}
}
@article{YAO202154,
title = {Application of artificial intelligence in renal disease},
journal = {Clinical eHealth},
volume = {4},
pages = {54-61},
year = {2021},
issn = {2588-9141},
doi = {https://doi.org/10.1016/j.ceh.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2588914121000083},
author = {Lijing Yao and Hengyuan Zhang and Mengqin Zhang and Xing Chen and Jun Zhang and Jiyi Huang and Lu Zhang},
keywords = {Artificial intelligence (AI), Machine learning (ML), Artificial neural network (ANN), Convolution neural network (CNN), Nephrology},
abstract = {Artificial intelligence (AI) has been applied widely in almost every area of our daily lives, due to the growth of computing power, advances in methods and techniques, and the explosion of data, it also plays a critical role in academic disciplines, medicine is not an exception. AI can augment the intelligence of clinicians in diagnosis, prognosis, and treatment decisions.Kidney disease causes great economic burden worldwide, with both acute kidney injury and chronic kidney disease bringing about high morbidity and mortality. Outstanding challenges in nephrology may be addressed by leveraging big data and AI.In this review, we summarized advances in machine learning (ML), artificial neural network (ANN), convolution neural network (CNN) and deep learning (DL), with a special focus on acute kidney injury (AKI), chronic kidney disease (CKD), end-stage renal disease (ESRD), dialysis, kidney transplantation and nephropathology. AI may not be anticipated to replace the nephrologists’ medical decision-making for now, but instead assisting them in providing optimal personalized therapy for patients.}
}
@article{DUPLESSIS2021100100,
title = {Necessity of making water smart for proactive informed decisive actions: A case study of the upper vaal catchment, South Africa},
journal = {Environmental Challenges},
volume = {4},
pages = {100100},
year = {2021},
issn = {2667-0100},
doi = {https://doi.org/10.1016/j.envc.2021.100100},
url = {https://www.sciencedirect.com/science/article/pii/S2667010021000792},
author = {Anja {du Plessis}},
keywords = {Data quality, Decisive action, Smart water management, Water quality, South Africa, Upper vaal catchment},
abstract = {The need for informed management of water resources has been continuously highlighted worldwide. Societies are increasingly faced with water quality challenges globally which directly translate into multifaceted challenges. South Africa has acknowledged that water is not receiving the attention and status it deserves. Wastage is rife and degradation widespread. The sustainability of South Africa's freshwater resources has reached a critical point and requires decisive action. Vast amounts of water quality data, varying in quality, is available however the seemingly lack of integrative data management has led to reactive planning and questionable decisions. The paper highlights the necessity for making water smart through a case study of the Upper Vaal catchment. The quality of available government data is mostly of an acceptable standard according to the evaluated data dimensions and elements. The practical application of determining hydrological responses to predict possible water quality changes towards land cover change in the Vaal river catchment emphasises that there is suitable data available and highlights the value of Smart Water Management (SWM). SWM can enable improved integrated water resource management by increasing sharing and effective use of real-time data of acceptable quality to promote proactive unambiguous strategies and decisions focused on overall improved water management and the evasion of a future water predicament.}
}
@article{TSAI2021105421,
title = {Sustainable supply chain management trends in world regions: A data-driven analysis},
journal = {Resources, Conservation and Recycling},
volume = {167},
pages = {105421},
year = {2021},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2021.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0921344921000288},
author = {Feng Ming Tsai and Tat-Dat Bui and Ming-Lang Tseng and Mohd Helmi Ali and Ming K. Lim and Anthony SF Chiu},
keywords = {Sustainable supply chain management, Data-driven analysis, Fuzzy Delphi method, Entropy weight method, Fuzzy decision-making trial and evaluation laboratory},
abstract = {This study proposes a data-driven analysis that describes the overall situation and reveals the factors hindering improvement in the sustainable supply chain management field. The literature has presented a summary of the evolution of sustainable supply chain management across attributes. Prior studies have evaluated different parts of the supply chain as independent entities. An integrated systematic assessment is absent in the extant literature and makes it necessary to identify potential opportunities for research direction. A hybrid of data-driven analysis, the fuzzy Delphi method, the entropy weight method and fuzzy decision-making trial and evaluation laboratory is adopted to address uncertainty and complexity. This study contributes to locating the boundary of fundamental knowledge to advance future research and support practical execution. Valuable direction is provided by reviewing the existing literature to identify the critical indicators that need further examination. The results show that big data, closed-loop supply chains, industry 4.0, policy, remanufacturing, and supply chain network design are the most important indicators of future trends and disputes. The challenges and gaps among different geographical regions is offered that provides both a local viewpoint and a state-of-the-art advanced sustainable supply chain management assessment.}
}
@article{FRYE2021142,
title = {Production rescheduling through product quality prediction},
journal = {Procedia Manufacturing},
volume = {54},
pages = {142-147},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001578},
author = {Maik Frye and Dávid Gyulai and Júlia Bergmann and Robert H. Schmitt},
keywords = {Machine Learning, Production Scheduling, Product Quality Prediction, Data Quality},
abstract = {In production management, efficient scheduling is key towards smooth and balanced production. Scheduling can be well-supported by real-time data acquisition systems, resulting in decisions that rely on actual or predicted status of production environment and jobs in progress. Utilizing advanced monitoring systems, prediction-based rescheduling method is proposed that can react on in-process scrap predictions, performed by machine learning algorithms. Based on predictions, overall production can be rescheduled with higher efficiency, compared to rescheduling after completion of the whole machining process with realization of scrap. Series of numerical experiments are presented to demonstrate potentials in prediction-based rescheduling, with early-stage scrap detection.}
}
@article{RAMSINGH2021107423,
title = {An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm},
journal = {Applied Soft Computing},
volume = {108},
pages = {107423},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107423},
url = {https://www.sciencedirect.com/science/article/pii/S156849462100346X},
author = {J. Ramsingh and V. Bhuvaneswari},
keywords = {Fuzzy classifier, MDBSCAN, MapReduce, Hadoop, Diabetes mellitus},
abstract = {In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people’s food habits, physical activity are extracted from social media using the API’s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.}
}
@article{PEER20212162,
title = {Developing and evaluating a pediatric asthma severity computable phenotype derived from electronic health records},
journal = {Journal of Allergy and Clinical Immunology},
volume = {147},
number = {6},
pages = {2162-2170},
year = {2021},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2020.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0091674920324052},
author = {Komal Peer and William G. Adams and Aaron Legler and Megan Sandel and Jonathan I. Levy and Renée Boynton-Jarrett and Chanmin Kim and Jessica H. Leibler and M. Patricia Fabian},
keywords = {Asthma, electronic health records, big data, respiratory function tests, selection bias, health care disparities, delivery of health care, observer variation, National Heart, Lung, and Blood Institute (US), pediatrics},
abstract = {Background
Extensive data available in electronic health records (EHRs) have the potential to improve asthma care and understanding of factors influencing asthma outcomes. However, this work can be accomplished only when the EHR data allow for accurate measures of severity, which at present are complex and inconsistent.
Objective
Our aims were to create and evaluate a standardized pediatric asthma severity phenotype based in clinical asthma guidelines for use in EHR-based health initiatives and studies and also to examine the presence and absence of these data in relation to patient characteristics.
Methods
We developed an asthma severity computable phenotype and compared the concordance of different severity components contributing to the phenotype to trends in the literature. We used multivariable logistic regression to assess the presence of EHR data relevant to asthma severity.
Results
The asthma severity computable phenotype performs as expected in comparison with national statistics and the literature. Severity classification for a child is maximized when based on the long-term medication regimen component and minimized when based only on the symptom data component. Use of the severity phenotype results in better, clinically grounded classification. Children for whom severity could be ascertained from these EHR data were more likely to be seen for asthma in the outpatient setting and less likely to be older or Hispanic. Black children were less likely to have lung function testing data present.
Conclusion
We developed a pragmatic computable phenotype for pediatric asthma severity that is transportable to other EHRs.}
}
@incollection{LEUNG2021197,
title = {Chapter 13 - A support vector machine–based voice disorders detection using human voice signal},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {197-208},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000140},
author = {Pak Ho Leung and Kwok Tai Chui and Kenneth Lo and Patricia Ordóñez {de Pablos}},
keywords = {Artificial intelligence, big data, human voice, imbalanced classification, machine learning, medical screening, smart city, smart healthcare, support vector machine, voice disorders},
abstract = {Voice disorders are common diseases; most of the people have had experienced in their life. Voice disorder sufferers are usually not seeking medical consultation attributable to time-consuming and costly medical expenditure. Recently, researchers have proposed various machine learning algorithms for rapid detection of voice disorders based on the analysis of human voice. In this chapter, we have taken the pronunciation of vowel /a/ as the input of support vector machine algorithm. The research problem is formulated as binary classification which output will be either healthy or pathological status. Our work achieves an accuracy of 69.3% (sensitivity of 83.3% and specificity of 33.3%) which improves by 6.4%–19.3% compared with existing works. The implication of research work suggests tackling the imbalanced classification by adding penalty or generating new training data to class of smaller size. Everybody could contribute the voice signal of vowel /a/ and serving as big data pool.}
}
@article{MARTINEZ2021100183,
title = {Data Science Methodologies: Current Challenges and Future Approaches},
journal = {Big Data Research},
volume = {24},
pages = {100183},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2214579620300514},
author = {Iñigo Martinez and Elisabeth Viles and Igor {G. Olaizola}},
keywords = {Data science, Big data, Data science methodology, Project life-cycle, Organizational impacts, Knowledge management},
abstract = {Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.}
}
@article{GLENNON2021100516,
title = {Challenges in modeling the emergence of novel pathogens},
journal = {Epidemics},
volume = {37},
pages = {100516},
year = {2021},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2021.100516},
url = {https://www.sciencedirect.com/science/article/pii/S1755436521000621},
author = {Emma E. Glennon and Marjolein Bruijning and Justin Lessler and Ian F. Miller and Benjamin L. Rice and Robin N. Thompson and Konstans Wells and C. Jessica E. Metcalf},
keywords = {Immune landscape, Genotype to phenotype map, Big data, Data integration, Fundamental theory, Health system functioning},
abstract = {The emergence of infectious agents with pandemic potential present scientific challenges from detection to data interpretation to understanding determinants of risk and forecasts. Mathematical models could play an essential role in how we prepare for future emergent pathogens. Here, we describe core directions for expansion of the existing tools and knowledge base, including: using mathematical models to identify critical directions and paths for strengthening data collection to detect and respond to outbreaks of novel pathogens; expanding basic theory to identify infectious agents and contexts that present the greatest risks, over both the short and longer term; by strengthening estimation tools that make the most use of the likely range and uncertainties in existing data; and by ensuring modelling applications are carefully communicated and developed within diverse and equitable collaborations for increased public health benefit.}
}
@article{MERHI2021121180,
title = {Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121180},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121180},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006132},
author = {Mohammad I. Merhi},
keywords = {Data intelligence, Systems implementation, Data analytics, Success factors, Public sector, AHP},
abstract = {This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.}
}
@article{CANHOTO2021441,
title = {Leveraging machine learning in the global fight against money laundering and terrorism financing: An affordances perspective},
journal = {Journal of Business Research},
volume = {131},
pages = {441-452},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306640},
author = {Ana Isabel Canhoto},
keywords = {Big data, Artificial intelligence, Machine learning, Algorithm, Customer profiling, Financial services, Anti-money laundering, United Nations, Sustainable development goals},
abstract = {Financial services organisations facilitate the movement of money worldwide, and keep records of their clients’ identity and financial behaviour. As such, they have been enlisted by governments worldwide to assist with the detection and prevention of money laundering, which is a key tool in the fight to reduce crime and create sustainable economic development, corresponding to Goal 16 of the United Nations Sustainable Development Goals. In this paper, we investigate how the technical and contextual affordances of machine learning algorithms may enable these organisations to accomplish that task. We find that, due to the unavailability of high-quality, large training datasets regarding money laundering methods, there is limited scope for using supervised machine learning. Conversely, it is possible to use reinforced machine learning and, to an extent, unsupervised learning, although only to model unusual financial behaviour, not actual money laundering.}
}
@article{WANG202183,
title = {The groundwater potential assessment system based on cloud computing: A case study in islands region},
journal = {Computer Communications},
volume = {178},
pages = {83-97},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002528},
author = {Daqing Wang and Haoli Xu and Yue Shi and Zhibin Ding and Zhengdong Deng and Zhixin Liu and Xingang Xu and Zhao Lu and Guangyuan Wang and Zijian Cheng and Xiaoning Zhao},
keywords = {Big data, Cloud computing, Remote sensing, Groundwater potential, Bedrock islands},
abstract = {Today’s intelligent system based on cloud computing platform can realize “unattended”, real-time monitoring observation and forecast by remote sensing. In order to import the development and efficiency of groundwater potential assessment(GPA) by remote sensing, the cloud computing platform was tried to use in the computing GPA. In this study, the Pearl River Estuary islands region(China) was selected as the study area. The slope, aspect, water-density(WD), land surface temperature(LST), NDVI and NDWI were used as the GPA indexes, which have been used before. Considering the similar geological and geomorphological conditions of the islands area, the analytic hierarchy process (AHP) method and these indexes can be used to assess GPA in the remote sensing cloud computing platform efficiently and conveniently. The results of the assessment were in good agreement with the actual hydrogeological map. Besides, the other intelligent algorithms can also be applied in this platform. Finally, this study realized the rapid “unattended” and “real-time monitoring” groundwater potential assessment, and carried out a multi-level GPA. It will be of certain reference significance to the exploitation of groundwater in the island area, which has realized convenient and efficient processing and analysis of data anytime and anywhere. At the same time, attention must be paid to the security of data and the maintenance of the system.}
}
@article{HASSANI2021121111,
title = {The science of statistics versus data science: What is the future?},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121111},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121111},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521005448},
author = {Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag Øivind Madsen},
keywords = {Perspective, Science, Statistics, Data science, Similarities, Differences, Pragmatism},
abstract = {The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.}
}
@article{EICHSTADT2021100232,
title = {Metrology for the digital age},
journal = {Measurement: Sensors},
volume = {18},
pages = {100232},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421001951},
author = {Sascha Eichstädt and Anke Keidel and Julia Tesch},
keywords = {Digital transformation, Digital certificates, Systemic metrology, Big data, Industry 4.0, Open science, FAIR},
abstract = {Based on digital technologies, big data, artificial intelligence and machine-readable information, the digital transformation rapidly changes society, industries, and economies. Metrology as a central element of international trade, for confidence in measurements and part of the quality infrastructure is facing several challenges and opportunities in these developments. In this contribution we discuss some of the key challenges and a potential future role of metrology in the digital age. We address metrological principles for confidence in data and Algorithms, cyber-physical systems, FAIR data and metrology, and the role of metrology in the digital transformation in the quality infrastructure.}
}
@article{GAHA2021216,
title = {Towards the implementation of the Digital Twin in CMM inspection process: opportunities, challenges and proposals},
journal = {Procedia Manufacturing},
volume = {54},
pages = {216-221},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001694},
author = {Raoudha Gaha and Alexandre Durupt and Benoit Eynard},
keywords = {Digital Twin, CMM, inspection, Model-based-defintion, Digital thread},
abstract = {The use of Digital Twin (DT) is adopted by manufacturers and have positive effects on the product manufacturing process. The aim of this paper is to define a Coordinate Measuring Machine (CMM) inspection DT model, based on inspection process digitalized functionalities, from one side, and Industry 4.0 opportunities (Digital thread, Big data, etc.), from the other side. A review about DT definition, is firstly presented. Secondly, we review related studies based on existing DT orientations and usages for CMM inspection. Thirdly, challenges related to the variation management are presented. Finally, a discussion about possible DT functionalities and opportunities is conducted then a CMM inspection DT model is presented.}
}