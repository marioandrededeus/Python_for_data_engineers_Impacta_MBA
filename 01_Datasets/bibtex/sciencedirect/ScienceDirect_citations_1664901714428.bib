@article{LI2021441,
title = {A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong},
journal = {Environmental Science & Policy},
volume = {124},
pages = {441-450},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121001714},
author = {Victor O.K. Li and Jacqueline C.K. Lam and Yang Han and Kenyon Chow},
keywords = {Air Pollution Monitoring, Health Management, Artificial Intelligence, Big Data, PM, Personalization, Smart Behavioural Intervention, Health and Well-being Improvement},
abstract = {All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.}
}
@article{BELL2021453,
title = {Exploring future challenges for big data in the humanitarian domain},
journal = {Journal of Business Research},
volume = {131},
pages = {453-468},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306172},
author = {David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan},
keywords = {Big data, Veracity, Granularity, Heterogeneous datasets, Humanitarian, Value},
abstract = {This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 “Partnerships for the Goals”. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous ‘exhaust trail’ of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality – that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.}
}
@article{ZHANG202134,
title = {Big data and human resource management research: An integrative review and new directions for future research},
journal = {Journal of Business Research},
volume = {133},
pages = {34-50},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002563},
author = {Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang},
keywords = {Human resource management research, Big data, Integrative review, Inductive and deductive paradigms},
abstract = {The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as “Inductive (Prediction paradigm: Data mining/Theory building) vs. Deductive (Explanation paradigm: Theory testing)”. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.}
}
@article{IBRAHIM2021121171,
title = {The convergence of big data and accounting: innovative research opportunities},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121171},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121171},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006041},
author = {Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat},
keywords = {Big data, Analytics, Accounting, Data science, Business intelligence},
abstract = {This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.}
}
@article{WANG202116,
title = {A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {16-32},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301990},
author = {Pei Wang and Ming Luo},
keywords = {Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing},
abstract = {Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.}
}
@incollection{PLOTKIN2021245,
title = {Chapter 10 - Big Data Stewardship and Data Lakes},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {245-255},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000103},
author = {David Plotkin},
keywords = {Big data, data lake, unstructured data, zone},
abstract = {Big Data Governance and big data stewardship are not so different from what we’ve been doing prior to the advent of big data and data lakes. Most of the same roles still need to be filled, and accountability for making data decisions is even more important because of the vast quantity of data, the many ways in which it can be changed, and increased consequences of “getting it wrong” due to not only the large quantities of data and metadata, but also the speed at which the data can change.}
}
@incollection{VALEEV2021209,
title = {Chapter 6 - Big data analytics and process safety},
editor = {Sagit Valeev and Natalya Kondratyeva},
booktitle = {Process Safety and Big Data},
publisher = {Elsevier},
pages = {209-270},
year = {2021},
isbn = {978-0-12-822066-5},
doi = {https://doi.org/10.1016/B978-0-12-822066-5.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220665000017},
author = {Sagit Valeev and Natalya Kondratyeva},
keywords = {Analytics, Machine learning, Prediction, Clustering, Classification, Regression, Time series, Text analysis, Image analysis},
abstract = {The chapter discusses the basics of big data analytics and the features of using analytical models in the field of process safety and risk management. The definition and basic principles of data analytics are necessary to understand the analytical techniques. The requirements for input data and the properties of analytical models are important for effective analytics. The concept, basic components, and varieties of machine learning are discussed. We consider such basic machine learning algorithms as clustering, classification, and regression. As advanced methods of data analytics, time series analysis methods, text analysis, and image analysis are proposed. Examples of the application of data analytics for risk management in the framework of process safety are considered.}
}
@article{HU2021107994,
title = {A blockchain-based trading system for big data},
journal = {Computer Networks},
volume = {191},
pages = {107994},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.107994},
url = {https://www.sciencedirect.com/science/article/pii/S138912862100116X},
author = {Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng},
keywords = {Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward},
abstract = {Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users’ application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.}
}
@article{ARFANUZZAMAN2021100127,
title = {Harnessing artificial intelligence and big data for SDGs and prosperous urban future in South Asia},
journal = {Environmental and Sustainability Indicators},
volume = {11},
pages = {100127},
year = {2021},
issn = {2665-9727},
doi = {https://doi.org/10.1016/j.indic.2021.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2665972721000283},
author = {Md. Arfanuzzaman},
keywords = {Artificial intelligence, Big data, Climate resilience, Data infrastructure, South Asia, SDG, Technological readiness, Urban transformation},
abstract = {Artificial intelligence (AI) and big data solutions are currently being utilized to offer low cost and efficient solutions in solving pressing urban socio-economic and environmental problems globally. The study found big data and AI have the potentiality to solve the common urban problems in South Asia and upsurge the efficiency of urban industries, increase competitiveness and productivity of the human and natural resources, reduce the cost of urban service delivery, and build climate resilience. The study has assessed the current AI and big data initiatives and technologies in mitigating the urban development challenges and their potentiality for scaling up in South Asian cities. The study also examined the latest innovations in AI and big data solutions for SDG monitoring and implementation in South Asia and their implication for transformational change. The study suggested that South Asia can harness the maximum benefit of AI and big data technologies by building big data and associated IT infrastructure, advancing research and innovations with regional cooperation, enhancing technological readiness, and eliminating week enabling conditions.}
}
@article{TAMYM2021102,
title = {A big data based architecture for collaborative networks: Supply chains mixed-network},
journal = {Computer Communications},
volume = {175},
pages = {102-111},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001924},
author = {Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri}},
keywords = {Big data architecture, Collaborative networks, Enterprises network, Supply chain network, Flexibility, Robustness},
abstract = {Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.}
}
@incollection{QING2021181,
title = {Chapter 9 - Global Practice of AI and Big Data in Oil and Gas Industry},
editor = {Patrick Bangert},
booktitle = {Machine Learning and Data Science in the Oil and Gas Industry},
publisher = {Gulf Professional Publishing},
pages = {181-210},
year = {2021},
isbn = {978-0-12-820714-7},
doi = {https://doi.org/10.1016/B978-0-12-820714-7.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207147000091},
author = {Wu Qing},
keywords = {artificial intelligence, big data, digital core, digital rock physics, multiphase flow physics information, oil recovery, molecule advanced planning and scheduling, system (MAPS), crude oil selection, crude oil property prediction, process optimization, CCR unit, FCC unit, ethylene cracking unit, correlation analysis, the anomaly detection, parameters optimization analysis, prediction analysis, equipment preventive maintenance, distributed equipment health monitoring system, time series, early warning system for equipment fault monitoring, residual life prediction for equipment},
abstract = {This chapter introduces typical cases of artificial intelligence and big data application in oil and gas industry. In the upstream field, it introduces how to combine digital rock physics with big data and AI to optimize recovery efficiency. Digital core (also known as digital petrophysics—DRP) technology enables more reliable physical information about pore-scale multiphase flows to determine the cause of low recovery and provides new ways for different injection solutions to improve reservoir performance. Combined with digital rock technology and AI, we can integrate the characteristics of digital rock databases into logging and well data, and use a variety of advanced classification techniques to identify the remaining oil and gas potential. Through the multi-phase flow simulation, the multi-scale model can predict the best injection method for maximum recovery under different conditions and propose possible solutions to optimize crude oil production. In the downstream field, the application of AI and big data analysis in planning and scheduling systems, process unit optimization, preventive maintenance of equipment, and other aspects is introduced. Among them, the molecular-level advanced planning and scheduling system (MAPS) can realize the cost performance measurement under different production schemes for potential types of processable crude oil, which is conducive to more accurate selection of crude oil and prediction of crude oil properties. In addition, the whole process simulation can be used to understand the product quality changes under different crude oil blending schemes and different unit operating conditions, which is conducive to timely adjusting the product blending schemes according to economic benefits or ex-factory demands. The operation conditions of secondary units and even the properties of mixed crude oil can be deduced according to different product quality requirements. In the process of optimization, the Continuous Catalytic Reforming (CCR) unit in the refinery process, for example, introduces the application of large data analysis, including correlation analysis, single index detection, multidimensional data anomaly detection, and the parameters of the single objective optimization, a multi-objective parameter optimization analysis, unstructured data analysis, and forecast analysis based on material properties. Good practices in CCR units have also been extended to other oil refining and chemical units, such as Fluid Catalytic Cracking (FCC) and ethylene cracking. In terms of equipment preventive maintenance, it introduced how to integrated application of Internet of things, deep machine learning, knowledge map and other technology to build real-time and on-line distributed equipment health monitoring and early warning system, for early detection of equipment hidden danger, early warning, early treatment of providing effective means, guarantee equipment run healthy and stable for a long period of time, to reduce unplanned downtime losses. In particular, the establishment of equipment prediction model based on time series and AI can realize effective monitoring and early warning of equipment faults such as shaft displacement, shaft fracture, shell cracking, power overload, and prediction of equipment remaining life.}
}
@article{SHAMIM2021106777,
title = {Big data management capabilities in the hospitality sector: Service innovation and customer generated online quality ratings},
journal = {Computers in Human Behavior},
volume = {121},
pages = {106777},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106777},
url = {https://www.sciencedirect.com/science/article/pii/S074756322100100X},
author = {Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah},
keywords = {Big data management, Dynamic capabilities, Service innovation, Knowledge creation, Customer generated online quality rating, Hospitality},
abstract = {Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations’ capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.}
}
@article{ZHAO2021101196,
title = {Prediction model of ecological environmental water demand based on big data analysis},
journal = {Environmental Technology & Innovation},
volume = {21},
pages = {101196},
year = {2021},
issn = {2352-1864},
doi = {https://doi.org/10.1016/j.eti.2020.101196},
url = {https://www.sciencedirect.com/science/article/pii/S2352186420314966},
author = {Lihong Zhao},
keywords = {Big data analysis, Ecological environment, Water demand, Prediction},
abstract = {The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.}
}
@article{LEON2021100253,
title = {Enhancing Precision Medicine: A Big Data-Driven Approach for the Management of Genomic Data},
journal = {Big Data Research},
volume = {26},
pages = {100253},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000708},
author = {Ana León and Óscar Pastor},
keywords = {Big Data, Genomics, Computer science, Theory and methods},
abstract = {The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.}
}
@article{PONTORIERO2021106239,
title = {Automated Data Quality Control in FDOPA brain PET Imaging using Deep Learning},
journal = {Computer Methods and Programs in Biomedicine},
volume = {208},
pages = {106239},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106239},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003138},
author = {Antonella D. Pontoriero and Giovanna Nordio and Rubaida Easmin and Alessio Giacomel and Barbara Santangelo and Sameer Jahuar and Ilaria Bonoldi and Maria Rogdaki and Federico Turkheimer and Oliver Howes and Mattia Veronese},
keywords = {FDOPA, PET, quality control, QC, convolutional neural networks},
abstract = {ABSTRACT
Introduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 ± 0.01, accuracy for SNR: 0.69 ± 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.}
}
@article{GU2021132,
title = {SparkDQ: Efficient generic big data quality management on distributed data-parallel computation},
journal = {Journal of Parallel and Distributed Computing},
volume = {156},
pages = {132-147},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521001246},
author = {Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang},
keywords = {Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data},
abstract = {In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.}
}
@article{HUANG2021101712,
title = {Analytics of location-based big data for smart cities: Opportunities, challenges, and future directions},
journal = {Computers, Environment and Urban Systems},
volume = {90},
pages = {101712},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101712},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001198},
author = {Haosheng Huang and Xiaobai Angela Yao and Jukka M. Krisp and Bin Jiang},
keywords = {Location-based big data (LocBigData), Smart cities, Data analytics, State-of-the-art review, Research agenda, Geodata},
abstract = {The growing ubiquity of location/activity sensing technologies and location-based services (LBS) has led to a large volume and variety of location-based big data (LocBigData), such as location tracking or sensing data, social media data, and crowdsourced geographic information. The increasing availability of such LocBigData has created unprecedented opportunities for research on urban systems and human environments in general. In this article, we first review the common types of LocBigData: mobile phone network data, GPS data, Location-based social media data, LBS usage/log data, smart card travel data, beacon log data (WiFi or Bluetooth), and camera imagery data. Secondly, we describe the opportunities fueled by LocBigData for the realization of smart cities, mainly via answering questions ranging from “what happened” and “why did it happen” to “what's likely to happen in the future” and “what to do next”. Thirdly, pitfalls of dealing with LocBigData are summarized, such as high volume/velocity/variety; non-random sampling; messy and not clean data; and correlations rather than causal relationships. Finally, we review the state-of-the-art research trends in this field, and conclude the article with a list of open research challenges and a research agenda for LocBigData research to help achieve the vision of smart and sustainable cities.}
}
@article{VETRO2021101619,
title = {A data quality approach to the identification of discrimination risk in automated decision making systems},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101619},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101619},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000551},
author = {Antonio Vetrò and Marco Torchiano and Mariachiara Mecati},
keywords = {Automated decision making, Data ethics, Data quality, Data bias, Algorithm fairness, Digital policy, Digital governance},
abstract = {Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation – or even amplification – of bias in the input data of ADM systems.}
}
@article{TRUJILLO2021101911,
title = {Conceptual modeling in the era of Big Data and Artificial Intelligence: Research topics and introduction to the special issue},
journal = {Data & Knowledge Engineering},
volume = {135},
pages = {101911},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101911},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000380},
author = {Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey},
keywords = {Conceptual modeling, Big Data, Machine learning, Artificial Intelligence},
abstract = {Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.}
}
@article{LV2021103298,
title = {Detecting the true urban polycentric pattern of Chinese cities in morphological dimensions: A multiscale analysis based on geospatial big data},
journal = {Cities},
volume = {116},
pages = {103298},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2021.103298},
url = {https://www.sciencedirect.com/science/article/pii/S0264275121001980},
author = {Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng},
keywords = {Polycentricity, Urban centers, Multi-scale, Street blocks, Geospatial big data, Chinese cities},
abstract = {With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.}
}
@incollection{SHARMA202137,
title = {Chapter 2 - Deep learning in big data and data mining},
editor = {Vincenzo Piuri and Sandeep Raj and Angelo Genovese and Rajshree Srivastava},
booktitle = {Trends in Deep Learning Methodologies},
publisher = {Academic Press},
pages = {37-61},
year = {2021},
series = {Hybrid Computational Intelligence for Pattern Analysis},
isbn = {978-0-12-822226-3},
doi = {https://doi.org/10.1016/B978-0-12-822226-3.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222263000027},
author = {Deepak Kumar Sharma and Bhanu Tokas and Leo Adlakha},
keywords = {Aspect extraction, Big data, CRISP-DM, Customer relations, Data mining, Data visualization, Deep learning, Distributed computing, LSTM, Machine learning},
abstract = {The growth of the digital age has led to a colossal leap in data generated by the average user. This growing data has several applications: businesses can use it to give a more personalized touch to their services, governments can use it to better allocate their funds, and companies can utilize it to select the best candidates for a job. While these applications may seem extremely enticing, there are a couple of problems that must be solved first, namely, data collection and extraction of useful patterns from the data. The disciplines of data mining and big data deal with these problems, respectively. But, as we have already discussed, the amount of data is so vast that any manual approach is extremely time intensive and costly. Thus this limits the potential outcomes from this data. This problem has been solved by the application of deep learning. Deep learning has allowed us to automate processes that were not only time intensive but also mentally arduous. It has achieved better than human accuracy in several types of discriminative and recognition tasks making it a viable alternative to inefficient human labor. Deep learning plays a vital role in this analysis and has enabled several businesses to comprehend customer needs and accordingly improve their own services, thus giving them the opportunity to outdo their competitors. Similarly, deep learning has also been instrumental in analyzing the trends and associations of securities in the financial market. It has even helped to create fraud detection and loan underwriting applications, which have contributed to making financial institutions more transparent and efficient. Apart from directly improving the efficiency in these fields, deep learning has also been instrumental in improving the fields of data mining and big data. Machine learning algorithms can actually utilize the existing data to predict the unknowns, including future trends in data. Due to its potential applications the field of machine learning is deeply interconnected with data mining. Nevertheless, machine learning algorithms are often heavily dependent on the availability of huge datasets to ensure useful accuracy. Deep learning algorithms have allowed the different components of data (i.e., multimedia data) in the data mining process itself to be identified. Similarly, semantic indexing and tagging algorithms have allowed the processes of big data to speed up. In this chapter, we will discuss the applications of deep learning in these fields and give a brief overview of the concepts involved.}
}
@article{RHAHLA2021102896,
title = {Guidelines for GDPR compliance in Big Data systems},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102896},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102896},
url = {https://www.sciencedirect.com/science/article/pii/S221421262100123X},
author = {Mouna Rhahla and Sahar Allegue and Takoua Abdellatif},
keywords = {The General Data Protection Regulation (GDPR), Big Data analytics, Privacy, Security},
abstract = {The implementation of the GDPR that aims at protecting European citizens’ privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR’s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.}
}
@article{SMALEC20215156,
title = {Big Data as a tool helpful in communication management},
journal = {Procedia Computer Science},
volume = {192},
pages = {5156-5165},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.293},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020329},
author = {Agnieszka Smalec},
keywords = {Big Data, marketing communication, management, data processing, collection, communication management},
abstract = {The boundaries between the online and offline worlds have become irretrievably blurred, especially as mobile devices have proliferated. As a result, more and more activities are transferred to the Internet. Every activity in the network leaves a trace, which is why the volume of available data is growing rapidly. The amount of increasing information affects all market participants, and the necessity to constantly collect and process large amounts of data becomes an everyday reality. The aim of the article is to present the concept of big data and to indicate examples of the use of big data to manage marketing communication with the environment. It should be emphasized that not only data transfer devices, but also human interaction contribute to the creation of very large data sets. Acquiring and correctly interpreting them plays an important role in market entities in terms of management, including communication management. Contemporary multi-directional communication, including communication in a hypermedia environment, creates new challenges and threats. The article was prepared based on a literature review, research reports and an analysis of secondary sources. It also outlines the practical implications. The considerations provided are the basis for further activities and empirical research.}
}
@article{LI2021241,
title = {AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making},
journal = {Environmental Science & Policy},
volume = {125},
pages = {241-246},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121002471},
author = {Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui},
abstract = {AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.}
}
@article{MISHRA20216864,
title = {An efficient approach for manufacturing process using Big data analytics},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {6864-6866},
year = {2021},
note = {International Conference on Advances in Design, Materials and Manufacturing},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.146},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321037226},
author = {Devendra Kumar Mishra and Arvind Kumar Upadhyay and Sanjiv Sharma},
keywords = {Manufacturing process, Bigdata, Structured data, Unstructured data},
abstract = {Manufacturing is a technique that produce finished goods after taking supplies, raw materials and ingredients. Manufacturing process is frequently used to produce food, chemicals and other things those have very important place in human’s life. This manufacturing process involve data for analysis and management of the process, but in current scenario the data that are generated by the process is increasing day by day. This huge amount of data in known as big data. Big data is difficult to handle by traditional data management tools. Data that are generated by the manufacturing process collected by the logs records and may be as structured or unstructured. Generally analysis is performed by structured data. Unstructured data also provide good insights in the manufacturing process if analysed in proper manner. This paper involved an efficient approach of big data analysis for manufacturing process.}
}
@article{GODOY2021100079,
title = {Transformations of trust in society: A systematic review of how access to big data in energy systems challenges Scandinavian culture},
journal = {Energy and AI},
volume = {5},
pages = {100079},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100079},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000331},
author = {Jaqueline de Godoy and Kathrin Otrel-Cass and Kristian Høyer Toft},
keywords = {Surveillance capitalism, Smart meters, Energy transition, Trust, Data Ethics, Big Data},
abstract = {In the era of information technology and big data, the extraction, commodification, and control of personal information is redefining how people relate and interact. However, the challenges that big data collection and analytics can introduce in trust-based societies, like those of Scandinavia, are not yet understood. For instance, in the energy sector, data generated through smart appliances, like smart metering devices, can have collateral implications for the end-users. In this paper, we present a systematic review of scientific articles indexed in Scopus to identify possible relationships between the practices of collecting, processing, analysing, and using people's data and people's responses to such practices. We contextualise this by looking at research about Scandinavian societies and link this to the academic literature on big data and trust, big data and smart meters, data ethics and the energy sector, surveillance capitalism, and subsequently performing a reflexive thematic analysis. We broadly situate our understanding of culture in this context on the interactions between cognitive norms, material culture, and energy practices. Our analysis identified a number of articles discussing problems and solutions to do with the practices of surveillance capitalism. We also found that research addresses these challenges in different ways. While some research focuses on technological amendments to address users’ privacy protection, only few examine the fundamental ethical questions that discuss how big data practices may change societies and increase their vulnerability. The literature suggests that even in highly trusting societies, like the ones found in Scandinavian countries, trust can be undermined and weakened.}
}
@incollection{HAYMOND202137,
title = {Chapter 3 - Machine learning and big data in pediatric laboratory medicine},
editor = {Dennis Dietzen and Michael Bennett and Edward Wong and Shannon Haymond},
booktitle = {Biochemical and Molecular Basis of Pediatric Disease (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
pages = {37-70},
year = {2021},
isbn = {978-0-12-817962-8},
doi = {https://doi.org/10.1016/B978-0-12-817962-8.00018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128179628000184},
author = {Shannon Haymond and Randall K. Julian and Emily L. Gill and Stephen R. Master},
keywords = {Artificial intelligence, Big data, Laboratory medicine, Machine learning, Pediatrics, Regulation},
abstract = {Clinical laboratories generate a large number of test results, creating opportunities for improved data management and the use of analytics. Aggregate analyses of these data have potential diagnostic value but require labs to utilize computational tools for the analysis of high-dimensional data. Machine learning can be used to aid decision-making, whether for clinical or operational purposes, using a variety of algorithms to analyze complex data sets and make reliable predictions. This chapter discusses key concepts related to big data and its application to pediatric laboratory medicine. Machine learning workflows, concepts, common algorithms, and related infrastructure requirements are also covered.}
}
@article{DESILVA2021104305,
title = {Clinical notes as prognostic markers of mortality associated with diabetes mellitus following critical care: A retrospective cohort analysis using machine learning and unstructured big data},
journal = {Computers in Biology and Medicine},
volume = {132},
pages = {104305},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104305},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521000998},
author = {Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel Jönsson and Ryan T. Demmer and Joanne Enticott},
keywords = {Critical care, Diabetes, Electronic health records, LASSO, Machine learning, Mortality, Natural language processing, Prognosis, Text mining},
abstract = {Background
Clinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies.
Objective
To determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care.
Materials and methods
Mortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n = 45253), nurses (159027), and both (n = 204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated.
Results
Models built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians’ and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care.
Conclusion
Clinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.}
}
@incollection{APPEL2021193,
title = {Chapter 6 - Psychological targeting in the age of Big Data},
editor = {Dustin Wood and Stephen J. Read and P.D. Harms and Andrew Slaughter},
booktitle = {Measuring and Modeling Persons and Situations},
publisher = {Academic Press},
pages = {193-222},
year = {2021},
isbn = {978-0-12-819200-9},
doi = {https://doi.org/10.1016/B978-0-12-819200-9.00015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128192009000156},
author = {Ruth E. Appel and Sandra C. Matz},
keywords = {Psychological targeting, Psychological profiling, Psychologically-informed interventions, Big Data, Digital footprints, Methods, Ethics, Privacy, Contextual integrity},
abstract = {Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals’ psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.}
}
@article{FUGINI2021100192,
title = {A Big Data Analytics Architecture for Smart Cities and Smart Companies},
journal = {Big Data Research},
volume = {24},
pages = {100192},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000095},
author = {Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli},
keywords = {Big Data platforms, Unstructured data, Text analytics, Machine learning, Virtual enterprises, Smart communities},
abstract = {This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the “Big Data Journey” status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.}
}
@article{WEN2021295,
title = {Big data driven Internet of Things for credit evaluation and early warning in finance},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {295-307},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001977},
author = {Chunhui Wen and Jinhai Yang and Liu Gan and Yang Pan},
keywords = {Internet of Things finance, Credit evaluation and early warning, Factor analysis, Particle swarm optimization, Big data driven},
abstract = {The big data technology framework has been successfully used in the Internet of Things, and the financial industry also hopes to use the advanced technology of big data to integrate and improve internal and external data related to credit risks. Relying on more efficient machine learning algorithms to get a reasonable prediction of credit risk can reduce the self-generated losses of the Internet of Things finance and increase profits. This article uses distributed search engine technology to customize web crawlers to obtain the required bank card and transaction data from the multi-source heterogeneous data of the Internet of Things financial industry, design the corresponding Spark parallel algorithm to preprocess the data, and establish an inverted table and two Level index file provides data source for big data analysis platform. After the data source is determined, the Mutually Exclusive Collectively Exhaustive (MECE) analysis method is combined with the scores of many financial business experts in the industry to obtain a set of candidate indicators and quantification methods for the financial credit risk evaluation of the Internet of Things, and analyze the correlation of indicators and risk grading. The random forest algorithm in the big data machine learning library is used to select the feature of the candidate index set, and a multi-level spatial association rule algorithm based on the Hash structure is designed to mine the financial risk information of the Internet of Things, and build a credit risk assessment and intelligent early warning model. This paper selects 26 indicators of Internet of Things finance as the research objects, uses SPSS26.0 software to perform sample Kaiser–Meyer–Olkin (KMO) test and Bartlett sphere test on the original data, and describes the results of factor analysis in detail. The particle swarm algorithm is introduced into the parameter optimization of random forest, and the financial credit risk assessment model of the Internet of Things is established. The results show that this method can significantly reduce the probability of banks making the first and second error rates when evaluating the credit risk of financing the Internet of Things Finance. This is conducive to the smooth development of the Internet of Things financial business for banks, which enables banks to enhance their own profitability while effectively reducing losses due to incorrect credit provision.}
}
@article{PAIGE20211467,
title = {A Versatile Big Data Health System for Australia: Driving Improvements in Cardiovascular Health},
journal = {Heart, Lung and Circulation},
volume = {30},
number = {10},
pages = {1467-1476},
year = {2021},
issn = {1443-9506},
doi = {https://doi.org/10.1016/j.hlc.2021.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S1443950621005175},
author = {Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree},
keywords = {Big data, Datasets, Cardiovascular disease, National platform},
abstract = {Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.}
}
@article{VANAM2021,
title = {Analysis of twitter data through big data based sentiment analysis approaches},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.11.486},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320391501},
author = {Harika Vanam and Jeberson {Retna Raj R}},
keywords = {Sentiment analysis, Twitter, Unstructured data analysis, Big data analytics, Machine learning algorithm},
abstract = {The common and various forms of Twitter information render this one of the best controlling and recording virtual environments of information. The growth in social media nowadays gives internet users immense interest. In several pups like prediction, advertisement, sentiment analysis …, the data on such a social network platform is used. People exchange good or bad views on problems, items and administrations through the web and informal communities. The capacity to assess such a data productively is presently observed as a noteworthy upper hand in settling on choices all the more proficiently. In this sense, associations use methods, for example, Sentiment Analysis (SA). The utilization of web based life around the globe is growing, however, greatly speeding up mass data generation and stopping us from providing useful insights in conventional SA systems. These data volumes can be processed effectively, using SA and Big Data technology. Big data is not a luxury, in fact, but an important prediction.}
}
@article{SOUIFI2021857,
title = {From Big Data to Smart Data: Application to performance management},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {857-862},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.100},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008491},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Big Data, Smart Data, Performance Management},
abstract = {In the context of digitalization, some companies are considering a transition to Industry 4.0 to ensure greater flexibility, productivity and responsiveness. The implementation of a relevant performance management system is then a real necessity to measure the degree of achievement of these objectives. In the era of Industry 4.0, the potential access to large amounts of data, i.e. Big Data, poses new challenges to the design and implementation of these systems. With the exponential growth of data generated from different sources, there is a need for extensive exploitation of data for performance management. Given the large volume of data, the speed at which it is generated and the variety of data sources, the manufacturing sector is facing with the challenge of creating value from large data sets. This paper introduces some potential benefits of Big Data for business and in particular its role in performance management systems. However, the key idea is that Big Data are not always neither available nor necessary. Authors focus on the concept of smart data, the result of the transformation of Big Data, and define a set of necessary and sufficient conditions the data should satisfy to be considered as Smart. The paper presents some methods of smart data extraction. Such smart data will be used to feed the performance management system in order to obtain more accurate, timely and representative key performance indicators.}
}
@article{SHAHOUD2021100432,
title = {An extended Meta Learning Approach for Automating Model Selection in Big Data Environments using Microservice and Container Virtualizationz Technologies},
journal = {Internet of Things},
volume = {16},
pages = {100432},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100432},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000767},
author = {Shadi Shahoud and Moritz Winter and Hatem Khalloof and Clemens Duepmeier and Veit Hagenmeyer},
keywords = {Meta learning, Machine learning, Microservice, Web-based applications, Big data},
abstract = {For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new concept for enhancing the predictive performance of meta learning classification models by generating new meta examples is introduced. Our concept is realized and evaluated in a microservice-based meta learning framework. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture. In this demonstration and for evaluation purpose, time series model selection is taken as a use case for applying meta learning. It is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, our new concept for generating new meta examples enhances the predictive performance of the meta learner up to 16.77% and 27.07% in the case of using the original and encoded representation forms of meta features respectively. The recommendation of the most appropriate forecasting model results in a well acceptable low framework overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in the context of Big Data.}
}
@article{SHARMA20215515,
title = {A framework based on BWM for big data analytics (BDA) barriers in manufacturing supply chains},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {5515-5519},
year = {2021},
note = {3rd International e-Conference on Frontiers in Mechanical Engineering and nanoTechnology},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.03.374},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321024263},
author = {Vikrant Sharma and Atul Kumar and Mukesh Kumar},
keywords = {Big data analytics, Barriers, Manufacturing supply chains, Best worst method (BWM)},
abstract = {Due to its potential utility, Big Data (BD) recently attracted researchers and practitioners in decision-making. Big Data analytics (BDA) becomes more common among manufacturing companies because it lets them gain insight and make decisions based on BD. Given the importance of both BD and BDA, this study aims to identify and analyse essential BDA adoption barriers in supply chains. This study explores the current knowledge base using a BWM (Best Worst Method) to discuss these barriers. Data were obtained from five Indian manufacturing companies. Research findings show that data-related barriers are most significant. The findings will help managers understand the exact nature of the challenges and possible advantages of the BDA and implement BDA policies for the growth and output of supply chain operations.}
}
@incollection{LYTRAS2021xvii,
title = {Preface: artificial intelligence and big data analytics for smart healthcare: a digital transformation of healthcare primer},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {xvii-xxvii},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00018-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000188},
author = {Miltiadis D. Lytras and Anna Visvizi and Akila Sarirete and Kwok Tai Chui}
}
@article{CISNEROSCABRERA2021114858,
title = {Experimenting with big data computing for scaling data quality-aware query processing},
journal = {Expert Systems with Applications},
volume = {178},
pages = {114858},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114858},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002992},
author = {Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris},
keywords = {Data quality-aware queries, Big data computing, Empirical evaluation},
abstract = {Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.}
}
@article{NORORI2021100347,
title = {Addressing bias in big data and AI for health care: A call for open science},
journal = {Patterns},
volume = {2},
number = {10},
pages = {100347},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100347},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921002026},
author = {Natalia Norori and Qiyang Hu and Florence Marcelle Aellen and Francesca Dalia Faraci and Athina Tzovara},
keywords = {artificial intelligence, deep learning, health care, bias, open science, participatory science, data standards},
abstract = {Summary
Artificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizing the field of health care. A major open challenge that AI will need to address before its integration in the clinical routine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups of the human population have a long history of being absent or misrepresented in existing biomedical datasets. If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.}
}
@incollection{FENG2021145,
title = {Chapter 10 - Spatiotemporal Big Data-Driven Vessel Traffic Risk Estimation for Promoting Maritime Healthcare: Lessons Learnt from Another Domain than Healthcare},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {145-160},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000061},
author = {Zikun Feng and Yan Li and Zhao Liu and Ryan Wen Liu},
keywords = {Data mining, maritime healthcare, maritime risk estimation, automatic identification system (AIS), maritime safety, artificial intelligence},
abstract = {With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and global maritime economy. Therefore it is of vital significance to study the risk of ship collision using big data mining techniques. The big data–driven computational results are beneficial for guaranteeing smart maritime healthcare in the fields of ocean engineering and maritime management. This chapter proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is first improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are then computed using the Monte Carlo simulation strategy. For the sake of better understanding, the kernel density estimation method is finally adopted to visually generate the ship collision risk in maps. Experimental results on realistic spatiotemporal big data have illustrated the effectiveness of the proposed method in crowded inland waterways.}
}
@incollection{WANG202135,
title = {Chapter 2 - Big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {35-49},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000179},
author = {Lidong Wang and Cheryl Alexander},
keywords = {Big data, Precision healthcare, Personalized healthcare, Big Data analytics, Telepsychiatry, Deep learning},
abstract = {Big data technologies enable correlation of multiple data sources into a coherent view. Big data and Big Data analytics have been used in public health, electronic consultation (e-consultation), real-time telediagnosis, precision healthcare, and personalized healthcare. e-Consultation is one aspect of telemedicine related to remote communication between medical specialists and clinicians, or clinicians and patients. It is generally implemented via the Internet or mobile communication devices (e.g., smartphone) and often generates big data. The concepts, characteristics, methods, emerging technologies, and software platforms or tools of big data and Big Data analytics are introduced in this chapter. Big data and applications in general healthcare are presented. Specifically, big data in precision healthcare and personalized healthcare are introduced. Challenges of big data and Big Data analytics in personalized healthcare are also outlined.}
}
@article{YIN2021102514,
title = {Integrating remote sensing and geospatial big data for urban land use mapping: A review},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {103},
pages = {102514},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102514},
url = {https://www.sciencedirect.com/science/article/pii/S030324342100221X},
author = {Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu},
keywords = {Integration methods, Urban functional zone classification, Urban management, Land use},
abstract = {Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.}
}
@article{BAG2021120420,
title = {Role of institutional pressures and resources in the adoption of big data analytics powered artificial intelligence, sustainable manufacturing practices and circular economy capabilities},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120420},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120420},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312464},
author = {Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi},
keywords = {Big data, Artificial intelligence, Industry 4.0, Circular economy, Sustainable manufacturing},
abstract = {ABSTRACT
The significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.}
}
@article{ROSADO2021102155,
title = {MARISMA-BiDa pattern: Integrated risk analysis for big data},
journal = {Computers & Security},
volume = {102},
pages = {102155},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820304284},
author = {David G. Rosado and Julio Moreno and Luis E. Sánchez and Antonio Santos-Olmo and Manuel A. Serrano and Eduardo Fernández-Medina},
keywords = {Big data, Risk assessment, Risk analysis, Information security, Security standards},
abstract = {Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.}
}
@article{LI202149,
title = {The critical need to establish standards for data quality in intelligent medicine},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {49-50},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000073},
author = {Ruiyang Li and Yahan Yang and Haotian Lin},
keywords = {Artificial intelligence, Intelligent medicine, Big data, Data collection, Data storage, Data management},
abstract = {Medical artificial intelligence (AI) is an important technical asset to support medical supply-side reforms and national development in the big data era. Clinical data from multiple disciplines represent building blocks for the development and application of AI-aided diagnostic and treatment systems based on medical big data. However, the inconsistent quality of these data resources in AI research leads to waste and inefficiencies. Therefore, it is crucial that the field formulates the requirements and content related to data processing as part of the development of intelligent medicine. To promote medical AI research worldwide, the “Belt and Road” International Ophthalmic Artificial Intelligence Research and Development Alliance will establish a series of expert recommendations for data quality in intelligent medicine.}
}
@article{BERNASCONI2021100009,
title = {Data quality-aware genomic data integration},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {1},
pages = {100009},
year = {2021},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2021.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2666990021000082},
author = {Anna Bernasconi},
keywords = {Data quality, Data integration, Data curation, Genomic datasets, Metadata, Interoperability},
abstract = {Genomic data are growing at unprecedented pace, along with new protocols, update polices, formats and guidelines, terminologies and ontologies, which are made available every day by data providers. In this continuously evolving universe, enforcing quality on data and metadata is increasingly critical. While many aspects of data quality are addressed at each individual source, we focus on the need for a systematic approach when data from several sources are integrated, as such integration is an essential aspect for modern genomic data analysis. Data quality must be assessed from many perspectives, including accessibility, currency, representational consistency, specificity, and reliability. In this article we review relevant literature and, based on the analysis of many datasets and platforms, we report on methods used for guaranteeing data quality while integrating heterogeneous data sources. We explore several real-world cases that are exemplary of more general underlying data quality problems and we illustrate how they can be resolved with a structured method, sensibly applicable also to other biomedical domains. The overviewed methods are implemented in a large framework for the integration of processed genomic data, which is made available to the research community for supporting tertiary data analysis over Next Generation Sequencing datasets, continuously loaded from many open data sources, bringing considerable added value to biological knowledge discovery.}
}
@article{BIESIALSKA2021106448,
title = {Big Data analytics in Agile software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {132},
pages = {106448},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106448},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301981},
author = {Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero},
keywords = {Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review},
abstract = {Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.}
}
@article{YANG2021100234,
title = {Risk Prediction of Renal Failure for Chronic Disease Population Based on Electronic Health Record Big Data},
journal = {Big Data Research},
volume = {25},
pages = {100234},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100234},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000514},
author = {Yujie Yang and Ye Li and Runge Chen and Jing Zheng and Yunpeng Cai and Giancarlo Fortino},
keywords = {Renal failure, Risk prediction, Electronic health record, Health big data, Machine learning},
abstract = {Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.}
}
@article{MA2021107580,
title = {A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving},
journal = {Computers & Industrial Engineering},
volume = {160},
pages = {107580},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107580},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221004848},
author = {Qiuping Ma and Hongyan Li and Anders Thorstenson},
keywords = {Quality management, Data mining, Machine Learning, Multi-class classification, Neural Network},
abstract = {Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.}
}
@article{FENG2021103636,
title = {Tunnel boring machines (TBM) performance prediction: A case study using big data and deep learning},
journal = {Tunnelling and Underground Space Technology},
volume = {110},
pages = {103636},
year = {2021},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103636},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820305903},
author = {Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing},
keywords = {TBM performance prediction, Deep belief network (DBN), Yingsong Water Diversion Project, Field penetration index prediction},
abstract = {This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728 days’ continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n + 1th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.}
}
@article{VALENCIAPARRA2021113450,
title = {DMN4DQ: When data quality meets DMN},
journal = {Decision Support Systems},
volume = {141},
pages = {113450},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113450},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620302050},
author = {Álvaro Valencia-Parra and Luisa Parody and Ángel Jesús Varela-Vaca and Ismael Caballero and María Teresa Gómez-López},
keywords = {Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement},
abstract = {To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.}
}
@article{ALI2021101600,
title = {Is big data used by cities? Understanding the nature and antecedents of big data use by municipalities},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101600},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101600},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000368},
author = {Hamza Ali and Ryad Titah},
keywords = {Big data use, Big data use in municipalities, Smart cities, E-government, Municipal use of IT, Digital government},
abstract = {It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.}
}
@article{GUPTA2021120986,
title = {Big data and firm marketing performance: Findings from knowledge-based view},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120986},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120986},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521004182},
author = {Shivam Gupta and Théo Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen},
keywords = {Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view},
abstract = {A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.}
}
@article{SINGH2021157,
title = {Big data, industry 4.0 and cyber-physical systems integration: A smart industry context},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {157-162},
year = {2021},
note = {2nd International Conference on Manufacturing Material Science and Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.170},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320352639},
author = {Harpreet Singh},
keywords = {Agile management, Heterogeneity, Internet-of-things, Smart factory, Smart manufacturing},
abstract = {The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the next-generation industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms.}
}
@article{BAZZAZABKENAR2021101517,
title = {Big data analytics meets social media: A systematic review of techniques, open issues, and future directions},
journal = {Telematics and Informatics},
volume = {57},
pages = {101517},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101517},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320301763},
author = {Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii},
keywords = {Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review},
abstract = {Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V’s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.}
}
@incollection{HULSEN202169,
title = {Chapter 4 - Challenges and solutions for big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {69-94},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000167},
author = {Tim Hulsen},
keywords = {Big data, Precision medicine, Personalized healthcare, Data science, Big data analytics},
abstract = {“Big data” is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to “long data,” “wide data,” and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.}
}
@article{ALGHAMDI2021462,
title = {Data quality-aware task offloading in Mobile Edge Computing: An Optimal Stopping Theory approach},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {462-479},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2033079X},
author = {Ibrahim Alghamdi and Christos Anagnostopoulos and Dimitrios P. Pezaros},
keywords = {Mobile edge computing, Tasks offloading, Data quality, Optimal stopping theory, Sequential decision making},
abstract = {An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.}
}
@article{CECH20211947,
title = {Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {1947-1953},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00061f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008856},
author = {Nadja B. Cech and Marnix H. Medema and Jon Clardy},
abstract = {ABSTRACT
Systematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.}
}
@article{SUNDARAKANI2021102452,
title = {Big data driven supply chain design and applications for blockchain: An action research using case study approach},
journal = {Omega},
volume = {102},
pages = {102452},
year = {2021},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2021.102452},
url = {https://www.sciencedirect.com/science/article/pii/S030504832100061X},
author = {Balan Sundarakani and Aneesh Ajaykumar and Angappa Gunasekaran},
keywords = {Big data architecture, Action research, Case study research, Blockchain adoption, Supply chain management},
abstract = {Blockchain appears to still be nascent in its growth and a relatively untapped asset. This research investigates the need of blockchain in Industry 4.0 environment from Big Data perspective in supply chain management. The research method used in this study involves a combination of an Action Research method and Case Study research. More specifically, the action research method was applied in two industry case studies that implemented and tested the designed architecture in a global logistics environment. Case Study A examined the blockchain application in cross-border cargo movements whereas Case Study B investigated the application in a liquid chemical logistics company serving to petroleum industries. Our research analysis has identified that the Case A subject had disconnected systems and services for blockchain wherein the big data interactions had failed (failure case). Whereas in Case B, the company has achieved nearly 25% increase in revenue through its customer service after the blockchain implementation and thereby reduction in paperwork and carbon emissions (success case). This research contributes to the advancement of the body of knowledge to big data and blockchain by identifying key implementation guideline and issues for blockchain in supply chain management. Further, action-based research coupled with a case study approach has been used to evaluate the application aspects of the architecture's scalability and functionality of bigdata and blockchain in supply chain management.}
}
@incollection{MCGILVRAY20217,
title = {Chapter 1 - Data Quality and the Data-Dependent World},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {7-14},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000219},
author = {Danette McGilvray},
keywords = {Data-dependent world, data-driven, assets, technology, legal and regulatory tsunami, Internet of Things (IoT), big data, artificial intelligence (AI), machine learning (ML), The Leader’s Data Manifesto, data literacy, change, COVID-19},
abstract = {Chapter 1 addresses topics in our world today, shows how they are dependent on data and information, why data quality is more relevant and critical now than ever before, and how the Ten Steps methodology will help. Topics include COVID-19, the legal and regulatory tsunami, big data, Internet of Things (IoT), 5G, artificial intelligence (AI) and machine learning (ML). Our data-dependent world is broader than, yet encompasses, being data-driven. Data and information are assets to be managed and are compared to how human and financial resources are managed. The Leader’s Data Manifesto is introduced as a starting point for conversations about the importance of managing data and information assets. While the Ten Steps methodology is meant for specific audiences, three recommendations were provided that anyone, in any organization, can do to help raise awareness of the importance of data quality: add data and information to the conversation, increase data literacy in the workplace, and include data (quality) management in learning institutions at all levels.}
}
@article{WANG202110,
title = {An interview with Shouyang Wang: research frontier of big data-driven economic and financial forecasting},
journal = {Data Science and Management},
volume = {1},
number = {1},
pages = {10-12},
year = {2021},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666764921000011},
author = {Shouyang Wang},
keywords = {Big data, Economic forecasting, Data mining, Spatio-temporal},
abstract = {The development of big data generation, acquisition, storage, processing, and other technologies has greatly enriched our sensory world and fundamentally changed the basis of traditional economic and financial forecasting. Unexpected events in the economic and financial fields challenge our confidence in the performance of forecasting models. Obviously, the big data-driven decision theories and analysis methods are different from the traditional methods. In view of the important role of big data-driven economic and financial forecasting in social stability, innovative development, and sustainability, the research frontiers of big data-driven economic and financial forecasting in the future include: feature mining of complex economic systems with big data representation; accurate real-time correction of theories and methods of dynamic forecasting and early warning; general paradigm of big data forecasting research; formation and process of big data-driven economic and financial system management mechanism, etc. Systematic research on such issues will contribute to the formation of decision-making theories and research systems in the context of big data, thus improving the adaptability and scientificity of management decisions.}
}
@article{BENITEZHIDALGO2021107489,
title = {TITAN: A knowledge-based platform for Big Data workflow management},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107489},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107489},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007516},
author = {Antonio Benítez-Hidalgo and Cristóbal Barba-González and José García-Nieto and Pedro Gutiérrez-Moncayo and Manuel Paneque and Antonio J. Nebro and María del Mar Roldán-García and José F. Aldana-Montes and Ismael Navas-Delgado},
keywords = {Big Data analytics, Semantics, Knowledge extraction},
abstract = {Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.}
}
@article{YANG2021107550,
title = {Optimal timing of big data application in a two-period decision model with new product sales},
journal = {Computers & Industrial Engineering},
volume = {160},
pages = {107550},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107550},
url = {https://www.sciencedirect.com/science/article/pii/S036083522100454X},
author = {Lei Yang and Anqian Jiang and Jiahua Zhang},
keywords = {Supply chain management, Optimal strategy, Big data application, Two-period model, Social welfare},
abstract = {We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.}
}
@article{LOPEZMARTINEZ2021263,
title = {A big data-centric architecture metamodel for Industry 4.0},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {263-284},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002156},
author = {Patricia {López Martínez} and Ricardo Dintén and José María Drake and Marta Zorrilla},
keywords = {Data-centric architecture, Data-intensive applications, Model-based development, Industry 4.0, Big data, Metamodel},
abstract = {The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.}
}
@article{GUALO2021110938,
title = {Data quality certification using ISO/IEC 25012: Industrial experiences},
journal = {Journal of Systems and Software},
volume = {176},
pages = {110938},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110938},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000352},
author = {Fernando Gualo and Moisés Rodriguez and Javier Verdugo and Ismael Caballero and Mario Piattini},
keywords = {Data quality evaluation process, Data quality certification, Data quality management, ISO/IEC 25012, ISO/IEC 25024, ISO/IEC 25040},
abstract = {The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.}
}
@article{RAUT2021103368,
title = {Big data analytics: Implementation challenges in Indian manufacturing supply chains},
journal = {Computers in Industry},
volume = {125},
pages = {103368},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103368},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306023},
author = {Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede},
keywords = {Big data analytics, DEMATEL, Indian manufacturing supply chains, Interpretive structural modeling, MICMAC analysis},
abstract = {Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.}
}
@article{KOZIARA202184,
title = {Introduction to Big Data in trauma and orthopaedics},
journal = {Orthopaedics and Trauma},
volume = {35},
number = {2},
pages = {84-89},
year = {2021},
issn = {1877-1327},
doi = {https://doi.org/10.1016/j.mporth.2021.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S187713272100004X},
author = {Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley},
keywords = {Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy},
abstract = {The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.}
}
@article{FAHEEM2021100236,
title = {CBI4.0: A cross-layer approach for big data gathering for active monitoring and maintenance in the manufacturing industry 4.0},
journal = {Journal of Industrial Information Integration},
volume = {24},
pages = {100236},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100236},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000364},
author = {Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor},
keywords = {Internet of things, Industry 4.0, Big data, Multi-channel communication, Wireless sensor network},
abstract = {Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.}
}
@article{LEI2021101570,
title = {Modelling and analysis of big data platform group adoption behaviour based on social network analysis},
journal = {Technology in Society},
volume = {65},
pages = {101570},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101570},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21000452},
author = {Zhimei Lei and Yandan Chen and Ming K. Lim},
keywords = {Big data, Platforms, Technology adoption, Corporate group behaviour, Social network analysis},
abstract = {Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks—i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network—are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.}
}
@incollection{BACHHETY202145,
title = {2 - Big Data Analytics for healthcare: theory and applications},
editor = {Ashish Khanna and Deepak Gupta and Nilanjan Dey},
booktitle = {Applications of Big Data in Healthcare},
publisher = {Academic Press},
pages = {45-67},
year = {2021},
isbn = {978-0-12-820203-6},
doi = {https://doi.org/10.1016/B978-0-12-820203-6.00008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202036000084},
author = {Shivam Bachhety and Shivani Kapania and Rachna Jain},
keywords = {Big Data, healthcare, Big Data Analytics, Hadoop},
abstract = {In the past 10 years, the healthcare industry is growing at a remarkable rate. The healthcare industry is generating enormous amounts of data in terms of volume, velocity, and variety. Big Data methodologies in healthcare can not only increase the business value but will also add to the improvement of healthcare services. Several techniques can be implemented to develop early disease diagnose systems and improve treatment procedures using detailed analysis over time. In such a situation, Big data Analytics proposes to connect intricate databases to achieve more useful results. In this chapter, we will discuss the procedure of big data analytics in the healthcare sector with some practical applications along with its challenges. We will also have a look at the various big data techniques and their tools for implementation. We conclude this chapter with a discussion on potential opportunities for analytics in the healthcare sector.}
}
@article{LACAM2021100406,
title = {Big data and Smart data: two interdependent and synergistic digital policies within a virtuous data exploitation loop},
journal = {The Journal of High Technology Management Research},
volume = {32},
number = {1},
pages = {100406},
year = {2021},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2021.100406},
url = {https://www.sciencedirect.com/science/article/pii/S1047831021000031},
author = {Jean-Sébastien Lacam and David Salvetat},
keywords = {Big data, Smart data, Volume, Velocity, Variety, Automotive distribution},
abstract = {This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.}
}
@article{BERGIER2021102864,
title = {Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: Where do we stand?},
journal = {Autoimmunity Reviews},
volume = {20},
number = {8},
pages = {102864},
year = {2021},
issn = {1568-9972},
doi = {https://doi.org/10.1016/j.autrev.2021.102864},
url = {https://www.sciencedirect.com/science/article/pii/S1568997221001361},
author = {Hugo Bergier and Loïc Duron and Christelle Sordet and Lou Kawka and Aurélien Schlencker and François Chasset and Laurent Arnaud},
keywords = {Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine},
abstract = {The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.}
}
@incollection{MORRA2021841,
title = {Fresh Outlook on Numerical Methods for Geodynamics. Part 2: Big Data, HPC, Education},
editor = {David Alderton and Scott A. Elias},
booktitle = {Encyclopedia of Geology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {841-855},
year = {2021},
isbn = {978-0-08-102909-1},
doi = {https://doi.org/10.1016/B978-0-08-102908-4.00111-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081029084001119},
author = {Gabriele Morra and David A. Yuen and Henry M. Tufo and Matthew G. Knepley},
keywords = {Big Data, High performance computing, Education, Modeling, Geodynamics},
abstract = {Since 2015 much has developed in geodynamical modeling because of the arrival of Big Data. We present in two parts an overview of numerical techniques but also a scan of the new opportunities in this age of Big Data and prepare the community for the coming decade, the roaring twenties, when Data Analytics will reign. We begin with a review of traditional numerical methods (Part I), followed by a survey of the current techniques used for data analytics and high-performance computing (HPC) (Part II). Our aim is to cover topics of machine learning, neural networks and deep learning, unsupervised learning as well as the role that HPC will play in the Big Data era, especially in hardware of various calibers. Finally, we will address the need for education of students and professionals, in particular, on the use of the emerging programming languages and the importance of scientific software communities.}
}
@article{OPREA2021106902,
title = {Insights into demand-side management with big data analytics in electricity consumers’ behaviour},
journal = {Computers & Electrical Engineering},
volume = {89},
pages = {106902},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106902},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620307540},
author = {Simona-Vasilica Oprea and Adela Bâra and Bogdan George Tudorică and Maria Irène Călinoiu and Mihai Alexandru Botezatu},
keywords = {Big data, Machine learning, Smart meters, Electricity consumption, Clustering, Questionnaire analytics},
abstract = {The consumption data from smart meters and complex questionnaires reveals the electricity consumers’ willingness to adapt their lifestyle to reduce or change their behaviour in electricity usage to flatten the peak in electricity consumption and release the stress in the power grid. Thus, the electricity consumption can support the enforcement of tariff and demand response strategies. Although the plethora of complex, unstructured and heterogeneous data is collected from various devices connected to the Internet, smart meters, plugs, sensors and complex questionnaires, there is an undoubted challenge to handle the data flow that does not provide much information as it remains unprocessed. Therefore, in this paper, we propose an innovative methodology that organizes and extracts valuable information from the increasing volume of data, such as data about the electricity consumption measured and recorded at 30 min intervals, as well as data collected from complex questionnaires.}
}
@article{HUANG2021144535,
title = {An updated model-ready emission inventory for Guangdong Province by incorporating big data and mapping onto multiple chemical mechanisms},
journal = {Science of The Total Environment},
volume = {769},
pages = {144535},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.144535},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720380669},
author = {Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan},
keywords = {Emission inventory, Guangdong Province, Ship emissions, Big data, VOCs speciation},
abstract = {An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3 km × 3 km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23–55%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5–17% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.}
}
@article{LI2021102064,
title = {Big data driven vehicle battery management method: A novel cyber-physical system perspective},
journal = {Journal of Energy Storage},
volume = {33},
pages = {102064},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2020.102064},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X20318971},
author = {Shuangqi Li and Pengfei Zhao},
keywords = {Electric vehicles, Battery energy storage, Cyber-physical battery management system, Big data, Deep learning},
abstract = {The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.}
}
@article{LIU2021257,
title = {Massive-scale carbon pollution control and biological fusion under big data context},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {257-262},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000042},
author = {Yi Liu and Jie Xu and Weijie Yi},
keywords = {Internet-scale network, Dense subgraph mining, Low carbon, Multiple features, Biologically-aware},
abstract = {In the modern society, there are a rich number of low-carbon enterprises that the explicitly/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs’ cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions/correlations of million-scale enterprises.}
}
@article{KOZIEL2021116057,
title = {Investments in data quality: Evaluating impacts of faulty data on asset management in power systems},
journal = {Applied Energy},
volume = {281},
pages = {116057},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116057},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920314896},
author = {Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh},
keywords = {Asset management, Component replacement, Data quality costs, Electric power distribution, Optimization, Trade-off},
abstract = {Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.}
}
@article{ARDAGNA2021107215,
title = {Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107215},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107215},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002081},
author = {Claudio A. Ardagna and Valerio Bellandi and Ernesto Damiani and Michele Bezzi and Cedric Hebert},
keywords = {Artificial intelligence, Big Data Analytics, Machine learning, Security and privacy},
abstract = {We live in an interconnected and pervasive world where huge amount of data are collected every second. Fully exploiting data through advanced analytics, machine learning and artificial intelligence, becomes crucial for businesses, from micro to large enterprises, resulting in a key advantage (or shortcoming) in the global market competition, as well as in a strong market driver for business analytics solutions. This scenario is deeply changing the security landscape, introducing new risks and threats that affect security and privacy of systems, on one side, and safety of users, on the other side. Many domains that can benefit from novel solutions based on data analytics have stringent security requirements to fulfill. The Energy domain’s Smart Grid is a major example of systems at the crossroads of security and data-driven intelligence. The Smart Grid plays a crucial role in modern energy infrastructure. However, it must face two major challenges related to security: managing front-end intelligent devices such as power assets and smart meters securely, and protecting the huge amount of data received from these devices. Starting from these considerations, setting up proper analytics is a complex problem because security controls could have the undesired side effect of decreasing the accuracy of the analytics themselves. This is even more critical when the configuration of security controls is let to the security expert, who often has only basic skills in data science. In this paper, we propose a solution based on the concept of Model-Based Big Data Analytics-as-a-Service (MBDAaaS) that bridges the gap between security experts and data scientists. Our solution acts as a middleware allowing a security expert and a data scientist to collaborate to the deployment of an analytics addressing their needs.}
}
@article{RAUT2021102170,
title = {Big Data Analytics as a mediator in Lean, Agile, Resilient, and Green (LARG) practices effects on sustainable supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {145},
pages = {102170},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.102170},
url = {https://www.sciencedirect.com/science/article/pii/S1366554520308139},
author = {Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Manoj Dora and Mengqi Liu},
keywords = {Big data analytics, Manufacturing firms, Supply chain and logistics management, LARG, Business performance and innovation, Sustainability},
abstract = {The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of ‘Big Data Analytics’ (BDA) as a mediator between ‘sustainable supply chain business performance’ and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirty-seven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.}
}
@article{YILDIRIM2021114840,
title = {Big data analytics for default prediction using graph theory},
journal = {Expert Systems with Applications},
volume = {176},
pages = {114840},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114840},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002815},
author = {Mustafa Yıldırım and Feyza Yıldırım Okay and Suat Özdemir},
keywords = {Big data analytics, Graph theory, Machine learning, Default prediction, SHAP value},
abstract = {With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010–2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.}
}
@article{LI2021103928,
title = {Search query of English translation text based on embedded system and big data},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103928},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.103928},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121001071},
author = {Zhihong Li},
keywords = {Cross-language information retrieval, Optical character recognition, Embedded applications},
abstract = {Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.}
}
@article{BRINCH2021539,
title = {Firm-level capabilities towards big data value creation},
journal = {Journal of Business Research},
volume = {131},
pages = {539-548},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320304859},
author = {Morten Brinch and Angappa Gunasekaran and Samuel {Fosso Wamba}},
keywords = {Big data, Supply chain management, Operations management, Value creation, Business analytics, Capabilities},
abstract = {Big data has played an increasingly important role in using data to improve business value. In response to several big data challenges, the purpose of this study is to identify firm-level capabilities required to create value from big data. The adjacent theories of business process management and IT business value underpinned the study, together with an in-depth case study that led to the identification of twenty-four types of capabilities related to IT, process, performance, human, strategic, and organizational practices. The findings confirmed the application of practices and capabilities of adjacent theories, as well as certain practices and attributes that were both changed and reinforced at the intersection of big data. As an outstanding additional support to the extant big data studies, this work empirically confirms and portrays hitherto unexplored capabilities of big data and set their roles, thus providing a holistic overview of firm-level capabilities that are required for big data value creation.}
}
@article{SHAW2021108953,
title = {Marine big data analysis of ships for the energy efficiency changes of the hull and maintenance evaluation based on the ISO 19030 standard},
journal = {Ocean Engineering},
volume = {232},
pages = {108953},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.108953},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821003887},
author = {Heiu-Jou Shaw and Cheng-Kuan Lin},
keywords = {Energy efficiency management, ISO 19030, Hull and propeller maintenance},
abstract = {This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.}
}
@article{RAKIPI2021100357,
title = {Correlates of the internal audit function’s use of data analytics in the big data era: Global evidence},
journal = {Journal of International Accounting, Auditing and Taxation},
volume = {42},
pages = {100357},
year = {2021},
issn = {1061-9518},
doi = {https://doi.org/10.1016/j.intaccaudtax.2020.100357},
url = {https://www.sciencedirect.com/science/article/pii/S1061951820300586},
author = {Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza},
keywords = {Internal audit, Data analytics, Big data, Soft skills, Fraud detection, IT audit},
abstract = {In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs’ ability to extract value from big data, helping IAFs to enhance their activities’ efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs’ DA usage. From the literature, we identify five main variables expected to be associated with IAFs’ DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs’ ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs’ soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs’ involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.}
}
@article{RUAN2021100110,
title = {Health-adjusted life expectancy (HALE) in Chongqing, China, 2017: An artificial intelligence and big data method estimating the burden of disease at city level},
journal = {The Lancet Regional Health - Western Pacific},
volume = {9},
pages = {100110},
year = {2021},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2021.100110},
url = {https://www.sciencedirect.com/science/article/pii/S2666606521000195},
author = {Xiaowen Ruan and Yue Li and Xiaohui Jin and Pan Deng and Jiaying Xu and Na Li and Xian Li and Yuqi Liu and Yiyi Hu and Jingwen Xie and Yingnan Wu and Dongyan Long and Wen He and Dongsheng Yuan and Yifei Guo and Heng Li and He Huang and Shan Yang and Mei Han and Bojin Zhuang and Jiang Qian and Zhenjie Cao and Xuying Zhang and Jing Xiao and Liang Xu},
abstract = {Background
A universally applicable approach that provides standard HALE measurements for different regions has yet to be developed because of the difficulties of health information collection. In this study, we developed a natural language processing (NLP) based HALE estimation approach by using individual-level electronic medical records (EMRs), which made it possible to calculate HALE timely in different temporal or spatial granularities.
Methods
We performed diagnostic concept extraction and normalisation on 13•99 million EMRs with NLP to estimate the prevalence of 254 diseases in WHO Global Burden of Disease Study (GBD). Then, we calculated HALE in Chongqing, 2017, by using the life table technique and Sullivan's method, and analysed the contribution of diseases to the expected years “lost” due to disability (DLE).
Findings
Our method identified a life expectancy at birth (LE0) of 77•9 years and health-adjusted life expectancy at birth (HALE0) of 71•7 years for the general Chongqing population of 2017. In particular, the male LE0 and HALE0 were 76•3 years and 68•9 years, respectively, while the female LE0 and HALE0 were 80•0 years and 74•4 years, respectively. Cerebrovascular diseases, cancers, and injuries were the top three deterioration factors, which reduced HALE by 2•67, 2•15, and 1•19 years, respectively.
Interpretation
The results demonstrated the feasibility and effectiveness of EMRs-based HALE estimation. Moreover, the method allowed for a potentially transferable framework that facilitated a more convenient comparison of cross-sectional and longitudinal studies on HALE between regions. In summary, this study provided insightful solutions to the global ageing and health problems that the world is facing.
Funding
National Key R and D Program of China (2018YFC2000400).}
}
@article{YE2021106293,
title = {Management of medical and health big data based on integrated learning-based health care system: A review and comparative analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {209},
pages = {106293},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106293},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003679},
author = {Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang},
keywords = {Integrated learning, Health care system, Elaboration Likelihood Machine, System design, Medical big data, Internet of Medical Things},
abstract = {Purpose
We present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system.
Method
The method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity.
Results
The HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450 ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3×28TB.
Conclusion
The experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.}
}
@incollection{ILMUDEEN202133,
title = {Chapter 3 - Big data-based frameworks for healthcare systems},
editor = {Pradeep N and Sandeep Kautish and Sheng-Lung Peng},
booktitle = {Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics},
publisher = {Academic Press},
pages = {33-56},
year = {2021},
isbn = {978-0-12-821633-0},
doi = {https://doi.org/10.1016/B978-0-12-821633-0.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128216330000039},
author = {Aboobucker Ilmudeen},
keywords = {Big data, Frameworks, Healthcare, Healthcare systems},
abstract = {Today, the term big data involves various applications, technologies, architectures, services, and standards in the healthcare domain. The advanced technology and healthcare systems have been extremely knotted together in recent times. As the adoption of wearable biosensors and their applications have begun across the world, eHealth and mHealth have emerged. Hence, wearable sensor devices produce organized and unorganized big data that cannot be easily processed and analyzed due to its complexity; that hinders effective medical decision making. Modern advances in healthcare systems have increased the size of health records such as through electronic health records, patient care, clinical reports, regulations, and compliance requirements. However, the present data-processing technologies are not capable of handling the growing amount of large datasets. This chapter discusses theoretical illustrations that pinpoint various aspects of big data-related frameworks and proposes a large data-based conceptual framework in healthcare systems.}
}
@incollection{BUDNITZ2021665,
title = {Transport Modes and Big Data},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {665-670},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10601-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717106013},
author = {Hannah D Budnitz and Emmanouil Tranos and Lee Chapman},
keywords = {Application programming interfaces, Automation, Big data, Crowd-sourced, Digitization, Geolocation, Information and communication technology, Intelligent transport systems, Internet of things, Location-based services, Mobility as a service, Real time information systems, Timestamp, Vehicle telematics},
abstract = {Transport modes and big data considers the characteristics of “Big Data” as described by the 5 “Vs,” Volume, Velocity, Variety, Veracity, and Value. It reviews the many sources of big data within the transport sector by modal group, and the sources of big data from the Information and Communication Technology (ICT) sector that may be applied to the understanding of transport. It briefly considers the uses, advantages, and disadvantages of these data sources, and the challenges to analyzing and interpreting them. It concludes that Big Data is necessary to prepare for the uncertain future of transport, but recognizes the challenges to applying it accurately and effectively to transport problems and policies.}
}
@article{REN2021128154,
title = {Research on big data analysis model of multi energy power generation considering pollutant emission—Empirical analysis from Shanxi Province},
journal = {Journal of Cleaner Production},
volume = {316},
pages = {128154},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128154},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621023726},
author = {Dongfang Ren and Xiaopeng Guo and Cunbin Li},
keywords = {Multi-energy power generation, Pollutant emission, Big data analysis, Renewable energy generation, Thermal power},
abstract = {With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.}
}
@article{MAJEED2021102026,
title = {A big data-driven framework for sustainable and smart additive manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {67},
pages = {102026},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102026},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302374},
author = {Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin},
keywords = {Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization},
abstract = {From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.}
}
@article{LOFGREN2021R1312,
title = {Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes},
journal = {Current Biology},
volume = {31},
number = {19},
pages = {R1312-R1325},
year = {2021},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2021.06.083},
url = {https://www.sciencedirect.com/science/article/pii/S096098222100912X},
author = {Lotus A. Lofgren and Jason E. Stajich},
abstract = {Summary
Fungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.}
}
@article{BARJAMARTINEZ2021111459,
title = {Artificial intelligence techniques for enabling Big Data services in distribution networks: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {150},
pages = {111459},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111459},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121007413},
author = {Sara Barja-Martinez and Mònica Aragüés-Peñalba and Íngrid Munné-Collado and Pau Lloret-Gallego and Eduard Bullich-Massagué and Roberto Villafafila-Robles},
keywords = {Machine learning, Deep learning, Smart grid, Distribution grid, Smart energy service},
abstract = {Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.}
}
@incollection{RISTEVSKI202185,
title = {4 - Healthcare and medical Big Data analytics},
editor = {Ashish Khanna and Deepak Gupta and Nilanjan Dey},
booktitle = {Applications of Big Data in Healthcare},
publisher = {Academic Press},
pages = {85-112},
year = {2021},
isbn = {978-0-12-820203-6},
doi = {https://doi.org/10.1016/B978-0-12-820203-6.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202036000059},
author = {Blagoj Ristevski and Snezana Savoska},
keywords = {Big Data, medical and healthcare Big Data, Big Data Analytics, databases, healthcare information systems},
abstract = {In the era of big data, a huge volume of heterogeneous healthcare and medical data are generated daily. These heterogeneous data, that are stored in diverse data formats, have to be integrated and stored in a standard way and format to perform suitable efficient and effective data analysis and visualization. These data, which are generated from different sources such as mobile devices, sensors, lab tests, clinical notes, social media, demographics data, diverse omics data, etc., can be structured, semistructured, or unstructured. These varieties of data structures require these big data to be stored not only in the standard relational databases but also in NoSQL databases. To provide effective data analysis, suitable classification and standardization of big data in medicine and healthcare are necessary, as well as excellent design and implementation of healthcare information systems. Regarding the security and privacy of the patient’s data, we suggest employing suitable data governance policies. Additionally, we suggest choosing of proper software development frameworks, tools, databases, in-database analytics, stream computing and data mining algorithms (supervised, unsupervised and semisupervised) to reveal valuable knowledge and insights from these healthcare and medical big data. Ultimately we propose the development of not only patient-oriented but also decision- and population-centric healthcare information systems.}
}
@incollection{GONCALVESPINHO2021155,
title = {Chapter 8 - The use of Big Data in Psychiatry—The role of administrative databases},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {155-165},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822884500009X},
author = {Manuel Gonçalves-Pinho and Alberto Freitas},
keywords = {Administrative database, Mental Health, Secondary data, Psychiatry, Research design},
abstract = {Administrative databases (AD) are repositories of administrative and clinical data related to patient contact episodes with all sorts of health facilities (primary care, hospitals, pharmacies, etc.). The use of AD data is increasing in Mental Health research as the advantages of using AD surpass some of the difficulties Mental health researchers find when using data from other sources (clinical trials, cohort studies, etc.). The large number of patients/contact episodes available, the systematic and broad register, and the fact that AD provides real-world data are some of the pros in using AD data. There are some methodological aspects that must be addressed when using this type of databases in order to provide solid and valid results. The possibility of clinical and administrative errors in an AD is a reality when using secondary data in Mental Health Research, and diagnostic code validation studies may be performed to estimate clinical and administrative accuracy. This chapter described in detail the pros and cons of using secondary data in mental health research and specifies the methodological steps a researcher must follow in order to find valid conclusions in AD from a clinical point of view.}
}
@article{BRESCIANI2021102347,
title = {Using big data for co-innovation processes: Mapping the field of data-driven innovation, proposing theoretical developments and providing a research agenda},
journal = {International Journal of Information Management},
volume = {60},
pages = {102347},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000402},
author = {Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris},
keywords = {Big data, Co-innovation, Open innovation, Bibliometric analysis, Literature review},
abstract = {This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.}
}
@article{QIAN2021645,
title = {Visual recognition processing of power monitoring data based on big data computing},
journal = {Energy Reports},
volume = {7},
pages = {645-657},
year = {2021},
note = {2021 International Conference on Energy Engineering and Power Systems},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.09.205},
url = {https://www.sciencedirect.com/science/article/pii/S235248472101009X},
author = {Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi},
keywords = {Power control data, Monitoring, Visual identification, Iterative screening, CARIMA},
abstract = {The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2–7.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.}
}
@article{JI2021108267,
title = {Building life-span prediction for life cycle assessment and life cycle cost using machine learning: A big data approach},
journal = {Building and Environment},
volume = {205},
pages = {108267},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108267},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321006673},
author = {Sukwon Ji and Bumho Lee and Mun Yong Yi},
keywords = {Building life span, Life cycle cost, Life cycle assessment, Big data, Machine learning, Deep neural network},
abstract = {Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72–4.6 and the coefficients of determination of 0.932–0.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.}
}
@article{NIKOLOV2021100440,
title = {Conceptualization and scalable execution of big data workflows using domain-specific languages and software containers},
journal = {Internet of Things},
volume = {16},
pages = {100440},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100440},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000834},
author = {Nikolay Nikolov and Yared Dejene Dessalk and Akif Quddus Khan and Ahmet Soylu and Mihhail Matskin and Amir H. Payberah and Dumitru Roman},
keywords = {Big data workflows, Internet of Things, Domain-specific languages, Software containers},
abstract = {Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures’ elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach’s scalability with that of Argo Workflows – one of the most prominent tools in the area of Big Data workflows – and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.}
}
@article{LUNDBERG2021100244,
title = {Editorial to the Special Issue on Big Data in Industrial and Commercial Applications},
journal = {Big Data Research},
volume = {26},
pages = {100244},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100244},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000617},
author = {Lars Lundberg and Håkan Grahn and Valeria Cardellini and Andreas Polze and Sogand Shirinbab}
}