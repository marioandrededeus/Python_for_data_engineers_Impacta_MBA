@article{ROEHL201895,
title = {Modeling fouling in a large RO system with artificial neural networks},
journal = {Journal of Membrane Science},
volume = {552},
pages = {95-106},
year = {2018},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2018.01.064},
url = {https://www.sciencedirect.com/science/article/pii/S0376738817318665},
author = {Edwin A. Roehl and David A. Ladner and Ruby C. Daamen and John B. Cook and Jana Safarik and Donald W. Phipps and Peng Xie},
keywords = {Reverse osmosis system, Full-scale, Fouling, Flux decline, Modeling, Neural network},
abstract = {Artificial neural network (ANN) models were developed from a six-year process database to quantify causes of membrane fouling in the first stage of a full-scale, three-stage reverse osmosis (RO) system. The data comprised 59 hydraulic and water quality parameters, representing 190 runs between membrane cleanings. The runs were segmented into a Phase 1 period of initial particle deposition followed by a Phase 2 period of gradual biofilm and scale growth. The phases were modeled separately. Rather than specific flux, a fouling indicator Pfoul′ was calculated from RO system pressures which are normally modulated in part to compensate for fouling. The ANN modeling found that the best predictors of Phase 1 fouling were total chlorine, electrical conductance, TDS, ammonia, and the cartridge filter pressure drop. The best predictors of Phase 2 fouling were turbidity, nitrate, organic nitrogen, nitrite, and total chlorine. These results are consistent with known Phase 1 and 2 fouling mechanisms. The predictive electrical conductance, TDS, and turbidity are “bulk” water quality parameters which were found significantly correlated to sparsely measured cations, sulfates, chlorides, and alkalinity. Simulations with different chlorine concentrations demonstrate how the model could be used to reduce fouling rates.}
}
@article{KRUK2018e1196,
title = {High-quality health systems in the Sustainable Development Goals era: time for a revolution},
journal = {The Lancet Global Health},
volume = {6},
number = {11},
pages = {e1196-e1252},
year = {2018},
issn = {2214-109X},
doi = {https://doi.org/10.1016/S2214-109X(18)30386-3},
url = {https://www.sciencedirect.com/science/article/pii/S2214109X18303863},
author = {Margaret E Kruk and Anna D Gage and Catherine Arsenault and Keely Jordan and Hannah H Leslie and Sanam Roder-DeWan and Olusoji Adeyi and Pierre Barker and Bernadette Daelmans and Svetlana V Doubova and Mike English and Ezequiel García-Elorrio and Frederico Guanais and Oye Gureje and Lisa R Hirschhorn and Lixin Jiang and Edward Kelley and Ephrem Tekle Lemango and Jerker Liljestrand and Address Malata and Tanya Marchant and Malebona Precious Matsoso and John G Meara and Manoj Mohanan and Youssoupha Ndiaye and Ole F Norheim and K Srinath Reddy and Alexander K Rowe and Joshua A Salomon and Gagan Thapa and Nana A Y Twum-Danso and Muhammad Pate}
}
@incollection{KILLEEN2018213,
title = {Chapter 10 - Global Financial Institutions 2.011All views are those of the authors and do not reflect the views of the World Bank or its member countries.},
editor = {David {Lee Kuo Chuen} and Robert Deng},
booktitle = {Handbook of Blockchain, Digital Finance, and Inclusion, Volume 2},
publisher = {Academic Press},
pages = {213-242},
year = {2018},
isbn = {978-0-12-812282-2},
doi = {https://doi.org/10.1016/B978-0-12-812282-2.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122822000103},
author = {Alyse Killeen and Rosanna Chan},
keywords = {Bitcoin, Cryptocurrency, Global Financial Institutions, Financial Institution, Global Institutions, Corporate Blockchain, Ledger technology},
abstract = {The bitcoin blockchain provides the new ability to securely transact in the transfer of value or data without intermediaries, and is indispensible when the distribution of influence is valuable or preferred over centralized operations. With blockchain technology, the ledger necessary to verify and record identity and asset ownership for transactional access - once the purview of trusted central institutions (development banks, Bretton Woods Institutions, large-scale investment firms, etc.) - is free from the limitations imposed by centralized controls. While a number of old needs now served by these institutions will become obsolete, new blockchain-enabled institutional mechanisms will address a variety of remaining needs more efficiently. New needs will also emerge to be addressed by old, new, and blockchain-refined institutional mechanisms. A continuous iterative process is imperative during such periods of rapid change. Global financial institutions must be responsive to the cultural shift and dynamic societal values accompanying blockchain innovation.}
}
@incollection{YUZBASIOGLU201857,
title = {Chapter 4 - Biobanks as Basis of Individualized Medicine: Challenges Toward Harmonization},
editor = {Hans-Peter Deigner and Matthias Kohl},
booktitle = {Precision Medicine},
publisher = {Academic Press},
pages = {57-77},
year = {2018},
isbn = {978-0-12-805364-5},
doi = {https://doi.org/10.1016/B978-0-12-805364-5.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053645000044},
author = {Ayse Yuzbasioglu and Burcu Kesikli and Meral Ozguç},
keywords = {Biobanks, Governance, Harmonization, Ethical guidelines, Quality assurance management},
abstract = {Biobanking is a recent field where human biospecimens and related personal and clinical data are collected and stored for long-term use in biomedical research. The governance models for biobanking research need to involve all stakeholders and must be organized in both ethics and legal aspects and quality management systems. Today, biobanking is moving toward a global scale, and harmonization of all governance steps must be well defined to ensure that a larger research community can benefit from shared materials of high standards. Main ethical issues involved are mostly around protection of donor privacy, informed consent where individual autonomy must be exercised should be a norm, and use of harmonized consent model can facilitate international networking. Quality management of biobanks assures high quality and security of biosamples and associated data; this further fosters public trust and utility of sample collections. This chapter summarizes current governance structures and challenges for international harmonization of biobanks.}
}
@article{DLUGOSZ2017145,
title = {Generalized partially linear regression with misclassified data and an application to labour market transitions},
journal = {Computational Statistics & Data Analysis},
volume = {110},
pages = {145-159},
year = {2017},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167947317300166},
author = {Stephan Dlugosz and Enno Mammen and Ralf A. Wilke},
keywords = {Semiparametric regression, Measurement error, Side information},
abstract = {Large data sets that originate from administrative or operational activity are increasingly used for statistical analysis as they often contain very precise information and a large number of observations. But there is evidence that some variables can be subject to severe misclassification or contain missing values. Given the size of the data, a flexible semiparametric misclassification model would be good choice but their use in practise is scarce. To close this gap a semiparametric model for the probability of observing labour market transitions is estimated using a sample of 20 m observations from Germany. It is shown that estimated marginal effects of a number of covariates are sizeably affected by misclassification and missing values in the analysis data. The proposed generalized partially linear regression extends existing models by allowing a misclassified discrete covariate to be interacted with a nonparametric function of a continuous covariate.}
}
@article{GALLINUCCI201813,
title = {Schema profiling of document-oriented databases},
journal = {Information Systems},
volume = {75},
pages = {13-25},
year = {2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916304483},
author = {Enrico Gallinucci and Matteo Golfarelli and Stefano Rizzi},
keywords = {NoSQL, Document-oriented databases, Schema discovery, Decision trees},
abstract = {In document-oriented databases, schema is a soft concept and the documents in a collection can be stored using different local schemata. This gives designers and implementers augmented flexibility; however, it requires an extra effort to understand the rules that drove the use of alternative schemata when sets of documents with different —and possibly conflicting— schemata are to be analyzed or integrated. In this paper we propose a technique, called schema profiling, to explain the schema variants within a collection in document-oriented databases by capturing the hidden rules explaining the use of these variants. We express these rules in the form of a decision tree (schema profile). Consistently with the requirements we elicited from real users, we aim at creating explicative, precise, and concise schema profiles. The algorithm we adopt to this end is inspired by the well-known C4.5 classification algorithm and builds on two original features: the coupling of value-based and schema-based conditions within schema profiles, and the introduction of a novel measure of entropy to assess the quality of a schema profile. A set of experimental tests made on both synthetic and real datasets demonstrates the effectiveness and efficiency of our approach.}
}
@article{WANG2018367,
title = {High resolution mapping of soil organic carbon stocks using remote sensing variables in the semi-arid rangelands of eastern Australia},
journal = {Science of The Total Environment},
volume = {630},
pages = {367-378},
year = {2018},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2018.02.204},
url = {https://www.sciencedirect.com/science/article/pii/S004896971830603X},
author = {Bin Wang and Cathy Waters and Susan Orgill and Jonathan Gray and Annette Cowie and Anthony Clark and De Li Liu},
keywords = {Soil organic carbon stocks, Seasonal fractional cover, Remote sensing, Machine learning, Digital soil mapping},
abstract = {Efficient and effective modelling methods to assess soil organic carbon (SOC) stock are central in understanding the global carbon cycle and informing related land management decisions. However, mapping SOC stocks in semi-arid rangelands is challenging due to the lack of data and poor spatial coverage. The use of remote sensing data to provide an indirect measurement of SOC to inform digital soil mapping has the potential to provide more reliable and cost-effective estimates of SOC compared with field-based, direct measurement. Despite this potential, the role of remote sensing data in improving the knowledge of soil information in semi-arid rangelands has not been fully explored. This study firstly investigated the use of high spatial resolution satellite data (seasonal fractional cover data; SFC) together with elevation, lithology, climatic data and observed soil data to map the spatial distribution of SOC at two soil depths (0–5cm and 0–30cm) in semi-arid rangelands of eastern Australia. Overall, model performance statistics showed that random forest (RF) and boosted regression trees (BRT) models performed better than support vector machine (SVM). The models obtained moderate results with R2 of 0.32 for SOC stock at 0–5cm and 0.44 at 0–30cm, RMSE of 3.51MgCha−1 at 0–5cm and 9.16MgCha−1 at 0–30cm without considering SFC covariates. In contrast, by including SFC, the model accuracy for predicting SOC stock improved by 7.4–12.7% at 0–5cm, and by 2.8–5.9% at 0–30cm, highlighting the importance of including SFC to enhance the performance of the three modelling techniques. Furthermore, our models produced a more accurate and higher resolution digital SOC stock map compared with other available mapping products for the region. The data and high-resolution maps from this study can be used for future soil carbon assessment and monitoring.}
}
@article{JABEUR2018495,
title = {Crowd social media computing: Applying crowd computing techniques to social media},
journal = {Applied Soft Computing},
volume = {66},
pages = {495-505},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2017.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S1568494617305653},
author = {Nafaâ Jabeur and Ahmed Nait-Sidi-Moh and Sherali Zeadally},
keywords = {Social media, Crowd computing, Return of investment, Audience engagement},
abstract = {Social media producers are currently having a fierce competition worldwide to increase their revenues. To achieve this goal, they are investigating alternative ways to attract more users, generate new user activities, and collect valuable data for personalizing contents and services. One such alternative is crowd computing. Our vision is based on the great potential of a well-coordinated and controlled joint utilization of human intelligence and computer systems that can help solve problems that would be difficult to do with individual capabilities alone. To achieve this vision, which we summarize under our concept of crowd social media computing, we investigate and model the characteristics of the social media ecosystem, we discuss the characteristics of crowd computing, and then we demonstrate how crowd computing can play a pivotal role in emerging social media applications. We also propose a new approach to evaluate the impact of crowd computing on the issue of social media Return of Investment (ROI).}
}
@article{WEGENER20181,
title = {Dawn of new machining concepts:: Compensated, intelligent, bioinspired},
journal = {Procedia CIRP},
volume = {77},
pages = {1-17},
year = {2018},
note = {8th CIRP Conference on High Performance Cutting (HPC 2018)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.08.194},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118310436},
author = {Konrad Wegener and Thomas Gittler and Lukas Weiss},
keywords = {Biologicalization, Industrie 4.0, Cognitive Technical Systems, Machine learning Strategies, Bioinspired, Biointelligent Manufacturing, Adaptive Thermal Compensation, Self-Organization, Self-Healing, Sensors in Abundance},
abstract = {The impact of Industrie 4.0 onto machine tools is significant, despite the fact, that quite some of the novelties discussed within this new paradigm have their roots decades earlier. But especially the concerted action, which strives the development of sensors, controls, data processing together with connectivity, unprecedented data integration and the notion of cyber physical production systems open up new development lines towards manufacturing systems as enablers for the progress in manufacturing. Highly developed compensation concepts are developing into state depending AI-supported strategies. Maintenance becomes predictive, as learning of machines becomes global and model based. Further inspirations taken from biological systems are adopted for machining centres and drive a biological transformation of manufacturing machines. Machine intelligence becomes the basis for executing manufacturing processes, which requires a close integration of process intelligence (CAM-systems) and machine controls.}
}
@article{TOBOGU2018240,
title = {ICT adoption in road freight transport in Nigeria – A case study of the petroleum downstream sector},
journal = {Technological Forecasting and Social Change},
volume = {131},
pages = {240-252},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517312568},
author = {Abiye Tob-Ogu and Niraj Kumar and John Cullen},
keywords = {ICT adoption and adaptation, Road freight transport, Resource based view, Institutional theory},
abstract = {This paper advances the ICT adoption discourse to explore ICT mechanism use, adaptation and contextual influences on management strategies in Africa. A polar-type multiple case studies approach is used to guide empirical data collection across 10 individual cases. 21 interviews were conducted with top executives and these were corroborated with over 30h of non-participant observations and archival documentation from these cases. Using a tripartite coding frame, thematic and content analyses were performed to identify patterns and themes in the collected data. Findings of this study evidence ICT use at firm level with significant links to local contextual factors. Additionally, whilst affirming relationships between size and adoption, the findings also suggest an inverted parallel between both variables. The paper contributes by empirically highlighting the influence of contextual factors on ICT use in road freight transportation as well as highlighting the potential for ICT developers and OEMs to acquire innovative input from local adaptation practices within the industry.}
}
@article{WANG201831,
title = {Spatio-temporal fusion for daily Sentinel-2 images},
journal = {Remote Sensing of Environment},
volume = {204},
pages = {31-42},
year = {2018},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2017.10.046},
url = {https://www.sciencedirect.com/science/article/pii/S0034425717305096},
author = {Qunming Wang and Peter M. Atkinson},
keywords = {Sentinel-2, Sentinel-3, Image fusion, Downscaling},
abstract = {Sentinel-2 and Sentinel-3 are two newly launched satellites for global monitoring. The Sentinel-2 Multispectral Imager (MSI) and Sentinel-3 Ocean and Land Colour Instrument (OLCI) sensors have very different spatial and temporal resolutions (Sentinel-2 MSI sensor 10m, 20m and 60m, 10days, albeit 5days with 2 sensors, conditional upon clear skies; Sentinel-3 OLCI sensor 300m, <1.4days with 2 sensors). For local monitoring (e.g., the growing cycle of plants) one either has the desired spatial or temporal resolution, but not both. In this paper, spatio-temporal fusion is considered to fuse Sentinel-2 with Sentinel-3 images to create nearly daily Sentinel-2 images. A challenging issue in spatio-temporal fusion is that there can be very few cloud-free fine spatial resolution images temporally close to the prediction time, or even available, strong temporal (i.e., seasonal) changes may exist. To this end, a three-step method consisting of regression model fitting (RM fitting), spatial filtering (SF) and residual compensation (RC) is proposed, which is abbreviated as Fit-FC. The Fit-FC method can be performed using only one Sentinel-3–Sentinel-2 pair and is advantageous for cases involving strong temporal changes (i.e., mathematically, the correlation between the two Sentinel-3 images is small). The effectiveness of the method was validated using two datasets. The created nearly daily Sentinel-2 time-series images have great potential for timely monitoring of highly dynamic environmental, agricultural or ecological phenomena.}
}
@article{LACOSTE2018183,
title = {Assessing regional farming system diversity using a mixed methods typology: the value of comparative agriculture tested in broadacre Australia},
journal = {Geoforum},
volume = {90},
pages = {183-205},
year = {2018},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2018.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S001671851830023X},
author = {Myrtille Lacoste and Roger Lawes and Olivier Ducourtieux and Ken Flower},
keywords = {Farming systems, Agrarian systems, Farm typology, Farmer practices, Mixed methods, Multivariate cluster analysis},
abstract = {Most farm and farmer typologies focus on specific aspects and use standard structural and socio-economic indicators. Regional assessments of agricultural diversity based on farming systems are rarely done, as detailed and representative information is difficult to collect. The discipline of comparative agriculture addresses these challenges but remains little known, and seldom applied to broadacre situations. This study demonstrates in Western Australia the value of its mixed methods and multi-disciplinary concepts to determine the level and nature of regional farming system heterogeneity. The typology built comprised six farming systems based on 36 farms that represented half the farming population of a 4000 km2 area (broadacre rainfed systems dominated by winter cereals and sheep, Mediterranean climate). The farm groups corresponding to these farming systems differed across 36 variables representing biophysical, technical, and social aspects at varied spatial and temporal scales. Results were compared with five sets of farm clusters produced through multivariate clustering procedures commonly employed to build typologies. These farm clusters differed across fewer variables than the farm groups of the comparative agriculture typology. The analytical, methodological and conceptual tools used in comparative agriculture to solve the challenges associated with the holistic study of farming system heterogeneity are discussed. These included basing data collection and analysis on an empirical approach that assessed groups of farms rather than individuals, solving data scarcity through a range of qualitative techniques, and progressively informing the choice of typology criteria. Comparative agriculture thus provides an alternative to standard typology paradigms that deserves wider application.}
}
@article{THORBURN2017129,
title = {iCLIC Data Mining and Data Sharing workshop: The present and future of data mining and data sharing in the EU},
journal = {Computer Law & Security Review},
volume = {33},
number = {1},
pages = {129-137},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0267364916302382},
author = {Robert Thorburn and Sophie Stalla-Bourdillon and Eleonora Rosati},
keywords = {Data mining, Copyright, Database rights, Date sharing, Data protection, Intellectual property rights, General data protection regulation (EU-GDPR), Web scraping},
abstract = {Held at Southampton University's Highfield campus and hosted by iCLIC, an interdisciplinary core on Law, the Internet and Culture, the Data Mining and Data Sharing workshop brought together attendees and speakers from industry, government, academia and a range of disciplines alike. The workshop comprised two sessions, each with a keynote and an associated panel. The first session was chaired by Eleonora Rosati and dealt with copyright and database rights, data mining and data sharing. The second session, chaired by Sophie Stalla-Bourdillon, focussed on data protection, data mining and data sharing. The following report covers both sessions, associated panel discussions and the subsequent question and answer sessions.}
}
@incollection{MOSIER201855,
title = {Chapter 5 - The Brain Initiative—Implications for a Revolutionary Change in Clinical Medicine via Neuromodulation Technology},
editor = {Elliot S. Krames and P. Hunter Peckham and Ali R. Rezai},
booktitle = {Neuromodulation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {55-68},
year = {2018},
isbn = {978-0-12-805353-9},
doi = {https://doi.org/10.1016/B978-0-12-805353-9.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805353900005X},
author = {Elizabeth M. Mosier and Michael Wolfson and Erika Ross and James Harris and Doug Weber and Kip A. Ludwig},
keywords = {Bioelectronic medicines, BRAIN initiative, Deep brain stimulation, Defense Applied Research Projects Agency, Electroceuticals, National Institutes of Health, National Science Foundation, Neuromodulation, Neuroprosthetics, Neurotechnology, Public/private partnerships},
abstract = {Launched in April 2013, the goal of the White House Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative is to catalyze the development of neurotechnology to provide new insights into the fundamental mechanisms of disease process. This ongoing international effort spans both the public and private sectors and has produced a number of specialized programs intended to facilitate partnerships between engineers, scientists, clinicians, industry, and regulatory agencies to translate new neuromodulation technologies into initial clinical studies. In this chapter, the history behind the launch of the BRAIN Initiative and the implications of the new BRAIN programs on the development of neuromodulation technologies are described.}
}
@incollection{2018783,
title = {Index},
editor = {Robert Nisbet and Gary Miner and Ken Yale},
booktitle = {Handbook of Statistical Analysis and Data Mining Applications (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Boston},
pages = {783-792},
year = {2018},
isbn = {978-0-12-416632-5},
doi = {https://doi.org/10.1016/B978-0-12-416632-5.09989-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166325099898}
}
@article{TANG2017302,
title = {Towards a trust evaluation middleware for cloud service selection},
journal = {Future Generation Computer Systems},
volume = {74},
pages = {302-312},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1600011X},
author = {Mingdong Tang and Xiaoling Dai and Jianxun Liu and Jinjun Chen},
keywords = {Cloud services, Trust evaluation, Reputation, Service selection, Middleware},
abstract = {With the advent of cloud computing, employing various cloud services to build highly reliable cloud applications has become increasingly popular. The trustworthiness of cloud services is a critical issue that hinders the development of cloud applications, and thus is an urgently-required research problem. Previous studies evaluate trustworthiness of services via either QoS monitoring mechanisms or user feedback ratings, while seldom they combine both of them for enhancing service trust evaluation. This paper proposes a trustworthy selection framework for cloud service selection, named TRUSS. Aiming at developing an effective trust evaluation middleware for TRUSS, we propose an integrated trust evaluation method via combining objective trust assessment and subjective trust assessment. The objective trust assessment is based on QoS monitoring, while the subjective trust assessment is based on user feedback ratings. Experiments conducted using a synthesized dataset show that our proposed method significantly outperforms the other trust and reputation methods.}
}
@incollection{HERRMANN2018187,
title = {Chapter 11 - Designing Health Care That Works—Socio-technical Conclusions},
editor = {Mark S. Ackerman and Sean P. Goggins and Thomas Herrmann and Michael Prilla and Christian Stary},
booktitle = {Designing Healthcare That Works},
publisher = {Academic Press},
pages = {187-203},
year = {2018},
isbn = {978-0-12-812583-0},
doi = {https://doi.org/10.1016/B978-0-12-812583-0.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128125830000110},
author = {Thomas Herrmann and Mark S. Ackerman and Sean P. Goggins and Christian Stary and Michael Prilla}
}
@article{RUAN201777,
title = {Introducing cybernomics: A unifying economic framework for measuring cyber risk},
journal = {Computers & Security},
volume = {65},
pages = {77-89},
year = {2017},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2016.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167404816301407},
author = {Keyun Ruan},
keywords = {Cybernomics, Cyber risk unit, Economic modelling, Risk analytics, Enterprise risk management},
abstract = {This is the first in a series of papers on the risk measures and unifying economic framework encompassing the cross-disciplinary field of “Cybernomics”. This is also the first academic paper to formally propose measurement units for cyber risk. In this paper, multidisciplinary methodologies are used to apply proven risk measurement methods in finance and medicine to define novel risk units central to cybernomics. Leveraging established risk units – MicroMort (MM) for measuring medical risk and Value-at-Risk (VaR) for measuring market risk – BitMort (BM) and hekla (named after an Icelandic volcano) are defined as cyber risk units. Risk calculation methods and examples are introduced in this paper to measure cost-effectiveness of control factors, articulate an entity's “willingness-to-pay” (risk pricing) for cyber risk reduction, cyber risk limit, and cyber risk appetite. Built around BM and hekla, cybernomics integrates cyber risk management and economics to study the requirements of a databank in order to improve risk analytics solutions for: 1) the valuation of digital assets; 2) the measurement of risk exposure of digital assets; and 3) the capital optimization for managing residual cyber risk. Establishing adequate, holistic and statistically robust data points on the entity, portfolio and global levels for the development of a cybernomics databank are essential for the resilience of our shared digital future. This paper explains the need to establish data schemes such as International Digital Asset Classification (IDAC) and International Classification of Cyber Incidents (ICCI).}
}
@article{EISENBERG201846,
title = {Applying novel technologies and methods to inform the ontology of self-regulation},
journal = {Behaviour Research and Therapy},
volume = {101},
pages = {46-57},
year = {2018},
note = {An experimental medicine approach to behavior change: The NIH Science of Behavior Change (SOBC)},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2017.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0005796717302048},
author = {Ian W. Eisenberg and Patrick G. Bissett and Jessica R. Canning and Jesse Dallery and A. Zeynep Enkavi and Susan Whitfield-Gabrieli and Oscar Gonzalez and Alan I. Green and Mary Ann Greene and Michaela Kiernan and Sunny Jung Kim and Jamie Li and Michael R. Lowe and Gina L. Mazza and Stephen A. Metcalf and Lisa Onken and Sadev S. Parikh and Ellen Peters and Judith J. Prochaska and Emily A. Scherer and Luke E. Stoeckel and Matthew J. Valente and Jialing Wu and Haiyi Xie and David P. MacKinnon and Lisa A. Marsch and Russell A. Poldrack},
keywords = {Self-regulation, Ontology, Neuroimaging, Intervention, Obesity, Smoking},
abstract = {Self-regulation is a broad construct representing the general ability to recruit cognitive, motivational and emotional resources to achieve long-term goals. This construct has been implicated in a host of health-risk behaviors, and is a promising target for fostering beneficial behavior change. Despite its clear importance, the behavioral, psychological and neural components of self-regulation remain poorly understood, which contributes to theoretical inconsistencies and hinders maximally effective intervention development. We outline a research program that seeks to define a neuropsychological ontology of self-regulation, articulating the cognitive components that compose self-regulation, their relationships, and their associated measurements. The ontology will be informed by two large-scale approaches to assessing individual differences: first purely behaviorally using data collected via Amazon's Mechanical Turk, then coupled with neuroimaging data collected from a separate population. To validate the ontology and demonstrate its utility, we will then use it to contextualize health risk behaviors in two exemplar behavioral groups: overweight/obese adults who binge eat and smokers. After identifying ontological targets that precipitate maladaptive behavior, we will craft interventions that engage these targets. If successful, this work will provide a structured, holistic account of self-regulation in the form of an explicit ontology, which will better clarify the pattern of deficits related to maladaptive health behavior, and provide direction for more effective behavior change interventions.}
}
@article{ANSON2017131,
title = {Analysing social media data for disaster preparedness: Understanding the opportunities and barriers faced by humanitarian actors},
journal = {International Journal of Disaster Risk Reduction},
volume = {21},
pages = {131-139},
year = {2017},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2016.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S221242091630156X},
author = {Susan Anson and Hayley Watson and Kush Wadhwa and Karin Metz},
keywords = {Social media, Data, Analysis tools, Preparedness, Humanitarian},
abstract = {The use of social media applications by citizens, public authorities, and humanitarian organisations generates vast quantities of data. Research predominantly focuses on the use of social media and associated analysis tools during the short-term response phase of a disaster. As such, the use of social media analysis tools to harness social media data for preparedness purposes is currently unclear. This research uses a combination of semi-structured interviews with 20 Red Cross Red Crescent and humanitarian actors, an online survey, two workshops and desk-based research to examine the opportunities and barriers faced by humanitarian actors in using social media analysis tools to analyse social media data for disaster preparedness. Whilst social media analysis tools provide humanitarian actors with an opportunity to understand the effectiveness of their preparedness communication on social media, to monitor risks and disasters, and to build community preparedness networks, this study identified a limited use of social media analysis tools by research participants. This (non) use of social media analysis tools was influenced by the interaction between seven categories of barriers relating to the user of the tool or the tool itself: language, culture, value, financial, human resources, technology, and data. In discussing these barriers, the authors highlight the key role that context plays in determining the significance of each barrier on the selection and use of social media analysis tools for preparedness.}
}
@article{GOTZE2018285,
title = {Insights from recent gravity satellite missions in the density structure of continental margins – With focus on the passive margins of the South Atlantic},
journal = {Gondwana Research},
volume = {53},
pages = {285-308},
year = {2018},
note = {Rifting to Passive Margins},
issn = {1342-937X},
doi = {https://doi.org/10.1016/j.gr.2017.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S1342937X1730206X},
author = {Hans-Jürgen Götze and Roland Pail},
keywords = {Continental margins, Satellite gravity missions, Spatial resolution, Omission error, Interpretation gravity effects, Interpretation gravity gradients},
abstract = {We focus on new gravity and gravity gradient data sets from modern satellite missions GOCE, GRACE and CHAMP, and their geophysical interpretation at passive continental margins of the South Atlantic. Both sides, South Africa and South America, have been targets of hydrocarbon exploration and academic research of the German Priority Program SAMPLE (South Atlantic Margin Processes and Links with onshore Evolution). The achievable spatial resolution, driven by GOCE, is 70–80km. Therefore, most of the geological structures, which cause a significant gravity effect (by both size and density contrast), can be resolved. However, one of the most important aspects is the evaluation of the omission error, which is not always in the focus of interpreters. It results from high-frequency signals of very rough topographic and bathymetric structures, which cannot be resolved by satellite gravimetry due to the exponential signal attenuation with altitude. The omission error is estimated from the difference of the combined gravity model EIGEN-6C4 and the satellite-only model GOCO05S. It can be significantly reduced by topographic reductions. Simple 2D density models and their related mathematical formulas provide insights in the magnitude of the gravity effect of masses that form a passive continental margin. They are contrasted with results from satellite-only and combined gravity models. Example geophysical interpretations are given for the western and eastern margin of the South Atlantic Ocean, where standard deviations vary from 25 to 16mGal and 21–11mGal, respectively. It could be demonstrated, that modern satellite gravity data provide significant added value in the geophysical gravity data processing domain and in the validation of heterogeneous terrestrial data bases. Combined models derived from high-resolution terrestrial gravity and homogeneous satellite data will lead to more detailed and better constrained lithospheric density models, and hence will improve our knowledge about structure, evolution and state of stress in the lithosphere.}
}
@article{SCHOPFEL2017305,
title = {Research Data in Current Research Information Systems},
journal = {Procedia Computer Science},
volume = {106},
pages = {305-320},
year = {2017},
note = {13th International Conference on Current Research Information Systems, CRIS2016, Communicating and Measuring Research Responsibly: Profiling, Metrics, Impact,Interoperability},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917302983},
author = {Joachim Schöpfel and Hélène Prost and Violane Rebouillat},
keywords = {Current research information systems, research data, evaluation, metadata, identifiers, conservation, open access, open science},
abstract = {The paper provides an overview of recent research and publications on the integration of research data in Current Research Information Systems (CRIS) and addresses three related issues, i.e. the object of evaluation, identifier schemes and conservation. Our focus is on social sciences and humanities. As research data gradually become a crucial topic of scientific communication and evaluation, current research information systems must be able to consider and manage the great variety and granularity levels of data as sources and results of scientific research. More empirical and moreover conceptual work is needed to increase our understanding of the reality of research data and the way they can and should be used for the needs and objectives of research evaluation. The paper contributes to the debate on the evaluation of research data, especially in the environment of open science and open data, and will be helpful in implementing CRIS and research data policies.}
}
@incollection{BERMAN2018263,
title = {Chapter 8 - Precision Data},
editor = {Jules J. Berman},
booktitle = {Precision Medicine and the Reinvention of Human Disease},
publisher = {Academic Press},
pages = {263-326},
year = {2018},
isbn = {978-0-12-814393-3},
doi = {https://doi.org/10.1016/B978-0-12-814393-3.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128143933000081},
author = {Jules J. Berman},
keywords = {Timestamp, Identifiers, Data sharing, Data repurposing, Data reanalysis, Resampling statistics, Metadata},
abstract = {Virtually all of the biomedical data collected today is unusable in its raw form. In this chapter, we will describe modern principles that govern proper data collection and annotation. In the past few decades, an impressive array of sophisticated statistical and mathematical techniques has been developed for the purposes of analyzing large and complex collections of data. Consequently, there is a growing perception that the analysis of data in the Precision Medicine era is something best left to the professionals. We will recommend simple approaches that biologists can use to understand what their own precision data is trying to tell them. Finally, all data analyses must be considered preliminary and tentative until they have been validated, and this often requires data sharing and data reanalysis. This chapter discusses why data sharing is crucial to the advancement of Precision Medicine.}
}
@article{NG20173,
title = {The Internet-of-Things: Review and research directions},
journal = {International Journal of Research in Marketing},
volume = {34},
number = {1},
pages = {3-21},
year = {2017},
issn = {0167-8116},
doi = {https://doi.org/10.1016/j.ijresmar.2016.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167811615301890},
author = {Irene C.L. Ng and Susan Y.L. Wakenshaw},
keywords = {Information technology, Consumer experience},
abstract = {This paper presents a review of the Internet-of-Things (IoT) through four conceptualizations: IoT as liquification and density of information of resources; IoT as digital materiality; IoT as assemblage or service system; and IoT as modules, transactions, and service. From the conceptualizations, we provide a definition of IoT and present its implications and impact on future research in Marketing that interfaces with information systems, design and innovation, data science and cybersecurity, as well as organizational studies and economics. By integrating the implications of IoT with extant literature, we then propose a set of priorities for future research in this area.}
}
@article{MARZAL20183610,
title = {Current challenges and future trends in the field of communication architectures for microgrids},
journal = {Renewable and Sustainable Energy Reviews},
volume = {82},
pages = {3610-3622},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2017.10.101},
url = {https://www.sciencedirect.com/science/article/pii/S1364032117314703},
author = {Silvia Marzal and Robert Salas and Raúl González-Medina and Gabriel Garcerá and Emilio Figueres},
keywords = {Microgrid, Communication protocols, Multi-agent systems, Peerto-Peer, Distributed architectures},
abstract = {The concept of microgrid has emerged as a feasible answer to cope with the increasing number of distributed renewable energy sources which are being introduced into the electrical grid. The microgrid communication network should guarantee a complete and bidirectional connectivity among the microgrid resources, a high reliability and a feasible interoperability. This is in a contrast to the current electrical grid structure which is characterized by the lack of connectivity, being a centralized-unidirectional system. In this paper a review of the microgrids information and communication technologies (ICT) is shown. In addition, a guideline for the transition from the current communication systems to the future generation of microgrid communications is provided. This paper contains a systematic review of the most suitable communication network topologies, technologies and protocols for smart microgrids. It is concluded that a new generation of peer-to-peer communication systems is required towards a dynamic smart microgrid. Potential future research about communications of the next microgrid generation is also identified.}
}
@article{SCHAGNER201744,
title = {Monitoring recreation across European nature areas: A geo-database of visitor counts, a review of literature and a call for a visitor counting reporting standard},
journal = {Journal of Outdoor Recreation and Tourism},
volume = {18},
pages = {44-55},
year = {2017},
issn = {2213-0780},
doi = {https://doi.org/10.1016/j.jort.2017.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2213078017300099},
author = {Jan Philipp Schägner and Joachim Maes and Luke Brander and Maria-Luisa Paracchini and Volkmar Hartje and Gregoire Dubois},
keywords = {Nature recreation, Visitor monitoring, Visitor counting review, Reporting standard, Visitor statistics, Visitor data sharing},
abstract = {Nature recreation and tourism is a substantial ecosystem service of Europe's countryside that has a substantial economic value and contributes considerably to income and employment of local communities. Highlighting the recreational value and economic contribution of nature areas can be used as a strong argument for the funding of protected and recreational areas. The total number of recreational visits of a nature area has been recognised as a major determinant of its economic recreational value and its contribution to local economies. This paper presents an international geo-database on recreational visitor numbers to non-urban ecosystems, containing 1267 observations at 518 separate case study areas throughout Europe. The monitored sites are described by their centroid coordinates and shape files displaying the exact extension of the sites. Therefore, the database illustrates the spatial distribution of visitor counting throughout Europe and can be used for secondary research, such as for validation of spatially explicit recreational ecosystem service models and for identifying relevant drivers of recreational ecosystem services. To develop the database, we review visitor monitoring literature throughout Europe and give an overview of such activities with special attention to visitor counting. We identify one major shortcoming in the available literature, which relates to the presentation, study area definition and methodological reporting of conducted visitor counting studies. Insufficient reporting hampers the identification of the study area, the comparability of different studies and the evaluation of the studies' quality. Based on our findings, we propose a standardised reporting template for visitor counting studies and advanced data sharing for recreational visitor data. Researchers and institutions are invited to report on their visitor counting studies via our web interface at rris.biopama.org/visitor-reporting and thereby contribute to a global visitor database that will be shared via the ESP Visualisation tool.
Management implications
The total annual visitor number is the most important variable for defining the relative importance and the economic recreational value of different recreational areas. Due to the importance of visitor counting and its increased attention in the scientific literature we: •present a geo-database on recreational visitor statistics for nature areas, which allows identifying sites for which visitor statistics exist and which can be used for secondary research•review current practice in recreational visitor counting across nature areas in Europe and give guidance for future applications,•identify shortcomings in the methodological reporting of recent visitor monitoring and counting studies and•present and recommend reporting standard for all future visitor counting studies in order to improve their comparability and to allow assessing their quality.•The reporting standard is translated into a web interface for visitor data collection, which allows for data sharing via a global map-browser.}
}
@incollection{SHEMSHADI201737,
title = {Chapter 2 - The Anatomy of An Intent Based Search and Crawler Engine for the Web of Things},
editor = {Quan Z. Sheng and Yongrui Qin and Lina Yao and Boualem Benatallah},
booktitle = {Managing the Web of Things},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {37-72},
year = {2017},
isbn = {978-0-12-809764-9},
doi = {https://doi.org/10.1016/B978-0-12-809764-9.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097649000032},
author = {Ali Shemshadi and Quan Z. Sheng and Yongrui Qin},
keywords = {Web of Things, Internet of Things, Big Data, Search engine, Flight delay},
abstract = {Web of Things (WoT) is becoming increasingly interesting for researchers and professionals over the past few years. It provides numerous opportunities by disseminating the data that is generated by physical things and fills the gap between the physical and the virtual world. Despite its importance, WoT search has not been studied enough in the past. Given the dynamic challenge of the WoT, collecting data from WoT resources is not well developed. Furthermore, the effectiveness of WoT search can be significantly improved if the users' intention of the search is also considered. This can be facilitated by knowing the existing status of the WoT in real-world. In this chapter, we address multiple challenges in this area. Firstly, we depict the analytical structure of the future WoT which facilitate crawling, indexing and searching the data from physical things. Secondly, we show how we can identify WoT and extract the data from it. Thirdly, we use our crawler to crawl and analyse WoT data on the Internet. Furthermore, we provide a showcase in the analysis of the flights delay data. Finally, we provide a discussion on future research in this area.}
}
@article{ANAGNOSTOPOULOS20187,
title = {Fintech and regtech: Impact on regulators and banks},
journal = {Journal of Economics and Business},
volume = {100},
pages = {7-25},
year = {2018},
note = {FinTech – Impact on Consumers, Banking and Regulatory Policy},
issn = {0148-6195},
doi = {https://doi.org/10.1016/j.jeconbus.2018.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S014861951730142X},
author = {Ioannis Anagnostopoulos},
keywords = {FinTech, RegTech, Business models, Regulation, Financial services, Future research direction},
abstract = {The purpose of this paper is to develop an insight and review the effect of FinTech development against the broader environment in financial technology. We further aim to offer various perspectives in order to aid the understanding of the disruptive potential of FinTech, and its implications for the wider financial ecosystem. By drawing upon very recent and highly topical research on this area this study examines the implications for financial institutions, and regulation especially when technology poses a challenge to the global banking and regulatory system. It is driven by a wide-ranging overview of the development, the current state, and possible future of fintech. This paper attempts to connect practitioner-led and academic research. While it draws on academic research, the perspective it takes is also practice-oriented. It relies on the current academic literature as well as insights from industry sources, action research and other publicly available commentaries. It also draws on professional practitioners’ roundtable discussions, and think-tanks in which the author has been an active participant. We attempt to interpret banking, and regulatory issues from a behavioural perspective. The last crisis exposed significant failures in regulation and supervision. It has made the Financial Market Law and Compliance a key topic on the current agenda. Disruptive technological change also seems to be important in investigating regulatory compliance followed by change. We contribute to the current literature review on financial and digital innovation by new entrants where this has also practical implications. We also provide for an updated review of the current regulatory issues addressing the contextual root causes of disruption within the financial services domain. The aim here is to assist market participants to improve effectiveness and collaboration. The difficulties arising from extensive regulation may suggest a more liberal and principled approach to financial regulation. Disruptive innovation has the potential for welfare outcomes for consumers, regulatory, and supervisory gains as well as reputational gains for the financial services industry. It becomes even more important as the financial services industry evolves. For example, the preparedness of the regulators to instil culture change and harmonise technological advancements with regulation could likely achieve many desired outcomes. Such results range from achieving an orderly market growth, further aiding systemic stability and restoring trust and confidence in the financial system. Our action-led research results have implications for both research and practice. These should be of interest to regulatory standard setters, investors, international organisations and other academics who are researching regulatory and competition issues, and their manifestation within the financial and social contexts. As a perspective on a social construct, this study appeals to regulators and law makers, entrepreneurs, and investors who participate in technology applied within the innovative financial services domain. It is also of interest to bankers who might consider FinTech and strategic partnerships as a prospective, future strategic direction.11We thank two anonymous referees for their very thoughtful and constructing comments who took a keen interest in reviewing our research and in this way contributed materially to the development of the paper. All other omissions and/or errors remain our own.}
}
@incollection{PETERS2018271,
title = {Chapter 12 - Blockchain Architectures for Electronic Exchange Reporting Requirements: EMIR, Dodd Frank, MiFID I/II, MiFIR, REMIT, Reg NMS and T2S},
editor = {David {Lee Kuo Chuen} and Robert Deng},
booktitle = {Handbook of Blockchain, Digital Finance, and Inclusion, Volume 2},
publisher = {Academic Press},
pages = {271-329},
year = {2018},
isbn = {978-0-12-812282-2},
doi = {https://doi.org/10.1016/B978-0-12-812282-2.00012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122822000127},
author = {Gareth W. Peters and Guy R. Vishnia},
keywords = {Blockchain, Blockchain transaction reporting, Dodd–Frank, EMIR, Exchange Regulation, MiFID I, MiFID II and MiFIR, REMIT, Reg NMS, T2S and CSD, Trade reporting, Transparency},
abstract = {Several international electronic primary financial exchanges have begun to announce they will explore the adoption of blockchain technology in their trade processing and reporting for execution and clearing. Therefore, in this work we will begin by providing a detailed discussion and overview of the new exchange regulations appearing in different jurisdictions around the world, including EMIR, Dodd–Frank, MiFID I/II, MiFIR, REMIT, Reg NMS and T2S. We will discuss their key features, specifically in regard to transparency reporting and trade/transaction reporting requirements. To achieve this we first discuss the emergence of a multitude of different trading venues that have arisen under the fragmentation directives for each asset class: equities, commodities, derivatives and currency. We highlight how each fits into this universe of different market and processing venues both primary, secondary, dark and lit and OTC.}
}
@article{YANG201860,
title = {History and trends in solar irradiance and PV power forecasting: A preliminary assessment and review using text mining},
journal = {Solar Energy},
volume = {168},
pages = {60-101},
year = {2018},
note = {Advances in Solar Resource Assessment and Forecasting},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2017.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X17310022},
author = {Dazhi Yang and Jan Kleissl and Christian A. Gueymard and Hugo T.C. Pedro and Carlos F.M. Coimbra},
keywords = {Text mining, Solar forecasting, Review, Photovoltaics},
abstract = {Text mining is an emerging topic that advances the review of academic literature. This paper presents a preliminary study on how to review solar irradiance and photovoltaic (PV) power forecasting (both topics combined as “solar forecasting” for short) using text mining, which serves as the first part of a forthcoming series of text mining applications in solar forecasting. This study contains three main contributions: (1) establishing the technological infrastructure (authors, journals & conferences, publications, and organizations) of solar forecasting via the top 1000 papers returned by a Google Scholar search; (2) consolidating the frequently-used abbreviations in solar forecasting by mining the full texts of 249 ScienceDirect publications; and (3) identifying key innovations in recent advances in solar forecasting (e.g., shadow camera, forecast reconciliation). As most of the steps involved in the above analysis are automated via an application programming interface, the presented method can be transferred to other solar engineering topics, or any other scientific domain, by means of changing the search word. The authors acknowledge that text mining, at its present stage, serves as a complement to, but not a replacement of, conventional review papers.}
}
@article{ROMANOU201899,
title = {The necessity of the implementation of Privacy by Design in sectors where data protection concerns arise},
journal = {Computer Law & Security Review},
volume = {34},
number = {1},
pages = {99-110},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917302054},
author = {Anna Romanou},
keywords = {Privacy by Design, Privacy, Practical implementation of Privacy by Design, Biometrics, e-health, Video-surveillance},
abstract = {This article examines the extent to which Privacy by Design can safeguard privacy and personal data within a rapidly evolving society. This paper will first briefly explain the theoretical concept and the general principles of Privacy by Design, as laid down in the General Data Protection Regulation. Then, by indicating specific examples of the implementation of the Privacy by Design approach, it will be demonstrated why the implementation of Privacy by Design is a necessity in a number of sectors where specific data protection concerns arise (biometrics, e-health and video-surveillance) and how it can be implemented.}
}
@article{HARMS2018972,
title = {Extending the Human Connectome Project across ages: Imaging protocols for the Lifespan Development and Aging projects},
journal = {NeuroImage},
volume = {183},
pages = {972-984},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918318652},
author = {Michael P. Harms and Leah H. Somerville and Beau M. Ances and Jesper Andersson and Deanna M. Barch and Matteo Bastiani and Susan Y. Bookheimer and Timothy B. Brown and Randy L. Buckner and Gregory C. Burgess and Timothy S. Coalson and Michael A. Chappell and Mirella Dapretto and Gwenaëlle Douaud and Bruce Fischl and Matthew F. Glasser and Douglas N. Greve and Cynthia Hodge and Keith W. Jamison and Saad Jbabdi and Sridhar Kandala and Xiufeng Li and Ross W. Mair and Silvia Mangia and Daniel Marcus and Daniele Mascali and Steen Moeller and Thomas E. Nichols and Emma C. Robinson and David H. Salat and Stephen M. Smith and Stamatios N. Sotiropoulos and Melissa Terpstra and Kathleen M. Thomas and M. Dylan Tisdall and Kamil Ugurbil and Andre {van der Kouwe} and Roger P. Woods and Lilla Zöllei and David C. {Van Essen} and Essa Yacoub},
keywords = {Connectomics, Resting-state, Functional connectivity, Task, Diffusion, Perfusion, Development, Aging, Lifespan},
abstract = {The Human Connectome Projects in Development (HCP-D) and Aging (HCP-A) are two large-scale brain imaging studies that will extend the recently completed HCP Young-Adult (HCP-YA) project to nearly the full lifespan, collecting structural, resting-state fMRI, task-fMRI, diffusion, and perfusion MRI in participants from 5 to 100+ years of age. HCP-D is enrolling 1300+ healthy children, adolescents, and young adults (ages 5–21), and HCP-A is enrolling 1200+ healthy adults (ages 36–100+), with each study collecting longitudinal data in a subset of individuals at particular age ranges. The imaging protocols of the HCP-D and HCP-A studies are very similar, differing primarily in the selection of different task-fMRI paradigms. We strove to harmonize the imaging protocol to the greatest extent feasible with the completed HCP-YA (1200+ participants, aged 22–35), but some imaging-related changes were motivated or necessitated by hardware changes, the need to reduce the total amount of scanning per participant, and/or the additional challenges of working with young and elderly populations. Here, we provide an overview of the common HCP-D/A imaging protocol including data and rationales for protocol decisions and changes relative to HCP-YA. The result will be a large, rich, multi-modal, and freely available set of consistently acquired data for use by the scientific community to investigate and define normative developmental and aging related changes in the healthy human brain.}
}
@article{KINAWY2017239,
title = {Mismatches in stakeholder communication: The case of the Leslie and Ferrand transit stations, Toronto, Canada},
journal = {Sustainable Cities and Society},
volume = {34},
pages = {239-249},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717302445},
author = {S.N. Kinawy and M. {Nik Bakht} and T.E. El-Diraby},
keywords = {Community engagement, Urban infrastructure planning, Information retrieval, Content analysis, Co-creation and collaborative planning},
abstract = {Using content analysis, an approach is presented to help extract topics of interest to local community during project planning. This is helpful for fine-tuning and customizing the language used in communication with the public. Hopefully, reducing communication mismatches can help support constructive dialogue that is not “lost in translation”. The extraction of community issues/interest is becoming important also to help guide the development of plans/projects and their features in a manner that meets their needs. The two cases used in this study presented a suitable target for developing and showcasing the proposed approach. There was a reversal of public decision based on community debates/objections. This allowed us to study the mismatches before and after the decision. The proposed approach used a context-based taxonomy of terms and content analysis to compare terms/topics contained in a related twitter account, relevant news articles, and documents/presentations used in public meetings—before and after the decision. The proposed approach was designed to be mostly automatic to help future re-use. Of course, the use of such approach is only one step in a much bigger qualitative and context-specific process. Specific to the two cases, it was observed that news media articles and the contents of twitter chats had higher matching levels in the topics/themes they covered. Contents of public meetings had some levels of mismatching. Particular to the two cases, public official tended to emphasize the technical aspects of the projects with limited/clear analysis of their functions or impacts on community. It is argued that, as a result, public officials should study twitter chats and news articles as they prepare official public documents and presentations to citizens; attempt to specifically address prevalent issues in them; and even use the same nomenclature. Using a (semi) automated tool can be very helpful in this regard.}
}
@article{CHEN2017673,
title = {Multi-scaling allometric analysis for urban and regional development},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {465},
pages = {673-689},
year = {2017},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2016.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378437116305246},
author = {Yanguang Chen},
keywords = {Allometric growth, Allometric scaling, Fractal dimension, Complex spatial system, Spatio-temporal evolution, Urbanization},
abstract = {The concept of allometric growth is based on scaling relations, and it has been applied to urban and regional analysis for a long time. However, most allometric analyses were devoted to the single proportional relation between two elements of a geographical system. Few researches focus on the allometric scaling of multielements. In this paper, a process of multiscaling allometric analysis is developed for the studies on spatio-temporal evolution of complex systems. By means of linear algebra, general system theory, and by analogy with the analytical hierarchy process, the concepts of allometric growth can be integrated with the ideas from fractal dimension. Thus a new methodology of geo-spatial analysis and the related theoretical models emerge. Based on the least squares regression and matrix operations, a simple algorithm is proposed to solve the multiscaling allometric equation. Applying the analytical method of multielement allometry to Chinese cities and regions yields satisfying results. A conclusion is reached that the multiscaling allometric analysis can be employed to make a comprehensive evaluation for the relative levels of urban and regional development, and explain spatial heterogeneity. The notion of multiscaling allometry may enrich the current theory and methodology of spatial analyses of urban and regional evolution.}
}
@incollection{KORRES201851,
title = {2.04 - GIS for Hydrology},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {51-80},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09635-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489096354},
author = {Wolfgang Korres and Karl Schneider},
keywords = {Decision support systems, GIS, Hydrological data, Hydrological models, Hydrology, Spatial interpolation, Terrain analysis, Water resources, Watershed delineation},
abstract = {This article aims at providing an overview of GIS applications for hydrology. Hydrology is inherently a spatiotemporal science. Thus, a vast and ever-growing number of applications of GIS in hydrology exists. This chapter therefore shall serve as a point of reference rather than a comprehensive account. While GIS has strongly impacted hydrological model development, data analysis and communication of issues of hydrology, hydrological applications have on the other hand evolved into standard GIS applications. This article provides an overview about some of the key applications, procedures and tasks. Data structures used in hydrology, data sources, and uncertainty in hydrological data are discussed. Standard GIS methods for hydrology are presented, including terrain analysis for flow direction, watershed delineation and drainage network processing or spatial interpolation of point data. Frequently used methods to infer hydrological parameters from generally available data sets are discussed using the example of the NRCS curve number approach and pedotransfer functions (PTFs). Numerical modeling has a long tradition in hydrology. Different integrating approaches of GIS and hydrological models exist. These are presented, along with examples of GIS applications for hydrology by international, national and local entities, including spatial data infrastructure, data portals and data services. Water resources management requires careful consideration of the interests and needs of the different stakeholders, as well as understanding of the spatial and temporal dynamics of water. Spatial decision support systems which integrate natural as well as societal processes are essential to understand water use potentials and limits. Some key elements of these decision support systems are presented and discussed, particularly against the background of the challenges of Global Change. Water is both an essence of life as well as a risk. Thus hydrological information is essential to experts and laymen alike. Approaches for citizen participation and citizen science are presented using web and smartphone applications. This article concludes with some remarks on future prospects for GIS for hydrology.}
}
@incollection{KANTERAKIS201815,
title = {Chapter 2 - Creating Transparent and Reproducible Pipelines: Best Practices for Tools, Data, and Workflow Management Systems},
editor = {Christophe G. Lambert and Darrol J. Baker and George P. Patrinos},
booktitle = {Human Genome Informatics},
publisher = {Academic Press},
pages = {15-43},
year = {2018},
isbn = {978-0-12-809414-3},
doi = {https://doi.org/10.1016/B978-0-12-809414-3.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094143000024},
author = {Alexandros Kanterakis and George Potamias and Morris A. Swertz and George P. Patrinos},
keywords = {Workflows, Bioinformatics, Open science},
abstract = {Recently, the practice of properly sharing the source code, analysis pipelines, and protocols of published studies has become commonplace in bioinformatics. In addition, there is a plethora of technically mature workflow management systems (WMS) that offer simple and user-friendly environments where users can submit tools and build transparent, shareable, and reproducible pipelines. Arguably, the adoption of open science policies and the availability of efficient WMSs constitute major progress toward battling the replication crisis, advancing research dissemination, and creating new collaborations. Yet now we still see that it is very difficult to include a large range of tools in a scientific pipeline, whereas on the other side, certain technical and design choices of modern WMSs discourage users from doing just this. Here we present three sets of easily applicable “best practices” targeting (i) bioinformatics tool developers, (ii) data curators, and (iii) WMS engineers, respectively. These practices aim to make it easier to add tools to a pipeline, to make it easier to directly process data, and to make WMSs widely hospitable for any external tool or pipeline. We also show how following these guidelines can directly benefit the research community.}
}
@article{BONNEL201869,
title = {Origin-Destination estimation using mobile network probe data},
journal = {Transportation Research Procedia},
volume = {32},
pages = {69-81},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301649},
author = {Patrick Bonnel and Mariem Fekih and Zbigniew Smoreda},
keywords = {origin-destination matrix, mobile phone data, travel survey, passive data, Rhône-Alpes},
abstract = {Mobile phone operators produce enormous amounts of data. In this paper we present applications performed with a dataset (probe data) collected by the operator Orange in 2017 in Rhône Alpes Region, France. Trips are deduced from the spatio-temporal trajectory of devices through a hypothesis of stationarity in order to define activities. Trips are then aggregated in an origin-destination matrix which is compared with traditional data (household travel survey). With some hypothesis we obtain somewhat similar origin-destination matrix, with a slope close to one when we regress the number of trips of each origin-destination from mobile phone data with household survey data.}
}
@article{HINDLE2018836,
title = {Developing a business analytics methodology: A case study in the foodbank sector},
journal = {European Journal of Operational Research},
volume = {268},
number = {3},
pages = {836-851},
year = {2018},
note = {Community Operational Research: Innovations, internationalization and agenda-setting applications},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717305702},
author = {Giles A. Hindle and Richard Vidgen},
keywords = {Analytics, OR for community development, Data mining, Problem structuring methods, Soft systems methodology},
abstract = {The current research seeks to address the following question: how can organizations align their business analytics development projects with their business goals? To pursue this research agenda we adopt an action research framework to develop and apply a business analytics methodology (BAM). The four-stage BAM (problem situation structuring, business model mapping, analytics leverage analysis, and analytics implementation) is not a prescription. Rather, it provides a logical structure and logical precedence of activities that can be used to guide the practice of analytics (i.e., a mental model). The client for the action research project is The Trussell Trust, which is a UK charity with the mission of empowering local communities to combat poverty and exclusion. As part of the action research project the research team created the UK's first dynamic visualization tool for crises related to food poverty. The prototype uses foodbank data to map geographical demand and aligns findings to 2011 Census data to predict where additional foodbanks may be needed. Research findings are that: (1) the analytics methodology provides an umbrella for, and applies equally to, data science and Operational Research (OR); (2) that the practice of business analytics is an entangled and emergent mix of top-down analysis and bottom-up action; and, (3) that, for the third sector in particular, analytics can be usefully approached as a collective and community endeavor.}
}
@article{WASESA201737,
title = {The seaport service rate prediction system: Using drayage truck trajectory data to predict seaport service rates},
journal = {Decision Support Systems},
volume = {95},
pages = {37-48},
year = {2017},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616302032},
author = {Meditya Wasesa and Andries Stam and Eric {van Heck}},
keywords = {Predictive analytics, Gradient boosting model, Trajectory data, Vehicle telematics system, Seaport appointment system, Drayage operation},
abstract = {For drayage operators the service rate of seaports is crucial for organizing their container pick-up/delivery operations. This study presents a seaport service rate prediction system that could help drayage operators to improve their predictions of the duration of the pick-up/delivery operations at a seaport by using the subordinate trucks' trajectory data. The system is constructed based on three components namely, trajectory reconstruction, geo-fencing analysis, and gradient boosting modelling. Using predictive analytic techniques, the prediction system is trained and validated using more than 15million data records from over 200 trucks over a period of 19months. The gradient boosting model-based solution provides better predictions compared with the linear model benchmark solution. Conclusions and implications are formulated.}
}
@article{SLIKKER2018115,
title = {Emerging technologies for food and drug safety},
journal = {Regulatory Toxicology and Pharmacology},
volume = {98},
pages = {115-128},
year = {2018},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0273230018301971},
author = {William Slikker and Thalita Antony {de Souza Lima} and Davide Archella and Jarbas Barbosa {de Silva} and Tara Barton-Maclaren and Li Bo and Danitza Buvinich and Qasim Chaudhry and Peiying Chuan and Hubert Deluyker and Gary Domselaar and Meiruze Freitas and Barry Hardy and Hans-Georg Eichler and Marta Hugas and Kenneth Lee and Chia-Ding Liao and Lit-Hsin Loo and Haruhiro Okuda and Orish Ebere Orisakwe and Anil Patri and Carl Sactitono and Leming Shi and Primal Silva and Frank Sistare and Shraddha Thakkar and Weida Tong and Mary Lou Valdez and Maurice Whelan and Anna Zhao-Wong},
keywords = {Global coalition, Emerging technologies, Food safety, Drug safety, Regulatory agencies, Alternative methods, Best practices, Cross-training},
abstract = {Emerging technologies are playing a major role in the generation of new approaches to assess the safety of both foods and drugs. However, the integration of emerging technologies in the regulatory decision-making process requires rigorous assessment and consensus amongst international partners and research communities. To that end, the Global Coalition for Regulatory Science Research (GCRSR) in partnership with the Brazilian Health Surveillance Agency (ANVISA) hosted the seventh Global Summit on Regulatory Science (GSRS17) in Brasilia, Brazil on September 18–20, 2017 to discuss the role of new approaches in regulatory science with a specific emphasis on applications in food and medical product safety. The global regulatory landscape concerning the application of new technologies was assessed in several countries worldwide. Challenges and issues were discussed in the context of developing an international consensus for objective criteria in the development, application and review of emerging technologies. The need for advanced approaches to allow for faster, less expensive and more predictive methodologies was elaborated. In addition, the strengths and weaknesses of each new approach was discussed. And finally, the need for standards and reproducible approaches was reviewed to enhance the application of the emerging technologies to improve food and drug safety. The overarching goal of GSRS17 was to provide a venue where regulators and researchers meet to develop collaborations addressing the most pressing scientific challenges and facilitate the adoption of novel technical innovations to advance the field of regulatory science.}
}
@incollection{LOHR201825,
title = {Chapter 3 - Genomic Approaches to Hematology},
editor = {Ronald Hoffman and Edward J. Benz and Leslie E. Silberstein and Helen E. Heslop and Jeffrey I. Weitz and John Anastasi and Mohamed E. Salama and Syed Ali Abutalib},
booktitle = {Hematology (Seventh Edition)},
publisher = {Elsevier},
edition = {Seventh Edition},
pages = {25-36},
year = {2018},
isbn = {978-0-323-35762-3},
doi = {https://doi.org/10.1016/B978-0-323-35762-3.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323357623000032},
author = {Jens G. Lohr and Birgit Knoechel and Todd R. Golub},
keywords = {Next generation DNA- and RNA- sequencing, Genomics, Epigenetics/Epigenomics, Genome editing, High throughput profiling, Unsupervised computational analysis}
}
@article{REN20181,
title = {Structure-oriented prediction in complex networks},
journal = {Physics Reports},
volume = {750},
pages = {1-51},
year = {2018},
note = {Structure-oriented prediction in complex networks},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0370157318301339},
author = {Zhuo-Ming Ren and An Zeng and Yi-Cheng Zhang},
keywords = {Complex networks, Prediction, Network structure, Network dynamics},
abstract = {Complex systems are extremely hard to predict due to its highly nonlinear interactions and rich emergent properties. Thanks to the rapid development of network science, our understanding of the structure of real complex systems and the dynamics on them has been remarkably deepened, which meanwhile largely stimulates the growth of effective prediction approaches on these systems. In this article, we aim to review different network-related prediction problems, summarize and classify relevant prediction methods, analyze their advantages and disadvantages, and point out the forefront as well as critical challenges of the field.}
}
@article{OTGONBAYAR2018238,
title = {K-VARP: K-anonymity for varied data streams via partitioning},
journal = {Information Sciences},
volume = {467},
pages = {238-255},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.07.057},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518305772},
author = {Ankhbayar Otgonbayar and Zeeshan Pervez and Keshav Dahal and Steve Eager},
keywords = {Internet of things, Data privacy, Data streams, Anonymization, Missing values},
abstract = {The Internet-of-Things (IoT) produces and transmits enormous amounts of data. Extracting valuable information from this enormous volume of data has become an important consideration for businesses and research. However, extracting information from this data without providing privacy protection puts individuals at risk. Data has to be sanitized before use, and anonymization provides solution to this problem. Since, IoT is a collection of numerous different devices, data streams from these devices tend to vary over time thus creating varied data streams. However, implementing traditional data stream anonymization approaches only provide privacy protection for data streams that have predefined and fixed attributes. Therefore, conventional methods cannot directly work on varied data streams. In this work, we propose K-VARP (K-anonymity for VARied data stream via Partitioning) to publish varied data streams. K-VARP reads the tuple and assigns them to partitions based on description, and all tuples must be anonymized before expiring. It tries to anonymize expiring tuple within a partition if its partition is eligible to produce a K-anonymous cluster. Otherwise, partition merging is applied. In K-VARP we propose a new merging criterion called R-likeness to measure similarity distance between tuple and partitions. Moreover, flexible re-using and imputation free-publication is implied in K-VARP to achieve better anonymization quality and performance. Our experiments on a real datasets show that K-VARP is efficient and effective compared to existing algorithms. K-VARP demonstrated approximately three to nine and ten to twenty percent less information loss on two real datasets, while forming a similar number of clusters within a comparable computation time.}
}
@incollection{JUTZ2018150,
title = {1.06 - Copernicus Program},
editor = {Shunlin Liang},
booktitle = {Comprehensive Remote Sensing},
publisher = {Elsevier},
address = {Oxford},
pages = {150-191},
year = {2018},
isbn = {978-0-12-803221-3},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10317-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489103173},
author = {S. Jutz and M.P. Milagro-Pérez},
keywords = {Climate change, Data policy, Earth observation, Environment, Monitoring, Policy makers, Satellite, Sensor, Sentinel, Space component},
abstract = {Copernicus is one of the most ambitious, most comprehensive Earth observation systems worldwide. It aims at giving decision makers reliable and up-to-date information to coordinate policy areas and formulate strategies relating to the environment, at global, continental, national, and regional levels. It is led by the European Union, with the European Space Agency as the main partner. The Copernicus program has now entered into its operational phase with information services up and running in the environmental domains. The full benefits of the program will however materialize only when the wealth of data from the entire fleet of dedicated satellites will be delivered to users in an accurate and timely manner.}
}
@article{OLIVARESOLIVARES2018628,
title = {Technological innovation within the Spanish tax administration and data subjects' right to access: An opportunity knocks},
journal = {Computer Law & Security Review},
volume = {34},
number = {3},
pages = {628-639},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917303825},
author = {Bernardo D. {Olivares Olivares}},
keywords = {Right to access, Personal information, Tax related data, Spanish tax administration, GDPR},
abstract = {In this paper, we analyse the data subjects' right to access their personal data in the context of the Spanish Tax Administration and the legal consequences of the upcoming General Data Protection Regulation. The results show that there are still difficulties related to the scope of this right, the establishment of proper storage criteria, and in the procedures used by the data controllers to provide accurate information to the data subjects. This situation highlights the necessity to incorporate such technological innovation as metadata labelling and automatic computerised procedures to ensure an optimum management of the data subjects' access to their tax related personal information.}
}
@article{SADEGHIAN201894,
title = {Robust probabilistic principal component analysis based process modeling: Dealing with simultaneous contamination of both input and output data},
journal = {Journal of Process Control},
volume = {67},
pages = {94-111},
year = {2018},
note = {Big Data: Data Science for Process Control and Operations},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2017.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959152417300768},
author = {A. Sadeghian and O. Wu and B. Huang},
keywords = {Robust predictive models, Input–output outliers, Gaussian location mixture distribution, Probabilistic principal component analysis (PPCA), Expectation Maximization (EM) algorithm, Robustness, Soft sensors},
abstract = {In this work, one of the common issues, the robustness of the soft sensors, in development of such predictive models is discussed and the solution is provided. Large random errors, also known as outliers are one inseparable characteristic of data sets which can be caused by various reasons. Robust probabilistic predictive models overcome this problem by appropriate formulation of noise distributions. In this work possible outliers are considered for both input and output data in contrast to the traditional robust algorithms that have focused on output outliers only. Probabilistic principal component analysis based regression is used for the predictive model in this work and Expectation Maximization algorithm is applied to solve a complex robust estimation problem. Finally the performance of the developed robust predictive model is evaluated by simulated and industrial case studies. This work is a generalization to the traditional robust probabilistic principal component analysis based regression modeling work which considered a different type of outliers that occur in the output only.}
}
@article{BISCHOF201822,
title = {Enriching integrated statistical open city data by combining equational knowledge and missing value imputation},
journal = {Journal of Web Semantics},
volume = {48},
pages = {22-47},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300355},
author = {Stefan Bischof and Andreas Harth and Benedikt Kämpgen and Axel Polleres and Patrik Schneider},
keywords = {Open data, Linked Data, Data cleaning, Data integration},
abstract = {Several institutions collect statistical data about cities, regions, and countries for various purposes. Yet, while access to high quality and recent such data is both crucial for decision makers and a means for achieving transparency to the public, all too often such collections of data remain isolated and not re-useable, let alone comparable or properly integrated. In this paper we present the Open City Data Pipeline, a focused attempt to collect, integrate, and enrich statistical data collected at city level worldwide, and re-publish the resulting dataset in a re-useable manner as Linked Data. The main features of the Open City Data Pipeline are: (i) we integrate and cleanse data from several sources in a modular and extensible, always up-to-date fashion; (ii) we use both Machine Learning techniques and reasoning over equational background knowledge to enrich the data by imputing missing values, (iii) we assess the estimated accuracy of such imputations per indicator. Additionally, (iv) we make the integrated and enriched data, including links to external data sources, such as DBpedia, available both in a web browser interface and as machine-readable Linked Data, using standard vocabularies such as QB and PROV. Apart from providing a contribution to the growing collection of data available as Linked Data, our enrichment process for missing values also contributes a novel methodology for combining rule-based inference about equational knowledge with inferences obtained from statistical Machine Learning approaches. While most existing works about inference in Linked Data have focused on ontological reasoning in RDFS and OWL, we believe that these complementary methods and particularly their combination could be fruitfully applied also in many other domains for integrating Statistical Linked Data, independent from our concrete use case of integrating city data.}
}
@article{ATKINSON2017216,
title = {Scientific workflows: Past, present and future},
journal = {Future Generation Computer Systems},
volume = {75},
pages = {216-227},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311202},
author = {Malcolm Atkinson and Sandra Gesing and Johan Montagnat and Ian Taylor},
keywords = {Scientific workflows, Scientific methods, Optimisation, Performance, Usability},
abstract = {This special issue and our editorial celebrate 10 years of progress with data-intensive or scientific workflows. There have been very substantial advances in the representation of workflows and in the engineering of workflow management systems (WMS). The creation and refinement stages are now well supported, with a significant improvement in usability. Improved abstraction supports cross-fertilisation between different workflow communities and consistent interpretation as WMS evolve. Through such re-engineering the WMS deliver much improved performance, significantly increased scale and sophisticated reliability mechanisms. Further improvement is anticipated from substantial advances in optimisation. We invited papers from those who have delivered these advances and selected 14 to represent today’s achievements and representative plans for future progress. This editorial introduces those contributions with an overview and categorisation of the papers. Furthermore, it elucidates responses from a survey of major workflow systems, which provides evidence of substantial progress and a structured index of related papers. We conclude with suggestions on areas where further research and development is needed and offer a vision of future research directions.}
}
@article{LEE201729,
title = {Feature selection in multimedia: The state-of-the-art review},
journal = {Image and Vision Computing},
volume = {67},
pages = {29-42},
year = {2017},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0262885617301518},
author = {Pui Yi Lee and Wei Ping Loh and Jeng Feng Chin},
keywords = {Feature selection, Multimedia, Data mining, Search strategies},
abstract = {Multimedia data mining, particularly feature selection (FS), has been successfully applied in recent classification and recognition works. However, only a few studies in the contemporary literature have reviewed FS (e.g., analyses of data pre-processing prior to classification and clustering). This study aimed to fill this research gap by presenting an extensive survey on the current development of FS in multimedia. A total of 70 related papers published from 2001 to 2017 were collected from multiple databases. Breakdowns and analyses were performed on data types, methods, search strategies, performance measures, and challenges. The development trend of FS presages the increased prominence of heuristic search strategies and hybrid FS in the latest multimedia data mining.}
}
@article{GIANNIOU2018840,
title = {Clustering-based analysis for residential district heating data},
journal = {Energy Conversion and Management},
volume = {165},
pages = {840-850},
year = {2018},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2018.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S019689041830236X},
author = {Panagiota Gianniou and Xiufeng Liu and Alfred Heller and Per Sieverts Nielsen and Carsten Rode},
keywords = {Clustering, Load pattern, Load profiling, Load transition, Variability},
abstract = {The wide use of smart meters enables collection of a large amount of fine-granular time series, which can be used to improve the understanding of consumption behavior and used for consumption optimization. This paper presents a clustering-based knowledge discovery in databases method to analyze residential heating consumption data and evaluate information included in national building databases. The proposed method uses the K-means algorithm to segment consumption groups based on consumption intensity and representative patterns and ranks the groups according to daily consumption. This paper also examines the correlation between energy intensity and the characteristics of buildings and occupants, load profiles of households, consumption behavior changes over time, and consumption variability. The results show that the majority of the customers can be represented by fairly constant load profiles. Calendar context has an impact not only on the patterns but also on the consumption intensity and user behaviors. The variability studies show that consumption patterns are serially correlated, the customers with high energy consumption have lower variability, and the consumption is more stable over time. These findings will be valuable for district heating utilities and energy planners to optimize their operations, design demand-side management strategies, and develop targeting energy-efficiency programs or policies.}
}
@article{CRIMMINS20181127,
title = {Commentary: Integrating non-targeted and targeted chemical screening in Great Lakes fish monitoring programs},
journal = {Journal of Great Lakes Research},
volume = {44},
number = {5},
pages = {1127-1135},
year = {2018},
issn = {0380-1330},
doi = {https://doi.org/10.1016/j.jglr.2018.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0380133018301345},
author = {Bernard S. Crimmins and Harry B. McCarty and Sujan Fernando and Michael S. Milligan and James J. Pagano and Thomas M. Holsen and Philip K. Hopke},
keywords = {Great Lakes, Monitoring, Fish, Non-targeted, Persistent bioacccumulative toxics (PBTs), Emerging chemicals of concern},
abstract = {The Great Lakes are a vital resource for drinking water and recreation and provide a major fishery for millions of people. As part of the Great Lakes Water Quality Agreement, the US and Canadian governments have been charged with the protection of this system. Persistent, bioaccumulative, and toxic (PBTs) contaminants were found to be affecting the lake water quality as early as the late 1960s, and various programs sponsored by the US and Canada have been created to monitor PBTs such as polychlorinated biphenyls (PCBs) and organochlorine pesticides (OCPs). These programs have refined measurement techniques to quantify trace level contaminants using a targeted analytical approach. However, new PBTs are being detected in the environment, and the traditional targeted methodology is inadequate for understanding the complex chemical mixture affecting Great Lakes wildlife. Fortunately, new analytical technologies are emerging that allow for comprehensive screening of PBTs beyond targeted methods. The current commentary presents an outline of a new framework for contemporary monitoring programs. The goal is to facilitate the compilation of legacy, emerging PBT, and archive PBT signatures by utilizing the basic practices of traditional targeted analysis. This example focuses on fish monitoring programs, and how they are ideally suited for legacy monitoring as well as data-driven discovery of new chemicals of concern.}
}
@article{ALVAREZ201860,
title = {Discovering role interaction models in the Emergency Room using Process Mining},
journal = {Journal of Biomedical Informatics},
volume = {78},
pages = {60-77},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S153204641730285X},
author = {Camilo Alvarez and Eric Rojas and Michael Arias and Jorge Munoz-Gama and Marcos Sepúlveda and Valeria Herskovic and Daniel Capurro},
keywords = {Healthcare, Processes, Process mining, Case studies, Organizational mining, Organizational team patterns},
abstract = {Objectives
A coordinated collaboration among different healthcare professionals in Emergency Room (ER) processes is critical to promptly care for patients who arrive at the hospital in a delicate health condition, claiming for an immediate attention. The aims of this study are (i) to discover role interaction models in (ER) processes using process mining techniques; (ii) to understand how healthcare professionals are currently collaborating; and (iii) to provide useful knowledge that can help to improve ER processes.
Methods
A four step method based on process mining techniques is proposed. An ER process of a university hospital was considered as a case study, using 7160 episodes that contains specific ER episode attributes.
Results
Insights about how healthcare professionals collaborate in the ER was discovered, including the identification of a prevalent role interaction model along the major triage categories and specific role interaction models for different diagnoses. Also, common and exceptional professional interaction models were discovered at the role level.
Conclusions
This study allows the discovery of role interaction models through the use of real-life clinical data and process mining techniques. Results show a useful way of providing relevant insights about how healthcare professionals collaborate, uncovering opportunities for process improvement.}
}
@article{SANCHEZTRIGUEROS201713,
title = {Assessing the effects of temporal ambivalence on defining palaeosystem interrelations, and applicability to the analysis of archaeological survey data},
journal = {Quaternary International},
volume = {435},
pages = {13-34},
year = {2017},
note = {Staring at the ground: Archaeological surveys as a research tool},
issn = {1040-6182},
doi = {https://doi.org/10.1016/j.quaint.2015.09.089},
url = {https://www.sciencedirect.com/science/article/pii/S1040618215011349},
author = {Fernando Sánchez-Trigueros and Alfonso Benito-Calvo and Marta Navazo and Antoni Canals},
keywords = {Uncertainty, Epistemic ambivalence, Archaeological survey data, Data mining, Middle Palaeolithic, Sierra de Atapuerca},
abstract = {We present a possibility-based method with applications to the analysis of archaeological survey data under constraints of temporal ambivalence. The method is a generalizable heuristic for automated learning and data mining that estimates the set of possible responses associated with ambivalent numerical data, by (i) assessing the degree of epistemic ambivalence that is carried in a data set with ambivalent information, and by (ii) monitoring the effects of this degree on data analysis, enabling the adaptation of classical data analysis to the treatment of both crisp and fuzzy information. Owing to the strong methodological foundations of the application, a detailed introduction of the ambivalent paradigm is presented first so its adaptability to archaeological survey data can be followed later. The approach is demonstrated in selecting informative features of uncertain palaeosystems by ambivalent Principal Component Analysis (PCA), and exemplified with archaeological survey data collected in the Sierra de Atapuerca and the Arlanzón valley (Burgos, Spain), focusing on the effects of temporal ambivalence on the analysis of associations between lithological palaeoenvironments and Middle Palaeolithic technologies in an open-air context. Although survey data in this region is affected by varying degrees of temporal ambivalence that, on occasion, span several isotopic stages of the late Pleistocene sequence, its effects on feature selection, regarding PCA, are not significant enough to produce contradictory reprojections of palaeolithological data onto the principal space, thanks to a gradual stabilization of the relief in the Middle Arlanzón valley, on a regional scale, during the late Pleistocene. By applying correlation-based dissimilarity distances and cluster analysis to the grouping of ambivalent score patterns, ambivalent features that describe the palaeolithological context of the Middle Palaeolithic scatters can be grouped on the PC1 × PC2 principal plane into four contexts with a similar palaeolithological evolution among group members: the Mesozoic crest of the Sierra de Atapuerca (Group 1), the Neogene levels of the Villalval-Rubena platform (Group 2), lithologically mixed areas between fluvial valleys and Neogene hills (Group 3), and the valleys of the Arlanzón, Pico and Vena rivers (Group 4). In none of these groups the evolution of the local palaeolandscape is notable enough, once again on a regional scale, to produce conflicting descriptions of the palaeolithological signatures where the archaeological scatters occur, although Group 3 (a geographical interface between different lithological palaeoenvironments) has been more dynamic than the rest of environments over the time range the lithic scatters would have formed.}
}
@article{SCHUH2018144,
title = {Data-Based Determination of the Product-Oriented Complexity Degree},
journal = {Procedia CIRP},
volume = {70},
pages = {144-149},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.293},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304748},
author = {Günther Schuh and Christian Dölle and Stephan Schmitz and Jan Koch and Marius Höding and Alexander Menges},
keywords = {complexity management, innovation management, portfolio management, product data management (PDM), data driven design},
abstract = {Available data within today´s digitalized product life cycles comprise necessary information and is currently used in an unstructured way without sufficient description of interrelations. One major challenge in managing product complexity is the tradeoff between standardization measures and customized solutions. Therefore, a transparent overview and effective controlling of product-induced complexity is required and could be supported by product-related data analysis. This paper aims at the development of a generic approach for the data-based determination of the complexity degree.}
}
@article{LIGTHART2018691,
title = {Genome Analyses of >200,000 Individuals Identify 58 Loci for Chronic Inflammation and Highlight Pathways that Link Inflammation and Complex Disorders},
journal = {The American Journal of Human Genetics},
volume = {103},
number = {5},
pages = {691-706},
year = {2018},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2018.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0002929718303203},
author = {Symen Ligthart and Ahmad Vaez and Urmo Võsa and Maria G. Stathopoulou and Paul S. {de Vries} and Bram P. Prins and Peter J. {Van der Most} and Toshiko Tanaka and Elnaz Naderi and Lynda M. Rose and Ying Wu and Robert Karlsson and Maja Barbalic and Honghuang Lin and René Pool and Gu Zhu and Aurélien Macé and Carlo Sidore and Stella Trompet and Massimo Mangino and Maria Sabater-Lleal and John P. Kemp and Ali Abbasi and Tim Kacprowski and Niek Verweij and Albert V. Smith and Tao Huang and Carola Marzi and Mary F. Feitosa and Kurt K. Lohman and Marcus E. Kleber and Yuri Milaneschi and Christian Mueller and Mahmudul Huq and Efthymia Vlachopoulou and Leo-Pekka Lyytikäinen and Christopher Oldmeadow and Joris Deelen and Markus Perola and Jing Hua Zhao and Bjarke Feenstra and Behrooz Z. Alizadeh and H. Marike Boezen and Lude Franke and Pim {van der Harst} and Gerjan Navis and Marianne Rots and Harold Snieder and Morris Swertz and Bruce H.R. Wolffenbuttel and Cisca Wijmenga and Marzyeh Amini and Emelia Benjamin and Daniel I. Chasman and Abbas Dehghan and Tarunveer Singh Ahluwalia and James Meigs and Russell Tracy and Behrooz Z. Alizadeh and Symen Ligthart and Josh Bis and Gudny Eiriksdottir and Nathan Pankratz and Myron Gross and Alex Rainer and Harold Snieder and James G. Wilson and Bruce M. Psaty and Josee Dupuis and Bram Prins and Urmo Vaso and Maria Stathopoulou and Lude Franke and Terho Lehtimaki and Wolfgang Koenig and Yalda Jamshidi and Sophie Siest and Ali Abbasi and Andre G. Uitterlinden and Mohammadreza Abdollahi and Renate Schnabel and Ursula M. Schick and Ilja M. Nolte and Aldi Kraja and Yi-Hsiang Hsu and Daniel S. Tylee and Alyson Zwicker and Rudolf Uher and George Davey-Smith and Alanna C. Morrison and Andrew Hicks and Cornelia M. {van Duijn} and Cavin Ward-Caviness and Eric Boerwinkle and J. Rotter and Ken Rice and Leslie Lange and Markus Perola and Eco {de Geus} and Andrew P. Morris and Kari Matti Makela and David Stacey and Johan Eriksson and Tim M. Frayling and Eline P. Slagboom and Jari Lahti and Katharina E. Schraut and Myriam Fornage and Bhoom Suktitipat and Wei-Min Chen and Xiaohui Li and Teresa Nutile and Giovanni Malerba and Jian’an Luan and Tom Bak and Nicholas Schork and Fabiola {Del Greco M.} and Elisabeth Thiering and Anubha Mahajan and Riccardo E. Marioni and Evelin Mihailov and Joel Eriksson and Ayse Bilge Ozel and Weihua Zhang and Maria Nethander and Yu-Ching Cheng and Stella Aslibekyan and Wei Ang and Ilaria Gandin and Loïc Yengo and Laura Portas and Charles Kooperberg and Edith Hofer and Kumar B. Rajan and Claudia Schurmann and Wouter {den Hollander} and Tarunveer S. Ahluwalia and Jing Zhao and Harmen H.M. Draisma and Ian Ford and Nicholas Timpson and Alexander Teumer and Hongyan Huang and Simone Wahl and YongMei Liu and Jie Huang and Hae-Won Uh and Frank Geller and Peter K. Joshi and Lisa R. Yanek and Elisabetta Trabetti and Benjamin Lehne and Diego Vozzi and Marie Verbanck and Ginevra Biino and Yasaman Saba and Ingrid Meulenbelt and Jeff R. O’Connell and Markku Laakso and Franco Giulianini and Patrik K.E. Magnusson and Christie M. Ballantyne and Jouke Jan Hottenga and Grant W. Montgomery and Fernando Rivadineira and Rico Rueedi and Maristella Steri and Karl-Heinz Herzig and David J. Stott and Cristina Menni and Mattias Frånberg and Beate {St. Pourcain} and Stephan B. Felix and Tune H. Pers and Stephan J.L. Bakker and Peter Kraft and Annette Peters and Dhananjay Vaidya and Graciela Delgado and Johannes H. Smit and Vera Großmann and Juha Sinisalo and Ilkka Seppälä and Stephen R. Williams and Elizabeth G. Holliday and Matthijs Moed and Claudia Langenberg and Katri Räikkönen and Jingzhong Ding and Harry Campbell and Michele M. Sale and Yii-Der I. Chen and Alan L. James and Daniela Ruggiero and Nicole Soranzo and Catharina A. Hartman and Erin N. Smith and Gerald S. Berenson and Christian Fuchsberger and Dena Hernandez and Carla M.T. Tiesler and Vilmantas Giedraitis and David Liewald and Krista Fischer and Dan Mellström and Anders Larsson and Yunmei Wang and William R. Scott and Matthias Lorentzon and John Beilby and Kathleen A. Ryan and Craig E. Pennell and Dragana Vuckovic and Beverly Balkau and Maria Pina Concas and Reinhold Schmidt and Carlos F. {Mendes de Leon} and Erwin P. Bottinger and Margreet Kloppenburg and Lavinia Paternoster and Michael Boehnke and A.W. Musk and Gonneke Willemsen and David M. Evans and Pamela A.F. Madden and Mika Kähönen and Zoltán Kutalik and Magdalena Zoledziewska and Ville Karhunen and Stephen B. Kritchevsky and Naveed Sattar and Genevieve Lachance and Robert Clarke and Tamara B. Harris and Olli T. Raitakari and John R. Attia and Diana {van Heemst} and Eero Kajantie and Rossella Sorice and Giovanni Gambaro and Robert A. Scott and Andrew A. Hicks and Luigi Ferrucci and Marie Standl and Cecilia M. Lindgren and John M. Starr and Magnus Karlsson and Lars Lind and Jun Z. Li and John C. Chambers and Trevor A. Mori and Eco J.C.N. {de Geus} and Andrew C. Heath and Nicholas G. Martin and Juha Auvinen and Brendan M. Buckley and Anton J.M. {de Craen} and Melanie Waldenberger and Konstantin Strauch and Thomas Meitinger and Rodney J. Scott and Mark McEvoy and Marian Beekman and Cristina Bombieri and Paul M. Ridker and Karen L. Mohlke and Nancy L. Pedersen and Alanna C. Morrison and Dorret I. Boomsma and John B. Whitfield and David P. Strachan and Albert Hofman and Peter Vollenweider and Francesco Cucca and Marjo-Riitta Jarvelin and J. Wouter Jukema and Tim D. Spector and Anders Hamsten and Tanja Zeller and André G. Uitterlinden and Matthias Nauck and Vilmundur Gudnason and Lu Qi and Harald Grallert and Ingrid B. Borecki and Jerome I. Rotter and Winfried März and Philipp S. Wild and Marja-Liisa Lokki and Michael Boyle and Veikko Salomaa and Mads Melbye and Johan G. Eriksson and James F. Wilson and Brenda W.J.H. Penninx and Diane M. Becker and Bradford B. Worrall and Greg Gibson and Ronald M. Krauss and Marina Ciullo and Gianluigi Zaza and Nicholas J. Wareham and Albertine J. Oldehinkel and Lyle J. Palmer and Sarah S. Murray and Peter P. Pramstaller and Stefania Bandinelli and Joachim Heinrich and Erik Ingelsson and Ian J. Deary and Reedik Mägi and Liesbeth Vandenput and Pim {van der Harst} and Karl C. Desch and Jaspal S. Kooner and Claes Ohlsson and Caroline Hayward and Terho Lehtimäki and Alan R. Shuldiner and Donna K. Arnett and Lawrence J. Beilin and Antonietta Robino and Philippe Froguel and Mario Pirastu and Tine Jess and Wolfgang Koenig and Ruth J.F. Loos and Denis A. Evans and Helena Schmidt and George Davey Smith and P. Eline Slagboom and Gudny Eiriksdottir and Andrew P. Morris and Bruce M. Psaty and Russell P. Tracy and Ilja M. Nolte and Eric Boerwinkle and Sophie Visvikis-Siest and Alex P. Reiner and Myron Gross and Joshua C. Bis and Lude Franke and Oscar H. Franco and Emelia J. Benjamin and Daniel I. Chasman and Josée Dupuis and Harold Snieder and Abbas Dehghan and Behrooz Z. Alizadeh},
keywords = {C-reactive protein, genome-wide association study, inflammation, Mendelian randomization, inflammatory disorders, DEPICT, coronary artery disease, schizophrenia, system biology},
abstract = {C-reactive protein (CRP) is a sensitive biomarker of chronic low-grade inflammation and is associated with multiple complex diseases. The genetic determinants of chronic inflammation remain largely unknown, and the causal role of CRP in several clinical outcomes is debated. We performed two genome-wide association studies (GWASs), on HapMap and 1000 Genomes imputed data, of circulating amounts of CRP by using data from 88 studies comprising 204,402 European individuals. Additionally, we performed in silico functional analyses and Mendelian randomization analyses with several clinical outcomes. The GWAS meta-analyses of CRP revealed 58 distinct genetic loci (p < 5 × 10−8). After adjustment for body mass index in the regression analysis, the associations at all except three loci remained. The lead variants at the distinct loci explained up to 7.0% of the variance in circulating amounts of CRP. We identified 66 gene sets that were organized in two substantially correlated clusters, one mainly composed of immune pathways and the other characterized by metabolic pathways in the liver. Mendelian randomization analyses revealed a causal protective effect of CRP on schizophrenia and a risk-increasing effect on bipolar disorder. Our findings provide further insights into the biology of inflammation and could lead to interventions for treating inflammation and its clinical consequences.}
}
@article{IFAEI2017138,
title = {A systematic approach of bottom-up assessment methodology for an optimal design of hybrid solar/wind energy resources – Case study at middle east region},
journal = {Energy Conversion and Management},
volume = {145},
pages = {138-157},
year = {2017},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2017.04.097},
url = {https://www.sciencedirect.com/science/article/pii/S019689041730417X},
author = {Pouya Ifaei and Abdolreza Karbassi and Gabriel Jacome and ChangKyoo Yoo},
keywords = {Bottom-up assessment, Clustering, GIS map, Hybrid solar/wind, Optimal design, Renewable energy assessment},
abstract = {In the current study, an algorithm-based data processing, sizing, optimization, sensitivity analysis and clustering approach (DaSOSaCa) is proposed as an efficient simultaneous solar/wind assessment methodology. Accordingly, data processing is performed to obtain reliable high quality meteorological data among various datasets, which are used for hybrid photovoltaic/wind turbine/storage/converter system optimal design for consequent sites in a large region. The optimal hybrid systems are consequently simulated to meet hourly power demand in various sites. The solar/wind fraction and net present cost of the systems are then used as the technical and economic clustering variables, respectively. The clustering results are finally used as input to obtain novel hybrid solar/wind GIS maps. Iran is selected as the case study to validate the proposed methodology and detail its applicability. Ten minute annual global horizontal radiation, wind speed, and temperature data are analyzed, and the optimal, robust hybrid systems are simulated for various sites in order to classify the country. The generated GIS maps show that Iran can be efficiently clustered into four technical and five economic clusters under optimal conditions. The clustering results prove that Iran is mainly a solar country with approximately 74% solar power fraction under optimum conditions. A macroeconomic evaluation using DaSOSaCa also reveals that the nominal discount rate is recommended to be greater than 20% considering the current economic situation for the renewable energy sector in Iran. An environmental analysis results show that an average 106.68tonCO2-eq/year is produced for such hybrid systems application in Iran during a cradle to grave life cycle. Thus, Iran energy sector can be eminently promoted to an environmentally efficient stage with regard to the proposed classification plan and economic considerations.}
}
@article{NGOYE201798,
title = {Predicting the helpfulness of online reviews using a scripts-enriched text regression model},
journal = {Expert Systems with Applications},
volume = {71},
pages = {98-110},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416306649},
author = {Thomas L. Ngo-Ye and Atish P. Sinha and Arun Sen},
keywords = {Business intelligence, Online customer reviews, Review helpfulness, Script theory, Human annotation, Text regression},
abstract = {In this paper, we examine the utility of script analysis for predicting the helpfulness of online customer reviews. We employ the lens of cognitive scripts and posit that people share a cognitive script for what constitutes a helpful review in a given domain. Conceptually, a script includes the salient elements that readers look for before determining whether a review is helpful. To operationalize the construct of cognitive script, we seek the help of human annotators and ask them to highlight phrases that they believe are important for determining review helpfulness. The words in the annotated phrases are collected and become part of the script lexicon for a given domain. The lexicon entries represent the shared conception of essential elements, which are key to the evaluation of review helpfulness. We employ the words in the script lexicon as features in a text regression model to predict review helpfulness. Furthermore, we develop and empirically validate a new approach for combining script analysis and dimension reduction. The purpose of the study is to propose a new method to predict review helpfulness and to evaluate the effectiveness and efficiency of the scripts-enriched model. To demonstrate the efficacy of the scripts-enriched model, we compare it with benchmark models – a Baseline model and a bag-of-words (BOW) model. The results show that the scripts-enriched text regression model not only produces the highest accuracy, but also the lowest training, testing, and feature selection times.}
}
@article{LIU201711,
title = {Enabling effective workflow model reuse: A data-centric approach},
journal = {Decision Support Systems},
volume = {93},
pages = {11-25},
year = {2017},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616301518},
author = {Zhiyong Liu and Shaokun Fan and Harry Jiannan Wang and J. Leon Zhao},
keywords = {Workflow model reuse, Workflow model management, Data flow perspective, Data dependency},
abstract = {With increasingly widespread adoption of workflow technology as a standard solution to business process management, a large number of workflow models have been put in use in companies in the era of electronic commerce. These workflow models form a valuable resource for workflow domain knowledge, which should be reused to support workflow model design. However, current workflow modeling approaches do not facilitate workflow model reuse as a fundamental requirement, leading to a research gap in effective workflow model reuse. In this paper, we propose a novel approach called Data-centric Workflow Model Reuse framework (DWMR) to provide a solution to workflow model reuse. DWMR compliments existing control-flow-focused workflow modeling approaches by explicitly storing workflow data information, such as data dependency, data task relationships, and data similarity scores. DWMR also provides data-driven workflow model search and composition algorithms to satisfy user query requirements by automatically combining multiple workflow models. We demonstrate the feasibility of the DWMR approach by applying it to data from a well-known industry workflow model repository.}
}
@article{SHIN2017208,
title = {Public value mapping of network neutrality: Public values and net neutrality in Korea},
journal = {Telecommunications Policy},
volume = {41},
number = {3},
pages = {208-224},
year = {2017},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2016.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0308596116302804},
author = {Dong-Hee Shin and Min-Kyu Lee},
keywords = {Network neutrality, Public value mapping, South Korea, User-centered policy analysis},
abstract = {Network neutrality (NN) is of broad and current interest despite its complex and multifaceted nature. This study examines NN in the Korean context in terms of policy, industry, values, and society. It addresses the policy effectiveness of ongoing NN discussions by analyzing user attitudes and perceptions. The proposed model empirically tests policy effectiveness through user perceptions by incorporating factors representative of NN. Possible NN factors are derived from the previous literature and perceptions of NN concepts. The results find that regulation and competition are two primary factors constituting network neutrality, and these factors differently influence the formation of attitudes toward policy effectiveness. Political and social implications are discussed based on the proposed model. This study provides comprehensive analysis and heuristic data on user drivers, industry dynamics, and policy implications in the NN ecosystem.}
}
@article{SMITH2018636,
title = {Phenotypic Image Analysis Software Tools for Exploring and Understanding Big Image Data from Cell-Based Assays},
journal = {Cell Systems},
volume = {6},
number = {6},
pages = {636-653},
year = {2018},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405471218302412},
author = {Kevin Smith and Filippo Piccinini and Tamas Balassa and Krisztian Koos and Tivadar Danka and Hossein Azizpour and Peter Horvath},
keywords = {high-content screening, single-cell analysis, phenomics, oncology, drug screening, freely available tools, microscopy, machine learning, cell classification, phenotypic image analysis},
abstract = {Phenotypic image analysis is the task of recognizing variations in cell properties using microscopic image data. These variations, produced through a complex web of interactions between genes and the environment, may hold the key to uncover important biological phenomena or to understand the response to a drug candidate. Today, phenotypic analysis is rarely performed completely by hand. The abundance of high-dimensional image data produced by modern high-throughput microscopes necessitates computational solutions. Over the past decade, a number of software tools have been developed to address this need. They use statistical learning methods to infer relationships between a cell's phenotype and data from the image. In this review, we examine the strengths and weaknesses of non-commercial phenotypic image analysis software, cover recent developments in the field, identify challenges, and give a perspective on future possibilities.}
}
@incollection{2018345,
title = {Chapter 9 - Human Factors},
editor = {Felipe Jiménez},
booktitle = {Intelligent Vehicles},
publisher = {Butterworth-Heinemann},
pages = {345-394},
year = {2018},
isbn = {978-0-12-812800-8},
doi = {https://doi.org/10.1016/B978-0-12-812800-8.00009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128008000096},
keywords = {Diver monitoring, drowsiness, tiredness, HMI, interface, driver model},
abstract = {Apart from technological issues, human factors should also be taken into account in the intelligent vehicles even when high levels of automation are reached. In this sense, this chapter deals with two specific areas related to the human driver. The first one considers the driver monitoring in order to know whether his conditions are good enough to perform the driving tasks. In the second section, the human–machine interface is explained because this interaction is essential for the effectiveness of new systems and to not disturb the main task of the driver.}
}
@article{ULMEANU2017127,
title = {Hidden Markov Models revealing the household thermal profiling from smart meter data},
journal = {Energy and Buildings},
volume = {154},
pages = {127-140},
year = {2017},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2017.08.036},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816318229},
author = {Anatoli Paul Ulmeanu and Vlad Stefan Barbu and Vladimir Tanasiev and Adrian Badea},
keywords = {Hidden Markov chain, Emission probability matrix, Sequence observation, Building thermal load profile},
abstract = {This work describes a methodology based on Hidden Markov Models (HMMs) that are applied for revealing household thermal load profiles which are not available to direct observation. This research is motivated by the necessity of reducing the energy consumption for cooling and heating in residential buildings. Our methodology uses data that is becoming readily available at households – hourly energy consumption records collected from smart electricity meters, as well as hourly outdoor air temperature records. The heat transfer regime, namely the states corresponding to lower or higher building hourly thermal loads related to the outdoor air temperatures, will be considered as the underlying mechanism affecting the generation of observations. We aggregate the observed data to obtain a certain number of clusters. The problem of HMM estimation is addressed and the subsequent HMMs are compared on the basis of information criteria, like Akaike and Bayesian Information Criteria. Our goal is to reveal the dynamic of building thermal load (heating/cooling) under the uncertainties induced by the residents’ behavior. Consequently, we present examples of thermal load profiles generated using our best HMM on a testing facility located in the Polytechnic University of Bucharest campus, namely the UPB's passive building house.}
}
@article{JAMIL201834,
title = {Secure provenance using an authenticated data structure approach},
journal = {Computers & Security},
volume = {73},
pages = {34-56},
year = {2018},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167404817302122},
author = {Fuzel Jamil and Abid Khan and Adeel Anjum and Mansoor Ahmed and Farhana Jabeen and Nadeem Javaid},
keywords = {Provenance, Security, Confidentiality, Integrity, Availability},
abstract = {Data provenance is information used in reasoning about the present state of a data object, providing details such as the inputs used, transformations it underwent, entities responsible, and any other information that had an impact on its evolution. With a plethora of uses consisting of but not limited to provision of trust, gauging of quality, detecting intrusion and system changes, solving attribution problems, regulations compliance and in legal proceedings etc., provenance information needs to be secured. On the other hand use of tampered provenance information could lead to erroneous judgments and serious implications in many situations. The difference in sensitivity levels of provenance and the underlying data coupled with its DAG (Directed Acyclic Graph) structure leads to the need for a tailored security model. To date, proposed secure provenance schemes such as the Onion scheme, PKLC scheme, Mutual agreement scheme, rely on transitive trust; consecutive participating entities do not collude to attack the provenance chain. Furthermore, these schemes suffer from attacks such as ownership and lone attacks on provenance records. We propose a secure provenance scheme that uses the auditor as a witness to the chain build process whereby a verification tree is incrementally built by the auditor which serves as his view of the chain. Our scheme removes the transitive trust dependency hence collusion attacks by consecutive participating entities are successfully detected. Additionally, our scheme captures the DAG structure of provenance information and achieves secure provenance requirements; integrity, availability and confidentiality. Security analysis and empirical results show that the scheme provides better security guarantees than the previously proposed schemes with reasonable overheads involved that can be outweighed by the protection capabilities provided and removal of transitive trust which may not be feasible.}
}
@article{HAO201875,
title = {Recognizing multi-resident activities in non-intrusive sensor-based smart homes by formal concept analysis},
journal = {Neurocomputing},
volume = {318},
pages = {75-89},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218309834},
author = {Jianguo Hao and Abdenour Bouzouane and Sébastien Gaboury},
keywords = {Multi-resident activity recognition, Formal concept analysis, Sequential pattern mining, Smart homes, Ambient intelligence},
abstract = {Activity recognition is one of the most important prerequisites for smart home applications. It is a challenging topic due to the high requirements for reliable data acquisition and efficient data analysis. Besides, the heterogeneous layouts of smart homes, the number of residents and varied human behavioral patterns also aggravate the complexity of recognition. Therefore, most human activity recognition systems are based on an unrealistic assumption that there is only one resident performing activities. In this paper, we investigate the issue of multi-resident activity recognition and propose a knowledge-driven solution on the basis of formal concept analysis (FCA) to identify human activities from non-intrusive sensor data. We extract the ontological correlations among sequential behavioral patterns. At the same time, these correlations are well organized in a graphical knowledge base, without intervention from domain experts. We propose an incremental lattice search strategy in order to retrieve the best inference given a few sensor events. Compared with other conventional probabilistic methods, our solution outperforms on the CASAS multi-resident benchmark dataset. Furthermore, we open up a promising solution of sequential pattern mining to discover the ontological features of temporal and sequential sensor data.}
}
@article{DEVITO20181191,
title = {Calibrating chemical multisensory devices for real world applications: An in-depth comparison of quantitative machine learning approaches},
journal = {Sensors and Actuators B: Chemical},
volume = {255},
pages = {1191-1210},
year = {2018},
issn = {0925-4005},
doi = {https://doi.org/10.1016/j.snb.2017.07.155},
url = {https://www.sciencedirect.com/science/article/pii/S0925400517313692},
author = {S. {De Vito} and E. Esposito and M. Salvato and O. Popoola and F. Formisano and R. Jones and G. {Di Francia}},
keywords = {Distributed chemical sensing, Multisensors calibration algorithms, Dynamic machine learning, Air quality monitoring, Indicative measurements, Internet of Things},
abstract = {Chemical multisensor devices need calibration algorithms to estimate gas concentrations. Their possible adoption as indicative air quality measurements devices poses new challenges due to the need to operate in continuous monitoring modes in uncontrolled environments. Several issues, including slow dynamics, continue to affect their real world performances. At the same time, the need for estimating pollutant concentrations on board the devices, especially for wearables and IoT deployments, is becoming highly desirable. In this framework, several calibration approaches have been proposed and tested on a variety of proprietary devices and datasets; still, no thorough comparison is available to researchers. This work attempts a benchmarking of the most promising calibration algorithms according to recent literature with a focus on machine learning approaches. We test the techniques against absolute and dynamic performances, generalization capabilities and computational/storage needs using three different datasets sharing continuous monitoring operation methodology. Our results can guide researchers and engineers in the choice of optimal strategy. They show that non-linear multivariate techniques yield reproducible results, outperforming linear approaches. Specifically, the Support Vector Regression method consistently shows good performances in all the considered scenarios. We highlight the enhanced suitability of shallow neural networks in a trade-off between performance and computational/storage needs. We confirm, on a much wider basis, the advantages of dynamic approaches with respect to static ones that only rely on instantaneous sensor array response. The latter have been shown to be best choice whenever prompt and precise response is needed.}
}
@article{VERKADE201736,
title = {Is the Resource Man coming home? Engaging with an energy monitoring platform to foster flexible energy consumption in the Netherlands},
journal = {Energy Research & Social Science},
volume = {27},
pages = {36-44},
year = {2017},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2214629617300567},
author = {Nick Verkade and Johanna Höffken},
keywords = {Smart grids, Flexibility, Resource Man, Energy practices},
abstract = {In this article we empirically study the notion of ‘The Resource Man’ put forward by Strengers (2014): a motivated and knowledgeable micro-resource manager, who uses domestic smart grid innovations to manage energy demand in a sustainable, affordable and grid-friendly way. To explore this notion, we analyse a case study where energy cooperative members engaged with an ICT-based monitoring platform focussing on three domestic energy-managing activities – energy monitoring, planning and sharing. We find that although this case provided the best prerequisites for the Resource Man to emerge, none of these activities was sustained during the project. This outcome underlines that the Resource Man perspective held by many actors in the energy industry has a narrow understanding of energy consumption and how it can be changed or made more flexible. We suggest that it is easier to understand householders’ engagement with energy through the concept of energy practice or “e-practices”. E-practices go beyond managing energy with smart devices, and can include being actively involved in an energy collective, generating, trading, storing or discussing energy. We argue that in general, domestic smart grid technology can play a potential but limited role in effecting changes to complex and interlinked daily practices.}
}
@article{CHMIELEWSKI201897,
title = {Citizen science and WebGIS for outdoor advertisement visual pollution assessment},
journal = {Computers, Environment and Urban Systems},
volume = {67},
pages = {97-109},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S019897151730234X},
author = {Szymon Chmielewski and Marta Samulowska and Michał Lupa and Danbi Lee and Bogdan Zagajewski}
}
@article{OLTEDAL2017422,
title = {The Global ECT-MRI Research Collaboration (GEMRIC): Establishing a multi-site investigation of the neural mechanisms underlying response to electroconvulsive therapy},
journal = {NeuroImage: Clinical},
volume = {14},
pages = {422-432},
year = {2017},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2017.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S2213158217300438},
author = {Leif Oltedal and Hauke Bartsch and Ole Johan Evjenth Sørhaug and Ute Kessler and Christopher Abbott and Annemieke Dols and Max L Stek and Lars Ersland and Louise Emsell and Philip {van Eijndhoven} and Miklos Argyelan and Indira Tendolkar and Pia Nordanskog and Paul Hamilton and Martin Balslev Jorgensen and Iris E Sommer and Sophie M Heringa and Bogdan Draganski and Ronny Redlich and Udo Dannlowski and Harald Kugel and Filip Bouckaert and Pascal Sienaert and Amit Anand and Randall Espinoza and Katherine L Narr and Dominic Holland and Anders M Dale and Ketil J Oedegaard},
keywords = {Electroconvulsive therapy, MRI, Longitudinal, Mega analysis, Multi-site},
abstract = {Major depression, currently the world's primary cause of disability, leads to profound personal suffering and increased risk of suicide. Unfortunately, the success of antidepressant treatment varies amongst individuals and can take weeks to months in those who respond. Electroconvulsive therapy (ECT), generally prescribed for the most severely depressed and when standard treatments fail, produces a more rapid response and remains the most effective intervention for severe depression. Exploring the neurobiological effects of ECT is thus an ideal approach to better understand the mechanisms of successful therapeutic response. Though several recent neuroimaging studies show structural and functional changes associated with ECT, not all brain changes associate with clinical outcome. Larger studies that can address individual differences in clinical and treatment parameters may better target biological factors relating to or predictive of ECT-related therapeutic response. We have thus formed the Global ECT-MRI Research Collaboration (GEMRIC) that aims to combine longitudinal neuroimaging as well as clinical, behavioral and other physiological data across multiple independent sites. Here, we summarize the ECT sample characteristics from currently participating sites, and the common data-repository and standardized image analysis pipeline developed for this initiative. This includes data harmonization across sites and MRI platforms, and a method for obtaining unbiased estimates of structural change based on longitudinal measurements with serial MRI scans. The optimized analysis pipeline, together with the large and heterogeneous combined GEMRIC dataset, will provide new opportunities to elucidate the mechanisms of ECT response and the factors mediating and predictive of clinical outcomes, which may ultimately lead to more effective personalized treatment approaches.}
}
@article{COSTANTINI2017250,
title = {Analysis of surface deformations over the whole Italian territory by interferometric processing of ERS, Envisat and COSMO-SkyMed radar data},
journal = {Remote Sensing of Environment},
volume = {202},
pages = {250-275},
year = {2017},
note = {Big Remotely Sensed Data: tools, applications and experiences},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2017.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0034425717303322},
author = {Mario Costantini and Alessandro Ferretti and Federico Minati and Salvatore Falco and Francesco Trillo and Davide Colombo and Fabrizio Novali and Fabio Malvarosa and Claudio Mammone and Francesco Vecchioli and Alessio Rucci and Alfio Fumagalli and Jacopo Allievi and Maria Grazia Ciminelli and Salvatore Costabile},
keywords = {SAR interferometry, Persistent scatterer, Ground surface deformation},
abstract = {Interferometric processing of series of data acquired over time by synthetic aperture radar (SAR) satellites makes it possible to measure millimetric deformations (typically due to landslides, subsidence and earthquake or volcanic phenomena) and to monitor the stability of terrain and infrastructures. Despite the unique capability to observe very large areas, this technology has been typically applied to the analysis of relatively small sites or specific geophysical phenomena. In this work, we present the first application of this technology to a national scale project, which required the processing, through advanced persistent scatterer interferometry (PSI) techniques, of about 20,000 SAR images acquired from 1992 to 2014 over the whole Italian territory by the ERS, Envisat, and COSMO-SkyMed satellites. The obtained results provide a huge database of surface deformation measurements covering the last 20years and the whole Italian territory, which represents an extremely useful and pioneering service for geo-hazards mapping and prevention.}
}
@incollection{MAKRIS2018511,
title = {Chapter 45 - Current Approaches to Risk Assessment for Developmental Neurotoxicity},
editor = {William Slikker and Merle G. Paule and Cheng Wang},
booktitle = {Handbook of Developmental Neurotoxicology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {511-526},
year = {2018},
isbn = {978-0-12-809405-1},
doi = {https://doi.org/10.1016/B978-0-12-809405-1.00045-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094051000456},
author = {Susan L. Makris and Andrew D. Kraft},
keywords = {developmental neurotoxicity, risk assessment, hazard identification, dose response analysis, exposure assessment, risk characterization, data gaps},
abstract = {Risk assessment for developmental neurotoxicity of chemicals follows a standardized paradigm that includes problem formulation and scoping, hazard identification, dose–response assessment, exposure assessment, and risk characterization. In each step, relevant data are systematically evaluated and integrated into a determination of potential risk to humans of neurological perturbation following exposure during periods of nervous system development. Human and animal data are supplemented by information from nonmammalian species, in vitro assays, toxicokinetic data, and other mechanistic studies when available. Developmental neurotoxicity risk assessment incorporates considerations of intrinsic susceptibilities, critical developmental windows of exposure and periods of assessment, the major manifestations of developmental toxicity (death, structural abnormalities, delayed growth, and functional effects), and an assessment of dose response. Methodological challenges are related to the outcomes assessed, the populations of concern, and the complexity and trajectory of nervous system development. Developmental neurotoxicity risk assessment is an indispensable tool to inform decisions for public health protection.}
}
@article{LEBO20189,
title = {Bioinformatics in Clinical Genomic Sequencing},
journal = {Advances in Molecular Pathology},
volume = {1},
number = {1},
pages = {9-26},
year = {2018},
note = {Advances in Molecular Pathology},
issn = {2589-4080},
doi = {https://doi.org/10.1016/j.yamp.2018.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S2589408018300036},
author = {Matthew S. Lebo and Limin Hao and Chiao-Feng Lin and Arti Singh},
keywords = {Genome sequencing, Exome sequencing, Alignment, Variant calling, Annotation, Filtration, Validation, Bioinformatic infrastructure}
}
@article{CHEN2018519,
title = {How “small” reflects “large”?—Representative information measurement and extraction},
journal = {Information Sciences},
volume = {460-461},
pages = {519-540},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.08.096},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517309246},
author = {Guoqing Chen and Cong Wang and Mingyue Zhang and Qiang Wei and Baojun Ma},
keywords = {Representative, Coverage, Redundancy, Consistency, Compactness, Extraction},
abstract = {While web services avail a rapid growth of data volume for use, identifying helpful information is of great value, especially when users face with an unwilling glut of information. Thus, it is deemed relevant and meaningful to provide users with a representative subset (i.e., small set) that could well reflect the original information corpus (i.e., large set). In such a large–small context, this paper addresses the issues of representativeness in light of measurement and extraction by reviewing our previous efforts. Specifically, we first discuss various metrics from different perspectives of representativeness, then present a series of related representativeness extraction methods. Finally as a supplement and extension, a recent effort is introduced, which aims to take information quality into account in deriving a ranked subset. The proposed extraction method is justified by extensive real-world data experiments, showing its superiority to others in both effectiveness and efficiency.}
}
@article{YATES2017169,
title = {Genomic Evolution of Breast Cancer Metastasis and Relapse},
journal = {Cancer Cell},
volume = {32},
number = {2},
pages = {169-184.e7},
year = {2017},
issn = {1535-6108},
doi = {https://doi.org/10.1016/j.ccell.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1535610817302970},
author = {Lucy R. Yates and Stian Knappskog and David Wedge and James H.R. Farmery and Santiago Gonzalez and Inigo Martincorena and Ludmil B. Alexandrov and Peter {Van Loo} and Hans Kristian Haugland and Peer Kaare Lilleng and Gunes Gundem and Moritz Gerstung and Elli Pappaemmanuil and Patrycja Gazinska and Shriram G. Bhosle and David Jones and Keiran Raine and Laura Mudie and Calli Latimer and Elinor Sawyer and Christine Desmedt and Christos Sotiriou and Michael R. Stratton and Anieta M. Sieuwerts and Andy G. Lynch and John W. Martens and Andrea L. Richardson and Andrew Tutt and Per Eystein Lønning and Peter J. Campbell},
keywords = {breast cancer, metastasis, relapse, genomics, somatic mutation},
abstract = {Summary
Patterns of genomic evolution between primary and metastatic breast cancer have not been studied in large numbers, despite patients with metastatic breast cancer having dismal survival. We sequenced whole genomes or a panel of 365 genes on 299 samples from 170 patients with locally relapsed or metastatic breast cancer. Several lines of analysis indicate that clones seeding metastasis or relapse disseminate late from primary tumors, but continue to acquire mutations, mostly accessing the same mutational processes active in the primary tumor. Most distant metastases acquired driver mutations not seen in the primary tumor, drawing from a wider repertoire of cancer genes than early drivers. These include a number of clinically actionable alterations and mutations inactivating SWI-SNF and JAK2-STAT3 pathways.}
}
@article{ZHOU20181,
title = {Topological mapping and assessment of multiple settlement time series in deep excavation: A complex network perspective},
journal = {Advanced Engineering Informatics},
volume = {36},
pages = {1-19},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617303075},
author = {Cheng Zhou and Lieyun Ding and Ying Zhou and Hanbin Luo},
keywords = {Deep excavation, Settlement time series, Complex network, Similarity matrix, Topological analysis, Node influence},
abstract = {This study proposed a novel methodology that integrates complex network theory and multiple time series to enhance the systematic understanding of the daily settlement behavior in deep excavation. The original time series of ground surface, surrounding buildings, and structure settlement instrumentation data over an excavation time period were measured into a similarity matrix with correlation coefficients. A threshold was then determined and binarized into adjacent matrix to identify the optimal topology and structure of the complex network. The reconstructed settlement network has nodes corresponding to multiple settlement time series individually and edges regarded as nonlinear relationships between them. A deep excavation case study of the metro station project in the Wuhan Metro network, China, was applied to validate the feasibility and potential value of the proposed approach. Results of the topological analysis corroborate a small-world phenomenon with highly compacted interactions and provide the assessment of the significance among multiple settlement time series. This approach, which provides a new way to assess the safety monitoring data in underground construction, can be implemented as a tool for extracting macro- and micro-level decision information from multiple settlement time series in deep excavation from complex system perspectives.}
}
@incollection{MARKE201835,
title = {Chapter 4 - Decoding the Current Global Climate Finance Architecture},
editor = {Alastair Marke},
booktitle = {Transforming Climate Finance and Green Investment with Blockchains},
publisher = {Academic Press},
pages = {35-59},
year = {2018},
isbn = {978-0-12-814447-3},
doi = {https://doi.org/10.1016/B978-0-12-814447-3.00004-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128144473000045},
author = {Alastair Marke and Bianca Sylvester},
keywords = {Adaptation, Blockchain, climate finance, fintech, Green Climate Fund, green investment, mitigation, NDC, Paris Agreement, trust and transparency},
abstract = {To keep the planet on the 2°C trajectory, developing countries alone will require US$500 billion annually by 2030 to adequately mitigate their carbon emissions, in addition to several hundred billion additional dollars for adaptation needs. The US$100 billion a year pledged by developed countries through the Green Climate Fund is such insufficient that most of the climate finance required will have to come from the private sector. As a scene-setting chapter, it will delineate the scale of climate investment required, the state of climate finance post-Paris Agreement, climate mitigation and adaptation investment trends, global climate finance landscape, specific hurdles against climate investment, and the problems with climate finance tracking and monitoring. The lack of trust and transparency in the global climate finance landscape provides an excellent ground on which to deploy Blockchain technology to turbo-boost global climate finance flows. This chapter will initiate the discussion on how Blockchain as a “trust machine” could address the many deeply-rooted institutional problems.}
}
@article{STAMATE2018146,
title = {The cloudUPDRS app: A medical device for the clinical assessment of Parkinson’s Disease},
journal = {Pervasive and Mobile Computing},
volume = {43},
pages = {146-166},
year = {2018},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1574119217303607},
author = {C. Stamate and G.D. Magoulas and S. Kueppers and E. Nomikou and I. Daskalopoulos and A. Jha and J.S. Pons and J. Rothwell and M.U. Luchini and T. Moussouri and M. Iannone and G. Roussos},
abstract = {Parkinson’s Disease is a neurological condition distinguished by characteristic motor symptoms including tremor and slowness of movement. To enable the frequent assessment of PD patients, this paper introduces the cloudUPDRS app, a Class I medical device that is an active transient non-invasive instrument, certified by the Medicines and Healthcare products Regulatory Agency in the UK. The app follows closely Part III of the Unified Parkinson’s Disease Rating Scale which is the most commonly used protocol in the clinical study of PD; can be used by patients and their carers at home or in the community unsupervised; and, requires the user to perform a sequence of iterated movements which are recorded by the phone sensors. The cloudUPDRS system addresses two key challenges towards meeting essential consistency and efficiency requirements, namely: (i) How to ensure high-quality data collection especially considering the unsupervised nature of the test, in particular, how to achieve firm user adherence to the prescribed movements; and (ii) How to reduce test duration from approximately 25 min typically required by an experienced patient, to below 4 min, a threshold identified as critical to obtain significant improvements in clinical compliance. To address the former, we combine a bespoke design of the user experience tailored so as to constrain context, with a deep learning approach based on Recurrent Convolutional Neural Networks, to identify failures to follow the movement protocol. We address the latter by developing a machine learning approach to personalize assessments by selecting those elements of the test that most closely match individual symptom profiles and thus offer the highest inferential power, hence closely estimating the patent’s overall score.}
}
@article{ZHANG201842,
title = {Updating authoritative spatial data from timely sources: A multiple representation approach},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {72},
pages = {42-56},
year = {2018},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2018.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0303243418302101},
author = {Xiang Zhang and Weijun Yin and Min Yang and Tinghua Ai and Jantien Stoter},
keywords = {Volunteered geographic information, OpenStreetMap, Incremental update, Multiple representation databases, Data matching, Cartographic generalization},
abstract = {Integrating updates from timely sources such as volunteered geographic information (VGI) into the spatial data maintained at official agencies is becoming a more demanding requirement but presents many challenges. This paper proposes an approach to addressing the technical challenge of propagating updates from timely sources (e.g. OpenStreetMap) to spatial data maintained at separate map scales. The main idea is to establish a multiple representation database (MRDB) for datasets at different scales and time to facilitate incremental update, where linkages between corresponding objects at different datasets are made explicit. First, two ways in which the timely sources can be integrated into official data for incremental update are discussed. To derive the linkages between different datasets, a data matching procedure based on computer vision is presented and fine-tuned to match data in different scale ranges. Furthermore, the generalization history used to produce smaller scale data from the larger ones in official data is inferred based on the linkages, and is then used to guide the update propagation. Finally, a framework for incremental generalization in MRDBs is proposed, where crucial issues like strategies for update propagation, cartographic generalization, and the so-called ‘chain reaction’ are addressed. The framework is implemented as a fully automated process where operators like simplification, enlargement, compression, displacement and typification are incorporated into the incremental update process. By testing the framework against real world data sets (i.e. OpenStreetMap and official data at 1:10k, 1:50k and 1:100k), we show that the updates are integrated consistently into existing data in terms of spatial relations and cartographic quality. Our work suggests that making use of timely sources by official mapping agencies and companies in a continuous or event-driven data update is technically feasible, with further improvement and extensions discussed.}
}
@article{KODAMANA201868,
title = {Approaches to robust process identification: A review and tutorial of probabilistic methods},
journal = {Journal of Process Control},
volume = {66},
pages = {68-83},
year = {2018},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418300416},
author = {Hariprasad Kodamana and Biao Huang and Rishik Ranjan and Yujia Zhao and Ruomu Tan and Nima Sammaknejad},
keywords = {Robust identification, Outliers, -Distribution, Laplace distribution, Mixture of Gaussian distribution, Flat-topped distribution, Bayesian methods, EM algorithm, Variational Bayesian approach, Expectation Propagation, Monte Carlo methods, ARX models},
abstract = {Industrial data sets are often contaminated with outliers due to sensor malfunctions, signal interference, and other disturbances as well as interplay of various other factors. The effect of data abnormalities due to the outliers has to be systematically accounted while developing models that are resistant towards unforeseen effects of the outliers. The spectrum of methods that account for irregularities in process data while modeling are collectively known as robust identification methods. Even though, there are various non-probabilistic methods to tackle robust identification, few of them have considered the effect of outliers explicitly. In contrast to that, probabilistic identification methods ensure that these effects are given due attention. Despite these advantages, the probabilistic robust identification strategies have hardly been explored by practitioners. This review paper provides a general introduction to the probabilistic methods for robust identification, illustrates the main steps involved in the development of models, and reviews the related literature. Further, the paper contains two tutorial examples, including an industrial case study, to highlight various steps involved in the robust identification process.}
}
@article{ROTH2017307,
title = {Futures of a distributed memory. A global brain wave measurement (1800–2000)},
journal = {Technological Forecasting and Social Change},
volume = {118},
pages = {307-323},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S004016251730269X},
author = {Steffen Roth and Carlton Clark and Nikolay Trofimov and Artur Mkrtichyan and Markus Heidingsfelder and Laura Appignanesi and Miguel Pérez-Valls and Jan Berkel and Jari Kaivo-oja},
keywords = {Global brain, Google Ngram Viewer, Culturomics, Secularization, Capitalism, Functional differentiation},
abstract = {If the global brain is a suitable model of the future information society, then one future of research in this global brain will be in its past, which is its distributed memory. In this paper, we draw on Francis Heylighen, Marta Lenartowicz, and Niklas Luhmann to show that future research in this global brain will have to reclaim classical theories of social differentiation in general and theories of functional differentiation in particular to develop higher resolution images of this brain's function and sub-functions. This claim is corroborated by a brain wave measurement of a considerable section of the global brain. We used the Google Ngram Viewer, an online graphing tool which charts annual counts of words or sentences as found in the largest available corpus of digitalized books, to analyse word frequency time-series plots of key concepts of social differentiation in the English as well as in the Spanish, French, German, Russian, and Italian sub-corpora between 1800 and 2000. The results of this socioencephalography suggest that the global brain's memory recalls distinct and not yet fully conscious biases to particular sub-functions, which are furthermore not in line with popular trend statements and self-descriptions of modern societies. We speculate that an increasingly intelligent global brain will start to critically reflect upon these biases and learn how to anticipate or even design its own desired futures.}
}
@article{XU2018195,
title = {MULAPI: Improving API method recommendation with API usage location},
journal = {Journal of Systems and Software},
volume = {142},
pages = {195-205},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.04.060},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300840},
author = {Congying Xu and Xiaobing Sun and Bin Li and Xintong Lu and Hongjing Guo},
keywords = {API method recommendation, API usage location, Feature request, Feature location},
abstract = {During the evolution of a software system, a large number of feature requests are continuously proposed by users. To implement these feature requests, developers often utilize existing third-party libraries and make use of Application Programming Interfaces (APIs) to accelerate the feature implementation process. However, it is not always obvious which API methods are suitable and where these API methods can be used in the target program. In this paper, we propose an approach, MULAPI (Method Usage and Location for API), to recommend API methods and figure out the API usage location where these API methods would be used. MULAPI employs feature location to identify feature related files as API usage location. Further, these feature related files are taken into account to recommend API methods by exploring the source code repository and API libraries as well. We evaluate MULAPI on more than 1000 feature requests of eight Java projects (Axis/Java, CXF, Hadoop Common, Hbase, Struts2, Hadoop HDFS, Hive and Hadoop Map/Reduce), and recommend API methods from ten third-party libraries. The empirical results show that MULAPI can accurately recommend API methods and usage location, and moreover, MULAPI improves the effectiveness of API method recommendation, compared with the state-of-the-art approach.}
}
@article{JANOWSKI2018S1,
title = {Platform governance for sustainable development: Reshaping citizen-administration relationships in the digital age},
journal = {Government Information Quarterly},
volume = {35},
number = {4, Supplement },
pages = {S1-S16},
year = {2018},
note = {Platform Governance for Sustainable Development},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18303836},
author = {Tomasz Janowski and Elsa Estevez and Rehema Baguma},
keywords = {Public governance, Platform governance, Digital government, Sustainable development, Citizen-administration relationships},
abstract = {Changing governance paradigms has been shaping and reshaping the landscape of citizen-administration relationships, from impartial application of rules and regulations by administration to exercise its authority over citizens (bureaucratic paradigm), through provision of public services by administration to fulfil the needs of citizens (consumerist paradigm), to responsibility-sharing between administration and citizens for policy and service processes (participatory paradigm). The recent trend is the administration empowering citizens to create public value by themselves, through socio-technical systems that bring data, services, technologies and people together to respond to changing societal needs. Such systems are called “platforms” and the trend is called “platform paradigm”. The aim of this article is to offer a conceptual framework for citizen-administration relationships under the platform paradigm. While existing models of citizen-administration relationships mainly focus on specific types of relationships, e.g. citizen trust versus administrative transparency, or citizen satisfaction versus administrative performance, the proposed framework identifies a comprehensive set of relationships that explain how decisions by citizens or administration and the policy environment mutually agreed by them contribute to shaping such relationships and building individual and collective capacity for pursuing sustainable development. The framework comprises 15 types of relationships organized along the four governance paradigms. It is illustrated through the analysis of 11 case studies published in the current issue. Based on this analysis, the article also formulates some insights that are relevant to researchers and policymakers who intend to utilize platform governance for sustainable development.}
}
@article{PATCHELL2018941,
title = {Can the implications of the GHG Protocol's scope 3 standard be realized?},
journal = {Journal of Cleaner Production},
volume = {185},
pages = {941-958},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618306528},
author = {Jerry Patchell},
keywords = {GHG protocol scope 3, CDP, Transaction costs, Power, measurement and management},
abstract = {The GHG Protocol has become the standard for corporate reporting of greenhouse gases, adopted by governments for regulations, NGOs for accountability and corporations for compliance. The GHG Protocol's scope 1 and 2 have largely succeeded in gaining compliance from large firms to report their internal GHG emissions and those from electricity purchases. Achieving Scope 3's intent of a full audit of value chain emissions GHG, however, is a much more complicated affair and according to the CDP, scope 3 is much less successful. This lack of success challenges the premise and purpose of the standard, especially, the expectation that the power of MNCs can be used to leverage reporting and reductions through the value chain. This paper constructs a heuristic framework to explain why success has been limited. The paper discusses six interdependent factors that inhibit scope 3's ambition of promoting the measurement and management of GHG emissions throughout the value chain. These factors are transaction costs, power, responsibility allocation, uncertainty, location contingency and production costs. The impact of these factors on likelihood of compliance to the scope 3 are revealed by an examination of what the sustainable supply chain management, the supply chain management and other literature tell us about value chain interactions on environmental performance. The weight of these factors cast doubt on scope 3's ambition to compel firms to report a full audit of their scope 3 emissions. Moreover, the pursuit of that ambition diverts corporate efforts from more efficient and effective environmental efforts. The paper concludes with a discussion of options for harnessing the power of the lifecycle approach to reforming the value chain.}
}
@article{AVCI2018390,
title = {Managing electricity price modeling risk via ensemble forecasting: The case of Turkey},
journal = {Energy Policy},
volume = {123},
pages = {390-403},
year = {2018},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2018.08.053},
url = {https://www.sciencedirect.com/science/article/pii/S0301421518305834},
author = {Ezgi Avci and Wolfgang Ketter and Eric {van Heck}},
keywords = {Electricity markets, Day ahead auctions, Risk of price modeling, Ensemble forecasting, Multi-seasonality, Exogenous variable selection, Semi-transparent market},
abstract = {There are two ways of managing market price risk in electricity day ahead markets, forecasting and hedging. In emerging markets, since hedging possibilities are limited, forecasting becomes the foremost important tool to manage spot price risk. Despite the existence of great diversity of spot price forecasting methods, due to the unique characteristics of electricity as a commodity, there are still three key forecasting challenges that a market participant has to take into account: risk of selection of an inadequate forecasting method and transparency level of the market (availability level of public data) and country-specific multi-seasonality factors. We address these challenges by using a detailed market-level data from the Turkish electricity day-ahead auctions, which is an interesting research setting in that it presents a number of challenges for forecasting. We reveal the key distinguishing features of this market quantitatively which then allow us to propose individual and ensemble forecasting models that are particularly well suited to it. This forecasting study is pioneering for Turkey as it is the very first to focus specifically on electricity spot prices since the country's day-ahead market was established in 2012. We also suggested applicable policy and managerial implications for both regulatory bodies, market makers and participants.}
}
@article{KARIMI201739,
title = {Online review helpfulness: Impact of reviewer profile image},
journal = {Decision Support Systems},
volume = {96},
pages = {39-48},
year = {2017},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923617300179},
author = {Sahar Karimi and Fang Wang},
keywords = {Online reviews, Review helpfulness, Reviewer image, Reviewer profile image, Identity information, Decorative images},
abstract = {Despite the growing number of studies on online reviews, the impact of visual cues on consumer's evaluation of review helpfulness has remained underexplored. It is not yet known whether and how images influence the way online reviews are perceived. This paper introduces and empirically examines the potential effects of reviewer profile image, a photo/image displayed next to the reviewer name, on review helpfulness by drawing on the decorative and information functions of images. With a sample of 2178 reviews from mobile gaming applications, we report that reviewer profile image can significantly enhance consumer's evaluation of review helpfulness; whereas there is no differential effect among image types (i.e. self, family, or random images). Interestingly, the effect of reviewer profile image on review helpfulness is moderated by review length, but not review valence and equivocality. Results suggest that reviewer profile image enhances the perception of review helpfulness by serving mainly as a visual decoration that creates affective responses rather than identity information.}
}
@article{KAMISALIC2018207,
title = {Formalization and acquisition of temporal knowledge for decision support in medical processes},
journal = {Computer Methods and Programs in Biomedicine},
volume = {158},
pages = {207-228},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717309756},
author = {Aida Kamišalić and David Riaño and Tatjana Welzer},
keywords = {Temporal knowledge representation, Time modelling, Time constraints generation, Decision support, Medical procedural knowledge, Clinical practice guidelines, Cardiovascular diseases},
abstract = {Background: In medical practice, long term interventions are common and they require timely planning of the involved processes. Unfortunately, evidence-based statements about time are hard to find in Clinical Practice Guidelines (CPGs) and in other sources of medical knowledge. At the same time, health care centers use medical records and information systems to register data about clinical processes and patients, including time information about the encounters, prescriptions, and other clinical actions. Consequently, medical records and health care information systems are promising sources of data from which we can detect temporal medical knowledge. Objective: The objectives were to (1) Analyze and classify the sorts of time constraints in medical processes, (2) Propose a formalism to represent these sorts of clinical time constraints, (3) Use these formalisms to enable the automatic generation of temporal models from clinical data, and (4) Study the adherence of these intervention models to CPG recommendations. Methods: In order to achieve these objectives, we carried out four studies: The identification of the sort of times involved in the long-term diagnostic and therapeutic medical procedures of fifty patients, the supervision of the indications about time contained in six CPGs on chronic diseases, the study of the time structures of two standard data models, as well as ten languages to computerize CPGs. Based on the provided studies, we synthesized two representation formalisms: Micro- and macro-temporality. We developed three algorithms for automatic generation of generalized time constraints in the form of micro- and macro-temporalities from clinical databases, which were double tested. Results: A full classification of time constraints for medical procedures is proposed. Two formalisms called micro- and macro-temporality are introduced and validated to represent these time constraints. Time constraints were generated automatically from the data about 8781 Arterial Hypertension (AH) patients. The generated macro-temporalities restricted visits to be between 1–7 weeks, whereas CPGs recommend 2–4 weeks. Micro-temporal constraints on drug-dosage therapies distinguished between the initial dosage and the target dosage, with visits every 1–6 weeks, and 2–5 months, respectively. Our algorithms obtained semi-complete maps of dosage increments and the maximum dosages for 7 drug types. Data-based time limits for lifestyle change counsels and blood pressure (BP) check-ups were fixed to 6 and 3 months, for patients with low- and high-BP, respectively, when CPGs specify a general 3–6 month range. Conclusions: Experience-based temporal knowledge detected using our algorithms complements the evidence-based knowledge about clinical procedures contained in the CPGs. Our temporal model is simple and highly descriptive when dealing with general or specific time constraints’ representations, offering temporal knowledge representation of varying detail. Therefore, it is capable of capturing all the temporal knowledge we can find in medical procedures, when dealing with chronic diseases. With our model and algorithms, an adherence analysis emerges naturally to detect CPG-compliant interventions, but also deviations whose causes and possible rationales can call into question CPG recommendations (e.g., our analysis of AH patients showed that the time between visits recommended by CPGs were too long for a proper drug therapy decision, dosage titration, or general follow-up).}
}
@incollection{GROOT201719,
title = {Chapter 2 - Taxonomy of Financial Data},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {19-64},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000028},
author = {Martijn Groot},
keywords = {financial data, classification, data types, taxonomy},
abstract = {This chapter presents different ways to classify financial information. It introduces a data model to organize financial information and analyzes financial data in terms of master data versus transactional data and structured versus unstructured data. The chapter presents a categorization of different types of data sources from public, business relationship, commercial, and internal sources and the implications of these different sources for data integration. The second half of the chapter provides a taxonomy of the different data categories used in financial services—covering all master data types (instruments, clients, trade reference data, tax), transactional data types, research, analytics, and regulatory information.}
}
@article{DONG20181,
title = {Secure partial encryption with adversarial functional dependency constraints in the database-as-a-service model},
journal = {Data & Knowledge Engineering},
volume = {116},
pages = {1-20},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16300520},
author = {Boxiang Dong and Hui (Wendy) Wang},
keywords = {Database-as-a-service, Data outsourcing, Security, integrity, and protection, Database management, Management of integrity constraints},
abstract = {Cloud computing enables end-users to outsource their dataset and data management needs to a third-party service provider. One of the major security concerns of the outsourcing paradigm is how to protect sensitive information in the outsourced dataset. In some applications, only partial values are considered sensitive. In general, the sensitive information can be protected by encryption. However, data dependency constraints (together with the unencrypted data) in the outsourced data may serve as adversary knowledge and bring security vulnerabilities to the encrypted data. In this paper, we focus on functional dependency (FD), an important type of data dependency constraints, and study the security threats by the adversarial FDs. We design a practical scheme that can defend against the FD attack by encrypting a small amount of non-sensitive data (encryption overhead). We prove that finding the scheme that leads to the optimal encryption overhead is NP-complete, and design efficient heuristic algorithms, under the presence of one or multiple FDs. We design a secure query rewriting scheme that enables the service provider to answer various types of queries on the encrypted data with provable security guarantee. We extend our study to enforce security when there are conditional functional dependencies (CFDs) and data updates. We conduct an extensive set of experiments on two real-world datasets. The experiment results show that our heuristic approach brings small amounts of encryption overhead (at most 1% more than the optimal overhead), and enjoys a 10-time speedup compared with the optimal solution. Besides, our approach can reduce up to 90% of the encryption overhead of state-of-the-art solution.}
}
@incollection{MCGREGOR201821,
title = {2 - Architectures of Transportation Cyber-Physical Systems},
editor = {Lipika Deka and Mashrur Chowdhury},
booktitle = {Transportation Cyber-Physical Systems},
publisher = {Elsevier},
pages = {21-49},
year = {2018},
isbn = {978-0-12-814295-0},
doi = {https://doi.org/10.1016/B978-0-12-814295-0.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128142950000022},
author = {John D. McGregor and Roselane S. Silva and Eduardo S. Almeida},
keywords = {AADL, Cyber-physical systems, Internet of things, NIST, Quality attributes, System architecture, Transportation engineers},
abstract = {Architecture is the fundamental structure of a system. That structure is based on the relationships among the modules that provide the behaviour of the system. These structures appear, with variations, in many systems that address related problems. Because software is so malleable, much of this variation is implemented in the software portion of the product. These different architectures have different performance characteristics such as different levels of reliability and safety. In this chapter we will survey some of the popular architectures for cyber-physical systems, the quality attributes enhanced and degraded by each architecture and analysis techniques that are used to evaluate these qualities in the context of actual applications. Our intent is to provide the type of knowledge needed for the transportation engineer to participate in developing or acquiring software for smart transportation systems.}
}
@article{SHIN201712,
title = {Energy efficiency of milling machining: Component modeling and online optimization of cutting parameters},
journal = {Journal of Cleaner Production},
volume = {161},
pages = {12-29},
year = {2017},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617309381},
author = {Seung-Jun Shin and Jungyub Woo and Sudarsan Rachuri},
keywords = {Metal cutting, Predictive modeling, Optimization, Energy efficiency, STEP-NC, MTConnect},
abstract = {Energy consumption is a major sustainability focus in the metal cutting industry. As a result, process planning is increasingly concerned with reducing energy consumption in machine tools. The relevant literature has been categorized into two research areas. The first includes energy prediction models, which characterize the relationships between cutting parameters – the main outputs of process planning - and energy consumption. The second involves energy-consumption optimization, which uses the prediction models to find the cutting parameters that minimize energy use. However, previous energy prediction models are limited to predict energy for tool paths coded in a Numerical Control (NC) program. Previous energy optimization methods typically do not use online optimization, which enables fast optimization decision-making for supporting on-demand process planning and real-time machine control. This paper presents a component-based energy-modeling methodology to implement the online optimization needed for real-time control. Models that can predict energy up to the tool path-level at specific machining configurations are called component-models in this paper. These component-models are created using historical data that includes process plans, NC programs, and machine-monitoring data. The online optimization is implemented using a dynamic composition of component-models together with a divide-and-conquer technique. The feasibility and effectiveness of our methodology has been demonstrated in a milling-machine example.}
}
@incollection{WITTEN2017161,
title = {Chapter 5 - Credibility: Evaluating what’s been learned},
editor = {Ian H. Witten and Eibe Frank and Mark A. Hall and Christopher J. Pal},
booktitle = {Data Mining (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {161-203},
year = {2017},
isbn = {978-0-12-804291-5},
doi = {https://doi.org/10.1016/B978-0-12-804291-5.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042915000052},
author = {Ian H. Witten and Eibe Frank and Mark A. Hall and Christopher J. Pal},
keywords = {Training set, test set, cross-validation, bootstrap, paired -test, quadratic and informational loss, misclassification costs, performance charts, model selection, minimum description length principle},
abstract = {The success of machine learning in practical applications hinges on proper evaluation. This section discusses how the quality of predictions can be measured reliably. We consider the basic train-test setup for estimating predictive accuracy, before moving on to more sophisticated variants known as “cross-validation” and the “bootstrap” method. We also discuss the importance of proper parameter tuning when applying and evaluating machine learning, and explain how to use statistical significance tests when comparing the performance of two learning algorithms in a particular application domain. As well as basic classification accuracy, we consider other measures for evaluating the quality of probability estimates, learning and prediction with misclassification costs, and measures for evaluating numeric prediction schemes. The final section discusses model selection, which is the process of determining an appropriate model complexity, using the compression-based minimum description length principle, on the one hand, and evaluation on a validation set, on the other.}
}
@article{WERNER201757,
title = {Financial process mining - Accounting data structure dependent control flow inference},
journal = {International Journal of Accounting Information Systems},
volume = {25},
pages = {57-80},
year = {2017},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300264},
author = {Michael Werner},
keywords = {Process mining, Financial audits, Journal entries, Business process intelligence, Business process modeling, Control flow inference, Design science research, Enterprise resource planning systems},
abstract = {The increasing integration of computer technology for the processing of business transactions and the growing amount of financially relevant data in organizations create new challenges for external auditors. The availability of digital data opens up new opportunities for innovative audit procedures. Process mining can be used as a novel data analysis technique to support auditors in this context. Process mining algorithms produce process models by analyzing recorded event logs. Contemporary general purpose mining algorithms commonly use the temporal order of recorded events for determining the control flow in mined process models. The presented research shows how data dependencies related to the accounting structure of recorded events can be used as an alternative to the temporal order of events for discovering the control flow. The generated models provide accurate information on the control flow from an accounting perspective and show a lower complexity compared to those generated using timestamp dependencies. The presented research follows a design science research approach and uses three different real world data sets for evaluation purposes.}
}
@article{KANARACHOS2018867,
title = {Smartphones as an integrated platform for monitoring driver behaviour: The role of sensor fusion and connectivity},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {95},
pages = {867-882},
year = {2018},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2018.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18303954},
author = {Stratis Kanarachos and Stavros-Richard G. Christopoulos and Alexander Chroneos},
keywords = {Smartphones, Driver behaviour, Sensor fusion, Connectivity, Cybernetics, Crowd-sensing},
abstract = {Nowadays, more than half of the world’s web traffic comes from mobile phones, and by 2020 approximately 70 percent of the world’s population will be using smartphones. The unprecedented market penetration of smartphones combined with the connectivity and embedded sensing capability of smartphones is an enabler for the large-scale deployment of Intelligent Transportation Systems (ITS). On the downside, smartphones have inherent limitations such as relatively limited energy capacity, processing power, and accuracy. These shortcomings may potentially limit their role as an integrated platform for monitoring driver behaviour in the context of ITS. This study examines this hypothesis by reviewing recent scientific contributions. The Cybernetics theoretical framework was employed to allow a systematic comparison. First, only a few studies consider the smartphone as an integrated platform. Second, a lack of consistency between the approaches and metrics used in the literature is noted. Last but not least, areas such as fusion of heterogeneous information sources, Deep Learning and sparse crowd-sensing are identified as relatively unexplored, and future research in these directions is suggested.}
}
@incollection{PETERS2017155,
title = {Chapter 9 - Advanced Personal Genome Sequencing as the Ultimate Diagnostic Test},
editor = {George P. Patrinos},
booktitle = {Molecular Diagnostics (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {155-172},
year = {2017},
isbn = {978-0-12-802971-8},
doi = {https://doi.org/10.1016/B978-0-12-802971-8.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128029718000092},
author = {B.A. Peters and S. Drmanac and J.S. Liu and X. Xun and R. Drmanac},
keywords = {Co-barcoding, DNA nanoballs, Genetic disease, Genome interpretation, Long fragment read technology, Perfect genome, Virtual health coaching, Whole-genome sequencing},
abstract = {In this chapter we provide a brief history of whole-genome sequencing (WGS) and the field of molecular diagnostics. Next, we focus on some important areas for improvement in WGS and what is being done. We explore some areas where WGS is being used and why it makes sense to sequence the whole human genome as the ultimate molecular diagnostic. Finally, we close with what kind of infrastructure will be necessary for population-scale WGS, what tools will be needed to understand and use that information fully, and how having it can benefit each of us in many aspects of our lives.}
}
@article{HOUBORG2018211,
title = {A Cubesat enabled Spatio-Temporal Enhancement Method (CESTEM) utilizing Planet, Landsat and MODIS data},
journal = {Remote Sensing of Environment},
volume = {209},
pages = {211-226},
year = {2018},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2018.02.067},
url = {https://www.sciencedirect.com/science/article/pii/S0034425718300786},
author = {Rasmus Houborg and Matthew F. McCabe},
keywords = {CubeSat, Planet, Landsat, MODIS, Cubist, Machine-learning, VNIR, Spatio-temporal enhancement},
abstract = {Satellite sensing in the visible to near-infrared (VNIR) domain has been the backbone of land surface monitoring and characterization for more than four decades. However, a limitation of conventional single-sensor satellite missions is their limited capacity to observe land surface dynamics at the very high spatial and temporal resolutions demanded by a wide range of applications. One solution to this spatio-temporal divide is an observation strategy based on the CubeSat standard, which facilitates constellations of small, inexpensive satellites. Repeatable near-daily image capture in RGB and near-infrared (NIR) bands at 3–4 m resolution has recently become available via a constellation of >130 CubeSats operated commercially by Planet. While the observing capacity afforded by this system is unprecedented, the relatively low radiometric quality and cross-sensor inconsistencies represent key challenges in the realization of their full potential as a game changer in Earth observation. To address this issue, we developed a Cubesat Enabled Spatio-Temporal Enhancement Method (CESTEM) that uses a multi-scale machine-learning technique to correct for radiometric inconsistencies between CubeSat acquisitions. The CESTEM produces Landsat 8 consistent atmospherically corrected surface reflectances in blue, green, red, and NIR bands, but at the spatial scale and temporal frequency of the CubeSat observations. An application of CESTEM over an agricultural dryland system in Saudi Arabia demonstrated CubeSat-based reproduction of Landsat 8 consistent VNIR data with an overall relative mean absolute deviation of 1.6% or better, even when the Landsat 8 and CubeSat acquisitions were temporally displaced by >32 days. The consistently high retrieval accuracies were achieved using a multi-scale target sampling scheme that draws Landsat 8 reference data from a series of scenes by using MODIS-consistent surface reflectance time series to quantify relative changes in Landsat-scale reflectances over given Landsat-CubeSat acquisition timespans. With the observing potential of Planet's CubeSats approaching daily nadir-pointing land surface imaging of the entire Earth, CESTEM offers the capacity to produce daily Landsat 8 consistent VNIR imagery with a factor of 10 increase in spatial resolution and with the radiometric quality of actual Landsat 8 observations. Realization of this unprecedented Earth observing capacity has far reaching implications for the monitoring and characterization of terrestrial systems at the precision scale.}
}
@article{BANERJEE2018149,
title = {A blockchain future for internet of things security: a position paper},
journal = {Digital Communications and Networks},
volume = {4},
number = {3},
pages = {149-160},
year = {2018},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864817302900},
author = {Mandrita Banerjee and Junghee Lee and Kim-Kwang Raymond Choo},
keywords = {Blockchain, Blockchain security, Collaborative security, Internet-of-military things, IoT dataset, IoT self-healing, IoT security, Intrusion-prevention system, Predictive IoT security, Predictive security},
abstract = {Internet of Things (IoT) devices are increasingly being found in civilian and military contexts, ranging from smart cities and smart grids to Internet-of-Medical-Things, Internet-of-Vehicles, Internet-of-Military-Things, Internet-of-Battlefield-Things, etc. In this paper, we survey articles presenting IoT security solutions published in English since January 2016. We make a number of observations, including the lack of publicly available IoT datasets that can be used by the research and practitioner communities. Given the potentially sensitive nature of IoT datasets, there is a need to develop a standard for sharing IoT datasets among the research and practitioner communities and other relevant stakeholders. Thus, we posit the potential for blockchain technology in facilitating secure sharing of IoT datasets (e.g., using blockchain to ensure the integrity of shared datasets) and securing IoT systems, before presenting two conceptual blockchain-based approaches. We then conclude this paper with nine potential research questions.}
}
@article{BITTERMANN2017410,
title = {Assessing the toxicity of ionic liquids – Application of the critical membrane concentration approach},
journal = {Chemosphere},
volume = {183},
pages = {410-418},
year = {2017},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2017.05.097},
url = {https://www.sciencedirect.com/science/article/pii/S0045653517308019},
author = {Kai Bittermann and Kai-Uwe Goss},
keywords = {Membrane, Baseline toxicity, Membrane-water partition coefficient, COSMO, Toxicity, Ions},
abstract = {Charged organic chemicals are a prevailing challenge for toxicity modelling. In this contribution we strive to recapitulate the lessons learned from the well-known modelling of narcosis (or baseline toxicity) of neutral chemicals and apply the concept to charged chemicals. First we reevaluate the organism- and chemical independent critical membrane concentration causing 50% mortality,.cmemtox, based on a critical revision of a previously published toxicity dataset for neutral chemicals. In accordance to values reported in the literature we find a mean value for cmemtox of roughly 100 mmol/kg (membrane lipid) for a broad variety of 42 aquatic organisms (333 different chemicals), albeit with a considerable scatter. Then we apply this concept to permanently charged ionic liquids (ILs). Using COSMOmic, a quantum mechanically based mechanistic model that makes use of the COSMO-RS theory, we predict membrane-water partition coefficients (Kmem/w) of the anionic and cationic IL components. Doing so, cmemtox(total) for permanently charged ILs can be estimated assuming independent, concentration additive contributions of the cationic and its respective anionic species. The resulting values for some of the toxicity data for ionic liquids are consistent with the expected range for baseline toxicity for neutral chemicals while other values are consistently greater or smaller. Based on the calculation of toxic ratios we identify ILs that exert a specific mode of toxic action. Limitations of the modelling approach especially but not exclusively due to the use of nominal concentrations instead of freely-dissolved concentrations in the published literature are critically discussed.}
}
@article{AUSLOOS2017238,
title = {Data science for assessing possible tax income manipulation: The case of Italy},
journal = {Chaos, Solitons & Fractals},
volume = {104},
pages = {238-256},
year = {2017},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0960077917303314},
author = {Marcel Ausloos and Roy Cerqueti and Tariq A. Mir},
keywords = {Data science, Benford law, Aggregated income tax, Data manipulation, Italy},
abstract = {This paper explores a real-world fundamental theme under a data science perspective. It specifically discusses whether fraud or manipulation can be observed in and from municipality income tax size distributions, through their aggregation from citizen fiscal reports. The study case pertains to official data obtained from the Italian Ministry of Economics and Finance over the period 2007–2011. All Italian (20) regions are considered. The considered data science approach concretizes in the adoption of the Benford first digit law as quantitative tool. Marked disparities are found, - for several regions, leading to unexpected “conclusions”. The most eye browsing regions are not the expected ones according to classical imagination about Italy financial shadow matters.}
}
@article{BOUADI2017229,
title = {A data warehouse to explore multidimensional simulated data from a spatially distributed agro-hydrological model to improve catchment nitrogen management},
journal = {Environmental Modelling & Software},
volume = {97},
pages = {229-242},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216305655},
author = {Tassadit Bouadi and Marie-Odile Cordier and Pierre Moreau and René Quiniou and Jordy Salmon-Monviola and Chantal Gascuel-Odoux},
keywords = {Multidimensional modeling, Simulation data, Data warehouse, OLAP, Water quality, Nitrogen, Catchment, Distributed agro-hydrological model},
abstract = {Spatially distributed agro-hydrological models allow researchers and stakeholders to represent, understand and formulate hypotheses about the functioning of agro-environmental systems and to predict their evolution. These models have guided agricultural management by simulating effects of landscape structure, farming system changes and their spatial arrangement on stream water quality. Such models generate many intermediate results that should be managed, analyzed and transformed into usable information. We describe a data warehouse (N-Catch) built to store and analyze simulation data from the spatially distributed agro-hydrological model TNT2. We present scientific challenges to and tools for building data warehouses and describe the three dimensions of N-Catch: space, time and an original hierarchical description of cropping systems. We show how to use OLAP to explore and extract all kinds of useful high-level information by aggregating the data along these three dimensions and how to facilitate exploration of the spatial dimension by coupling N-Catch with GIS. Such tool constitutes an efficient interface between science and society, simulation remaining a research activity, exploration of the results becoming an easy task accessible for a large audience.}
}
@article{YANG2017344,
title = {Efficient traffic congestion estimation using multiple spatio-temporal properties},
journal = {Neurocomputing},
volume = {267},
pages = {344-353},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S092523121731072X},
author = {Yongjian Yang and Yuanbo Xu and Jiayu Han and En Wang and Weitong Chen and Lin Yue},
keywords = {Traffic congestion estimation, Large-scale road networks, Multiple spatio-temporal properties, Dynamic weight calculation, GPS data},
abstract = {Traffic estimation is an important issue to analyze the traffic congestion in large-scale urban traffic situations. Recently, many researchers have used GPS data to estimate traffic congestion. However, how to fuse the multiple data reasonably and guarantee the accuracy and efficiency of these methods are still challenging problems. In this paper, we propose a novel method Multiple Data Estimation (MDE) to estimate the congestion status in urban environment with GPS trajectory data efficiently, where we estimate the congestion status of the area through utilizing multiple properties, including density, velocity, inflow and previous status. Among them, traffic inflow and previous status (combination of time and space factors) are not both used in other existing methods. In order to ensure the accuracy and efficiency, we apply dynamic weights of data and parameters in MDE method. To evaluate our methods, we apply it on large-scale taxi GPS data of Beijing and Shanghai. Extensive experiments on these two real-world datasets demonstrate the significant improvements of our method over several state-of-the-art methods.}
}
@incollection{TSETSOS2019363,
title = {Genetics and Population Analysis},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {363-378},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20114-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338201143},
author = {Fotis Tsetsos and Petros Drineas and Peristera Paschou},
keywords = {Admixture, Disease, Genetics, Genomics, Genotypes, Haplotypes, History, Populations, Variants},
abstract = {Population genetics, the systematic study of patterns of genetic variation, has been undergoing an unprecedented, revolutionary phase in the recent years. The advances of modern technology have enabled the rapid and accurate mass-scale output of modern and ancient genetic data. In this article, we delve into the state-of-the-art methods utilized for computational genetic analysis, the knowledge base required for crafting analytical protocols to avoid biased data, along with important considerations for the proper assessment and interpretation of the data, the methods and their output. We also present illustrative examples of population genetics in the exploration of the human past, as well as its applications in disease mapping and association studies.}
}