@article{PATERSON2018277,
journal = {Accounting Forum},
volume = {42},
number = {3},
pages = {277-280},
year = {2018},
issn = {0155-9982},
doi = {https://doi.org/10.1016/j.accfor.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0155998218300565},
author = {Audrey S. Paterson and Melanie J. Wilson}
}
@article{ALHARTHI2018749,
title = {Healthcare predictive analytics: An overview with a focus on Saudi Arabia},
journal = {Journal of Infection and Public Health},
volume = {11},
number = {6},
pages = {749-756},
year = {2018},
issn = {1876-0341},
doi = {https://doi.org/10.1016/j.jiph.2018.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1876034118300303},
author = {Hana Alharthi},
keywords = {Predictive analytics, Healthcare analytics, Data mining, Saudi Arabia},
abstract = {Despite a newfound wealth of data and information, the healthcare sector is lacking in actionable knowledge. This is largely because healthcare data, though plentiful, tends to be inherently complex and fragmented. Health data analytics, with an emphasis on predictive analytics, is emerging as a transformative tool that can enable more proactive and preventative treatment options. This review considers the ways in which predictive analytics has been applied in the for-profit business sector to generate well-timed and accurate predictions of key outcomes, with a focus on key features that may be applicable to healthcare-specific applications. Published medical research presenting assessments of predictive analytics technology in medical applications are reviewed, with particular emphasis on how hospitals have integrated predictive analytics into their day-to-day healthcare services to improve quality of care. This review also highlights the numerous challenges of implementing predictive analytics in healthcare settings and concludes with a discussion of current efforts to implement healthcare data analytics in the developing country, Saudi Arabia.}
}
@incollection{HUANG2018xxiii,
title = {Preface},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {xxiii-xxiv},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-804660-9.05001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046609050014},
author = {Bo Huang}
}
@incollection{HALL2017217,
title = {Chapter 15 - Predictive Analytics and Population Health},
editor = {Aziz Sheikh and Kathrin M. Cresswell and Adam Wright and David W. Bates},
booktitle = {Key Advances in Clinical Informatics},
publisher = {Academic Press},
pages = {217-225},
year = {2017},
isbn = {978-0-12-809523-2},
doi = {https://doi.org/10.1016/B978-0-12-809523-2.00015-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095232000157},
author = {Peter S. Hall and Andrew Morris},
keywords = {Predictive analytics, statistical modeling, precision medicine, stratified medicine},
abstract = {The prediction of expected clinical outcomes is a basis for many of the decisions made by clinicians and patients about their care. Traditional methods for the development of predictive tools to aid decisions have relied on data from research studies, using well-established medical statistical approaches. New opportunities are now emerging with the increasing availability of routinely collected population-wide healthcare data. These opportunities are coming at a time when rapidly evolving technical capabilities for the handling of large datasets are making possible the application of computationally intense methods. We are now able to develop more sophisticated predictive models that are more representative of real-world situations and are of more relevance to patients treated in framework of precision medicine. This chapter reviews the origins and scientific basis for prediction in healthcare, discusses the current status and the likely challenges if predictive analytics are to be widely adopted across health systems.}
}
@article{RAVINDRANATH2018P1067,
title = {DEEP LEARNING IN AUTOMATED CLASSIFICATION OF ADVERSE EVENTS IN CLINICAL STUDIES OF ALZHEIMER’S DISEASE},
journal = {Alzheimer's & Dementia},
volume = {14},
number = {7, Supplement },
pages = {P1067-P1068},
year = {2018},
issn = {1552-5260},
doi = {https://doi.org/10.1016/j.jalz.2018.06.1369},
url = {https://www.sciencedirect.com/science/article/pii/S1552526018315413},
author = {Pradeep Anand Ravindranath and Rema Raman and Tiffany W. Chow and Michael S. Rafii and Paul S. Aisen and Gustavo Jimenez-Maggiora}
}
@article{PAPARODITIS20171,
title = {Theme issue “Papers from Geospatial Week 2015”},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {127},
pages = {1-2},
year = {2017},
note = {Geospatial Week 2015},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2017.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0924271617302289},
author = {Nicolas Paparoditis and Ian Dowman}
}
@incollection{ZALANI20191110,
title = {Biodiversity Databases and Tools},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {1110-1123},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20200-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338202008},
author = {Lina A.M. Zalani and Kho S. Jye and Shaarmini Balakrishnan and Sarinder K. Dhillon},
keywords = {Biodiversity data warehouse, Biodiversity databases, Biological databases, Life data technology, Life sciences data, Taxonomy},
abstract = {Computers, advances in computational technologies and the Internet have changed the way we store, manage and retrieve data. As in biodiversity, which describes the variety of life found on Earth, the historical way of recording in hard copies have given way to storage in computers. Being one of the most studied field, biodiversity resources are inevitably digital and stored in a wide variety of databases by researchers or stakeholders. In this article, available biodiversity databases are discussed to provide a clear review on the subject. These biodiversity databases are predominantly used by research sites for data visualization, data integration and data mining using simple variables such as image and text data. This article also laid out some of the significant importance of biodiversity databases and tools. Finally, the future direction anticipated in this field is discussed.}
}
@article{ELNAQA20181070,
title = {Radiation Therapy Outcomes Models in the Era of Radiomics and Radiogenomics: Uncertainties and Validation},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {102},
number = {4},
pages = {1070-1073},
year = {2018},
note = {Imaging in Radiation Oncology},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2018.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0360301618335685},
author = {Issam {El Naqa} and Gaurav Pandey and Hugo Aerts and Jen-Tzung Chien and Christian Nicolaj Andreassen and Andrzej Niemierko and Randall K. {Ten Haken}}
}
@article{MULLER2018122,
title = {The Impact of Industry 4.0 on Supply Chains in Engineer-to-Order Industries - An Exploratory Case Study},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {122-127},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.245},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318313697},
author = {Julian M. Müller and Kai-Ingo Voigt},
keywords = {Industry 4.0, Case study, ETO industry, Supply Chain Management},
abstract = {By employing Cyber-Physical-Systems and real-time interconnection in industrial value creation, the term Industry 4.0 expresses expectations towards a fourth industrial revolution. Current research in context of Industry 4.0 mainly focuses on production itself or on production-related logistics processes. However, interconnection across the entire supply chain is required to successfully obtain the potentials predicted for Industry 4.0 Still, supply chain management been scarcely investigated by current research in contrast to solutions based on Industry 4.0 in production. Therefore, this paper attempts to address the topic of supply chain management in context of Industry 4.0. We employ a case study design of a German Engineer-to-Order industrial enterprise and its five logistics partners, which together composes an entire supply chain, finding challenges, potentials and recommendations for Industry 4.0 integration.}
}
@incollection{FARHANHUSSAIN202319,
title = {Chapter 2 - Smart upstream sector: Smartness in upstream sector of the oil and gas industry},
editor = {Razin {Farhan Hussain} and Ali Mokhtari and Ali Ghalambor and Mohsen {Amini Salehi}},
booktitle = {IoT for Smart Operations in the Oil and Gas Industry},
publisher = {Gulf Professional Publishing},
pages = {19-56},
year = {2023},
isbn = {978-0-323-91151-1},
doi = {https://doi.org/10.1016/B978-0-32-391151-1.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911511000113},
author = {Razin {Farhan Hussain} and Ali Mokhtari and Ali Ghalambor and Mohsen {Amini Salehi}},
keywords = {Upstream, Exploration, Survey, Machine learning, Virtual reality, Drilling, Automation, SCADA system},
abstract = {Among three major sectors of O&G, upstream utilizes the most sophisticated and versatile technologies. The upstream sector mainly consists of exploration and production phases. In the exploration phase, various advanced computing systems and technologies are utilized to verify the existence and cost-efficiency of the hydrocarbon extraction from a reservoir. Moreover, the computing systems and technologies are becoming an indispensable part of the production phase to efficiently extract O&G with the minimum side-effects on the workforce and the environment. To understand the usage of IoT and smart technologies, the chapter first provides a computing taxonomy of the upstream sector that reflects the scope of computing systems adapted in the O&G industry. The taxonomy also serves as a roadmap for the whole upstream sector. Then, it traverses the taxonomy, specifically, explaining the computational aspects in detail, as this is the book's main objective. The upstream operations from exploration surveys to production wells and the scopes of using advanced state-of-the-art computing technologies to improve oil and gas production are explored and explained in a detailed manner. In addition, it provides various use cases where utilizing smart computing solutions and automation can improve the safe operation of the oil fields via minimizing its environmental impacts and the human interactions with the hazardous aspects.}
}
@article{SATPATHY2018281,
title = {Sensing and Actuation as a Service Delivery Model in Cloud Edge centric Internet of Things},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {281-296},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17320642},
author = {Suchismita Satpathy and Bibhudatta Sahoo and Ashok Kumar Turuk},
keywords = {Internet of Things, Cloud Computing, Cloud Edge centric IoT, , Sensing and actuation, Virtualization},
abstract = {Internet of Things (IoT) is an advanced innovation of the Internet which allows communications among all living or non-living things. The smart devices have become powerful and intelligent that they can sense, communicate, compute and actuate among themselves to provide a smart environment. So, in short, it could be possibly known as the Internet of Everything (IoE). However, with the current architecture, the contributors are not motivated to share the sensed data or provide their actuators to others as a service. The security and various node managements are some of the main issues to be addressed for motivating the contributors. Therefore, we introduce “Sensing and Actuation as a Service Delivery Model (SAaaSDM)”, which is a cloud edge-centric service delivery model. It authorizes access to the IoT Architecture (IoT-A), where sensed, actuated, and computed data from various existing mobile devices can be used by the end user through SAaaSDM on a pay as you go fashion. Participatory node management, virtual node management, and quality review management are the emerging components of this architecture. Participatory nodes along with device owners claim for various challenges like cost, reliability, trustworthiness, quality, utility etc. Similarly, the expectations of the end users also appear as a big challenge. In this paper, we present the SAaaSDM system model which can deal with open issues and also discuss the future directions for the researchers in this field.}
}
@article{UREN2023102588,
title = {Technology readiness and the organizational journey towards AI adoption: An empirical study},
journal = {International Journal of Information Management},
volume = {68},
pages = {102588},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2022.102588},
url = {https://www.sciencedirect.com/science/article/pii/S0268401222001220},
author = {Victoria Uren and John S. Edwards},
keywords = {Artificial Intelligence, Data, IT-business alignment, System development, Technology adoption},
abstract = {Artificial Intelligence (AI) is viewed as having potential for significant economic and social impact. However, its history of boom and bust cycles can make potential adopters wary. A cross-sectional, qualitative study was carried out, with a purposive sample of AI experts from research, development and business functions, to gain a deeper understanding of the adoption process. Technology Readiness Levels were used as a benchmark against which the experts could align their experiences. A model of AI adoption is proposed which embeds an extended version of the People, Processes, Technology lens, incorporating Data. The model suggests that people, process and data readiness are required in addition to technology readiness to achieve long term operational success with AI. The findings further indicate that innovative organizations should build bridges between technical and business functions.}
}
@article{GALLUZZI2018281,
title = {Mapping uncertainty of ICP-Forest biodiversity data: From standard treatment of diffusion to density-equalizing cartograms},
journal = {Ecological Informatics},
volume = {48},
pages = {281-289},
year = {2018},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1574954118300463},
author = {Marta Galluzzi and Duccio Rocchini and Roberto Canullo and Ronald E. McRoberts and Gherardo Chirici},
keywords = {Density-equalizing cartograms, Uncertainty map, Plant species richness, European biodiversity inference},
abstract = {Data uncertainty due to spatial gaps and heterogeneity is a fundamental problem in conservation and environmental planning. Thus, investigation of issues related to data uncertainty contributes to more efficient conservation plans. We evaluated the uncertainty of data related to forest diversity descriptors using a diffusion-based cartogram approach that visually displays how data information change in function with respect to degree of uncertainty. We used ground vegetation data for 3093 plots collected as part of the BioSoil project through the ICP Forests Level I network and stored in the LI-BioDiv database. For each plot, we assigned an uncertainty value based on the survey season and the mean monthly temperature for the survey period. The density-equalizing map or cartogram highlights that data collected in Spain, the United Kingdom and the German federal states of Berlin and Brandenburg have smaller values of species richness corresponding to larger values of uncertainty. We found that an awareness of the negative relationship between the survey period and species richness can lead to improved data handling and analysis. We demonstrated that cartograms are efficient tools for evaluating and managing uncertainty and can strengthen the results of data analysis by providing alternative perspectives and interpretations of spatial phenomena.}
}
@incollection{LIU2017xv,
title = {Introduction},
editor = {Xiwei Liu and Rangachari Anand and Gang Xiong and Xiuqin Shang and Xiaoming Liu and Jianping Cao},
booktitle = {Big Data and Smart Service Systems},
publisher = {Academic Press},
pages = {xv-xxix},
year = {2017},
isbn = {978-0-12-812013-2},
doi = {https://doi.org/10.1016/B978-0-12-812013-2.00016-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128120132000162},
author = {X. Liu and W. Wei and X. Shang and X. Dong}
}
@incollection{FRYMAN2017137,
title = {Chapter 5 - Aligning the Language of Business: The Business Glossary},
editor = {Lowell Fryman and Gregory Lampshire and Dan Meers},
booktitle = {The Data and Analytics Playbook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {137-157},
year = {2017},
isbn = {978-0-12-802307-5},
doi = {https://doi.org/10.1016/B978-0-12-802307-5.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023075000058},
author = {Lowell Fryman and Gregory Lampshire and Dan Meers},
keywords = {Business glossary, Business terms, Data assets, Data governance, Data lineage, Data quality metrics, Data standards, Data stewardship, Governance workflow, Issue management, IT assets, Metadata, Semantics, Vocabulary},
abstract = {The business glossary is a critical deliverable from the governance team. Many industry experts agree that the business glossary is the most important component of the data governance team and its processes. It is the team's responsibility to ensure the definitions of the business terms and data assets are managed by the data stewards and governance processes. This chapter provides the objectives the data governance team needs to implement a business glossary using the data Playbook or other technology. We discuss alternatives to the business glossary standards, roles and responsibilities involved, alignment activities, leveraging of IT, and data assets as well as the use of and accessibility of the glossary. We also discuss how the different styles of governance impact the workflow processes and lifecycles of the data in the business glossary.}
}
@article{VERHELSTJ2017673,
title = {Model selection for continuous commissioning of HVAC-systems in office buildings: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {76},
pages = {673-686},
year = {2017},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2017.01.119},
url = {https://www.sciencedirect.com/science/article/pii/S1364032117301284},
author = { {Verhelst J.} and  {Van Ham G.} and  {Saelens D.} and  {Helsen L.}},
keywords = {HVAC, Control, Office building, Review, Model, FDDe, Continuous commissioning},
abstract = {This paper presents an overview of literature and procedures about real-life, state-of-the-art implementations of model-based (MB) Continuous Commissioning (CCx) in office buildings. The focus is on the building- and HVAC-models used for each of three distinct CCx-domains: The identification of energy conserving opportunities (ECOs), fault detection, diagnosis, evaluation and overhaul (FDDe) and model-based control (MBC). For each domain, the relations between chosen model structure, model order, parameter estimation procedure, available sensor data quality and calculation power are highlighted. These insights are critical for office building managers, BEMS manufacturers and researchers involved or interested in the selection and implementation of MBCC strategies. The analyses indicate that the chosen model order and parameter estimation technique depend highly on available calculation power and data availability. Full model-sharing between different subtopics is rarely performed, presumably due to the diversity of model requirements for each CCx-domain. Several model structures and parameter estimation procedures -e.g. multi-step-ahead and subspace identification- are recurring frequently within one domain -e.g. MBC-. Also, both within and between CCx-domains, the exchange of available expert knowledge and measurements for parameter estimation improves the accuracy of the resulting models.}
}
@article{NGUYEN201872,
title = {Computer-aided discovery of debris disk candidates: A case study using the Wide-Field Infrared Survey Explorer (WISE) catalog},
journal = {Astronomy and Computing},
volume = {23},
pages = {72-82},
year = {2018},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2213133717300197},
author = {T. Nguyen and V. Pankratius and L. Eckman and S. Seager},
keywords = {Debris disk, WISE, Machine learning, Classification},
abstract = {Debris disks around stars other than the Sun have received significant attention in studies of exoplanets, specifically exoplanetary system formation. Since debris disks are major sources of infrared emissions, infrared survey data such as the Wide-Field Infrared Survey (WISE) catalog potentially harbors numerous debris disk candidates. However, it is currently challenging to perform disk candidate searches for over 747 million sources in the WISE catalog due to the high probability of false positives caused by interstellar matter, galaxies, and other background artifacts. Crowdsourcing techniques have thus started to harness citizen scientists for debris disk identification since humans can be easily trained to distinguish between desired artifacts and irrelevant noises. With a limited number of citizen scientists, however, increasing data volumes from large surveys will inevitably lead to analysis bottlenecks. To overcome this scalability problem and push the current limits of automated debris disk candidate identification, we present a novel approach that uses citizen science results as a seed to train machine learning based classification. In this paper, we detail a case study with a computer-aided discovery pipeline demonstrating such feasibility based on WISE catalog data and NASA’s Disk Detective project. Our approach of debris disk candidates classification was shown to be robust under a wide range of image quality and features. Our hybrid approach of citizen science with algorithmic scalability can facilitate big data processing for future detections as envisioned in future missions such as the Transiting Exoplanet Survey Satellite (TESS) and the Wide-Field Infrared Survey Telescope (WFIRST).}
}
@article{DRZYMALSKI2018110,
title = {Editorial commentary: Death after acute myocardial infarction, possible to predict?},
journal = {Trends in Cardiovascular Medicine},
volume = {28},
number = {2},
pages = {110-111},
year = {2018},
issn = {1050-1738},
doi = {https://doi.org/10.1016/j.tcm.2017.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1050173817301305},
author = {Krzysztof Drzymalski and Joshua Schulman-Marcus}
}
@article{ROUGH20181136,
title = {When Does Size Matter? Promises, Pitfalls, and Appropriate Interpretation of “Big” Medical Records Data},
journal = {Ophthalmology},
volume = {125},
number = {8},
pages = {1136-1138},
year = {2018},
issn = {0161-6420},
doi = {https://doi.org/10.1016/j.ophtha.2018.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0161642018304135},
author = {Kathryn Rough and John T. Thompson}
}
@article{ZHENG2018665,
title = {Business intelligence for patient-centeredness: A systematic review},
journal = {Telematics and Informatics},
volume = {35},
number = {4},
pages = {665-676},
year = {2018},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2017.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0736585316306943},
author = {Wenzhi Zheng and Yen-Chun Jim Wu and Liangyong Chen},
keywords = {Business intelligence, Patient-centeredness, Systematic review, Clinician–patient relationship, Patient involvement, Health data},
abstract = {This study utilized a systematic review to provide an overall understanding of how academic research can be incorporated into business intelligence (BI) to ensure patient-centeredness (PC). Using the BI maturity model, this study analyzed findings of previous studies from four time periods within the period 2000–2016 to determine how BI can facilitate PC through organization, human-orientation, and technology, as well as other PC-specific conditions. Our results indicate that the number of BI applications that include PC have continued to grow since 2010, and that they primarily focus on the dimensions of organization, humanism, and PC-specific conditions; additionally, we noted that a time-based correlation exists between the related results. This study then explored the extent to which BI supports the subdimensions of PC (e.g., principles, enablers, and activities). Finally, future research focuses and directions were proposed.}
}
@article{LI2023118543,
title = {Time series clustering based on complex network with synchronous matching states},
journal = {Expert Systems with Applications},
volume = {211},
pages = {118543},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118543},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016153},
author = {Hailin Li and Zechen Liu and Xiaoji Wan},
keywords = {Time series clustering, Complex network, Synchronous matching, Data mining},
abstract = {Due to the extensive existence of time series in various fields, more and more research on time series data mining, especially time series clustering, has been done in recent years. Clustering technology can extract valuable information and potential patterns from time series data. This paper proposes a time series Clustering method based on Synchronous matching of Complex networks (CSC). This method uses density peak clustering algorithm to identify the state of each time point and obtains the state sequence according to the timeline of the original time series. State sequences is a new method to represent time series. By comparing two state sequences synchronously, the length of state sequence with step is calculated and the similarity is presented, which forms a new method to calculate the similarity of time series. Based on the obtained time series similarity, the relationship network of time series is constructed. Simultaneously, the community discovery technology is applied to cluster the relationship network and further achieve the complete time series clustering. The detailed process and simulation experiments of CSC method are given. Experimental results on different datasets show that CSC method is superior to other traditional time series clustering methods.}
}
@article{LIU201888,
title = {Special issue on internet plus government: New opportunities to solve public problems?},
journal = {Government Information Quarterly},
volume = {35},
number = {1},
pages = {88-97},
year = {2018},
note = {Internet Plus Government: Advancement of Networking Technology and Evolution of the Public Sector},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18300248},
author = {Shuhua Monica Liu and Yushim Kim},
abstract = {This editorial opens by introducing Internet Plus Government, a new government initiative emerging after the US presidential election in 2008. Comparing to the more descriptive definitions of e-government, supporters of ‘Internet Plus Government’ emphasize the transformative and normative aspect of the newest generation of Information and Communication Technology (ICTs). They argue that the new initiative designates how government should operate and in turn implies how state-citizen relationships are transformed. To understand the core of this initiative and whether it offers new opportunities to solve public problems, we conducted analyses of research articles published in the e-governance11E-governance is majorly concerned with the adoption, implementation and use of ICTs in the relationships between state and the society in the complex process of delivery of public policy and public operations (Mueleman, 2008). It includes three distinct strands: Administrative governance; interactive governance and governance as self-organizing networks (Osborne, 2010). area between 2008 and 2017. Our analysis suggests that the Internet Plus Government initiative has enriched the government information infrastructure. That is, it has enabled the accumulation and use of huge volumes of data for better decision making. The advancement of open data, the wide use of social media, and the potential of data analytics have also generated pressure to address challenging questions and issues in e-democracy.22E-democracy involves the use of digital networks by which government solicits or receives the views of citizens, businesses and other organizations “on matters ranging from full-scale legislative change to the tweaking of the management of services and programs” (Perri 6, 2004) However, the analysis leads us to deliberate on whether Internet Plus Government initiatives worldwide have actually achieved their goal. After introducing papers included in this special issue, we present challenges to be addressed before Internet Plus Government initiatives realize their potential towards better public governance.}
}
@article{COBO2018364,
title = {Industry 4.0: a perspective based on bibliometric analysis},
journal = {Procedia Computer Science},
volume = {139},
pages = {364-371},
year = {2018},
note = {6th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.278},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319483},
author = {M.J. Cobo and B. Jürgens and V. Herrero-Solana and M.A. Martínez and E. Herrera-Viedma},
keywords = {Bibliometric Analysis, Science Mapping Analysis, Co-words analysis, Industry 4.0},
abstract = {The main aim of this contribution is to develop a co-words analysis of the Industry 4.0 research field in order to highlight the themes covered in the last five years (2013–2017). The software tool SciMAT is employed using an approach that allows us to uncover the main research themes and analyze them according to their performance and impact measures. An amount of 333 documents were retrieved from the Web of Science. Our key findings are that the most important research themes were Cyber-Physical-Systems and Cloud-Computing.}
}
@article{KARTHIK2023104176,
title = {Convolution neural networks for optical coherence tomography (OCT) image classification},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104176},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104176},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006309},
author = {Karri Karthik and Manjunatha Mahadevappa},
keywords = {Image classification, Optical coherence tomography (OCT), Convolution neural networks, Retinal diseases, Cross activation},
abstract = {Optical coherence tomography (OCT) is an imaging modality used to obtain a cross-sectional image of the retina for retinal disease diagnosis. Modern diagnosis systems use Convolutional Neural Networks. Our model increases the contrast in the residual connection, so high contrast regions, such as the retinal layers, are prominent in feature maps. Our model increases the contrast of the derivatives to generate sharper feature maps. We replaced the residual connection in standard ResNet architectures with our design. The proposed activation function retains negative weights and reinforces smaller gradients. We have used two OCT datasets with four and eight classes of diseases, respectively. We performed graphical analysis using Precision–Recall curves. We used accuracy, precision, recall, and F1 score for evaluation. In our laboratory conditions, We have successfully increased the classification accuracy with our proposed design. The gain in accuracy is limited, i.e. <1% when the initial accuracy is more than 98%, and 1.6% when the initial accuracy is lower. In confusion matrices, we observed the maximum performance increase when the number of samples is less in one class, which will be helpful if data is imbalanced. The retinal boundary is enhanced, with the background (the region outside the retinal layers) suppressed but not entirely removed. In ablation studies, We observed an average accuracy loss of 0.875% with OCT-C4 data and 1.39% for OCT-C8 data. The p-values from Wilcoxon signed-rank test range from 1.65 × 10−6 to 0.025, and 0.51 for ResNet50 with the OCT-C8 dataset.}
}
@article{LIU20181056,
title = {Environmental accounting: In between raw data and information use for management practices},
journal = {Journal of Cleaner Production},
volume = {197},
pages = {1056-1068},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.06.194},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618318420},
author = {Gengyuan Liu and Xinan Yin and Walter Pengue and Enrico Benetto and Donald Huisingh and Hans Schnitzer and Yutao Wang and Marco Casazza},
keywords = {Environmental accounting, Environmental management, Sustainable development goals, Environmental data, Socio-economic data, Accounting methods},
abstract = {Scholars in environmental accounting have developed many methods, capable to transform raw environmental and socio-economic data into useful information, both to protect natural ecosystems and to define the most appropriate policy and planning options to meet the existing sustainable development goals. Due to the high number of existing research challenges and needs, Beijing Normal University organized a World Summit on Environmental Accounting and Management on “Designing A Prosperous and Sustainable Future”, which was held in Beijing on July 4–6, 2016. The main topic of the conference was the inclusion of system-wide effects into on-site environmental impacts, considering an integrated environmental accounting and management framework. The outcomes of this international summit, partially represented by the papers published in this Special Volume, provide an opportunity to assess the most recent progresses in biophysical and socioeconomic accounting, as well as in modelling the impacts of anthropogenic activities on environmental and socioeconomic systems. This SV includes cutting-edge papers, that focused on promoting the theories, ideas and practices involved in ecological accounting and management. All the works are aimed to develop broader perspectives, which can be applied to ecosystem protection, as well as on planning and policy-making in view of a transition toward more sustainable and equitable societies, as indicated through the Sustainable Development Goals.}
}
@article{ZHANG2018239,
title = {An intelligent power distribution service architecture using cloud computing and deep learning techniques},
journal = {Journal of Network and Computer Applications},
volume = {103},
pages = {239-248},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517302941},
author = {Weishan Zhang and Gaowa Wulan and Jia Zhai and Liang Xu and Dehai Zhao and Xin Liu and Su Yang and Jiehan Zhou},
keywords = {LSTM, MQTT, Power distribution, SVR, Prediction, Cloud computing, XGBoost},
abstract = {Smart management of power consumption for green living is important for sustainable development. Existing approaches could not provide a complete solution for both smart monitoring of electricity consumption, and also intelligent processing of the collected data effectively. This paper presents a cloud-based intelligent power distribution service architecture, where an intelligent electricity box (IEB) is designed using Zigbee and Raspberry Pi, and a standard MQTT (Message Queuing Telemetry Transport) protocol is used to transfer monitored data to the backend Cloud computing infrastructure using open source software packages. The IEB provides cloud services of real-time electricity information checking, power consumption monitoring, and remote control of switches. The current and historical data are stored in HBase and analyzed using Long Short Term Memory (LSTM). Evaluations and practical usage show that our proposed solution is very efficient in terms of availability, performance, and the deep learning based approach has better prediction accuracy than that of both classical SVR based approach and the latest XGBoost approach.}
}
@article{JOHNSON2018xv,
title = {Advanced Epidemiologic Methods for the Study of Rheumatic and Musculoskeletal Diseases},
journal = {Rheumatic Disease Clinics of North America},
volume = {44},
number = {2},
pages = {xv-xvi},
year = {2018},
note = {Advanced Epidemiologic Methods for the Study of Rheumatic Diseases},
issn = {0889-857X},
doi = {https://doi.org/10.1016/j.rdc.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0889857X18300140},
author = {Sindhu R. Johnson}
}
@incollection{GUDIVADA2018173,
title = {7 - Data Management Issues in Cyber-Physical Systems},
editor = {Lipika Deka and Mashrur Chowdhury},
booktitle = {Transportation Cyber-Physical Systems},
publisher = {Elsevier},
pages = {173-200},
year = {2018},
isbn = {978-0-12-814295-0},
doi = {https://doi.org/10.1016/B978-0-12-814295-0.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128142950000071},
author = {Venkat N. Gudivada and Srini Ramaswamy and Seshadri Srinivasan},
keywords = {Cyber-physical systems, Cybersecurity, Data management, Embedded systems, NoSQL systems},
abstract = {The overarching goal of this chapter is to describe data management and security-related issues in cyber-physical systems (CPS). First, we discuss CPS as a synergistic confluence several domains. Next, we analyse their data management and security needs. The recent emergence of NoSQL systems and numerous choices for data management in CPS are discussed. Elasticsearch, a NoSQL system for unstructured data processing, analytics and management, is described. The chapter concludes by indicating emerging trends and data-related research issues in CPS.}
}
@article{LEE20181338,
title = {Clustering learning model of CCTV image pattern for producing road hazard meteorological information},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1338-1350},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17318253},
author = {Jiwan Lee and Bonghee Hong and Sunghoon Jung and Victor Chang},
abstract = {A method for real-time estimation of weather, especially the amount of rainfall, by analyzing CCTV images, is much cheaper than one using the existing expensive weather observation equipment. In this paper, we propose a method to find an estimation model function whose input is CCTV images and output is the amount of rainfall. Using CCTV images, we propose an algorithm for selecting the number and size of the region of interest optimized for rainfall estimation, generating a data pattern graph showing a clear distinction from the number of regions of interest, clustering the pattern data graphs, and estimating the amount of rainfall. Experiments using real CCTV images show that the estimation accuracy is greater than 80%.}
}
@article{TSENG201775,
title = {Incidence and mortality of pancreatic cancer on a rapid rise in Taiwan, 1999–2012},
journal = {Cancer Epidemiology},
volume = {49},
pages = {75-84},
year = {2017},
issn = {1877-7821},
doi = {https://doi.org/10.1016/j.canep.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877782117300838},
author = {Chao-Ming Tseng and Shih-Pei Huang and Wei-Chih Liao and Chun-Ju Chiang and Ya-Wen Yang and Chi-Yang Chang and Yao-Chun Hsu and Hui-Chi Chen and Han-Sun Chiang and Jaw-Town Lin},
keywords = {Pancreatic cancer, Epidemiology, Secular trend, National Health Insurance database},
abstract = {Background
Accumulating data has revealed a rapidly rising incidence of pancreatic cancer in Western countries, but convincing evidence from the East remains sparse. We aimed to quantify how the incidence and mortality rates of pancreatic malignancy changed over time in Taiwan, and to develop future projection for the next decade.
Methods
This nationwide population-based study analyzed the Taiwan National Cancer Registry and the National Cause of Death Registry to calculate the annual incidence and mortality rates of pancreatic malignancy from 1999 to 2012 in this country. The secular trend of the incidence was also examined by data from the National Health Insurance Research Database.
Results
A total of 21,986 incident cases of pancreatic cancer and 20,720 related deaths occurred during the study period. The age-standardized incidence rate increased from 3.7 per 100,000 in 1999 to 5.0 per 100,000 in 2012, with a significant rising trend (P<0.01). The increase was nationwide, consistently across subgroups stratified by age, gender, geographic region, and urbanization. Data from the National Health Insurance Research Database corroborated the rise of incident pancreatic cancer. Mortality also increased with time, with the age-standardized rate rising from 3.5 per 100,000 in 1999 to 4.1 per 100,000 in 2012 (P<0.01). In accordance with the incidence, the mortality trend was consistent in all subgroups. Both the incidence and mortality were projected to further increase by approximately 20% from 2012 to 2027.
Conclusion
The incidence and mortality of pancreatic cancer have been rapidly rising and presumably will continue to rise in Taiwan.}
}
@article{DAMIANSEGRELLESQUILIS202338,
title = {A federated cloud architecture for processing of cancer images on a distributed storage},
journal = {Future Generation Computer Systems},
volume = {139},
pages = {38-52},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200303X},
author = {J. {Damián Segrelles Quilis} and Sergio López-Huguet and Pau Lozano and Ignacio Blanquer},
keywords = {Medical imaging, Biomarkers, Storage and computing backends},
abstract = {The increased accuracy and exhaustivity of modern Artificial Intelligence techniques in supporting the analysis of complex data, such as medical images, have exponentially increased real-world data collection for research purposes. This fact has led to the development of international repositories and high-performance computing solutions to deal with the computational demand for training models. However, other stages in the development of medical imaging biomarkers do not require such intensive computing resources, which has led to the convenience of integrating different computing backends tailored for the processing demands of the various stages of processing workflows. We present in this article a distributed and federated repository architecture for the development and application of medical image biomarkers that combines multiple cloud storages with cloud and HPC processing backends. The architecture has been deployed to serve the PRIMAGE (H2020 826494) project, aiming to collect and manage data from paediatric cancer. The repository seamlessly integrates distributed storage backends, an elastic Kubernetes cluster on a cloud on-premises and a supercomputer. Processing jobs are handled through a single control platform, synchronising data on demand. The article shows the specification of the different types of applications and a validation through a use case that make use of most of the features of the platform.}
}
@incollection{CHESSELL2017xxv,
title = {Foreword by Mandy Chessell},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xxv-xxvii},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00026-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000260},
author = {Mandy Chessell}
}
@article{KEY2017508,
title = {Mining Metagenomic Data Sets for Ancient DNA: Recommended Protocols for Authentication},
journal = {Trends in Genetics},
volume = {33},
number = {8},
pages = {508-520},
year = {2017},
issn = {0168-9525},
doi = {https://doi.org/10.1016/j.tig.2017.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0168952517300860},
author = {Felix M. Key and Cosimo Posth and Johannes Krause and Alexander Herbig and Kirsten I. Bos},
keywords = {ancient DNA authentication, ancient humans, metagenomics, paleomicrobiology, ancient pathogens, historical infectious disease},
abstract = {While a comparatively young area of research, investigations relying on ancient DNA data have been highly valuable in revealing snapshots of genetic variation in both the recent and the not-so-recent past. Born out of a tradition of single-locus PCR-based approaches that often target individual species, stringent criteria for both data acquisition and analysis were introduced early to establish high standards of data quality. Today, the immense volume of data made available through next-generation sequencing has significantly increased the analytical resolution offered by processing ancient tissues and permits parallel analyses of host and microbial communities. The adoption of this new approach to data acquisition, however, requires an accompanying update on methods of DNA authentication, especially given that ancient molecules are expected to exist in low proportions in archaeological material, where an environmental signal is likely to dominate. In this review, we provide a summary of recent data authentication approaches that have been successfully used to distinguish between endogenous and nonendogenous DNA sequences in metagenomic data sets. While our discussion mostly centers on the detection of ancient human and ancient bacterial pathogen DNA, their applicability is far wider.}
}
@article{DEANS2018375,
title = {Learning health systems},
journal = {Seminars in Pediatric Surgery},
volume = {27},
number = {6},
pages = {375-378},
year = {2018},
note = {A Clinical Research Primer for Pediatric Surgeons},
issn = {1055-8586},
doi = {https://doi.org/10.1053/j.sempedsurg.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1055858618300933},
author = {Katherine J. Deans and Sara Sabihi and Christopher B. Forrest},
keywords = {Learning health system, Pediatrics, PEDSnet, Electronic health record},
abstract = {Healthcare organizations have invested significant resources into integrating comprehensive electronic health record (EHR) systems into clinical care. EHRs digitize healthcare in ways that allow for repurposing of clinical information to support quality improvement, research, population health, and health system analytics. This has facilitated the development of Learning Health Systems. Learning health systems (LHS) merge healthcare delivery with research, data science, and quality improvement processes. The LHS cycle begins and ends with the clinician-patient interaction, and aspires to provide continuous improvements in quality, outcomes, and health care efficiency. Although, the health sector has been slow to embrace the LHS concept, innovative approaches for improving healthcare, such as a LHS, have shown that better outcomes can be achieved by engaging patients and physicians in communities committed to a common purpose. Here, we explore the mission of a pediatric LHS, such as PEDSnet, which is driven by the distinctive goals of a child's well-being. Its vision is to create a national LHS architecture in which all pediatric institutions can participate. While challenges still exist in the development and adoption of LHS, these challenges are being met with innovative strategies and strong collaborative relationships to reduce system uncertainty while improving patient outcomes.}
}
@article{PENG2023794,
title = {Observation-based sources evolution of non-methane hydrocarbons (NMHCs) in a megacity of China},
journal = {Journal of Environmental Sciences},
volume = {124},
pages = {794-805},
year = {2023},
issn = {1001-0742},
doi = {https://doi.org/10.1016/j.jes.2022.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S1001074222000468},
author = {Yarong Peng and Hongli Wang and Qian Wang and Shengao Jing and Jingyu An and Yaqin Gao and Cheng Huang and Rusha Yan and Haixia Dai and Tiantao Cheng and Qiang Zhang and Meng Li and Jianlin Hu and Zhihao Shi and Li Li and Shengrong Lou and Shikang Tao and Qinyao Hu and Jun Lu and Changhong Chen},
keywords = {NMHCs, Characteristics, Source apportionment, Observation-based, Interannual trend, Shanghai},
abstract = {Both concentrations and emissions of many air pollutants have been decreasing due to implement of control measures in China, in contrast to the fact that an increase in emissions of non-methane hydrocarbons (NMHCs) has been reported. This study employed seven years continuous NMHCs measurements and the related activities data of Shanghai, a megacity in China, to explore evolution of emissions and effectiveness of air pollution control measures. The mixing ratio of NMHCs showed no statistical interannual changes, of which their compositions exhibited marked changes. This resulted in a decreasing trend of ozone formation potential by 3.8%/year (p < 0.05, the same below), which should be beneficial to ozone pollution mitigation as its production in Shanghai is in the NMHCs-limited regime. Observed alkanes, aromatics and acetylene changed by +3.7%/year, -5.9%/year and -7.4%/year, respectively, and alkenes showed no apparent trend. NMHCs sources were apportioned by a positive matrix factorization model. Accordingly, vehicular emissions (-5.9%/year) and petrochemical industry emissions (-7.1%/year) decreased significantly, but the decrease slowed down; significant reduction in solvent usage (-9.0%/year) appeared after 2010; however, emissions of natural gas (+12.6%/year) and fuel evaporation (with an increasing fraction) became more important. The inconsistency between observations and inventories was found in interannual trend and speciation as well as source contributions, emphasizing the need for further validation in NMHCs emission inventory. Our study confirms the effectiveness of measures targeting mobile and centralized emissions from industrial sources and reveals a need focusing on fugitive emissions, which provided new insights into future air policies in polluted region.}
}
@article{LEVIN2017115,
title = {An evaluation of crowdsourced information for assessing the visitation and perceived importance of protected areas},
journal = {Applied Geography},
volume = {79},
pages = {115-126},
year = {2017},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2016.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0143622816308268},
author = {Noam Levin and Alex Mark Lechner and Greg Brown},
keywords = {Flickr, OpenStreetMap, Place values, Public participation GIS (PPGIS), Volunteered geographic information (VGI), Wikipedia},
abstract = {Parks and protected areas provide a wide range of benefits, but methods to evaluate their importance to society are often ad hoc and limited. In this study, the quality of crowdsourced information from Public Participation GIS (PPGIS) and Volunteered Geographic Information (VGI) sources (Flickr, OpenStreetMap (OSM), and Wikipedia) was compared with visitor counts that are presumed to reflect social importance. Using the state of Victoria, Australia as a case study, secondary crowdsourced VGI data, primary crowdsourced (PPGIS data) and visitor statistics were examined for their correspondence and differences, and to identify spatial patterns in park popularity. Data completeness—the percent of protected areas with data—varied between sources, being highest for OSM (90%), followed by Flickr (41%), PPGIS (24%), visitation counts (5%), and Wikipedia articles (4%). Statistically significant correlations were found between all five measures of popularity for protected areas. Using stepwise multiple linear regression, the explained variability in visitor numbers was greater than 70%, with PPGIS, Flickr and OSM having the largest standardized coefficients. The social importance of protected areas varied as a function of accessibility and the types of values (direct or indirect use) expressed for the areas. Crowdsourced data may provide an alternative to visitor counts for assessing protected area social importance and spatial variability of visitation. However, crowdsourced data appears to be an unreliable proxy for the full range of values and importance of protected areas, especially for non-use values such as biological conservation.}
}
@incollection{2018225,
title = {Index},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Federal Data Science},
publisher = {Academic Press},
pages = {225-229},
year = {2018},
isbn = {978-0-12-812443-7},
doi = {https://doi.org/10.1016/B978-0-12-812443-7.18001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124437180019}
}
@article{XUE2017926,
title = {Fault detection and operation optimization in district heating substations based on data mining techniques},
journal = {Applied Energy},
volume = {205},
pages = {926-940},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S0306261917310401},
author = {Puning Xue and Zhigang Zhou and Xiumu Fang and Xin Chen and Lin Liu and Yaowen Liu and Jing Liu},
keywords = {District heating substation, Data mining, Automatic meter reading system, Fault detection, Operation optimization},
abstract = {The present generation of district heating (DH) technologies will have to be further developed into the 4th generation to fulfil the important role in future smart energy systems. At present, automatic meter reading systems have been installed in DH systems. These systems make hourly or even minutely meter readings available at low cost. However, the sheer quantity and complex of the data poses a challenge at various levels for traditional data analysis approaches. Data mining is a promising technology and is used to automatically extract valuable knowledge hidden in large amounts of data. To investigate the potential application of descriptive data mining techniques in DH systems, this study proposes a method based on descriptive data mining to improve the energy performance of DH substations. The proposed method consists of five steps: data cleaning, data transformation, cluster analysis, association analysis, and interpretation/evaluation. Data cleaning and transformation are implemented to improve data quality and transform data into forms that are appropriate for mining. Cluster analysis is performed to identify distinct operating patterns of substations. Based on each pattern, association analysis is then adopted to discover the unsuspected knowledge in the form of rules. Interpretation/evaluation is performed to select and interpret potentially useful rules. To demonstrate its applicability, the proposed method is used to analyze the datasets obtained from an automatic meter reading system at two substations in the DH system in Changchun, China. This application reveals that the method can effectively extract potentially useful knowledge and thereby provide essential guidance for the fault detection and operation optimization of DH substations.}
}
@article{COSTA2017726,
title = {The data scientist profile and its representativeness in the European e-Competence framework and the skills framework for the information age},
journal = {International Journal of Information Management},
volume = {37},
number = {6},
pages = {726-734},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S026840121730600X},
author = {Carlos Costa and Maribel Yasmina Santos},
keywords = {Conceptual model, Data science, Data scientist, Knowledge, Skills},
abstract = {The activities in our current world are mainly supported by data-driven web applications, making extensive use of databases and data services. Such phenomenon led to the rise of Data Scientists as professionals of major relevance, which extract value from data and create state-of-the-art data artifacts that generate even more increased value. During the last years, the term Data Scientist attracted significant attention. Consequently, it is relevant to understand its origin, knowledge base and skills set, in order to adequately describe its profile and distinguish it from others like Business Analyst. This work proposes a conceptual model for the professional profile of a Data Scientist and evaluates the representativeness of this profile in two commonly recognized competences/skills frameworks in the field of Information and Communications Technology (ICT), namely in the European e-Competence (e-CF) framework and the Skills Framework for the Information Age (SFIA). The results indicate that a significant part of the knowledge base and skills set of Data Scientists are related with ICT competences/skills, including programming, machine learning and databases. The Data Scientist professional profile has an adequate representativeness in these two frameworks, but it is mainly seen as a multi-disciplinary profile, combining contributes from different areas, such as computer science, statistics and mathematics.}
}
@incollection{2018xxv,
title = {Author's Preface to First Edition},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {xxv-xxviii},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.09999-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094099994}
}
@article{PENG2017483,
title = {A hybrid data mining approach on BIM-based building operation and maintenance},
journal = {Building and Environment},
volume = {126},
pages = {483-495},
year = {2017},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2017.09.030},
url = {https://www.sciencedirect.com/science/article/pii/S0360132317304444},
author = {Yang Peng and Jia-Rui Lin and Jian-Ping Zhang and Zhen-Zhong Hu},
keywords = {Data mining, Building information modeling, Operation and maintenance, Cluster analysis, Pattern analysis, Outlier detection},
abstract = {Huge amounts of data are generated daily during the operation and maintenance (O&M) phase of buildings. These accumulated data have the potential to provide deep information that can help improve facility management. Building Information Model/Modeling (BIM) technology has proven potential in O&M management in some studies, making it possible to store massive data. However, the complex and non-intuitive data records, as well as inaccurate manual inputs, raise difficulties in making full use of information in current O&M activities. This paper aims to address these problems by proposing a BIM-based Data Mining (DM) approach for extracting meaningful laws and patterns, as well as detecting improper records. In this approach, the BIM database is first transformed into a data warehouse. After that, three DM methods are combined to find useful information from the BIM. Specifically, the cluster analysis can find relationships of similarity among records, the outlier detection detects manually input improper data and keeps the database fresh, and the improved pattern mining algorithm finds deeper logic links among records. Particular emphasis is put on introducing the algorithms and how they should be used by building managers. Hence, the value of BIM is increased based on rules, extracted from data of O&M phase that appear irregular and disordered. Validated by an integrated on-site practice in an airport terminal, the proposed DM methods are helpful in prediction, early warning, and decision making, leading to the improvements of resource usage and maintenance efficiency during the O&M phase.}
}
@article{TONNE2017155,
title = {New frontiers for environmental epidemiology in a changing world},
journal = {Environment International},
volume = {104},
pages = {155-162},
year = {2017},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2017.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0160412017301459},
author = {Cathryn Tonne and Xavier Basagaña and Basile Chaix and Maud Huynen and Perry Hystad and Tim S. Nawrot and Remy Slama and Roel Vermeulen and Jennifer Weuve and Mark Nieuwenhuijsen},
keywords = {Environment, Epidemiology, Demographics, Technology, OMICs, Sensors},
abstract = {Background
In the next 25years, transformative changes, in particular the rapid pace of technological development and data availability, will require environmental epidemiologists to prioritize what should (rather than could) be done to most effectively improve population health.
Objectives
In this essay, we map out key driving forces that will shape environmental epidemiology in the next 25years. We also identify how the field should adapt to best take advantage of coming opportunities and prepare for challenges.
Discussion
Future environmental epidemiologists will face a world shaped by longer lifespans but also larger burdens of chronic health conditions; shifting populations by region and into urban areas; and global environmental change. Rapidly evolving technologies, particularly in sensors and OMICs, will present opportunities for the field. How should it respond? We argue, the field best adapts to a changing world by focusing on healthy aging; evidence gaps, especially in susceptible populations and low-income countries; and by developing approaches to better handle complexity and more formalized analysis.
Conclusions
Environmental epidemiology informing disease prevention will continue to be valuable. However, the field must adapt to remain relevant. In particular, the field must ensure that public health importance drives research questions, while seizing the opportunities presented by new technologies. Environmental epidemiologists of the future will require different, refined skills to work effectively across disciplines, ask the right questions, and implement appropriate study designs in a data-rich world.}
}
@article{YEN20181591,
title = {Determination of risk factors for burn mortality based on a regional population study in Taiwan},
journal = {Burns},
volume = {44},
number = {6},
pages = {1591-1601},
year = {2018},
issn = {0305-4179},
doi = {https://doi.org/10.1016/j.burns.2018.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S0305417918301578},
author = {Cheng-I Yen and Meng-Jiun Chiou and Chang-Fu Kuo and Han-Tsung Liao},
keywords = {Burn, Epidemiology, Mortality, Risk factors},
abstract = {Background
Burns are not only major personal catastrophic events but also constitute a national health problem due to its associated morbidity, rehabilitation, mortality and high cost medical services. Advances in care and treatment have increased survival from major burn injury. However, information on the epidemiology and risk factors of burn mortality in Taiwan is limited. The study aim was to determine the nationwide epidemiological characteristics, trends, and mortality risk factors of burn inpatients in Taiwan.
Methods
This nationwide population-based study evaluated data retrieved from the Taiwan National Health Insurance database. Patients hospitalized for burns (ICD-9-CM codes 940-949) between 2003 and 2013 were identified from hospitalization records.
Results
A total of 73,774 patients were included. The data showed increases in age, revised Baux score, and Charlson Comorbidity Index during the study period, but it was also accompanied by a continuing decrease in burn incidence and a significant shortening of the length of hospital stay. The average in-hospital mortality was 17.5/1000 in 2003 and 12.2/1000 in 2013 but did not showed significant change. Male gender, older age, higher Charlson Comorbidity Index, presence of inhalation injury, large total burn surface area (TBSA), and higher revised Baux score were significant predictors of mortality.
Conclusion
Population-based burn epidemiology data demonstrated ongoing improvement in hospital care during the past decade. Male gender, older age, higher Charlson Comorbidity Index, presence of inhalation injury, large TBSA, and higher revised Baux score were significant predictors of mortality.}
}
@article{LI2018133,
title = {Toward a blockchain cloud manufacturing system as a peer to peer distributed network platform},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {54},
pages = {133-144},
year = {2018},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2018.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S073658451830022X},
author = {Zhi Li and Ali Vatankhah Barenji and George Q. Huang},
keywords = {Blockchain, Cloud manufacturing, Peer to peer network, Security and scalability},
abstract = {New emerging manufacturing paradigms such as cloud manufacturing, IoT enabled manufacturing and service-oriented manufacturing, have brought many advantages to the manufacturing industry and metamorphosis the industrial IT infrastructure. However, all existing paradigms still suffer from the main problem related to centralized industrial network and third part trust operation. In a nutshell, centralized networking has had issues with flexibility, efficiency, availability, and security. Therefore, the main aim of this paper is to present a distributed peer to peer network architecture that improves the security and scalability of the CMfg. The proposed architecture was developed based on blockchain technology, this facilitated the development of a distributed peer to peer network with high security, scalability and a well-structured cloud system. The proposed architecture which was named as the “BCmfg” is made up of five layers namely; resource layer, perception layer, manufacturing layer, infrastructure layer and application layer. In this paper, the concept of its architecture, secure data sharing, and typical characteristic are discussed and investigated as well as the key technologies required for the implementation of this proposed architecture is explained based on demonstrative case study. The proposed architecture is explained based on a case study which contains five service providers and 15 end users with considering 32 OnCloud services. For evaluation purpose, the qualitative and quantitative methods are utilized and the results show that the proposed methodology can bring more advantages to CMfg than the security and scalability.}
}
@article{HARRIS201882,
title = {Critical Care Health Informatics Collaborative (CCHIC): Data, tools and methods for reproducible research: A multi-centre UK intensive care database},
journal = {International Journal of Medical Informatics},
volume = {112},
pages = {82-89},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618300078},
author = {Steve Harris and Sinan Shi and David Brealey and Niall S. MacCallum and Spiros Denaxas and David Perez-Suarez and Ari Ercole and Peter Watkinson and Andrew Jones and Simon Ashworth and Richard Beale and Duncan Young and Stephen Brett and Mervyn Singer},
keywords = {Electronic health records, Database, Clinical decision support, Critical care, Reproducibility},
abstract = {Objective
To build and curate a linkable multi-centre database of high resolution longitudinal electronic health records (EHR) from adult Intensive Care Units (ICU). To develop a set of open-source tools to make these data ‘research ready’ while protecting patient’s privacy with a particular focus on anonymisation.
Materials and methods
We developed a scalable EHR processing pipeline for extracting, linking, normalising and curating and anonymising EHR data. Patient and public involvement was sought from the outset, and approval to hold these data was granted by the NHS Health Research Authority’s Confidentiality Advisory Group (CAG). The data are held in a certified Data Safe Haven. We followed sustainable software development principles throughout, and defined and populated a common data model that links to other clinical areas.
Results
Longitudinal EHR data were loaded into the CCHIC database from eleven adult ICUs at 5 UK teaching hospitals. From January 2014 to January 2017, this amounted to 21,930 and admissions (18,074 unique patients). Typical admissions have 70 data-items pertaining to admission and discharge, and a median of 1030 (IQR 481–2335) time-varying measures. Training datasets were made available through virtual machine images emulating the data processing environment. An open source R package, cleanEHR, was developed and released that transforms the data into a square table readily analysable by most statistical packages. A simple language agnostic configuration file will allow the user to select and clean variables, and impute missing data. An audit trail makes clear the provenance of the data at all times.
Discussion
Making health care data available for research is problematic. CCHIC is a unique multi-centre longitudinal and linkable resource that prioritises patient privacy through the highest standards of data security, but also provides tools to clean, organise, and anonymise the data. We believe the development of such tools are essential if we are to meet the twin requirements of respecting patient privacy and working for patient benefit.
Conclusion
The CCHIC database is now in use by health care researchers from academia and industry. The ‘research ready' suite of data preparation tools have facilitated access, and linkage to national databases of secondary care is underway.}
}
@article{MIRAGLIOTTA201819,
title = {Data driven management in Industry 4.0: a method to measure Data Productivity},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {19-24},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.228},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318313521},
author = {Giovanni Miragliotta and Andrea Sianesi and Elisa Convertini and Rossella Distante},
keywords = {Data productivity, Performance measurement, data driven decision making, Industry 4.0, Information Management},
abstract = {In the early 1900s, together with the birth of mass production, modern managerial approaches were conceived, under the motto “you can’t manage what you don’t measure”. Since then, operations managers throughout the world had been getting used to measure the productivity of materials, machines and workers to control and improve their own businesses. Nowadays, in the Industry 4.0 era, the emphasis is shifting toward data, under the new motto “data is the new oil”. Despite many managers pledging allegiance to the principles of data driven decision making, still no comprehensive approach exists to measure how good a company is at exploiting the potential of its own information assets; in other words, no “data productivity” measure exists. In this paper, we present a first method to define and measure data productivity. Relying on a comprehensive literature review, and inspired by the traditional OEE framework, this new method brings some innovative perspectives. First, data productivity is broken into data availability, quality and performance of the decision-making process using those data. Second, it includes both technical and organizational factors, helping companies to evaluate their current level of productivity, and actions to improve it. The model has been tested through three cases studies and it results as effectively implementable. The results obtained from its application reflect the expectations of companies’ managers accelerating the cultural shift needed to fully express the potential of Industry 4.0.}
}
@article{MONTORI2018111,
title = {The Curse of Sensing: Survey of techniques and challenges to cope with sparse and dense data in mobile crowd sensing for Internet of Things},
journal = {Pervasive and Mobile Computing},
volume = {49},
pages = {111-125},
year = {2018},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S1574119217306417},
author = {Federico Montori and Prem Prakash Jayaraman and Ali Yavari and Alireza Hassani and Dimitrios Georgakopoulos},
keywords = {Internet of Things (IoT), Mobile crowdsensing (MCS), Context awareness, The curse of sensing},
abstract = {In this paper we present a survey on mobile crowdsensing (MCS) techniques that have been developed to address the Curse of Sensing problem i.e. propensity of MCS applications to generate sparse or dense data that can lead to significant gaps in the extracted knowledge. In order to do so, we identify features, based on the terminologies used in the literature, in order to develop a clear classifications among MCS and crowdsourcing applications and methods. Subsequently, we propose a taxonomy outlining both factors and objectives that need to be considered in designing MCS systems and have a direct impact on MCS applications’ tendency to fall into the Curse of Sensing. We then evaluate the majority of the research proposed in the field of MCS and addressing the Curse of Sensing problem with reference to the proposed taxonomy. Finally, we highlight the existing gaps in the literature and possible directions for future research.}
}
@article{MEZNI2018139,
title = {The uncertain cloud: State of the art and research challenges},
journal = {International Journal of Approximate Reasoning},
volume = {103},
pages = {139-151},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18300574},
author = {Haithem Mezni and Sabeur Aridhi and Allel Hadjali},
keywords = {Cloud computing, Uncertainty, Uncertain cloud services, Uncertainty models},
abstract = {During the last decade, cloud computing became a natural choice to host and provide various computing resources as on-demand services. The correct characterization and management of cloud environment objects (clouds, data centers, providers, services, data, users, etc.) is the first step towards effective provisioning and integration of cloud services. However, cloud computing environment is often subject to uncertainty. This could be attributed to the incompleteness and imprecision of cloud available information, as well as the highly changing conditions. The purpose of this survey is to study, criticize and classify the already existing works that deal with uncertainty in the cloud. We present a taxonomy on the uncertainty in the cloud and we study how such concept was tackled by researchers in cloud environments. Finally, we identify the challenges and the requirements to deal with uncertain data in the cloud, as well as the future directions.}
}
@article{MESTRE20171,
title = {An efficient spark-based adaptive windowing for entity matching},
journal = {Journal of Systems and Software},
volume = {128},
pages = {1-10},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300559},
author = {Demetrio Gomes Mestre and Carlos Eduardo Santos Pires and Dimas Cassimiro Nascimento and Andreza Raquel Monteiro {de Queiroz} and Veruska Borges Santos and Tiago Brasileiro Araujo},
keywords = {Adaptive windowing, Entity matching, Load balancing, Spark},
abstract = {Entity Matching (EM), i.e., the task of identifying records that refer to the same entity, is a fundamental problem in every information integration and data cleansing system, e.g., to find similar product descriptions in databases. The EM task is known to be challenging when the datasets involved in the matching process have a high volume due to its pair-wise nature. For this reason, studies about challenges and possible solutions of how EM can benefit from modern parallel computing programming models, such as Apache Spark (Spark), have become an important demand nowadays (Christen, 2012a; Kolb et al., 2012b). The effectiveness and scalability of Spark-based implementations for EM depend on how well the workload distribution is balanced among all workers. In this article, we investigate how Spark can be used to perform efficiently (load balanced) parallel EM using a variation of the Sorted Neighborhood Method (SNM) that uses a varying (adaptive) window size. We propose Spark Duplicate Count Strategy (S-DCS++), a Spark-based approach for adaptive SNM, aiming to increase even more the performance of this method. The evaluation results, based on real-world datasets and cluster infrastructure, show that our approach increases the performance of parallel DCS++ regarding the EM execution time.}
}
@article{SACHDEVA2017410,
title = {Evolving large scale healthcare applications using open standards},
journal = {Health Policy and Technology},
volume = {6},
number = {4},
pages = {410-425},
year = {2017},
issn = {2211-8837},
doi = {https://doi.org/10.1016/j.hlpt.2017.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211883717300679},
author = {Shelly Sachdeva and Shivani Batra and Subhash Bhalla},
keywords = {Electronic Health Records (EHRs), Indian languages, User interface, Healthcare Application, Large Scale Application, open Standards},
abstract = {Electronic Health Records (EHRs) are becoming more prevalent in health care. Worldwide exchange of healthcare data demands adherence to semantic interoperable standards to overcome the language and platform barriers. Various healthcare organizations in developing countries such as, India adopt their own independent information systems without adhering to standard guidelines. Thus, this tends to sacrifice interoperability. This affects permanent persistence of longitudinal health records for future reference and research purpose. Current research implements a standard based clinical application to be used for healthcare domain in India. The study has been done for enhancing the data quality through standardization. It aims at providing a generic permanent persistence to track life-long interoperable health records of patients. This is the first effort for exploring its adoption for various regional languages in India. The user interfaces have been generated for various Indian languages for testing on a sample set of archetypes. The clinical application deployed in ‘Hindi’ language can be easily deployed for other people in ‘Tamil’ language, while maintaining semantic interoperability. The persistence will also be maintained, with the same meaning (of data) for both the regions. Implementing these standard based healthcare applications helps in reducing the costs while enhancing patient care. Thus, this study aims to build a standard based, and platform independent healthcare application to provide support for interoperability, usability and generic persistence.}
}
@article{WANG2018150,
title = {Semantic line framework-based indoor building modeling using backpacked laser scanning point cloud},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {143},
pages = {150-166},
year = {2018},
note = {ISPRS Journal of Photogrammetry and Remote Sensing Theme Issue “Point Cloud Processing”},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S092427161830090X},
author = {Cheng Wang and Shiwei Hou and Chenglu Wen and Zheng Gong and Qing Li and Xiaotian Sun and Jonathan Li},
keywords = {Point clouds, Indoor modeling, Mobile laser scanning, Line framework extraction, Semantic labeling},
abstract = {Indoor building models are essential in many indoor applications. These models are composed of the primitives of the buildings, such as the ceilings, floors, walls, windows, and doors, but not the movable objects in the indoor spaces, such as furniture. This paper presents, for indoor environments, a novel semantic line framework-based modeling building method using backpacked laser scanning point cloud data. The proposed method first semantically labels the raw point clouds into the walls, ceiling, floor, and other objects. Then line structures are extracted from the labeled points to achieve an initial description of the building line framework. To optimize the detected line structures caused by furniture occlusion, a conditional Generative Adversarial Nets (cGAN) deep learning model is constructed. The line framework optimization model includes structure completion, extrusion removal, and regularization. The result of optimization is also derived from a quality evaluation of the point cloud. Thus, the data collection and building model representation become a united task-driven loop. The proposed method eventually outputs a semantic line framework model and provides a layout for the interior of the building. Experiments show that the proposed method effectively extracts the line framework from different indoor scenes.}
}
@article{CHEN201843,
title = {Two-stage aggregation paradigm for HFLTS possibility distributions: A hierarchical clustering perspective},
journal = {Expert Systems with Applications},
volume = {104},
pages = {43-66},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418301532},
author = {Zhen-Song Chen and Luis Martínez and Kwai-Sang Chin and Kwok-Leung Tsui},
keywords = {Hesitant fuzzy linguistic term set, Hesitant 2-tuple linguistic term set, Possibility distribution, Hierarchical clustering},
abstract = {The integration of possibility distribution into hesitant fuzzy linguistic term set (HFLTS) adds an extra dimension to individual opinion approximation process and significantly leads to enhanced data quality and reliability. However, aggregation of HFLTS possibility distributions involves merely associated possibilities of linguistic terms without taking into account all possible combinations of individual linguistic opinions. Therefore, computing with HFLTS possibility distributions in such a way has a high possibility of distorting final decisions due to loss of information. The introduction of hesitant 2-tuple linguistic term set (H2TLTS), which technically includes the HFLTS as a special case, offers us a different point of view in consolidating the aggregation process of HFLTSs. Due to the resemblance with H2TLTS, the alternative explantation of HFLTS, i.e., possibility distribution, can be analogously adapted to the theory of H2TLTS. By means of a conceptually simple recasting of HFLTS possibility distribution into a unified framework for H2TLTS possibility distribution with the development of possibilistic 2-tuple linguistic pair (P2TLP) concept, we develop a novel two-stage aggregation paradigm for HFLTS possibility distributions. At the first stage, the initial aggregation takes all possible combinations of P2TLPs in separate HFLTS possibility distributions together to generate an aggregated set of P2TLPs. Building on that, the subsequent stage proposes a similarity measure-based agglomerative hierarchical clustering (SM-AggHC) algorithm to reduce the cardinality of the aggregate set under consideration. The centroid approach combined with the normalization process finally guarantees the aggregation outcomes to be operated as H2TLTS possibility distributions.}
}
@article{JESSE2018486,
title = {Organizational Evolution - How Digital Disruption Enforces Organizational Agility},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {486-491},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.310},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318329835},
author = {Norbert Jesse},
keywords = {Disruption, Data Science, Organizational Agility, Chief Data Officer},
abstract = {The success of almost any business depends on the promotion of digital excellence. Disruptive technologies like Hadoop, IoT, Blockchain or DevOps pose completely new challenges for business development. While there is no doubt about the need for keeping pace with the technical progress it is blurry how much this affects leadership and organizational agility. At least in Germany, many analysts record a gap between the engineering innovativeness and an inadequate digital leadership. It is evident, that digitization leads to new occupational profiles and a specific stress on the organizational fabric. Many companies experiment with start-up culture, entrepreneurial responsibility and more flexible team patterns, but a consistent approach for manager is still missing. In this preliminary discussion we address three dimensions to increase a company’s agility in times of digital Darwinism: structural flexibility, software-driven occupational profiles and requirements for leadership.}
}
@article{MEISSNER201881,
title = {Digitalization as a catalyst for lean production: A learning factory approach for digital shop floor management},
journal = {Procedia Manufacturing},
volume = {23},
pages = {81-86},
year = {2018},
note = {“Advanced Engineering Education & Training for Manufacturing Innovation”8th CIRP Sponsored Conference on Learning Factories (CLF 2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.03.165},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918304694},
author = {Alyssa Meissner and Marvin Müller and Alexander Hermann and Joachim Metternich},
keywords = {digital shop floor management, effects of digitalization, literature review, future research avenues},
abstract = {Digitalization and Industry 4.0 have a strong impact on today’s production environment. The established methods of lean production are also affected and can be further improved through new technologies. The following paper addresses how and whether digitalization enhances shop floor management (SFM) as a key element of the philosophy of lean production. Based on the Darmstadt shop floor management model, it will be discussed where digitalization is expected to be value-adding or bearing risks in which elements of SFM and how the transition to a digital system opens up new possibilities. The potential of digital SFM systems are evaluated through a literature review and interviews with experts. Based on the results a digital SFM prototype is evaluated and further developed and applied to the learning factory Center for industrial Productivity (CiP) and its underlying process control system. The versatile environment of the learning factory is consequently being used for evaluation of digital SFM in different working situations like manual labor or machining.}
}
@article{HUANG2017698,
title = {Analysis of the grain loss in harvest based on logistic regression},
journal = {Procedia Computer Science},
volume = {122},
pages = {698-705},
year = {2017},
note = {5th International Conference on Information Technology and Quantitative Management, ITQM 2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.426},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917326741},
author = {Tingkai Huang and Bingchan Li and Dongqin Shen and Jie Cao and Bo Mao},
keywords = {Logistic regression, stochastic gradient descent algorithm, grain harvest step loss},
abstract = {In this paper, the grain loss assessment was studied based on logistic regression, and 5400 samples of 31 provinces in our country in the year 2012-2014 were selected, and the 7 typical provinces among them were respectively tested. Using the logistic regression model to predict the loss rate of grain harvesting step, the prediction result is 86.25%. The stochastic gradient descent algorithm is used to optimize the parameters of the model, when the learning_rate is 0.1, the prediction results of grain harvest losses of up to 92.53%, which further improves the prediction accuracy of grain harvest step loss.}
}
@article{XU2018423,
title = {A 3D convolutional neural network method for land cover classification using LiDAR and multi-temporal Landsat imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {144},
pages = {423-434},
year = {2018},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0924271618302223},
author = {Zewei Xu and Kaiyu Guan and Nathan Casler and Bin Peng and Shaowen Wang},
keywords = {Big data analysis, Convolutional neural network, Land cover classification, LiDAR, Multi-temporal Landsat imagery},
abstract = {Terrestrial landscape has complex three-dimensional (3D) features that are difficult to extract using traditional methods based on 2D representations. These methods often relegate such features to raster or metric-based (two-dimensional) representations based on Digital Surface Models (DSM) or Digital Elevation Models (DEM), and thus are not suitable for resolving morphological and intensity features for fine-scale land cover mapping. Small-footprint LiDAR provides an ideal way for capturing these 3D features. This research develops a novel method of integrating airborne LiDAR derived features and multi-temporal Landsat images to classify land cover types. We tested our approach in Williamson County, Illinois, which has diverse and mixed landscape features. Specifically, our method applied a 3D convolutional neural network (CNN) approach to extract features from LiDAR point clouds by (1) creating an occupancy grid, an intensity grid at 1-meter resolution, and then (2) normalizing and incorporating data into the 3D CNN. The extracted features (e.g., morphological and intensity features) from the 3D CNN were finally combined with multi-temporal spectral data to enhance the performance of land cover classification based on a Support Vector Machine classifier. Visual interpretation from both hyper-resolution photos and point clouds was used for training and preparation of testing data. The classification results show that our method outperforms a traditional method by 2.65% (from 81.52% to 84.17%) when solely using LiDAR and 2.19% (from 90.20% to 92.57%) when combining all available imageries. We demonstrate that our method can effectively extract LiDAR features and improve fine-scale land cover mapping through fusion of complementary types of remote sensing data.}
}
@article{LUTHRA2018168,
title = {Evaluating challenges to Industry 4.0 initiatives for supply chain sustainability in emerging economies},
journal = {Process Safety and Environmental Protection},
volume = {117},
pages = {168-179},
year = {2018},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2018.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0957582018301320},
author = {Sunil Luthra and Sachin Kumar Mangla},
keywords = {Industry 4.0 challenges, Supply chain management, Sustainability, Analytical Hierarchy Process (AHP), Explanatory Factor Analysis (EFA), Emerging economies},
abstract = {Industry 4.0 initiatives can influence whole business system via transforming the means the products are designed, produced, delivered and discarded. Industry 4.0 is relatively novel to developing nations, especially in India and needs a clear definition for proper understanding and practice in business. This paper aims to recognize key challenges to Industry 4.0 initiatives and analyze the identified key challenges to prioritize them for effective Industry 4.0 concepts for supply chain sustainability in emerging economies by taking Indian manufacturing industry perspective. Industry 4.0 initiatives can help industries to incorporate environmental protection and control initiatives as well as process safety measures in supply chains towards sustainable supply chains. However, adoption of Industry 4.0 initiatives is not so easy due to existence of many challenges. Therefore, the present research identifies 18 key challenges to Industry 4.0 initiatives for developing supply chain sustainability using an extensive literature review. These challenges were analyzed through 96 responses received from Indian manufacturing sector using a questionnaire based survey. Explanatory Factor Analysis results classified identified challenges into four key dimensions of challenges. Analytical Hierarchy Process further ranks the identified dimensions of challenges and related challenges. Findings of the study revealed that Organizational challenges holds the highest importance followed by Technological challenges, Strategic challenges, and Legal and ethical issues. This work is very useful for practitioners, policy makers, regulatory bodies and managers to develop an in-depth understanding of Industry 4.0 initiatives and eradicate the potential challenges in adopting Industry 4.0 initiatives for supply chain sustainability.}
}
@incollection{2018445,
title = {Index},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {445-452},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.09988-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012815609409988X}
}
@article{MA201860,
title = {A new aspect on P2P online lending default prediction using meta-level phone usage data in China},
journal = {Decision Support Systems},
volume = {111},
pages = {60-71},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300836},
author = {Lin Ma and Xi Zhao and Zhili Zhou and Yuanyuan Liu},
keywords = {P2P online lending, Meta-level phone usage data, Default prediction, AdaBoost},
abstract = {P2P online lending platforms provide services where individuals lend money to others without the involvement of traditional financial institutions. Due to its convenience, the platforms have gained in popularity. However, these platforms may suffer a significant loss if they cannot make good loan decisions based on default prediction results. In this paper, we aim to support the loan decision on P2P platforms based on meta-level phone usage data when the information asymmetry exists for mass borrowers. We extract variables from phone usage data, and use an empirical study to analyze the relationship between these variables and loan default. Then a default prediction method is conducted for P2P lending based on the AdaBoost algorithm. The data used in this study are from the generalized used mobile phones, which make the method applicable to a wide range of users. The empirical study shows that phone usage patterns, including telecommunication patterns, mobility patterns, and App usage patterns contain predictive capability of loan default. The experiments on prediction method demonstrate satisfying performance, which suggests the proposed method has favorable potential being implemented in real-world P2P lending platforms.}
}
@article{GAN20172498,
title = {Supervisory Ability Development and Dual Control Actions of Water Resources},
journal = {Energy Procedia},
volume = {142},
pages = {2498-2502},
year = {2017},
note = {Proceedings of the 9th International Conference on Applied Energy},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.12.189},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217359234},
author = {Zhiguo Gan and Yunzhong Jiang},
keywords = {Dual Control, Supervisory ability, Water Resource, China},
abstract = {Water is the foundation of survival, the source of civilization and the base of ecology. Water security is significant to the stability and security of a country. In order to make sustainable use of Water Resources, China government aim at " "implementation of water resources consumption and intensity of double control action". Strengthening Water Resources monitoring capacity, more refined and scientific water management, is an important measure to achieve this goal.}
}
@article{CHOI2023148,
title = {Measuring taxi ridesharing effects and its spatiotemporal pattern in Seoul, Korea},
journal = {Travel Behaviour and Society},
volume = {30},
pages = {148-162},
year = {2023},
issn = {2214-367X},
doi = {https://doi.org/10.1016/j.tbs.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214367X22001016},
author = {Junyong Choi and Youngchul Kim and Minchul Kwak and Minju Park and David Lee},
keywords = {Taxi ridesharing, Spatiotemporal pattern analysis, Mobility, Taxi data, Travel pattern},
abstract = {Most studies on dynamic ridesharing, which allows any driver to ferry separate passengers with overlapping routes in a single trip, have focused on developing efficient algorithms. While dynamic ridesharing demands extensive computing infrastructure, this paper suggests a simple taxi ridesharing approach that allows only passengers from certain taxi hotspots to share a ride with another who has a similar destination and measures the effects, spatiotemporal patterns and its benefits. Compared to dynamic ridesharing, the proposed method does not require dedicated driver fleets, networked communication system, or monopoly of all passenger information. We identify taxi pickup hotspots and analyze the spatiotemporal patterns of the simple ridesharing approach. The results show that 48 % of rides from hotspots could be shared, reducing the overall vehicle-km traveled by 1.2 km for each shared ride. We also find that spatiotemporal patterns of the ridesharing could represent urban characteristics. For example, places with high ridesharing potential and low saved-trip distances could imply low public transportation accessibility while areas with high shareability during working hours on both weekdays and weekends could represent public transportation hubs. The proposed method is expected to be useful to identify taxi stands that have high ridesharing opportunities. Policy makers can use our approach to support simple ridesharing scheme. Moreover, with the characterized taxi stands, such as longer saved-trip distance and nightlife peaks, the proposed method could be used as decision support tools for temporary allowed ridesharing. In addition, spatiotemporal patterns of the taxi stands would be used in designing public transportation systems.}
}
@incollection{YE201842,
title = {1.05 - Open Data and Open Source GIS},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {42-49},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09592-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489095920},
author = {Xinyue Ye},
keywords = {Open data, Open GIS, Open source},
abstract = {The multiple dimensions and scales of emerging open data pose numerous challenges for the application and evaluation of public policies. At the same time, domain researchers have been relatively slow to adopt and implement new spatiotemporally explicit data analysis methods due to the availability of suitable data and the lack of extensible software packages, which becomes a major impediment to the promotion of spatiotemporal thinking and collaboration. In this regard, more attention to open data and open source geographic information system (GIS) is necessary. Free access to the data and source code allows the broader GIS and domain science communities to incorporate additional advances in theoretical perspectives and analytical methods, thus facilitating interdisciplinary collaboration of spatial science and education. A case study of comparative LISA time path is illustrated in the open source GIS context. Additionally, open source implementation of new methods can expedite comparative studies of geographical dynamics.}
}
@article{GEISSLER2018250,
title = {Clearing the fog on phosphate rock data – Uncertainties, fuzziness, and misunderstandings},
journal = {Science of The Total Environment},
volume = {642},
pages = {250-263},
year = {2018},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2018.05.381},
url = {https://www.sciencedirect.com/science/article/pii/S0048969718320382},
author = {Bernhard Geissler and Gerald Steiner and Michael C. Mew},
keywords = {Mineral economics, Resource management, Global ore grade distribution, Data science, Data analytics, Phosphates trade},
abstract = {Big Data, blockchains, and cloud computing have become ubiquitous in today's mass media and are universally known terms used in everyday speech. If we look behind these often misused buzzwords, we find at least one common element, namely data. Although we hardly use these terms in the “classic discipline” of mineral economics, we find various similarities. The case of phosphate data bears numerous challenges in multiple forms such as uncertainties, fuzziness, or misunderstandings. Often simulation models are used to support decision-making processes. For all these models, reliable and accurate sets of data are an essential premise. A significant number of data series relating to the phosphorus supply chain, including resource inventory or production, consumption, and trade data ranging from phosphate rock to intermediates like marketable concentrate to final phosphate fertilizers, is available. Data analysts and modelers must often choose from various sources, and they also depend on data access. Based on a transdisciplinary orientation, we aim to help colleagues in all fields by illustrating quantitative differences among the reported data, taking a somewhat engineering approach. We use common descriptive statistics to measure and causally explain discrepancies in global phosphate-rock production data issued by the US Geological Survey, the British Geological Survey, Austrian World Mining Data, the International Fertilizer Association, and CRU International over time, with a focus on the most recent years. Furthermore, we provide two snapshots of global-trade flows for phosphate-rock concentrate, in 2015 and 1985, and compare these to an approach using total-nutrient data. We find discrepancies of up to 30% in reported global production volume, whereby the major share could be assigned directly to China and Peru. Consequently, we call for a global, independent agency to collect and monitor phosphate data in order to reduce uncertainties or fuzziness and, thereby, ultimately support policy-making processes.}
}
@incollection{FRYMAN20171,
title = {Chapter 1 - Purpose, Scope and Audience},
editor = {Lowell Fryman and Gregory Lampshire and Dan Meers},
booktitle = {The Data and Analytics Playbook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-21},
year = {2017},
isbn = {978-0-12-802307-5},
doi = {https://doi.org/10.1016/B978-0-12-802307-5.00001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023075000010},
author = {Lowell Fryman and Gregory Lampshire and Dan Meers},
keywords = {Data analytics, Data governance, Data management, Playbook, Reference data},
abstract = {This book describes an approach to making data governance repeatable, reliable, and cost-effective in your organization. Our approach uses a Playbook, a play-by-play description of how to set expectations and perform data governance jobs right the first time. Before we dig deeper into the Playbook, we highlight the impact of data issues on organizations, industries, countries, and individuals. While many of the stories were taken from recent headlines and seem like once-in-a-million events, some of the stories describe recurring themes. These stories illustrate the type of data governance issues the Playbook was designed to address and reflect real problems that can ultimately cost an organization money or its reputation.}
}
@article{RAVINDRANATH2018P193,
title = {DEEP LEARNING IN AUTOMATED CLASSIFICATION OF ADVERSE EVENTS IN CLINICAL STUDIES OF ALZHEIMER’S DISEASE},
journal = {Alzheimer's & Dementia},
volume = {14},
number = {7, Supplement },
pages = {P193},
year = {2018},
issn = {1552-5260},
doi = {https://doi.org/10.1016/j.jalz.2018.06.2032},
url = {https://www.sciencedirect.com/science/article/pii/S1552526018322040},
author = {Pradeep Anand Ravindranath and Rema Raman and Tiffany W. Chow and Michael S. Rafii and Paul S. Aisen and Gustavo Jimenez-Maggiora}
}
@incollection{2018181,
title = {Index},
editor = {Ida Arlene Joiner},
booktitle = {Emerging Library Technologies},
publisher = {Chandos Publishing},
pages = {181-188},
year = {2018},
series = {Chandos Information Professional Series},
isbn = {978-0-08-102253-5},
doi = {https://doi.org/10.1016/B978-0-08-102253-5.00022-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022535000228}
}
@article{2017S1,
title = {Detailed Contents},
journal = {American Journal of Kidney Diseases},
volume = {69},
number = {6, Supplement 2},
pages = {S1-S8},
year = {2017},
note = {China Kidney Disease Network (CK-NET) 2014 Annual Data Report},
issn = {0272-6386},
doi = {https://doi.org/10.1053/j.ajkd.2016.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0272638616302931}
}
@article{QIU2018428,
title = {The burden of overall and cause-specific respiratory morbidity due to ambient air pollution in Sichuan Basin, China: A multi-city time-series analysis},
journal = {Environmental Research},
volume = {167},
pages = {428-436},
year = {2018},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2018.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0013935118304444},
author = {Hang Qiu and Haiyan Yu and Liya Wang and Xiaojuan Zhu and Mengdie Chen and Li Zhou and Ren Deng and Yanlong Zhang and Xiaorong Pu and Jingping Pan},
keywords = {Air pollution, Hospital admission, Respiratory diseases, Burden, Sichuan Basin},
abstract = {Few studies have investigated the respiratory morbidity burden due to ambient air pollution in China, especially in a multi-city setting. This study aimed to estimate the short-term effects of ambient air pollutants (PM10, PM2.5, NO2 and SO2) on hospital admissions (HAs) for overall and cause-specific respiratory diseases, as well as the associated burden in 17 cities of Sichuan Basin, China during 2015–2016. Firstly, city-specific effect estimates for each pollutant on respiratory HAs were obtained using generalized additive model with quasi-Poisson link, and then random- or fixed-effects meta-analysis was applied to pool the effect estimates at the regional level. Subgroup analyses by sex, age, season and region were also performed. A total of 757,712 respiratory HAs were collected from all the tertiary and secondary hospitals located in the 17 cities. Risks of HAs for overall and cause-specific respiratory diseases were elevated following increased PM10, PM2.5, NO2 and SO2 exposure. An increase of 10 μg/m3 in PM10 at lag01, PM2.5 at lag01, NO2 at lag0 and SO2 at lag02 was associated with a 0.43% (95% CI: 0.33%, 0.53%), 0.53% (95% CI: 0.39%, 0.68%), 2.36% (95% CI: 1.75%, 2.98%) and 2.54% (95% CI: 1.51%, 3.59%) increases in total respiratory HAs, respectively. Children (≤ 14 years) and elderly (≥ 65 years) appeared to be more vulnerable to the effects of ambient air pollutants. Comparing to the WHO's air quality guidelines, we estimated that 1.84% (95%CI: 1.42%, 2.25%), 1.73% (95%CI: 1.27%, 2.19%) and 0.34% (95%CI: 0.21%, 0.48%) of respiratory HAs were due to PM10, PM2.5 and SO2 exposure, respectively. This study suggests that air pollution might be an important trigger of respiratory admissions, and result in substantial burden of HAs for respiratory diseases in Sichuan Basin.}
}
@incollection{CALIFF2018411,
title = {Chapter 28 - Large Clinical Trials and Registries—Clinical Research Institutes},
editor = {John I. Gallin and Frederick P. Ognibene and Laura Lee Johnson},
booktitle = {Principles and Practice of Clinical Research (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
address = {Boston},
pages = {411-444},
year = {2018},
isbn = {978-0-12-849905-4},
doi = {https://doi.org/10.1016/B978-0-12-849905-4.00028-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128499054000289},
author = {Robert M. Califf},
keywords = {Clinical trial, Controlled clinical trial, Electronic health record, Evidence-based medicine, Medical record linkage, Pragmatic clinical trial, Randomized controlled trial, Research ethics, Statistical data analysis, Statistical data interpretation},
abstract = {The growing need for high-quality scientific evidence to inform medical practice and decision-making is placing increasing pressure on the clinical research enterprise. Although new diagnostic and therapeutic technologies offer dramatic opportunities to improve health outcomes, they continue to drive up health-care costs while resources for evaluating such technologies are becoming increasingly constrained. In this environment, researchers and clinicians must have a thorough grasp of the strengths and limitations of various research methods and be able to critically evaluate and apply not only traditional approaches, but also novel techniques now emerging in the era of “big data.” In this chapter, we will explore the history, background, and general concepts of clinical research. We will also examine trial design; ethical, legal, and organizational aspects of clinical research; application of statistical methods; and the integration of these concepts into practice. Finally, we will visit current controversies in clinical research and explore future possibilities.}
}
@article{SCHIMBINSCHI2017301,
title = {Topology-regularized universal vector autoregression for traffic forecasting in large urban areas},
journal = {Expert Systems with Applications},
volume = {82},
pages = {301-316},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417302518},
author = {Florin Schimbinschi and Luis Moreira-Matias and Vinh Xuan Nguyen and James Bailey},
keywords = {Topology regularized universal vector autoregression, Multivariate timeseries forecasting, Spatiotemporal autocorrelation, Traffic prediction, Big data, Structural risk minimization},
abstract = {Autonomous vehicles are soon to become ubiquitous in large urban areas, encompassing cities, suburbs and vast highway networks. In turn, this will bring new challenges to the existing traffic management expert systems. Concurrently, urban development is causing growth, thus changing the network structures. As such, a new generation of adaptive algorithms are needed, ones that learn in real-time, capture the multivariate nonlinear spatio-temporal dependencies and are easily adaptable to new data (e.g. weather or crowdsourced data) and changes in network structure, without having to retrain and/or redeploy the entire system. We propose learning Topology-Regularized Universal Vector Autoregression (TRU-VAR) and examplify deployment with of state-of-the-art function approximators. Our expert system produces reliable forecasts in large urban areas and is best described as scalable, versatile and accurate. By introducing constraints via a topology-designed adjacency matrix (TDAM), we simultaneously reduce computational complexity while improving accuracy by capturing the non-linear spatio-temporal dependencies between timeseries. The strength of our method also resides in its redundancy through modularity and adaptability via the TDAM, which can be altered even while the system is deployed. The large-scale network-wide empirical evaluations on two qualitatively and quantitatively different datasets show that our method scales well and can be trained efficiently with low generalization error. We also provide a broad review of the literature and illustrate the complex dependencies at intersections and discuss the issues of data broadcasted by road network sensors. The lowest prediction error was observed for TRU-VAR, which outperforms ARIMA in all cases and the equivalent univariate predictors in almost all cases for both datasets. We conclude that forecasting accuracy is heavily influenced by the TDAM, which should be tailored specifically for each dataset and network type. Further improvements are possible based on including additional data in the model, such as readings from different metrics.}
}
@article{COLANGELO2018191,
title = {Substitution and Complementation of Production Management Functions with Data Analytics},
journal = {Procedia CIRP},
volume = {72},
pages = {191-196},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.145},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303020},
author = {Eduardo Colangelo and Torsten Kröger and Thomas Bauernhansl},
keywords = {Data Analytics, Complexity, Production Planning, Control, PPC, MES, Industrie 4.0},
abstract = {Complexity in production systems is steadily growing– one of the drivers is the customer’s desire for personalization of products and services. Existing production management systems rely on deterministic functions. Given the diversity and varying influence of these functions, current methods reach their limits and may not meet future needs. We introduce new opportunities provided by replacing and complementing these functionalities with data analytics. With a focus on production management and data-driven analysis we offer a flexible method that extends existing Industrie 4.0 technologies for several application areas (e.g., logistics, inventory management).}
}
@article{CUOMO2017508,
title = {Analysis of a data-flow in a financial IoT system},
journal = {Procedia Computer Science},
volume = {113},
pages = {508-512},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.313},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917317234},
author = {Salvatore Cuomo and Vittorio {Di Somma} and Federica Sica},
keywords = {Internet of Things, Financial system, data-flow management},
abstract = {Data retrieving, analysis e management are usually known as complex task in financial contexts. In an Internet of Things (IoT) system data-flow processes represent the knowledge base used in mathematical models for credits and financial products. Several sources such as distributed database systems, portals and local information are generally used as input of inferring models. In this paper we describe an overview of software tools, methodologies and strategies in real data-flow system.}
}
@article{ABDELHAMEED201874,
title = {Privacy-preserving tabular data publishing: A comprehensive evaluation from web to cloud},
journal = {Computers & Security},
volume = {72},
pages = {74-95},
year = {2018},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167404817301840},
author = {Saad A. Abdelhameed and Sherin M. Moussa and Mohamed E. Khalifa},
keywords = {Data privacy, Privacy-preserving data publishing, Data anonymization, Data streams, Multiple sensitive attributes, Single sensitive attribute},
abstract = {The amount of data collected by various organizations about individuals is continuously increasing. This includes diverse data sources often for data of high dimensionality. Most of these data are stored in tabular format and can include sensitive content. Preserving data privacy is an essential task in order to allow such data to be published for different research and analysis purposes. In this context, Privacy-Preserving Tabular Data Publishing (PPTDP) has drawn considerable attention, where different approaches have been proposed to preserve the privacy of individuals' tabular data. Such data can include Single Sensitive Attributes (SSA) or Multiple Sensitive Attributes (MSA) or come from data streams. In this paper, we conduct a comprehensive study to analyze and evaluate the main different data anonymization approaches that have been introduced in PPTDP. The study investigates the three broad areas of research: SSA, MSA and data streams. A detailed criticism is presented to highlight the strengths and the weaknesses of each approach including their deployment in the cloud and Internet of Things (IoT) environments. A research gap analysis is discussed with a focus on capturing current state of the art in this field in order to highlight the future directions that can be considered.}
}
@article{FENSEL20181,
title = {Preface},
journal = {Procedia Computer Science},
volume = {137},
pages = {1-8},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316053},
author = {Anna Fensel and Victor de Boer and Tassilo Pellegrini and Elmar Kiesling and Bernhard Haslhofer and Laura Hollink and Alexander Schindler}
}
@article{BOKRANTZ2017154,
title = {Maintenance in digitalised manufacturing: Delphi-based scenarios for 2030},
journal = {International Journal of Production Economics},
volume = {191},
pages = {154-169},
year = {2017},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2017.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S092552731730172X},
author = {Jon Bokrantz and Anders Skoogh and Cecilia Berlin and Johan Stahre},
keywords = {Maintenance, Digitalised manufacturing, Industrie 4.0, Scenario planning, Delphi},
abstract = {Despite extensive research on future manufacturing and the forthcoming fourth industrial revolution (implying extensive digitalisation), there is a lack of understanding regarding the specific changes that can be expected for maintenance organisations. Therefore, developing scenarios for future maintenance is needed to define long-term strategies for the realisation of digitalised manufacturing. This empirical Delphi-based scenario planning study is the first within the maintenance realm, examining a total of 34 projections about potential changes to the internal and external environment of maintenance organisations, considering both hard (technological) and soft (social) dimensions. The paper describes a probable future of maintenance organisations in digitalised manufacturing in the year 2030, based on an extensive three-round Delphi survey with 25 maintenance experts at strategic level from the largest companies within the Swedish manufacturing industry. In particular, the study contributes with development of probable as well as wildcard scenarios for future maintenance. This includes e.g. advancement of data analytics, increased emphasis on education and training, novel principles for maintenance planning with a systems perspective, and stronger environmental legislation and standards. The scenarios may serve as direct input to strategic development in industrial maintenance organisations and are expected to substantially improve preparedness to the changes brought by digitalised manufacturing.}
}
@article{20171151,
title = {Global, regional, and national age-sex specific mortality for 264 causes of death, 1980–2016: a systematic analysis for the Global Burden of Disease Study 2016},
journal = {The Lancet},
volume = {390},
number = {10100},
pages = {1151-1210},
year = {2017},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(17)32152-9},
url = {https://www.sciencedirect.com/science/article/pii/S0140673617321529},
author = {Mohsen Naghavi and Amanuel Alemu Abajobir and Cristiana Abbafati and Kaja M Abbas and Foad Abd-Allah and Semaw Ferede Abera and Victor Aboyans and Olatunji Adetokunboh and Ashkan Afshin and Anurag Agrawal and Alireza Ahmadi and Muktar Beshir Ahmed and Amani Nidhal Aichour and Miloud Taki Eddine Aichour and Ibtihel Aichour and Sneha Aiyar and Fares Alahdab and Ziyad Al-Aly and Khurshid Alam and Noore Alam and Tahiya Alam and Kefyalew Addis Alene and Ayman Al-Eyadhy and Syed Danish Ali and Reza Alizadeh-Navaei and Juma M Alkaabi and Ala'a Alkerwi and François Alla and Peter Allebeck and Christine Allen and Rajaa Al-Raddadi and Ubai Alsharif and Khalid A Altirkawi and Nelson Alvis-Guzman and Azmeraw T Amare and Erfan Amini and Walid Ammar and Yaw Ampem Amoako and Nahla Anber and Hjalte H Andersen and Catalina Liliana Andrei and Sofia Androudi and Hossein Ansari and Carl Abelardo T Antonio and Palwasha Anwari and Johan Ärnlöv and Megha Arora and Al Artaman and Krishna Kumar Aryal and Hamid Asayesh and Solomon W Asgedom and Tesfay Mehari Atey and Leticia Avila-Burgos and Euripide Frinel G Avokpaho and Ashish Awasthi and Tesleem Kayode Babalola and Umar Bacha and Kalpana Balakrishnan and Aleksandra Barac and Miguel A Barboza and Suzanne L Barker-Collo and Simon Barquera and Lars Barregard and Lope H Barrero and Bernhard T Baune and Neeraj Bedi and Ettore Beghi and Yannick Béjot and Bayu Begashaw Bekele and Michelle L Bell and James R Bennett and Isabela M Bensenor and Adugnaw Berhane and Eduardo Bernabé and Balem Demtsu Betsu and Mircea Beuran and Samir Bhatt and Sibhatu Biadgilign and Kelly Bienhoff and Boris Bikbov and Donal Bisanzio and Rupert R A Bourne and Nicholas J K Breitborde and Lemma Negesa Bulto Bulto and Blair R Bumgarner and Zahid A Butt and Lucero Cahuana-Hurtado and Ewan Cameron and Julio Cesar Campuzano and Josip Car and Rosario Cárdenas and Juan Jesus Carrero and Austin Carter and Daniel C Casey and Carlos A Castañeda-Orjuela and Ferrán Catalá-López and Fiona J Charlson and Chioma Ezinne Chibueze and Odgerel Chimed-Ochir and Vesper Hichilombwe Chisumpa and Abdulaal A Chitheer and Devasahayam Jesudas Christopher and Liliana G Ciobanu and Massimo Cirillo and Aaron J Cohen and Danny Colombara and Cyrus Cooper and Benjamin C Cowie and Michael H Criqui and Lalit Dandona and Rakhi Dandona and Paul I Dargan and José {das Neves} and Dragos V Davitoiu and Kairat Davletov and Barbora {de Courten} and Barthelemy Kuate Defo and Louisa Degenhardt and Selina Deiparine and Kebede Deribe and Amare Deribew and Subhojit Dey and Daniel Dicker and Eric L Ding and Shirin Djalalinia and Huyen Phuc Do and David Teye Doku and Dirk Douwes-Schultz and Tim R Driscoll and Manisha Dubey and Bruce Bartholow Duncan and Michelle Echko and Ziad Ziad El-Khatib and Christian Lycke Ellingsen and Ahmadali Enayati and Sergey Petrovich Ermakov and Holly E Erskine and Sharareh Eskandarieh and Alireza Esteghamati and Kara Estep and Carla Sofia e Sa Farinha and André Faro and Farshad Farzadfar and Valery L Feigin and Seyed-Mohammad Fereshtehnejad and João C Fernandes and Alize J Ferrari and Tesfaye Regassa Feyissa and Irina Filip and Samuel Finegold and Florian Fischer and Christina Fitzmaurice and Abraham D Flaxman and Nataliya Foigt and Tahvi Frank and Maya Fraser and Nancy Fullman and Thomas Fürst and Joao M Furtado and Emmanuela Gakidou and Alberto L Garcia-Basteiro and Teshome Gebre and Gebremedhin Berhe Gebregergs and Tsegaye Tewelde Gebrehiwot and Delelegn Yilma Gebremichael and Johanna M Geleijnse and Ricard Genova-Maleras and Hailay Abrha Gesesew and Peter W Gething and Richard F Gillum and Ababi Zergaw Giref and Maurice Giroud and Giorgia Giussani and William W Godwin and Audra L Gold and Ellen M Goldberg and Philimon N Gona and Sameer Vali Gopalani and Hebe N Gouda and Alessandra Carvalho Goulart and Max Griswold and Rajeev Gupta and Tanush Gupta and Vipin Gupta and Parkash C Gupta and Juanita A Haagsma and Nima Hafezi-Nejad and Alemayehu Desalegne Hailu and Gessessew Bugssa Hailu and Randah Ribhi Hamadeh and Mitiku Teshome Hambisa and Samer Hamidi and Mouhanad Hammami and Jamie Hancock and Alexis J Handal and Graeme J Hankey and Yuantao Hao and Hilda L Harb and Habtamu Abera Hareri and Mohammad Sadegh Hassanvand and Rasmus Havmoeller and Simon I Hay and Fei He and Mohammad T Hedayati and Nathaniel J Henry and Ileana Beatriz Heredia-Pi and Claudiu Herteliu and Hans W Hoek and Masako Horino and Nobuyuki Horita and H Dean Hosgood and Sorin Hostiuc and Peter J Hotez and Damian G Hoy and Chantal Huynh and Kim Moesgaard Iburg and Chad Ikeda and Bogdan Vasile Ileanu and Asnake Ararsa Irenso and Caleb Mackay Salpeter Irvine and Sheikh Mohammed Shariful Islam and Kathryn H Jacobsen and Nader Jahanmehr and Mihajlo B Jakovljevic and Mehdi Javanbakht and Sudha P Jayaraman and Panniyammakal Jeemon and Vivekanand Jha and Denny John and Catherine O Johnson and Sarah Charlotte Johnson and Jost B Jonas and Mikk Jürisson and Zubair Kabir and Rajendra Kadel and Amaha Kahsay and Ritul Kamal and André Karch and Seyed M Karimi and Chante Karimkhani and Amir Kasaeian and Nigussie Assefa Kassaw and Nicholas J Kassebaum and Srinivasa Vittal Katikireddi and Norito Kawakami and Peter Njenga Keiyoro and Laura Kemmer and Chandrasekharan Nair Kesavachandran and Yousef Saleh Khader and Ejaz Ahmad Khan and Young-Ho Khang and Abdullah Tawfih Abdullah Khoja and Mohammad Hossein Khosravi and Ardeshir Khosravi and Jagdish Khubchandani and Aliasghar Ahmad Kiadaliri and Christian Kieling and Daniel Kievlan and Yun Jin Kim and Daniel Kim and Ruth W Kimokoti and Yohannes Kinfu and Niranjan Kissoon and Mika Kivimaki and Ann Kristin Knudsen and Jacek A Kopec and Soewarta Kosen and Parvaiz A Koul and Ai Koyanagi and Xie Rachel Kulikoff and G Anil Kumar and Pushpendra Kumar and Michael Kutz and Hmwe H Kyu and Dharmesh Kumar Lal and Ratilal Lalloo and Tea Lallukka Nkurunziza Lambert and Qing Lan and Van C Lansingh and Anders Larsson and Paul H Lee and James Leigh and Janni Leung and Miriam Levi and Yongmei Li and Darya {Li Kappe} and Xiaofeng Liang and Misgan Legesse Liben and Stephen S Lim and Patrick Y Liu and Angela Liu and Yang Liu and Rakesh Lodha and Giancarlo Logroscino and Stefan Lorkowski and Paulo A Lotufo and Rafael Lozano and Timothy C D Lucas and Stefan Ma and Erlyn Rachelle King Macarayan and Emilie R Maddison and Mohammed {Magdy Abd El Razek} and Marek Majdan and Reza Majdzadeh and Azeem Majeed and Reza Malekzadeh and Rajesh Malhotra and Deborah Carvalho Malta and Helena Manguerra and Tsegahun Manyazewal and Chabila C Mapoma and Laurie B Marczak and Desalegn Markos and Jose Martinez-Raga and Francisco Rogerlândio Martins-Melo and Ira Martopullo and Colm McAlinden and Madeline McGaughey and John J McGrath and Suresh Mehata and Toni Meier and Kidanu Gebremariam Meles and Peter Memiah and Ziad A Memish and Melkamu Merid Mengesha and Desalegn Tadese Mengistu and Bereket Gebremichael Menota and George A Mensah and Tuomo J Meretoja and Atte Meretoja and Anoushka Millear and Ted R Miller and Shawn Minnig and Mojde Mirarefin and Erkin M Mirrakhimov and Awoke Misganaw and Shiva Raj Mishra and Ibrahim Abdelmageem Mohamed and Karzan Abdulmuhsin Mohammad and Alireza Mohammadi and Shafiu Mohammed and Ali H Mokdad and Glen Liddell D Mola and Sarah K Mollenkopf and Mariam Molokhia and Lorenzo Monasta and Julio C Montañez and Marcella Montico and Meghan D Mooney and Maziar Moradi-Lakeh and Paula Moraga and Lidia Morawska and Chloe Morozoff and Shane D Morrison and Cliff Mountjoy-Venning and Kalayu Birhane Mruts and Kate Muller and Gudlavalleti Venkata Satyanarayana Murthy and Kamarul Imran Musa and Jean B Nachega and Aliya Naheed and Luigi Naldi and Vinay Nangia and Bruno Ramos Nascimento and Jamal T Nasher and Gopalakrishnan Natarajan and Ionut Negoi and Josephine Wanjiku Ngunjiri and Cuong Tat Nguyen and Quyen Le Nguyen and Trang Huyen Nguyen and Grant Nguyen and Minh Nguyen and Emma Nichols and Dina Nur Anggraini Ningrum and Vuong Minh Nong and Jean Jacques N Noubiap and Felix Akpojene Ogbo and In-Hwan Oh and Anselm Okoro and Andrew Toyin Olagunju and Helen E Olsen and Bolajoko Olubukunola Olusanya and Jacob Olusegun Olusanya and Kanyin Ong and John Nelson Opio and Eyal Oren and Alberto Ortiz and Majdi Osman and Erika Ota and Mahesh PA and Rosana E Pacella and Smita Pakhale and Adrian Pana and Basant Kumar Panda and Songhomitra Panda-Jonas and Christina Papachristou and Eun-Kee Park and Scott B Patten and George C Patton and Deepak Paudel and Katherine Paulson and David M Pereira and Fernando Perez-Ruiz and Norberto Perico and Aslam Pervaiz and Max Petzold and Michael Robert Phillips and David M Pigott and Christine Pinho and Dietrich Plass and Martin A Pletcher and Suzanne Polinder and Maarten J Postma and Farshad Pourmalek and Caroline Purcell and Mostafa Qorbani and Beatriz Paulina Ayala Quintanilla and Amir Radfar and Anwar Rafay and Vafa Rahimi-Movaghar and Mohammad Hifz Ur Rahman and Mahfuzar Rahman and Rajesh Kumar Rai and Chhabi Lal Ranabhat and Zane Rankin and Puja C Rao and Goura Kishor Rath and Salman Rawaf and Sarah E Ray and Jürgen Rehm and Robert C Reiner and Marissa B Reitsma and Giuseppe Remuzzi and Satar Rezaei and Mohammad Sadegh Rezai and Mohammad Bagher Rokni and Luca Ronfani and Gholamreza Roshandel and Gregory A Roth and Dietrich Rothenbacher and George Mugambage Ruhago and Rizwan SA and Soheil Saadat and Perminder S Sachdev and Nafis Sadat and Mahdi Safdarian and Sare Safi and Saeid Safiri and Rajesh Sagar and Ramesh Sahathevan and Joseph Salama and Payman Salamati and Joshua A Salomon and Abdallah M Samy and Juan Ramon Sanabria and Maria Dolores Sanchez-Niño and Damian Santomauro and Itamar S Santos and Milena M {Santric Milicevic} and Benn Sartorius and Maheswar Satpathy and Maria Inês Schmidt and Ione J C Schneider and Sam Schulhofer-Wohl and Aletta E Schutte and David C Schwebel and Falk Schwendicke and Sadaf G Sepanlou and Edson E Servan-Mori and Katya Anne Shackelford and Saeid Shahraz and Masood Ali Shaikh and Mansour Shamsipour and Morteza Shamsizadeh and Jayendra Sharma and Rajesh Sharma and Jun She and Sara Sheikhbahaei and Muki Shey and Peilin Shi and Chloe Shields and Mika Shigematsu and Rahman Shiri and Shreya Shirude and Ivy Shiue and Haitham Shoman and Mark G Shrime and Inga Dora Sigfusdottir and Naris Silpakit and João Pedro Silva and Jasvinder A Singh and Abhishek Singh and Eirini Skiadaresi and Amber Sligar and David L Smith and Alison Smith and Mari Smith and Badr H A Sobaih and Samir Soneji and Reed J D Sorensen and Joan B Soriano and Chandrashekhar T Sreeramareddy and Vinay Srinivasan and Jeffrey D Stanaway and Vasiliki Stathopoulou and Nicholas Steel and Dan J Stein and Caitlyn Steiner and Sabine Steinke and Mark Andrew Stokes and Mark Strong and Bryan Strub and Michelle Subart and Muawiyyah Babale Sufiyan and Bruno F Sunguya and Patrick J Sur and Soumya Swaminathan and Bryan L Sykes and Rafael Tabarés-Seisdedos and Santosh Kumar Tadakamadla and Ken Takahashi and Jukka S Takala and Roberto Tchio Talongwa and Mohammed Rasoul Tarawneh and Mohammad Tavakkoli and Nuno Taveira and Teketo Kassaw Tegegne and Arash Tehrani-Banihashemi and Mohamad-Hani Temsah and Abdullah Sulieman Terkawi and J S Thakur and Ornwipa Thamsuwan and Kavumpurathu Raman Thankappan and Katie E Thomas and Alex H Thompson and Alan J Thomson and Amanda G Thrift and Ruoyan Tobe-Gai and Roman Topor-Madry and Anna Torre and Miguel Tortajada and Jeffrey Allen Towbin and Bach Xuan Tran and Christopher Troeger and Thomas Truelsen and Derrick Tsoi and Emin Murat Tuzcu and Stefanos Tyrovolas and Kingsley N Ukwaja and Eduardo A Undurraga and Rachel Updike and Olalekan A Uthman and Benjamin S Chudi Uzochukwu and Job F M {van Boven} and Tommi Vasankari and Narayanaswamy Venketasubramanian and Francesco S Violante and Vasiliy Victorovich Vlassov and Stein Emil Vollset and Theo Vos and Tolassa Wakayo and Mitchell T Wallin and Yuan-Pang Wang and Elisabete Weiderpass and Robert G Weintraub and Daniel J Weiss and Andrea Werdecker and Ronny Westerman and Brian Whetter and Harvey A Whiteford and Tissa Wijeratne and Charles Shey Wiysonge and Belete Getahun Woldeyes and Charles D A Wolfe and Rachel Woodbrook and Abdulhalik Workicho and Denis Xavier and Qingyang Xiao and Gelin Xu and Mohsen Yaghoubi and Bereket Yakob and Yuichiro Yano and Mehdi Yaseri and Hassen Hamid Yimam and Naohiro Yonemoto and Seok-Jun Yoon and Marcel Yotebieng and Mustafa Z Younis and Zoubida Zaidi and Maysaa El Sayed Zaki and Elias Asfaw Zegeye and Zerihun Menlkalew Zenebe and Taddese Alemu Zerfu and Anthony Lin Zhang and Xueying Zhang and Ben Zipkin and Sanjay Zodpey and Alan D Lopez and Christopher J L Murray},
abstract = {Summary
Background
Monitoring levels and trends in premature mortality is crucial to understanding how societies can address prominent sources of early death. The Global Burden of Disease 2016 Study (GBD 2016) provides a comprehensive assessment of cause-specific mortality for 264 causes in 195 locations from 1980 to 2016. This assessment includes evaluation of the expected epidemiological transition with changes in development and where local patterns deviate from these trends.
Methods
We estimated cause-specific deaths and years of life lost (YLLs) by age, sex, geography, and year. YLLs were calculated from the sum of each death multiplied by the standard life expectancy at each age. We used the GBD cause of death database composed of: vital registration (VR) data corrected for under-registration and garbage coding; national and subnational verbal autopsy (VA) studies corrected for garbage coding; and other sources including surveys and surveillance systems for specific causes such as maternal mortality. To facilitate assessment of quality, we reported on the fraction of deaths assigned to GBD Level 1 or Level 2 causes that cannot be underlying causes of death (major garbage codes) by location and year. Based on completeness, garbage coding, cause list detail, and time periods covered, we provided an overall data quality rating for each location with scores ranging from 0 stars (worst) to 5 stars (best). We used robust statistical methods including the Cause of Death Ensemble model (CODEm) to generate estimates for each location, year, age, and sex. We assessed observed and expected levels and trends of cause-specific deaths in relation to the Socio-demographic Index (SDI), a summary indicator derived from measures of average income per capita, educational attainment, and total fertility, with locations grouped into quintiles by SDI. Relative to GBD 2015, we expanded the GBD cause hierarchy by 18 causes of death for GBD 2016.
Findings
The quality of available data varied by location. Data quality in 25 countries rated in the highest category (5 stars), while 48, 30, 21, and 44 countries were rated at each of the succeeding data quality levels. Vital registration or verbal autopsy data were not available in 27 countries, resulting in the assignment of a zero value for data quality. Deaths from non-communicable diseases (NCDs) represented 72·3% (95% uncertainty interval [UI] 71·2–73·2) of deaths in 2016 with 19·3% (18·5–20·4) of deaths in that year occurring from communicable, maternal, neonatal, and nutritional (CMNN) diseases and a further 8·43% (8·00–8·67) from injuries. Although age-standardised rates of death from NCDs decreased globally between 2006 and 2016, total numbers of these deaths increased; both numbers and age-standardised rates of death from CMNN causes decreased in the decade 2006–16—age-standardised rates of deaths from injuries decreased but total numbers varied little. In 2016, the three leading global causes of death in children under-5 were lower respiratory infections, neonatal preterm birth complications, and neonatal encephalopathy due to birth asphyxia and trauma, combined resulting in 1·80 million deaths (95% UI 1·59 million to 1·89 million). Between 1990 and 2016, a profound shift toward deaths at older ages occurred with a 178% (95% UI 176–181) increase in deaths in ages 90–94 years and a 210% (208–212) increase in deaths older than age 95 years. The ten leading causes by rates of age-standardised YLL significantly decreased from 2006 to 2016 (median annualised rate of change was a decrease of 2·89%); the median annualised rate of change for all other causes was lower (a decrease of 1·59%) during the same interval. Globally, the five leading causes of total YLLs in 2016 were cardiovascular diseases; diarrhoea, lower respiratory infections, and other common infectious diseases; neoplasms; neonatal disorders; and HIV/AIDS and tuberculosis. At a finer level of disaggregation within cause groupings, the ten leading causes of total YLLs in 2016 were ischaemic heart disease, cerebrovascular disease, lower respiratory infections, diarrhoeal diseases, road injuries, malaria, neonatal preterm birth complications, HIV/AIDS, chronic obstructive pulmonary disease, and neonatal encephalopathy due to birth asphyxia and trauma. Ischaemic heart disease was the leading cause of total YLLs in 113 countries for men and 97 countries for women. Comparisons of observed levels of YLLs by countries, relative to the level of YLLs expected on the basis of SDI alone, highlighted distinct regional patterns including the greater than expected level of YLLs from malaria and from HIV/AIDS across sub-Saharan Africa; diabetes mellitus, especially in Oceania; interpersonal violence, notably within Latin America and the Caribbean; and cardiomyopathy and myocarditis, particularly in eastern and central Europe. The level of YLLs from ischaemic heart disease was less than expected in 117 of 195 locations. Other leading causes of YLLs for which YLLs were notably lower than expected included neonatal preterm birth complications in many locations in both south Asia and southeast Asia, and cerebrovascular disease in western Europe.
Interpretation
The past 37 years have featured declining rates of communicable, maternal, neonatal, and nutritional diseases across all quintiles of SDI, with faster than expected gains for many locations relative to their SDI. A global shift towards deaths at older ages suggests success in reducing many causes of early death. YLLs have increased globally for causes such as diabetes mellitus or some neoplasms, and in some locations for causes such as drug use disorders, and conflict and terrorism. Increasing levels of YLLs might reflect outcomes from conditions that required high levels of care but for which effective treatments remain elusive, potentially increasing costs to health systems.
Funding
Bill & Melinda Gates Foundation.}
}
@article{LI20181150,
title = {On neural networks and learning systems for business computing},
journal = {Neurocomputing},
volume = {275},
pages = {1150-1159},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.09.054},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217315734},
author = {Yawen Li and Weifeng Jiang and Liu Yang and Tian Wu},
keywords = {Neural networks, Deep learning, Machine learning, Business activities},
abstract = {Artificial intelligence, including neural networks, deep learning and machine learning, has made numerous progress and offered new opportunity for academic research and applications in many fields, especially for business activities and firm development. This paper summarizes different applications of artificial intelligence technologies in several domains of business administration. Finance, retail industry, manufacturing industry, and enterprise management are all included. In spite of all the existing challenges, we conclude that the rapid development of artificial intelligence will show its great impact on more fields.}
}
@article{MEIRE201726,
title = {The added value of social media data in B2B customer acquisition systems: A real-life experiment},
journal = {Decision Support Systems},
volume = {104},
pages = {26-37},
year = {2017},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2017.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167923617301781},
author = {Matthijs Meire and Michel Ballings and Dirk {Van den Poel}},
keywords = {Social media, Business-to-business, Customer acquisition, Experiment, Predictive analytics},
abstract = {Business-to-business organizations and scholars are becoming increasingly aware of the possibilities social media and predictive analytics offer. Despite the interest in social media, only few have analyzed the impact of social media on the sales process. This paper takes a quantitative view to examine the added value of Facebook data in the customer acquisition process. In order to do so, we devise a customer acquisition decision support system to qualify prospects as potential customers, and incorporate commercially purchased prospecting data, website data and Facebook data. Our system is subsequently used by Coca Cola Refreshments Inc. (CCR) to generate calling lists of beverage serving outlets, ranked by their likelihood of becoming a customer. In this paper we report the results, in terms of prospect-to-customer conversion, of a real-life experiment encompassing nearly 9000 prospects. The results show that Facebook is the most informative data source to qualify prospects, and is complementary with the other data sources in that it further improves predictive performance. We contribute to literature in that we are the first to investigate the effectiveness of social media information in acquiring B2B-customers. Our results imply that Facebook data challenge current best practices in customer acquisition.}
}
@article{VADEN2017289,
title = {An orthodontic registry: Producing evidence from existing resources},
journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
volume = {152},
number = {3},
pages = {289-291},
year = {2017},
issn = {0889-5406},
doi = {https://doi.org/10.1016/j.ajodo.2017.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0889540617305401},
author = {James L. Vaden and Christopher S. Riolo and Michael L. Riolo}
}
@article{GORAWSKI2017356,
title = {The TUBE algorithm: Discovering trends in time series for the early detection of fuel leaks from underground storage tanks},
journal = {Expert Systems with Applications},
volume = {90},
pages = {356-373},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417305547},
author = {M. Gorawski and A. Gorawska and K. Pasterak},
keywords = {Trend detection, Leak detection, Anomaly detection, Time series, Petrol station, Quality of data},
abstract = {Leaks and spills of hazardous fluids like petroleum endanger the environment, while remediation costs and penalties imposed when petroleum contaminates the ecosystem affect economics heavily. Therefore, it is crucial to detect any possible symptoms of a leak as soon as possible. Most of existing leak detection techniques require specialized equipment to be used, while purely software-based methods rely solely on data analysis and are very desirable since they can be deployed on petrol stations without any changes to the existing infrastructure. Moreover, such techniques can be considered as complementary to the hardware leak detection systems, as they provide additional security level. In this paper we present the TUBE algorithm, which detects fuel leaks from underground storage tanks, using only standard measurements that are normally registered on petrol stations, i.e. the amount of stored, sold, and delivered fuel. The TUBE algorithm is an autonomous solution capable of making decisions independently as well as supporting human-made decisions and thus can be considered as an expert leak detection system. The TUBE algorithm introduces a new data mining technique for trend detection and cleaning data over time series, which can be easily adapted to any other problem domain. A trend detection technique, called tubes, created for the TUBE algorithm is a novel data analysis method that allows to envelop uncertainties and oscillations in data and produce stable trends. Trend interpretation technique described in this paper has been designed especially for fuel leak detection purposes using our industrial experience. This paper includes a step-by-step usage example of the TUBE algorithm and its evaluation according to the United States Environmental Protection Agency requirements for leakage detection systems (the EPA SIR standard). Such an evaluation involves calculating the probability of detection and the probability of false alarm. The TUBE algorithm has obtained 98.84% probability of detection and 0.07% probability of false alarm while rejecting 42.22% of analyzed datasets due to their uncertainty. Rejecting datasets from analysis is compliant with the EPA SIR standard; however, rejection rate higher than 20% is not acceptable. Therefore we have evaluated the two-phase filtering stage of the algorithm in order to find the best combination of filters as means of data cleaning. Moreover, we have discussed the results pointing at the overall data quality problem, since it is the main cause of rejecting some datasets from the analysis. Finally, the TUBE algorithm has obtained 93.11% probability of detection and 0.73% probability of false alarm for the best combination of all parameters with 15.56% rejection rate, which is acceptable by the EPA SIR standard. The value of probability of detection is not fully compliant with the EPA SIR standard where 95% probability of detection with probability of false alarm lower than 5% is required. We have found that the requirements for the aforementioned probabilities have been completely fulfilled for datasets representing manifolded tank systems but not for single tank datasets. Such a situation was unexpected since manifolded tank systems are generally claimed to be more complex for analysis as they are in fact systems of multiple single tanks directly connected. In this paper we have also measured the time and memory complexity of the TUBE algorithm as well as discussed the issues connected to the TUBE algorithm deployment on petrol stations using our industrial experience in the topic.}
}
@incollection{MEZGHANI2018219,
title = {8 - Online Social Network Phenomena: Buzz, Rumor and Spam},
editor = {Florence Sèdes},
booktitle = {How Information Systems Can Help in Alarm/Alert Detection},
publisher = {Elsevier},
pages = {219-239},
year = {2018},
isbn = {978-1-78548-302-8},
doi = {https://doi.org/10.1016/B978-1-78548-302-8.50008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781785483028500083},
author = {Manel Mezghani and Mahdi Washha and Florence Sèdes},
keywords = {Buzz, Detection, OSN-based information, Quality research, Rumor, Social Network Phenomena, Spam},
abstract = {Abstract:
Online social networks (OSNs) provide data valuable for a tremendous range of applications such as search engines and recommendation systems. However, the easy-to-use interactive interfaces and the low barriers to publication have exposed various information quality (IQ) problems, decreasing the quality of user-generated content (UGC) in such networks. The existence of a particular kind of ill-intentioned user, who are responsible for polluting OSNs’ content, imposes challenges to maintaining an acceptable level of IQ. Such kinds of users may misuse all services provided by social networks to pollute their content in an automated way. IQ problems that appear on OSNs can be categorized into three different forms, buzz, rumor and spam, where each of which has properties and goals differing from others. As a natural reaction and because of the marked failure of OSNs in uprooting these IQ problems, various detection and filtering methods have been designed by various researchers to address the three uncorrelated IQ problems. Hence, in this chapter, we discuss in detail three common negative phenomena appearing in OSNs with the main strengths and drawbacks of their detection systems. Then, we provide detailed research perspectives showing the future work of these OSN phenomena.}
}
@article{PAPADOPOULOS2018251,
title = {Editorial},
journal = {Computers & Operations Research},
volume = {98},
pages = {251-253},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2018.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0305054818301382},
author = {Thanos Papadopoulos and Angappa Gunasekaran}
}
@article{SNOEK201858,
title = {Advancing food, nutrition, and health research in Europe by connecting and building research infrastructures in a DISH-RI: Results of the EuroDISH project},
journal = {Trends in Food Science & Technology},
volume = {73},
pages = {58-66},
year = {2018},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2017.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0924224416302837},
author = {Harriëtte M. Snoek and Lars M.T. Eijssen and Marjolein Geurts and Cecile Vors and Kerry A. Brown and Marc-Jeroen Bogaardt and Rosalie A.M. Dhonukshe-Rutten and Chris T. Evelo and Leopold K. Fezeu and Paul M. Finglas and Martine Laville and Marga Ocké and Giuditta Perozzi and Krijn Poppe and Nadia Slimani and Inge Tetens and Lada Timotijevic and Karin Zimmermann and Pieter {van ’t Veer}},
keywords = {Research infrastructures, Public health, Roadmap, Governance, Policy, Nutrition},
abstract = {Background
Research infrastructures (RIs) are essential to advance research on the relationship between food, nutrition, and health. RIs will facilitate innovation and allow insights at the systems level which are required to design (public health) strategies that will address societal challenges more effectively.
Approach
In the EuroDISH project we mapped existing RIs in the food and health area in Europe, identified outstanding needs, and synthesised this into a conceptual design of a pan-European DISH-RI. The DISH model was used to describe and structure the research area: Determinants of food choice, Intake of foods and nutrients, Status and functional markers of nutritional health, and Health and disease risk.
Key findings
The need to develop RIs in the food and health domain clearly emerged from the EuroDISH project. It showed the necessity for a unique interdisciplinary and multi-stakeholder RI that overarches the research domains. A DISH-RI should bring services to the research community that facilitate network and community building and provide access to standardised, interoperable, and innovative data and tools. It should fulfil the scientific needs to connect within and between research domains and make use of current initiatives. Added value can also be created by providing services to policy makers and industry, unlocking data and enabling valorisation of research insights in practice through public-private partnerships. The governance of these services (e.g. ownership) and the centralised and distributed activities of the RI itself (e.g. flexibility, innovation) needs to be organised and aligned with the different interests of public and private partners.}
}
@incollection{BERMAN2018373,
title = {18 - Data Sharing and Data Security},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {373-393},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00018-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000182},
author = {Jules J. Berman},
keywords = {Data sharing, Open access, Data hoarding, Data encumbrances, Usage agreements, National Patient Identifier},
abstract = {Without data sharing, we do not have science. We just have people with their own agendas asking us to believe their conclusions, without evidence. A seemingly infinite list of official position papers, urging researchers to share their research data, has been published, but progress has been slow. To be sure, there are technical obstacles to data sharing, but for every technical obstacle, there is a wealth of literature offering solutions. In this section, we will be looking at all of the impediments to data sharing, and suggesting some practical solutions.}
}
@article{NOVICK2018444,
title = {The AmeriFlux network: A coalition of the willing},
journal = {Agricultural and Forest Meteorology},
volume = {249},
pages = {444-456},
year = {2018},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0168192317303295},
author = {K.A. Novick and J.A. Biederman and A.R. Desai and M.E. Litvak and D.J.P. Moore and R.L. Scott and M.S. Torn},
keywords = {Eddy covariance, Network science, Climate change, Carbon cycle, Water cycle, Big data, Environmental observation networks},
abstract = {AmeriFlux scientists were early adopters of a network-enabled approach to ecosystem science that continues to transform the study of land-atmosphere interactions. In the 20 years since its formation, AmeriFlux has grown to include more than 260 flux tower sites in the Americas that support continuous observation of ecosystem carbon, water, and energy fluxes. Many of these sites are co-located within a similar climate regime, and more than 50 have data records that exceed 10 years in length. In this prospective assessment of AmeriFlux’s strengths in a new era of network-enabled ecosystem science, we discuss how the longevity and spatial distribution of AmeriFlux data make them exceptionally well suited for disentangling ecosystem response to slowly evolving changes in climate and land-cover, and to rare events like droughts and biological disturbances. More recently, flux towers have also been integrated into environmental observation networks that have broader scientific goals; in North America these include the National Ecological Observatory Network (NEON), Critical Zone Observatory network (CZO), and Long-Term Ecological Research network (LTER). AmeriFlux stands apart from these other networks in its reliance on voluntary participation of individual sites, which receive funding from diverse sources to pursue a wide, transdisciplinary array of research topics. This diffuse, grassroots approach fosters methodological and theoretical innovation, but also challenges network-level data synthesis and data sharing to the network. While AmeriFlux has had strong ties to other regional flux networks and FLUXNET, better integration with networks like NEON, CZO and LTER provides opportunities for new types of cooperation and synergies that could strengthen the scientific output of all these networks.}
}
@article{LIEBOVITZ20181099,
title = {Rebuttal From Drs Liebovitz and Fahrenbach},
journal = {Chest},
volume = {153},
number = {5},
pages = {1099-1100},
year = {2018},
issn = {0012-3692},
doi = {https://doi.org/10.1016/j.chest.2018.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0012369218302290},
author = {David M. Liebovitz and John Fahrenbach}
}
@article{CALLAHAN2017135,
title = {Developing a data sharing community for spinal cord injury research},
journal = {Experimental Neurology},
volume = {295},
pages = {135-143},
year = {2017},
issn = {0014-4886},
doi = {https://doi.org/10.1016/j.expneurol.2017.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0014488617301371},
author = {Alison Callahan and Kim D. Anderson and Michael S. Beattie and John L. Bixby and Adam R. Ferguson and Karim Fouad and Lyn B. Jakeman and Jessica L. Nielson and Phillip G. Popovich and Jan M. Schwab and Vance P. Lemmon},
keywords = {FAIR data principles, Reproducibility, Neuroscience, Informatics, Workshop proceedings, Open Data Commons},
abstract = {The rapid growth in data sharing presents new opportunities across the spectrum of biomedical research. Global efforts are underway to develop practical guidance for implementation of data sharing and open data resources. These include the recent recommendation of ‘FAIR Data Principles’, which assert that if data is to have broad scientific value, then digital representations of that data should be Findable, Accessible, Interoperable and Reusable (FAIR). The spinal cord injury (SCI) research field has a long history of collaborative initiatives that include sharing of preclinical research models and outcome measures. In addition, new tools and resources are being developed by the SCI research community to enhance opportunities for data sharing and access. With this in mind, the National Institute of Neurological Disorders and Stroke (NINDS) at the National Institutes of Health (NIH) hosted a workshop on October 5–6, 2016 in Bethesda, MD, in collaboration with the Open Data Commons for Spinal Cord Injury (ODC-SCI) titled “Preclinical SCI Data: Creating a FAIR Share Community”. Workshop invitees were nominated by the workshop steering committee (co-chairs: ARF and VPL; members: AC, KDA, MSB, KF, LBJ, PGP, JMS), to bring together junior and senior level experts including preclinical and basic SCI researchers from academia and industry, data science and bioinformatics experts, investigators with expertise in other neurological disease fields, clinical researchers, members of the SCI community, and program staff representing federal and private funding agencies. The workshop and ODC-SCI efforts were sponsored by the International Spinal Research Trust (ISRT), the Rick Hansen Institute, Wings for Life, the Craig H. Neilsen Foundation and NINDS. The number of attendees was limited to ensure active participation and feedback in small groups. The goals were to examine the current landscape for data sharing in SCI research and provide a path to its future. Below are highlights from the workshop, including perspectives on the value of data sharing in SCI research, workshop participant perspectives and concerns, descriptions of existing resources and actionable directions for further engaging the SCI research community in a model that may be applicable to many other areas of neuroscience. This manuscript is intended to share these initial findings with the broader research community, and to provide talking points for continued feedback from the SCI field, as it continues to move forward in the age of data sharing.}
}
@article{PEREIRA20181707,
title = {Predictive and Adaptive Management Approach for Omnichannel Retailing Supply Chains},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1707-1713},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.210},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318313314},
author = {Marina Meireles Pereira and Djonathan Luiz {de Oliveira} and Pedro Pfeifer {Portela Santos} and Enzo Morosini Frazzon},
keywords = {Supply Chain Management (SCM), Retail supply chain, Omnichannel, Machine Learning, Simulation-based Optimization},
abstract = {The convergence of physical and online retailing paves the way for the emergence of a retailing omnichannel. Omnichannel retailing supply chain management is challenged by uncertainty, oscillations in sales volume and supply-demand incompatibility. Dealing with those challenges requires the adoption of strategies focused on complex systems that properly employ new information and communication technologies as well as intelligent decision methods. In this context, this research paper aims to propose a reference model for a predictive and adaptive management approach for omnichannel retailing supply chain combining machine learning to minimize uncertainty and simulation based optimization to handle supply-demand synchronization.}
}
@article{YANG2018226,
title = {Offshore Pharmaceutical Trials: Evidence, Economics, and Ethics},
journal = {Mayo Clinic Proceedings: Innovations, Quality & Outcomes},
volume = {2},
number = {3},
pages = {226-228},
year = {2018},
issn = {2542-4548},
doi = {https://doi.org/10.1016/j.mayocpiqo.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2542454818300316},
author = {Y. Tony Yang and Brian Chen and Charles L. Bennett}
}
@article{NGUYEN2018129,
title = {A heuristics approach to mine behavioural data logs in mobile malware detection system},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {129-151},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17303063},
author = {Giang Nguyen and Binh Minh Nguyen and Dang Tran and Ladislav Hluchy},
keywords = {Mobile security, Situational awareness, Anomaly detection, Incremental machine learning, Natural language processing, Scalable solution design},
abstract = {Nowadays, in the era of Internet of Things when everything is connected via the Internet, the number of mobile devices has risen exponentially up to billions around the world. In line with this increase, the volume of data generated is enormous and has attracted malefactors who do ill deeds to others. For hackers, one of the popular threads to mobile devices is to spread malware. These actions are very difficult to prevent because the application installation and configuration rights are set by owners, who usually have very low knowledge or do not care about the security. In this study, our aim is to improve security in the environment of mobile devices by proposing a novel system to detect malware intrusions automatically. Our solution is based on modelling user behaviours and applying the heuristic analysis approach to mobile logs generated during the device operation process. Although behaviours of individual users have a significant impact on the social cyber-security, to achieve the user awareness has still remained one of the major challenges today. For this task, there is proposed a light-weight semantic formalization in the form of physical and logical taxonomy for classifying the collected raw log data. Then a set of techniques is used, like sliding windows, lemmatization, feature selection, term weighting, and so on, to process data. Meanwhile, malware detection tasks are performed based on incremental machine learning mechanisms, because of the potential complexity of this tasks. The solution is developed in the manner to allow the scalability with several blocks that cover pre-processing raw collected logs from mobile devices, automatically creating datasets for machine learning methods, using the best selected model for detecting suspicious activity surrounding malware intrusions, and supporting decision making using a predictive risk factor. We experimented cautiously with the proposal and achieved test results confirm the effectiveness and feasibility of the proposed system in applying to the large-scale mobile environment.}
}
@incollection{FISCHERHBNER2017759,
title = {Chapter 53 - Privacy-Enhancing Technologies},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
address = {Boston},
pages = {759-778},
year = {2017},
isbn = {978-0-12-803843-7},
doi = {https://doi.org/10.1016/B978-0-12-803843-7.00053-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038437000533},
author = {Simone Fischer-Hbner and Stefan Berthold},
keywords = {Data minimization, Data subjects, Legal privacy, Legitimacy, Personal privacy, Privacy, Privacy-enhancing technologies, Purpose limitation, Purpose specification, Transparency},
abstract = {In our modern information age, recent technical developments and trends, such as mobile and pervasive computing, big data, cloud computing, and Web 2.0 applications, increasingly pose privacy dilemmas. Due to the low costs and technical advances of storage technologies, masses of personal data can easily be stored. Once disclosed, these data may be retained forever, often without the knowledge of the individuals concerned, and be removed with difficulty. Hence, it has become hard for individuals to manage and control their personal spheres. Both legal and technical means are needed to protect privacy and to (re-)establish the individuals' control. This chapter provides an overview to the area of Privacy-Enhancing Technologies (PETs), which help to protect privacy by technically enforcing legal privacy principles. It will start with defining the legal foundations of PETs, and will present a classification of PETs as well as a definition of traditional privacy properties that PETs are addressing and metrics for measuring the level of privacy that PETs are providing. Then, a selection of the most relevant PETs is presented.}
}
@article{ZHOU20181683,
title = {Emissions and low-carbon development in Guangdong-Hong Kong-Macao Greater Bay Area cities and their surroundings},
journal = {Applied Energy},
volume = {228},
pages = {1683-1692},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.07.038},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918310675},
author = {Ya Zhou and Yuli Shan and Guosheng Liu and Dabo Guan},
keywords = {Guangdong-Hong Kong-Macao Greater Bay Area, Belt and Road Initiative, CO emissions, Low-carbon development pathways, Urban agglomeration},
abstract = {Cities are the major contributors to energy consumption and CO2 emissions, as well as being leading innovators and implementers of policy measures in climate change mitigation. Guangdong-Hong Kong-Macao Greater Bay Area (GBA) is an agglomeration of cities put forward by China to strengthen international cooperation among “Belt and Road” countries and promote low-carbon, inclusive, coordinated and sustainable development. Few studies have discussed the emission characteristics of GBA cities. This study, for the first time, compiles emission inventories of 11 GBA cities and their surroundings based on IPCC territorial emission accounting approach, which are consistent and comparable with the national and provincial inventories. Results show that (a) total emissions increased from 426 Mt in 2000 to 610 Mt in 2016, while emissions of GBA cities increased rapidly by 6.9% over 2000–2011 and peaked in 2014 (334 Mt); (b) raw coal and diesel oil are the top two emitters by energy type, while energy production sector and tertiary industry are the top two largest sectors; (c) GBA cities take the lead in low-carbon development, emitted 4% of total national emissions and contributed 13% of national GDP with less than a third of national emission intensities and less than three-quarters of national per capita emissions; (d) Macao, Shenzhen and Hong Kong have the top three lowest emission intensity in the country; (e) most of GBA cities are experiencing the shift from an industrial economy to a service economy, while Hong Kong, Shenzhen, Foshan and Huizhou reached their peak emissions and Guangzhou, Dongguan and Jiangmen remained decreasing emission tendencies; (g) for those coal-dominate or energy-production cities (i.e. Zhuhai, Zhongshan, Zhaoqing, Maoming, Yangjiang, Shanwei, Shaoguan and Zhanjiang) in mid-term industrialization, total emissions experienced soaring increases. The emission inventories provide robust, self-consistent, transparent and comparable data support for identifying spatial–temporal emission characteristics, developing low-carbon policies, monitoring mitigation progress in GBA cities as well as further emissions-related studies at a city-level. The low-carbon roadmaps designed for GBA cities and their surroundings also provide a benchmark for other developing countries/cities to adapting changing climate and achieve sustainable development.}
}
@article{KENETT2018141,
title = {A road map for applied data sciences supporting sustainability in advanced manufacturing: the information quality dimensions},
journal = {Procedia Manufacturing},
volume = {21},
pages = {141-148},
year = {2018},
note = {15th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.02.104},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918301392},
author = {Ron S. Kenett and Avigdor Zonnenshain and Gilead Fortuna},
keywords = {Advanced manufacturing, Industry 4.0, Information Quality (InfoQ), Data Science, Analytics},
abstract = {Data science is a multidisciplinary blend of data inference, algorithm development, and technology in order to solve analytically complex problems. Sustainability is a critical asset of a manufacturing enterprise. It enables a business to differentiate itself from competitors and to compete efficiently and effectively to the best of its ability. This paper is a review of data analytics, and how it supports advanced manufacturing with an emphasis on sustainability. The objective is to present a context for a roadmap for applied data science addressing such analytic challenges. We start with a general introduction to advanced manufacturing and trends in modern analytics tools and technology. We then list challenges of analytics supporting advanced manufacturing and sustainability aspects. The information quality (InfoQ) framework is proposed as a backbone to evaluate the analytics needed in advanced manufacturing. The eight InfoQ dimensions are: 1) Data Resolution, 2) Data Structure, 3) Data Integration, 4) Temporal Relevance, 5) Chronology of Data and Goal, 6) Generalizability, 7) Operationalization and 8) Communication. These dimensions provide a classification of advanced manufacturing analytics domains. The paper provides a roadmap for the development of applied analytic techniques supporting advanced manufacturing and sustainability. The objective is to motivate researchers, practitioners and industrialists to support such a roadmap.}
}
@article{WANG2018644,
title = {A prioritization-based analysis of local open government data portals: A case study of Chinese province-level governments},
journal = {Government Information Quarterly},
volume = {35},
number = {4},
pages = {644-656},
year = {2018},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17303428},
author = {Di Wang and Chuanfu Chen and Deborah Richards},
keywords = {Open government data, Analytic hierarchy process, Data evaluation, Open government data portal, Local government, Evaluation framework},
abstract = {While most data originate within a local context, our knowledge about the realization of open government data (OGD) at local levels is limited due to the lack of systematic analysis of local OGD portals. Thus, we focus on a core question about analysing and guiding the development of local OGD portals. An evaluation framework is developed based on the comparison of related studies and principles. To fill the gap in existing frameworks which lack clarity in the prioritization process, Analytic Hierarchy Process together with an expert survey is used to derive priorities of elements of the framework. To test the capability of the framework in analysing and guiding the development of local OGD portals, a case study of Chinese province-level OGD portals has been carried out. Results show that data accessibility and quality matter more than data quantity for a local OGD portal. Currently, Chinese province-level OGD portals are in their infancy of development, with a great gap between Taiwan and Hong Kong with other portals. Data relating to local statistics, credit records, and budget and spending are well released on portals in China. Population size and the size and wealth of the local government show no significant relation with the number of datasets. By combining priorities of the framework with evaluation results, it could help local governments to recognise their present shortcomings and give them recommendations for recognizing directions for OGD portal's future development.}
}
@incollection{2017xv,
title = {Introduction},
editor = {Lowell Fryman and Gregory Lampshire and Dan Meers},
booktitle = {The Data and Analytics Playbook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xv-xxi},
year = {2017},
isbn = {978-0-12-802307-5},
doi = {https://doi.org/10.1016/B978-0-12-802307-5.02001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023075020013}
}
@article{RUSSO2017929,
title = {Towards Antifragile Software Architectures},
journal = {Procedia Computer Science},
volume = {109},
pages = {929-934},
year = {2017},
note = {8th International Conference on Ambient Systems, Networks and Technologies, ANT-2017 and the 7th International Conference on Sustainable Energy Information Technology, SEIT 2017, 16-19 May 2017, Madeira, Portugal},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.426},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917311079},
author = {Daniel Russo and Paolo Ciancarini},
keywords = {Complex Systems, Software Engineering, Antifragility, Architecture},
abstract = {Abstract:
Antifragility is a rising issue in Software Engineering. Due to pervasiveness of software in a growing number of mission critical applications, traditional resilience and recovery systems may not be sufficient. Software has taken over many functionalities which are of vital interest in today and future world. We relay a lot on software applications which may be faulty and cause immense damages. To cope with this scenario, claiming to develop better software is not enough, since unexpected events a.k.a. Black Swans, may disrupt and overcome our system. The purpose of this paper is to propose a new architectural design that responds to the need to build antifragile systems for contemporary complex scenarios. We suggest a system which enhances its strength through experience and errors. It is a self adaptive system architecture improving when facing errors. The most relevant aspect of this approach is that architectures are not only resilient, they extract the intrinsic value of faults. This paper suggests that a fine grained architecture is the key issue to build antifragile systems.}
}
@article{GIBERT20184,
title = {Environmental Data Science},
journal = {Environmental Modelling & Software},
volume = {106},
pages = {4-12},
year = {2018},
note = {Special Issue on Environmental Data Science. Applications to Air quality and Water cycle},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218301269},
author = {Karina Gibert and Jeffery S. Horsburgh and Ioannis N. Athanasiadis and Geoff Holmes},
keywords = {Data Science, Environmental science, Data driven modelling},
abstract = {Environmental data are growing in complexity, size, and resolution. Addressing the types of large, multidisciplinary problems faced by today's environmental scientists requires the ability to leverage available data and information to inform decision making. Successfully synthesizing heterogeneous data from multiple sources to support holistic analyses and extraction of new knowledge requires application of Data Science. In this paper, we present the origins and a brief history of Data Science. We revisit prior efforts to define Data Science and provide a more modern, working definition. We describe the new professional profile of a data scientist and new and emerging applications of Data Science within Environmental Sciences. We conclude with a discussion of current challenges for Environmental Data Science and suggest a path forward.}
}
@article{LISMONT2017114,
title = {Defining analytics maturity indicators: A survey approach},
journal = {International Journal of Information Management},
volume = {37},
number = {3},
pages = {114-124},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216305655},
author = {Jasmien Lismont and Jan Vanthienen and Bart Baesens and Wilfried Lemahieu},
keywords = {Analytics maturity, Analytics techniques, Organizational characteristics, Survey research},
abstract = {The ability to derive new insights from data using advanced machine learning or analytics techniques can enhance the decision-making process in companies. Nevertheless, researchers have found that the actual application of analytics in companies is still in its initial stages. Therefore, this paper studies by means of a descriptive survey the application of analytics with regards to five different aspects as defined by the DELTA model: data, enterprise or organization, leadership, targets or techniques and applications, and the analysts who apply the techniques themselves. We found that the analytics organization in companies matures with regards to these aspects. As such, if companies started earlier with analytics, they apply nowadays more complex techniques such as neural networks, and more advanced applications such as HR analytics and predictive analytics. Moreover, analytics is differently propagated throughout companies as they mature with a larger focus on department-wide or organization-wide analytics and a more advanced data governance policy. Next, we research by means of clustering how these characteristics can indicate the analytics maturity stage of companies. As such, we discover four clusters with a clear growth path: no analytics, analytics bootstrappers, sustainable analytics adopters and disruptive analytics innovators.}
}
@article{MARCHEVSKY20171,
title = {Evidence-based pathology in its second decade: toward probabilistic cognitive computing},
journal = {Human Pathology},
volume = {61},
pages = {1-8},
year = {2017},
issn = {0046-8177},
doi = {https://doi.org/10.1016/j.humpath.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0046817716302106},
author = {Alberto M. Marchevsky and Ann E. Walts and Mark R. Wick},
keywords = {Evidence-based, Pathology, Cognitive computing, Predictive analytics, IBM Watson},
abstract = {Summary
Evidence-based pathology advocates using a combination of best available data (“evidence”) from the literature and personal experience for the diagnosis, estimation of prognosis, and assessment of other variables that impact individual patient care. Evidence-based pathology relies on systematic reviews of the literature, evaluation of the quality of evidence as categorized by evidence levels and statistical tools such as meta-analyses, estimates of probabilities and odds, and others. However, it is well known that previously “statistically significant” information usually does not accurately forecast the future for individual patients. There is great interest in “cognitive computing” in which “data mining” is combined with “predictive analytics” designed to forecast future events and estimate the strength of those predictions. This study demonstrates the use of IBM Watson Analytics software to evaluate and predict the prognosis of 101 patients with typical and atypical pulmonary carcinoid tumors in which Ki-67 indices have been determined. The results obtained with this system are compared with those previously reported using “routine” statistical software and the help of a professional statistician. IBM Watson Analytics interactively provides statistical results that are comparable to those obtained with routine statistical tools but much more rapidly, with considerably less effort and with interactive graphics that are intuitively easy to apply. It also enables analysis of natural language variables and yields detailed survival predictions for patient subgroups selected by the user. Potential applications of this tool and basic concepts of cognitive computing are discussed.}
}
@article{OUNOUGHI2023267,
title = {Data fusion for ITS: A systematic literature review},
journal = {Information Fusion},
volume = {89},
pages = {267-291},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001087},
author = {Chahinez Ounoughi and Sadok {Ben Yahia}},
keywords = {Intelligent transportation system (ITS), Data fusion, Information fusion, Sensor fusion, Systematic literature review},
abstract = {In recent years, the development of intelligent transportation systems (ITS) has involved the input of various kinds of heterogeneous data in real time and from multiple sources, which presents several additional challenges. Studies on Data Fusion (DF) have delivered significant enhancements in ITS and demonstrated a substantial impact on its evolution. This paper introduces a systematic literature review on recent data fusion methods and extracts the main issues and challenges of using these techniques in intelligent transportation systems (ITS). It endeavors to identify and discuss the multi-sensor data sources and properties used for various traffic domains, including autonomous vehicles, detection models, driving assistance, traffic prediction, Vehicular communication, Localization, and management systems. Moreover, it attempts to associate abstractions of observation-level fusion, feature-level fusion, and decision-level fusion with different methods to better understand how DF is used in ITS applications. Consequently, the main objective of this paper is to review DF methods used for ITS studies to extract its trendy challenges. The review outcomes are (i) a description of the current Data fusion methods that adopt multi-sensor sources of heterogeneous data under different evaluation strategies, (ii) identifying several research gaps, current challenges, and new research trends.}
}