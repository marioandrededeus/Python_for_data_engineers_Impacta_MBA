@incollection{SMITH2022319,
title = {1.13 - Geovisualization},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {319-361},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00147-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182345001474},
author = {Mike J. Smith and Jan-Christoph Otto and Antoni B. Moore and Carlos H. Grohmann and John Hillier and Martin Geilhausen},
keywords = {Augmented reality, Cartography, DEM, Filter, Globe, HCI, Interactivity, Kernel, Legend, Map, Symbol, Terrain, Virtual reality, Visualization},
abstract = {Geovisualization, the depiction of, and visual interaction with, geospatial data, is key to facilitating the generation of observational datasets through which Earth surface and solid-Earth processes may be understood. This article focuses upon the visualization of terrain morphology using satellite imagery and digital elevation models (DEMs), where manual interpretation remains prevalent in the study of geomorphological processes. Techniques to enhance satellite images and DEMs are covered in order to improve landform identification, as part of the manual mapping process, are presented. Visual interaction with geospatial data is also an important part of exploring and understanding landforms and geomorphological systems, and a variety of methods ranging from simple overlay, panning and zooming are discussed, along with 2.5D, 3D and temporal analyses. Visualization output products are outlined in the final section, which focuses on static and interactive methods of dissemination. Geomorphological mapping legends and the cartographic principles for map design are then introduced, followed by details of dynamic web-map systems that allow a greater immersive use by end users, as well as the dissemination of data and information.}
}
@article{PAN2022100754,
title = {Distribution patterns of lake-wetland cultural ecosystem services in highland},
journal = {Environmental Development},
volume = {44},
pages = {100754},
year = {2022},
issn = {2211-4645},
doi = {https://doi.org/10.1016/j.envdev.2022.100754},
url = {https://www.sciencedirect.com/science/article/pii/S2211464522000562},
author = {Jianfeng Pan and Yuewei Ma and Siqing Cai and Yan Chen and Yumei Chen},
keywords = {Cultural ecosystem services, Lake-wetland ecosystem, SolVES, Transfer value, Environmental variables},
abstract = {Environmental and ecological degradation are more prominent within lake-wetland ecosystem than any other ecosystem on Earth, especially in the highlands. The continued pressures of population growth and rapid urbanization on cultural ecosystem services (CES) provided by lake-wetland ecosystem in highlands present ongoing challenges to decision-makers and managers. In this paper, Social Values for Ecosystem Services model (SolVES) was used to map, quantify and assess CES in Dianchi lake basin (DLB) and Erhai lake basin (ELB) to understand the spatial dynamics of CES perceived by residents and tourists. After combining the field survey data with four environmental variables, our analysis shows that (1) The Maximum Value Index (M-VI) ranking of the three CES in descending order within DLB was aesthetic (M-VI = 10), recreation (M-VI = 10), cultural (M-VI = 8). (2) Different stakeholders (residents and tourists) had different perceptions of CES. Recreation value between residents and tourists had the same M-VI (M-VI = 10), but the M-VI of aesthetic value (M-VI = 10) perceived by tourists was higher than those perceived by residents. (3) The four environmental variables significantly influenced CES, especially distance to water and dominant landcover. (4) CES were transferable from DLB to ELB due to the potential transferability of Maxent's DLB models for each CES, and CES hotspots in ELB generated from SolVES were highly consistent with high kernel-density areas. In conclusion, SolVES incorporating CES can benefit the basin resource managers when seeking to integrate a social perspective into the resource management decision-making process. In particular, the involvement of various stakeholders ensures that they are not marginalized in environmental planning and management.}
}
@article{WANG2022113136,
title = {A new object-class based gap-filling method for PlanetScope satellite image time series},
journal = {Remote Sensing of Environment},
volume = {280},
pages = {113136},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113136},
url = {https://www.sciencedirect.com/science/article/pii/S0034425722002504},
author = {Jing Wang and Calvin K.F. Lee and Xiaolin Zhu and Ruyin Cao and Yating Gu and Shengbiao Wu and Jin Wu},
keywords = {Gap-filling, CubeSats, Image reconstruction, Cloud removal, Object-based segmentation},
abstract = {PlanetScope CubeSats data with a 3-m resolution, frequent revisits, and global coverage have provided an unprecedented opportunity to advance land surface monitoring over the recent years. Similar to other optical satellites, cloud-induced data missing in PlanetScope satellites substantially hinders its use for broad applications. However, effective gap-filling in PlanetScope image time series remains challenging and is subject to whether it can 1) consistently generate high accuracy results regardless of different gap sizes, especially for heterogeneous landscapes, and 2) effectively recover the missing pixels associated with rapid land cover changes. To address these challenges, we proposed an object-class based gap-filling (‘OCBGF’) method. Two major novelties of OCBGF include 1) adopting an object-based segmentation method in conjunction with an unsupervised classification method to help characterize the landscape heterogeneity and facilitate the search of neighboring valid pixels for gap-filling, improving its applicability regardless of the gap size; 2) employing a scenario-specific gap-filling approach that enables effective gap-filling of areas with rapid land cover change. We tested OCBGF at four sites representative of different land cover types (plantation, cropland, urban, and forest). For each site, we evaluated the performance of OCBGF on both simulated and real cloud-contaminated scenarios, and compared our results with three state-of-the-art methods, namely Neighborhood Similar Pixel Interpolator (NSPI), AutoRegression to Remove Clouds (ARRC), and Spectral-Angle-Mapper Based Spatio-Temporal Similarity (SAMSTS). Our results show that across all four sites, OCBGF consistently obtains the highest accuracy in gap-filling when applied to scenarios with various gap sizes (RMSE = 0.0065, 0.0090, 0.0092, and 0.0113 for OCBGF, SAMSTS, ARRC, and NSPI, respectively) and with/without rapid land cover changes (RMSE = 0.0082, 0.0112, 0.0119, and 0.0120 for OCBGF, SAMSTS, ARRC, and NSPI, respectively). These results demonstrate the effectiveness of OCBGF for gap-filling PlanetScope image time series, with potential to be extended to other satellites.}
}
@article{RAHMAN2022100249,
title = {A pilot study towards a smart-health framework to collect and analyze biomarkers with low-cost and flexible wearables},
journal = {Smart Health},
volume = {23},
pages = {100249},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100249},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000635},
author = {Md Juber Rahman and Bashir I. Morshed and Brook Harmon and Mamunur Rahman},
keywords = {Artificial intelligence, Edge computing, Events of interest, Heart rate variability, Inkjet-printed sensors, Smart health, Smart and connected community, Spatiotemporal monitoring, Flexible wearables},
abstract = {Artificial intelligence-enabled applications on edge devices have the potential to revolutionize disease detection and monitoring with smart health (sHealth) applications. The major challenges are user-friendly and flexible sensors for seamless collection of physiological data for long hours, online and on-device processing of sensitive medical data to facilitate privacy protection, reliable extraction of disease-related biomarkers, and implementation of lightweight artificially intelligent algorithms for inference at the edge without degrading the system performance. In this pilot project, we conducted a yearlong field study with 9 participants conducting 480 data collection sessions in the “living lab” environment. We used smartphones as the edge computing device and implemented pre-trained machine learning algorithms in the smartphone app for computing disease-related Events-of-Interest (EoI). We considered real-time data processing on the smartphone itself without sharing raw data with the cloud or any other computing facility to minimize privacy concerns, and network bandwidth requirements. We used a commercial smart band and a custom-designed zero-power inkjet-printed sensor for physiological sensing and capturing health biomarkers such as heart rate variability (HRV) and core body temperature. The extracted HRV feature values are within the 95% confidence interval of normative values. On top of that, the extracted HRV shows some informative trends i.e. hammock pattern for healthy subjects which may be helpful in subsequent research studies. Moreover, we used core body temperature with user-reported outcomes for estimating flu-related symptoms severity and visualizing the spatiotemporal trend in a cloud-server to facilitate personalized as well as community-wide health monitoring. Inference at the edge provided a data reduction of 3 order while the runtime latency, power consumption, memory requirement, and storage size of the smartphone app were 500 ms, 51.90 mAH, 9.4 MB, and 2.4 MB, respectively. Our developed framework of sHealth enables automated community-wide monitoring of symptoms severity in addition to personalized monitoring which paves the way for early monitoring of a disease outbreak for a smart and connected community.}
}
@article{SHAO2022252,
title = {Simplified quality assessment for small-molecule ligands in the Protein Data Bank},
journal = {Structure},
volume = {30},
number = {2},
pages = {252-262.e4},
year = {2022},
issn = {0969-2126},
doi = {https://doi.org/10.1016/j.str.2021.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0969212621003713},
author = {Chenghua Shao and John D. Westbrook and Changpeng Lu and Charmi Bhikadiya and Ezra Peisach and Jasmine Y. Young and Jose M. Duarte and Robert Lowe and Sijian Wang and Yana Rose and Zukang Feng and Stephen K. Burley},
keywords = {ligand structure quality, composite ranking score, ligand structure, small-molecule ligand, ligand quality indicator, multivariate analysis, principal component analysis, Protein Data Bank, PDB, RCSB PDB},
abstract = {Summary
More than 70% of the experimentally determined macromolecular structures in the Protein Data Bank (PDB) contain small-molecule ligands. Quality indicators of ∼643,000 ligands present in ∼106,000 PDB X-ray crystal structures have been analyzed. Ligand quality varies greatly with regard to goodness of fit between ligand structure and experimental data, deviations in bond lengths and angles from known chemical structures, and inappropriate interatomic clashes between the ligand and its surroundings. Based on principal component analysis, correlated quality indicators of ligand structure have been aggregated into two largely orthogonal composite indicators measuring goodness of fit to experimental data and deviation from ideal chemical structure. Ranking of the composite quality indicators across the PDB archive enabled construction of uniformly distributed composite ranking score. This score is implemented at RCSB.org to compare chemically identical ligands in distinct PDB structures with easy-to-interpret two-dimensional ligand quality plots, allowing PDB users to quickly assess ligand structure quality and select the best exemplars.}
}
@article{LU2022115540,
title = {Trace elements in public drinking water in Chinese cities: Insights from their health risks and mineral nutrition assessments},
journal = {Journal of Environmental Management},
volume = {318},
pages = {115540},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.115540},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722011136},
author = {Taotao Lu and Hao Peng and Feifei Yao and Aira Sacha {Nadine Ferrer} and Shuang Xiong and Geng Niu and Zhonghua Wu},
keywords = {Public drinking water, Trace element, Health risk, Nutritional assessment},
abstract = {The trace elements in the public drinking water have a duality: on the one hand, trace elements play an important role in maintaining human metabolism; on the other hand, high trace elements levels lead to significant health risks. To determine the impacts of trace elements in the public drinking water on physical health in China, water samples were collected from 314 Chinese cities to analyze the concentrations and spatial distributions of trace elements on a national scale. On this basis, the non-carcinogenic health risk assessments and the nutrient-based scores of trace elements (NSTEs) were applied to evaluate the public drinking water quality in terms of safety and nutrition. Most of the water samples were weakly alkaline: pH values fell in the range of 6.62–8.54, with a mean of 7.80. The results indicated that Sr and F− had the highest concentrations in public drinking water, with averages of 0.3604 mg/L and 0.2351 mg/L, respectively. Moreover, hazard index (HI) values in different regions followed the order: northwest China (NWC) > northern China (NC) > Qinghai-Tibetan Plateau (QT) > southern China (SC). The percentages of water samples with HI > 1 in SC, NC, NWC, and QT were 5.49%, 16.82%, 25.81%, and 16.67%, respectively, indicating that the public drinking water in some cities had significant non-carcinogenic health risks. In addition, the intakes of Mn, Fe, Cu, and Rb through public drinking water made negligible contributions to their recommended nutrient intakes. In contrast, trace elements like Sr, F, B, Li, Mo, etc., contributed a lot. The NSTEs in NWC and most parts of NC were relatively high with averages of 8.0300 and 11.2082, respectively; however, the NSTEs in SC and the northeast part of NC were low with averages of 3.3284 and 5.2106, respectively. The results from this study provide a reference for establishing the public drinking water standards and improving drinking water quality.}
}
@article{SENAY2022113011,
title = {Mapping actual evapotranspiration using Landsat for the conterminous United States: Google Earth Engine implementation and assessment of the SSEBop model},
journal = {Remote Sensing of Environment},
volume = {275},
pages = {113011},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113011},
url = {https://www.sciencedirect.com/science/article/pii/S0034425722001250},
author = {Gabriel B. Senay and MacKenzie Friedrichs and Charles Morton and Gabriel E.L. Parrish and Matthew Schauer and Kul Khand and Stefanie Kagone and Olena Boiko and Justin Huntington},
keywords = {Landsat thermal, Evapotranspiration, SSEBop, Cloud computing, Google Earth Engine, Evaluation, Eddy covariance, Water balance},
abstract = {The estimation and mapping of actual evapotranspiration (ETa) is an active area of applied research in the fields of agriculture and water resources. Thermal remote sensing-based methods, using coarse resolution satellites, have been successful at estimating ETa over the conterminous United States (CONUS) and other regions of the world. In this study, we present CONUS-wide ETa from Landsat thermal imagery-using the Operational Simplified Surface Energy Balance (SSEBop) model in the Google Earth Engine (GEE) cloud computing platform. Over 150,000 Landsat satellite images were used to produce 10 years of annual ETa (2010–2019) at unprecedented scale. The accuracy assessment of the SSEBop results included point-based evaluation using monthly Eddy Covariance (EC) data from 25 AmeriFlux stations as well as basin-scale comparison with annual Water Balance ETa (WBET) for more than 1000 sub-basins. Evaluations using EC data showed generally mixed performance with weaker (R2 < 0.6) correlation on sparsely vegetated surfaces such as grasslands or woody savanna and stronger correlation (R2 > 0.7) over well-vegetated surfaces such as croplands and forests, but location-specific conditions rather than cover type were attributed to the variability in accuracy. Croplands performed best with R2 of 0.82, root mean square error of 29 mm/month, and average bias of 12%. The WBET evaluation indicated that the SSEBop model is strong in explaining the spatial variability (up to R2 > 0.90) of ETa across large basins, but it also identified broad hydro-climatic regions where the SSEBop ETa showed directional biases, requiring region-specific model parameter improvement and/or bias correction with an overall 7% bias nationwide. Annual ETa anomalies over the 10-year period captured widely reported drought-affected regions, for the most part, in different parts of the CONUS, indicating their potential applications for mapping regional- and field-scale drought and fire effects. Due to the coverage of the Landsat Path/Row system, the availability of cloud-free image pixels ranged from less than 12 (mountainous cloud-prone regions and U.S. Northeast) to more than 60 (U.S. Southwest) per year. However, this study reinforces a promising application of Landsat satellite data with cloud-computing for quick and efficient mapping of ETa for agricultural and water resources assessments at the field scale.}
}
@article{STROMMER2022134322,
title = {Forward-looking impact assessment – An interdisciplinary systematic review and research agenda},
journal = {Journal of Cleaner Production},
pages = {134322},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134322},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203894X},
author = {Kiia Strömmer and Jarrod Ormiston},
keywords = {Impact assessment, Forward-looking, Temporality, Futures thinking},
abstract = {New and established ventures are under increasing pressure to consider how their current actions impact our future world. Whilst many practitioners are paying greater attention to their future impact, most impact assessment research focuses on the retrospective measurement of impact. Limited studies have explored how impact assessment is used as a tool to forecast or predict the intended impact of organisational action. This study aims to overcome this gap by exploring forward-looking approaches to impact assessment. An interdisciplinary systematic review of the impact assessment literature was conducted to answer the question: “How and why do organisations utilise forward-looking, future-oriented approaches to impact assessment?“. The findings elaborate on the common research themes, challenges, and gaps in understanding forward-looking impact assessment. An integrated process model is developed to show the relationships between various antecedents, methods, and effects of forward-looking impact assessment. Based on the review, the paper puts forward a research agenda to provoke further inquiry on forward-looking, future-oriented approaches to impact assessments related to four research themes: uncertainty, values and assumptions, stakeholder cooperation, and learning. The study contributes to the impact assessment literature by providing an overview of how the current literature comprehends forward-looking approaches and insights into how a more holistic view of temporality in impact assessment can be developed.}
}
@article{PLJAKIC2022,
title = {The influence of traffic-infrastructure factors on pedestrian accidents at the macro-level: The geographically weighted regression approach},
journal = {Journal of Safety Research},
year = {2022},
issn = {0022-4375},
doi = {https://doi.org/10.1016/j.jsr.2022.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S0022437522001359},
author = {Miloš Pljakić and Dragan Jovanović and Boško Matović},
keywords = {Macro-level modeling, Pedestrian accidents, GWPR, Traffic analysis zones, Transportation planning},
abstract = {Introduction: Walking is an active way of moving the population, but in recent years there have been more pedestrian casualties in traffic, especially in developing countries such as Serbia. Macro-level road safety studies enable the identification of influential factors that play an important role in creating pedestrian safety policies. Method: This study analyzes the impact of traffic and infrastructure characteristics on pedestrian accidents at the level of traffic analysis zones. The study applied a geographically weighted regression approach to identify and localize all factors that contribute to the occurrence of pedestrian accidents. Taking into account the spatial correlations between the zones and the frequency distribution of accidents, the geographically Poisson weighted model showed the best predictive performance. Results: This model showed 10 statistically significant factors influencing pedestrian accidents. In addition to exposure measures, a positive relationship with pedestrian accidents was identified in the length of state roads (class I), the length of unclassified streets, as well as the number of bus stops, parking spaces, and object units. However, a negative relationship was recorded with the total length of the street network and the total length of state roads passing through the analyzed area. Conclusion: These results indicate the importance of determining the categorization and function of roads in places where pedestrian flows are pronounced, as well as the perception of pedestrian safety near bus stops and parking spaces. Practical Applications: The results of this study can help traffic safety engineers and managers plan infrastructure measures for future pedestrian safety planning and management in order to reduce pedestrian casualties and increase their physical activity.}
}
@article{SIANG2022100021,
title = {Self-service analytics and the processing of hydrocarbons},
journal = {Digital Chemical Engineering},
volume = {3},
pages = {100021},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100021},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000126},
author = {Lim C. Siang and Shams Elnawawi and Darren Steele},
keywords = {Hydrocarbon processing, Self-service analytics, Data visualization, Process control},
abstract = {This paper describes the use of self-service analytics on time series process data for the troubleshooting and optimization of refinery operations in the context of data visualization principles and best practices. Refining-relevant examples are used to demonstrate how end-users can access real-time and historical process data and apply the following analytics operations across several refining functions, including (1) incident troubleshooting – identifying periods of interest and methods available to investigate related plant data, patterns, events and disturbances leading up to the incident, and (2) data cleansing – filtering sensor data to remove outliers and bad quality data, splicing and aligning data streams for more accurate analysis and to improve the confidence in the outputs of subsequent analysis, such as the outputs of multivariate, regression-based system identification. The paper also provides examples of how ad hoc analyses can be scaled up to plantwide analytics and evolve into routine, automated tasks. The importance of analytic provenance and collaboration in sharing new insights from data is also discussed. To address the human factors associated with self-service analytics innovation, the paper concludes with lessons learnt, observations and adaptations compared to the traditional “business-as-usual approaches, best practices for data governance, and the implications for engineers that operate in a safety-critical environment.}
}
@article{YANG2022119415,
title = {Long short-term memory suggests a model for predicting shale gas production},
journal = {Applied Energy},
volume = {322},
pages = {119415},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119415},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922007504},
author = {Run Yang and Xiangui Liu and Rongze Yu and Zhiming Hu and Xianggang Duan},
keywords = {Shale gas, Exponential smoothing method, LSTM model, Production prediction, Deep learning},
abstract = {Predicting the production behaviors of shale gas wells is of great importance for further developing future unconventional hydrocarbon strategies. An accurate prediction production, as well as reliable shale gas production models, are required to fully understand the shale gas exploitation budget. However, a major problem with classical analytic methods is the insufficient accuracy of the existing models, the time-consuming collection of historical production data, and the costly computational expense. To minimize this problem, a combination of the exponential smoothing method, autoregressive integrated moving average (ARIMA) model, and long short-term memory (LSTM) model was proposed to provide robust support for the production behaviors of shale gas. In this paper, we employed shale gas well production data to establish a database for model training and optimized the predicted model. Hereby, we sought to evaluate the production data predicted by conventional analytical methods, the exponential smoothing method, the ARIMA model, and the LSTM model. Shortly afterward, we objectively compared the predicted results obtained by the novel LSTM model and traditional analytical methods, such as Arps, stretched exponential decline (SEPD), and the Duong model. Herein, we compared the computational cost between the LSTM model and traditional numerical simulation. The combined interpretation of the proposed model demonstrates that the LSTM model achieved scientific accuracy and outstanding results in both short-term and long-term predictions, and realized production prediction of the adjacent well, with excellent agreement with the real shale gas production and a low error, making it an effective tool in forecasting shale gas production. This assay could be used as a potential approach for evaluating deep learning in the petroleum industry and for predicting the future production of unconventional hydrocarbons.}
}
@article{HSU2022134758,
title = {A mixed spatial prediction model in estimating spatiotemporal variations in benzene concentrations in Taiwan},
journal = {Chemosphere},
volume = {301},
pages = {134758},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2022.134758},
url = {https://www.sciencedirect.com/science/article/pii/S0045653522012516},
author = {Chin-Yu Hsu and Hong-Xin Xie and Pei-Yi Wong and Yu-Cheng Chen and Pau-Chung Chen and Chih-Da Wu},
keywords = {Benzene, Land-use regression (LUR), Machine learning, Mixed spatial prediction},
abstract = {It is well known benzene negatively impacts human health. This study is the first to predict spatial-temporal variations in benzene concentrations for the entirety of Taiwan by using a mixed spatial prediction model integrating multiple machine learning algorithms and predictor variables selected by Land-use Regression (LUR). Monthly benzene concentrations from 2003 to 2019 were utilized for model development, and monthly benzene concentration data from 2020, as well as mobile monitoring vehicle data from 2009 to 2019, served as external data for verifying model reliability. Benzene concentrations were estimated by running six LUR-based machine learning algorithms; these algorithms, which include random forest (RF), deep neural network (DNN), gradient boosting (GBoost), light gradient boosting (LightGBM), CatBoost, extreme gradient boosting (XGBoost), and ensemble algorithms (a combination of the three best performing models), can capture how nonlinear observations and predictions are related. The results indicated conventional LUR captured 79% of the variability in benzene concentrations. Notably, the LUR with ensemble algorithm (GBoost, CatBoost, and XGBoost) surpassed all other integrated methods, increasing the explanatory power to 92%. This study establishes the value of the proposed ensemble-based model for estimating spatiotemporal variation in benzene exposure.}
}
@article{ZHANG2022103574,
title = {Information fusion for automated post-disaster building damage evaluation using deep neural network},
journal = {Sustainable Cities and Society},
volume = {77},
pages = {103574},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103574},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721008398},
author = {Limao Zhang and Yue Pan},
keywords = {Deep neural network, Factorization machine, Earthquake-induced building damage, Automated evaluation, Decision making},
abstract = {This paper develops a hybrid neural network architecture named multi-class factorization machine with deep neural network (multi-FMDNN) to fuse multi-source information for the automatic post-earthquake building damage evaluation. The novel algorithm is a combination of the factorization machine (FM) and the deep neural network (DNN), which adopts the one-vs-all strategy to fuse results from multiple base classifiers. 39,352 buildings affected by the 2015 Nepal earthquake are taken as a case study to validate the effectiveness of the proposed multi-FMDNN. Experimental results confirm that the proposed model outperforms over many other popular machine learning methods due to the powerful feature learning ability, ultimately reaching an overall accuracy, macro F1-score, and weighted F1-score in the value of 0.703, 0.737, and 0.702, respectively. Features associated with building structural characteristics are found to contribute more to classifying damage grades precisely. Besides, data preprocessing for data cleaning, encoding, and transformation is a necessary step to bring additional performance enhancement. For significance in the knowledge aspect, a novel multi-FMDNN algorithm is developed, which is superior in extracting both low- and high-order feature representation automatically from large volumes of destroyed buildings-related data and learning the optimal feature interactions simultaneously to pursue more accurate classification. For significance in the application aspect, the predicted results provide deep insights into a better understanding of the building vulnerability in seismic areas and inform data-driven decisions in disaster relief efforts. A promising future scope is to make full use of the available pre-event data along with some post-event data, which is possible to return fairly promising predictions and reduce the burden in earthquake field investigations for rapid responses. In future work, advanced techniques associated with data augment, hyperparameter optimization, and others will be implemented to constantly improve the overall accuracy and generalizability of the prediction model.}
}
@article{KHORANA202275,
title = {The changing contours of global value chains post-COVID: Evidence from the Commonwealth},
journal = {Journal of Business Research},
volume = {153},
pages = {75-86},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.07.044},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322006567},
author = {Sangeeta Khorana and Hubert Escaith and Salamat Ali and Sushma Kumari and Quynh Do},
keywords = {COVID-19, Commonwealth, Reshoring, Decoupling, Global value chains, Protectionism},
abstract = {The COVID-19 pandemic emphasised the global value chains (GVCs) debate by focussing on whether gains from GVC participation outweigh firms associated risks of demand and supply shocks amid rising protectionism. This paper bridges the gap between the international trade and management literature by examining the impact of COVID-19 on Commonwealth countries, an area that has received scant attention in academic literature. Using the Eora database, we simulate scenarios to examine Commonwealth countries’ participation in GVCs post-COVID. We draw on the transaction cost economics (TCE) theory to develop a framework that investigates whether growing protectionism, associated with reshoring, decoupling and nearshoring, could potentially affect the constellation and participation of Commonwealth countries in GVCs post-COVID. Results show that trade protectionism is likely to impact the supply chains and lead to GVC reconfiguration, which could offer opportunities for the Commonwealth countries and firms to potentially gain following the geographical redistribution of suppliers.}
}
@article{JIN2022104733,
title = {Machine learning predicts cancer-associated deep vein thrombosis using clinically available variables},
journal = {International Journal of Medical Informatics},
volume = {161},
pages = {104733},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104733},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622000478},
author = {Shuai Jin and Dan Qin and Bao-Sheng Liang and Li-Chuan Zhang and Xiao-Xia Wei and Yu-Jie Wang and Bing Zhuang and Tong Zhang and Zhen-Peng Yang and Yi-Wei Cao and San-Li Jin and Ping Yang and Bo Jiang and Ben-Qiang Rao and Han-Ping Shi and Qian Lu},
keywords = {Neoplasms, Deep vein thrombosis, Machine learning, Decision making, Risk stratification},
abstract = {Purpose
To develop and validate machine learning (ML) models for cancer-associated deep vein thrombosis (DVT) and to compare the performance of these models with the Khorana score (KS).
Methods
We randomly extracted data of 2100 patients with cancer between Jan. 1, 2017, and Oct. 31, 2019, and 1035 patients who underwent Doppler ultrasonography were enrolled. Univariate analysis and Lasso regression were applied to select important predictors. Model training and hyperparameter tuning were implemented on 70% of the data using a ten-fold cross-validation method. The remaining 30% of the data were used to compare the performance with seven indicators (area under the receiver operating characteristic curve [AUC], sensitivity, specificity, accuracy, balanced accuracy, Brier score, and calibration curve), among all five ML models (linear discriminant analysis [LDA], logistic regression [LR], classification tree [CT], random forest [RF], and support vector machine [SVM]), and the KS.
Results
The incidence of cancer-associated DVT was 22.3%. The top five predictors were D-dimer level, age, Charlson Comorbidity Index (CCI), length of stay (LOS), and previous VTE (venous thromboembolism) history according to RF. Only LDA (AUC = 0.773) and LR (AUC = 0.772) outperformed KS (AUC = 0.642), and combination with D-dimer showed improved performance in all models. A nomogram and web calculator https://webcalculatorofcancerassociateddvt.shinyapps.io/dynnomapp/ were used to visualize the best recommended LR model.
Conclusion
This study developed and validated cancer-associated DVT predictive models using five ML algorithms and visualized the best recommended model using a nomogram and web calculator. The nomogram and web calculator developed in this study may assist doctors and nurses in evaluating individualized cancer-associated DVT risk and making decisions. However, other prospective cohort studies should be conducted to externally validate the recommended model.}
}
@article{HOLZINGER2022263,
title = {Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical artificial intelligence},
journal = {Information Fusion},
volume = {79},
pages = {263-278},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521002050},
author = {Andreas Holzinger and Matthias Dehmer and Frank Emmert-Streib and Rita Cucchiara and Isabelle Augenstein and Javier Del Ser and Wojciech Samek and Igor Jurisica and Natalia Díaz-Rodríguez},
keywords = {Artificial intelligence, Information fusion, Medical AI, Explainable AI, Robustness, Explainability, Trust, Graph-based machine learning, Neural-symbolic learning and reasoning},
abstract = {Medical artificial intelligence (AI) systems have been remarkably successful, even outperforming human performance at certain tasks. There is no doubt that AI is important to improve human health in many ways and will disrupt various medical workflows in the future. Using AI to solve problems in medicine beyond the lab, in routine environments, we need to do more than to just improve the performance of existing AI methods. Robust AI solutions must be able to cope with imprecision, missing and incorrect information, and explain both the result and the process of how it was obtained to a medical expert. Using conceptual knowledge as a guiding model of reality can help to develop more robust, explainable, and less biased machine learning models that can ideally learn from less data. Achieving these goals will require an orchestrated effort that combines three complementary Frontier Research Areas: (1) Complex Networks and their Inference, (2) Graph causal models and counterfactuals, and (3) Verification and Explainability methods. The goal of this paper is to describe these three areas from a unified view and to motivate how information fusion in a comprehensive and integrative manner can not only help bring these three areas together, but also have a transformative role by bridging the gap between research and practical applications in the context of future trustworthy medical AI. This makes it imperative to include ethical and legal aspects as a cross-cutting discipline, because all future solutions must not only be ethically responsible, but also legally compliant.}
}
@article{KIM2022,
title = {Redesigning culturally tailored intervention in the precision health era: Self-management science context},
journal = {Nursing Outlook},
year = {2022},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2022.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0029655422001038},
author = {Miyong T. Kim and Elizabeth M. Heitkemper and Emily T. Hébert and Jacklyn Hecht and Alison Crawford and Tonychris Nnaka and Tara S. Hutson and Hyekyun Rhee and Kavita Radhakrishnan},
keywords = {Cultural tailoring, Psychosocial phenotyping, Precision health, Chronic disease, Self-management, Social determinants of health},
abstract = {Background
Nurse scientists have significantly contributed to health equity and ensuring cultural tailoring of interventions to meet unique needs of individuals. Methodologies for cultural tailoring of self-mangament interventions among marginalized populations have limitedly accommodated intersectionality and group heterogeneity when addressing health needs.
Purpose
Identify methodological limitations in cultural tailoring of interventions among priority populations and issue recommendations on cultural elements that researchers can target to ensure valid cultural tailoring approaches.
Methods
Synthesis of literature on health equity, self-management, and implementation and dissemination research.
Findings
Among priority populations, intersectionality and group heterogeneity has made group-based cultural tailoring approaches less effective in eliciting desirable health outcomes. Precision health methodology could be useful for cultural tailoring of interventions due to the methodology's focus on individual-level tailoring approaches.
Discussion
We offer ways to advance health equity research using precision health approaches in cultural tailoring through targeting unique elements of culture and relevant psychosocial phenotypes.}
}
@article{CHEN2022104318,
title = {hECA: The cell-centric assembly of a cell atlas},
journal = {iScience},
volume = {25},
number = {5},
pages = {104318},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.104318},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222005892},
author = {Sijie Chen and Yanting Luo and Haoxiang Gao and Fanhong Li and Yixin Chen and Jiaqi Li and Renke You and Minsheng Hao and Haiyang Bian and Xi Xi and Wenrui Li and Weiyu Li and Mingli Ye and Qiuchen Meng and Ziheng Zou and Chen Li and Haochen Li and Yangyuan Zhang and Yanfei Cui and Lei Wei and Fufeng Chen and Xiaowo Wang and Hairong Lv and Kui Hua and Rui Jiang and Xuegong Zhang},
keywords = {Cell biology, Stem cells research, Bioinformatics},
abstract = {Summary
The accumulation of massive single-cell omics data provides growing resources for building biomolecular atlases of all cells of human organs or the whole body. The true assembly of a cell atlas should be cell-centric rather than file-centric. We developed a unified informatics framework for seamless cell-centric data assembly and built the human Ensemble Cell Atlas (hECA) from scattered data. hECA v1.0 assembled 1,093,299 labeled human cells from 116 published datasets, covering 38 organs and 11 systems. We invented three new methods of atlas applications based on the cell-centric assembly: “in data” cell sorting for targeted data retrieval with customizable logic expressions, “quantitative portraiture” for multi-view representations of biological entities, and customizable reference creation for generating references for automatic annotations. Case studies on agile construction of user-defined sub-atlases and “in data” investigation of CAR-T off-targets in multiple organs showed the great potential enabled by the cell-centric ensemble atlas.}
}
@article{LI2022100009,
title = {Fault Diagnosis for Lithium-ion Batteries in Electric Vehicles Based on Signal Decomposition and Two-dimensional Feature Clustering},
journal = {Green Energy and Intelligent Transportation},
volume = {1},
number = {1},
pages = {100009},
year = {2022},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2022.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2773153722000093},
author = {Shuowei Li and Caiping Zhang and Jingcai Du and Xinwei Cong and Linjing Zhang and Yan Jiang and Leyi Wang},
keywords = {Electric vehicle, Fault diagnosis, Extended average voltage, Dynamic time warping, Feature clustering},
abstract = {Battery fault diagnosis is essential for ensuring the reliability and safety of electric vehicles (EVs). The existing battery fault diagnosis methods are difficult to detect faults at an early stage based on the real-world vehicle data since lithium-ion battery systems are usually accompanied by inconsistencies, which are difficult to distinguish from faults. A fault diagnosis method based on signal decomposition and two-dimensional feature clustering is introduced in this paper. Symplectic geometry mode decomposition (SGMD) is introduced to obtain the components characterizing battery states, and distance-based similarity measures with the normalized extended average voltage and dynamic time warping distances are established to evaluate the state of batteries. The 2-dimensional feature clustering based on DBSCAN is developed to reduce the number of feature thresholds and differentiate flaw cells from the battery pack with only one parameter under a wide range of values. The proposed method can achieve fault diagnosis and voltage anomaly identification as early as 43 days ahead of the thermal runaway. And the results of four electric vehicles and the comparison with other traditional methods validated the proposed method with strong robustness, high reliability, and long time scale warning, and the method is easy to implement online.}
}
@article{NIU2022124384,
title = {Point and interval forecasting of ultra-short-term wind power based on a data-driven method and hybrid deep learning model},
journal = {Energy},
volume = {254},
pages = {124384},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124384},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222012877},
author = {Dongxiao Niu and Lijie Sun and Min Yu and Keke Wang},
keywords = {Wind power forecasting, Data-driven modeling, Bidirectional long short-term memory, Attention mechanism, Interval forecasting},
abstract = {Accurate and reliable wind power forecasting (WPF) is significant for ensuring power systems’ economic operation and safe dispatching and for reducing the technical and economic risks faced by power market participants. Based on data-driven and deep-learning methods, we propose a hybrid ultra-short-term WPF framework that can achieve accurate point and interval WPF. First, the multi-sourced and multi-dimensional data sets of wind power plant are preprocessed. Second, feature selection (FS) is conducted to eliminate redundant features. Third, the wind power sequence is decomposed through the variational modal decomposition improved by grey wolf optimization (GWO-VMD). Then, the BiLSTM-Attention model is established to predict each subsequence of wind power. Finally, the prediction intervals of wind power under different confidence levels are estimated by kernel density estimation with the Gaussian kernel function (KDE-Gaussian). The proposed FS-GWO-VMD-BiLSTM-Attention forecasting framework is compared with benchmark models to verify its practicability and reliability. Compared with the BPNN, the mean absolute error, mean absolute percentage error, and mean square error of the FS-GWO-VMD-BiLSTM-Attention model are reduced by 94.03%, 85.82%, and 99.51%, respectively. Furthermore, according to the coverage width-based criterion, KDE-Gaussian is superior to other interval forecasting methods, which can achieve more reliable forecasting of prediction interval.}
}
@article{YE2022117674,
title = {Multi-objective optimization of hydrocyclone by combining mechanistic and data-driven models},
journal = {Powder Technology},
volume = {407},
pages = {117674},
year = {2022},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2022.117674},
url = {https://www.sciencedirect.com/science/article/pii/S003259102200568X},
author = {Qing Ye and Peibo Duan and Shibo Kuang and Li Ji and Ruiping Zou and Aibing Yu},
keywords = {Hydrocyclone, Two-fluid model, Steepest ascent, Artificial neural network, Multi-objective optimization},
abstract = {This paper presents a cost-effective method to optimize hydrocyclones used for particle separation. It integrates a mechanistic model for data generation with data-driven models for prediction and optimization. The mechanistic model is based on a validated two-fluid model (TFM), and the data-driven models are the artificial neural network (ANN) and non-dominated sorting genetic algorithm II (NSGA-II). In this integration, the response surface methodology (RSM), coupled with the steepest ascent, is used to design the numerical experiments based on the TFM, aiming to achieve reliable prediction through limited numerical experiments or training data. The applicability of the proposed method is demonstrated by multi-variable and multi-objective optimization of hydrocyclone geometry to achieve low pressure drop and accurate separation, especially for fine particles. The optimization result is elucidated using the multiphase flows predicted by the TFM.}
}
@article{WAI2022128332,
title = {Applications of deep learning in water quality management: A state-of-the-art review},
journal = {Journal of Hydrology},
volume = {613},
pages = {128332},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.128332},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422009040},
author = {Kok Poh Wai and Min Yan Chia and Chai Hoon Koo and Yuk Feng Huang and Woon Chan Chong},
keywords = {Deep learning, Water quality prediction, Signal denoising, Convolutional neural network, Long short-term memory, Gated recurrent unit},
abstract = {Excellent water quality (WQ) is an indispensable element in ensuring sustainable water resource development. It is highly associated with the 3rd (good health and well-being), the 6th (clean water and sanitation), and the 14th (life below water) listed items of the United Nations’ Sustainable Development Goals. Thus, policymakers have always been seeking strategies to manage WQ efficiently. Recent advancements in computational technologies have created enthusiasm for using artificial intelligence, particularly deep learning (DL), in WQ management. This review provides a comprehensive overview of the application of DL in WQ management, covering developments from 2011 to 2022, in maintaining the temporal relevance of this review. In this paper, a brief description of different variants of DL models, including the recurrent neural network (RNN), long short-term memory network (LSTM), convolutional neural network (CNN), etc, are presented. The distinct approaches in the optimization, hybridization and relevant data pre-processing techniques suitable for the DL models, are also discussed. This is the first review paper that extensively discusses the application of DL models for forecasting WQ parameters in various water bodies, such as rivers, lakes, coastal areas, etc. The emergence of the Internet of Things (IoT) and cloud computing that revolutionized DL approaches in WQ management are also presented. This review paper serves as a complete easy guideline for the researchers in the field of DL-based WQ management. The findings of this review paper may help policymakers to enhance their decision-making process with the hope that regional environmental welfare can drastically be improved.}
}
@article{LIU2022,
title = {New metabolic alterations and predictive marker pipecolic acid in sera for esophageal squamous cell carcinoma},
journal = {Genomics, Proteomics & Bioinformatics},
year = {2022},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2021.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1672022922000286},
author = {Lei Liu and Jia Wu and Minxin Shi and Fengying Wang and Haimin Lu and Jibing Liu and Weiqin Chen and Guanzhen Yu and Dan Liu and Jing Yang and Qin Luo and Yan Ni and Xing Jin and Xiaoxia Jin and Wen-Lian Chen},
keywords = {Esophageal squamous cell carcinoma, Serum metabolome, Esophagectomy, Predictive potential, Pipecolic acid},
abstract = {Esophageal squamous cell carcinoma (ESCC) is a major histological subtype of esophageal cancer with a poor prognosis. Although several serum metabolomic investigations have been reported, ESCC tumor-associated metabolic alterations and predictive biomarkers in sera have not been defined. Here, we enrolled 34 treatment-naive patients with ESCC and collected their pre- and post-esophagectomy sera together with the sera from 34 healthy volunteers for a metabolomic survey. Our comprehensive analysis identified ESCC tumor-associated metabolic alterations as represented by a panel of 12 serum metabolites. Notably, postoperative abrosia and parenteral nutrition substantially perturbed the serum metabolome. Furthermore, we performed an examination using sera from carcinogen-induced mice at the dysplasia and ESCC stages and identified three ESCC tumor-associated metabolites conserved between mice and humans. Notably, among these metabolites, the level of pipecolic acid was observed to be progressively increasing in mouse sera from dysplasia to cancerization, and it could be used to accurately discriminate between mice at the dysplasia stage and healthy control mice. Furthermore, this metabolite is essential for ESCC cells to restrain oxidative stress-induced DNA damage and cell proliferation arrest. Together, this study revealed a panel of 12 ESCC tumor-associated serum metabolites with potential for monitoring therapeutic efficacy and disease relapse, presented evidence for refining parenteral nutrition composition, and highlighted serum pipecolic acid as an attractive biomarker for predicting ESCC tumorigenesis.}
}
@article{ZHANG2022103462,
title = {Towards automation of in-season crop type mapping using spatiotemporal crop information and remote sensing data},
journal = {Agricultural Systems},
volume = {201},
pages = {103462},
year = {2022},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2022.103462},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X22000981},
author = {Chen Zhang and Liping Di and Li Lin and Hui Li and Liying Guo and Zhengwei Yang and Eugene G. Yu and Yahui Di and Anna Yang},
keywords = {Crop mapping, Agriculture 4.0, Remote sensing, Cropland Data Layer, Sentinel-2, Google Earth Engine},
abstract = {CONTEXT
Mapping crop types from satellite images is a promising application in agricultural systems. However, it is a challenge to automate in-season crop type mapping over a large area because of the insufficiency of ground truth and issues of scalability, reusability, and accessibility of the classification model. This study introduces a framework for automatic crop type mapping using spatiotemporal crop information and Sentinel-2 data based on Google Earth Engine (GEE). The main advantage of the framework is using the trusted pixels extracted from the historical Cropland Data Layer (CDL) to replace ground truth and label training samples in satellite images.
OBJECTIVE
This paper will achieve three objectives: (1) assessing spatiotemporal crop information derived from the historical crop cover maps; (2) mapping crop cover, mainly crop fields without regular historical crop rotation patterns, from remote sensing data using supervised learning classification and validating mapping results; and (3) automating in-season crop mapping and exploring the scalability of the framework.
METHODS
The proposed crop mapping workflow consists of four stages. The data preparation stage preprocesses CDL and Sentinel-2 data into the required structure. The spatiotemporal crop information sampling stage extracts trusted pixels from the historical CDL time series and labels Sentinel-2 data. Then a crop type classification model can be trained using the supervised learning classifier in the model training stage. In the mapping/validation stage, an in-season crop cover map over the full Sentinel-2 tile will be produced using the trained model and the classification performance will be validated using CDL or other ground truth data.
RESULTS AND CONCLUSIONS
We systematically perform a group of experiments for in-season mapping of five major crop types (corn, cotton, rice, soybeans, and soybeans-wheat double cropping) over the Mississippi Delta region. The result indicates that the crop cover map of the study area is expected to reach 80%–90% agreement with CDL within the growing season. To further facilitate the use of the framework, we also develop a GEE-enabled online prototype, In-season Crop Mapping Kit, and explore its scalability over agricultural fields in various ecoregions including California, Idaho, Kansas, and Illinois.
SIGNIFICANCE
The mapping-without-ground-truth approach described in this paper can significantly reduce ground truthing process and save substantial resource needs and labor costs, which is applicable to the production of in-season CDL-like data for the entire United States. The findings and outputs will benefit the agriculture community and other agricultural sectors ranging from government, academia, and companies.}
}
@article{FESSENMAYR20221349,
title = {Selection of traceability-based, automated decision-making methods in global production networks},
journal = {Procedia CIRP},
volume = {107},
pages = {1349-1354},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.156},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004401},
author = {Franziska Fessenmayr and Martin Benfer and Patrizia Gartner and Gisela Lanza},
keywords = {Automated decision-making, framework, global production networks, traceability, supply chain, method selection, data, decisions, production, adaptable PPS},
abstract = {Automating traceability-based decision-making can shorten the reaction time to supply chain disruptions. This paper develops a framework for choosing automated decision-making (ADM) methods based on traceability data. It contains a toolbox comprising methods suitable for ADM, respective selection criteria and a new process to select a suitable ADM method based on companies’ requirements. This process is based on an evaluation matrix matching methods and criteria. As a result, the ADM framework suggests the most suitable method to automate a specifically chosen decision. The developed framework is validated in the supply chain of a globally operating truck manufacturer.}
}
@article{TIAN2022157730,
title = {Exploring a multisource-data framework for assessing ecological environment conditions in the Yellow River Basin, China},
journal = {Science of The Total Environment},
volume = {848},
pages = {157730},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.157730},
url = {https://www.sciencedirect.com/science/article/pii/S004896972204829X},
author = {Yuqing Tian and Zongguo Wen and Xiu Zhang and Manli Cheng and Mao Xu},
keywords = {Ecological environment conditions, Landscape ecological risk, Road network density, Industry density, Knowledge-based raster mapping},
abstract = {Ecological environment conditions (EEC) assessment plays an important role in watershed management. However, due to insufficient field data, EEC assessment in large-scale watersheds faces challenges. Our study was conducted to develop an effective EEC assessment method framework that was capable of reducing the use of field data. Three indicators were developed from multisource data, including landscape ecological risk index (LERI), road network density (RND), and industry density (ID). The knowledge-based raster mapping approach integrated the three indicators into an overall score of the EEC. Then model validation was conducted with principal components of water quality from field sampling data by Pearson correlation analysis methods. Finally, we applied and demonstrated the constructed method framework in the EEC assessment of the YRB.The results showed that bad EEC (0.5326 < Overall score ≤ 0.7679) areas were mainly distributed in the northern part of the YRB, showing a circular distribution pattern. The areas with bad EEC were 15.84 million km2, accounting for 19.87 % of the YRB. The area of the highest LERI (0.157 < LERI≤0.246), the highest RND (4.4435 < RND ≤ 8.5574), and the highest ID (0.1403 < ID≤0.2597) finally converted to bad EEC was 7.22 million km2, 0.78 million km2, and 0.91 million km2, respectively. The results indicated that the ecological risk factors were the primary challenges for improving EEC, followed by industrial agglomeration and road network factors. The primary factors affecting EEC varied between the provinces in the YRB, suggesting that provinces take the management strategies and measures should be adaptive. The correlation coefficients between EEC and the principal components of water quality characteristics were between 0.022 and 0.241, P < 0.05. These findings validated that our method framework could distinguish the spatial variation of EEC in detail and further provide effective support for watershed management.}
}
@article{KESHAVAMURTHY2022100439,
title = {Predicting infectious disease for biopreparedness and response: A systematic review of machine learning and deep learning approaches},
journal = {One Health},
volume = {15},
pages = {100439},
year = {2022},
issn = {2352-7714},
doi = {https://doi.org/10.1016/j.onehlt.2022.100439},
url = {https://www.sciencedirect.com/science/article/pii/S2352771422000714},
author = {Ravikiran Keshavamurthy and Samuel Dixon and Karl T. Pazdernik and Lauren E. Charles},
keywords = {Systematic review, Infectious diseases, Disease prediction, Disease forecast, Machine learning, Deep learning},
abstract = {The complex, unpredictable nature of pathogen occurrence has required substantial efforts to accurately predict infectious diseases (IDs). With rising popularity of Machine Learning (ML) and Deep Learning (DL) techniques combined with their unique ability to uncover connections between large amounts of diverse data, we conducted a PRISMA systematic review to investigate advances in ID prediction for human and animal diseases using ML and DL. This review included the type of IDs modeled, ML and DL techniques utilized, geographical distribution, prediction tasks performed, input features utilized, spatial and temporal scales, error metrics used, computational efficiency, uncertainty quantification, and missing data handling methods. Among 237 relevant articles published between January 2001 and May 2021, highly contagious diseases in humans were most often represented, including COVID-19 (37.1%), influenza/influenza-like illnesses (9.3%), dengue (8.9%), and malaria (5.1%). Out of 37 diseases identified, 51.4% were zoonotic, 37.8% were human-only, and 8.1% were animal-only, with only 1.6% economically significant, non-zoonotic livestock diseases. Despite the number of zoonoses, 86.5% of articles modeled humans whereas only a few articles (5.1%) contained more than one host species. Eastern Asia (32.5%), North America (17.7%), and Southern Asia (13.1%) were the most represented locations. Frequent approaches included tree-based ML (38.4%) and feed-forward neural networks (26.6%). Articles predicted temporal incidence (66.7%), disease risk (38.0%), and/or spatial movement (31.2%). Less than 10% of studies addressed uncertainty quantification, computational efficiency, and missing data, which are essential to operational use and deployment. This study highlights trends and gaps in ML and DL for ID prediction, providing guidelines for future works to better support biopreparedness and response. To fully utilize ML and DL for improved ID forecasting, models should include the full disease ecology in a One-Health context, important food and agricultural diseases, underrepresented hotspots, and important metrics required for operational deployment.}
}
@article{BARNWAL2022106692,
title = {Sugar and stops in drivers with insulin-dependent type 1 diabetes},
journal = {Accident Analysis & Prevention},
volume = {173},
pages = {106692},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2022.106692},
url = {https://www.sciencedirect.com/science/article/pii/S0001457522001282},
author = {Ashirwad Barnwal and Pranamesh Chakraborty and Anuj Sharma and Luis Riera-Garcia and Koray Ozcan and Sayedomidreza Davami and Soumik Sarkar and Matthew Rizzo and Jennifer Merickel},
keywords = {Naturalistic driving, Unsafe stopping, Driver risk, Type 1 diabetes, Hypoglycemia, Hyperglycemia},
abstract = {Background
Diabetes is a major public health challenge, affecting millions of people worldwide. Abnormal physiology in diabetes, particularly hypoglycemia, can cause driver impairments that affect safe driving. While diabetes driver safety has been previously researched, few studies link real-time physiologic changes in drivers with diabetes to objective real-world driver safety, particularly at high-risk areas like intersections. To address this, we investigated the role of acute physiologic changes in drivers with type 1 diabetes mellitus (T1DM) on safe stopping at stop intersections.
Methods
18 T1DM drivers (21–52 years, μ = 31.2 years) and 14 controls (21–55 years, μ = 33.4 years) participated in a 4-week naturalistic driving study. At induction, each participant’s personal vehicle was instrumented with a camera and sensor system to collect driving data (e.g., GPS, video, speed). Video was processed with computer vision algorithms detecting traffic elements (e.g., traffic signals, stop signs). Stop intersections were geolocated with clustering methods, state intersection databases, and manual review. Videos showing driver stop intersection approaches were extracted and manually reviewed to classify stopping behavior (full, rolling, and no stop) and intersection traffic characteristics.
Results
Mixed-effects logistic regression models determined how diabetes driver stopping safety (safe vs. unsafe stop) was affected by 1) disease and 2) at-risk, acute physiology (hypo- and hyperglycemia). Diabetes drivers who were acutely hyperglycemic (≥ 300 mg/dL) had 2.37 increased odds of unsafe stopping (95% CI: 1.26–4.47, p = 0.008) compared to those with normal physiology. Acute hypoglycemia did not associate with unsafe stopping (p = 0.537), however the lower frequency of hypoglycemia (vs. hyperglycemia) warrants a larger sample of drivers to investigate this effect. Critically, presence of diabetes alone did not associate with unsafe stopping, underscoring the need to evaluate driver physiology in licensing guidelines.
Conclusion
This study links acute, abnormal physiologic fluctuations in drivers with diabetes to driver safety based on unsafe stopping at stop-controlled intersections, providing recommendations for clinicians aimed at improving patient safety, fair licensing guidelines, and targets for developing advanced driver assistance systems.}
}
@article{KRETZSCHMAR2022100546,
title = {Challenges for modelling interventions for future pandemics},
journal = {Epidemics},
volume = {38},
pages = {100546},
year = {2022},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2022.100546},
url = {https://www.sciencedirect.com/science/article/pii/S1755436522000081},
author = {Mirjam E. Kretzschmar and Ben Ashby and Elizabeth Fearon and Christopher E. Overton and Jasmina Panovska-Griffiths and Lorenzo Pellis and Matthew Quaife and Ganna Rozhnova and Francesca Scarabel and Helena B. Stage and Ben Swallow and Robin N. Thompson and Michael J. Tildesley and Daniel Villela},
keywords = {Mathematical models, Pandemics, Pharmaceutical interventions, Non-pharmaceutical interventions, Policy support},
abstract = {Mathematical modelling and statistical inference provide a framework to evaluate different non-pharmaceutical and pharmaceutical interventions for the control of epidemics that has been widely used during the COVID-19 pandemic. In this paper, lessons learned from this and previous epidemics are used to highlight the challenges for future pandemic control. We consider the availability and use of data, as well as the need for correct parameterisation and calibration for different model frameworks. We discuss challenges that arise in describing and distinguishing between different interventions, within different modelling structures, and allowing both within and between host dynamics. We also highlight challenges in modelling the health economic and political aspects of interventions. Given the diversity of these challenges, a broad variety of interdisciplinary expertise is needed to address them, combining mathematical knowledge with biological and social insights, and including health economics and communication skills. Addressing these challenges for the future requires strong cross-disciplinary collaboration together with close communication between scientists and policy makers.}
}
@article{BUDHWANI2022,
title = {A hitchhiker’s guide to cancer models},
journal = {Trends in Biotechnology},
year = {2022},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2022.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167779922001019},
author = {Karim I. Budhwani and Zeelu H. Patel and Rachael E. Guenter and Areesha A. Charania},
keywords = {disease modeling, therapeutic discovery, clinical translation, personalized medicine, precision medicine, , , , , SWOT analysis},
abstract = {Cancer is a complex and uniquely personal disease. More than 1.7 million people in the United States are diagnosed with cancer every year. As the burden of cancer grows, so does the need for new, more effective therapeutics and for predictive tools to identify optimal, personalized treatment options for every patient. Cancer models that recapitulate various aspects of the disease are fundamental to making advances along the continuum of cancer treatment from benchside discoveries to bedside delivery. In this review, we use a thought experiment as a vehicle to arrive at four broad categories of cancer models and explore the strengths, weaknesses, opportunities, and threats for each category in advancing our understanding of the disease and improving treatment strategies.}
}
@article{XU2022113223,
title = {Generating 5 km resolution 1981–2018 daily global land surface longwave radiation products from AVHRR shortwave and longwave observations using densely connected convolutional neural networks},
journal = {Remote Sensing of Environment},
volume = {280},
pages = {113223},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113223},
url = {https://www.sciencedirect.com/science/article/pii/S0034425722003297},
author = {Jianglei Xu and Shunlin Liang and Han Ma and Tao He},
keywords = {Surface longwave radiation, AVHRR, Deep neural network, Long time series, Shortwave observation},
abstract = {Surface longwave radiation (SLWR) components, including downward longwave radiation (DLR), upward longwave radiation (ULR), and net longwave radiation (NLR), are major contributors to the Earth's surface radiation budget and play important roles in ecological, hydrological, and atmospheric processes. Previous SLWR products have different drawbacks, such as being temporally short (after 2000), spatially coarse (≥ 25 km), and instantaneous values, which hinder their in-depth applications in land surface process modeling and climate trends analysis. Here, we reported the Advanced Very High-Resolution Radiometer (AVHRR)-based Global LAnd Surface Satellites (GLASS-AVHRR) SLWR products over the global land surface at a 5 km spatial resolution and 1 day temporal resolution between 1981 and 2018. These products were generated using multiple densely connected convolutional neural networks (DesCNNs) from the AVHRR top-of-atmosphere (TOA) reflected and emitted observations and European Centre for Medium-Range Weather Forecasts (ECMWF) fifth generation reanalysis (ERA5) near-surface meteorological data. DesCNNs were trained using integrated SLWR samples derived from the Moderate Resolution Imaging Spectroradiometer (MODIS)-based GLASS, Clouds and the Earth's Radiant Energy System Synoptic (CERES-SYN), and ERA5 SLWR products. In situ measurements from 231 globally distributed sites were used to evaluate the GLASS-AVHRR SLWR estimates. The results illustrated the overall high accuracies of GLASS-AVHRR SLWR products with root-mean-square-errors (RMSEs) of 18.66, 14.92, and 16.29 Wm−2, and mean bias errors (MBEs) of −2.69, −3.77, and 0.49 Wm−2 for all-sky DLR, ULR, and NLR, respectively. We found good correlation and consistency between GLASS-AVHRR and both CERES-SYN and ERA5 in terms of spatial patterns, latitudinal gradient, and temporal evolution. Our results revealed the significant contribution of shortwave observations to SLWR estimation owing to the high amounts of clouds over polar regions and water vapor and clouds in tropical areas, which was not previously widely recognized by the remote sensing community. GLASS-AVHRR SLWR products were updated, documented, and made available to the public at www.glass.umd.edu and www.geodata.cn.}
}
@article{RAHAMAN2022101162,
title = {Identifying the effect of monsoon floods on vegetation and land surface temperature by using Google Earth Engine},
journal = {Urban Climate},
volume = {43},
pages = {101162},
year = {2022},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2022.101162},
url = {https://www.sciencedirect.com/science/article/pii/S2212095522000803},
author = {Sk Nafiz Rahaman and Nishat Shermin},
keywords = {Flood mapping, Vegetation, LST, Google Earth Engine, Satellite imagery},
abstract = {Flood is one of the most devastating climatic disasters around the world. The physical and infrastructural damage of floods is uncontrollable and challenging to recover. Though the mismanagement of the water system is one cause of flood, some countries face a seasonal natural flood, which is impossible to avoid. The history of flood affection in these countries are long, and people don't have any other choice but to adapt to this circumstance every year. The continuous flood event has several climatic impacts that are not broadly documented and remain in the shadows of severe infrastructural damage. This research aims to identify the effect of monsoon floods on vegetation and land surface temperature (LST). The study area is the northeast part of Bangladesh, a highly flood-prone area. The research incorporates Google Earth Engine (GEE) to manage the satellite image data related to this research which are Sentinel-1 SAR imagery and Landsat-8 imagery. Six years of data from 2015 to 2020 have been taken to continuously monitor the flooded area, vegetation, and LST dynamics. Primary results indicate a yearly increase and decrease of the flooded area with 57.3% highest increase rate in 2019. A continuous increase of Enhanced Vegetation Index (EVI) value and decrease of LST has a changing pattern similarity with flooded area fluctuation over the year. Also, the flooded areas have around 50% less mean EVI value than the non-flooded areas, eventually rising average LST in flooded areas. 10,024 grids of 1 km × 1 km have been used to extensively analyze the relationship of flood and EVI through correlation and linear regression. The final result reveals a clear negative correlation value (less than 0.56 for all the years) of EVI with flooded areas, having the highest R-squared value of 0.4325 in 2017.}
}
@article{XIAO2022117829,
title = {Status quo and opportunities for building energy prediction in limited data Context—Overview from a competition},
journal = {Applied Energy},
volume = {305},
pages = {117829},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117829},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921011570},
author = {Tong Xiao and Peng Xu and Ruikai He and Huajing Sha},
keywords = {Building energy, Energy prediction, Cross-building prediction, Hybrid model, Data-driven model, Data Preparation},
abstract = {With the evolution of new energy and carbon trading systems, it is important to accurately predict building energy consumption to help energy arrangements. Additionally, the widespread use of smart meters has introduced a new data context for building energy prediction. Building energy prediction techniques need improvement but the ideas of various new prediction methods are still on the way and have not yet been compared and tested side-by-side in the reported studies. Thus, we held a competition called ‘Energy Detective’. To investigate the status quo of the current prediction techniques, we designed a representative prediction case: cross-building prediction with limited physical parameters and historical data. A total of 195 participants formed 89 teams to participate in the competition. This paper describes the models presented in the competition. By analysing the methods and results, we identified strategies for the future development of energy prediction in hybrid modelling and data-driven modelling. For hybrid modelling, we discuss the basic strategies for hybrid models and suggest that more hybrid models can be developed by combining a wide variety of individual models in sequence or parallel or via feedback methods to achieve accurate and interpretable models. For data-driven modelling, we analyse and discuss the areas of improvement for the current data-driven workflow and suggest that processes other than model application are also important and should be carefully considered. Considering the increasing amount of data available for prediction, we discuss the shortcomings and suggestions for improving the current data preparation process. We recommend comprehensive consideration of the anomaly types in data pre-processing and a focus on feature engineering for higher accuracy and model interpretability, while emphasising the vital role of data selection in cross-building energy prediction.}
}
@article{NATANELOV2022100389,
title = {Blockchain smart contracts for supply chain finance: Mapping the innovation potential in Australia-China beef supply chains},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100389},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100389},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000565},
author = {Valeri Natanelov and Shoufeng Cao and Marcus Foth and Uwe Dulleck},
keywords = {Blockchain, Smart contracts, Supply chain finance, Reverse factoring, Supply chain risk},
abstract = {This paper explores and demonstrates the innovation potential of blockchain and smart contracts for supply chain finance (SCF) based on cross-border beef supply chains from Australia to China. Our study adopts mechanism design and design-driven activities, and specifically employs the Agents Events Data (AED) process mapping method, which is a hybrid approach that combines Business Process Redesign (BPR) and the Resources Events Assets (REA) accounting model. The AED method consists of three sequential stages: (1) map supply chain's present state or condition; (2) introduce blockchain and smart contracts to improve supply chain processes and “traditional” SCF models; and (3) evaluate the technology and innovation impact and create new models for SCF innovation. This study identified examples of how financial risks can be mitigated or reduced with blockchain and smart contracts; and, where the credit financing itself could be fundamentally transformed. Based on this analysis, the paper's contribution is twofold: First, it proposes the group buying business model as a basis of whole-of-supply-chain finance promising new areas for SCF models that can reduce financial cost and improve cash flow performance due to greater buyer-led financing certainty and the direct involvement of buy-side demand for supply-side improvements. Second, the study demonstrates the utility of the AED process mapping as a holistic framework with fine granular levels for future empirical and fieldwork and opens up new avenues for research.}
}
@article{AYOUBSHAIKH2022107119,
title = {Towards leveraging the role of machine learning and artificial intelligence in precision agriculture and smart farming},
journal = {Computers and Electronics in Agriculture},
volume = {198},
pages = {107119},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107119},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922004367},
author = {Tawseef {Ayoub Shaikh} and Tabasum Rasool and Faisal {Rasheed Lone}},
keywords = {Smart Farming (SF), Precision Agriculture (PA), Automated Irrigation Control, Unarmed Aerial Vehicles (UAV), Machine Learning},
abstract = {The digitalization of data has resulted in a data tsunami in practically every industry of data-driven enterprise. Furthermore, man-to-machine (M2M) digital data handling has dramatically amplified the information wave. There has been a significant development in digital agriculture management applications, which has impacted information and communication technology (ICT) to deliver benefits for both farmers and consumers, as well as pushed technological solutions into rural settings. This paper highlights the potential of ICT technologies in traditional agriculture, as well as the challenges that may arise when they are used in farming techniques. Robotics, Internet of things (IoT) devices, and machine learning issues, as well as the functions of machine learning, artificial intelligence, and sensors in agriculture, are all detailed. In addition, drones are being considered for crop observation as well as crop yield optimization management. When applicable, worldwide and cutting-edge IoT-based farming systems and platforms are also highlighted. We do a thorough review of the most recent literature in each area of expertise. We conclude the present and future trends in artificial intelligence (AI) and highlight existing and emerging research problems in AI in agriculture due to this comprehensive assessment.}
}
@article{JACKSON2022,
title = {The International Malnutrition Task Force: A model for the future?},
journal = {Trends in Food Science & Technology},
year = {2022},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2022.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0924224422003739},
author = {Alan Jackson and Ann Ashworth and Reginald A. Annan},
keywords = {Severe acute malnutrition, Child health, International malnutrition task force, WHO guidelines, Obesity, School feeding},
abstract = {Background
The International Malnutrition Task Force (IMTF) of the International Union of Nutritional Sciences was set up as an advocacy and capacity-building initiative in 2005 at a time when malnutrition contributed to 60% of deaths among children under-five, and when a reduction in under-five mortality by two thirds had been set as Millennium Development Goal 4.
Findings and conclusions
By forging partnerships through regional networks of academics, and linking with international agencies and non-governmental organizations, IMTF was able to increase awareness of the urgent need to address the prevention and treatment of child malnutrition. With modest funding, but a common purpose, partners initiated capacity building efforts to reduce high inpatient mortality rates among children admitted with severe malnutrition. In all regions, these initiatives catalysed scaling-up of the WHO 10-step treatment guidelines, resulting in substantial improvements in child survival. Many lessons were learned during this process including the importance of operational research, supervision and teamwork, and political commitment, and the potential of eLearning. By establishing alliances between academics, health professionals, policy makers, and national and international paediatric and nutrition societies through a Task Force, we suggest that similar benefits might accrue in other fields, including childhood cancer and school feeding.}
}
@article{WANG2022133463,
title = {How does agricultural specialization affect carbon emissions in China?},
journal = {Journal of Cleaner Production},
volume = {370},
pages = {133463},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133463},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203044X},
author = {Ruru Wang and Yu Zhang and Cunming Zou},
keywords = {Agricultural carbon emissions, Agricultural specialization, Agrarian transition, China},
abstract = {Agricultural carbon emissions have long attracted scholars' interest. In recent decades, China has gradually explored the path of agricultural specialization without large-scale operation based on agricultural machinery service outsourcing driven by structural transformation. To date, our understanding of the impact of China's unique agricultural specialization on agricultural carbon emissions remains scant. Nevertheless, this issue is important for promoting China's carbon emission reduction and exploring a path of agricultural transformation that is both competitive and sustainable. Based on China's provincial panel data from 2000 to 2019, this paper constructed a mediating model to test empirically the impacts of agricultural specialization on agricultural carbon emissions. The results showed that agricultural specialization exerted a significant positive effect on agricultural carbon emissions by increasing agricultural external inputs. Specifically, in addition to promoting increased mechanization, agricultural specialization caused an even greater, excessive application of chemical fertilizers, which largely follows metabolic rift theory and substitution and remittance effects. The farmland operation scale had no significant effect on agricultural carbon emissions due to the offset of its significant negative effect on chemicalization and significant positive effect on mechanization. This finding demonstrates that, in the context of smallholder farming, the moderate expansion of farm size can reduce agricultural environmental pollution, but it cannot be proved to reduce agricultural carbon emissions. These findings challenge China's current unique path of agricultural specialization from the perspective of carbon emission reduction and demonstrate the applicability of the theory of metabolic rift to the current path of agricultural development in China. Accordingly, corresponding short-term and long-term implications related to agrarian transition and agricultural carbon emissions are provided.}
}
@article{ANGELIDOU2022121915,
title = {Emerging smart city, transport and energy trends in urban settings: Results of a pan-European foresight exercise with 120 experts},
journal = {Technological Forecasting and Social Change},
volume = {183},
pages = {121915},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121915},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522004371},
author = {M. Angelidou and C. Politis and A. Panori and T. Bakratsas and K. Fellnhofer},
keywords = {Foresight, Delphi survey, Urban development, Urban policy, Responsible Research and Innovation (RRI)},
abstract = {Forecasting future trends constitutes a key process for supporting urban and territorial policy making in general. In this work, we explore how the domains of smart cities, smart transport, and smart energy will evolve until 2030 from a scientific and technological perspective, as a means to inform future policies for urban development in Europe. We started our work with an extensive review of recent and relevant research, covering policy and market reports, scientific journal articles, and other scientific publications. Then, a two-round Delphi survey with 120 field experts was conducted to assess the plausibility of the literature review findings to materialize until 2030. According to our empirical findings, there will be several speedy and structural changes in the three domains: we were able to identify a set of 18 statements that are highly probable to become reality in the next decade, whereas 17 statements were classified as plausible but not highly probable, and three statements raised controversiality. This work provides significant added value in supporting territorial policymakers' and stakeholders' decision making under uncertainty, as well as in designing highly relevant research agendas, attuned to contemporary and emerging trends.}
}
@article{NISO2022119056,
title = {Good scientific practice in EEG and MEG research: Progress and perspectives},
journal = {NeuroImage},
volume = {257},
pages = {119056},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119056},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922001859},
author = {Guiomar Niso and Laurens R. Krol and Etienne Combrisson and A. Sophie Dubarry and Madison A. Elliott and Clément François and Yseult Héjja-Brichard and Sophie K. Herbst and Karim Jerbi and Vanja Kovic and Katia Lehongre and Steven J. Luck and Manuel Mercier and John C. Mosher and Yuri G. Pavlov and Aina Puce and Antonio Schettino and Daniele Schön and Walter Sinnott-Armstrong and Bertille Somon and Anđela Šoškić and Suzy J. Styles and Roni Tibon and Martina G. Vilas and Marijn {van Vliet} and Maximilien Chaumon},
keywords = {Magnetoencephalography (MEG), Electroencephalography (EEG), Good scientific practice},
abstract = {Good scientific practice (GSP) refers to both explicit and implicit rules, recommendations, and guidelines that help scientists to produce work that is of the highest quality at any given time, and to efficiently share that work with the community for further scrutiny or utilization. For experimental research using magneto- and electroencephalography (MEEG), GSP includes specific standards and guidelines for technical competence, which are periodically updated and adapted to new findings. However, GSP also needs to be regularly revisited in a broader light. At the LiveMEEG 2020 conference, a reflection on GSP was fostered that included explicitly documented guidelines and technical advances, but also emphasized intangible GSP: a general awareness of personal, organizational, and societal realities and how they can influence MEEG research. This article provides an extensive report on most of the LiveMEEG contributions and new literature, with the additional aim to synthesize ongoing cultural changes in GSP. It first covers GSP with respect to cognitive biases and logical fallacies, pre-registration as a tool to avoid those and other early pitfalls, and a number of resources to enable collaborative and reproducible research as a general approach to minimize misconceptions. Second, it covers GSP with respect to data acquisition, analysis, reporting, and sharing, including new tools and frameworks to support collaborative work. Finally, GSP is considered in light of ethical implications of MEEG research and the resulting responsibility that scientists have to engage with societal challenges. Considering among other things the benefits of peer review and open access at all stages, the need to coordinate larger international projects, the complexity of MEEG subject matter, and today's prioritization of fairness, privacy, and the environment, we find that current GSP tends to favor collective and cooperative work, for both scientific and for societal reasons.}
}
@article{OKORO2022,
title = {Predicting the effects of selected reservoir petrophysical properties on bottomhole pressure via three computational intelligence techniques},
journal = {Petroleum Research},
year = {2022},
issn = {2096-2495},
doi = {https://doi.org/10.1016/j.ptlrs.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096249522000473},
author = {Emmanuel E. Okoro and Samuel E. Sanni and Tamunotonjo Obomanu and Paul Igbinedion},
keywords = {Computational intelligence, Bottomhole pressure, Petrophysical properties, Heuristic search optimizer, Volvo field data},
abstract = {This study investigates the effects of selected petrophysical properties on predicting flowing well bottomhole pressure. To efficiently situate the essence of this investigation, genetic, imperialist competitive and whale optimization algorithms were used in predicting the bottomhole pressure of a reservoir using production data and some selected petrophysical properties as independent input variables. A total of 15,633 data sets were collected from Volvo field in Norway, and after screening the data, a total of 9161 data sets were used to develop apt computational intelligence models. The data were randomly divided into three different groups: training, validation, and testing data. Two case scenarios were considered in this study. The first scenario involved the prediction of flowing bottomhole pressure using only eleven independent variables, while the second scenario bothered on the prediction of the same flowing bottomhole pressure using the same independent variables and two selected petrophysical properties (porosity and permeability). Each of the two scenarios involved as implied in the first scenario, the use of three (3) heuristic search optimizers to determine optimal model architectures. The optimizers were allowed to choose the optimal number of layers (between 1 and 10), the optimal number of nodal points (between 10 and 100) for each layer and the optimal learning rate required per task/operation. the results, showed that the models were able to learn the problems well with the learning rate fixed from 0.001 to 0.0001, although this became successively slower as the leaning rate decreased. With the chosen model configuration, the results suggest that a moderate learning rate of 0.0001 results in good model performance on the trained and tested data sets. Comparing the three heuristic search optimizers based on minimum MSE, RMSE, MAE and highest coefficient of determination (R2) for the actual and predicted values, shows that the imperialist competitive algorithm optimizer predicted the flowing bottomhole pressure most accurately relative to the genetic and whale optimization algorithm optimizers.}
}
@article{WANG2022106226,
title = {Recognition on the working status of Acetes chinensis quota fishing vessels based on a 3D convolutional neural network},
journal = {Fisheries Research},
volume = {248},
pages = {106226},
year = {2022},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2022.106226},
url = {https://www.sciencedirect.com/science/article/pii/S0165783622000030},
author = {Shuxian Wang and Shengmao Zhang and Yang Liu and Jiaze Zhang and Yongwen Sun and Yuhao Yang and Huijuan Hu and Ying Xiong and Wei Fan and Fei Wang and Fenghua Tang},
keywords = {Acetes chinensis, Quota fishing, Fishing vessels status recognition, 3D convolutional neural network},
abstract = {In the present study, a method for identifying the status of Acetes chinensis fishing vessels based on a 3D convolutional neural network is proposed, so as to protect marine biodiversity, monitor the working status of Acetes chinensis fishing vessels and assist in the realization of quota fishing. The Vessel Monitoring System (VMS) was installed on the quota fishing vessels to collect work data from June 16, 2021 to July 13, 2021. According to the characteristics of the fishing vessels, the work status of the fishing vessels was divided into five statuses such as stopping, sailing, putting net, waiting and pulling net. The 3D convolutional neural network Acetes3DNet was designed to extract the multi-dimensional and multi-level features of the data and trained in the training set. Finally, the effectiveness of the model was verified in the validation set. The training results were combined with the Beidou ship position data to restore the working process of the fishing vessel. The experimental results reveal that after 150 epochs of training, the precision, recall, and f1 score of Acetes3DNet on the training set reached 99.02%, 99.19%, and 99.09%, respectively, while the precision, recall, and f1 score on the validation set reached 97.09%, 96.82%, and 96.68%. Research shows that Acetes3DNet can circumvent the limitations of traditional 2D neural networks in dynamic target detection, complete recognition of the working status of Acetes chinensis quota fishing vessels, and show the historical work process of the ship in an intuitive manner. The experimental results are conducive to standardizing the management of fishing vessels and protecting marine life.}
}
@article{PANAHIRIZI2022100584,
title = {A systematic review of technologies and solutions to improve security and privacy protection of citizens in the smart city},
journal = {Internet of Things},
volume = {20},
pages = {100584},
year = {2022},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2022.100584},
url = {https://www.sciencedirect.com/science/article/pii/S2542660522000737},
author = {Mohammad Hosein {Panahi Rizi} and Seyed Amin {Hosseini Seno}},
keywords = {Descriptive systematic review, Smart city, IoT, Privacy, Security},
abstract = {The development of smart cities through digital communications has improved citizens' quality of life and well-being. In these cities, IoT technology generates vast amounts of data at any given time, which is analyzed to provide services to citizens. In the proper implementation of these cities, a critical challenge is the violation of citizens' privacy and security, which leads to a lack of trust and pessimism toward the services of the smart city. To ensure citizens' participation, smart city developers should adequately protect their security and privacy from gaining their trust. If citizens don't want to participate, the main benefits of a smart city will be lost. This article presents a comprehensive review of smart city security issues and privacy. It provides a basis for categorizing current and future developments in this area and developing a thematic classification to highlight the requirements and security strategies for designing a smart and safe city. The paper identifies current security and privacy solutions and describes open research challenges and issues. An output of this study is a systematic map of literature on the subject that identifies critical concepts, evidence, challenges, solutions, and gaps. It summarizes the findings into a body of evidence that has previously been heterogeneous and complex.}
}
@article{PLOTNIKOVA2022102013,
title = {Applying the CRISP-DM data mining process in the financial services industry: Elicitation of adaptation requirements},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {102013},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102013},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000258},
author = {Veronika Plotnikova and Marlon Dumas and Fredrik P. Milani},
keywords = {Data mining, CRISP-DM, Case study, Requirements},
abstract = {Data mining techniques have gained widespread adoption over the past decades, particularly in the financial services domain. To achieve sustained benefits from these techniques, organizations have adopted standardized processes for managing data mining projects, most notably CRISP-DM. Research has shown that these standardized processes are often not used as prescribed, but instead, they are extended and adapted to address a variety of requirements. To improve the understanding of how standardized data mining processes are extended and adapted in practice, this paper reports on a case study in a financial services organization, aimed at identifying perceived gaps in the CRISP-DM process and characterizing how CRISP-DM is adapted to address these gaps. The case study was conducted based on documentation from a portfolio of data mining projects, complemented by semi-structured interviews with project participants. The results reveal 18 perceived gaps in CRISP-DM alongside their perceived impact and mechanisms employed to address these gaps. The identified gaps are grouped into six categories. Next, they were triangulated and augmented with the gaps discovered in the other studies. Then, the requirements for adapting CRISP-DM to address the gaps were derived, and the directions for the potential adaptations were outlined. The study presents a two-fold contribution. It provides practitioners with a structured set of gaps to be considered when applying CRISP-DM, or similar processes, in the financial services sector. Additionally, the study elicits the requirements and sketches the potential solutions to address these gaps. Also, the number of the identified gaps is generic and applicable to other sectors with similar concerns (e.g. privacy), such as telecom or e-commerce.}
}
@article{DOUMPOS2022,
title = {Operational research and artificial intelligence methods in banking},
journal = {European Journal of Operational Research},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S037722172200337X},
author = {Michalis Doumpos and Constantin Zopounidis and Dimitrios Gounopoulos and Emmanouil Platanakis and Wenke Zhang},
keywords = {Artificial Intelligence, Operational research, Banking},
abstract = {Banking is a popular topic for empirical and methodological research that applies operational research (OR) and artificial intelligence (AI) methods. This article provides a comprehensive and structured bibliographic survey of OR- and AI-based research devoted to the banking industry over the last decade. The article reviews the main topics of this research, including bank efficiency, risk assessment, bank performance, mergers and acquisitions, banking regulation, customer-related studies, and fintech in the banking industry. The survey results provide comprehensive insights into the contributions of OR and AI methods to banking. Finally, we propose several research directions for future studies that include emerging topics and methods based on the survey results.}
}
@article{LANGER2022119348,
title = {A benchmark for prediction of psychiatric multimorbidity from resting EEG data in a large pediatric sample},
journal = {NeuroImage},
volume = {258},
pages = {119348},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119348},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922004670},
author = {Nicolas Langer and Martyna Beata Plomecka and Marius Tröndle and Anuja Negi and Tzvetan Popov and Michael Milham and Stefan Haufe},
abstract = {Psychiatric disorders are among the most common and debilitating illnesses across the lifespan and begin usually during childhood and adolescence, which emphasizes the importance of studying the developing brain. Most of the previous pediatric neuroimaging studies employed traditional univariate statistics on relatively small samples. Multivariate machine learning approaches have a great potential to overcome the limitations of these approaches. On the other hand, the vast majority of existing multivariate machine learning studies have focused on differentiating between children with an isolated psychiatric disorder and typically developing children. However, this line of research does not reflect the real-life situation as the majority of children with a clinical diagnosis have multiple psychiatric disorders (multimorbidity), and consequently, a clinician has the task to choose between different diagnoses and/or the combination of multiple diagnoses. Thus, the goal of the present benchmark is to predict psychiatric multimorbidity in children and adolescents. For this purpose, we implemented two kinds of machine learning benchmark challenges: The first challenge targets the prediction of the seven most prevalent DSM-V psychiatric diagnoses for the available data set, of which each individual can exhibit multiple ones concurrently (i.e. multi-task multi-label classification). Based on behavioral and cognitive measures, a second challenge focuses on predicting psychiatric symptom severity on a dimensional level (i.e. multiple regression task). For the present benchmark challenges, we will leverage existing and future data from the biobank of the Healthy Brain Network (HBN) initiative, which offers a unique large-sample dataset (N = 2042) that provides a wide array of different psychiatric developmental disorders and true hidden data sets. Due to limited real-world practicability and economic viability of MRI measurements, the present challenge will permit only resting state EEG data and demographic information to derive predictive models. We believe that a community driven effort to derive predictive markers from these data using advanced machine learning algorithms can help to improve the diagnosis of psychiatric developmental disorders.}
}
@article{ROUSIS2022107436,
title = {Socioeconomic status and public health in Australia: A wastewater-based study},
journal = {Environment International},
volume = {167},
pages = {107436},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2022.107436},
url = {https://www.sciencedirect.com/science/article/pii/S0160412022003634},
author = {Nikolaos I. Rousis and Zhe Li and Richard Bade and Michael S. McLachlan and Jochen F. Mueller and Jake W. O'Brien and Saer Samanipour and Benjamin J. Tscharke and Nikolaos S. Thomaidis and Kevin V. Thomas},
keywords = {Wastewater-based epidemiology, Metoprolol, Atenolol acid, Venlafaxine, Sotalol, Sitagliptin},
abstract = {Analysis of untreated municipal wastewater is recognized as an innovative approach to assess population exposure to or consumption of various substances. Currently, there are no published wastewater-based studies investigating the relationships between catchment social, demographic, and economic characteristics with chemicals using advanced non-targeted techniques. In this study, fifteen wastewater samples covering 27% of the Australian population were collected during a population Census. The samples were analysed with a workflow employing liquid chromatography high-resolution mass spectrometry and chemometric tools for non-target analysis. Socioeconomic characteristics of catchment areas were generated using Geospatial Information Systems software. Potential correlations were explored between pseudo-mass loads of the identified compounds and socioeconomic and demographic descriptors of the wastewater catchments derived from Census data. Markers of public health (e.g., cardiac arrhythmia, cardiovascular disease, anxiety disorder and type 2 diabetes) were identified in the wastewater samples by the proposed workflow. They were positively correlated with descriptors of disadvantage in education, occupation, marital status and income, and negatively correlated with descriptors of advantage in education and occupation. In addition, markers of polypropylene glycol (PPG) and polyethylene glycol (PEG) related compounds were positively correlated with housing and occupation disadvantage. High positive correlations were found between separated and divorced people and specific drugs used to treat cardiac arrhythmia, cardiovascular disease, and depression. Our robust non-targeted methodology in combination with Census data can identify relationships between biomarkers of public health, human behaviour and lifestyle and socio-demographics of whole populations. Furthermore, it can identify specific areas and socioeconomic groups that may need more assistance than others for public health issues. This approach complements important public health information and enables large-scale national coverage with a relatively small number of samples.}
}
@article{WU2022102897,
title = {Spatiotemporal assessments of nutrients and water quality in coastal areas using remote sensing and a spatiotemporal deep learning model},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102897},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102897},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222000991},
author = {Sensen Wu and Jin Qi and Zhen Yan and Fangzheng Lyu and Tao Lin and Yuanyuan Wang and Zhenhong Du},
keywords = {Aquatic environment, Spatiotemporal deep learning, Water quality, Remote sensing, Coastal restoration},
abstract = {Revealing the spatiotemporal variations of nutrients in coastal waters is crucial to the understanding and evaluation of coastal environment, thereby providing efficient guidance for the aquatic environmental treatment. This study proposed a spatiotemporal-incorporated deep learning model, which is easily applicable to establish the quantitative relationships between measured environmental factors and large-scale satellite maps, and can reduce estimation errors by more than 40% compared with non-spatiotemporal-incorporated deep learning model. The spatiotemporal distributions of dissolved inorganic nitrogen (DIN) and dissolved inorganic phosphate (DIP) over 44400 km2 of the East China Sea on 8-day scale from 2010 to 2018 were obtained. Based on the spatiotemporal variations, the water quality patterns were depicted, and the fluctuation variations of the two essential nutrients were found in the harbors with complex anthropogenic influences, in the typical estuaries with multiple river inputs, and in the open seas with important fisheries. Although the concentration of DIN and DIP decreased by 24% and 19% in 9 years, respectively, the water quality level in the inshore sea has not been significantly improved, especially in autumn and winter. Further, we quantitatively analyzed the main factors of deteriorated water and provided scientific suggestions for targeted monitoring and regional cooperative governances.}
}
@article{GE2022106582,
title = {Spatial heterogeneity of long-term environmental changes in a large agricultural wetland in North China: Implications for wetland restoration},
journal = {CATENA},
volume = {219},
pages = {106582},
year = {2022},
issn = {0341-8162},
doi = {https://doi.org/10.1016/j.catena.2022.106582},
url = {https://www.sciencedirect.com/science/article/pii/S0341816222005689},
author = {Yawen Ge and Xin Mao and Zijing She and Linjing Liu and Lei Song and Yuecong Li and Changhong Liu},
keywords = {Sedimentary records, Spatial heterogeneity, Agricultural wetland, Hydrological processes, Land use, Lake Baiyangdian},
abstract = {Understanding long-term environmental changes and man-land interactions within wetlands is important for their ecological maintenance and restoration. However, there has been little focus on the spatial heterogeneity of environmental variations of wetlands, especially large agricultural wetlands, which hinders the development of targeted management programs for specific areas. We conducted a multi-proxy study of the sediments of a typical agricultural wetland, Lake Baiyangdian in North China, to assess the spatial heterogeneity of environmental changes over the past 70 years. The results reveal that the environmental changes in Lake Baiyangdian were spatially heterogeneous, mainly reflected by the response to hydrological variations before the 1970 s, and by changes in trophic level and land cover transitions since the 1990 s, which are strongly linked to changes in agricultural land use. Pollen and grain-size analyses revealed the large-scale spatial heterogeneity of changes in the sedimentary environment. Areas with greater openness, shallower water depths, and proximity to inflowing rivers were generally more sensitive to hydrological processes and provided regional records of environmental change. In contrast, populated areas far from inflows tended to capture more local changes related to anthropogenic impacts. Intensive agricultural activity has led to a continuous increase in nutrient enrichment and significant land cover transitions in Lake Baiyangdian, mainly the loss of natural wetland. Variations in Humulus pollen were a key indicator of these anthropogenic impacts. Hydrological processes and land use changes were together responsible for the spatial heterogeneity of the environmental variations in Lake Baiyangdian over the last 70 years, indicating the dominant role of human impacts on the spatio-temporal pattern of the evolution of this wetland ecosystem, and the complexity of its environmental management.}
}
@article{HAQUE2022108877,
title = {Short-term electrical load forecasting through heuristic configuration of regularized deep neural network},
journal = {Applied Soft Computing},
volume = {122},
pages = {108877},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108877},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002575},
author = {Ashraful Haque and Saifur Rahman},
keywords = {Short-term electrical load forecasting, Neural networks, Deep learning, Regularization, Times series analysis},
abstract = {An accurate electrical load forecasting is essential for optimal grid operation. The paper presents a methodology for the short-term commercial building electrical load forecasting through a regularized deep neural network: Long Short-Term Memory Recurrent Neural Network (LSTM-RNN). Detailed heuristic analysis regarding relevant input feature selection, the volume of training data, hyperparameter tuning and regularizer selection of an optimal LSTM-RNN network configuration is presented. The regularized LSTM-RNN is used to forecast 30-min and 24-h ahead electrical loads of two commercial buildings in Virginia, USA. The forecast is performed for one week each over four different months in 2019: January, April, July and October to represent four different seasons in North America. The performance of electrical load forecasts has been compared against actual smart meter data from the electric utility of these buildings. For the case study presented, Mean Absolute Percentage Error (MAPE%) with the regularized LSTM-RNN is 4.9%, compared to 6.4%, 9.2% and 13.3% with Shallow-ANN (Artificial Neural Network), Support Vector Regression (SVR) and Linear Regression (LR) respectively for 30-min ahead electrical load forecast. For 24-h ahead electrical load forecast, MAPE (%) is 11.6%, compared to 12.7%, 13.4% and 14.3% with shallow-ANN, SVR and LR respectively. The methodology to configure a deep neural network (LSTM-RNN) for electrical load forecasting presented in this paper can be utilized for optimal forecasting performance.}
}
@article{LENTZ2022103242,
title = {How do information problems constrain anticipating, mitigating, and responding to crises?},
journal = {International Journal of Disaster Risk Reduction},
volume = {81},
pages = {103242},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103242},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922004617},
author = {Erin C. Lentz and Daniel Maxwell},
keywords = {Early warning, Anticipatory action, Humanitarian action, Crisis},
abstract = {The explosion in data availability and new analytical tools combined with increasing humanitarian need and the imperative of anticipatory action compel us to rethink humanitarian information systems and humanitarian action for the future. Synthesizing interviews with humanitarian practitioners, donors, analysts, and researchers and analyses of early warning (EW) information systems and their linkages to Anticipatory Action (AA), we describe six information challenges within the current system: abundant but confusing information, the difficulty of predicting conflict, politicized information, limitations of new analytical tools, varying information needs, and limited data sharing. We then propose an approach to improve the timeliness and appropriateness of action for humanitarian crises and disasters. Rather than ask, “What can we do with the information (early warning and otherwise) that we have to inform action?” we propose asking, “What information do we need for anticipatory (and other) action?” In other words, we propose planning from known and likely hazards and actions back to information needs. Such an approach should help to mitigate shocks before they cause major humanitarian crises. While not all crises can be prevented, this approach could also support responsive action, which is equally important for protecting human life and dignity.}
}
@article{BARRIGA2022118255,
title = {Crop-water assessment in Citrus (Citrus sinensis L.) based on continuous measurements of leaf-turgor pressure using machine learning and IoT},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118255},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118255},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422013987},
author = {Jose A. Barriga and Fernando Blanco-Cipollone and Emiliano Trigo-Córdoba and Iván García-Tejero and Pedro J. Clemente},
keywords = {IoT, Machine learning, Expert system, Turgor pressure, Stem water potential, Irrigation scheduling},
abstract = {Water is the most limiting natural resource in many semi-arid areas. This, together with the current climate change scenario, is fostering a context of uncertainty and major challenges concerning the sustainability and viability of existing agroecosystems. Crop water status based on three pre-established values (severe, mild, and no stress) is the essential datum needed to implement optimised irrigation scheduling based on deficit irrigation. Currently however, its calculation is a repetitive, tedious, and technical process carried out by hand. This communication presents a novel system based on continuous measurements of leaf turgor pressure to assess the crop water status when deficit irrigation strategies are being applied and/or to optimise irrigation scheduling in water scarcity scenarios. To this end, a novel expert system based on machine learning, together with an IoT infrastructure based on continuous measurements of leaf turgor pressure, is able to predict the citrus crop ψstem with a 99% F1 score. Thus, crop irrigation strategies involving irrigation-restriction cycles can be applied based on stem water potential.}
}
@article{MAN2022106511,
title = {Transfer learning for spatio-temporal transferability of real-time crash prediction models},
journal = {Accident Analysis & Prevention},
volume = {165},
pages = {106511},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106511},
url = {https://www.sciencedirect.com/science/article/pii/S000145752100542X},
author = {Cheuk Ki Man and Mohammed Quddus and Athanasios Theofilatos},
keywords = {Transferability, Transfer learning, Imbalanced dataset, Generative adversarial network, Oversampling},
abstract = {Real-time crash prediction is a heavily studied area given their potential applications in proactive traffic safety management in which a plethora of statistical and machine learning (ML) models have been developed to predict traffic crashes in real-time. However, one of the fundamental issues relating to the application of these models is spatio-temporal transferability. The present paper attempts to address this gap of knowledge by combining Generative Adversarial Network (GAN) and transfer learning to examine the transferability of real-time crash prediction models under an extremely imbalanced data setting. Initially, a baseline model was developed using Deep Neural Network (DNN) with crash and microscopic traffic data collected from M1 Motorway in the UK in 2017. The dataset utilised in the baseline model is naturally imbalanced with 257 crash cases and 16,359,163 non-crash cases. To overcome data imbalance issue, Wasserstein GAN (WGAN) was utilised to generate synthetic crash data. Non-crash data were randomly undersampled due to computational limitations. The calibrated model was then applied to predict traffic crashes for five other datasets obtained from M1 (2018), M4 (2017 & 2018 separately) and M6 Motorway (2017 & 2018 separately) by using transfer learning. Model transferability was compared with standalone models and direct transfer from the baseline model. The study revealed that direct transfer is not feasible. However, models become transferable temporally, spatially, and spatio-temporally if transfer learning is applied. The predictability of the transferred models outperformed existing studies by achieving high Area Under Curve (AUC) values ranging between 0.69 and 0.95. The best transferred model can predict nearly 95% crashes with only a 5% false alarm rate by tuning thresholds. Furthermore, the performances of transferred models are on par with or better than the standalone model. The findings of this study proves that transfer learning can improve model transferability under extremely imbalanced settings which helps traffic engineers in developing highly transferable models in future.}
}
@article{JUNGBLUTH202215,
title = {A gaps-and-needs analysis of vaccine R&D in Europe: Recommendations to improve the research infrastructure},
journal = {Biologicals},
volume = {76},
pages = {15-23},
year = {2022},
issn = {1045-1056},
doi = {https://doi.org/10.1016/j.biologicals.2022.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1045105622000173},
author = {Stefan Jungbluth and Hilde Depraetere and Monika Slezak and Dennis Christensen and Norbert Stockhofe and Laurent Beloeil},
keywords = {Vaccines, Research and development, Research infrastructure, Science policy, Platform technologies, Manufacturing},
abstract = {The COVID-19 pandemic has brought into sharp focus the importance of strategies supporting vaccine development. During the pandemic, TRANSVAC, the European vaccine–research-infrastructure initiative, undertook an in-depth consultation of stakeholders to identify how best to position and sustain a European vaccine R&D infrastructure. The consultation included an online survey incorporating a gaps-and-needs analysis, follow-up interviews and focus-group meetings. Between October 2020 and June 2021, 53 organisations completed the online survey, including 24 research institutes and universities, and 9 pharmaceutical companies; 24 organisations participated in interviews, and 14 in focus-group meetings. The arising recommendations covered all aspects of the vaccine-development value chain: from preclinical development to financing and business development; and covered prophylactic and therapeutic vaccines, for both human and veterinary indications. Overall, the recommendations supported the expansion and elaboration of services including training programmes, and improved or more extensive access to expertise, technologies, partnerships, curated databases, and-data analysis tools. Funding and financing featured as critical elements requiring support throughout the vaccine-development programmes, notably for academics and small companies, and for vaccine programmes that address medical and veterinary needs without a great potential for commercial gain. Centralizing the access to these research infrastructures via a single on-line portal was considered advantageous.}
}
@article{ZHANG2022112704,
title = {A review on occupancy prediction through machine learning for enhancing energy efficiency, air quality and thermal comfort in the built environment},
journal = {Renewable and Sustainable Energy Reviews},
volume = {167},
pages = {112704},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112704},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122005937},
author = {Wuxia Zhang and Yupeng Wu and John Kaiser Calautit},
keywords = {Occupancy prediction, Machine learning, Thermal comfort, Energy efficiency, Building model simulation, Occupancy detection},
abstract = {The occupants' presence, activities, and behaviour can significantly impact the building's performance and energy efficiency. Currently, heating, ventilation, and air-conditioning (HVAC) systems are often run based on assumed occupancy levels and fixed schedules, or manually set by occupants based on their comfort needs. However, the unpredictability and variability of occupancy patterns can lead to over/under the conditioning of space when using such approaches, affecting indoor air quality and comfort. As a result, machine learning-based models and methodologies are progressively being used to forecast occupancy behaviour and routines in buildings, which may subsequently be used to aid in the design and operation of building systems. The present work reviews recent studies employing machine learning methods to predict occupancy behaviour and patterns, with a special focus on its related applications and benefits to building systems, improving energy efficiency, indoor air quality and thermal comfort. The review provides insight into the workflow of a machine learning-based occupancy prediction model, including data collection, prediction, and validation. An organised evaluation of the applicability or suitability of the different data collection methods, machine learning algorithms, and validation methods was carried out.}
}
@article{WU2022,
title = {A GTFS data acquisition and processing framework and its application to train delay prediction},
journal = {International Journal of Transportation Science and Technology},
year = {2022},
issn = {2046-0430},
doi = {https://doi.org/10.1016/j.ijtst.2022.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2046043022000090},
author = {Jianqing Wu and Bo Du and Zengyang Gong and Qiang Wu and Jun Shen and Luping Zhou and Chen Cai},
keywords = {General transit feed specification, Delay prediction, Train delay, Long short-term memory, Data fusion},
abstract = {With advanced artificial intelligence and deep learning techniques, a growing number of data sources are playing more and more critical roles in planning and operating transportation services. The General Transit Feed Specification (GTFS), with standard open-source data in both static and real-time formats, is being widely used in public transport planning and operation management. However, compared to other extensively studied data sources such as smart card data and GPS trajectory data, the GTFS data lacks proper investigation yet. Utilization of the GTFS data is challenging for both transport planners and researchers due to its difficulty and complexity of understanding, processing, and leveraging the raw data. In this paper, a GTFS data acquisition and processing framework is proposed to offer an efficient and effective benchmark tool for converting and fusing the GTFS data to a ready-to-use format. To validate and test the proposed framework, a multivariate multistep Long Short-Term Memory is developed to predict train delay with minor anomaly in Sydney as a case study. The contribution of this new framework will render great potential for broader applications and deeper research.}
}
@article{DEMELO2022174,
title = {Next-generation deep learning based on simulators and synthetic data},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {2},
pages = {174-187},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S136466132100293X},
author = {Celso M. {de Melo} and Antonio Torralba and Leonidas Guibas and James DiCarlo and Rama Chellappa and Jessica Hodgins},
keywords = {deep neural networks, synthetic data, graphics-rendering pipelines, generative adversarial networks, domain adaptation, next-generation learning},
abstract = {Deep learning (DL) is being successfully applied across multiple domains, yet these models learn in a most artificial way: they require large quantities of labeled data to grasp even simple concepts. Thus, the main bottleneck is often access to supervised data. Here, we highlight a trend in a potential solution to this challenge: synthetic data. Synthetic data are becoming accessible due to progress in rendering pipelines, generative adversarial models, and fusion models. Moreover, advancements in domain adaptation techniques help close the statistical gap between synthetic and real data. Paradoxically, this artificial solution is also likely to enable more natural learning, as seen in biological systems, including continual, multimodal, and embodied learning. Complementary to this, simulators and deep neural networks (DNNs) will also have a critical role in providing insight into the cognitive and neural functioning of biological systems. We also review the strengths of, and opportunities and novel challenges associated with, synthetic data.}
}
@article{XIONG2022103139,
title = {Intelligent additive manufacturing and design state of the art and future perspectives},
journal = {Additive Manufacturing},
volume = {59},
pages = {103139},
year = {2022},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2022.103139},
url = {https://www.sciencedirect.com/science/article/pii/S2214860422005280},
author = {Yi Xiong and Yunlong Tang and Qi Zhou and Yongsheng Ma and David W. Rosen},
keywords = {Additive manufacturing, Intelligent manufacturing, Cyber-physical system, Integrated design and manufacturing},
abstract = {In additive manufacturing (AM), intelligent technologies are proving to be a powerful tool for facilitating economic, efficient, and effective decision-making within the product and service development. Such capabilities hold great promise to significantly improve the producibility, repeatability, and reproducibility of the additive manufacturing process and unlock its complete design freedom for product innovation. This paper defines the concept of intelligent additive manufacturing and design (IAMD) while providing a triple-layer model for reference. Details about these three layers, i.e., digital thread layer, cyber-physical layer, and intelligent service layer, are presented. Moreover, both scientific and engineering challenges raised during the studies and implementations of IAMD are discussed together with potential solutions. The paper also outlines the future perspective on IAMD towards the directions of integrated design and manufacturing, cyber-physical AM, advanced artificial intelligence for AM, digital materials and products, as well as design for AM process chain.}
}
@article{WEI2022120057,
title = {Dynamics of microbial communities during inulin fermentation associated with the temporal response in SCFA production},
journal = {Carbohydrate Polymers},
volume = {298},
pages = {120057},
year = {2022},
issn = {0144-8617},
doi = {https://doi.org/10.1016/j.carbpol.2022.120057},
url = {https://www.sciencedirect.com/science/article/pii/S0144861722009626},
author = {Siyu Wei and Cheng Wang and Qifan Zhang and Hui Yang and Edward C. Deehan and Xin Zong and Yizhen Wang and Mingliang Jin},
keywords = {Dietary fiber, Gut microbiota, Individual variability, Butyrate, Swine},
abstract = {The ecology driving the remodeling of gut microbial consortia with dietary fiber intervention remains incomplete. We investigated the short-term dynamics of the gut microbiota and metabolic function during inulin fermentation with distinct microbiota from two swine breeds using an in vitro fermentation model. Different gut microbiota at a transient temporal time displayed a similar response to inulin intervention such as the similar fermentation stage and a rapid response followed by gradual stabilization of microbial diversity. Inulin-induced bacterial succession and individual metabolic change were determined by the original microbial compositions, in particular the α-diversity. Levels of short-chain fatty acids (SCFAs) were predictable with the key bacteria by the regression model, especially butyrate was associated with the abundance and ecological interactions of Lactobacillus delbrueckii, Bifidobacterium thermophilum and Megasphaera elsdenii. This study emphasizes the importance of complex ecology to understand fiber-induced microbiome and metabolic changes, thus providing a reference for predictable dietary responses.}
}
@article{MARIKYAN2022572,
title = {“Alexa, let’s talk about my productivity”: The impact of digital assistants on work productivity},
journal = {Journal of Business Research},
volume = {142},
pages = {572-584},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S014829632200025X},
author = {Davit Marikyan and Savvas Papagiannidis and Omer F. Rana and Rajiv Ranjan and Graham Morgan},
keywords = {Digital assistant, Artificial intelligence, Digitalisation, Satisfaction, Job engagement, Productivity},
abstract = {Digital assistants based on artificial intelligence (AI) have been increasingly used in contexts beyond home-oriented services to support individuals in carrying out work-related tasks. Given the lack of empirical evidence on this fast-developing area, this paper aims (1) to explore the factors which can lead to individuals' satisfaction with the use of technology, and (2) to examine the impact of satisfaction on productivity and job engagement. The model was tested using 536 responses from individuals who used digital assistants for work purposes. Results showed that performance expectancy, perceived enjoyment, intelligence, social presence and trust were positively related to satisfaction with digital assistants. Satisfaction with the digital assistants was found to correlate with productivity and engagement. The findings contribute to the literature focusing on the use of AI-based technology supporting and complementing work tasks. They also offer practical recommendations as to how digital assistants could be used in the workplace.}
}
@article{GLASS2022234,
title = {The Role of Machine Learning in Cardiovascular Pathology},
journal = {Canadian Journal of Cardiology},
volume = {38},
number = {2},
pages = {234-245},
year = {2022},
issn = {0828-282X},
doi = {https://doi.org/10.1016/j.cjca.2021.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0828282X21008679},
author = {Carolyn Glass and Kyle J. Lafata and William Jeck and Roarke Horstmeyer and Colin Cooke and Jeffrey Everitt and Matthew Glass and David Dov and Michael A. Seidman},
abstract = {Machine learning has seen slow but steady uptake in diagnostic pathology over the past decade to assess digital whole-slide images. Machine learning tools have incredible potential to standardise, and likely even improve, histopathologic diagnoses, but they are not yet widely used in clinical practice. We describe the principles of these tools and technologies and some successful preclinical and pretranslational efforts in cardiovascular pathology, as well as a roadmap for moving forward. In nonhuman animal models, one proof-of-principle application is in rodent progressive cardiomyopathy, which is of particular significance to drug toxicity studies. Basic science successes include screening the quality of differentiated stem cells and characterising cardiomyocyte developmental stages, with potential applications for research and toxicology/drug safety screening using derived or native human pluripotent stem cells differentiated into cardiomyocytes. Translational studies of particular note include those with success in diagnosing the various forms of heart allograft rejection. For fully realising the value of these tools in clinical cardiovascular pathology, we identify 3 essential challenges. First is image quality standardisation to ensure that algorithms can be developed and implemented on robust, consistent data. The second is consensus diagnosis; experts don’t always agree, and thus “truth” may be difficult to establish, but the algorithms themselves may provide a solution. The third is the need for large-enough data sets to facilitate robust algorithm development, necessitating large cross-institutional shared image databases. The power of histopathology-based machine learning technologies is tremendous, and we outline the next steps needed to capitalise on this power.
Résumé
Au cours de la dernière décennie, l’apprentissage automatique a connu une adoption lente, mais constante, dans l’analyse des images numérisées de lamelle entière pour le diagnostic de maladies. Les outils d’apprentissage automatique ont un fabuleux potentiel de standardisation, voire d’amélioration, des diagnostics histopathologiques; ils ne sont toutefois pas encore largement utilisés en pratique clinique. Cet article vise à expliquer les principes de ces outils et technologies et à présenter certaines expériences précliniques et prétranslationnelles fructueuses en pathologie cardiovasculaire, de même qu’une feuille de route pour aller de l’avant. Dans les modèles animaux non humains, la cardiomyopathie évolutive chez le rongeur constitue une démonstration de principe qui revêt une importance particulière pour le succès des études toxicologiques. Les réussites en sciences fondamentales comprennent la sélection de cellules souches différenciées de qualité et la caractérisation des étapes du développement des cardiomyocytes, de même que leurs applications potentielles en recherche et en analyse toxicologique/de l’innocuité des médicaments à l’aide de cellules souches pluripotentes humaines ou dérivées se différenciant en cardiomyocytes. Les études translationnelles notables comprennent celles ayant permis de diagnostiquer les diverses formes de rejet d’allogreffe cardiaque. Pour pleinement comprendre la valeur de ces outils en pathologie clinique cardiovasculaire, nous avons repéré trois défis essentiels. Le premier est la standardisation de la qualité des images pour garantir que les algorithmes soient élaborés et mis en œuvre à partir de données solides et cohérentes. Le second est le diagnostic consensuel; les experts ne sont pas toujours d’accord, et par conséquent, il peut être difficile d’établir la « vérité », mais les algorithmes pourraient eux-mêmes fournir une solution. Le troisième est la nécessité de constituer des ensembles de données suffisantes pour favoriser l’élaboration d’algorithmes solides, ce qui nécessite de vastes bases de données d’images partagées entre les établissements. La puissance des technologies d’apprentissage automatique fondées sur l’histopathologie est immense; nous décrirons les prochaines étapes qui nous permettront de tirer profit de leur puissance.}
}
@article{PREGNOLATO2022104421,
title = {Towards Civil Engineering 4.0: Concept, workflow and application of Digital Twins for existing infrastructure},
journal = {Automation in Construction},
volume = {141},
pages = {104421},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104421},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002941},
author = {M. Pregnolato and S. Gunner and E. Voyagaki and R. {De Risi} and N. Carhart and G. Gavriel and P. Tully and T. Tryfonas and J. Macdonald and C. Taylor},
keywords = {Digital Twin, Civil Engineering, Infrastructure, Monitoring, Bridge},
abstract = {Digital Twins (DTs) are forecasted to be used in two-thirds of large industrial companies in the next decade. In the Architecture, Engineering and Construction (AEC) sector, their actual application is still largely at the prototype stage. Industry and academia are currently reconciling many competing definitions and unclear processes for developing DTs. There is a compelling need to establish DTs as practice in AEC by developing common procedures and standards tailored to the sector's procedures and use cases. This paper proposes a step-by-step workflow process for developing a DT for an existing asset in the built environment, providing a proof-of-concept case study based on the Clifton Suspension Bridge in Bristol (UK). To achieve its aim, this paper (i) reviews the state-of-the-art of DTs in Civil Engineering, (ii) proposes a working DT-based workflow framework for the built environment applicable to existing assets, (iii) applies the framework and develops of the physical-virtual architecture to a case study of bridge management, and finally (iv) discusses insights from the application. The main novelty lies in the development of a versatile methodological framework that can be applied to the broad context of civil infrastructure. This paper's importance resides in the knowledge challenge, value proposition and operation dictated by developing a DT workflow for the built environment, which ultimately represents a relevant use case for the digital transformation of national infrastructure.}
}
@article{NTAMO2022100025,
title = {Industry 4.0 in Action: Digitalisation of a Continuous Process Manufacturing for Formulated Products},
journal = {Digital Chemical Engineering},
volume = {3},
pages = {100025},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100025},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000163},
author = {D. Ntamo and E. Lopez-Montero and J. Mack and C. Omar and M.I. Highett and D. Moss and N. Mitchell and P. Soulatintork and P.Z. Moghadam and M. Zandi},
keywords = {Digital Twin, Smart Manufacturing, Continuous manufacturing, Formulated Products, Digital Chemical Industry, Data visualisation, Continuous wet granulation process},
abstract = {The pharmaceutical industry is going through a significant change to adopt smart manufacturing for more integrated supply chains and improved sustainability. Today's competitive market demands have put pressure on healthcare systems to take a comprehensive assessment of the drug life cycle, its environmental effect, industrial use of energy and resources, supply chain, and impact on end-users. The exploitation of emerging Industry 4.0 technologies will allow a sustainable process design and personalised health care system through the realisation of digital twins, which could transform the pharmaceutical sector to be more flexible, robust, adaptive, and smart. A significant level of research and development has been applied to pharmaceutical manufacturing especially in existing, outdated design and scale-up paradigms in isolated unit operations. However, addressing the key challenges in pharmaceutical manufacturing requires whole systems approaches to incorporate Industry 4.0 concepts. This paper aims to share the latest development of an advanced digital twin of a continuous wet granulation and tableting process at The University of Sheffield. These include the delivery of a digital platform consisting of an Advanced Process Control system (APC), mechanistic model platform and industrial IoT platform for data analytics and visualisation. The combined solution aligns with the concepts of Industry 4.0 by providing a digital twin, cloud integration, sophisticated statistical, as well as hybrid and mechanistic models. The models are in turn, used for soft-sensors, Model Predictive Control and Optimisation algorithms to predict and control product Quality Attributes. The potential application of digital twins in the pharmaceutical industry will also be explored.}
}
@article{RASOOL2022103332,
title = {Security and privacy of internet of medical things: A contemporary review in the age of surveillance, botnets, and adversarial ML},
journal = {Journal of Network and Computer Applications},
volume = {201},
pages = {103332},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103332},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000017},
author = {Raihan Ur Rasool and Hafiz Farooq Ahmad and Wajid Rafique and Adnan Qayyum and Junaid Qadir},
keywords = {Internet of things, Edge computing, Healthcare, Internet of medical things, Security, Privacy},
abstract = {Internet of Medical Things (IoMT) supports traditional healthcare systems by providing enhanced scalability, efficiency, reliability, and accuracy of healthcare services. It enables the development of smart hardware as well as software platforms that operate on the basis of communication systems and the algorithms that process the data collected by the sensors to support decision-making. Although IoMT is involved in large-scale services provisioning in the medical paradigm; however, the resource-constrained nature of these devices makes them vulnerable to immense security and privacy issues. These vulnerabilities are not only disastrous for IoMT but threaten the whole healthcare ecosystem, which can in turn bring human lives in danger. During the past few years, threat vectors against IoMT have been evolved in terms of scalability, complexity, and diversity, which makes it challenging to detect and provide stringent defense solutions against these attacks. In this paper, we classify security and privacy challenges against different IoMT variants based on their actual usage in the healthcare domain. We provide a comprehensive attack taxonomy on the overall IoMT infrastructure comprising different device variants as well as elaborate taxonomies of security protocols to mitigate attacks against different devices, algorithms and describe their strengths and weaknesses. We also outline the security and privacy requirements for the development of novel security solutions for all the attack types against IoMT. Finally, we provide a comprehensive list of current challenges and future research directions that must be considered while developing sustainable security solutions for the IoMT infrastructure.}
}
@article{NEWCOMER2022,
title = {Beyond Vaccination Coverage: Population-Based Measurement of Early Childhood Immunization Schedule Adherence},
journal = {Academic Pediatrics},
year = {2022},
issn = {1876-2859},
doi = {https://doi.org/10.1016/j.acap.2022.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1876285922004119},
author = {Sophia R. Newcomer and Jason M. Glanz and Matthew F. Daley},
keywords = {immunization schedule, vaccines, vaccine hesitancy},
abstract = {The immunization schedule recommended by the U.S. Advisory Committee on Immunization Practices (ACIP) provides a structure for how 10 different vaccine series should be administered to children in the first 18 months of life. Progress toward US early childhood immunization goals has largely focused on measuring vaccination coverage at age 24 months. However, standard vaccination coverage measures do not reflect whether children received vaccine doses by recommended ages, or whether vaccines were given concomitantly, per the schedule. In this paper, we describe innovations in population-level measurement of immunization schedule adherence through quantifying vaccination timeliness and undervaccination patterns. Measuring vaccination timeliness involves comparing when children received vaccine doses relative to ACIP age recommendations. To assess undervaccination patterns, children's vaccination histories are analyzed to determine whether they were vaccinated consistent with the ACIP schedule. Some patterns, such as spreading out vaccines across visits, are indicative of parental hesitancy. Other patterns, such as starting all recommended series but missing doses, are largely indicative of other immunization services delivery challenges. Since 2003, at least 12 studies have used National Immunization Survey-Child, immunization information system, or integrated health plan data to measure vaccination timeliness or undervaccination patterns at national or state levels. Moving forward, these novel measures can be leveraged for population-based surveillance of vaccine confidence, and for distinguishing undervaccination due to parental vaccine hesitancy from undervaccination due to other causes. Broader adoption of these measures can facilitate identification of targeted strategies for improving timely and routine early childhood vaccination uptake across the United States.}
}
@article{WANG2022104512,
title = {State of health estimation based on modified Gaussian process regression for lithium-ion batteries},
journal = {Journal of Energy Storage},
volume = {51},
pages = {104512},
year = {2022},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.104512},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X22005333},
author = {Jiwei Wang and Zhongwei Deng and Tao Yu and Akihiro Yoshida and Lijun Xu and Guoqing Guan and Abuliti Abudula},
keywords = {Lithium-ion battery, State of health, Health indicator, Data-driven, Gaussian processes regression},
abstract = {State of health (SOH) estimation of lithium-ion batteries is a challenging and crucial task for consumer electronics, electric vehicles, and micro-rids. This study presents a data-driven battery SOH estimation method based on a novel integrated Gaussian process regression (GPR) model. First, the aging characteristics of batteries are analyzed from multiple perspectives, and three health indicators (HIs) are extracted from battery charging and discharging curves. Then, the Pearson correlation analysis method is used to quantitatively analyze the correlation between the selected HIs and SOH. Next, a novel compound kernel function is proposed for battery SOH estimation, and different pairs of mean function and kernel function chosen from four mean functions and sixteen kernel functions are used to construct GPR models, and their estimation accuracy is compared subsequently. Finally, four different batteries with various initial health conditions from the NASA battery dataset are used to verify the performance of the proposed method. Experiments show that the method proposed in this paper has satisfactory estimation results in terms of accuracy, generalization ability, and robustness. Specifically, its estimated mean-absolute-error (MAE) and root-mean-square-error (RMSE) is only 1.7%, and 2.41%, respectively.}
}
@article{SHEN2022299,
title = {An improved method for investigating urban municipal infrastructures carrying capacity},
journal = {Sustainable Production and Consumption},
volume = {29},
pages = {299-310},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2021.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352550921002992},
author = {Liyin Shen and Xi Chen and Xiaoyun Du and Zhenchuan Yang},
keywords = {Urban municipal infrastructures carrying capacity (UMICC), UMICC carrier, UMICC load, Chinese large cities, Sustainable urban development},
abstract = {Urban municipal infrastructures play an essential role in urban social and economic development. However, the imbalance between the supply and demand of urban municipal infrastructures appears a common problem in cities. This presents an urgent need for a proper method to investigate the carrying capacity of urban municipal infrastructures in the process of pursuing sustainable urban development. In line with this background, this paper defines the concept of urban municipal infrastructures carrying capacity (UMICC) and establishes an improved method for investigating this capacity. The performances of UMICC are designed to four levels, namely, Low-Utilization, Medium-Utilization, High-Utilization, and Over-Utilization. The effectiveness of this method is proven through an empirical analysis by using the data collected from 35 large cities in China. The empirical results reveal that the utilization intensity of UMICC has been decreasing in recent years in the large Chinese cities, in which UMICC carrier has been increasing at a higher speed than that of UMICC load. In particular, some better-developed cities present a “Low-Utilization” performance level, which is considered that urban policy-makers in some metropolises in China should pay more attention to the better utilization of existing municipal infrastructures. By applying the improved UMICC method introduced in this study, decision-makers can identify the specific areas that affecting the UMICC performance, thus tailor-made policy can be designed for improving UMICC performance by adjusting UMICC carrier and load.}
}
@article{FISHER2022108836,
title = {BEAUT: An ExplainaBle Deep LEarning Model for Agent-Based PopUlations With Poor DaTa},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108836},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108836},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003987},
author = {Andrew Fisher and Bart Gajderowicz and Eric Latimer and Tim Aubry and Vijay Mago},
keywords = {Deep q-learning, Neural fitted q-iteration, Simulation, Homelessness, Agent-based},
abstract = {When working with an agent-based system, it may be desirable to develop an algorithm that can predict future time periods for each individual. One such approach that works to accomplish this is the Markov decision process, which takes as its input available actions, unique agent reward functions, and a model of the overall environment. However, it can be difficult to identify the underlying factors that influence decision making when attempting to simulate the behavior of real-world populations. For example, when modeling the transitions of homeless individuals between states such as street and shelter, it can be a challenging task as the external factors impacting them (e.g., weather) may not be readily apparent. Therefore, this paper proposes and evaluates an approach to capture this information in an explainable way from aggregate, real-life data produced by the At Home/Chez Soi project. The proposed algorithm “BEAUT” is a consolidation of a modified deep q-learning (MDQL) and modified neural fitted q-iteration (MNFQ) algorithm that work together to generate a set of probabilistic transition matrices to describe state transitions. BEAUT is evaluated with experimental results that compare its accuracy against similar methods for time-series forecasting using real-world data. Our tests show that BEAUT provides an explainable forecasting model without a loss of accuracy, and in some instances results in higher accuracy.}
}
@article{SUHAIL2022103699,
title = {Towards situational aware cyber-physical systems: A security-enhancing use case of blockchain-based digital twins},
journal = {Computers in Industry},
volume = {141},
pages = {103699},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103699},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000963},
author = {Sabah Suhail and Saif Ur Rehman Malik and Raja Jurdak and Rasheed Hussain and Raimundas Matulevičius and Davor Svetinovic},
keywords = {Anomaly detection, Blockchain, Cyber-Physical Systems (CPSs), Digital Twins (DTs), Industrial Control Systems (ICSs), Internet of Things (IoT), Industry 4.0},
abstract = {The complexity of cyberattacks in Cyber-Physical Systems (CPSs) calls for a mechanism that can evaluate critical infrastructures’ operational behaviour and security without affecting the operation of live systems. In this regard, Digital Twins (DTs) provide actionable insights through monitoring, simulating, predicting, and optimizing the state of CPSs. Through the use cases, including system testing and training, detecting system misconfigurations, and security testing, DTs strengthen the security of CPSs throughout the product lifecycle. However, such benefits of DTs depend on an assumption about data integrity and security. Data trustworthiness becomes more critical while integrating multiple components among different DTs owned by various stakeholders to provide an aggregated view of the complex physical system. This article envisions a blockchain-based DT framework as Trusted Twins for Securing Cyber-Physical Systems (TTS-CPS). With the automotive industry as a CPS use case, we demonstrate the viability of the TTS-CPS framework through a proof of concept. To utilize reliable system specification data for building the process knowledge of DTs, we ensure the trustworthiness of data-generating sources through Integrity Checking Mechanisms (ICMs). Additionally, Safety and Security (S&S) rules evaluated during simulation are stored and retrieved from the blockchain, thereby establishing more understanding and confidence in the decisions made by the underlying systems. Finally, we perform formal verification of the TTS-CPS.}
}
@article{ZHENG2022,
title = {Hybrid model of a cement rotary kiln using an improved attention-based recurrent neural network},
journal = {ISA Transactions},
year = {2022},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822000775},
author = {Jinquan Zheng and Liang Zhao and Wenli Du},
keywords = {Hybrid model, Rotary kiln, Attention mechanism, Recurrent neural network},
abstract = {A rotary kiln is core equipment in cement calcination. Significant time delay, time-varying, and nonlinear characteristics cause challenges in the advance process control and operational optimization of the rotary kiln. However, the traditional mechanism model with many assumptions cannot accurately represent the dynamic kiln process because kinetic parameters are difficult to obtain. This paper proposes a novel hybrid strategy to develop a dynamic model of a rotary kiln by combining a process mechanism and a recurrent neural network to address this issue. A time delay mechanism is used to estimate the kiln’s residence time to compensate for the time delay. A long short-term memory model that combines an attention mechanism and an ordinary differential equation solver is proposed to capture the time-varying and nonlinear behaviors of the kiln process. Case studies from two real-world cement plants with different processing loads are used to verify the effectiveness of the proposed hybrid modeling strategy. The results show that the proposed method has better accuracy and robustness than the traditional methods. The sensitivity analysis of the proposed model also makes it practical for t control system design and real-time optimization. }
}
@article{WIRTZ2022101685,
title = {Governance of artificial intelligence: A risk and guideline-based integrative framework},
journal = {Government Information Quarterly},
volume = {39},
number = {4},
pages = {101685},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101685},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000181},
author = {Bernd W. Wirtz and Jan C. Weyerer and Ines Kehl},
keywords = {Artificial intelligence, Risks, Guidelines, Governance, Regulation, Framework},
abstract = {This study addresses the growing challenge of governing artificial intelligence (AI) arising from the risks that it increasingly poses to the public sector and society. Based on an in-depth literature analysis, we first identify AI risks and guidelines and classify them into six categories, including technological, data, and analytical risks and guidelines, informational and communicational risks and guidelines, economic risks and guidelines, social risks and guidelines, ethical risks and guidelines, as well as legal and regulatory risks and guidelines. These risks and guidelines are then elaborated and transferred into a four-layered conceptual framework for AI governance. The framework interrelates AI risks and AI guidelines by means of a risk management and guidance process, resulting in an AI governance layer depicting the process for implementation of customised risk mitigation guidelines. The framework constitutes a comprehensive reference point for developing and implementing AI governance strategies and measures in the public sector.}
}
@article{QIN2022102691,
title = {Research and application of machine learning for additive manufacturing},
journal = {Additive Manufacturing},
volume = {52},
pages = {102691},
year = {2022},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2022.102691},
url = {https://www.sciencedirect.com/science/article/pii/S2214860422000963},
author = {Jian Qin and Fu Hu and Ying Liu and Paul Witherell and Charlie C.L. Wang and David W. Rosen and Timothy W. Simpson and Yan Lu and Qian Tang},
keywords = {Additive manufacturing, 3D Printing, Rapid prototyping, Machine learning, Deep learning, Digital manufacturing, Intelligent manufacturing},
abstract = {Additive manufacturing (AM) is poised to bring a revolution due to its unique production paradigm. It offers the prospect of mass customization, flexible production, on-demand and decentralized manufacturing. However, a number of challenges stem from not only the complexity of manufacturing systems but the demand for increasingly complex and high-quality products, in terms of design principles, standardization and quality control. These challenges build up barriers to the widespread adoption of AM in the industry and the in-depth research of AM in academia. To tackle the challenges, machine learning (ML) technologies rise to play a critical role as they are able to provide effective ways to quality control, process optimization, modelling of complex systems, and energy management. Hence, this paper employs a systematic literature review method as it is a defined and methodical way of identifying, assessing, and analysing published literature. Then, a keyword co-occurrence and cluster analysis are employed for analysing relevant literature. Several aspects of AM, including Design for AM (DfAM), material analytics, in situ monitoring and defect detection, property prediction and sustainability, have been clustered and summarized to present state-of-the-art research in the scope of ML for AM. Finally, the challenges and opportunities of ML for AM are uncovered and discussed.}
}
@article{GHADIMI2022121394,
title = {The successful implementation of industry 4.0 in manufacturing: An analysis and prioritization of risks in Irish industry},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121394},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121394},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521008258},
author = {Pezhman Ghadimi and Oisin Donnelly and Kubra Sar and Chao Wang and Amir Hossein Azadnia},
keywords = {Industry 4.0, Risk factor, Manufacturing, Best-worst method, Irish industry},
abstract = {Industry 4.0 is anticipated to revolutionize the manufacturing sector through a digital transformation. With this transformation, many benefits are expected, such as the automation and decentralization of production processes. Nevertheless, enterprises face considerable risks upon successful implementation of Industry 4.0. The uncertainties regarding these risks are currently hindering enterprises’ implementation of Industry 4.0. Although several studies have investigated the adoption of Industry 4.0-related technologies, far too little attention has been devoted to identifying and analyzing the risk factors associated with the adoption of these technologies in manufacturing, especially in Irish industry. Therefore, this study contributes to the existing knowledge by proposing a systematic approach to identifying and ranking these risk factors along with recommending policies to mitigate the highest risks. Fourteen risk factors are identified, and the opinions of 12 industry experts across the Irish manufacturing sector are used to rank these risk factors using an adjusted best-worst method. The lack of standards and lack of methodological approaches was the highest-ranking risk factor, with the risk to capital investment, the lack of talent, the uncertainty in economic benefits and the potential delay to the manufacturing process ranking in the top 5. Policy recommendations to mitigate the highest-ranking risks are proposed based on an analysis of the Irish government's current Industry 4.0 policy. Governments should aim to assist industries in establishing comprehensive standards to increase the rate of successful Industry 4.0 implementation.}
}
@article{LIAO2022102323,
title = {Simplified estimation modeling of land surface solar irradiation: A comparative study in Australia and China},
journal = {Sustainable Energy Technologies and Assessments},
volume = {52},
pages = {102323},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102323},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822003757},
author = {Xuan Liao and Rui Zhu and Man Sing Wong},
keywords = {Land surface solar irradiation, Machine learning, Cloud optical thickness, Aerosol optical thickness, Himawari-8 satellite images, Meteorological data},
abstract = {Solar irradiation maps are fundamental geospatial datasets that have been used in a variety of research fields. However, it is difficult to estimate the continuous distribution of solar irradiation over large areas accurately by using conventional interpolation or extrapolation methods based on only a few observation stations. To tackle this problem, this study proposed a method to estimate spatially continuous land surface solar irradiation based on four machine learning models, namely, Gradient Boosting Machine (GBM), Random Forest (RF), Support Vector Regression (SVR), and Multilayer Perceptron (MLP). Clear-sky solar irradiation data were computed based on time and location, cloud optical thickness (COT) and aerosol optical thickness (AOT) that significantly influence solar irradiation were retrieved from Himawari-8 meteorological satellite images, and land surface solar irradiation data were obtained from observation stations for training and evaluation. To explore the weather effects on land surface solar irradiation, air temperatures, humidity, wind, and atmospheric pressure were also quantified and integrated into the models. As a comparative study, this study collected six-year historical data and estimated solar distribution at a 5-km spatial resolution in Australia and China. Based on the coefficient of determination (R2), normalized Root Mean Square Error (nRMSE), normalized mean bias error (nMBE), and consumption of time (t), the results show that GBM achieved the highest accuracy with R2 >0.7 at all stations, followed by RF, SVR, and MLP. It suggests that the proposed method can provide an accurate and reliable estimation of land surface solar irradiation, compared with the theoretical solar irradiation without the obstacle of the atmosphere. The annual solar distribution maps created by the built methods indicate that the proposed method is simple and effective for large geographical regions and can be used worldwide when similar datasets are obtained.}
}
@article{MIRAMIRKHANI2022101679,
title = {Enabling 5G indoor services for residential environment using VLC technology},
journal = {Physical Communication},
volume = {53},
pages = {101679},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101679},
url = {https://www.sciencedirect.com/science/article/pii/S1874490722000465},
author = {Farshad Miramirkhani and Mehdi Karbalayghareh and Engin Zeydan and Rangeet Mitra},
keywords = {Visible light communication (VLC), Ray-tracing, Adaptive transmission, 5G services},
abstract = {Visible light communication (VLC) has emerged as a viable complement to traditional radio frequency (RF) based systems and as an enabler for high data rate communications for beyond-5G (B5G) indoor communication systems. In particular, the emergence of new B5G-based applications with quality of service (QoS) requirements and massive connectivity has recently led to research on the required service-levels and the development of improved physical (PHY) layer methods. As part of recent VLC standards development activities, the IEEE has formed the 802.11bb “Light Communications (LC) for Wireless Local Area Networking” standardization group. This paper investigates the network requirements of 5G indoor services such as virtual reality (VR) and high-definition (HD) video for residential environments using VLC. In this paper, we consider such typical VLC scenarios with additional impairments such as light-emitting diode (LED) nonlinearity and imperfect channel feedback, and propose hyperparameter-free mitigation techniques using Reproducing Kernel Hilbert Space (RKHS) methods. In this context, we also propose using a direct current biased optical orthogonal frequency division multiplexing (DCO-OFDM)-based adaptive VLC transmission method that uses precomputed bit error rate (BER) expressions for these RKHS-based detection methods and performs adaptive BER-based modulation-order switching. Simulations of channel impulse responses (CIRs) show that the adaptive transmission method provides significantly improved error rate performance, which makes it promising for high data rate VLC-based 5G indoor services.}
}
@article{MORAN2022794,
title = {Curious instance selection},
journal = {Information Sciences},
volume = {608},
pages = {794-808},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522007149},
author = {Michal Moran and Tom Cohen and Yuval Ben-Zion and Goren Gordon},
keywords = {Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Machine learning, Data science, Instance selection},
abstract = {In the process of building machine learning models data sometimes must be sampled before the learning process can be applied. This step, known as instance selection, is mostly done to reduce the amount of data in a volume that will allow the computing resources required for the learning phase to be reduced. In addition, it also removes noisy data that can affect the learning quality. While the two objectives are often in conflict, in most current approaches, it is impossible to control the balance between them. We propose a reinforcement learning-based approach for instance selection, called curious instance selection (CIS), which evaluates clusters of instances using the curiosity loop architecture. The output of the algorithm is a matrix that represents the value of adding a cluster of instances to existing instances. This matrix enables the computation of the Pareto front and demonstrates the ability to balance the noise and volume reduction objectives. CIS was evaluated on five datasets, and its performance was compared with the performance of three state-of-the-art algorithms. Our results show that CIS not only provides enhanced flexibility but also achieves higher effectiveness (reduction times accuracy). This approach strengthens the appeal of using curiosity-based algorithms in data science.}
}
@article{MATTHEWMAN2022101115,
title = {Systems to model the personalized aspects of microbiome health and gut dysbiosis},
journal = {Molecular Aspects of Medicine},
pages = {101115},
year = {2022},
issn = {0098-2997},
doi = {https://doi.org/10.1016/j.mam.2022.101115},
url = {https://www.sciencedirect.com/science/article/pii/S0098299722000607},
author = {Cristina Matthewman and Alexandra Narin and Hannah Huston and Christopher Edward Hopkins},
abstract = {The human gut microbiome is a complex and dynamic microbial entity that interacts with the environment and other parts of the body including the brain, heart, liver, and immune system. These multisystem interactions are highly conserved from invertebrates to humans, however the complexity and diversity of human microbiota compositions often yield a context that is unique to each individual. Yet commonalities remain across species, where a healthy gut microbiome will be rich in symbiotic commensal biota while an unhealthy gut microbiota will be experiencing abnormal blooms of pathobiont bacteria. In this review we discuss how omics technologies can be applied in a personalized approach to understand the microbial crosstalk and microbial-host interactions that affect the delicate balance between eubiosis and dysbiosis in an individual gut microbiome. We further highlight the strengths of model organisms in identifying and characterizing these conserved synergistic and/or pathogenic host-microbe interactions. And finally, we touch upon the growing area of personalized therapeutic interventions targeting gut microbiome.}
}
@article{YANG2022100731,
title = {Nodes clustering and multi-hop routing protocol optimization using hybrid chimp optimization and hunger games search algorithms for sustainable energy efficient underwater wireless sensor networks},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100731},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100731},
url = {https://www.sciencedirect.com/science/article/pii/S221053792200066X},
author = {Yang Yang and Yunqiang Wu and Hongji Yuan and Mohammad Khishe and Mokhtar Mohammadi},
keywords = {Clustering, Chimp optimization algorithm, Routing, Hierarchical, Energy efficiency},
abstract = {Clustering and routing processes in underwater wireless sensor networks (UWSNs) are challenging tasks in the underwater environment due to the multiplicity of sensor nodes, transmission bandwidth, and limited energy resources. In order to address the shortcomings mentioned above, this paper proposes a novel hybrid Chimp Optimization and Hunger Games Search (ChOA-HGS) algorithms for clustering and multi-hop routing optimization in UWSNs. In this approach, first, the ChOA is used to choose cluster heads and efficiently structure clusters. Then, the HGS-based routing procedure is used to determine the network’s best pathways. The proposed approach combines the advantages of clustering and routing, resulting in optimal network lifetime and energy efficiency. The proposed ChOA-HGS is validated using a variety of measures after it is simulated using three different scenarios. In order to evaluate the performance of the ChOA-HGS, results are compared to PSO, MPSO, IPSO-GWO, TEEN, and LEACH. The results show that the ChOA-HGS outperformed other benchmarks in terms of lifetime and energy consumption.}
}
@article{KANG2022112113,
title = {How to better share energy towards a carbon-neutral city? A review on application strategies of battery energy storage system in city},
journal = {Renewable and Sustainable Energy Reviews},
volume = {157},
pages = {112113},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112113},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122000429},
author = {Hyuna Kang and Seunghoon Jung and Minhyun Lee and Taehoon Hong},
keywords = {, , , , },
abstract = {Despite the recent market growth and price reduction of technologies for a battery energy storage system (BESS), many technological, operational, and managerial challenges still need to be overcome to improve the system's feasibility and commercialization. Many studies suggested various solutions for applying BESS to the grid system, yet the existing literature is dispersed across different scopes, scales, and levels. Previous studies also lack a comprehensive overview of different application strategies of BESS within the urban landscape. Therefore, this study aimed to broaden our perspective and implement the concept of “BESS-based energy sharing” for carbon-neutrality at the urban level. Towards this end, this study investigated the key factors and main technologies for BESS application in the city through a comprehensive literature review. These results indicated that the limitations of current application technologies of the BESS and energy sharing using BESS should be overcome in the carbon-neutral city. As a result, this study proposed future direction of the BESS application strategies towards smart energy sharing in cities, as follows: (i) bi-directional interactions across hierarchical levels of the city; (ii) integrated management for high connectivity of distributed generation; and (iii) development of a BESS-based energy management system. This study would allow scholars, researchers, practitioners, and policymakers to better understand the energy sharing mechanism within the city and provide systematic guidelines and pathways towards smart energy sharing with the advanced energy infrastructure using BESS from a bi-directional perspective across different hierarchical levels of the city.}
}
@article{NAHAVANDI2022106541,
title = {Application of artificial intelligence in wearable devices: Opportunities and challenges},
journal = {Computer Methods and Programs in Biomedicine},
volume = {213},
pages = {106541},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106541},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721006155},
author = {Darius Nahavandi and Roohallah Alizadehsani and Abbas Khosravi and U Rajendra Acharya},
keywords = {Wearable devices, Healthcare, Machine learning, Deep learning, Internet of things},
abstract = {Background and objectives
: Wearable technologies have added completely new and fast emerging tools to the popular field of personal gadgets. Aside from being fashionable and equipped with advanced hardware technologies such as communication modules and networking, wearable devices have the potential to fuel artificial intelligence (AI) methods with a wide range of valuable data.
Methods
: Various AI techniques such as supervised, unsupervised, semi-supervised and reinforcement learning (RL) have already been used to carry out various tasks. This paper reviews the recent applications of wearables that have leveraged AI to achieve their objectives.
Results
: Particular example applications of supervised and unsupervised learning for medical diagnosis are reviewed. Moreover, examples combining the internet of things, wearables, and RL are reviewed. Application examples of wearables will be also presented for specific domains such as medical, industrial, and sport. Medical applications include fitness, movement disorder, mental health, etc. Industrial applications include employee performance improvement with the aid of wearables. Sport applications are all about providing better user experience during workout sessions or professional gameplays.
Conclusion
: The most important challenges regarding design and development of wearable devices and the computation burden of using AI methods are presented. Finally, future challenges and opportunities for wearable devices are presented.}
}
@article{KASPRZYKHORDERN2022107143,
title = {Wastewater-based epidemiology in hazard forecasting and early-warning systems for global health risks},
journal = {Environment International},
volume = {161},
pages = {107143},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2022.107143},
url = {https://www.sciencedirect.com/science/article/pii/S0160412022000691},
author = {B. Kasprzyk-Hordern and B. Adams and I.D. Adewale and F.O. Agunbiade and M.I. Akinyemi and E. Archer and F.A. Badru and J. Barnett and I.J. Bishop and M. {Di Lorenzo} and P. Estrela and J. Faraway and M.J. Fasona and S.A. Fayomi and E.J. Feil and L.J. Hyatt and A.T. Irewale and T. Kjeldsen and A.K.S. Lasisi and S. Loiselle and T.M. Louw and B. Metcalfe and S.A. Nmormah and T.O. Oluseyi and T.R. Smith and M.C. Snyman and T.O. Sogbanmu and D. Stanton-Fraser and S. Surujlal-Naicker and P.R. Wilson and G. Wolfaardt and C.O. Yinka-Banjo},
keywords = {Early warning system, Global health, Urban water fingerprinting, Wastewater-based epidemiology, Socio-economic fingerprints, Citizen science},
abstract = {With the advent of the SARS-CoV-2 pandemic, Wastewater-Based Epidemiology (WBE) has been applied to track community infection in cities worldwide and has proven succesful as an early warning system for identification of hotspots and changingprevalence of infections (both symptomatic and asymptomatic) at a city or sub-city level. Wastewater is only one of environmental compartments that requires consideration. In this manuscript, we have critically evaluated the knowledge-base and preparedness for building early warning systems in a rapidly urbanising world, with particular attention to Africa, which experiences rapid population growth and urbanisation. We have proposed a Digital Urban Environment Fingerprinting Platform (DUEF) – a new approach in hazard forecasting and early-warning systems for global health risks and an extension to the existing concept of smart cities. The urban environment (especially wastewater) contains a complex mixture of substances including toxic chemicals, infectious biological agents and human excretion products. DUEF assumes that these specific endo- and exogenous residues, anonymously pooled by communities’ wastewater, are indicative of community-wide exposure and the resulting effects. DUEF postulates that the measurement of the substances continuously and anonymously pooled by the receiving environment (sewage, surface water, soils and air), can provide near real-time dynamic information about the quantity and type of physical, biological or chemical stressors to which the surveyed systems are exposed, and can create a risk profile on the potential effects of these exposures. Successful development and utilisation of a DUEF globally requires a tiered approach including: Stage I: network building, capacity building, stakeholder engagement as well as a conceptual model, followed by Stage II: DUEF development, Stage III: implementation, and Stage IV: management and utilization. We have identified four key pillars required for the establishment of a DUEF framework: (1) Environmental fingerprints, (2) Socioeconomic fingerprints, (3) Statistics and modelling and (4) Information systems. This manuscript critically evaluates the current knowledge base within each pillar and provides recommendations for further developments with an aim of laying grounds for successful development of global DUEF platforms.}
}
@article{EGGER2022106874,
title = {Medical deep learning—A systematic meta-review},
journal = {Computer Methods and Programs in Biomedicine},
volume = {221},
pages = {106874},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106874},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722002565},
author = {Jan Egger and Christina Gsaxner and Antonio Pepe and Kelsey L. Pomykala and Frederic Jonske and Manuel Kurz and Jianning Li and Jens Kleesiek},
keywords = {Deep learning, Artificial neural networks, Machine learning, Data analysis, Image analysis, Medical image analysis, Medical image processing, Medical imaging, Patient data, Pathology, Detection, Segmentation, Registration, Generative adversarial networks, PubMed, Systematic, Review, Survey, Meta-review, Meta-survey},
abstract = {Deep learning has remarkably impacted several different scientific disciplines over the last few years. For example, in image processing and analysis, deep learning algorithms were able to outperform other cutting-edge methods. Additionally, deep learning has delivered state-of-the-art results in tasks like autonomous driving, outclassing previous attempts. There are even instances where deep learning outperformed humans, for example with object recognition and gaming. Deep learning is also showing vast potential in the medical domain. With the collection of large quantities of patient records and data, and a trend towards personalized treatments, there is a great need for automated and reliable processing and analysis of health information. Patient data is not only collected in clinical centers, like hospitals and private practices, but also by mobile healthcare apps or online websites. The abundance of collected patient data and the recent growth in the deep learning field has resulted in a large increase in research efforts. In Q2/2020, the search engine PubMed returned already over 11,000 results for the search term ‘deep learning’, and around 90% of these publications are from the last three years. However, even though PubMed represents the largest search engine in the medical field, it does not cover all medical-related publications. Hence, a complete overview of the field of ‘medical deep learning’ is almost impossible to obtain and acquiring a full overview of medical sub-fields is becoming increasingly more difficult. Nevertheless, several review and survey articles about medical deep learning have been published within the last few years. They focus, in general, on specific medical scenarios, like the analysis of medical images containing specific pathologies. With these surveys as a foundation, the aim of this article is to provide the first high-level, systematic meta-review of medical deep learning surveys.}
}
@article{IBRAHIM2022100063,
title = {A mapping towards a unified municipal platform: An investigative case study from a Norwegian municipality},
journal = {Sustainable Futures},
volume = {4},
pages = {100063},
year = {2022},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2022.100063},
url = {https://www.sciencedirect.com/science/article/pii/S2666188822000016},
author = {Ahmed M. Ibrahim},
keywords = {Digital transformation, Governance, Municipality, Platforms, Smart city, Sustainability, Sustainable development goals (SDGS)},
abstract = {Norwegian municipalities set citizens’ wellbeing a top priority, through transparent social practices and democratic conduct of public services. In 2015, the United Nations (UN) have set a collection of 17 sustainable development goals (SDGs), which are interlinked towards achieving a sustainable future by 2030. In smart sustainable cities context, a multi-stakeholder, physical and virtual platforms are considered pivotal to achieve the UN SDGs. They are core initiator for transformative actions, endeavoring on public service excellence. This research aims to investigate the digital governance platforms utilized, by a case study Norwegian municipality, to achieve the UN SDGs; towards a smart sustainable city. An empirical systematic approach was followed. The approach commenced by the conduct of a 1) narrative review and synthesis of the literature, and the municipal strategic development documents, 2) collection of a developed semi-structured survey, and 3) interviews with the municipal decision makers (including directors, and departmental professional advisors). As part of the study, broad aspects related to municipal governance and services have been discussed with the interviewees, where effectiveness measures have been identified and prioritized. Further, recommendations for improving the municipal platforms and initiatives were identified and discussed. The municipal investigated platforms and the strategic documents have led to formulating a novel conceptual model for the behavior needed towards making a unified municipal platform. Discussion and recommendations were presented to identify future development prospects, towards achieving a transformative integrated ecosystem. Further, the research presents main concepts, to articulate organizational administrative practices, and to inform municipal stakeholders, decision makers and professionals about the existing digital platforms, while proposing a model for a unified municipal governance transformation.}
}
@article{KONG2022,
title = {Process Intensification from conventional to advanced distillations: Past, present, and future},
journal = {Chemical Engineering Research and Design},
year = {2022},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2022.09.056},
url = {https://www.sciencedirect.com/science/article/pii/S0263876222005536},
author = {Zong Yang Kong and Eduardo Sánchez-Ramírez and Ao Yang and Weifeng Shen and Juan Gabriel Segovia-Hernández and Jaka Sunarso},
keywords = {Process intensification, Advanced distillation, Reactive distillation, Extractive distillation, Reactive-extractive distillation, Sustainability},
abstract = {This perspective paper features the process intensification (PI) application for advanced distillation-based processes. Starting with the historical background of generic PI, we subsequently narrow down the discussion to extractive distillation (ED), reactive distillation (RD), and hybrid reactive-extractive distillation (RED). We categorize the existing PI techniques onto internal and external intensification, where the former does not involve altering the distillation configuration while the latter does. Instead of deliberating the technical aspects, we explicitly highlight the contribution of PI applied to ED, RD, and RED towards societal impact covering energy, economic, environmental, control, and safety perspectives. The future perspectives of PI are discussed in the last section, covering the development of hybrid PI technologies, exploring the energy efficiency of different PI configurations, prioritizing PI beyond energy by considering some other sustainability aspects, and linking PI with the ever-increasing Industry 4.0 applications.}
}
@article{JEONG2022105451,
title = {Application of ToxCast/Tox21 data for toxicity mechanism-based evaluation and prioritization of environmental chemicals: Perspective and limitations},
journal = {Toxicology in Vitro},
volume = {84},
pages = {105451},
year = {2022},
issn = {0887-2333},
doi = {https://doi.org/10.1016/j.tiv.2022.105451},
url = {https://www.sciencedirect.com/science/article/pii/S0887233322001497},
author = {Jaeseong Jeong and Donghyeon Kim and Jinhee Choi},
keywords = {ToxCast/Tox21, Bioassay, Toxicity mechanism, Prioritization, Toxicity prediction, Risk assessment},
abstract = {In response to the need to minimize the use of experimental animals, new approach methodologies (NAMs) using advanced technology have emerged in the 21st century. ToxCast/Tox21 aims to evaluate the adverse effects of chemicals quickly and efficiently using a high-throughput screening and to transform the paradigm of toxicity assessment into mechanism-based toxicity prediction. The ToxCast/Tox21 database, which contains extensive data from over 1400 assays with numerous biological targets and activity data for over 9000 chemicals, can be used for various purposes in the field of chemical prioritization and toxicity prediction. In this study, an overview of the database was explored to aid mechanism-based chemical prioritization and toxicity prediction. Implications for the utilization of the ToxCast/Tox21 database in chemical prioritization and toxicity prediction were derived. The research trends in ToxCast/Tox21 assay data were reviewed in the context of toxicity mechanism identification, chemical priority, environmental monitoring, assay development, and toxicity prediction. Finally, the potential applications and limitations of using ToxCast/Tox21 assay data in chemical risk assessment were discussed. The analysis of the toxicity mechanism-based assays of ToxCast/Tox21 will help in chemical prioritization and regulatory applications without the use of laboratory animals.}
}
@article{FREDRIKSSON2022104038,
title = {Construction related urban disturbances: Identification and linking with an IoT-model},
journal = {Automation in Construction},
volume = {134},
pages = {104038},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104038},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004891},
author = {Anna Fredriksson and Ahmet Anil Sezer and Vangelis Angelakis and David Gundlegård},
keywords = {Construction transport, IoT modelling, Construction disturbances, Stakeholder management},
abstract = {While being a significant part of the urban development, construction projects disturb different stakeholders in various ways. There are three problems associated with construction disturbances: (i) most of these disturbances are not recognised by the people causing them, (ii) they are not monitored and (iii) if they are to be monitored, data is spread among stakeholders. This paper defines what a disturbance is, presents a list of disturbances, linking disturbances to stakeholders and, categorising them based on their distance from construction sites (responding to (i)). Next, a IoT domain model is developed, demonstrating how IoT in construction needs to be combined with the sensors of smart cities to capture the primitives of these disturbances (responding to (iii)). This is a first step towards enabling large-scale data-gathering of construction transport disturbances (responding to (ii)), which is a necessity to predict them and allow better construction transport planning to decrease disturbances.}
}
@article{HUSAK2022102609,
title = {CRUSOE: A toolset for cyber situational awareness and decision support in incident handling},
journal = {Computers & Security},
volume = {115},
pages = {102609},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102609},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822000086},
author = {Martin Husák and Lukáš Sadlek and Stanislav Špaček and Martin Laštovička and Michal Javorník and Jana Komárková},
keywords = {Cyber situational awareness, OODA Loop, Decision support, Network monitoring, Incident response},
abstract = {The growing size and complexity of today’s computer network make it hard to achieve and maintain so-called cyber situational awareness, i.e., the ability to perceive and comprehend the cyber environment and be able to project the situation in the near future. Namely, the personnel of cybersecurity incident response teams or security operation centers should be aware of the security situation in the network to effectively prevent or mitigate cyber attacks and avoid mistakes in the process. In this paper, we present a toolset for achieving cyber situational awareness in a large and heterogeneous environment. Our goal is to support cybersecurity teams in iterating through the OODA loop (Observe, Orient, Decide, Act). We designed tools to help the operator make informed decisions in incident handling and response for each phase of the cycle. The Observe phase builds on common tools for active and passive network monitoring and vulnerability assessment. In the Orient phase, the data on the network are structured and presented in a comprehensible and visually appealing manner. The Decide phase opens opportunities for decision-support systems, in our case, a recommender system that suggests the most resilient configuration of the critical infrastructure. Finally, the Act phase is supported by a service that orchestrates network security tools and allows for prompt mitigation actions. Finally, we present lessons learned from the deployment of the toolset in the campus network and the results of a user evaluation study.}
}
@article{HATAMZAD2022108682,
title = {Intelligent cost-effective winter road maintenance by predicting road surface temperature using machine learning techniques},
journal = {Knowledge-Based Systems},
volume = {247},
pages = {108682},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108682},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003136},
author = {Mahshid Hatamzad and Geanette Cleotilde Polanco Pinerez and Johan Casselgren},
keywords = {Decision-making units, Decision support systems, Machine learning techniques, Road surface temperature, Winter road maintenance},
abstract = {Since Winter Road Maintenance (WRM) is an important activity in Nordic countries, accurate intelligent cost-effective WRM can create precise advance plans for developing decision support systems to improve traffic safety on the roads, while reducing cost and negative environmental impacts. Lack of comprehensive knowledge and inaccurate WRM information would lead to a certain loss of WRM budget, safety reduction, and irreparable environmental damage. This study proposes an intelligent methodology that uses data envelopment analysis and machine learning techniques. In the proposed methodology, WRM efficiency is calculated by data envelopment analysis for different decision-making units (roads), and inefficient units need to be considered for further assessments. Therefore, road surface temperature is predicted by means of machine learning methods, in order to achieve efficient and effective WRM on the roads during winter in cold regions. In total, four different methods have been used to predict road surface temperature on an inefficient road. One of these is linear regression, which is a classical statistical regression technique (ordinary least square regression); the other three methods are machine-learning techniques, including support vector regression, multilayer perceptron artificial neural network, and random forest regression. Graphical and numerical results indicate that support vector regression is the most accurate method.}
}
@article{MENDES2022157428,
title = {Evaluating the BFAST method to detect and characterise changing trends in water time series: A case study on the impact of droughts on the Mediterranean climate},
journal = {Science of The Total Environment},
volume = {846},
pages = {157428},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.157428},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722045260},
author = {Maria Paula Mendes and Victor Rodriguez-Galiano and David Aragones},
keywords = {Water resources, LOESS, BFAST, Time series analysis, Climate variability, Climate resilience},
abstract = {Mediterranean climate regions are facing increased aridity conditions and water scarcity, thus needing integrated management of water resources. Detecting and characterising changes in water resources over time is the natural first step towards identifying the drivers of these changes and understanding the mechanism of change. The aim of this study is to evaluate the potential of Breaks For Additive Seasonal and Trend (BFAST) method to identify gradual (trend) and abrupt (step- change) changes in the freshwater resources time series over a long-term period. This research shows an alternative to the Pettitt's test, LOESS (locally estimated scatterplot smoothing) filter, Mann–Kendall trend test among other common methods for change detection in hydrological data, and paves the way for further scientific investigation related to climate variability and its influence on water resources. We used the monthly accumulated stored water in three reservoirs, the monthly groundwater levels of three hydrological settings and a standardized precipitation index to show BFAST performance. BFAST was successfully applied, enabling: (1) assessment of the suitability of past management decisions when tackling drought events; (2) detection of recovery and drawdown periods (duration and magnitude values) of accumulated stored water in reservoirs and groundwater bodies after wet and dry periods; 3) measurement of resilience to drought conditions; (4) establishment of similarities/differences in trends between different reservoirs and groundwater bodies with regard to drought events.}
}
@article{DIETRICH202253,
title = {Towards a model for holistic mapping of supply chains by means of tracking and tracing technologies},
journal = {Procedia CIRP},
volume = {107},
pages = {53-58},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002256},
author = {Fabian Dietrich and Moritz Hoffmann and Mario Angos Mediavilla and Louis Louw and Daniel Palm},
keywords = {Supply Chain Mapping, Tracking, Tracing, Transparency, Industry 4.0},
abstract = {The usage of tracking and tracing technologies not only enables transparency and visibility of supply chains but also offers far-reaching advantages for companies, such as ensuring product quality or reducing supplier risks. Increasing the amount of shared information supports both internal and external planning processes as well as the stability and resilience of globally operating value chains. This paper aims to differentiate and define the functionalities of tracking and tracing technologies that are frequently used interchangeably in literature. Furthermore, this paper incorporates influencing factors impacting a sequencing of the connected world in Industry4.0 supply chain networks. This includes legal influences, the embedment of supply chain-related standards, and new possibilities of emerging technologies. Finally, the results are summarized in a model for the holistic mapping of supply chains by means of tracking and tracing technologies. The resulting technological solutions that can be derived from the model enable companies to address missing elements in order to enable the holistic mapping of supply chain events as well as the transparent representation of a digital shadow throughout the entire supply chain.}
}
@article{LI2022111817,
title = {Distance measures in building informatics: An in-depth assessment through typical tasks in building energy management},
journal = {Energy and Buildings},
volume = {258},
pages = {111817},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111817},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821011014},
author = {Ao Li and Cheng Fan and Fu Xiao and Zhijie Chen},
keywords = {Distance measure, Clustering, Pattern recognition, Time-series analysis},
abstract = {Distance measurement (also known as similarity measurement) is used to evaluate pairwise similarities between data samples. It has been widely used in diverse building informatics research and applications to classify or cluster massive building data with the aim of improving prediction accuracy, identifying operation patterns, benchmarking and diagnosing building performance, etc. Various distance measures have been adopted to measure the distance/similarity of building data. However, the intrinsic complexity and diversity of building operational data bring considerable difficulties to the selection of a suitable distance measure for a specific task. There is a strong and urgent need for a comprehensive review and systematic comparison of existing distance measures in building informatics. This study provides a comprehensive review of various distance measures and their applications in building operational data analysis. A systematic comparison is undertaken based on two typical tasks relying on building informatics, i.e., building energy usage pattern recognition, and clustering-based weather data segmentation for the customized development of building energy prediction models. Nine widely adopted distance measures have been reviewed and compared, including Euclidean distance, Chebyshev distance, Manhattan distance, Mahalanobis distance, Hausdorff distance, Pearson correlation distance, Dynamic Time Warping, Edit distance on Real Sequence, and Cosine distance. Novel internal and external clustering validation approaches based on the cross-test and prediction accuracy are proposed and adopted to compare the clustering performance. The results in case studies showed that weather data clustering using the Cosine distance and Pearson correlation distance helps to obtain better energy prediction results in terms of MAPE (13.22% and 12.91%, respectively) than the commonly-used Euclidean distance (13.99%). The results also revealed that better clustering performance does not necessarily lead to higher prediction accuracy. The research results and insights obtained are valuable to guide distance-based research in building informatics.}
}
@article{ALEXANDERWHITE2022105094,
title = {A 10-step framework for use of read-across (RAX) in next generation risk assessment (NGRA) for cosmetics safety assessment},
journal = {Regulatory Toxicology and Pharmacology},
volume = {129},
pages = {105094},
year = {2022},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2021.105094},
url = {https://www.sciencedirect.com/science/article/pii/S027323002100235X},
author = {Camilla Alexander-White and Dagmar Bury and Mark Cronin and Matthew Dent and Eric Hack and Nicola J. Hewitt and Gerry Kenna and Jorge Naciff and Gladys Ouedraogo and Andreas Schepky and Catherine Mahony and Cosmetics Europe},
keywords = {Next generation read-across (RAX), New approach methodology (NAM), Next generation risk assessment (NGRA), Cosmetics safety assessment, Systemic toxicity, Physiologically-based biokinetic modelling (PBK), Caffeine, Parabens},
abstract = {This paper presents a 10-step read-across (RAX) framework for use in cases where a threshold of toxicological concern (TTC) approach to cosmetics safety assessment is not possible. RAX builds on established approaches that have existed for more than two decades using chemical properties and in silico toxicology predictions, by further substantiating hypotheses on toxicological similarity of substances, and integrating new approach methodologies (NAM) in the biological and kinetic domains. NAM include new types of data on biological observations from, for example, in vitro assays, toxicogenomics, metabolomics, receptor binding screens and uses physiologically-based kinetic (PBK) modelling to inform about systemic exposure. NAM data can help to substantiate a mode/mechanism of action (MoA), and if similar chemicals can be shown to work by a similar MoA, a next generation risk assessment (NGRA) may be performed with acceptable confidence for a data-poor target substance with no or inadequate safety data, based on RAX approaches using data-rich analogue(s), and taking account of potency or kinetic/dynamic differences.}
}
@article{COUET2022153425,
title = {Integrated high-throughput research in extreme environments targeted toward nuclear structural materials discovery},
journal = {Journal of Nuclear Materials},
volume = {559},
pages = {153425},
year = {2022},
issn = {0022-3115},
doi = {https://doi.org/10.1016/j.jnucmat.2021.153425},
url = {https://www.sciencedirect.com/science/article/pii/S0022311521006450},
author = {Adrien Couet},
abstract = {Arguably one of the most important factors in the fast deployment of advanced nuclear reactors, with major improvements in safety, is the development and qualification of radiation and corrosion tolerant materials, that serve as the structural components in reactor cores. However, the discovery, improvement, and assessment of materials resistant to radiation and corrosion in the advanced reactors’ extreme environments is quite demanding, time-consuming, and costly, which represents a significant barrier to materials innovation and qualification for nuclear energy. This short review highlights a novel, integrated, high-throughput (HTP) research framework to develop understanding and predictive models for irradiation at high doses and molten salt corrosion responses of structural Compositionally Complex Alloys (CCAs), with the objective to accelerate materials discovery for high-temperature nuclear structural applications. Using a novel in situ alloying technique, arrays of additively manufactured bulk CCAs are processed, heat-treated, and characterized, while still attached to the build plate. Leveraging recent development in automation of heavy-ion irradiation experiments at the University of Wisconsin Ion Beam Laboratory, arrays of CCAs can be rapidly irradiated to hundreds of dpa up to 800 °C. An innovative droplet corrosion method is also used to test molten salt corrosion behavior of CCAs arrays. Automated and rapid characterization methods are used to assess the irradiation and molten salt corrosion resistance of CCAs. Finally, a brief discussion of the results is presented considering future use of machine-learning-based methods to develop useful trends and highlight features of importance. Using this novel HTP approach, a robust and reliable database containing literally hundreds of data points for irradiation and corrosion responses of CCAs can be established within a year, which is considered a significant increase in the pace of nuclear structural materials research and discovery.}
}
@article{ZHU2022116187,
title = {An ensemble machine learning model for water quality estimation in coastal area based on remote sensing imagery},
journal = {Journal of Environmental Management},
volume = {323},
pages = {116187},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.116187},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722017601},
author = {Xiaotong Zhu and Hongwei Guo and Jinhui Jeanne Huang and Shang Tian and Wang Xu and Youquan Mai},
keywords = {Machine learning, Ensemble model, Water quality, Remote sensing, Coastal area},
abstract = {The accurate estimation of coastal water quality parameters (WQPs) is crucial for decision-makers to manage water resources. Although various machine learning (ML) models have been developed for coastal water quality estimation using remote sensing data, the performance of these models has significant uncertainties when applied to regional scales. To address this issue, an ensemble ML-based model was developed in this study. The ensemble ML model was applied to estimate chlorophyll-a (Chla), turbidity, and dissolved oxygen (DO) based on Sentinel-2 satellite images in Shenzhen Bay, China. The optimal input features for each WQP were selected from eight spectral bands and seven spectral indices. A local explanation strategy termed Shapley Additive Explanations (SHAP) was employed to quantify contributions of each feature to model outputs. In addition, the impacts of three climate factors on the variation of each WQP were analyzed. The results suggested that the ensemble ML models have satisfied performance for Chla (errors = 1.7%), turbidity (errors = 1.5%) and DO estimation (errors = 0.02%). Band 3 (B3) has the highest positive contribution to Chla estimation, while Band Ration Index2 (BR2) has the highest negative contribution to turbidity estimation, and Band 7 (B7) has the highest positive contribution to DO estimation. The spatial patterns of the three WQPs revealed that the water quality deterioration in Shenzhen Bay was mainly influenced by input of terrestrial pollutants from the estuary. Correlation analysis demonstrated that air temperature (Temp) and average air pressure (AAP) exhibited the closest relationship with Chla. DO showed the strongest negative correlation with Temp, while turbidity was not sensitive to Temp, average wind speed (AWS), and AAP. Overall, the ensemble ML model proposed in this study provides an accurate and practical method for long-term Chla, turbidity, and DO estimation in coastal waters.}
}
@article{HU2022128558,
title = {Linking electron ionization mass spectra of organic chemicals to toxicity endpoints through machine learning and experimentation},
journal = {Journal of Hazardous Materials},
volume = {431},
pages = {128558},
year = {2022},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2022.128558},
url = {https://www.sciencedirect.com/science/article/pii/S0304389422003466},
author = {Song Hu and Guohong Liu and Jin Zhang and Jiachen Yan and Hongyu Zhou and Xiliang Yan},
keywords = {Chemical structure identification, Machine learning, Mass spectra, Chemical toxicity prediction, Environmental health and safety},
abstract = {Quantitative structure-activity relationship (QSAR) modeling has been widely used to predict the potential harm of chemicals, in which the prediction heavily relies on the accurate annotation of chemical structures. However, it is difficult to determine the accurate structure of an unknown compound in many cases, such as in complex water environments. Here, we solved the above problem by linking electron ionization mass spectra (EI-MS) of organic chemicals to toxicity endpoints through various machine learning methods. The proposed method was verified by predicting 50% growth inhibition of Tetrahymena pyriformis (T. pyriformis) and liver toxicity. The optimal model performance obtained an R2 > 0.7 or balanced accuracy > 0.72 for both the training set and test set. External experimentation further verified the application potential of our proposed method in the toxicity prediction of unknown chemicals. Feature importance analysis allowed us to identify critical spectral features that were responsible for chemical-induced toxicity. Our approach has the potential for toxicity prediction in such fields that it is difficult to determine accurate chemical structures.}
}
@article{GANDHI2022,
title = {Multimodal Sentiment Analysis: A Systematic review of History, Datasets, Multimodal Fusion Methods, Applications, Challenges and Future Directions},
journal = {Information Fusion},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001634},
author = {Ankita Gandhi and Kinjal Adhvaryu and Soujanya Poria and Erik Cambria and Amir Hussain},
keywords = {Affective Computing, Sentiment Analysis, Multimodal Fusion, Fusion technique},
abstract = {In the field of artificial intelligence (AI) and natural language processing (NLP), sentiment analysis (SA) is gaining traction and becoming a buzzword. There is a growing demand for automizing the process of analysing the user's sentiments towards any product or services due to numerous SA applications. As more and more opinions are shared in the form of videos rather than text only, SA using multiple modalities known as Multimodal Sentiment Analysis (MSA) is become very much important research area. MSA uses recent machine learning innovations for its progress. All the latest advancements in machine learning and deep learning are utilised at each different stage in MSA like multimodal features extraction, multimodal fusion, sentiment polarity detection with minimized error rate and improved speed. This survey paper focuses mainly on the primary taxonomy and newly released Multimodal Fusion architectures, and it divides numerous recent developments in MSA architectures into ten categories. Early fusion, late fusion, hybrid fusion, model-level fusion, tensor fusion, hierarchical fusion, bi-modal fusion, attention-based fusion, quantum-based fusion and word-level fusion are the ten categories. This manuscript mainly contributes in a comparison of several architectural evolutions in MSA fusion, strengths, and limitations. It also discusses research gaps, applications in various sectors, and future scope.}
}
@article{NAWROCKI2022102491,
title = {Modeling adaptive security-aware task allocation in mobile cloud computing},
journal = {Simulation Modelling Practice and Theory},
volume = {116},
pages = {102491},
year = {2022},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2022.102491},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X22000041},
author = {Piotr Nawrocki and Jakub Pajor and Bartlomiej Sniezynski and Joanna Kolodziej},
keywords = {Security, Mobile Cloud Computing, Machine learning, Simulation},
abstract = {Security is one of the most important criteria in the management of cloud resources. In Mobile Cloud Computing (MCC), secure allocation of tasks remains challenging due to the limited storage, battery life and computational power of mobile devices connected to the core cloud cluster infrastructure. Secure wireless communication channels and protocols for protecting the data and information sent to the cloud, and remote access to secure cloud services, are other important problems related to task scheduling and processing in dynamic MCC. In this paper, we developed a new security-aware task allocation model strategy in Mobile Cloud Computing. In this model, we define an allocation algorithm which generates an optimal and secure configuration of communication protocols in order to meet the specific data confidentiality requirements defined by end users. Resource utilization is predicted using Machine Learning methods, and the optimal secure service for task execution is selected. We developed a simulation environment (MocSecSim) for the evaluation of the algorithms proposed in several scenarios based on the users’ requirements. The results of simulations and experiments have demonstrated that the model proposed significantly improves the level of security of calculations in comparison with a configuration where processing time and energy consumption are the main criteria for optimizing task allocation.}
}
@article{ZHANG2022281,
title = {QAPP: A quality-aware and privacy-preserving medical image release scheme},
journal = {Information Fusion},
volume = {88},
pages = {281-295},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522000707},
author = {Xu Zhang and Yufeng Wang and Jianhua Ma and Qun Jin},
keywords = {Differential privacy, Discrete cosine transform, Image quality improvement, Privacy protection},
abstract = {The direct release of medical image may face the dilemma: the privacy protection of medical images inevitably affects the visual quality of images. To balance medical image quality and privacy, this paper proposes a quality-aware and privacy-preserving medical image release scheme, QAPP, which effectively integrates the discrete cosine transform (DCT) with differential privacy (DP). Specifically, QAPP is composed of three phases. First, DCT is applied to each medical image to obtain its cosine coefficients matrix. Second, the original cosine coefficients matrix is compressed into k*k cosine coefficients matrix, which can retain the main features of each image. Third, the appropriate Laplace noise is injected into the formed k*k matrix to achieve differential privacy, and these noise-added coefficients are used to reconstruct the noise-added medical images through inverse DCT. Especially, considering there two error sources affecting the image quality in our work: the compression error caused by DCT, and the injected noise error caused by DP, Therefore, a selection function is proposed to determine the optimal compression dimension k, which can minimize the influence of these two errors to improve the visualization quality of the medical image. Subjective and objective image quality evaluation, and extensive experiments of image classification and segmentation using the real medical image dataset demonstrate that the proposed method QAPP can better balance medical image quality and privacy than other similar DP-based methods.}
}
@article{ZHOU2022104344,
title = {Smartphone-based road manhole cover detection and classification},
journal = {Automation in Construction},
volume = {140},
pages = {104344},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104344},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002175},
author = {Baoding Zhou and Wenjian Zhao and Wenhao Guo and Linchao Li and Dejin Zhang and Qingzhou Mao and Qingquan Li},
keywords = {Smartphone, Manhole cover, Automatic detection},
abstract = {Road surface condition detection is an important application for many intelligent transportation systems (ITSs). A manhole cover depression is one of the common factors affecting road conditions. Smartphones are equipped with different sensors, which can be used to collect image data and inertial data. A new large-scale manhole cover detection dataset is developed by using smartphones to collect road image data, and a hierarchical classification method based on the convolutional neural network is proposed in this paper. The proposed method first coarsely classifies the images into nonrainy and rainy types and then performs manhole cover detections based on the coarse classification results. As a result, the proposed method achieves an accuracy of approximately 86.3% for road manhole cover detection. Based on the observation that different degrees of manhole cover subsidence produce different degrees of inertial sensor data, this paper used a machine learning method, which can automatically classify the detected manhole covers into different degrees of subsidence, namely good, average, and poor. The average recalls, average precisions, and average F1-measures achieve approximately 87.3%, 86.9%, and 87.2% accuracy, respectively. The results show that the proposed approach can effectively detect manhole covers in different weather and road conditions, which can effectively reduce the cost of road manhole cover data collection and detection, providing a new method for road manhole cover detection.}
}
@article{COOKE2022102865,
title = {Technoscience and the modernization of freshwater fisheries assessment and management},
journal = {Environmental Technology & Innovation},
volume = {28},
pages = {102865},
year = {2022},
issn = {2352-1864},
doi = {https://doi.org/10.1016/j.eti.2022.102865},
url = {https://www.sciencedirect.com/science/article/pii/S2352186422003066},
author = {S.J. Cooke and M.F. Docker and N.E. Mandrak and N. Young and D.D. Heath and K.M. Jeffries and A. Howarth and J.W. Brownscombe and J. Livernois and C.A.D. Semeniuk and P.A. Venturelli and A.J. Danylchuk and R.J. Lennox and I. Jarić and A.T. Fisk and C.S. Vandergoot and J.R. Britton and A.M. Muir},
keywords = {Technoscience, Modernization, Fisheries, Resource management, Assessment, Freshwater},
abstract = {Inland fisheries assessment and management are challenging given the inherent complexity of working in diverse habitats (e.g., rivers, lakes, wetlands) that are dynamic on organisms that are often cryptic and where fishers are often highly mobile. Yet, technoscience is offering new tools that have the potential to reimagine how inland fisheries are assessed and managed. So-called “technoscience” refers to instances in which science and technology unfurl together, offering novel ways of spurring and achieving meaningful change. This paper considers the role of technoscience and its potential for modernizing the assessment and management of inland fisheries. It first explores technoscience and its potential benefits, followed by presentation of a series of synopses that explore the application (both successes and challenges) of new technologies such as environmental DNA (eDNA), genomics, electronic tags, drones, phone apps, iEcology, and artificial intelligence to assessment and management. The paper also considers the challenges and barriers that exist in adopting new technologies. The paper concludes with a provocative assessment of the potential of technoscience to reform and modernize inland fisheries assessment and management. Although these tools are increasingly being embraced, there is a lack of platforms for aggregating these data streams and providing managers with actionable information in a timely manner. The ideas presented here should serve as a catalyst for beginning to work collectively and collaboratively towards fisheries assessment and management systems that harness the power of technology and serve to modernize inland fisheries management. Such transformation is urgently needed given the dynamic nature of environmental change, the evolving threat matrix facing inland waters, and the complex behavior of fishers. Quite simply, a dynamic world demands dynamic fisheries management; technoscience has made that within reach.}
}
@article{SANCHEZ202246,
title = {The impact of visualizing operational deviations on overall quality in assembly lines},
journal = {Procedia CIRP},
volume = {107},
pages = {46-52},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002244},
author = {Ebly Sanchez and Axel Joelsson and Matthias Andersson Baumgartner and Knut Åkesson},
keywords = {Operational deviations, Quality performance, Volvo Production System},
abstract = {A framework for data collection and visualization of operational deviation at a Volvo truck manufacturing plant implementing Volvo Group’s production system, Volvo Production System (VPS), is presented. This includes visualisation of daily quality performance indicators to support decision making and improvement actions at the shop foor team level. The approach is evaluated in a qualitatively study using a survey instrument to collect responses from managers and team leaders, which acted as input for the actual use of the data and as validation of the framework. The results from this evaluation show that potentially operational deviations can be reduced, impacting positively quality performance indicators such as frst-time through (FTT). This paper also provides a brief description of the VPS in connection with data analytics and visualization.}
}