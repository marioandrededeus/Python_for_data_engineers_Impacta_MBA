@article{LIU2022158461,
title = {Soil tungsten contamination and health risk assessment of an abandoned tungsten mine site},
journal = {Science of The Total Environment},
volume = {852},
pages = {158461},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.158461},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722055607},
author = {Sijia Liu and Rongxiao Yuan and Xuedong Wang and Zengguang Yan},
keywords = {Tungsten mine site, Soil contamination, Leaching, Geo-accumulation index, Health risk assessment},
abstract = {The mining and beneficiation of tungsten ores, including waste treatment and tailings disposal, may cause soil contamination in the mining area and environments. Few studies have addressed soil contamination in tungsten mine sites. The current research quantitated the leachates in the surface and subsurface soil samples from mining and beneficiation areas, peripheral area, sand-making area, dumping area, and tailings pond of an abandoned tungsten mine site in Ganzhou City, Jiangxi Province of China. We further evaluated the degree of soil tungsten pollution and the risk to human health. The results showed that soil tungsten contamination mainly occurred in the sand-making area where tailings were used to make sand. The highest tungsten content in the surface and subsurface soils of the sand-making area was 1250 and 3020 mg/kg, respectively, exceeding the EPA's Regional Screening Level of tungsten (930 mg/kg) for industrial land use. The leaching concentrations of soil tungsten had similar distribution patterns to that of total soil tungsten, with the highest leaching concentration (0.860 mg/L) found in the sand-making area. The geo-accumulation index evaluation indicated heavy tungsten contamination at the sand-making area and tailings pond. The hazard quotient (HQ = 1.34) of tungsten contamination in the surface soils of the sand-making area exceeded the acceptable level (HQ = 1), implying a significant risk to human health. The present study provided valuable information for pollution control and risk management of soil contamination in tungsten mine sites.
Capsule
We studied the degree of soil tungsten pollution and health risk assessment in an abandoned tungsten mining area to provide helpful information for soil pollution control and risk management in China's tungsten mining areas.}
}
@article{MIRBAGHERI2022101011,
title = {Developing the required data set for the integration of breast cancer registry systems in Iran},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101011},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101011},
url = {https://www.sciencedirect.com/science/article/pii/S235291482200154X},
author = {Esmat Mirbagheri and Mohsen Shafiee and Mostafa Shanbezadeh and Hadi Kazemi-Arpanahi},
keywords = {Breast neoplasms, Registries, Common data elements, Information management, Information systems},
abstract = {Background
Breast cancer is a major public health concern due to its increasing incidence and mortality rates. A large volume of data is generated from different healthcare settings with inconsistent and heterogeneous data frameworks. There is an increasing demand to integrate breast cancer data between various information systems to answer specific research questions and future clinical trials. Thus, this study aimed to develop a minimum data set (MDS) for integrating breast cancer registry systems as a prerequisite for multi-center data exchange and research cooperation.
Methods
The proposed MDS was developed using a multi-stage process. First, a systematic search was performed in scientific databases. Available data sets and registries related to breast cancer were also reviewed until data saturation. Then, a two-round Delphi survey was performed to reach an agreement on the primary data items. Finally, an additional Delphi stage was carried out to validate the content of the final MDS by calculating the individual item content validity index (CVI), overall scale CVI (S-CVI), and face validity.
Results
After the literature review, the primary data set for breast cancer including 309 data items was identified. After the Delphi phase and calculation of I-CVI, S-CVI, and face validity, the breast cancer MDS was finalized with 14 classes and 205 data items.
Conclusions
This agreed-upon MDS enables accurate, consistent, and comparable inter-organizational data collection between breast cancer care centers. This data homogeneity enhances the analytic power and depth of variables, thereby contributing to multicenter, large-scale, and more generalizable epidemiological and predictive studies on breast cancer.}
}
@article{YU2022101698,
title = {How does intelligent manufacturing reconcile the conflict between process standards and technological innovation?},
journal = {Journal of Engineering and Technology Management},
volume = {65},
pages = {101698},
year = {2022},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2022.101698},
url = {https://www.sciencedirect.com/science/article/pii/S0923474822000285},
author = {Kangkang Yu and Cheng Qian and Jinliang Chen},
keywords = {Process standards, Intelligent manufacturing, Technological innovation, Case study},
abstract = {Understanding whether process standards foster or hinder technological innovation lacks consensus in the literatures. From the process management perspective, this study aimes at exploring the mechanisms underlying this relationship under a specific situation of intelligent manufacturing. By analyzing interview data from four case companies in the environmental instrument industry in China and secondary data at the firm level, we find that (i) process standards have a positive effect on technological innovation and (ii) process standards in companies with relatively high levels of intelligent manufacturing raise technological innovation. In particular, these four companies are differentiated with respect to intelligent manufacturing context: two companies with relatively high levels of intelligent manufacturing; and the other two with relatively low levels of intelligent manufacturing. Based on the case analysis, we find that there is an open logic, tightening both internal and external communication linkages, between process standards and technological innovation, so process standards positively affect technological innovation. Further, we find that for those with a higher level of intelligent manufacturing, firms typically benefit more organizational learning competency, which enhances the impact of process standards on technological innovation. The findings are also confirmed by a post hoc survey. Above all, this study contributes to the literatures on process management and technological innovation as well as the new trend of intelligent manufacturing in Industry 4.0.}
}
@article{WANG2022,
title = {Integration of computational analysis and spatial transcriptomics in single-cell study},
journal = {Genomics, Proteomics & Bioinformatics},
year = {2022},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2022.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1672022922000845},
author = {Ran Wang and Guangdun Peng and Patrick P.L. Tam and Naihe Jing},
keywords = {scRNA-seq, Computational methodology, Spatial transcriptome, Data integration, Mathematical model},
abstract = {Recent advances of single-cell transcriptomics technologies and allied computational methodologies have revolutionized molecular cell biology. Meanwhile, pioneering explorations in spatial transcriptomics have opened avenues to address fundamental biological questions in health and diseases. Here, we reviewed the technical attributes of single-cell RNA sequencing (scRNA-seq) and spatial transcriptomics, and the core concepts of computational data analysis. We further highlighted the challenges in the application of data integration methodologies and the interpretation of the biological context of the findings.}
}
@article{KARIM2022101760,
title = {Development of “Biosearch System” for biobank management and storage of disease associated genetic information},
journal = {Journal of King Saud University - Science},
volume = {34},
number = {2},
pages = {101760},
year = {2022},
issn = {1018-3647},
doi = {https://doi.org/10.1016/j.jksus.2021.101760},
url = {https://www.sciencedirect.com/science/article/pii/S1018364721004225},
author = {Sajjad Karim and Mona Al-Kharraz and Zeenat Mirza and Hend Noureldin and Heba Abusamara and Nofe Alganmi and Adnan Merdad and Saddig Jastaniah and Sudhir Kumar and Mahmood Rasool and Adel Abuzenadah and Mohammed Al-Qahtani},
keywords = {Biosearch system, LIMS database, Biobank, Genomics, Microarray, Bioinformatics},
abstract = {Objective
Databases and softwares are important to manage modern high-throughput laboratories and store clinical and genomic information for quality assurance. Commercial softwares are expensive with proprietary code issue while academic versions have adaptation issue. Our aim was to develop an adaptable in-house software that can stores specimen and disease-associated genetic information in biobank to facilitate translational research.
Methods
Prototype was designed as per the research requirements and computational tools were used to develop software under three tiers; Visual Basic and ASP.net for presentation tier, SQL server for data tier, and Ajax and JavaScript for business tier. We retrieved specimens from biobank using this software and performed microarray based transcriptomic analysis to detect differentialy expressed genes (DEGs) with FC ±2 and P-value <0.05 in triple negative breast cancer cases. Ingenuity pathway analysis tool was used to predict canonical molecular pathways associated with disease. Overall performance and utility of software was evaluated by JMeter software, CRUD function test and set of feedback questioners.
Results
We developed “Biosearch System”, a web-based software enabling management of biobank samples (tissue, blood, FTTP slides) and their extracts (DNA, RNA and proteins) with clinical and experimental details. The client satisfaction feedback was excellent with score 4.7/5. We identified a total of 1181 DEGs including both upregulated (IFI6, LEF1, FANCI, CASC5, PLXNA3 etc.) and down-regulated (ADH1B, LYVE1, ADH1C, ADH1B, ADIPOQ, PLIN1, LYVE1 etc.) genes in triple negative breast cancer. Pathway analysis of DEGs revealed significant activation of interferon signaling (z-score 2.646) and kinetochore metaphase signaling pathway (z-score 2.138) in cancer.
Conclusion
Biosearch System is a user friendly LIMS for collection, storage and retrieval of specimen and clinical information. It is secure, efficient, and very convenient in sample tracking and data analysis. We illustrated its utility in transcriptomic study of breast cancer. Additionally, it can facilitate and speed up any genomic study and translational research publications.}
}
@article{SON2022108879,
title = {Integrated framework for estimating remaining useful lifetime through a deep neural network},
journal = {Applied Soft Computing},
volume = {122},
pages = {108879},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108879},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002587},
author = {Seho Son and Ki-Yong Oh},
keywords = {Deep neural network, Genetic algorithm, Moving-time window, Feature extraction, Feature reasoning, Remaining useful life, Step differential method},
abstract = {This paper proposes an integrated framework for a deep neural network to estimate the remaining useful life (RUL) to ensure the reliability and safety of complex mechanical systems and enable proactive maintenance for intelligent operation. This data-driven method can predict complex and highly nonlinear degradation characteristics that are difficult to predict using physics-based prognostics and health management. In particular, this study focused on feature preprocessing and hyperparameter optimization, whereas previous studies had focused on the neural network architecture to improve prediction accuracy and robustness. The proposed integrated framework comprises four phases: feature preprocessing, feature reasoning using a deep neural network, hyperparameter optimization using a genetic algorithm, and RUL estimation. In the first phase, sensor measurements sensitive to degradation are selected and separated into primary and dynamic degradation trends. In addition, step differential values are extracted to account for multiple operational modes using an unsupervised clustering method. In the second phase, feature reasoning is performed using a deep neural network to characterize hidden complex and highly nonlinear degradation features. The health indicators manipulated in the first phase are trained using the proposed deep neural network. In the third phase, a genetic algorithm is introduced to optimize the hyperparameters used in feature preprocessing and reasoning. The final phase estimates the RUL using the proposed deep neural network with optimized hyperparameters. The proposed method was validated on the C-MAPSS dataset. The results show that the proposed integrated framework outperformed other state-of-the-art machine learning and deep learning methods under different operational conditions, suggesting that efficient feature preprocessing and hyperparameter optimization significantly improve the prediction accuracy and robustness of RUL for data-driven prognostics and health management.}
}
@article{VRAIN2022122051,
title = {The discontinuance of low carbon digital products and services},
journal = {Technological Forecasting and Social Change},
volume = {185},
pages = {122051},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.122051},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522005728},
author = {Emilie Vrain and Charlie Wilson and Barnaby Andrews},
keywords = {Climate change, Diffusion of innovations, Covid-19, Post-adoption, Attributes},
abstract = {Digital consumer innovations offer low-carbon alternatives to mainstream consumption practices. We address a lack of research on the factors influencing post-adoption decisions of discontinuance for this important class of innovations. We conducted a repeat survey with UK consumers (n = 995) in 2019 and 2020 to investigate 16 digital products and services across mobility, food, homes, and energy domains. Our survey captured temporal changes in adoption, personal and contextual characteristics, social influences, innovation experiences and perceived attributes. We also provide a unique contribution by assessing the impacts of Covid-19 on post-adoption processes. Our results indicate that discontinuance is associated with: 1) services more than products; 2) perceived functional attributes not met by experienced attributes; 3) a lack of positive social influence, including word-of-mouth; 4) a lack of social network connections to other adopters; and 5) a decline in an individual's financial situation. Covid-19 was not found to be a significant factor influencing innovation discontinuance. Findings highlight generalisable insights regarding issues that need addressing to overcome discontinuance. For example, while digital services offer low-carbon promise, continued adoption is sensitive to their strong performance attributes. There is a need for continued innovation to sustain market position relative to more familiar incumbents.}
}
@article{SOUZA2022108403,
title = {City Information Modelling as a support decision tool for planning and management of cities: A systematic literature review and bibliometric analysis},
journal = {Building and Environment},
volume = {207},
pages = {108403},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108403},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321008003},
author = {Letícia Souza and Cristiane Bueno},
keywords = {City information modelling, Support decision tool, Urban management tool, Future urban planning},
abstract = {The growing demand of the population has caused serious problems for cities and has become one of the main challenges for city managers. This demand has occurred much faster than the tooling of public management. In this context, the urgent need for implementations that meet current requirements highlighting the advantages of City Information Modelling (CIM). The CIM helps in the search for information on future demands, providing a holistic view of the city. However, the absence of a full application and an established concept has been observed in the literature. Considering this, a systematic literature review and bibliometric analysis was conducted, resulting in 80 articles that composed the final analysis. We identified five recurring topics in the articles, the most important of the CIM, and discussed them in depth. We found a direction towards the CIM concept: the integration of Building Information Modelling (BIM), geographic information system (GIS), and a complete and up-to-date urban database, which enables analysis and simulation. The research concluded that a major effort will still be needed to establish the CIM, and its full implementation also depends on the dissemination of knowledge and demonstration of the tool's potential.}
}
@article{ZHANG2022231110,
title = {A machine learning-based framework for online prediction of battery ageing trajectory and lifetime using histogram data},
journal = {Journal of Power Sources},
volume = {526},
pages = {231110},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.231110},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322001331},
author = {Yizhou Zhang and Torsten Wik and John Bergström and Michael Pecht and Changfu Zou},
keywords = {Lithium-ion batteries, State of health prediction, Remaining useful life, Machine learning, Online adaptive learning, Real-world fleet data},
abstract = {Accurately predicting batteries’ ageing trajectory and remaining useful life is not only required to ensure safe and reliable operation of electric vehicles (EVs) but is also the fundamental step towards health-conscious use and residual value assessment of the battery. The non-linearity, wide range of operating conditions, and cell to cell variations make battery health prediction challenging. This paper proposes a prediction framework that is based on a combination of global models offline developed by different machine learning methods and cell individualised models that are online adapted. For any format of raw data collected under diverse operating conditions, statistic properties of histograms can be still extracted and used as features to learn battery ageing. Our framework is trained and tested on three large datasets, one being retrieved from 7296 plug-in hybrid EVs. While the best global models achieve 0.93% mean absolute percentage error (MAPE) on laboratory data and 1.41% MAPE on the real-world fleet data, the adaptation algorithm further reduced the errors by up to 13.7%, all requiring low computational power and memory. Overall, this work proves the feasibility and benefits of using histogram data and also highlights how online adaptation can be used to improve predictions.}
}
@article{WANG2022102759,
title = {Business Innovation based on artificial intelligence and Blockchain technology},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102759},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102759},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002405},
author = {Zeyu Wang and Mingyu Li and Jia Lu and Xin Cheng},
keywords = {Artificial intelligence, Blockchain, business},
abstract = {The growing business evolution and the latest Artificial Intelligence (AI) make the different business practices to be enhanced by the ability to create new means of collaboration. Such growing technology helps to deliver brand services and even some new kinds of corporate interactions with customers and staff. AI digitization simultaneously emphasized businesses to focus on the existing strategies and regularly and early pursue new market opportunities. While digital technology research in the framework of business innovation is gaining greater interest and the privacy of data can be maintained by Blockchain technology. Therefore in this paper, Business Innovation based on artificial intelligence and Blockchain technology (BI-AIBT) has been proposed to enhance the business practices and maintain the secured interaction among the various clients. The collection of qualitative empirical data is made up of few primary respondents from two distinct business sectors. BI-AIBT has been evaluated by undertaking and exploring the difference and similarities between digitalization's impact on value development, proposal, and business capture. Besides, organizational capacities and staff skills interaction issues can be improved by BT. The experimental result suggests that digital transformation is usually regarded as essential and improves business innovation strategies. The numerical result proposed BI-AIBT improves the demand prediction ratio (97.1%), product quality ratio (98.3%), Business development ratio (98.9%), customer behavior analysis ratio (96.3%), and customer satisfaction ratio (97.2%).}
}
@article{YIM2022118835,
title = {Rise and fall of lung cancers in relation to tobacco smoking and air pollution: A global trend analysis from 1990 to 2012},
journal = {Atmospheric Environment},
volume = {269},
pages = {118835},
year = {2022},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2021.118835},
url = {https://www.sciencedirect.com/science/article/pii/S1352231021006579},
author = {Steve H.L. Yim and T. Huang and Jason M.W. Ho and Amy S.M. Lam and Sarah T.Y. Yau and Thomas W.H. Yuen and G.H. Dong and Kelvin K.F. Tsoi and Joseph J.Y. Sung},
keywords = {Air quality, Lung cancer, Smoking, Lung adenocarcinoma, Lung squamous cell carcinoma},
abstract = {Lung cancer remains the leading cancer in incidence and mortality in both genders and most countries. Global cancer statistics show a declining trend of lung squamous cell carcinoma (LSCC) but an uprising trend of lung adenocarcinoma (LADC). The reasons behind their opposite trends are unclear. This study aims to analyze the global trends of LSCC and LADC during 1990–2012 in relation to tobacco consumption and air pollution. Results show a 1% decline of smoking prevalence of 7 years ago is associated with a 9% (95% conﬁdence interval: 8%, 10%) drop in the LSCC incidence globally, whereas a 0.1 μg/m3 increment of BC of 7 years ago is associated with a 12% (9%, 16%) increase in LADC incidence globally. Association between BC and LSCC (or LADC) is more prominent in females, with a 14% (7%, 20%) increase in LSCC [or 14% (11%, 19%) increase in LADC] incidence for a 0.1 μg/m3 increment of BC of 8 (or 6) years ago. Associations vary with different genders across different continents. For instance, concentration of BC is positively associated with incidence of both LSCC and LADC in Europe and North America, whereas concentration of sulfate is positively associated with LSCC incidence in Europe and Oceania, and with LADC incidence in Asia, Oceania and South America. We conclude global decreasing LSCC incidence is associated with the reduced tobacco consumption, whereas the global increasing LADC incidence is likely associated with air pollution. Various particulate species have divergent effects on LADC incidence in different continents.}
}
@article{LEI2022288,
title = {Will tourists take mobile travel advice? Examining the personalization-privacy paradox},
journal = {Journal of Hospitality and Tourism Management},
volume = {50},
pages = {288-297},
year = {2022},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2022.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1447677022000195},
author = {Soey Sut Ieng Lei and Irene Cheng Chu Chan and Jingyi Tang and Shun Ye},
keywords = {Experiment, Mobile travel advice, Perceived personalization, Personalization-privacy paradox, Personalization strategy},
abstract = {Recognizing the penetration of mobile devices and its transformational impact on travel behavior, tourism and hospitality businesses have been investing on various mobile initiatives to connect with travelers, hoping to influence their decision-making behaviors at different stages throughout the trip. One of these strategies is to provide personalized contents that are more attractive and persuasive. As research regarding the impact of mobile-driven personalization practices on travel behavior is limited, this study investigates the factors affecting travelers' adoption of personalized mobile travel advice. Based on personalization-privacy paradox and self-referencing effect, a 2 (self-reference: high vs. low) × 2 (relevance: high vs. low) between-subjects experiment was carried out. The results demonstrate the mechanism underlying the effects of personalization cues (self-reference and content relevance) on travelers’ intention to adopt personalized mobile travel advice. The competitive mediating roles of perceived personalization and privacy concern on the relationship between personalization cues and adoption intention are highlighted.}
}
@article{WU2022108456,
title = {Consortium blockchain-enabled smart ESG reporting platform with token-based incentives for corporate crowdsensing},
journal = {Computers & Industrial Engineering},
volume = {172},
pages = {108456},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108456},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004909},
author = {Wei Wu and Yelin Fu and Zicheng Wang and Xinlai Liu and Yuxiang Niu and Bing Li and George Q. Huang},
keywords = {ESG reporting, Consortium blockchain, Internet of Things, Incentive mechanism, Corporate crowdsensing, Shapley value},
abstract = {Environmental, social and governance (ESG) issues arouse wide concern in both industry and academia to promote sustainable development. Listed companies assume the responsibility to submit annual reports to disclose their ESG outcomes genuinely. However, the challenge is that companies may overstate or slur their actual sustainable performance, and the entire ESG reporting process is hidden backward, thereby making the report untrustworthy. Accordingly, this paper proposes an architecture of smart ESG reporting platform leveraging the Internet of Things (IoT) and blockchain technologies to enable corporate crowdsensing for environmental data and enhance the security, transparency and creditability of ESG reporting process. In addition, with the aim of motivating firms to upload massive ESG raw data of high quality, we devise an incentive mechanism that grants them crypto tokens as a reputation for sustainable performance disclosure, exposed to industry and investors for reference. The maximum tokens settlement for each environmental key performance indicator (KPI) is regarded as a cooperative game for premiums, and Shapley value is applied to fairly distribute the tokens in line with the disclosure significance scored by experienced investors. Here, we take the ESG reporting guide issued by the Hong Kong exchange (HKEX) and the practice in the Hong Kong apparel industry as the context. An experimental simulation is conducted to illustrate the feasibility and effectiveness of the proposed platform architecture and approach of token allocation. This study holds the promise of providing a precedent for adopting the advanced technologies to address the greenwashing in firms and actualize intelligent and trustable ESG reporting.}
}
@article{PASSAMONTI2022100914,
title = {The future of research in hematology: Integration of conventional studies with real-world data and artificial intelligence},
journal = {Blood Reviews},
volume = {54},
pages = {100914},
year = {2022},
issn = {0268-960X},
doi = {https://doi.org/10.1016/j.blre.2021.100914},
url = {https://www.sciencedirect.com/science/article/pii/S0268960X2100120X},
author = {Francesco Passamonti and Giovanni Corrao and Gastone Castellani and Barbara Mora and Giulia Maggioni and Robert Peter Gale and Matteo Giovanni {Della Porta}},
keywords = {Real-world evidence, Real-world data, Artificial intelligence, Haematological cancers, Laeukemia, Lymphoma, Myelofibrosis},
abstract = {Most national health-care systems approve new drugs based on data of safety and efficacy from large randomized clinical trials (RCTs). Strict selection biases and study-entry criteria of subjects included in RCTs often do not reflect those of the population where a therapy is intended to be used. Compliance to treatment in RCTs also differs considerably from real world settings and the relatively small size of most RCTs make them unlikely to detect rare but important safety signals. These and other considerations may explain the gap between evidence generated in RCTs and translating conclusions to health-care policies in the real world. Real-world evidence (RWE) derived from real-world data (RWD) is receiving increasing attention from scientists, clinicians, and health-care policy decision-makers - especially when it is processed by artificial intelligence (AI). We describe the potential of using RWD and AI in Hematology to support research and health-care decisions.}
}
@article{JASIULEWICZKACZMAREK2022223,
title = {Assessing the Barriers to Industry 4.0 Implementation From a Maintenance Management Perspective - Pilot Study Results},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {223-228},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.197},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001987},
author = {Malgorzata Jasiulewicz-Kaczmarek and Katarzyna Antosz and Chao Zhang and Robert Waszkowski},
keywords = {Industry 4.0 technologies, Maintenance 4.0, barriers I4.0 technologies implementation},
abstract = {The purpose of this paper is to identify the barriers of I4.0 technologies implementation in maintenance. Based on literature analysis twenty two barriers of I4.0 technologies implementation are determined and classified into five main groups such as “Strategy and Organization”; “Maintenance staff knowledge and training”; “Resources”; “Technology and infrastructure” and “Security and confidentiality”. Experts’ opinions were taken to finalize the identified barriers. The data for the study were collected from academia experts and have been further analyzed. The results show that from twenty two identified I4.0 technologies implementation barriers only nine are the most representative ones.}
}
@article{ZANELLA2022107279,
title = {CEIFA: A multi-level anomaly detector for smart farming},
journal = {Computers and Electronics in Agriculture},
volume = {202},
pages = {107279},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107279},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005919},
author = {Angelita Rettore de Araujo Zanella and Eduardo {da Silva} and Luiz Carlos Pessoa Albini},
keywords = {Smart agriculture, Anomaly detection, Security, Reliability, Internet of Things},
abstract = {Climate change, the water crisis, and population growth add new challenges for food production. The modernization of agricultural methods is essential to increase production rates and preserve natural resources. Smart agriculture provides resources that can enhance farming tasks by efficiently controlling actuators, optimizing utility and resource use, managing production, maximizing profit, and minimizing costs. For these technologies to become popular, they must have a high level of reliability and safety. To improve the reliability in Smart Agriculture, this paper proposes CEIFA, a low-cost, hybrid anomaly detector capable of identifying failures, faults, errors and attacks that impact these systems. CEIFA operates on local or remote cloud servers, filtering data sent by agricultural system sensors. It can operate on resource-restricted devices and save financial resources related to computing costs. Real tests on a set of faults, failures, and errors point to an efficiency greater than 95% in anomaly detection.}
}
@article{PRITCHARD2022100282,
title = {Monitoring populations at increased risk for SARS-CoV-2 infection in the community using population-level demographic and behavioural surveillance},
journal = {The Lancet Regional Health - Europe},
volume = {13},
pages = {100282},
year = {2022},
issn = {2666-7762},
doi = {https://doi.org/10.1016/j.lanepe.2021.100282},
url = {https://www.sciencedirect.com/science/article/pii/S2666776221002684},
author = {Emma Pritchard and Joel Jones and Karina-Doris Vihta and Nicole Stoesser and Prof Philippa C. Matthews and David W. Eyre and Thomas House and John I Bell and Prof John N Newton and Jeremy Farrar and Prof Derrick Crook and Susan Hopkins and Duncan Cook and Emma Rourke and Ruth Studley and Prof Ian Diamond and Prof Tim Peto and Koen B. Pouwels and Prof A. Sarah Walker},
keywords = {SARS-CoV-2, community, monitoring},
abstract = {Summary
Background
The COVID-19 pandemic is rapidly evolving, with emerging variants and fluctuating control policies. Real-time population screening and identification of groups in whom positivity is highest could help monitor spread and inform public health messaging and strategy.
Methods
To develop a real-time screening process, we included results from nose and throat swabs and questionnaires taken 19 July 2020-17 July 2021 in the UK's national COVID-19 Infection Survey. Fortnightly, associations between SARS-CoV-2 positivity and 60 demographic and behavioural characteristics were estimated using logistic regression models adjusted for potential confounders, considering multiple testing, collinearity, and reverse causality.
Findings
Of 4,091,537 RT-PCR results from 482,677 individuals, 29,903 (0·73%) were positive. As positivity rose September-November 2020, rates were independently higher in younger ages, and those living in Northern England, major urban conurbations, more deprived areas, and larger households. Rates were also higher in those returning from abroad, and working in healthcare or outside of home. When positivity peaked December 2020-January 2021 (Alpha), high positivity shifted to southern geographical regions. With national vaccine roll-out from December 2020, positivity reduced in vaccinated individuals. Associations attenuated as rates decreased between February-May 2021. Rising positivity rates in June-July 2021 (Delta) were independently higher in younger, male, and unvaccinated groups. Few factors were consistently associated with positivity. 25/45 (56%) confirmed associations would have been detected later using 28-day rather than 14-day periods.
Interpretation
Population-level demographic and behavioural surveillance can be a valuable tool in identifying the varying characteristics driving current SARS-CoV-2 positivity, allowing monitoring to inform public health policy.
Funding
Department of Health and Social Care (UK), Welsh Government, Department of Health (on behalf of the Northern Ireland Government), Scottish Government, National Institute for Health Research.}
}
@article{WANG2022116236,
title = {Multi-classification assessment of bank personal credit risk based on multi-source information fusion},
journal = {Expert Systems with Applications},
volume = {191},
pages = {116236},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116236},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015475},
author = {Tianhui Wang and Renjing Liu and Guohua Qi},
keywords = {Personal credit risk, Multi-classification assessment, Information fusion, D-S evidence theory},
abstract = {There have been many studies on machine learning and data mining algorithms to improve the effect of credit risk assessment. However, there are few methods that can meet its universal and efficient characteristics. This paper proposes a new multi-classification assessment model of personal credit risk based on the theory of information fusion (MIFCA) by using six machine learning algorithms. The MIFCA model can simultaneously integrate the advantages of multiple classifiers and reduce the interference of uncertain information. In order to verify the MIFCA model, dataset collected from a real data set of commercial bank in China. Experimental results show that MIFCA model has two outstanding points in various assessment criteria. One is that it has higher accuracy for multi-classification assessment, and the other is that it is suitable for various risk assessments and has universal applicability. In addition, the results of this research can also provide references for banks and other financial institutions to strengthen their risk prevention and control capabilities, improve their credit risk identification capabilities, and avoid financial losses.}
}
@article{FRAISL202281,
title = {Demonstrating the potential of Picture Pile as a citizen science tool for SDG monitoring},
journal = {Environmental Science & Policy},
volume = {128},
pages = {81-93},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121003208},
author = {D. Fraisl and L. See and T. Sturn and S. MacFeely and A. Bowser and J. Campbell and I. Moorthy and O. Danylo and I. McCallum and S. Fritz},
keywords = {Citizen science, Crowdsourcing, SDGs, Citizen science tools, SDG monitoring, Earth Observation, Sustainable Development Goals},
abstract = {The SDGs are a universal agenda to address the world’s most pressing societal, environmental and economic challenges. The supply of timely, relevant and reliable data is essential in guiding policies and decisions for successful implementation of the SDGs. Yet official statistics cannot provide all of the data needed to populate the SDG indicator framework. Citizen science offers a novel solution and an untapped opportunity to complement traditional sources of data, such as household surveys, for monitoring progress towards the SDGs, while at the same time mobilizing action and raising awareness for their achievement. This paper presents the potential offered by one specific citizen science tool, Picture Pile, to complement and enhance official statistics to monitor several SDGs and targets. Designed to be a generic and flexible tool, Picture Pile is a web-based and mobile application for ingesting imagery from satellites, orthophotos, unmanned aerial vehicles or geotagged photographs that can then be rapidly classified by volunteers. The results show that Picture Pile could contribute to the monitoring of fifteen SDG indicators under goals 1, 2, 11, 13, 14 and 15 based on the Picture Pile campaigns undertaken to date. Picture Pile could also be modified to support other SDGs and indicators in the areas of ecosystem health, eutrophication and built-up areas, among others. In order to leverage this particular tool for SDG monitoring, its potential must be showcased through the development of use cases in collaboration with governments, NSOs and relevant custodian agencies. Additionally, mutual trust needs to be built among key stakeholders to agree on common goals that would facilitate the use of Picture Pile or other citizen science tools and data for SDG monitoring and impact.}
}
@article{HERRERA2022121466,
title = {The manipulation of Euribor: An analysis with machine learning classification techniques},
journal = {Technological Forecasting and Social Change},
volume = {176},
pages = {121466},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121466},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100901X},
author = {Rubén Herrera and Francisco Climent and Pedro Carmona and Alexandre Momparler},
keywords = {Euribor, Rate-fixing, Manipulation, Collusion, Panel bank, Machine learning, Classification},
abstract = {The manipulation of the Euro Interbank Offered Rate (Euribor) was an affair which had a great impact on international financial markets. This study tests whether advanced data processing techniques are capable of classifying Euribor panel banks as either manipulating or non-manipulating on the basis of patterns found in quotes submissions. For this purpose, panel banks’ daily contributions have been studied and monthly variables obtained that denote different contribution patterns for Euribor panel banks. Thus, in accordance with the court verdict, banks are categorized as manipulating and non-manipulating and Machine Learning classification techniques such as Supervised Learning, Anomaly Detection and Cluster Analysis are applied in order to discriminate between convicted and acquitted banks. The results show that out of seven manipulative banks, five are detected by Machine Learning using Deep Learning algorithms, all five presenting very similar contribution patterns. This is consistent with Anomaly Detection which confirms that several manipulating banks present similar levels of abnormality in their contributions. In addition, the Cluster Analysis facilitates gathering the five most active banks in illicit actions. In conclusion, administrators and supervisors might find these techniques useful to detect potentially illicit actions by banks involved in the Euribor rate-setting process.}
}
@article{SILERYTE2022131767,
title = {European Waste Statistics data for a Circular Economy Monitor: Opportunities and limitations from the Amsterdam Metropolitan Region},
journal = {Journal of Cleaner Production},
volume = {358},
pages = {131767},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.131767},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622013786},
author = {Rusne Sileryte and Arnout Sabbe and Vasileios Bouzas and Kozmo Meister and Alexander Wandl and Arjan {van Timmeren}},
keywords = {Circular Economy Monitor, European Waste Statistics, Amsterdam Metropolitan Region, Circular Economy Action Plan, Waste mapping},
abstract = {As appointed in the EU Circular Economy Action Plan, cities and regions in EU member countries start accompanying their circular economy strategies by monitoring frameworks, often called Circular Economy Monitors (CEM). Having the task to assess the performance towards the achievement of set targets and to steer decision-making, CEMs need to rely on a multitude of statistics and datasets. Waste statistics play an important role in circular economy monitoring as they provide insights into the remaining linear part of the economy. The collection of waste statistics is mandated by the European Commission which provides general guidelines on data collection and processing. The Netherlands has one of the most detailed waste registries among the EU countries. The country’s largest metropolitan region, Amsterdam, is currently building a CEM which tracks progress over time towards the set goals, highlights which areas need improvement and estimates target feasibility. This paper uses the Amsterdam CEM as a case-study to explore how the existing system of waste registration in the Netherlands is able to support decision-making. The data is explored with the help of four queries that relate to the CEM’s goals and require data mapping to be answered. The data mapping and analysis process has revealed several limitations present in the waste data collection and a number of gaps present in current circular economy research and data analysis. At the same time, the available data already supports significant insights into the status quo of the current waste system and provides opportunities for circular economy monitoring.}
}
@article{GUENTHER2022251,
title = {AI-Based Failure Management: Value Chain Approach in Commercial Vehicle Industry},
journal = {Procedia CIRP},
volume = {109},
pages = {251-256},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122006941},
author = {Robin Guenther and Sebastian Beckschulte and Martin Wende and Hendrik Mende and Robert H. Schmitt},
keywords = {Artificial intelligence, failure detection, failure management, machine learning, production, value chain},
abstract = {This paper describes an artificial intelligence (AI) based failure management approach across the value chain for the commercial vehicle industry by integrating and utilizing lifecycle data for product and production optimization. The amount of available data throughout a product lifecycle has increased significantly in previous years, primarily driven by the development and deployment of cyber-physical systems. While data from a single entity in the value chain already enables failure management-related analysis and services, including AI-based methods such as predictive maintenance, there remains a lack of systematic approaches to utilize data across the entire value chain. This paper proposes an AI-based failure management approach, which relies on integrating a variety of diverse data sources along the value chain. At first, three so-called application areas were defined: process and product optimization, availability optimization, and performance optimization. Consequently, practice-relevant use cases are identified for each area, for which it is shown how failures in the value chain can be proactively eliminated with the support of AI. Methods for predictive analytics are adapted for cross-value chain failure management to derive correlations between different stages of the production process and product usage. Based on these results and human expert knowledge, proactive measures are recommended by a decision support system (DSS) to resolve failures before arising. The commercial vehicle industry serves as an overarching validation case study for the practice-relevant verification of the targeted applications. The paper gives an outlook on the envisaged research work for the realization of holistic failure management.}
}
@article{JUNG202219,
title = {Comparative analysis of network-based approaches and machine learning algorithms for predicting drug-target interactions},
journal = {Methods},
volume = {198},
pages = {19-31},
year = {2022},
note = {Network-Based Approaches in Bioinformatics and Biomedicine},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2021.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1046202321002474},
author = {Yi-Sue Jung and Yoonbee Kim and Young-Rae Cho},
keywords = {Drug-target interactions, DTIs, DTI networks, Network-based approaches},
abstract = {Computational prediction of drug–target interactions (DTIs) is of particular importance in the process of drug repositioning because of its efficiency in selecting potential candidates for DTIs. A variety of computational methods for predicting DTIs have been proposed over the past decade. Our interest is which methods or techniques are the most advantageous for increasing prediction accuracy. This article provides a comprehensive overview of network-based, machine learning, and integrated DTI prediction methods. The network-based methods handle a DTI network along with drug and target similarities in a matrix form and apply graph-theoretic algorithms to identify new DTIs. Machine learning methods use known DTIs and the features of drugs and target proteins as training data to build a predictive model. Integrated methods combine these two techniques. We assessed the prediction performance of the selected state-of-the-art methods using two different benchmark datasets. Our experimental results demonstrate that the integrated methods outperform the others in general. Some previous methods showed low accuracy on predicting interactions of unknown drugs which do not exist in the training dataset. Combining similarity matrices from multiple features by data fusion was not beneficial in increasing prediction accuracy. Finally, we analyzed future directions for further improvements in DTI predictions.}
}
@article{ZHANG2022119510,
title = {Spatiotemporal neural network for estimating surface NO2 concentrations over north China and their human health impact},
journal = {Environmental Pollution},
volume = {307},
pages = {119510},
year = {2022},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2022.119510},
url = {https://www.sciencedirect.com/science/article/pii/S0269749122007242},
author = {Chengxin Zhang and Cheng Liu and Bo Li and Fei Zhao and Chunhui Zhao},
keywords = {Exposure assessment, Surface nitrogen dioxide, Deep learning, Health impact, Satellite remote sensing, Air quality prediction},
abstract = {Atmospheric nitrogen dioxide (NO2) is an important reactive gas pollutant harmful to human health. The spatiotemporal coverage provided by traditional NO2 monitoring methods is insufficient, especially in the suburban and rural areas of north China, which have a high population density and experience severe air pollution. In this study, we implemented a spatiotemporal neural network (STNN) model to estimate surface NO2 from multiple sources of information, which included satellite and in situ measurements as well as meteorological and geographical data. The STNN predicted NO2 with high accuracy, with a coefficient of determination (R2) of 0.89 and a root mean squared error of 5.8 μg/m3 for sample-based 10-fold cross-validation. Based on the surface NO2 concentration determined by the STNN, we analyzed the spatial distribution and temporal trends of NO2 pollution in north China. We found substantial drops in surface NO2 concentrations ranging between 9.1% and 33.2% for large cities during the 2020 COVID-19 lockdown when compared to those in 2019. Moreover, we estimated the all-cause deaths attributed to NO2 exposure at a high spatial resolution of about 1 km, with totals of 6082, 4200, and 18,210 for Beijing, Tianjin, and Hebei Provinces in 2020, respectively. We observed remarkable regional differences in the health impacts due to NO2 among urban, suburban, and rural areas. Generally, the STNN model could incorporate spatiotemporal neighboring information and infer surface NO2 concentration with full coverage and high accuracy. Compared with machine learning regression techniques, STNN can effectively avoid model overfitting and simultaneously consider both spatial and temporal correlations of input variables using deep convolutional networks with residual blocks. The use of the proposed STNN model, as well as the surface NO2 dataset, can benefit air quality monitoring, forecasting, and health burden assessments.}
}
@article{RUI2022123927,
title = {High-accuracy transient fuel consumption model based on distance correlation analysis},
journal = {Fuel},
volume = {321},
pages = {123927},
year = {2022},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2022.123927},
url = {https://www.sciencedirect.com/science/article/pii/S0016236122007864},
author = {Ding Rui and Jin Hui},
keywords = {Transient fuel consumption model, Model optimization, Distance correlation analysis},
abstract = {With the gradual aggravation of energy shortage, automobile energy saving has gained widespread attention from scholars. However, due to the lack of a high-accuracy practical fuel consumption model, it is difficult to estimate transient fuel consumption and evaluate the actual effect of real-time fuel consumption control strategies. Therefore, it is necessary to establish a more accurate and practical model according to the transient motion characteristics of the vehicles. To ensure the accuracy of the model, an integrated structure of the steady-state base module and the transient correction module is determined as the overall structure of the model. Based on the steady-state fuel consumption data, the steady-state base module is established. Then, based on the easily obtained vehicle and engine state parameters, principal component analysis and cluster analysis are used to reasonably classify different driving conditions of the vehicles. Following that, the distance correlation analysis is applied to find the combination of state parameters with the strongest correlation with the estimation error of the steady-state module, and a transient correction module is established according to the optimal state parameter combination obtained. After that, the optimal transient correction module structure is determined based on the Bayesian criterion. Finally, the model is tested, and the results show that the mean absolute percentage error (MAPE) of the fuel consumption estimation of the new model is about 15%, while that of the classical VT-Micro model and the VT-CPFM model are about 28% and 20%, respectively. It can be seen that the new model has a higher accuracy. On the other hand, compared with structured physical fuel consumption models such as VT-CPEM model, the new model has a simpler structure, a shorter computation time, and a higher computational speed. In addition, the new model has high practicability due to its clear structure and easy access to parameters.}
}
@article{DING2022116163,
title = {A study on data-driven hybrid heating load prediction methods in low-temperature district heating: An example for nursing homes in Nordic countries},
journal = {Energy Conversion and Management},
volume = {269},
pages = {116163},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116163},
url = {https://www.sciencedirect.com/science/article/pii/S019689042200944X},
author = {Yiyu Ding and Thomas Ohlson Timoudas and Qian Wang and Shuqin Chen and Helge Brattebø and Natasa Nord},
keywords = {Nursing homes, District heating load prediction, Linear regression, Artificial neural network, Low-temperature district heating},
abstract = {In the face of green energy initiatives and progressively increasing shares of more energy-efficient buildings, there is a pressing need to transform district heating towards low-temperature district heating. The substantially lowered supply temperature of low-temperature district heating broadens the opportunities and challenges to integrate distributed renewable energy, which requires enhancement on intelligent heating load prediction. Meanwhile, to fulfill the temperature requirements for domestic hot water and space heating, separate energy conversion units on user-side, such as building-sized boosting heat pumps shall be implemented to upgrade the temperature level of the low-temperature district heating network. This study conducted hybrid heating load prediction methods with long-term and short-term prediction, and the main work consisted of four steps: (1) acquisition and processing of district heating data of 20 district heating supplied nursing homes in the Nordic climate (2016–2019); (2) long-term district heating load prediction through linear regression, energy signature curve in hourly resolution, providing an overall view and boundary conditions for the unit sizing; (3) short-term district heating load prediction through two Artificial Neural Network models, f72 and g120, with different prediction input parameters; (4) evaluation of the predicted load profiles based on the measured data. Although the three prediction models met the quality criteria, it was found that including the historical hourly heating loads as the input to the forecasting model enhanced the prediction quality, especially for the peak load and low-mild heating season. Furthermore, a possible application of the heating load profiles was proposed by integrating two building-sized heat pumps in low-temperature district heating, which may be a promising heat supply method in low-temperature district heating.}
}
@article{LIU2022102503,
title = {Real-time multiscale prediction of structural performance in material extrusion additive manufacturing},
journal = {Additive Manufacturing},
volume = {49},
pages = {102503},
year = {2022},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102503},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421006503},
author = {Xin Liu and Chen Kan and Zehao Ye},
keywords = {Multiscale modeling, In-situ monitoring, Material extrusion, Honeycomb structure, Geometric defects},
abstract = {The material extrusion additive manufacturing (AM) has been extensively used in fabricating structures with complex geometries. However, geometric defects often exist in an AM structure, which could compromise its final performance. In this paper, a real-time multiscale performance evaluation method is developed for material extrusion-based honeycomb structures. The representative cell boundary is extracted from three-dimensional (3D) point clouds obtained via an in-situ monitoring approach. The cell boundary is then used to generate the digital twin of the unit cell of the printed layer based on the finite element (FE) method. A physics-based multiscale modeling approach called mechanics of structure genome (MSG) is then employed to predict the effective material properties of the printed layer and plate stiffness matrix of the final structure. The proposed approach provides a highly efficient way to predict the real-time performance of the as-manufactured products. Moreover, the numerical example shows that the geometric defects could result in complex mechanical behaviors in the defected parts, which cannot be captured by the conventional approaches based on the shape deviations. The numerical results are validated by the three-point bending tests. The proposed method can be used in the closed-loop control of material extrusion-based manufacturing systems.}
}
@article{HE2022694,
title = {How digitalized interactive platforms create new value for customers by integrating B2B and B2C models? An empirical study in China},
journal = {Journal of Business Research},
volume = {142},
pages = {694-706},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322000054},
author = {Jiaxun He and Shuang Zhang},
keywords = {Digitalized interactive platform, Platform value, Customer share, B2B2C},
abstract = {The traditional B2B model can’t meet the firm’s marketing and customer relationship management demands in the digital environment, and B2B and B2C separately is insufficient to the industry. Thus, we propose a digitalized interactive platform, which is a hybrid of B2B and B2C business models. This B2B2C (business-to-business-to-consumer) business model has transitioned from a traditional channel-driven mode to the integration of platform resources, reflecting the platform value. This study analyzes survey data collect from manufacturing and service industries based in China. The study finds that multi-dimensional platform value positively affects overall platform value, which in turn positively affects platform brand engagement. Moreover, platform brand engagement positively influences platform brand loyalty, ultimately increasing customer share. Thus, the study provides a new theoretical explanation and managerial implication for the value creation of digitalized interactive platforms.}
}
@article{BUMAN2022112984,
title = {Towards consistent assessments of in situ radiometric measurements for the validation of fluorescence satellite missions},
journal = {Remote Sensing of Environment},
volume = {274},
pages = {112984},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.112984},
url = {https://www.sciencedirect.com/science/article/pii/S0034425722000980},
author = {Bastian Buman and Andreas Hueni and Roberto Colombo and Sergio Cogliati and Marco Celesti and Tommaso Julitta and Andreas Burkart and Bastian Siegmann and Uwe Rascher and Matthias Drusch and Alexander Damm},
keywords = {Sun-induced chlorophyll fluorescence, Spectroradiometer, Uncertainty, Bias, Measurement variability, Spectral shift, FLEX, FloX},
abstract = {The upcoming Fluorescence Explorer (FLEX) satellite mission aims to provide high quality radiometric measurements for subsequent retrieval of sun-induced chlorophyll fluorescence (SIF). The combination of SIF with other observations stemming from the FLEX/Sentinel-3 tandem mission holds the potential to assess complex ecosystem processes. The calibration and validation (cal/val) of these radiometric measurements and derived products are central but challenging components of the mission. This contribution outlines strategies for the assessment of in situ radiometric measurements and retrieved SIF. We demonstrate how in situ spectrometer measurements can be analysed in terms of radiometric, spectral and spatial uncertainties. The analysis of more than 200 k spectra yields an average bias between two radiometric measurements by two individual spectrometers of 8%, with a larger variability in measurements of downwelling radiance (25%) compared to upwelling radiance (6%). Spectral shifts in the spectrometer relevant for SIF retrievals are consistently below 1 spectral pixel (up to 0.75). Found spectral shifts appear to be mostly dependent on temperature (as measured by a temperature probe in the instrument). Retrieved SIF shows a low variability of 1.8% compared with a noise reduced SIF estimate based on APAR. A combination of airborne imaging and in situ non-imaging fluorescence spectroscopy highlights the importance of a homogenous sampling surface and holds the potential to further uncover SIF retrieval issues as here shown for early evening acquisitions. Our experiments clearly indicate the need for careful site selection, measurement protocols, as well as the need for harmonized processing. This work thus contributes to guiding cal/val activities for the upcoming FLEX mission.}
}
@article{CHONDRODIMA2022100086,
title = {Particle swarm optimization and RBF neural networks for public transport arrival time prediction using GTFS data},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100086},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100086},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000295},
author = {Eva Chondrodima and Harris Georgiou and Nikos Pelekis and Yannis Theodoridis},
keywords = {Estimated time of arrival (ETA), Fuzzy means, General transit feed specification (GTFS), Intelligent transportation systems, Neural networks (NN), Particle swarm optimization (PSO), Public transport},
abstract = {Accurate prediction of Public Transport (PT) mobility is important for intelligent transportation. Nowadays, mobility data have become increasingly available with the General Transit Feed Specification (GTFS) being the format for PT agencies to disseminate such data. Estimated Time of Arrival (ETA) of PT is crucial for the public, as well as the PT agency for logistics, route-optimization, maintenance, etc. However, prediction of PT-ETA is a challenging task, due to the complex and non-stationary urban traffic. This work introduces a novel data-driven approach for predicting PT-ETA based on RBF neural networks, using a modified version of the successful PSO-NSFM algorithm for training. Additionally, a novel pre-processing pipeline (CR-GTFS) is designed for cleansing and reconstructing the GTFS data. The combination of PSO-NSFM and CR-GTFS introduces a complete framework for predicting PT-ETA accurately with real-world data feeds. Experiments on GTFS data verify the proposed approach, outperforming state-of-the-art in prediction accuracy and computational times.}
}
@article{BORCH2022101852,
title = {Machine learning, knowledge risk, and principal-agent problems in automated trading},
journal = {Technology in Society},
volume = {68},
pages = {101852},
year = {2022},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101852},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21003274},
author = {Christian Borch},
keywords = {Automated trading, Financial markets, Knowledge risk, Machine learning, Principal-agent problems},
abstract = {Present-day securities trading is dominated by fully automated algorithms. These algorithmic systems are characterized by particular forms of knowledge risk (adverse effects relating to the use or absence of certain forms of knowledge) and principal-agent problems (goal conflicts and information asymmetries arising from the delegation of decision-making authority). Where automated trading systems used to be based on human-defined rules, increasingly, machine-learning (ML) techniques are being adopted to produce machine-generated strategies. Drawing on 213 interviews with market participants involved in automated trading, this study compares the forms of knowledge risk and principal-agent relations characterizing both human-defined and ML-based automated trading systems. It demonstrates that certain forms of ML-based automated trading lead to a change in knowledge risks, particularly concerning dramatically changing market settings, and that they are characterized by a lack of insight into how and why trading rules are being produced by the ML systems. This not only intensifies but also reconfigures principal-agent problems in financial markets.}
}
@article{JADHAV2022100328,
title = {A review study of the blockchain-based healthcare supply chain},
journal = {Social Sciences & Humanities Open},
volume = {6},
number = {1},
pages = {100328},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100328},
url = {https://www.sciencedirect.com/science/article/pii/S2590291122000821},
author = {Jayendra S. Jadhav and Jyoti Deshmukh},
keywords = {Blockchain, Blockchain technology, Healthcare supply chain, Healthcare, Review, Hybrid reinforcement, IPFS},
abstract = {Technological acclimatization in today's healthcare industry is a subject of new inventions. The worldwide Covid-19 epidemic has led to increase in the use of technology for healthcare supply chain, patient data management, and claims settlement. Data management in healthcare industry is a complex structure where multiple organizations provide proper supply chain services in day to day life. Improper data management disrupts the supply chain, which has a long-term impact on the healthcare sector. Various issues in the present supply chain must be addressed. Blockchain-based crypto-currencies are well-known nowadays for their ability to create safe and traceable solutions. With the growing use of crypto-currencies, it also governs new range of applications and opportunities, including healthcare applications. Blockchain-based solutions are effective in the health sector for secure data retrieval and storage, resulting in more effectual product creation and tracking. Such system can provide data provenance, promotes genuine healthcare sector demands, and ensures the immutability of multi-direction transactions. In this study, we contribute a thorough overview of the literature on how Blockchain technology is changing the way healthcare supply chains operate. We looked at 61 papers from 2019 to 2021 that highlighted various difficulties with the traditional healthcare supply chain. We scrutinized different barriers and opportunity of Blockchain-based healthcare supply chain at the end of the research.}
}
@article{HAO202228428,
title = {Exploration of the oxidation and ablation resistance of ultra-high-temperature ceramic coatings using machine learning},
journal = {Ceramics International},
volume = {48},
number = {19, Part A},
pages = {28428-28437},
year = {2022},
issn = {0272-8842},
doi = {https://doi.org/10.1016/j.ceramint.2022.06.156},
url = {https://www.sciencedirect.com/science/article/pii/S0272884222021599},
author = {Jie Hao and Lihong Gao and Zhuang Ma and Yanbo Liu and Ling Liu and Shizhen Zhu and Weizhi Tian and Xiaoyu Liu and Zhigang Zhou and Alexandr A. Rogachev and Hanyang Liu},
keywords = {Ultra-high-temperature ceramic coatings, Oxidation and ablation resistance, Machine learning, Property prediction},
abstract = {Carbon fiber-reinforced carbon matrix composites (C/C) will be easily oxidized in high temperatures, which will have a great negative effect on their performance. Preparing ultra-high-temperature ceramic (UHTC) coatings is a well-established method to improve the oxidation and ablation resistance of C/C. However, it is time-consuming and costly to obtain these coatings through the traditional experimental method. Motivated by the outstanding performance of machine learning (ML) algorithms in many fields, this study adopts ML algorithms based on historical experimental datasets to build a model. This model will predict the oxidation and ablation resistance, represented by mass ablation rate. For this purpose, variables that affect the mass ablation rate and are easily accessible were used as input features. That includes the chemical composition and essential physics/chemistry properties of coatings and experimental parameters. Seven different ML algorithms were used to establish the model; namely, ridge regression (Ridge), lasso regression (Lasso), kernel ridge regression (KRR), support vector regression (SVR), random forest regression (RFR), AdaBoost regression (ABR), and bagging regression (Bagging). The results show that RFR has the optimal generalization performance with a mean absolute error (MAE) of 0.55, mean-squared error (MSE) of 0.71 and coefficient of determination (R2) of 0.87 on the testing set. SHapley Additive exPlanations (SHAP) analysis of the RFR model explained how these input features affect the mass ablation rate and further provided the critical features for performance prediction. The model established in this study can predict coating performance accurately and accelerate the development of UHTC-coated C/C composites from a data-driven perspective.}
}
@article{CECCHI202251,
title = {Current state and future directions of genomic medicine in aortic dissection: A path to prevention and personalized care},
journal = {Seminars in Vascular Surgery},
volume = {35},
number = {1},
pages = {51-59},
year = {2022},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2022.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0895796722000060},
author = {Alana C. Cecchi and Madeline Drake and Chrisanne Campos and Jake Howitt and Jonathan Medina and Scott M. Damrauer and Sherene Shalhub and Dianna M. Milewicz},
abstract = {ABSTRACT
Aortic dissection confers high mortality and morbidity rates despite advances in treatment, impacts quality of life, and contributes immense burden to the healthcare system globally. Efforts to prevent aortic dissection through screening and management of modifiable risk factors and early detection of aneurysms should incorporate genomic information, as it is integral to stratifying risk. However, effective integration of genomic-guided risk assessment into clinical practice will require addressing implementation barriers that currently permeate our healthcare systems. The Aortic Dissection Collaborative was established to define aortic dissection research priorities through patient engagement. Using a collaborative patient-centered feedback model, our Genomic Medicine Working Group identified related research priorities that could be investigated by pragmatic interventional studies aimed at aortic dissection prevention, utilization of genomic information to improve patient outcomes, and access to genomic medicine services. Further research is also needed to identify the genomic, lifestyle, and environmental risk factors that contribute to aortic dissection so these data can be incorporated into future comparative effectiveness studies to prevent aortic dissection.}
}
@article{WU2022103421,
title = {Amenity, firm agglomeration, and local creativity of producer services in Shanghai},
journal = {Cities},
volume = {120},
pages = {103421},
year = {2022},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2021.103421},
url = {https://www.sciencedirect.com/science/article/pii/S0264275121003206},
author = {Yangyi Wu and Yehua Dennis Wei and Han Li and Meitong Liu},
keywords = {Producer services, Creativity, Urban amenity, Shanghai},
abstract = {Studies of creativity in urban China are heavily confined at the interurban level and have been criticized for unclear spatial mechanisms and missing local context. This study constructs a theoretical framework to understand the role of urban amenity on the local attractiveness to producer services and further analyzes such attractiveness in Shanghai in terms of agglomeration and creativity using open data. We find that creative firms are more clustered than other producer service firms and urban amenities in Shanghai. The regression results show that urban amenity is strong in explaining local attractiveness to creativity rather than firm agglomeration at the 1-km scale. The attractiveness of the local urban area to creativity may be affected by urban amenity in various ways, including co-location, accessibility, and high-density clusters. Such relationships also follow Shanghai's monocentric structure. The importance of urban amenity decays as the distance to the central business district (CBD) increases in regards to firm agglomeration but persists in terms of creativity level. These findings highlight the importance of considering the co-existence of different spatial relationships and accentuate the differentiated applicability of industrial agglomeration and creativity theories.}
}
@article{ANUOLUWABAMIDELE2022101593,
title = {Discovery and prediction capabilities in metal-based nanomaterials: An overview of the application of machine learning techniques and some recent advances},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101593},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101593},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622000659},
author = {Emmanuel {Anuoluwa Bamidele} and Ahmed {Olanrewaju Ijaola} and Michael Bodunrin and Oluwaniyi Ajiteru and Afure {Martha Oyibo} and Elizabeth Makhatha and Eylem Asmatulu},
keywords = {Machine Learning, Metal-based nanomaterials, Nanoinformatics, Computational Materials, Nanotechnology, Inorganic nanoparticles},
abstract = {The application of machine learning (ML) techniques to metal-based nanomaterials has contributed greatly to understanding the interaction of nanoparticles, properties prediction, and new materials discovery. However, the prediction accuracy and efficiency of distinctive ML algorithms differ with different metal-based nanomaterials problems. This, alongside the high dimensionality and nonlinearity of available datasets in metal-based nanomaterials problems, makes it imperative to review recent advances in the implementation of ML techniques for these kinds of problems. In addition to understanding the applicability of different ML algorithms to various kinds of metal-based nanomaterials problems, it is hoped that this work will help facilitate understanding and promote interest in this emerging and less explored area of materials informatics. The scope of this review covers the introduction of metal-based nanomaterials, several techniques used in generating datasets for training ML models, feature engineering techniques used in nanomaterials-machine learning applications, and commonly applied ML algorithms. Then, we present the recent advances in ML applications to metal-based nanomaterials, with emphasis on the procedure and efficiency of algorithms used for such applications. In the concluding section, we identify the most common and efficient algorithms for distinctive property predictions. The common problems encountered in ML applications for metal-based nanoinformatics were mentioned. Finally, we propose suitable solutions and future outlooks for various challenges in metal-based nanoinformatics research.}
}
@article{ZANTI2022102093,
title = {Leveraging integrated data for program evaluation: Recommendations from the field},
journal = {Evaluation and Program Planning},
volume = {95},
pages = {102093},
year = {2022},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2022.102093},
url = {https://www.sciencedirect.com/science/article/pii/S0149718922000477},
author = {Sharon Zanti and Emily Berkowitz and Matthew Katz and Amy Hawn Nelson and T.C. Burnett and Dennis Culhane and Yixi Zhou},
keywords = {Cross-sector data linkage, Integrated data systems, Administrative data reuse},
abstract = {Use of administrative data to inform decision making is now commonplace throughout the public sector, including program and policy evaluation. While reuse of these data can reduce costs, improve methodologies, and shorten timelines, challenges remain. This article informs evaluators about the growing field of Integrated Data Systems (IDS), and how to leverage cross-sector administrative data in evaluation work. This article is informed by three sources: a survey of current data integration efforts in the United States (U.S.) (N=63), informational interviews with experts, and internal knowledge cultivated through Actionable Intelligence for Social Policy’s (AISP) 12+ years of work in the field. A brief discussion of the U.S. data integration context and history is provided, followed by discussion of tangible recommendations for evaluators, examples of evaluations relying on integrated data, and a list of U.S. IDS sites with publicly available processes for external data requests. Despite the challenges associated with reusing administrative data for program evaluation, IDS offer evaluators a new set of tools for leveraging data across institutional silos.}
}
@article{GUENDUEZ2022101719,
title = {Strategically constructed narratives on artificial intelligence: What stories are told in governmental artificial intelligence policies?},
journal = {Government Information Quarterly},
pages = {101719},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101719},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000521},
author = {Ali A. Guenduez and Tobias Mettler},
keywords = {Artificial intelligence (AI), Policy research, Structural topic modeling (STM), Narrative policy framework (NPF), Role of government},
abstract = {What stories are told in national artificial intelligence (AI) policies? Combining the novel technique of structural topic modeling (STM) and qualitative narrative analysis, this paper examines the policy narratives in 33 countries’ AI policies. We uncover six common narratives that are dominating the political agenda concerning AI. Our findings show that the policy narratives' saliences vary across time and countries. We make several contributions. First, our narratives describe well-grounded, supportable conceptions of AI among governments, and show that AI is still a fairly novel, multilayered, and controversial phenomenon. Building on the premise that human sensemaking is best represented and supported by narration, we address the applied rhetoric of governments to either minimize the risks or exalt the opportunities of AI. Second, we uncover the four prominent roles governments seek  to take concerning AI implementation: enabler, leader, regulator, and/or user. Third, we make a methodological contribution toward data-driven, computationally-intensive theory development. Our methodological approach and the identified narratives present key starting points for further research.}
}
@article{WANG2022122309,
title = {Research on thermal load prediction of district heating station based on transfer learning},
journal = {Energy},
volume = {239},
pages = {122309},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122309},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221025573},
author = {Chendong Wang and Jianjuan Yuan and Ke Huang and Ji Zhang and Lihong Zheng and Zhihua Zhou and Yufeng Zhang},
keywords = {District heating, District heating station, Heating energy consumption prediction, Machine learning, Transfer learning},
abstract = {Precise prediction of thermal load plays a critical role in China to fulfill the demand of energy saving, carbon emission reduction and environmental protection, and to realize the "3060″ target. This study proposed layer transfer model and merged transfer model for thermal load prediction of the district heating station. Experiment schemes were elaborated to simulate cross-year and cross-site scenarios, and practical data was collected serving the experiments. The prediction accuracy can be maintained without degradation in cross-year scenario, specifically, the coefficient of variation of the root mean squared error fluctuated between −1.09% and +0.45% compared to previous heating season when proposed two models were used. In the cross-site scenario, proposed models can achieve good prediction performance when the training data is insufficient. The coefficient of variation of the root mean squared error of the new model with insufficient training data was reduced by 7.62% on average when merged transfer model was used, which is equivalent to an overall reduction of 41.67%. Furthermore, proposed models can be applied to further optimize the prediction performance, even if beyond the scenarios discussed in this study.}
}
@article{SINGH20225021,
title = {Blockchain with cloud for handling healthcare data: A privacy-friendly platform},
journal = {Materials Today: Proceedings},
volume = {62},
pages = {5021-5026},
year = {2022},
note = {International Conference on Innovative Technology for Sustainable Development},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.04.910},
url = {https://www.sciencedirect.com/science/article/pii/S221478532203098X},
author = {Suruchi Singh and Bhatt Pankaj and K. Nagarajan and Neha {P. Singh} and Veer Bala},
keywords = {Data management, Cloud computing, e-Health, Cloud E-health, E-health security},
abstract = {In addition to its unique characteristics, health care data make it an attractive target for criminals. Additionally, healthcare data is highly regulated by US & EU privacy and security laws and international laws governing data storage in the cloud. The value of healthcare data and these regulatory requirements have motivated organizations to use blockchain to protect data. Blockchain will facilitate effective data exchange while maintaining patient privacy and data protection. Cloud data has long been a draw for cyber attackers. Today, cloud health data has been their latest interest. The cloud-based application that theoretically transforms the operation and communication of existing healthcare networks is EHR. The EHR is a digital file comprising the personal details of citizens/patients (name, photo, address, age, etc.) and health (existing diseases, previous surgeries, allergies, blood type, allergies, vaccinations, etc.). Maximizing cost efficiency is one of the drivers for integrating cloud services in health care. The return on investment for healthcare services is better when using cloud solutions. The cloud infrastructure includes a centralized medical record entry point, allowing many clinicians to display laboratory reports or consult patient notes. Loud technologies would allow researchers to explore these data and analyses public health more accurately. Cloud technology is not the best way to handle the medical services and records of the healthcare system. Nevertheless, this technology allows patients and physicians access to software that will enhance patient care. The Cloud also provides an excellent opportunity to utilize patient data on a health computer scale to draw insights on health and allow patients to manage personal details quickly and efficiently. There are hurdles in introducing emerging technologies and threatening privacy violations, but health services will utilize this increasingly growing and valuable technology with the appropriate training and framework. As part of this paper, we present a patient-centric healthcare data management solution that uses blockchain technology as a storage mechanism that helps to ensure patient privacy.}
}
@article{YAN2022103663,
title = {Weak celestial source fringes detection based on channel attention shrinkage networks and cluster-based anchor boxes generation algorithm},
journal = {Digital Signal Processing},
volume = {129},
pages = {103663},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103663},
url = {https://www.sciencedirect.com/science/article/pii/S1051200422002809},
author = {Ruiqing Yan and Rong Ma and Wei Liu and Zongyao Yin and Zhengang Zhao and Siying Chen and Sheng Chang and Hui Zhu and Dan Hu and Xianchuan Yu},
keywords = {Deep learning, Weak signal detection, Celestial source fringe, Soft thresholding, Astronomical image processing},
abstract = {Detecting weak celestial source signals from massive radio data is a very challenging task because the radiation received by radio telescope is very weak and prone to disturbances. In order to detect these weak signals, we propose a two-stage object detection method that performs more finely in computer vision tasks. The novelty of the proposed method is to combine traditional soft thresholding denoising methods with attention mechanisms in deep neural networks. We propose a channel attention shrinkage network as the backbone of the object detection model to extract the features of weak signals from celestial sources by removing noise-related information. Moreover, targeting the characteristics of celestial source fringes in phase images, we propose a cluster-based anchor boxes generation algorithm to improve the accuracy of fringes position detection. We also introduce the CIoU loss function to improve the performance of the model because of the large aspect ratio of the celestial source fringes in the phase image. We generate simulated celestial source fringes data based on the parameters of the observation system to train our model and conduct experiments to evaluate the performance of the proposed algorithm. Our model obtains satisfactory detection accuracy and accurate for the location of celestial source fringes.}
}
@article{GRENYER202237,
title = {Multistep prediction of dynamic uncertainty under limited data},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {37},
pages = {37-54},
year = {2022},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2022.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1755581722000025},
author = {Alex Grenyer and Oliver Schwabe and John A. Erkoyuncu and Yifan Zhao},
keywords = {Forecast, Limited data, Long-short term memory (LSTM), Multistep, Prediction, Spatial geometry, Uncertainty},
abstract = {Engineering systems are growing in complexity, requiring increasingly intelligent and flexible methods to account for and predict uncertainties in service. This paper presents a framework for dynamic uncertainty prediction under limited data (UPLD). Spatial geometry is incorporated with LSTM networks to enable real-time multistep prediction of quantitative and qualitative uncertainty over time. Validation is achieved through two case studies. Results demonstrate robust prediction of trends in limited and dynamic uncertainty data with parallel determination of geometric symmetry at each time unit. Future work is recommended to explore alternative network architectures suited to limited data scenarios.}
}
@article{NEUNZIG2022320,
title = {Model Selection for Predictive Quality in Hydraulic Testing},
journal = {Procedia CIRP},
volume = {107},
pages = {320-325},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.052},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002682},
author = {Christian Neunzig and Simon Fahle and Jürgen Schulz and Matthias Möller and Bernd Kuhlenkötter},
keywords = {Hydraulic Testing, Machine Learning, Predictive Quality, Supervised Learning},
abstract = {Manufacturing companies are confronted with enormous challenges such as increasing product complexity, shorter product life cycles and growing product diversity. Politically and socially, increased demands regarding sustainability and resource consumption are streaming into the focus of companies. One solution strategy to increase the productivity of existing production systems while ensuring existing quality standards is the application of data-driven analytical methods such as machine learning. Due to the frequent changes in production conditions, the analysis of real manufacturing data is linked to sophisticated data pre-processing. Changes in production data are manifested in trends and systematic shifts over time. Data pre-processing includes rule-based data cleaning, the application of dimension reduction techniques and the identification of comparable data subsets. Within the used dataset of hydraulic valves by Bosch, the comparability of the same production conditions in the manufacturing of hydraulic valves can be identified within certain periods. Machine learning methods can process large amounts of data, unfavorable row-column ratios and discover dependencies between the input data and the specified target variable as well as evaluate the multidimensional influence of all input variables on the target variable. For use cases in manufacturing, neural networks, support vector machines and tree-based methods have so far proved to be very successful. The use of cross-process production data along the value chain of hydraulic valves is a promising approach to predict the quality characteristics of workpieces. Within this research, machine learning methods with deep and shallow structures are applied to predict the internal leakage of hydraulic valves based on geometric gauge blocks from machining, mating data from assembly and hydraulic measurement data from end-of-line testing. Moreover, the most suitable methods are selected, and accurate quality predictions are obtained.}
}
@article{JADHAV2022186,
title = {An enhanced and secured predictive model of Ada-Boost and Random-Forest techniques in HCV detections},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {186-195},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.071},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321036476},
author = {Dhaval A Jadhav},
keywords = {Ada-Boost, Random-Forest, ECC, Hepatitis, HCC, SHA, HCV- predictions},
abstract = {The Evolution of HCV-Hepatitis C-virus plays the vital cause for liver-related complications in humans, global-wide. But also, specific tools were employed to eliminate the impacts of virus in humans. This phenomena does not rely in the prior stage diagnosis of the virus and in the treatment phases. In this study, the implementation of Ada-Boost algorithm and Random-forest algorithm in the framing the HCV-Hepatitis-C-Virus prediction design prevailing in humans. According to the methodology, Random-forest algorithm were utilized as the weak-learner for choosing the instances of the weight to enhance the stability factors, accuracy-factors and to decrease the outfitting complications. The paper aims to design the secured framework in the improvisation of the user’s privacy. Hence for this purpose, The techniques PPDM-Privacy-Preserving-Data mining approach were developed to keep preserve of the personal data of the userscattered in Distributed-data-mining operational system and also enhancing the security in centralized systems of data-mining as well. The data-set were subjected to the comparison of the ECC-encryption algorithm and the RSA-based encryption along with the blend of SHA-algorithm. The algorithms were evaluated with various sizes of key and proceeded with the decryption process. The Decrypted data, applied with Ada-boost and Random-Forest algorithm in qualitative predictions. Data were partitioned as the test and training sets by the classifiers. The overall-efficiency of the proposed hybrid framework were assessed by the aid of performance-metrics such as accuracy factor, time-factor and specificity-factor. The experimental analysis of the framework, establishes the classifier and various merged classifiers in the predictions of HCV-Stains. The outcomes of the framework exhibited higher accuracy rate in comparison with the other approaches, thus enabling the efficient predictions of HCV.}
}
@article{SI2022103384,
title = {Can government regulation, carbon-emission reduction certification and information publicity promote carpooling behavior?},
journal = {Transportation Research Part D: Transport and Environment},
volume = {109},
pages = {103384},
year = {2022},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2022.103384},
url = {https://www.sciencedirect.com/science/article/pii/S1361920922002127},
author = {Hongyun Si and Yangyue Su and Guangdong Wu and Wenxiang Li and Long Cheng},
keywords = {Carpooling, Carbon-emission reduction, Information publicity, Theory of planned behavior, Shared mobility, Ridesharing},
abstract = {This study constructs a novel theoretical framework to uncover the effects of government regulation, carbon-emission reduction certification and information publicity on people’s carpooling behavior. Survey data on 1056 potential users from China were empirically examined using partial least squares structural equation modeling and multigroup analysis. The results reveal that government regulation and carbon-emission reduction certification can significantly improve users’ carpooling intention. Information publicity not only positively influences carpooling intention but also increases carpooling behavior. Interestingly, government regulation has a more significant effect on female users, users with high educational levels and young users. Carbon-emission reduction certification may negatively affect female users’ carpooling behavior. Information publicity is even effective at improving the older users’ carpooling intention. This research provides new evidence and serves as an insightful decision-making reference for policymakers and operators worldwide seeking to encourage people’s carpooling behavior.}
}
@article{BOEING2022e907,
title = {Using open data and open-source software to develop spatial indicators of urban design and transport features for achieving healthy and sustainable cities},
journal = {The Lancet Global Health},
volume = {10},
number = {6},
pages = {e907-e918},
year = {2022},
issn = {2214-109X},
doi = {https://doi.org/10.1016/S2214-109X(22)00072-9},
url = {https://www.sciencedirect.com/science/article/pii/S2214109X22000729},
author = {Geoff Boeing and Carl Higgs and Shiqin Liu and Billie Giles-Corti and James F Sallis and Ester Cerin and Melanie Lowe and Deepti Adlakha and Erica Hinckson and Anne Vernez Moudon and Deborah Salvo and Marc A Adams and Ligia V Barrozo and Tamara Bozovic and Xavier Delclòs-Alió and Jan Dygrýn and Sara Ferguson and Klaus Gebel and Thanh Phuong Ho and Poh-Chin Lai and Joan C Martori and Kornsupha Nitvimol and Ana Queralt and Jennifer D Roberts and Garba H Sambo and Jasper Schipperijn and David Vale and Nico {Van de Weghe} and Guillem Vich and Jonathan Arundel},
abstract = {Summary
Benchmarking and monitoring of urban design and transport features is crucial to achieving local and international health and sustainability goals. However, most urban indicator frameworks use coarse spatial scales that either only allow between-city comparisons, or require expensive, technical, local spatial analyses for within-city comparisons. This study developed a reusable, open-source urban indicator computational framework using open data to enable consistent local and global comparative analyses. We show this framework by calculating spatial indicators—for 25 diverse cities in 19 countries—of urban design and transport features that support health and sustainability. We link these indicators to cities’ policy contexts, and identify populations living above and below critical thresholds for physical activity through walking. Efforts to broaden participation in crowdsourcing data and to calculate globally consistent indicators are essential for planning evidence-informed urban interventions, monitoring policy effects, and learning lessons from peer cities to achieve health, equity, and sustainability goals.}
}
@article{DESILVA2022100489,
title = {An artificial intelligence life cycle: From conception to production},
journal = {Patterns},
volume = {3},
number = {6},
pages = {100489},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100489},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922000745},
author = {Daswin {De Silva} and Damminda Alahakoon},
keywords = {artificial intelligence, AI, AI life cycle, machine learning, AI design, AI development, AI deployment, AI operationalization},
abstract = {Summary
This paper presents the “CDAC AI life cycle,” a comprehensive life cycle for the design, development, and deployment of artificial intelligence (AI) systems and solutions. It addresses the void of a practical and inclusive approach that spans beyond the technical constructs to also focus on the challenges of risk analysis of AI adoption, transferability of prebuilt models, increasing importance of ethics and governance, and the composition, skills, and knowledge of an AI team required for successful completion. The life cycle is presented as the progression of an AI solution through its distinct phases—design, develop, and deploy—and 19 constituent stages from conception to production as applicable to any AI initiative. This life cycle addresses several critical gaps in the literature where related work on approaches and methodologies are adapted and not designed specifically for AI. A technical and organizational taxonomy that synthesizes the functional value of AI is a further contribution of this article.}
}
@article{HOSSAINLIPU2022105752,
title = {Deep learning enabled state of charge, state of health and remaining useful life estimation for smart battery management system: Methods, implementations, issues and prospects},
journal = {Journal of Energy Storage},
volume = {55},
pages = {105752},
year = {2022},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.105752},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X22017406},
author = {M.S. {Hossain Lipu} and Shaheer Ansari and Md. Sazal Miah and Sheikh T. Meraj and Kamrul Hasan and A.S.M. Shihavuddin and M.A. Hannan and Kashem M. Muttaqi and Aini Hussain},
keywords = {Deep learning, State of charge, State of health, Remaining useful life, Battery management system, And electric vehicle},
abstract = {State of Charge (SOC), state of health (SOH), and remaining useful life (RUL) are the crucial indexes used in the assessment of electric vehicle (EV) battery management systems (BMS). The performance and efficiency of EVs are subject to the precise estimation of SOC, SOH, and RUL in BMS which enhances the battery reliability, safety, and longevity. However, the estimation of SOC, SOH, and RUL is challenging due to the battery capacity degradation and varying environmental conditions. Recently, deep learning (DL) has received wide attention for battery SOC, SOH, and RUL estimation due to the accessibility of a vast amount of data, large storage volume, and powerful computing processors. Nevertheless, the application of DL in SOC, SOH, and RUL estimation for EVs is still limited. Therefore, the novelty of this paper is to deliver a comprehensive review of DL-enabled SOC, SOH, and RUL estimation for BMS, focusing on methods, implementations, strengths, weaknesses, issues, accuracy, and contributions. Moreover, this study explores the numerous important implementation factors of DL methods concerning data type, features, size, preprocessing, algorithm operation, functions, hyperparameter adjustments, and performance evaluation. Additionally, the review explores various limitations and challenges of DL in BMS related to battery, algorithm, and operational issues. Finally, future opportunities and prospects are delivered that would support the EV engineers and automotive industries to establish an accurate and robust DL-based SOC, SOH, and RUL estimation technique towards smart BMS in future sustainable EV applications.}
}
@article{MU2022119683,
title = {A two-stage scheduling method for integrated community energy system based on a hybrid mechanism and data-driven model},
journal = {Applied Energy},
volume = {323},
pages = {119683},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119683},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922009813},
author = {Yunfei Mu and Yurui Xu and Yan Cao and Wanqing Chen and Hongjie Jia and Xiaodan Yu and Xiaolong Jin},
keywords = {Dynamic energy hub (DEH), integrated community energy system (ICES), Off-design performance of equipment, Hybrid mechanism and data-driven model, Two-stage scheduling method},
abstract = {The integrated community energy system (ICES) is an effective means to promote the synergies among multiple energy carriers. However, the off-design performance of equipment challenges the accurate and economical scheduling of the ICES. To solve this problem, a two-stage scheduling method for the ICES based on a hybrid mechanism and data-driven model is proposed in this paper. Combing the mechanism energy hub (EH) model with a data-driven efficiency correction model, a hybrid-driven dynamic energy hub (DEH) with variable equipment efficiency is built first. The EH describes the multi-energy coupling relationship; the embedded efficiency correction model adopts data-driven approaches of polynomial regression (PR) and backpropagation neural networks (BPNNs) to accurately extract nonlinear characteristics of equipment efficiency. On this basis, a two-stage scheduling model for the ICES is developed. In the day-ahead stage, the PR method is applied to calculate equipment efficiency which varies with load rate. The day-ahead scheduling model is established with the aim of minimizing the operating cost. In the intraday stage, considering the effects of load rate, temperature, and atmospheric pressure, the BPNNs method is employed to further correct equipment efficiency using the latest data. Furthermore, a rolling optimization (RO) strategy is used to address the uncertainties of equipment efficiency and load demand to improve the accuracy and economy of the scheduling scheme. Case studies demonstrate that the proposed method can improve the solution speed and accuracy of the scheduling model, and enhance the operating economy of the ICES.}
}
@article{DWIVEDI2022102456,
title = {Climate change and COP26: Are digital technologies and information management part of the problem or the solution? An editorial reflection and call to action},
journal = {International Journal of Information Management},
volume = {63},
pages = {102456},
year = {2022},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102456},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221001493},
author = {Yogesh K. Dwivedi and Laurie Hughes and Arpan Kumar Kar and Abdullah M. Baabdullah and Purva Grover and Roba Abbas and Daniela Andreini and Iyad Abumoghli and Yves Barlette and Deborah Bunker and Leona {Chandra Kruse} and Ioanna Constantiou and Robert M. Davison and Rahul De’ and Rameshwar Dubey and Henry Fenby-Taylor and Babita Gupta and Wu He and Mitsuru Kodama and Matti Mäntymäki and Bhimaraya Metri and Katina Michael and Johan Olaisen and Niki Panteli and Samuli Pekkola and Rohit Nishant and Ramakrishnan Raman and Nripendra P. Rana and Frantz Rowe and Suprateek Sarker and Brenda Scholtz and Maung Sein and Jeel Dharmeshkumar Shah and Thompson S.H. Teo and Manoj Kumar Tiwari and Morten Thanning Vendelø and Michael Wade},
keywords = {Climate change, COP26, Digital world, Information management, Information systems, Information technology, Sustainability, Sustainable Development Goals (SDGs)},
abstract = {The UN COP26 2021 conference on climate change offers the chance for world leaders to take action and make urgent and meaningful commitments to reducing emissions and limit global temperatures to 1.5 °C above pre-industrial levels by 2050. Whilst the political aspects and subsequent ramifications of these fundamental and critical decisions cannot be underestimated, there exists a technical perspective where digital and IS technology has a role to play in the monitoring of potential solutions, but also an integral element of climate change solutions. We explore these aspects in this editorial article, offering a comprehensive opinion based insight to a multitude of diverse viewpoints that look at the many challenges through a technology lens. It is widely recognized that technology in all its forms, is an important and integral element of the solution, but industry and wider society also view technology as being part of the problem. Increasingly, researchers are referencing the importance of responsible digitalization to eliminate the significant levels of e-waste. The reality is that technology is an integral component of the global efforts to get to net zero, however, its adoption requires pragmatic tradeoffs as we transition from current behaviors to a more climate friendly society.}
}
@article{ASCHENBRENNER20221455,
title = {FlexiCell: 5G location-based context-aware agile manufacturing},
journal = {Procedia CIRP},
volume = {107},
pages = {1455-1460},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.174},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004589},
author = {Doris Aschenbrenner and Marvin Scharle and Stephan Ludwig},
keywords = {5G, agile manufacturing, human-robot co-production, context awareness, location-based system},
abstract = {Manufacturing machines need to be re-tooled approximately 15 times per week and in the future even more often because of decreasing batch sizes and increasing short-cyclic demands. Collaborative robots promise to offer a versatile automation approach for priorly manual tasks in small and medium-sized enterprises. However, their configuration needs to change at least as often as the re-tooling rate because different parts are produced by the machines or might require different handling in general. Therefore, it would be great if robots and autonomous factory systems, in general, would automatically adjust to these changes in an intelligent way. In our approach, we propose a context-aware and location-based approach for agile manufacturing, in which the manufacturing plant parts, especially the collaborative robots, store i) their constellation, ii) their configuration, and iii) their adaptation strategy, and can react to re-tooling changes and even re-location changes adaptively. For example, moving one collaborative robot to a different location next to the production machine will automatically load its new configuration and consult with the operator on the adaptation strategy (i.e., the safety requirements). Such an approach requires precise location information. To realize the localization and the network capabilities, we propose to use localization based on heterogeniclocalization technologies like ultrasound and wireless communications. We suggest a wireless small-cell-based approach around a nomadic 5G core network, which integrates multiple wireless and wired communication technologies as well as localization support combined with an intelligent asset management strategy. Such nomadic cells can operate on an island without a heavy operator backend and optimize end-to-end communication. Furthermore, these small cells can federate with each other and thus extend their coverage when getting into each other's range.}
}
@article{GARCIA2022108463,
title = {Towards a connected Digital Twin Learning Ecosystem in manufacturing: Enablers and challenges},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108463},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108463},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004922},
author = {Álvaro García and Anibal Bregon and Miguel A. Martínez-Prieto},
keywords = {Digital twin, Learning ecosystem, Manufacturing, Human–machine collaboration, Learning factory, Cyber–physical system},
abstract = {The evolution of digital twin, leveraged by the progressive physical–digital convergence, has provided smart manufacturing systems with knowledge-generation ecosystems based on new models of collaboration between the workforce and industrial processes. Digital twin is expected to be a decision-making solution underpinned by real-time communication and data-driven enablers, entailing close cooperation between workers, systems and processes. But industry will need to face the challenges of building and supporting new technical and digital infrastructures, while workers’ skills development eventually manages to include the increased complexity of industrial processes. This paper is intended to reach a better understanding of learning opportunities offered by emerging Industry 4.0 digital twin ecosystems in manufacturing. Diverse learning approaches focused on the potential application of the digital twin concept in theoretical and real manufacturing ecosystems are reviewed. In addition, we propose an original definition of Digital Twin Learning Ecosystem and the conceptual layered architecture. Existing key enablers of the digital twin physical–digital convergence, such as collaborative frameworks, data-driven approaches and augmented interfaces, are also described. The role of the Learning Factory concept is highlighted, providing a common understanding between academia and industry. Academic applications and complex demonstration scenarios are combined in line with the enablement of connected adaptive systems and the empowerment of workforce skills and competences. The adoption of digital twin in production is still at an initial stage in the manufacturing industry, where specific human and technological challenges must be addressed. The research priorities presented in this work are considered as a recognised basis in industry, which should help digital twin with the objective of its progressive integration as a future learning ecosystem.}
}
@article{JIANG2022104601,
title = {Building demolition estimation in urban road widening projects using as-is BIM models},
journal = {Automation in Construction},
volume = {144},
pages = {104601},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104601},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200471X},
author = {Feng Jiang and Ling Ma and Tim Broyd and Ke Chen and Hanbin Luo and Muzi Du},
keywords = {As-is BIM, Building information modelling (BIM), Road widening, Road engineering, Alignment fitting, Building demolition, Cost estimation},
abstract = {Building demolition caused by urban road widening projects can lead to engineering, economic, and environmental issues and should be planned at the design stage. Based on as-is BIM, this paper proposes a method to estimate the building demolition caused by urban road widening using online map data and statistics on government websites. The as-is BIM models of the existing old road and its surrounding buildings are created, and the BIM models of the newly widened road are built based on the as-is BIM models considering road components in accordance with road engineering expressions to assist building demolition estimation using clash detection. This paper presents a cost-effective building demolition estimation in urban road widening projects without field surveys. It was tested on the M4 Motorway project in London. It has been proved to be a very practical approach to facilitate urban road planning and decision making.}
}
@article{LOUCA2022104243,
title = {Machine learning integration of multimodal data identifies key features of blood pressure regulation},
journal = {eBioMedicine},
volume = {84},
pages = {104243},
year = {2022},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2022.104243},
url = {https://www.sciencedirect.com/science/article/pii/S235239642200425X},
author = {Panayiotis Louca and Tran Quoc Bao Tran and Clea du Toit and Paraskevi Christofidou and Tim D. Spector and Massimo Mangino and Karsten Suhre and Sandosh Padmanabhan and Cristina Menni},
keywords = {Blood pressure, Machine learning, Genomics, Metabolomics, Diet},
abstract = {Summary
Background
Association studies have identified several biomarkers for blood pressure and hypertension, but a thorough understanding of their mutual dependencies is lacking. By integrating two different high-throughput datasets, biochemical and dietary data, we aim to understand the multifactorial contributors of blood pressure (BP).
Methods
We included 4,863 participants from TwinsUK with concurrent BP, metabolomics, genomics, biochemical measures, and dietary data. We used 5-fold cross-validation with the machine learning XGBoost algorithm to identify features of importance in context of one another in TwinsUK (80% training, 20% test). The features tested in TwinsUK were then probed using the same algorithm in an independent dataset of 2,807 individuals from the Qatari Biobank (QBB).
Findings
Our model explained 39·2% [4·5%, MAE:11·32 mmHg (95%CI, +/- 0·65)] of the variance in systolic BP (SBP) in TwinsUK. Of the top 50 features, the most influential non-demographic variables were dihomo-linolenate, cis-4-decenoyl carnitine, lactate, chloride, urate, and creatinine along with dietary intakes of total, trans and saturated fat. We also highlight the incremental value of each included dimension. Furthermore, we replicated our model in the QBB [SBP variance explained = 45·2% (13·39%)] cohort and 30 of the top 50 features overlapped between cohorts.
Interpretation
We show that an integrated analysis of omics, biochemical and dietary data improves our understanding of their in-between relationships and expands the range of potential biomarkers for blood pressure. Our results point to potentially key biological pathways to be prioritised for mechanistic studies.
Funding
Chronic Disease Research Foundation, Medical Research Council, Wellcome Trust, Qatar Foundation.}
}
@article{OCHELLA2022108332,
title = {An RUL-informed approach for life extension of high-value assets},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108332},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108332},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222003849},
author = {Sunday Ochella and Mahmood Shafiee and Chris Sansom},
keywords = {Remaining useful life (RUL), Life extension (LE), Prognostics and health management (PHM), Machine learning (ML), Reliability centered maintenance (RCM), Turbofan engines},
abstract = {The conventional approaches for life-extension (LE) of industrial assets are largely qualitative and focus only on a few indicators at the end of an asset’s design life. However, an asset may consist of numerous individual components with different useful lives and therefore applying a single LE strategy to every component will not result in an efficient outcome. In recent years, many advanced analytics techniques have been proposed to estimate the remaining useful life (RUL) of the assets equipped with sensor technology. This paper proposes a data-driven model for LE decision-making based on RUL values predicted on a real-time basis during the asset’s operational life. Our proposed LE model is conceptually targeted at the component, unit, or subsystem level; however, an asset-level decision is made by aggregating information across all components. Consequently, LE is viewed and assessed as a series of ongoing activities, albeit carefully orchestrated in a manner similar to operation and maintenance (O&M). The application of the model is demonstrated using the publicly available NASA C-MAPSS dataset for large commercial turbofan engines. This approach will be very beneficial to asset owners and maintenance engineers as it seamlessly weaves LE strategies into O&M activities, thus optimizing resources.}
}
@article{ZHOU2022114636,
title = {T-DNA integration and its effect on gene expression in dual Bt gene transgenic Populus ×euramericana cv. Neva},
journal = {Industrial Crops and Products},
volume = {178},
pages = {114636},
year = {2022},
issn = {0926-6690},
doi = {https://doi.org/10.1016/j.indcrop.2022.114636},
url = {https://www.sciencedirect.com/science/article/pii/S0926669022001194},
author = {Xinglu Zhou and Yachao Ren and Shijie Wang and Xinghao Chen and Chao Zhang and Minsheng Yang and Yan Dong},
keywords = { gene, Poplar 107, T-DNA preferentially inserts, RNA-seq, Unintended effect},
abstract = {The integration of Bacillus thuringiensis (Bt) genes is often accompanied by unintended effects along with the improved resistance to targeted pests. The insertion information of transfer DNA (T-DNA) and the expression information of upstream and downstream genes are of great significance for related research on unexpected effects and molecular-level mechanisms. In this study, six dual Bt transgenic Populus × euramericana cv. Neva (poplar 107) lines were used as research objects. We determined growth and physiological indices, and characterized the T-DNA integration using next-generation sequencing technology. The transgenic and non-transgenic lines showed no significant difference in growth index. However, while insect resistance was enhanced, effects related to the integration sites of the Bt gene occurred. A total of 15 insertion sites were detected among the six transgenic lines, and T-DNA preferentially inserted into AT-rich regions of the poplar genome. RNA sequencing and weighted gene co-expression network analysis were used to explore the effects of T-DNA insertion on the gene expression of poplar 107. As a result, the metabolic pathways most affected by the Bt gene were starch and sucrose metabolism and pentose and gluconate conversion. The expression of exogenous Bt had a greater impact than the insertion position or copy number on poplar 107 gene expression, and the Cry1Ac toxin protein also played a major role. This study provides theoretical support for the cultivation and management of poplar 107 new varieties, and provides reference for Poplar insect resistance breeding and its molecular response to Bt gene.}
}
@article{NUNAN2022451,
title = {Value creation in an algorithmic world: Towards an ethics of dynamic pricing},
journal = {Journal of Business Research},
volume = {150},
pages = {451-460},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322005689},
author = {Daniel Nunan and MariaLaura {Di Domenico}},
keywords = {Dynamic pricing, Pricing ethics, Algorithms, AI ethics},
abstract = {Choice of pricing strategy plays a central role in value creation and the effective functioning of markets. Shifts in technology and the growing availability of data are facilitating ever more innovative forms of pricing strategy. Within the emerging literature on pricing ethics, there is a gap in our understanding of the specific challenges of algorithmically generated dynamic pricing. Increasing pricing automation shifts the managerial focus from the selection of prices to the choice of algorithms. This paper expands the literature on pricing ethics by conceptualizing the ethical challenges raised by the contemporary use of dynamic pricing. We propose a governance model for algorithmically generated dynamic pricing, taking into account the role of the customer as a stakeholder in value generation.}
}
@article{JIANG2022100117,
title = {Static-shift suppression and anti-interference signal processing for CSAMT based on Guided Image Filtering},
journal = {Earthquake Research Advances},
volume = {2},
number = {1},
pages = {100117},
year = {2022},
issn = {2772-4670},
doi = {https://doi.org/10.1016/j.eqrea.2022.100117},
url = {https://www.sciencedirect.com/science/article/pii/S2772467022000057},
author = {Enhua Jiang and Rujun Chen and Debin Zhu and Weiqiang Liu and Regean Pitiya},
keywords = {CSAMT, Static shift, Guided image filtering, Anti-interference},
abstract = {Shallow conductive heterogeneity can lead to static shifts ain the apparent resistivity sounding curve of controlled-source audio-frequency magnetotellurics (CSAMT). The static effect will shift the apparent resistivity curves along with axial log-log coordinates. Such an effect, if not properly processed, can distort the resistivity of rock formation and the depth of interfaces, and even make the geological structures unrecognizable. In this paper, we discuss the reasons and characteristics of the static shift and summarize the previous studies regarding static shift correction. Then, we propose the Guided Image Filtering algorithm to suppress static shifts in CSAMT. In detail, we use the multi-window superposition method to superimpose 1D signals into a 2D matrix image, which is subsequently processed with Guided Image Filtering. In the synthetic model study and field examples, the Guided Image Filtering algorithm has effectively corrected and suppressed static shifts, and finally improved the precision of data interpretation.}
}
@article{CANEPARO2022104012,
title = {Semantic knowledge in generation of 3D layouts for decision-making},
journal = {Automation in Construction},
volume = {134},
pages = {104012},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104012},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004635},
author = {Luca Caneparo},
keywords = {Ontology (computer science), Semantic knowledge, Generative design, Layout generation, Layout planning, Site management, Multiobjective performance optimisation, Pareto set, AEC, Smart City},
abstract = {Generative computation has the potential to enhance the accuracy, effectiveness, and creativity of spatial layout in design and planning. The paper proposes a methodology to separate the knowledge about objects, spatial relationships, and constraints from the generative process. The separation between the knowledge in a domain and its possible practical uses is an important achievement of semantic technologies, because it grants access to a large body of knowledge, spanning various aspects and processes across buildings and cities, which is being codified into formal ontologies. The present study has reused existing knowledge from two established ontologies. An illustrative case-project demonstrates the suitability of the methodology for a complex layout planning problem, involving a large number of decision-makers, with multiple competing objectives and criteria. The system implements multidimensional visual interactive tools to assist designers, planners, and decision-makers in exploring the layouts and the criteria, to develop their confidence in what qualifies as a good and effective solution.}
}
@article{ZEFRI2022102652,
title = {Developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big UAV imagery data},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {106},
pages = {102652},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102652},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421003597},
author = {Yahya Zefri and Imane Sebari and Hicham Hajji and Ghassane Aniba},
keywords = {Digital photogrammetry, Unmanned Aerial Vehicle, Thermography, Photovoltaics, Big imagery data, Deep learning},
abstract = {The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.}
}
@article{CUI2022131208,
title = {A robust approach for the decomposition of high-energy-consuming industrial loads with deep learning},
journal = {Journal of Cleaner Production},
volume = {349},
pages = {131208},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.131208},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622008393},
author = {Jia Cui and Yonghui Jin and Renzhe Yu and Martin Onyeka Okoye and Yang Li and Junyou Yang and Shunjiang Wang},
keywords = {Smart grid, Load awareness, Non-intrusive load decomposition, Deep learning},
abstract = {The knowledge of the users’ electricity consumption pattern is an important coordinating mechanism between the utility company and the electricity consumers in terms of key decision makings. The load decomposition is therefore crucial to reveal the underlying relationship between the load consumption and its characteristics. However, load decomposition is conventionally performed on the residential and commercial loads, and adequate consideration has not been given to the high-energy-consuming industrial loads leading to inefficient results. This paper thus focuses on the load decomposition of the industrial park loads (IPL). The commonly used parameters in a conventional method are however inapplicable in high-energy-consuming industrial loads. Therefore, a more robust approach is developed comprising a three-algorithm model to achieve this goal on the IPL. First, the improved variational mode decomposition (IVMD) algorithm is introduced to denoise the training data of the IPL and improve its stability. Secondly, the convolutional neural network (CNN) and simple recurrent units (SRU) joint algorithms are used to achieve a non-intrusive and non-invasive decomposition process of the IPL using a double-layer deep learning network based on the IPL characteristics. Specifically, CNN is used to extract the IPL data characteristics while the improved long and short-term memory (LSTM) network, SRU, is adopted to develop the decomposition model and further train the load data. Through the robust decomposition process, the underlying relationship in the load consumption is extracted. The results obtained from the numerical examples show that this approach outperforms the state-of-the-art in the conventional decomposition process.}
}
@article{ZHANG2022105255,
title = {Heat wave tracker: A multi-method, multi-source heat wave measurement toolkit based on Google Earth Engine},
journal = {Environmental Modelling & Software},
volume = {147},
pages = {105255},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105255},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221002978},
author = {Mingxi Zhang and Xihua Yang and Jamie Cleverly and Alfredo Huete and Hong Zhang and Qiang Yu},
keywords = {Extreme heat wave, Google Earth Engine, Climate datasets, Risk analysis, GCM, Australia},
abstract = {Under ongoing global warming due to climate change, heat waves in Australia are expected to become more frequent and severe. Extreme heat waves have devastating impacts on both terrestrial and marine ecosystems. A multi-characteristic heat wave framework is used to estimate historical and future projected heat waves across Australia. A Google Earth Engine-based toolkit named heat wave tracker (HWT) is developed, which can be used for dynamic visualization, extraction, and processing of complex heat wave events. The toolkit exploits the public long-term high-resolution climate datasets to developed nine heat wave datasets across Australia for extreme heat wave value analysis. To examine climate change on heat waves and how they vary in time and space, we also explore the probability and return periods of extreme heat waves over a period of 100 years. The datasets, toolkit and findings we developed contribute to global studies on heat waves under accelerated global warming.}
}
@article{NGUYEN2022109636,
title = {Predicting the opening state of a group of windows in an open-plan office by using machine learning models},
journal = {Building and Environment},
pages = {109636},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109636},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322008666},
author = {Thi Hao Nguyen and Anda Ionescu and Evelyne Géhin and Olivier Ramalho},
keywords = {Indoor air quality, Windows opening state, Machine learning model, Time series, Autocorrelation functions, Open-plan office},
abstract = {Window operation is among one of the most influencing factors on the indoor air quality (IAQ). The opening state of the windows can modify the air exchange rate and as such the pollutant transfer between indoor and outdoor environments. In this paper, we focus on the modeling of the windows opening state in a real open-plan office with five windows. For this purpose, three machine learning-based models were implemented: (i) Decision Tree, (ii) k-Nearest Neighbors and (iii) Kernel Approximation. IAQ, climatic parameters and the opening state of the windows have been monitored during an entire period of 18 months. The information about: (i) the environmental factors from the previous 24th hour and (ii) the current time (month, day of the week, hour of the day) was used to predict the current state of the windows. The predictor importance estimation and the calculated autocorrelation functions showed that the three most relevant factors were: the previous 24th hour of the windows status, the current time and the previous 24th hour of the prevailing mean outdoor air temperature. The three models perform well with the testing sets according to the different evaluation indicators. The developed methods can be helpful for understanding occupant behavior and also for controlling indoor air pollutants levels in buildings, either as a standalone model or a part of a real-time IAQ monitoring system.}
}
@article{RAUH2022576,
title = {Towards AI Lifecycle Management in Manufacturing Using the Asset Administration Shell (AAS)},
journal = {Procedia CIRP},
volume = {107},
pages = {576-581},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003122},
author = {Lukas Rauh and Sascha Gärtner and David Brandt and Michael Oberle and Daniel Stock and Thomas Bauernhansl},
keywords = {AI Lifecycle, AI Asset, Industry 4.0, Digital Twin, Asset Administration Shell},
abstract = {Driven by the digital transformation, manufacturing companies face the challenge of managing, but more importantly, enabling rapid operationalization of AI to achieve the full advantage of the exponential data growth. Heterogeneous data structures, the continuously growing variety of implementation frameworks, and the lack of standards for the semantic description of AI solution components, the effort required to manage and share datasets and models between stakeholders impedes efficient and reproducible progression. This paper addresses the current challenges in industrial AI applications currently hindering their acceptance and widespread adoption in manufacturing. Based on an overview of the AI application lifecycle, we present our approach for AI asset meta-data management utilizing the technical concept of the Asset Administration Shell (AAS). Following the definition of the AAS as a reference implementation of the digital twin that provides a digital representation of physical assets and their properties, we propose an AAS for AI assets. The AI AAS maps relevant properties of an AI model together with properties of the corresponding dataset and learning algorithm in order to integrate the AI lifecycle in the Industry4.0 ecosphere.}
}
@article{YAO2022109084,
title = {Regularizing Autoencoders with Wavelet Transform for Sequence Anomaly Detection},
journal = {Pattern Recognition},
pages = {109084},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109084},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005647},
author = {Yueyue Yao and Jianghong Ma and Yunming Ye},
keywords = {Sequence anomaly detection, Regularized autoencoder, Frequency domain, Discrete wavelet transform, Statistical analysis, Sample-adaptive regularization weight},
abstract = {Nowadays, systems or entities are usually monitored by devices, generating large amounts of time series. Detecting anomalies in them help prevent potential loss, thus arousing much research interest. Existing studies have adopted autoencoders to detect anomalies, where reconstruction errors are used to indicate outliers. However, sometimes autoencoders may also reconstruct anomalies well due to the learned general features in latent spaces. To solve the above problem, we propose to regularize autoencoders to grasp specific features of normal sequences. Specifically, spectral unique patterns are captured by statistical analysis on discrete wavelet transform (DWT) coefficients of input sequences, restricting latent spaces to reflect unique patterns of normal sequences in both time and frequency domains. Furthermore, a Weight Controller calculating sample-adaptive regularization weights is designed to fully utilize the regularization effect. Extensive experiments on three public benchmarks demonstrate the effectiveness and superiority of the proposed model compared with state-of-the-art algorithms.}
}
@article{FENG2022112357,
title = {Data-driven personal thermal comfort prediction: A literature review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {161},
pages = {112357},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112357},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122002672},
author = {Yanxiao Feng and Shichao Liu and Julian Wang and Jing Yang and Ying-Ling Jao and Nan Wang},
keywords = {Personalization, Experimental design, Data collection, Data-driven method, Prediction accuracy, Online learning},
abstract = {Personal thermal comfort prediction modeling has become a trending topic in efforts to improve individual indoor comfort, a notion that is closely related to the design and performance of building systems, especially in sustainable and smart buildings. This research provides a comprehensive overview of data-driven approaches and processes for predicting personal thermal comfort in a building environment, as derived from a systematic review of 25 studies published in the last 10 years. After refining the concept of personal thermal comfort inspired by predictive modeling in personalized medicine and healthcare, the selection criteria were identified for the reviewed research. Then, three key elements affecting the data-driven modeling process were focused and reviewed, including experimental design, data collection, and modeling techniques. A special emphasis was placed on modeling techniques across the selected studies through a categorization process and comparison of their prediction accuracies. Feature selection and issues important for particular personal thermal comfort models were also reviewed and summarized. Upon reviewing these studies, the authors also considered inter- and intra-individual variability issues in sampling and modeling, data quantity and quality resulting from the collection procedure, model performance, feature importance, and implications for potential online learning techniques. Throughout these analyses, limitations of the current state-of-the-art and possible avenues for future study were addressed.}
}
@article{ZHANG2022109441,
title = {Automatic tracking for seismic horizons using convolution feature analysis and optimization algorithm},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109441},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109441},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521010846},
author = {Kai Zhang and Niantian Lin and Dong Zhang and Jianbin Zhang and Jiuqiang Yang and Gaopeng Tian},
keywords = {Seismic reflection horizons, Auto-tracking, Machine learning, Convolution feature analysis, Viterbi algorithm},
abstract = {Seismic horizon tracking is a fundamental aspect of seismic data interpretation. However, seismic horizons are typically obtained using manual tracking or a combination of manual tracking and traditional auto-tracking techniques, either of which is a time-consuming and error-prone process. To improve the efficiency and the accuracy of seismic horizon tracking, we developed a convolution feature analysis method on the basis of the traditional coherent technology combined with the Viterbi algorithm, and proposed a method for auto-tracking seismic horizons in complex exploration areas. Firstly, the local seismic waveform data of the target horizon passing through the drilling area have been extracted as the convolution kernel (i.e. the standard seismic trace). Then, the waveform data of each seismic trace in the whole region have been treated with the convolution to obtain the convolution feature in a sliding time window (i.e, the similarity seismic attribute profile). Finally, the convolution feature data have been taken as the inputs, and constraint optimization is performed for the automatic tracking of the seismic horizon. It has the ability to search for the maximum value by integrating the maximum similarity forward and searching for the shortest path method on synthetic and real seismic data. The obtained results show that the proposed method performs effectively for seismic horizons auto-tracking of low signal-to-noise ratio seismic data in complex exploration areas, and also traces the seismic horizons with good continuity and high accuracy.}
}
@article{SANTOSPEREIRA20224968,
title = {Top data mining tools for the healthcare industry},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {4968-4982},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S131915782100135X},
author = {Judith Santos-Pereira and Le Gruenwald and Jorge Bernardino},
keywords = {Data mining, Healthcare, Open-source data mining tools},
abstract = {The healthcare industry has become increasingly challenging, requiring retrieval of knowledge from large amounts of complex data to find the best treatments. Several works have suggested the use of Data Mining tools to overcome the challenges; however, none of them has suggested the best tool to do so. To fill this gap, this paper presents a survey of popular open-source data mining tools in which data mining tool selection criteria based on healthcare application requirements is proposed and the best ones using the proposed selection criteria are identified. The following popular open-source data mining tools are assessed: KNIME, R, RapidMiner, Scikit-learn, and Spark. The study shows that KNIME and RapidMiner provide the largest coverage of healthcare data mining requirements.}
}
@article{MUEHLBAUER2022364,
title = {Data driven logistics-oriented value stream mapping 4.0: A guideline for practitioners},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {16},
pages = {364-369},
year = {2022},
note = {18th IFAC Workshop on Control Applications of Optimization CAO 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.051},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322012277},
author = {K. Muehlbauer and M. Wuennenberg and S. Meissner and J. Fottner},
keywords = {Data Maturity, Data Science, Inconsistencies, Logistics in Manufacturing, Machine Learning, Optimization, Value Stream Management},
abstract = {The use of data-oriented approaches like data mining or machine learning has an increasing potential for application in the planning and control of production and logistics systems. The growing amount of digital process information helps to expand the existing process understanding in order to determine weaknesses in the process landscape. Due to the extensive complexity within production and logistics systems, a comprehensive approach is required to ensure a systematic analysis. This article presents an extension of the value stream method based on the existing approaches that is intended to support operators of logistics systems in the company. This methodology collects all relevant process information and validates the data maturity. Hence, indications for the use of data-oriented approaches can be given and potential machine learning-based analysis scenarios can be derived.}
}
@article{DANTRACCOLI2022101512,
title = {Maps of relative floristic ignorance and virtual floristic lists: An R package to incorporate uncertainty in mapping and analysing biodiversity data},
journal = {Ecological Informatics},
volume = {67},
pages = {101512},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101512},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121003034},
author = {Marco D'Antraccoli and Gianni Bedini and Lorenzo Peruzzi},
keywords = {Algorithms, Flora, GIS, Occurrence records, Spatial and temporal uncertainties},
abstract = {The vast amount of occurrence records currently available offers increasing opportunities for biodiversity data analyses. This amount of data poses new challenges for the reliability and correct interpretation of the results. Indeed, to safely deal with occurrence records, their uncertainty and associated biases should be taken into account. We developed an R package to explicitly include spatial and temporal uncertainties during the mapping and listing of plant occurrence records for a given study area. Our workflow returns two objects: (a) a ‘Map of Relative Floristic Ignorance’ (MRFI), which represents the spatial distribution of the lack of floristic knowledge; (b) a ‘Virtual Floristic List’ (VFL), i.e. a list of taxa potentially occurring in the area with an associated probability of occurrence. The method implemented in the package can manage a large amount of occurrence data and represents relative floristic ignorance across a study area with a sustainable computational effort. Several parameters can be set by the user, conferring high flexibility to the method. Uncertainty is not avoided, but incorporated into biodiversity analyses through appropriate methodological approaches and innovative spatial representations. Our study introduces a workflow that pushes forward the analytical capacities to deal with uncertainty in biological occurrence records, allowing to produce more accurate outputs.}
}
@article{LEE2022102590,
title = {When does AI pay off? AI-adoption intensity, complementary investments, and R&D strategy},
journal = {Technovation},
volume = {118},
pages = {102590},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102590},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222001377},
author = {Yong Suk Lee and Taekyun Kim and Sukwoong Choi and Wonjoon Kim},
keywords = {Artificial intelligence, High-tech ventures, Firm performance, Complementary investment, R&D strategy},
abstract = {This paper examines how high-tech venture performance varies with AI-adoption intensity. We find that firm revenue increases only after sufficient investment in AI, and the benefits of AI adoption are greater at firms that also invest in complementary technologies and pursue internal R&D strategy. Specifically, AI adoption at low levels does not suggest significant revenue growth, but, as the intensity of AI adoption increases revenue growth occurs. We find that such performance gains from adoption is larger among firms that invest in complementary technologies such as cloud computing and database systems. Moreover, the positive relationship between AI adoption intensity and revenue growth is stronger among firms that pursue a more exclusive R&D strategy specific to the venture.}
}
@article{GULMA2022101748,
title = {A new geodemographic classification of the influence of neighbourhood characteristics on crime: The case of Leeds, UK},
journal = {Computers, Environment and Urban Systems},
volume = {92},
pages = {101748},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101748},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001551},
author = {Usman Lawal Gulma},
keywords = {Community, Classification, Neighbourhood, Clustering, Social cohesion, Crime},
abstract = {Understanding spatial composition and characteristics of urban communities is an important factor determining the potential for social interactions between residents, greater chances for neighbourhood safety and policy planning. Until recently, neighbourhood classifications were traditionally census-based. This study presents a different approach in developing a community-based area classification for exploring the relationship between neighbourhood characteristics and crime through the integration of new data sources from social media with contextual variables from the traditional census. In this study, partition around medoids (PAM) clustering algorithm is used to partition 105 Leeds community areas into four distinct groups based on their crime rates. Silhouette index and adjusted rand index (ARI) were used to evaluate the validity of clusters internally and externally. The new typology developed, has revealed new insights demonstrating the relationship between social cohesion and crime rates in Leeds community areas. The new partitioned clusters also provide area-based specific information, on criminal activities that could help in policy planning for community building and better resource allocation towards reducing crime rates.}
}
@article{MIN2022100484,
title = {Applications of knowledge graphs for food science and industry},
journal = {Patterns},
volume = {3},
number = {5},
pages = {100484},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100484},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922000691},
author = {Weiqing Min and Chunlin Liu and Leyi Xu and Shuqiang Jiang},
keywords = {knowledge graph, artificial intelligence, ontology, food science and industry, nutrition and health, new recipe development, food analysis},
abstract = {Summary
The deployment of various networks (e.g., Internet of Things [IoT] and mobile networks), databases (e.g., nutrition tables and food compositional databases), and social media (e.g., Instagram and Twitter) generates huge amounts of food data, which present researchers with an unprecedented opportunity to study various problems and applications in food science and industry via data-driven computational methods. However, these multi-source heterogeneous food data appear as information silos, leading to difficulty in fully exploiting these food data. The knowledge graph provides a unified and standardized conceptual terminology in a structured form, and thus can effectively organize these food data to benefit various applications. In this review, we provide a brief introduction to knowledge graphs and the evolution of food knowledge organization mainly from food ontology to food knowledge graphs. We then summarize seven representative applications of food knowledge graphs, such as new recipe development, diet-disease correlation discovery, and personalized dietary recommendation. We also discuss future directions in this field, such as multimodal food knowledge graph construction and food knowledge graphs for human health.}
}
@article{BLANKENBERG2022314,
title = {Using a graph database for the ontology-based information integration of business objects from heterogenous Business Information Systems},
journal = {Procedia Computer Science},
volume = {196},
pages = {314-323},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022420},
author = {Carolin Blankenberg and Berit Gebel-Sauer and Petra Schubert},
keywords = {IS Integration, Enterprise Knowledge Graph, Ontology, Graph Database},
abstract = {This paper reports on findings from a project on information integration from multiple Business Information Systems with the help of a user-specific Enterprise Knowledge Graph. Most ERP systems currently in use store information objects in relational databases. Research in Web Sciences has shown that graph structures present information in a more intuitive way that is easier to interpret for humans. Following a DSR approach, we developed a concept for storing an ontology in a graph database that allows us to map ERP objects and load them at runtime. This allows the end user to navigate through the graph structure, thus providing an intuitive and quick access to essential job-related information. We evaluated the suggested concept with a prototype following the paradigm of polyglot persistence; the prototype was equipped with a graph database to store the company-specific ontology in its native form. The program code was encapsulated into a separate module following a service-oriented software design.}
}
@article{WANG202224,
title = {DeepCS: Training a deep learning model for cervical spondylosis recognition on small-labeled sensor data},
journal = {Neurocomputing},
volume = {472},
pages = {24-34},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016829},
author = {Nana Wang and Chunjie Luo and Xi Huang and Yunyou Huang and Jianfeng Zhan},
keywords = {Cervical spondylosis recognition, High-dimensional time series sensor data, Convolutional neural network, Network architecture search, Feature extraction},
abstract = {Cervical spondylosis (CS) recognition systems provide regular screening services outside of a hospital and promote early detection and treatment of CS. However, in this paper, we propose a deep learning-based CS recognition system. Concerning the state-of-the-art and state-of-the-practice systems, the innovations of our approaches and algorithms are as follows: First, to elevate the reliance upon the sample number required for training the high-quality model, we reduce sample dimension and find optimal neural network architectures to reduce the number of model parameters to fit. Second, we incorporate multi-stream parallel network architecture search with multi-view feature extraction by converting time series classification into an image classification task. Specifically, five feature extraction methods (time-domain, frequency-domain, time–frequency domain, model-based, nonlinear feature extraction) are firstly utilized to extract features from multiple perspectives and form low-dimensional data set with multi-properties. Third, we reorganize low-dimensional data into image one representing the spatio-temporal relationship of muscle activity pattern. Finally, a multi-stream parallel network architecture search is proposed to use a bypass mechanism for optimal neural network architecture, each of which processes a kind of features mentioned above with an idea of the sparse connection of convolution neural network. The results on the real-world data set show that our CS recognition system achieves the average accuracy of 95.54%, average sensitivity of 99.09%, and average specificity of 90.00%, outperforming the state-of-the-art ones.}
}
@article{SOUIDEN2022100463,
title = {A survey of outlier detection in high dimensional data streams},
journal = {Computer Science Review},
volume = {44},
pages = {100463},
year = {2022},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100463},
url = {https://www.sciencedirect.com/science/article/pii/S1574013722000107},
author = {Imen Souiden and Mohamed Nazih Omri and Zaki Brahmi},
keywords = {Outlier detection, High dimensional data, Data streams},
abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.}
}
@article{ERHART2022106686,
title = {Application of North European characterisation factors, population density and distance-to-coast grid data for refreshing the Swedish human toxicity and ecotoxicity footprint analysis},
journal = {Environmental Impact Assessment Review},
volume = {92},
pages = {106686},
year = {2022},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2021.106686},
url = {https://www.sciencedirect.com/science/article/pii/S0195925521001360},
author = {Szilárd Erhart and Kornél Erhart},
keywords = {Chemical footprint, Hazardous chemicals, -PRTR, Human toxicity, Ecotoxicity, USEtox, ESG rating, Sustainability rating, European Green Deal, Sustainable Devoelopment Goals (SDGs)},
abstract = {Here, we develop further the national chemical footprint assessment methods using Sweden as an example to enhance the precision of calculations. First, we integrate grid data on population density and distance-to-seacoast into the analytical framework to better match the European Pollutant Release and Transfer Register on the sub-compartment level with USEtox toxicity characterisation factors. Second, we use the latest USEtox 2.12 model version and its more punctual North European characterisation factors. Third, we conduct trend and geographic analysis and rank Swedish facilities in terms of toxicity potential. We show that total human toxicity potential in Sweden was smaller than previously estimated when using the North European USEtox landscape settings and sloped downwards over time. We confirm toxicity potential of major pollutants in previous research papers (Zn, Hg, Pb, Ni) and find that Hg’s relative human toxicity potential in a longer period can be larger than previously estimated on shorter periods. Human toxicity is estimated to be mostly non-cancer type in Sweden. Results are largely invariant to the choice of air sub-compartments. Companies in the metals manufacturing sector are estimated to have the largest human toxicity potential in Sweden in the period between 2001 and 2017 and companies in the paper manufacturing industry have the largest ecotoxicity potential.}
}
@article{LIOTTA2022103832,
title = {Testing the monocentric standard urban model in a global sample of cities},
journal = {Regional Science and Urban Economics},
volume = {97},
pages = {103832},
year = {2022},
issn = {0166-0462},
doi = {https://doi.org/10.1016/j.regsciurbeco.2022.103832},
url = {https://www.sciencedirect.com/science/article/pii/S0166046222000710},
author = {Charlotte Liotta and Vincent Viguié and Quentin Lepetit},
keywords = {Urbanization, Standard urban model, Urban spatial structure, Between-country comparisons},
abstract = {Using a unique dataset containing gridded data on population densities, rents, housing sizes, and transportation in 192 cities worldwide, we investigate the empirical relevance of the monocentric standard urban model (SUM). Overall, the SUM seems surprisingly capable of capturing the inner structure of cities, both in developed and developing countries. As expected, cities spread out when they are richer, more populated, and when transportation or farmland is cheaper. Respectively 100% and 87% of the cities exhibit the expected negative density and rent gradients: on average, a 1% decrease in income net of transportation costs leads to a 21% decrease in densities and a 3% decrease in rents per m2. We also investigate the heterogeneity between cities of different characteristics in terms of monocentricity, informality, and amenities.}
}
@article{BRIDGEWOOD20222660,
title = {T Helper 2 IL-4/IL-13 Dual Blockade with Dupilumab Is Linked to Some Emergent T Helper 17‒Type Diseases, Including Seronegative Arthritis and Enthesitis/Enthesopathy, but Not to Humoral Autoimmune Diseases},
journal = {Journal of Investigative Dermatology},
volume = {142},
number = {10},
pages = {2660-2667},
year = {2022},
issn = {0022-202X},
doi = {https://doi.org/10.1016/j.jid.2022.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0022202X22002561},
author = {Charlie Bridgewood and Miriam Wittmann and Tom Macleod and Abdulla Watad and Darren Newton and Kanchan Bhan and Howard Amital and Giovanni Damiani and Sami Giryes and Nicola Luigi Bragazzi and Dennis McGonagle},
abstract = {Dupilumab, an IL-4/IL-13 receptor blocker, has been linked to emergent seronegative inflammatory arthritis and psoriasis that form part of the spondyloarthropathy spectrum. We systematically investigated patterns of immune disorders, including predominantly T helper 17‒(spondyloarthropathy pattern) and T helper 2‒mediated disorders and humoral autoimmune pattern diseases, using VigiBase, the World Health Organization’s global pharmacovigilance of adverse drug reactions. Several bioinformatics databases and repositories were mined to couple dupilumab-related immunopharmacovigilance with molecular cascades relevant to reported findings. A total of 37,848 dupilumab adverse drug reaction cases were reported, with skin, eye, and musculoskeletal systems most affected. Seronegative arthritis (OR = 9.61), psoriasis (OR = 1.48), enthesitis/enthesopathy (OR = 12.65), and iridocyclitis (OR = 3.77) were highly associated. However, ankylosing spondylitis and inflammatory bowel disease were not conclusively associated. Overall, classic polygenic humoral‒mediated autoimmune diseases such as rheumatoid arthritis and systemic lupus erythematosus were not associated with dupilumab use. Pathway analysis identified several biological pathways potentially involved in dupilumab‒associated adverse drug reactions, including the fibroblast GF receptor (in particular, FGFR2) pathway. MicroRNAs analysis revealed the potential involvement of hsa-miR-21-5p and hsa-miR-335-5p. In conclusion, IL-4/IL-13 blockers are not unexpectedly protective against humoral autoimmune diseases but dynamically skew immune responses toward some IL-23/IL-17 cytokine pathway‒related diseases. IL-4/13 axis also plays a role in homeostatic tissue repair and we noted evidence for a link with ocular and arterial pathology.}
}
@article{SIALA2022114782,
title = {SHIFTing artificial intelligence to be responsible in healthcare: A systematic review},
journal = {Social Science & Medicine},
volume = {296},
pages = {114782},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2022.114782},
url = {https://www.sciencedirect.com/science/article/pii/S0277953622000855},
author = {Haytham Siala and Yichuan Wang},
keywords = {Systematic literature review, Responsible artificial intelligence (AI), Health-medicine, AI ethics, Digital health, Virtue ethics},
abstract = {A variety of ethical concerns about artificial intelligence (AI) implementation in healthcare have emerged as AI becomes increasingly applicable and technologically advanced. The last decade has witnessed significant endeavors in striking a balance between ethical considerations and health transformation led by AI. Despite a growing interest in AI ethics, implementing AI-related technologies and initiatives responsibly in healthcare settings remains a challenge. In response to this topical challenge, we reviewed 253 articles pertaining to AI ethics in healthcare published between 2000 and 2020, summarizing the coherent themes of responsible AI initiatives. A preferred reporting items for systematic review and meta-analysis (PRISMA) approach was employed to screen and select articles, and a hermeneutic approach was adopted to conduct systematic literature review. By synthesizing relevant knowledge from AI governance and ethics, we propose a responsible AI initiative framework that encompasses five core themes for AI solution developers, healthcare professionals, and policy makers. These themes are summarized in the acronym SHIFT: Sustainability, Human centeredness, Inclusiveness, Fairness, and Transparency. In addition, we unravel the key issues and challenges concerning responsible AI use in healthcare, and outline avenues for future research.}
}
@article{BARBIERI2022100396,
title = {Decentralized federated learning for extended sensing in 6G connected vehicles},
journal = {Vehicular Communications},
volume = {33},
pages = {100396},
year = {2022},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2021.100396},
url = {https://www.sciencedirect.com/science/article/pii/S2214209621000656},
author = {Luca Barbieri and Stefano Savazzi and Mattia Brambilla and Monica Nicoli},
keywords = {Cooperative sensing, Connected automated driving, Distributed computing, Federated learning, 6G V2X, Consensus},
abstract = {Research on smart connected vehicles has recently targeted the integration of vehicle-to-everything (V2X) networks with Machine Learning (ML) tools and distributed decision making. Among these convergent paradigms, Federated Learning (FL) allows the vehicles to train a deep ML model collaboratively, by exchanging model parameters (i.e., neural network weights and biases), rather than raw sensor data, via V2X links. Early FL approaches resorted to a server-client architecture, where a Parameter Server (PS) acts as edge device to orchestrate the learning process. Novel FL tools, on the other hand, target fog architectures where the model parameters are mutually shared by vehicles and synchronized in a distributed manner via consensus algorithms. These tools do not rely on the PS, but take advantage of low-latency V2X links. In line with this recent research direction, in this paper we investigate distributed FL methods for augmenting the capability of road user/object classification based on Lidar data. More specifically, we propose a new modular, decentralized approach to FL, referred to as consensus-driven FL (C-FL), suitable for PointNet compliant deep ML architectures and Lidar point cloud processing for road actor classification. The C-FL process is evaluated by simulating a realistic V2X network, based on the Collective Perception Service (CPS), for mutual sharing of the PointNet model parameters. The performance validation considers the impact of the degree of connectivity of the vehicular network, the benefits of continual learning over heterogeneous training data, convergence time and loss/accuracy tradeoffs. Experimental results indicate that C-FL complies with the extended sensors use cases for high levels of driving automation, it provides a low-latency training service, compared with existing distributed ML approaches, and it outperforms ego learning with minimal bandwidth usage.}
}
@article{MADDIKUNTA2022103464,
title = {Incentive techniques for the Internet of Things: A survey},
journal = {Journal of Network and Computer Applications},
volume = {206},
pages = {103464},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103464},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001138},
author = {Praveen Kumar Reddy Maddikunta and Quoc-Viet Pham and Dinh C. Nguyen and Thien Huynh-The and Ons Aouedi and Gokul Yenduri and Sweta Bhattacharya and Thippa Reddy Gadekallu},
keywords = {Incentives, Internet of Things, Game theory, Blockchain, Artificial intelligence, Smart applications},
abstract = {The Internet of Things (IoT) has remarkably evolved over the last few years to realize a wide range of newly emerging services and applications empowered by the unprecedented proliferation of smart devices. The quality of IoT networks heavily relies on the involvement of devices for undertaking functions from data sensing, computation to communication and IoT intelligence. Stimulating IoT devices to actively participate and contribute to the network is a practical challenge, where incentive techniques such as blockchain, game theory, and Artificial Intelligence (AI) are highly desirable to build a sustainable IoT ecosystem. In this article, we present a systematic literature review of the incentive techniques for IoT, aiming to provide general readers with an overview of incentive-enabled IoT from background, motivations, and enabling techniques. Particularly, we first present the fundamentals of IoT data network infrastructure, and several key incentive techniques for IoT are described in details, including blockchain, game theory, and AI. We next provide an extensive review on the use of these incentive techniques in a number of key IoT services, such as IoT data sharing, IoT data offloading and caching, IoT mobile crowdsensing, and IoT security and privacy. Subsequently, we explore the potential of incentives in important IoT applications, ranging from smart healthcare, smart transportation to smart city and smart industry. The research challenges of incentive techniques in IoT networks are highlighted, and the potential directions are also pointed out for future research of this important area.}
}
@article{CIFUENTESGONZALEZ2022349,
title = {Colombian Ocular Infectious Epidemiology Study (COIES): Ocular Toxoplasmosis Incidence and Sociodemographic Characterization, 2015-2019},
journal = {International Journal of Infectious Diseases},
volume = {117},
pages = {349-355},
year = {2022},
issn = {1201-9712},
doi = {https://doi.org/10.1016/j.ijid.2022.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S1201971222001035},
author = {Carlos Cifuentes-González and Estefanía Zapata-Bravo and María Camila Sierra-Cote and Laura Boada-Robayo and Ángela Paola Vargas-Largo and Juliana Reyes-Guanes and Alejandra de-la-Torre},
keywords = {Incidence Study, Ocular Toxoplasmosis, Ophthalmology, Toxoplasmosis},
abstract = {Objectives
This study aims to describe the incidence of ocular involvement in patients with toxoplasmosis and describe the sociodemographic characteristics by age, sex, and region in Colombia, based on the National Health Registry of data between January 1, 2015, and December 31, 2019.
Methods
We conducted a cross-sectional study using the Integrated Social Protection Information System database from the Colombian Ministry of Health, the unique official database in the country. We used the International Classification of Diseases for all codes of toxoplasmosis with a specific filter for ocular toxoplasmosis (OT) from 2015 to 2019 to estimate the incidence and the demographic status of the disease in Colombia.
Results
During the 5 years of study, the crude unadjusted incidence of OT was 42.02 (Confidence Interval 30.29-56.19) cases in 1,000 patients with toxoplasmosis per year, showing a significant increase of incidence when comparing the year 2019 to the year 2015. There was a predominance of female patients (58% of the cases). The distribution by age shows an increase in cases of the disease in subjects aged 15 to 49 years (65.2%). The geographic analysis showed a higher proportion of cases in the Andean region, followed by the Pacific and the Atlantic regions.
Conclusion
This is the first study that determines the epidemiological characteristics of OT based on a National Health database in Colombia, showing a public health problem and evidencing the neediness of solidifying preventive and screening strategies in the Colombian population.}
}
@article{TAN2022100097,
title = {A systematic review of artificial intelligence techniques for collaborative learning over the past two decades},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100097},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100097},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000522},
author = {Seng Chee Tan and Alwyn Vwen Yen Lee and Min Lee},
keywords = {Artificial intelligence, Collaborative learning, Systematic review, AI in Education},
abstract = {This systematic review focuses on publications related to studies of the use of artificial intelligence (AI) for collaborative learning. The use of AI for collaborative learning is a recent phenomenon and a systematic review of such studies is lacking. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol, 41 journal articles were shortlisted. These articles were analysed in terms of the contexts of study (year, group size, platforms of interaction, and types of learners). Using a thematic approach, two broad foci and five sub-categories of use of AI for collaborative learning were identified: (1) learning outcomes – (a) collective performances, and (b) content of learning; (2) social interactions and processes – (c) sentiments and emotions, (d) discourse patterns and talk moves, and (e) learner characteristic and behaviours. The AI techniques were coded according to three broad purposes (discovering, learning and reasoning) and nine techniques: clustering, ensemble, regression algorithms, deep learning, decision trees, natural language processing, instance-based, fuzzy logic, and agents. A Sankey diagram was used to depict the relationships among the nine AI techniques to the five sub-categories of how AI was used to support collaborative learning. The gaps in the selected articles and limitations of the current review were discussed to suggest future areas of study.}
}
@article{SCHEIBEIN2022103505,
title = {Assessing open science and citizen science in addictions and substance use research: A scoping review},
journal = {International Journal of Drug Policy},
volume = {100},
pages = {103505},
year = {2022},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2021.103505},
url = {https://www.sciencedirect.com/science/article/pii/S0955395921004187},
author = {Florian Scheibein and William Donnelly and John SG Wells},
keywords = {Open science, Open data, Open source, Open peer review, Citizen science},
abstract = {Background
The EU promotes ‘Open Science’ as a public good. Complementary to its implementation is Citizen Science, which redefines the relationship between the scientific community, civic society and the individual. Open Science and Citizen Science poses challenges for the substance use and addictions research community but may provide positive opportunities for future European addiction research. This paper explores both current barriers and potential facilitators for the implementation of Open Science and Citizen Science in substance use and addictions research.
Methodology
A scoping review was used to examine barriers and facilitators identified in the substance use and addiction research literature for the adoption of Open Science and Citizen Science.
Results
‘Technical’ facilitators included the pre-registration of study protocols; publication of open-source datasets; open peer review and online tools. ‘Motivational’ facilitators included enhanced reputation; embracing co-creation; engaged citizenship and gamification. ‘Economic’ facilitators included the use of free tools and balanced remuneration of crowdworkers. ‘Political’ facilitators included better informed debates through the ‘triple helix’ approach and trust-generating transparency. ‘Legal’ facilitators included epidemiologically informed law enforcement; better policy surveillance and the validation of other datasets. ‘Ethical’ facilitators included the ‘democratisation of science’ and opportunities to explore new concepts of ethics in addiction research.
Conclusion
Open Science and Citizen Science in substance use and addictions research may provide a range of benefits in relation to the democratisation of science; transparency; efficiency and the reliability/validity of data. However, its implementation raises a range of research integrity and ethical issues that need be considered. These include issues related to participant recruitment; privacy; confidentiality; security; cost and industry involvement. Progressive journal policies to support Open Science practices; a shift in researcher norms; the use of free tools and the greater availability of methodological and ethical standards are likely to increase adoption in the field.}
}
@article{MORTEZAEI2022101054,
title = {Computational methods for analyzing RNA-sequencing contaminated samples and its impact on cancer genome studies},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101054},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101054},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001939},
author = {Zahra Mortezaei},
keywords = {Next-generation sequencing (NGS), RNA-Seq, Contaminations, Computational methods, Cancer},
abstract = {A tumor is a group of cells with abnormal cell growth. Sequencing technology can help recognize genetic mutations that cause cancer. Next-generation sequencing (NGS) can supply genetic data and determine the number of mutant genes in different cancers by sequencing the whole genome, exome, and/or transcriptome. For a specific organism, ribonucleic acid (RNA) content is represented by the transcriptome containing information about different diseases, functional genome elements, and molecular components of tissues and cells. Whole transcriptome shotgun sequencing, known as RNA-Seq, is a technology that uses NGS to show a snapshot of RNA at a given time from millions of individual RNAs. There are some biases in RNA-Seq, which can be classified as nucleotide composition, guanine-cytosine (GC) content, insert size, cell type, and contaminations. In molecular biology, contaminations can lead to biases in the genetic analysis results and difficulties in observing viral infections. Here we reviewed RNA-Seq methodologies, different contaminations that may occur during RNA-Seq preparation, and some methods that can be applied to estimate the transcript abundances of contaminated samples.}
}
@article{TOP2022106909,
title = {Cultivating FAIR principles for agri-food data},
journal = {Computers and Electronics in Agriculture},
volume = {196},
pages = {106909},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106909},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922002265},
author = {Jan Top and Sander Janssen and Hendrik Boogaard and Rob Knapen and Görkem Şimşek-Şenel},
keywords = {Agriculture, Food supply chain, Data sharing, FAIR principles, Ontology, Controlled vocabulary},
abstract = {Data generated by the global food system is crucial in the transformation towards sustainable, resilient, and high-quality food production. Although the amount of potentially useful data is growing rapidly, its (re)use is still limited. The FAIR-principles have been developed for making data findable, accessible, interoperable, and reusable both by humans and machines. This paper explores the further operationalization of the FAIR principles in agriculture and food. Experience shows that several conditions must be fulfilled before data can be effectively shared and reused. First, automated tools must be available for data providers and users. Secondly, we need a community-based approach in developing tools and vocabularies. Thirdly, data cannot be shared by an open-by-default policy only. Finally, scientific insight is needed in how data is actually (re)used in scientific communities. We conclude that bringing the FAIR-principles to full maturity requires a fair balance of efforts within the agri-food communities, supported by a proper infrastructure.}
}
@article{HIMEUR2022129786,
title = {Techno-economic assessment of building energy efficiency systems using behavioral change: A case study of an edge-based micro-moments solution},
journal = {Journal of Cleaner Production},
volume = {331},
pages = {129786},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129786},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621039615},
author = {Yassine Himeur and Abdullah Alsalemi and Faycal Bensaali and Abbes Amira and Iraklis Varlamis and George Bravos and Christos Sardianos and George Dimitrakopoulos},
keywords = {Energy efficiency, Behavioral change, Anomaly detection, Recommendation generation, Business model, Market drivers/barriers},
abstract = {Energy efficiency based on behavioral change has attracted increasing interest in recent years, although, solutions in this area lack much needed techno-economic analysis. That is due to the absence of both prospective studies and consumer awareness. To close such gap, this paper proposes the first techno-economic assessment of a behavioral change-based building energy efficiency solution, to the best of the authors’ knowledge. From the one hand, the technical assessment is conducted through (i) introducing a novel edge-based energy efficiency solution; (ii) analyzing energy data using machine learning tools and micro-moments, and producing intelligent, personalized, and explainable action recommendations; and (iii) proceeding with a technical evaluation of four application scenarios, i.e., data collection, data analysis and anomaly detection, recommendation generation, and data visualization. On the other hand, economic assessment is performed by examining the marketability potential of the proposed solution via a market and research analysis of behavioral change-based systems for energy efficiency applications. Also, various factors impacting the commercialization of the final product are investigated before providing recommended actions to ensure its potential marketability via conducting a Go/No-Go evaluation. In conclusion, the proposed solution is designed at a low cost and can save up to 28%–68% of the consumed energy, which results in a Go decision to commercialize the technology.}
}
@article{AHMAD2022287,
title = {Advances in Machine Learning Approaches to Heart Failure with Preserved Ejection Fraction},
journal = {Heart Failure Clinics},
volume = {18},
number = {2},
pages = {287-300},
year = {2022},
note = {Digital Health},
issn = {1551-7136},
doi = {https://doi.org/10.1016/j.hfc.2021.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1551713621001173},
author = {Faraz S. Ahmad and Yuan Luo and Ramsey M. Wehbe and James D. Thomas and Sanjiv J. Shah},
keywords = {Heart failure, Artificial intelligence, Machine learning, Deep learning, Natural language processing}
}
@article{SHON2022104222,
title = {Autonomous condition monitoring-based pavement management system},
journal = {Automation in Construction},
volume = {138},
pages = {104222},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104222},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522000954},
author = {Heeseung Shon and Chung-Suk Cho and Young-Ji Byon and Jinwoo Lee},
keywords = {Autonomous condition monitoring, Pavement management system, Connected autonomous vehicles, Social cost, Real-time data collection, Prediction, Inspection, Condition-based policies},
abstract = {Due to high operation cost of dedicated inspection vehicles, conventional pavement management systems (PMS) suffer from limited data quantity collected from periodic inspections. However, increasing market penetration of connected autonomous vehicles (CAVs) offers opportunities to monitor pavement conditions more frequently through sensors, including vision cameras and accelerometers, originally installed for autonomous driving. In this paper, we proposed an autonomous condition monitoring-based pavement management system (ACM-PMS) with real-time data collection using CAVs traveling voluntarily. We presented a novel mathematical framework to evaluate potential benefits of ACM-PMS in reducing social costs for both users and agency, systematically accounting for its unique three advantages: (i) large amount of condition data increases prediction model accuracy; (ii) aggregated measurement of current facility condition improves inspection accuracy; (iii) agency can perform maintenance activities at optimal timings, achieving continuous-time and condition-based policies. Results of numerical examples confirm that ACM-PMS significantly reduces the social cost of conventional PMS.}
}
@article{KOUNTA2022200112,
title = {Multimodal deep learning for predicting the choice of cut parameters in the milling process},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200112},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200112},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322000503},
author = {Cheick Abdoul Kadir A Kounta and Bernard Kamsu-Foguem and Farid Noureddine and Fana Tangara},
keywords = {Deep learning, Multimodal data processing, Heterogeneous data fusion, Unstructured data, Manufacturing processes, Roughness},
abstract = {In this paper, we use multimodal deep learning to predict the choice of optimal cutting parameters (cutting speed, depth of cut, and feed rate per tooth) and the appropriate cutting tool for reproducing an existent piece of the same surface state, considering the footprints left by the cutting tool. We use the image of the aluminum plate's surface states considering the tool's footprints, the cutting parameters, and the roughness average (Ra) obtained with a roughness meter to drive our model. We built a late multimodal fusion model with two networks, a convolutional neural network (CNN) and a recurrent neural network with long short-term memory layers (LSTM). The first network consists of the first branch with a convolutional network that receives the input images. In the second network, modeling is performed by the LSTM network to receive the digital input data. This provides a framework to integrate information from two modalities to ensure surface quality in machining processes. This approach aims to assist in selecting the appropriate cutting tool and cutting parameters to automatically reproduce a machined piece using the image and roughness of an already existing piece. It is observed that the performance of the multimodal model is better than that of the unimodal model on image data. The accuracy continues to improve on both sets (training and validation), and the multimodal model finally reaches good accuracy results. Contrary to the unimodal model, which fails to generalize the training on a validation dataset. The results estimated by the multimodal fusion model are encouraging when applied to the milling activity in industrial production processes.}
}
@article{LIN2022149,
title = {Data fusion and transfer learning empowered granular trust evaluation for Internet of Things},
journal = {Information Fusion},
volume = {78},
pages = {149-157},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001780},
author = {Hui Lin and Sahil Garg and Jia Hu and Xiaoding Wang and Md. Jalil Piran and M. Shamim Hossain},
keywords = {Data fusion, Trust evaluation, Transfer learning, Deep reinforcement learning, Privacy preservation, Internet of Things},
abstract = {In the Internet of Things (IoT), a huge amount of valuable data is generated by various IoT applications. As the IoT technologies become more complex, the attack methods are more diversified and can cause serious damages. Thus, establishing a secure IoT network based on user trust evaluation to defend against security threats and ensure the reliability of data source of collected data have become urgent issues, in this paper, a Data Fusion and transfer learning empowered granular Trust Evaluation mechanism (DFTE) is proposed to address the above challenges. Specifically, to meet the granularity demands of trust evaluation, time–space empowered fine/coarse grained trust evaluation models are built utilizing deep transfer learning algorithms based on data fusion. Moreover, to prevent privacy leakage and task sabotage, a dynamic reward and punishment mechanism is developed to encourage honest users by dynamically adjusting the scale of reward or punishment and accurately evaluating users’ trusts. The extensive experiments show that: (i) the proposed DFTE achieves high accuracy of trust evaluation under different granular demands through efficient data fusion; (ii) DFTE performs excellently in participation rate and data reliability.}
}
@article{HASANI202213,
title = {Artificial Intelligence in Medical Imaging and its Impact on the Rare Disease Community: Threats, Challenges and Opportunities},
journal = {PET Clinics},
volume = {17},
number = {1},
pages = {13-29},
year = {2022},
note = {Artificial Intelligence and PET Imaging, Part II},
issn = {1556-8598},
doi = {https://doi.org/10.1016/j.cpet.2021.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S155685982100078X},
author = {Navid Hasani and Faraz Farhadi and Michael A. Morris and Moozhan Nikpanah and Arman Rahmim and Yanji Xu and Anne Pariser and Michael T. Collins and Ronald M. Summers and Elizabeth Jones and Eliot Siegel and Babak Saboury},
keywords = {Artificial intelligence, Rare diseases, Positron emission tomography, Medical imaging}
}
@article{PANG2022105400,
title = {Geothermal regime and implications for basin resource exploration in the Qaidam Basin, northern Tibetan Plateau},
journal = {Journal of Asian Earth Sciences},
volume = {239},
pages = {105400},
year = {2022},
issn = {1367-9120},
doi = {https://doi.org/10.1016/j.jseaes.2022.105400},
url = {https://www.sciencedirect.com/science/article/pii/S1367912022003315},
author = {Yumao Pang and Kaizhen Zou and Xingwei Guo and Yan Chen and Jian Zhao and Fei Zhou and Jun Zhu and Lifeng Duan and Guoxin Yang},
keywords = {Qaidam Basin, Geothermal regime, Heat flow, Thermal structure, Petroleum distribution, Geothermal resources},
abstract = {The Qaidam Basin was formed under the background of the continuous uplift of the Tibetan Plateau, and its tectonic location imparts unique geothermal regime. The geothermal regime of the Qaidam Basin was studied based on oil-test static temperatures from 60 wells, 195 thermal conductivities measured by the optical scanning method, and 142 radiogenic heat production values. The geothermal gradient in the Qaidam Basin is 17.1–47.6 °C/km, with an average of 31.3 °C/km, and the thermal conductivity is 0.523–4.379 W/m‧K. The heat flow is 28.3–83.1 mW/m2, with an average of 59.6 mW/m2, and it is “high in the west and low in the east.” The heat flow can exceed 70 mW/m2 in the northern marginal fold-and-thrust belt of western Kunlun and Nanyishan and generally exceeds 65 mW/m2 in the Mangya Depression. The average radiogenic heat production rate (HPR) is 2.53 μW/m3, which is close to the HPR of granite in northern Tibetan Plateau. The crustal and mantle heat flows in the Qaidam Basin are 32.9 mW/m2 and 26.7 mW/m2, respectively. The crustal contribution to the surface heat flow was approximately 55 %. The geothermal regime may be dominated by lithospheric thickness, HPR of sedimentary cover, and extra heat production related to late Cenozoic tectonic movement. The proven hydrocarbon reserves are primarily distributed around hydrocarbon-generating kitchens, and the existence of abnormally high-temperature zones significantly influences hydrocarbon distribution. The Qaidam Basin satisfies the fundamental temperature conditions for the development of low- and medium-temperature geothermal resources using abandoned oil and gas drilling wells.}
}
@article{COLLINS2022100518,
title = {Review: Smart agri-systems for the pig industry},
journal = {animal},
volume = {16},
pages = {100518},
year = {2022},
note = {Manipulating Pig Production XVIII: Proceedings of the Eighteenth Biennial Conference of the Australasian Pig Science Association (APSA), 15-18 November 2021, Brisbane, Australia},
issn = {1751-7311},
doi = {https://doi.org/10.1016/j.animal.2022.100518},
url = {https://www.sciencedirect.com/science/article/pii/S1751731122000696},
author = {L.M. Collins and L.M. Smith},
keywords = {Digital farming, Pork, Precision Livestock Farming, Production, Systems approach},
abstract = {The projected rise in the global human population and the anticipated increase in demand for meat and animal products, albeit with a greatly reduced environmental footprint, offers a difficult set of challenges to the livestock sector. Primarily, how do we produce more, but in a way that is healthier for the animals, public, and the environment? Implementing a smart agri-systems approach, utilising multiplatform precision technologies, internet of things, data analytics, machine learning, digital twinning and other emerging technologies can support a more informed decision-making and forecasting position that will allow us to move towards greater sustainability in future. If we look to precision agronomy, there are a wide range of technologies available and examples of how digitalisation and integration of platform outputs can lead to advances in understanding the agricultural system and forecasting upcoming events and performance that have hitherto been impossible to achieve. There is much for the livestock sector and animal scientists to learn from the developments of precision technologies and smart agri-system approaches in the arable and horticultural contexts. However, there are several barriers the livestock sector must overcome: (i) the development and implementation of precision livestock farming technologies that can be easily integrated and analysed without the support of a dedicated data analyst in house; (ii) the lack of extensive validation of many developed and available precision livestock farming technologies means that reliability and accuracy are likely to be compromised when applied in commercial practice; (iii) the best smart agri-systems approaches are reliant on large quantities of data from across a wide variety of conditions, but at present the complications of data sharing, commercial sensitivities, data ownership, and permissions make it challenging to obtain or knit together data from different parts of the system into a comprehensive picture; and (iv) the high level of investment needed to develop and scale these technologies is substantial and represents significant risk for companies when a technology is emerging. Using a case study of the National Pig Centre (a flagship pig research facility in the UK) we discuss how a smart agri-systems approach can be applied in practice to investigate alternative future systems for production, and enable monitoring of these systems as a commercial demonstrator site for future pork production.}
}
@article{VERSTRYNGE2022128732,
title = {Steel corrosion damage monitoring in reinforced concrete structures with the acoustic emission technique: A review},
journal = {Construction and Building Materials},
volume = {349},
pages = {128732},
year = {2022},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2022.128732},
url = {https://www.sciencedirect.com/science/article/pii/S0950061822023893},
author = {Els Verstrynge and Charlotte {Van Steen} and Eline Vandecruys and Martine Wevers},
keywords = {Acoustic emission monitoring, Reinforced concrete, Corrosion, Experimental datasets, Literature review, On-site application},
abstract = {Corrosion is a major issue in construction and infrastructure management, giving rise to high repair costs, safety concerns and structural failures. Especially reinforcement corrosion in concrete structures is a complex issue. As degradation starts internally, visual inspection lacks efficiency in early stages of the corrosion process. Acoustic Emission (AE) monitoring provides major advantages as this technique enables to detect internal damage in an early stage. However, the use of AE monitoring also comes with certain challenges. The aim of this paper is to review appropriate protocols for AE-based reinforced concrete (RC) corrosion monitoring based on an overview of reported lab experiments, highlighting challenges and pitfalls, and to provide a solid basis for extension of the research focus towards on-site AE-based corrosion monitoring in RC structures. Therefore, this paper firstly discusses AE monitoring during (accelerated) corrosion tests, including methods for data analysis and best-practice protocols. Secondly, application of the AE technique for condition assessment of corroded RC elements during load testing is discussed. Thirdly, an overview is presented of reported on-site AE monitoring studies on corroding RC structures. The paper concludes with a discussion and future research challenges, especially towards on-site monitoring of existing RC structures.}
}
@article{MAROUFKHANI2022102622,
title = {Digital transformation in the resource and energy sectors: A systematic review},
journal = {Resources Policy},
volume = {76},
pages = {102622},
year = {2022},
issn = {0301-4207},
doi = {https://doi.org/10.1016/j.resourpol.2022.102622},
url = {https://www.sciencedirect.com/science/article/pii/S030142072200071X},
author = {Parisa Maroufkhani and Kevin C. Desouza and Robert K. Perrons and Mohammad Iranmanesh},
keywords = {Digital technologies, Resource sector, Energy, Oil and gas, Mining, Digitalization},
abstract = {The forces of digital transformation have delivered significant benefits like sustainable development and economic growth in a range of early adopter industries such as retail and manufacturing but, despite these potential benefits, the resource and energy sectors have been relative latecomers to digitalization simply because they are frequently slower to absorb new technologies. Here we present the results of a systematic literature review identifying the ways in which digital technologies have been applied in the oil and gas, mining, and energy domains. We applied content and descriptive analysis to evaluate and discuss 151 academic articles selected from the Scopus database. Two particularly interesting trends emerge from the analysis. First, over 75% of the papers were about the energy sector excluding the oil & gas industry, and only a small minority were from the mining or oil & gas sectors. Second, the most frequently discussed objective of digital transformation was the reduction of operational expenses. By surveying the different ways in which these innovations have been used in these industries and identifying trends and patterns in how digital technologies have been applied, the findings of this review deepen our understanding of the current state of digital technologies within the resource and energy sectors and, in so doing, shine a useful amount of light on the contributions that digital transformation has made to businesses in these sectors. This paper also highlights for future scholars, practitioners, and policymakers the six research areas that they should focus on in the future to help the resource and energy sectors accelerate the digital transformation process and improve their ability to deliver value with these innovations.}
}
@article{SHAHAAB2022101759,
title = {Public service operational efficiency and blockchain – A case study of Companies House, UK},
journal = {Government Information Quarterly},
pages = {101759},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101759},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000958},
author = {Ali Shahaab and Imtiaz A. Khan and Ross Maude and Chaminda Hewage and Yingli Wang},
keywords = {Blockchain, Distributed ledger technology, Public service operation, Design science, Case study},
abstract = {Despite the increasing interest and exploration of the use of blockchain technology in public service organisations (PSOs), academic understanding of its transformative impact on the operational excellence of PSOs remains limited. This study adopts an action design science research methodology to develop a proof of concept (POC) blockchain based application for Companies House, a government agency that is registering companies across UK. The application addresses the operational challenges of Companies House as well as issues citizens face when accessing its services. We draw from the public value framework proposed by Twizeyimana and Andersson (2019) and demonstrate the significance of the emerging blockchain technology in relation to their democratic practices based on six dimensions. We further discuss the related challenges and barriers for its implementation and evaluate the POC with the stakeholders of Companies House. We also present an illustrative case study, where we explored the appropriateness of the POC in relation to the draft legislation, “Registration of Overseas Entities and Beneficial Owners” (ROEBO) bill which proposes the introduction of a register of the beneficial owners of overseas legal entities that own real estate in the UK. Our research is one of the few studies that will provide in-depth empirical insights about the relationship between blockchain and operational excellence of PSOs.}
}
@article{YU2022104002,
title = {Developing an ETL tool for converting the PCORnet CDM into the OMOP CDM to facilitate the COVID-19 data integration},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {104002},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104002},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000181},
author = {Yue Yu and Nansu Zong and Andrew Wen and Sijia Liu and Daniel J. Stone and David Knaack and Alanna M. Chamberlain and Emily Pfaff and Davera Gabriel and Christopher G. Chute and Nilay Shah and Guoqian Jiang},
abstract = {Objective
The large-scale collection of observational data and digital technologies could help curb the COVID-19 pandemic. However, the coexistence of multiple Common Data Models (CDMs) and the lack of data extract, transform, and load (ETL) tool between different CDMs causes potential interoperability issue between different data systems. The objective of this study is to design, develop, and evaluate an ETL tool that transforms the PCORnet CDM format data into the OMOP CDM.
Methods
We developed an open-source ETL tool to facilitate the data conversion from the PCORnet CDM and the OMOP CDM. The ETL tool was evaluated using a dataset with 1000 patients randomly selected from the PCORnet CDM at Mayo Clinic. Information loss, data mapping accuracy, and gap analysis approaches were conducted to assess the performance of the ETL tool. We designed an experiment to conduct a real-world COVID-19 surveillance task to assess the feasibility of the ETL tool. We also assessed the capacity of the ETL tool for the COVID-19 data surveillance using data collection criteria of the MN EHR Consortium COVID-19 project.
Results
After the ETL process, all the records of 1000 patients from 18 PCORnet CDM tables were successfully transformed into 12 OMOP CDM tables. The information loss for all the concept mapping was less than 0.61%. The string mapping process for the unit concepts lost 2.84% records. Almost all the fields in the manual mapping process achieved 0% information loss, except the specialty concept mapping. Moreover, the mapping accuracy for all the fields were 100%. The COVID-19 surveillance task collected almost the same set of cases (99.3% overlaps) from the original PCORnet CDM and target OMOP CDM separately. Finally, all the data elements for MN EHR Consortium COVID-19 project could be captured from both the PCORnet CDM and the OMOP CDM.
Conclusion
We demonstrated that our ETL tool could satisfy the data conversion requirements between the PCORnet CDM and the OMOP CDM. The outcome of the work would facilitate the data retrieval, communication, sharing, and analysis between different institutions for not only COVID-19 related project, but also other real-world evidence-based observational studies.}
}
@article{PRATI2022104791,
title = {Correlates of quality of life, happiness and life satisfaction among European adults older than 50 years: A machine‐learning approach},
journal = {Archives of Gerontology and Geriatrics},
volume = {103},
pages = {104791},
year = {2022},
issn = {0167-4943},
doi = {https://doi.org/10.1016/j.archger.2022.104791},
url = {https://www.sciencedirect.com/science/article/pii/S0167494322001789},
author = {Gabriele Prati},
keywords = {Well-being, Machine learning, Random forest, Gradient boosting},
abstract = {Background and objectives
Previous research has documented the role of different categories of psychosocial factors (i.e., sociodemographic factors, personality, subjective life circumstances, activity, physical health, and childhood circumstances) in predicting subjective well-being and quality of life among older adults. No previous study has simultaneously modeled a large number of these psychosocial factors using a well-powered sample and machine learning algorithms to predict quality of life, happiness, and life satisfaction among older adults. The aim of this paper was to investigate the correlates of quality of life, happiness, and life satisfaction among European adults older than 50 years using machine learning techniques.
Research design and methods
Data drawn from the Survey of Health, Ageing and Retirement in Europe (SHARE) Wave 7 were used. Participants were 62,500 persons aged 50 years and over living in 26 Continental EU Member States, Switzerland, and Israel. Multiple machine learning regression approaches were used.
Results
The algorithms captured 53%, 33%, and 18% of the variance of quality of life, life satisfaction, and happiness, respectively. The most important categories of correlates of quality of life and life satisfaction were physical health and subjective life circumstances. Sociodemographic factors (mostly country of residence) and psychological variables were the most important categories of correlates of happiness.
Discussion and implications
This study highlights subjective poverty, self-perceived health, country of residence, subjective survival probability, and personality factors (especially neuroticism) as important correlates of quality of life, happiness, and life satisfaction. These findings provide evidence-based recommendations for practice and/or policy implications.}
}