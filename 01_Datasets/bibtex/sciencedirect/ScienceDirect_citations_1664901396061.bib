@article{SINGH2018883,
title = {Deep Learning for Plant Stress Phenotyping: Trends and Future Perspectives},
journal = {Trends in Plant Science},
volume = {23},
number = {10},
pages = {883-898},
year = {2018},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1360138518301572},
author = {Asheesh Kumar Singh and Baskar Ganapathysubramanian and Soumik Sarkar and Arti Singh},
keywords = {high throughput, phenomics, machine learning, diseases, transfer learning, imaging, smartphone app, automation},
abstract = {Deep learning (DL), a subset of machine learning approaches, has emerged as a versatile tool to assimilate large amounts of heterogeneous data and provide reliable predictions of complex and uncertain phenomena. These tools are increasingly being used by the plant science community to make sense of the large datasets now regularly collected via high-throughput phenotyping and genotyping. We review recent work where DL principles have been utilized for digital image–based plant stress phenotyping. We provide a comparative assessment of DL tools against other existing techniques, with respect to decision accuracy, data size requirement, and applicability in various scenarios. Finally, we outline several avenues of research leveraging current and future DL tools in plant science.}
}
@article{LIN2018729,
title = {User experience-based product design for smart production to empower industry 4.0 in the glass recycling circular economy},
journal = {Computers & Industrial Engineering},
volume = {125},
pages = {729-738},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S036083521830295X},
author = {Kuo-Yi Lin},
keywords = {Circular economy, Product design, User experience, Smart production, Glass recycling, Product decision-making information system},
abstract = {Glass recycling can help reduce resource consumption, waste, energy use, and air and water pollution. Recycled glass products are figuring more prominently in global environmental policy trends. However, glass recycling product design is confronted with uncertainty of market demands. Manufacturers of various glass recycling product types such as glass art, glassware, and building materials face challenges of better understanding the needs of users and connecting user demand to product design prototypes, resulting in product design and user barriers. This study aimed to define a user experience-based product design approach from a people-oriented perspective. The proposed smart production approach to empowering industry 4.0 in the circular economy of the glass recycling industry explores product decision-making information systems and data-driven innovation. To understand user preferences for products, this study focused on key product factors and crucial user characteristics. Decision-making information systems were employed to identify the user experience for product and service design to provide useful information for smart production. This study was conducted in cooperation with a leading glass recycling company in Taiwan to enhance recycling incentives and empower industry 4.0 in the circular economy.}
}
@incollection{PAUL20181,
title = {Chapter One - Citizen Science and Low-Cost Sensors for Integrated Water Resources Management},
editor = {Jan Friesen and Leonor Rodríguez-Sinobas},
series = {Advances in Chemical Pollution, Environmental Management and Protection},
publisher = {Elsevier},
volume = {3},
pages = {1-33},
year = {2018},
booktitle = {Advanced Tools for Integrated Water Resources Management},
issn = {2468-9289},
doi = {https://doi.org/10.1016/bs.apmp.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468928918300194},
author = {Jonathan D. Paul and Wouter Buytaert},
keywords = {Citizen science, IWRM, Knowledge cocreation, Low-cost sensors, Participatory monitoring, Polycentric governance},
abstract = {One of the principal objectives of Integrated Water Resources Management (IWRM) is to solve water and development problems using methods that are environmentally sustainable, socially equitable, and efficient. In this chapter, we demonstrate how the principles of citizen science—the intrinsic involvement of nonscientist local stakeholders throughout the entire life cycle of a project—have the potential to complement and benefit those of IWRM; in particular, in relation to creating scientific evidence that underpins the governance of water resources. The rapid, recent development of information and communication technologies such as the Internet, smartphones, and social media has already demonstrated their capacity to make knowledge creation and governance more multidirectional, decentralized, diverse, and inclusive. We show how such new technologies can be harnessed with a citizen science approach to interrogate different aspects of the water cycle. One particularly promising development is to support citizen science with robust and low-cost sensor and networking technology. Such networks offer not only improved data coverage (especially in remote or data-scarce regions) but also enhanced sustainability at a time when monitoring networks are in decline globally. We argue that, in certain cases, citizen science and polycentric governance approaches can enrich and complement IWRM and merit greater attention.}
}
@incollection{INDU2017257,
title = {Chapter 13 - Making the Most of the Earth Observation Data Using Effective Sampling Techniques},
editor = {George P. Petropoulos and Prashant K. Srivastava},
booktitle = {Sensitivity Analysis in Earth Observation Modelling},
publisher = {Elsevier},
pages = {257-272},
year = {2017},
isbn = {978-0-12-803011-0},
doi = {https://doi.org/10.1016/B978-0-12-803011-0.00013-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128030110000136},
author = {J. Indu and D. {Nagesh Kumar}},
keywords = {Bootstrap, Data assimilation, TRMM},
abstract = {High-resolution data products from Earth observing satellites facilitate continuous monitoring of the physical, chemical, and biological processes of the Earth system. With the plethora of data made available from the foreseen dense halo of Earth observing satellites, it is of utmost importance to produce the best estimate of the Earth system state for diverse applications. This is achieved using data assimilation, which carefully considers the measurement process, its associated errors, governing equations of the system, and the expected errors in these equations. In data assimilation, sampling techniques serve as an effective means to propagate information from data-rich regions to data-poor regions. This is essential to overcome the limited spatial and temporal sampling of satellite information. Both scientists and researchers have followed various sampling schemes. This chapter summarizes some of the prominent sampling strategies, which have evolved with respect to analyzing data from Earth observing satellites. The applicability of bootstrap and Latin hypercube techniques in assessing sampling errors of Tropical Rainfall Measuring Mission orbital data are discussed as a case study.}
}
@incollection{LO2018141,
title = {Chapter 11 - Using Multicenter Clinical Registries to Improve Outcomes},
editor = {Daniel J. Guillaume and Matthew A. Hunt},
booktitle = {Quality and Safety in Neurosurgery},
publisher = {Academic Press},
pages = {141-167},
year = {2018},
isbn = {978-0-12-812898-5},
doi = {https://doi.org/10.1016/B978-0-12-812898-5.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128985000126},
author = {William B. Lo and François Mathieu and Jay Riva-Cambrin and John R.W. Kestle and Abhaya V. Kulkarni},
keywords = {Multicenter, Registries, Quality, Outcome improvement, Research, Randomized controlled trial, Pragmatic registry-based observational studies},
abstract = {In the past few decades, a growing number of clinical registries have been set up across specialties, by research collaborators, professional societies, or as a result of health authority requirement. Multicenter neurosurgery and subspecialty registries cover a large number of patients and provide real-world data, and have been increasingly employed for safety and quality assurance, outcome improvement projects, and research in clinical and cost-effectiveness. This chapter focuses on background, principles, and practical issues of clinical registries, as illustrated by examples. Furthermore, over the past 20–30 years, there has been a rise in pragmatic registry-based observational studies which offer some advantages over the traditional gold standard of randomized control trials. A comparison of the two research methodologies is made. Their relative strengths, limitations, and challenges are highlighted and their complementary roles are emphasized.}
}
@incollection{GALAR201773,
title = {Chapter 2 - Data Collection},
editor = {Diego Galar and Uday Kumar},
booktitle = {eMaintenance},
publisher = {Academic Press},
pages = {73-128},
year = {2017},
isbn = {978-0-12-811153-6},
doi = {https://doi.org/10.1016/B978-0-12-811153-6.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128111536000026},
author = {Diego Galar and Uday Kumar},
keywords = {ETL process, Internet of things, Random data, Secure erase, SQL, Write zero},
abstract = {Industrial planners and managers need to understand the dynamics of complete supply chain, i.e., stocks, operations, infrastructure, communities, and individuals involved in the sector to set policy and manage industrial assets. Data collection and analysis, for example, can provide information on how industry is likely to respond to different policies. Constraints on production and development of new factories can be identified. Prices and cost changes in the manufacturing facilities can be assessed. Stocks likely to receive increased levels of exploitation may be identified before resource levels drop to a crisis point.}
}
@incollection{FEDERER201849,
title = {5 - Providing meaningful information: Part C—Data management and visualization},
editor = {Antonio P. DeRosa},
booktitle = {A Practical Guide for Informationists},
publisher = {Chandos Publishing},
pages = {49-62},
year = {2018},
isbn = {978-0-08-102017-3},
doi = {https://doi.org/10.1016/B978-0-08-102017-3.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102017300005X},
author = {Lisa Federer},
keywords = {Data management, Data visualization, Data services, Informationist services, Research data management, Research support services},
abstract = {This chapter provides an introduction to how informationists can support one of the biggest challenges that researchers and medical professionals now face: how to deal with the rapidly increasing data deluge. With the size of research and clinical data growing exponentially, and with new policies from funders and journals, researchers and clinicians need help to ensure that they are able to work effectively with their data in ways that comply with requirements but do not present an undue burden. Specifically, this chapter considers how informationists can provide support for data management and visualization. Many of the skills that informationists already have are applicable to these problems, and this chapter also provides suggestions for how informationists can get started with gaining new skills and designing data services.}
}
@article{RADENKOVIC201840,
title = {Harnessing business intelligence in smart grids: A case of the electricity market},
journal = {Computers in Industry},
volume = {96},
pages = {40-53},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517301926},
author = {Miloš Radenković and Jelena Lukić and Marijana Despotović-Zrakić and Aleksandra Labus and Zorica Bogdanović},
keywords = {Smart grid, Business intelligence, Electricity market, Data warehouse},
abstract = {This paper discusses analytical aspects of smart grids and offers insights into the development of a business intelligence solution for the electricity market. The goal is to design a system that provides an emerging electricity market with the necessary data flows and information for forecasting, data analysis and decision making, leading to better business results and more control over the market. By employing a methodology specifically suited to the electricity market domain, we designed a business intelligence solution for the Serbian electricity market operator “Elektromreža Srbije”. The research results show that the proposed approach leads to more effective market management in data-rich smart grid environments, while still being dynamic enough to adapt to frequent rule changes in the still developing grids and their markets.}
}
@article{THANARAJASINGAM2018e563,
title = {Beyond maximum grade: modernising the assessment and reporting of adverse events in haematological malignancies},
journal = {The Lancet Haematology},
volume = {5},
number = {11},
pages = {e563-e598},
year = {2018},
issn = {2352-3026},
doi = {https://doi.org/10.1016/S2352-3026(18)30051-6},
url = {https://www.sciencedirect.com/science/article/pii/S2352302618300516},
author = {Gita Thanarajasingam and Lori M Minasian and Frederic Baron and Franco Cavalli and R Angelo {De Claro} and Amylou C Dueck and Tarec C El-Galaly and Neil Everest and Jan Geissler and Christian Gisselbrecht and John Gribben and Mary Horowitz and S Percy Ivy and Caron A Jacobson and Armand Keating and Paul G Kluetz and Aviva Krauss and Yok Lam Kwong and Richard F Little and Francois-Xavier Mahon and Matthew J Matasar and María-Victoria Mateos and Kristen McCullough and Robert S Miller and Mohamad Mohty and Philippe Moreau and Lindsay M Morton and Sumimasa Nagai and Simon Rule and Jeff Sloan and Pieter Sonneveld and Carrie A Thompson and Kyriaki Tzogani and Flora E {van Leeuwen} and Galina Velikova and Diego Villa and John R Wingard and Sophie Wintrich and John F Seymour and Thomas M Habermann},
abstract = {Summary
Tremendous progress in treatment and outcomes has been achieved across the whole range of haematological malignancies in the past two decades. Although cure rates for aggressive malignancies have increased, nowhere has progress been more impactful than in the management of typically incurable forms of haematological cancer. Population-based data have shown that 5-year survival for patients with chronic myelogenous and chronic lymphocytic leukaemia, indolent B-cell lymphomas, and multiple myeloma has improved markedly. This improvement is a result of substantial changes in disease management strategies in these malignancies. Several haematological malignancies are now chronic diseases that are treated with continuously administered therapies that have unique side-effects over time. In this Commission, an international panel of clinicians, clinical investigators, methodologists, regulators, and patient advocates representing a broad range of academic and clinical cancer expertise examine adverse events in haematological malignancies. The issues pertaining to assessment of adverse events examined here are relevant to a range of malignancies and have been, to date, underexplored in the context of haematology. The aim of this Commission is to improve toxicity assessment in clinical trials in haematological malignancies by critically examining the current process of adverse event assessment, highlighting the need to incorporate patient-reported outcomes, addressing issues unique to stem-cell transplantation and survivorship, appraising challenges in regulatory approval, and evaluating toxicity in real-world patients. We have identified a range of priority issues in these areas and defined potential solutions to challenges associated with adverse event assessment in the current treatment landscape of haematological malignancies.}
}
@article{OJHA201797,
title = {Metaheuristic design of feedforward neural networks: A review of two decades of research},
journal = {Engineering Applications of Artificial Intelligence},
volume = {60},
pages = {97-116},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2017.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0952197617300234},
author = {Varun Kumar Ojha and Ajith Abraham and Václav Snášel},
keywords = {Feedforward neural network, Metaheuristics, Nature-inspired algorithms, Multiobjective, Ensemble},
abstract = {Over the past two decades, the feedforward neural network (FNN) optimization has been a key interest among the researchers and practitioners of multiple disciplines. The FNN optimization is often viewed from the various perspectives: the optimization of weights, network architecture, activation nodes, learning parameters, learning environment, etc. Researchers adopted such different viewpoints mainly to improve the FNN's generalization ability. The gradient-descent algorithm such as backpropagation has been widely applied to optimize the FNNs. Its success is evident from the FNN's application to numerous real-world problems. However, due to the limitations of the gradient-based optimization methods, the metaheuristic algorithms including the evolutionary algorithms, swarm intelligence, etc., are still being widely explored by the researchers aiming to obtain generalized FNN for a given problem. This article attempts to summarize a broad spectrum of FNN optimization methodologies including conventional and metaheuristic approaches. This article also tries to connect various research directions emerged out of the FNN optimization practices, such as evolving neural network (NN), cooperative coevolution NN, complex-valued NN, deep learning, extreme learning machine, quantum NN, etc. Additionally, it provides interesting research challenges for future research to cope-up with the present information processing era.}
}
@article{BUYUKOZKAN2018157,
title = {Digital Supply Chain: Literature review and a proposed framework for future research},
journal = {Computers in Industry},
volume = {97},
pages = {157-177},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517304487},
author = {Gülçin Büyüközkan and Fethullah Göçer},
keywords = {Digital Supply Chain (DSC), Literature review, Technology enablers, DSC framework},
abstract = {Suppliers, partners, companies and dealers in supply chains do use, generate and share information with others. These associations lead to a multitude of challenges and opportunities within the supply chains. A Digital Supply Chain (DSC) is a smart, value-driven, efficient process to generate new forms of revenue and business value for organizations and to leverage new approaches with novel technological and analytical methods DSC is not about whether goods and services are digital or physical, it is about the way how supply chain processes are managed with a wide variety of innovative technologies, e.g. unmanned aerial vehicles, cloud computing, and internet of things, among others. Recent literature highlights the importance of DSC and many industrial researchers discuss its applications. This article reviews the state-of-the-art of existing DSC literature in detail from both academic and industrial points of view. It identifies key limitations and prospects in DSC, summarizes prior research and identifies knowledge gaps by providing advantages, weaknesses and limitations of individual methods The article also aims at providing a development framework as a roadmap for future research and practice.}
}
@article{OTEROSROZAS201874,
title = {Using social media photos to explore the relation between cultural ecosystem services and landscape features across five European sites},
journal = {Ecological Indicators},
volume = {94},
pages = {74-86},
year = {2018},
note = {Landscape Indicators – Monitoring of Biodiversity and Ecosystem Services at Landscape Level},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2017.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X17300572},
author = {Elisa Oteros-Rozas and Berta Martín-López and Nora Fagerholm and Claudia Bieling and Tobias Plieninger},
keywords = {Landscape values, Non-material benefits, Photos, Social preferences, User generated content (UGC)},
abstract = {Cultural ecosystem services, such as aesthetic and recreational enjoyment, as well as sense of place and local identity, play an outstanding role in the contribution of landscapes to human well-being. Online data shared on social networks, particularly geo-tagged photos, are becoming an increasingly attractive source of information about cultural ecosystem services. Landscape photographs tell about the significance of human relationships with landscapes, human practices in landscapes and the landscape features that might possess value in terms of cultural ecosystem services. Despite all the recent advances in this emerging methodological approach, some challenges remain to be explored: (a) how to assess a broad suite of cultural ecosystem services, beyond aesthetic beauty of landscapes, (b) how to identify the landscape features that are relevant for providing cultural ecosystem services and determine trade-offs and synergies among cultural ecosystem services. To address these challenges, we have developed a methodological approach suitable for eliciting the importance of cultural ecosystem services and the landscape features underpinning their provision across five different sites in Europe (in Estonia, Greece, Spain, Sweden and Switzerland). We have performed a content analysis of 1.404 photos uploaded in Flickr and Panoramio platforms that can represent cultural ecosystem services. Four bundles of landscapes features and cultural ecosystem services showed the relation of recreation with mountain areas (terrestrial recreation) and with water bodies (aquatic recreation). Cultural heritage, social and spiritual values were particularly attached to landscapes with woodpastures and grasslands, as well as urban features and infrastructures, i.e. to more anthropogenic landscapes. A positive though weak relationship was found between landscape diversity and cultural ecosystem services diversity. Particularly wood-pastures and shrubs were more frequently portrayed in all study sites in comparison with their actual land cover. The results can be of interest both for methodological purposes in the face of an increasing trend in the use of geo-tagged photos in the ecosystem services research and for the elicitation and comparison of landscape values across European cultural landscapes.}
}
@article{PIELMEIER2017271,
title = {Modeling Approach for Situational Event-handling within Production Planning and Control Based on Complex Event Processing},
journal = {Procedia CIRP},
volume = {63},
pages = {271-276},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.158},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117303402},
author = {Julia Pielmeier and Stefan Braunreuther and Gunther Reinhart},
keywords = {Modeling, Production, Process control},
abstract = {Nowadays industrial production environments are complex, volatile, and driven by uncertainties. Manufacturing companies are striving for flexibility and adaptability to cope with these challenges and remain competitive. Market requirements such as shortened product life cycles, increasing number of variants, and customized products lead to complexity in manufacturing systems. Possible approaches to cope with such challenges can be found in the field of ‘Industrie 4.0’. In particular, decision-making and real-time reaction systems are one way to handle the complexity. To cope with this complexity, digitalization like the vision of ‘Industrie 4.0’ can offer different solutions. However, digitalization leads to an increase of the amount of data describing the status of products and resources within an industrial production environment. In order to achieve a near real time monitoring and control of production and logistics processes, intelligent processing and analyzing of the acquired data is necessary. As a result of this development, so called “complex event processing” (CEP) is essential for analyzing extensive data streams in real-time. In order to derive the rules for a CEP engine, an event model has to be described to visualize the relations, constraints and abstraction levels of production processes. The main focus within this paper is a modeling approach for the situational handling of events within production planning and control. The requirements of the modeling method are focused on the use case of a mass production for carbon-fiber-reinforced plastic CFRP components.}
}
@article{MILLER20181365,
title = {A review of unsupervised statistical learning and visual analytics techniques applied to performance analysis of non-residential buildings},
journal = {Renewable and Sustainable Energy Reviews},
volume = {81},
pages = {1365-1377},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2017.05.124},
url = {https://www.sciencedirect.com/science/article/pii/S1364032117307608},
author = {Clayton Miller and Zoltán Nagy and Arno Schlueter},
keywords = {Building performance analysis, Data mining, Unsupervised learning, Visual analytics, Clustering, Novelty detection, Smart meter analysis, Portfolio analysis, Review, Building controls and optimization},
abstract = {Measured and simulated data sources from the built environment are increasing rapidly. It is becoming normal to analyze data from hundreds, or even thousands of buildings at once. Mechanistic, manual analysis of such data sets is time-consuming and not realistic using conventional techniques. Thus, a significant body of literature has been generated using unsupervised statistical learning techniques designed to uncover structure and information quickly with fewer input parameters or metadata about the buildings collected. Further, visual analytics techniques are developed as aids in this process for a human analyst to utilize and interpret the results. This paper reviews publications that include the use of unsupervised machine learning techniques as applied to non-residential building performance control and analysis. The categories of techniques covered include clustering, novelty detection, motif and discord detection, rule extraction, and visual analytics. The publications apply these technologies in the domains of smart meters, portfolio analysis, operations and controls optimization, and anomaly detection. A discussion is included of key challenges resulting from this review, such as the need for better collaboration between several, disparate research communities and the lack of open, benchmarking data sets. Opportunities for improvement are presented including methods of reproducible research and suggestions for cross-disciplinary cooperation.}
}
@article{DEOLIVEIRA2017259,
title = {Spatial and temporal characterization of energy demand and resources for an existing and dense urban district in Geneva},
journal = {Energy Procedia},
volume = {122},
pages = {259-264},
year = {2017},
note = {CISBAT 2017 International ConferenceFuture Buildings & Districts – Energy Efficiency from Nano to Urban Scale},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.07.312},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217329107},
author = {Fleury {de Oliveira} and Stefan Schneider and Loïc Quiquerez and Bernard Lachal and Pierre Hollmuller},
keywords = {Energy planning, Urban simulation, GIS, Load curves},
abstract = {This paper focuses on the characterization of the energy demand and the locally available renewable resources for an existing and dense urban district. Objective is setting up an integrated spatio-temporal database, for the evaluation of diverse strategies concerning the integration of renewables in the existing urban fabric. In a first step, the various demands (heating, cooling and electricity) are characterized in terms of annual values and spatial distribution (at building level), by merging of diverse databases containing actual monitored values, in combination with specific models for the completion of missing data, and for split up in terms of the various usages. In a second step, the aggregated demands (at district level) are characterized in terms of hourly dynamic, for a common reference year, by re-scaling of monitored load curves. Similarly, the existing resources (solar, hydro- and aero-thermal) are characterized by way of monitored values, in terms of hourly dynamic (for the same common reference year), as well as spatial distribution (in particular concerning solar irradiation on building roofs and geothermal resources).}
}
@article{DICKINSON2018682,
title = {Remote Monitoring of Patients With Heart Failure: A White Paper From the Heart Failure Society of America Scientific Statements Committee},
journal = {Journal of Cardiac Failure},
volume = {24},
number = {10},
pages = {682-694},
year = {2018},
issn = {1071-9164},
doi = {https://doi.org/10.1016/j.cardfail.2018.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1071916418310625},
author = {Michael G. Dickinson and Larry A. Allen and Nancy A. Albert and Thomas DiSalvo and Gregory A. Ewald and Amanda R. Vest and David J. Whellan and Michael R. Zile and Michael M. Givertz},
keywords = {Telehealth, CardioMEMS, heart failure, remote patient monitoring},
abstract = {Background
After several neutral telehealth trials, the positive findings and subsequent Food and Drug Administration approval of an implantable pulmonary arterial pressure monitor (PAPM) led to renewed interest in remote patient monitoring (RPM). Here we seek to provide contemporary guidance on the appropriate use of RPM technology.
Results
Although early trials of external RPM devices suggested benefit, subsequent multicenter trials failed to demonstrate improved outcomes. Monitoring features of cardiac implantable electronic devices (CIEDs) also did not deliver improved HF outcomes, newer, multisensor algorithms may be better. Earlier technologies using direct pressure measurement via implanted devices failed to show benefit owing to complications or failure. Recently, 1 PAPM showed benefit in a randomized controlled trial. Although not showing cost reduction, cost-benefit analysis of that device suggests that it may meet acceptable standards. Additional research is warranted and is in progress. Consumer-owned electronic devices are becoming more pervasive and hold hope for future benefit in HF management. Practical aspects around RPM technology include targeting of risk populations, having mechanisms to ensure patient adherence to monitoring, and health care team structures that act on the data.
Conclusions
Based on available evidence, routine use of external RPM devices is not recommended. Implanted devices that monitor pulmonary arterial pressure and/or other parameters may be beneficial in selected patients or when used in structured programs, but the value of these devices in routine care requires further study. Future research is also warranted to better understand the cost-effectiveness of these devices.}
}
@article{VELUPILLAI201811,
title = {Using clinical Natural Language Processing for health outcomes research: Overview and actionable suggestions for future advances},
journal = {Journal of Biomedical Informatics},
volume = {88},
pages = {11-19},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418302016},
author = {Sumithra Velupillai and Hanna Suominen and Maria Liakata and Angus Roberts and Anoop D. Shah and Katherine Morley and David Osborn and Joseph Hayes and Robert Stewart and Johnny Downs and Wendy Chapman and Rina Dutta},
keywords = {Natural Language Processing, Information extraction, Text analytics, Evaluation, Clinical informatics, Mental Health Informatics, Epidemiology, Public Health},
abstract = {The importance of incorporating Natural Language Processing (NLP) methods in clinical informatics research has been increasingly recognized over the past years, and has led to transformative advances. Typically, clinical NLP systems are developed and evaluated on word, sentence, or document level annotations that model specific attributes and features, such as document content (e.g., patient status, or report type), document section types (e.g., current medications, past medical history, or discharge summary), named entities and concepts (e.g., diagnoses, symptoms, or treatments) or semantic attributes (e.g., negation, severity, or temporality). From a clinical perspective, on the other hand, research studies are typically modelled and evaluated on a patient- or population-level, such as predicting how a patient group might respond to specific treatments or patient monitoring over time. While some NLP tasks consider predictions at the individual or group user level, these tasks still constitute a minority. Owing to the discrepancy between scientific objectives of each field, and because of differences in methodological evaluation priorities, there is no clear alignment between these evaluation approaches. Here we provide a broad summary and outline of the challenging issues involved in defining appropriate intrinsic and extrinsic evaluation methods for NLP research that is to be used for clinical outcomes research, and vice versa. A particular focus is placed on mental health research, an area still relatively understudied by the clinical NLP research community, but where NLP methods are of notable relevance. Recent advances in clinical NLP method development have been significant, but we propose more emphasis needs to be placed on rigorous evaluation for the field to advance further. To enable this, we provide actionable suggestions, including a minimal protocol that could be used when reporting clinical NLP method development and its evaluation.}
}
@article{FOX201848,
title = {The semantics of populations: A city indicator perspective},
journal = {Journal of Web Semantics},
volume = {48},
pages = {48-65},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300015},
author = {Mark S. Fox},
keywords = {Population semantics, Ontology, City indicators, Consistency checking},
abstract = {This paper addresses the question of how to represent the semantics of populations. This question is unusual in the sense that statistics is directly concerned with the definition of populations but is essentially silent on the representation of population definitions from a data modeling perspective. The motivation for this work is the development of ontologies for the representation of city indicator definitions. A city indicator measures the performance of a city in areas such as education, transportation and the environment. The definitions of city indicators rely on definitions for populations of people, built form, events, activities, and sensor measurements. This paper provides a model for representing membership extent, temporal extent, spatial extent, and measurement of populations. It demonstrates the approach by representing the definitions of city indicators as defined by ISO 37120, the interpretation of these definitions by cities, and their comparison to ascertain whether a city’s interpretation is consistent with the standard.}
}
@article{VANCALSTER2017775,
title = {ProfARIMA: A profit-driven order identification algorithm for ARIMA models in sales forecasting},
journal = {Applied Soft Computing},
volume = {60},
pages = {775-785},
year = {2017},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2017.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S1568494617300893},
author = {Tine {Van Calster} and Bart Baesens and Wilfried Lemahieu},
keywords = {Sales forecasting, Seasonal ARIMA, Order identification, Genetic algorithms, Particle Swarm Optimization, Simulated Annealing},
abstract = {In forecasting, evolutionary algorithms are often linked to existing forecasting methods to optimize their input parameters. Traditionally, the fitness function of these search heuristics is based on an accuracy measure. In this paper, however, we combine forecasting accuracy with business expertise by defining a flexible and easily interpretable profit function for sales forecasting, which is based on the profit margin of a given product, the volume of its sales and the accuracy of the forecast. ProfARIMA is a new procedure that selects the lags of a Seasonal ARIMA model according to the profit of a model's forecasts by taking advantage of search heuristics. This procedure is tested on both publicly available datasets and a real-life application with datasets of The Coca-Cola Company in order to assess its performance, both in profit and accuracy. Three different evolutionary algorithms were implemented during this testing process, i.e. Genetic Algorithms, Particle Swarm Optimization and Simulated Annealing. The results indicate that ProfARIMA always performs at least equally to the Box–Jenkins methodology and often outperforms this traditional procedure. For The Coca-Cola Company, our new algorithm in combination with Genetic Algorithms even leads to a significantly larger profit for out-of-sample forecasts.}
}
@article{COSTIN2018257,
title = {Building Information Modeling (BIM) for transportation infrastructure – Literature review, applications, challenges, and recommendations},
journal = {Automation in Construction},
volume = {94},
pages = {257-281},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517309470},
author = {Aaron Costin and Alireza Adibfar and Hanjin Hu and Stuart S. Chen},
keywords = {Building Information Modeling (BIM), Bridge Information Modeling (BrIM), Civil Integrated Management (CIM), Civil information Modeling (CiM), Industry Foundation Classes (IFC), Transportation infrastructure, Emerging technologies, Literature review},
abstract = {Transportation infrastructure is a critical component to a nation’s economy, security, and wellbeing. In order to keep up with the rising population, there is a great need for more efficient and cost-effective technologies and techniques to not only repair the infrastructure, but also to advance and expand the transportation infrastructure to sustain the growing population. Building Information Modeling (BIM) has been widely adopted in the building industry, and its established methods and technologies show enormous potential in benefiting the transportation industry. The purpose of this paper is to present a literature review and critical analysis of BIM for transportation infrastructure. A total of 189 publications in the area of BIM for transportation infrastructure were reviewed, including journal articles, conference proceedings, and published reports. Additionally, schemas and file formats from 9 main categories and 34 areas related to transportation infrastructure were reviewed. An application was developed to collect, store, and analyze the publications. Various algorithms were developed and implemented to help in the automation and analysis of the review. The goal of this paper is to provide a comprehensive, up-to-date literature review and critical analysis of research areas regarding BIM for transportation infrastructure to further facilitate research and applications in this domain. Based on the results of the analysis, current topics and trends, applications and uses, emerging technologies, benefits, challenges and limitations, research gaps, and future needs are discussed. Significantly, the contribution of this paper is providing the foundation of current research, gaps, and emerging technologies needed to facilitate further research and applications for both academia and industry stakeholders to develop more efficient and cost-effective techniques necessary to repair, advance, and expand the transportation infrastructure. Furthermore, the results show that the use of BIM for transportation infrastructure has been increasing, although the research has mainly been focusing on roads, highways, and bridges. The results also reveal a major need for a standard neutral exchange format and schema to promote interoperability. Most importantly, the continuing collaboration between academia and industry is required to mitigate most challenges and to realize the full potential of BIM for transportation infrastructure.}
}
@incollection{YANG2017291,
title = {Building Energy Management Systems},
editor = {Martin A. Abraham},
booktitle = {Encyclopedia of Sustainable Technologies},
publisher = {Elsevier},
address = {Oxford},
pages = {291-309},
year = {2017},
isbn = {978-0-12-804792-7},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10199-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012409548910199X},
author = {Tong Yang and Derek Clements-Croome and Matthew Marson},
keywords = {Building automation, Energy management, Interoperability and adaptability, Sustainable design and operation, Technology and system integration},
abstract = {Building energy management systems (BEMS) are integrated building automation and energy management systems, utilizing IT or ICT, intelligent and interoperable digital communication technologies promoting a holistic approach to controls and providing adaptive operational optimization. The system may have multiple levels from individual sensors and actuators to users’ interface, to facilitate data collection, analysis, diagnose, trend finding, and decision-making. BEMS could provide flexible access to the building automation systems from several different platforms and locations. By using service-oriented abstractions to connect building, systems, and people, BEMS dynamically control indoor climate in a cost-effective manner and ensures the comfort, safety, and wellbeing of the occupants in buildings.}
}
@article{HERRERASEMENETS2018272,
title = {A data reduction strategy and its application on scan and backscatter detection using rule-based classifiers},
journal = {Expert Systems with Applications},
volume = {95},
pages = {272-279},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.11.041},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417307972},
author = {Vitali Herrera-Semenets and Osvaldo {Andrés Pérez-García} and Raudel Hernández-León and Jan {van den Berg} and Christian Doerr},
keywords = {Data mining, Data reduction, Intrusion detection, Scan, Backscatter},
abstract = {In the last few years, the telecommunications scenario has experienced an increase in the volume of information generated, as well as in the execution of malicious activities. In order to complement Intrusion Detection Systems (IDSs), data mining techniques have begun to play a fundamental role in data analysis. On the other hand, the presence of useless information and the amount of data generated by telecommunication services (leading to a huge dimensional problem), can affect the performance of traditional IDSs. In this sense, a data preprocessing strategy is necessary to reduce data, but reducing data without affecting the accuracy of IDSs represents a challenge. In this paper, we propose a new data preprocessing strategy which reduces the number of features and instances in the training collection without greatly affecting the achieved accuracy of IDSs. Finally, our proposal is evaluated using four different rule-based classifiers, which are tested on real scan and backscatter data collected by a network telescope.}
}
@article{STURMLINGER2018232,
title = {Development of a wear model of a manufacturing system based on external smart production data on the example of a spring coiling machine},
journal = {Procedia CIRP},
volume = {72},
pages = {232-236},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.260},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304311},
author = {Tobias Stürmlinger and Christoph Haar and Julian Pandtle and Volker Niemeyer},
keywords = {Industry 4.0, data exchange, smart data, manufacturing system engineering, wear model},
abstract = {Industry 4.0 is mostly used to reduce production waste, lead times and costs and to increase flexibility. In this paper, the authors present an approach how to use Industry 4.0 data to support the development of a new generation of a manufacturing system based on production and sensor data acquired at an external, producing company. A wear model of forming tools is developed to support finding profiles for the next machine generation. In particular the production system developer can identify the customers load cases for the dimensioning of their next generation tools. Furthermore, predictive maintenance is supported by the acquired data.}
}
@article{THOMAZ2017786,
title = {Content mining framework in social media: A FIFA world cup 2014 case analysis},
journal = {Information & Management},
volume = {54},
number = {6},
pages = {786-801},
year = {2017},
note = {Smart Tourism: Traveler, Business, and Organizational Perspectives},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2016.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0378720616303354},
author = {Guilherme M. Thomaz and Alexandre A. Biz and Eduardo M. Bettoni and Luiz Mendes-Filho and Dimitrios Buhalis},
keywords = {Social media, Content mining, Twitter, Tourist services, Brazil, FIFA world cup 2014},
abstract = {This paper proposes a social media content mining framework that consists of seven phases. The framework was tested empirically during the FIFA World Cup 2014 at Curitiba (Brazil) as one of the main host city destinations. The research focused on the mining of Twitter content with tourist services ontology (hospitality, food and beverages, and transportation). In total, 58,686 valid messages were collected, analyzed, and associated with an application ontology. Content analysis demonstrated an accurate real-time reflection of tourism services. The framework is effective to collect relevant content and identify popular topics in social media toward strategic and operational tourism management.}
}
@article{LEE2018199,
title = {Social media analytics for enterprises: Typology, methods, and processes},
journal = {Business Horizons},
volume = {61},
number = {2},
pages = {199-210},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S000768131730157X},
author = {In Lee},
keywords = {Social media analytics, Social media, Sentiment analysis, Competitive analysis, Social network analysis, Social media intelligence},
abstract = {This article provides an overview of social media analytics for managers that seek to utilize the practice for social media intelligence. Currently, managers are challenged to analyze an abundance of social media data but lack a framework within which to do so. Toward this end, this article presents a simple typology of social media analytics for enterprises. It also discusses various analytics methods for social media data. Then, this article discusses management processes of social media analytics for enterprises. An illustration of social media analytics is provided with real-world consumer review data. Finally, four challenges are discussed.}
}
@article{GALLINUCCI201886,
title = {Interactive multidimensional modeling of linked data for exploratory OLAP},
journal = {Information Systems},
volume = {77},
pages = {86-104},
year = {2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916305956},
author = {Enrico Gallinucci and Matteo Golfarelli and Stefano Rizzi and Alberto Abelló and Oscar Romero},
keywords = {Multidimensional modeling, Data warehouse design, Linked data, Exploratory OLAP},
abstract = {Exploratory OLAP aims at coupling the precision and detail of corporate data with the information wealth of LOD. While some techniques to create, publish, and query RDF cubes are already available, little has been said about how to contextualize these cubes with situational data in an on-demand fashion. In this paper we describe an approach, called iMOLD, that enables non-technical users to enrich an RDF cube with multidimensional knowledge by discovering aggregation hierarchies in LOD. This is done through a user-guided process that recognizes in the LOD the recurring modeling patterns that express roll-up relationships between RDF concepts, then translates these patterns into aggregation hierarchies to enrich the RDF cube. Two families of aggregation patterns are identified, based on associations and generalization respectively, and the algorithms for recognizing them are described. To evaluate iMOLD in terms of efficiency and effectiveness we compare it with a related approach in the literature, we propose a case study based on DBpedia, and we discuss the results of a test made with real users.}
}
@article{TALAEIKHOEI201822,
title = {Identifying people at risk of developing type 2 diabetes: A comparison of predictive analytics techniques and predictor variables},
journal = {International Journal of Medical Informatics},
volume = {119},
pages = {22-38},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S138650561830399X},
author = {Amir Talaei-Khoei and James M. Wilson},
keywords = {Classification Algorithms, Machine Learning, Diabetes},
abstract = {Background
The present study aims to identify the patients at risk of type 2 diabetes (T2D). There is a body of literature that uses machine learning classification algorithms to predict development of T2D among patients. The current study compares the performance of these classification algorithms to identify patients who are at risk of developing T2D in short, medium and long terms. In addition, the list of predictor variables important for prediction for T2D progression is provided.
Methods
This study uses 10,911 records generated in 36 clinics from the 15th of November 2008–15th of November 2016. Syntactic minority oversampling and random under sampling were used to create a balanced dataset. The performance of Neural Networks, Support Vector Machines, Decision Tress and Logistic Regression to identify patients developing T2D in short, medium and long terms was compared. The measures were Area Under Curve, Sensitivity, Specificity, Matthew correlation coefficient and Mean Calibration Error. Through importance analysis and information fusion techniques the predictors of developing T2D were identified for short, medium and long-term risk analysis.
Results
The findings show that the performance of analytics techniques depends on both period and purpose of prediction whether the prediction is to identify people who will not develop T2D or to determine at risk patients. Oversampling as opposed to under sampling improved performance. 16 predictors and their importance to determine patients at risk of T2D in short, medium and long terms were identified.
Conclusions
This study provides guidelines for an automated system to prompt patients for screening. Several predictors are reportable by patients, others can be examined by physicians or ordered for further lab examination, which offers a potential reduction of the burden placed upon the clinical settings.}
}
@article{JACOB2017260,
title = {Mind the Gap: Analyzing the Impact of Data Gap in Millennium Development Goals’ (MDGs) Indicators on the Progress toward MDGs},
journal = {World Development},
volume = {93},
pages = {260-278},
year = {2017},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2016.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X15310433},
author = {Arun Jacob},
keywords = {MDG, SDGs, value of data, performance measurement, monitoring and evaluation, goal setting},
abstract = {Summary
This paper analyzes the impact of data gap in Millennium Development Goals’ (MDGs) performance indicators on actual performance success of MDGs. Performance success, within the MDG framework, is quantified using six different ways proposed in the existing literature, including both absolute and relative performance and deviation from historical transition paths of MDG indicators. The empirical analysis clearly shows that the data gap in performance measurement is a significant predictor of poor MDG performance in terms of any of the six progress measures. Larger the data gap or weaker the performance measurement system, lesser is the probability of MDG performance success. The empirical methodology used in the paper combines a Heckman correction and instrumental variable estimation strategies to simultaneously account for potential endogeneity of the key data gap variable and bias due to sample selection. This result holds true even after controlling for overall national statistical capacity and a variety of socioeconomic factors. The paper underlines the need to strengthen the performance measurement system attached to the 2030 agenda for sustainable development and the associated Sustainable Development Goals (SDGs). This paper is the first attempt at empirically evaluating the value of data in the context of international development goals and gives empirical evidence for the need to harness the “data revolution” for sustainable development.}
}
@article{JIN201798,
title = {Evaluating cities' vitality and identifying ghost cities in China with emerging geographical data},
journal = {Cities},
volume = {63},
pages = {98-109},
year = {2017},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2017.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S026427511630261X},
author = {Xiaobin Jin and Ying Long and Wei Sun and Yuying Lu and Xuhong Yang and Jingxian Tang},
keywords = {Residential development, Road junction, Points of interest, Social network, Urban vitality},
abstract = {With the rapid urbanization of China, plenty of new urban lands have been developed with the great expectation to deal with all kinds of issues in old urban areas such as high population density, great demand on limited land resources, and decaying environment. However, a great proportion of vacancy in these newly developed units leads to the undesired observation of ghost cities. Lacking of clear and effectively evaluation criterion, the understanding of ghost cities in China is then rather limited. Considering the fact of ghost cities, we borrow the theory of urban vitality to identify and evaluate ghost cities in this paper. We argue that ghost cities are associated with very low urban vitality. In the light of big/open data, we are able to profile ghost cities of China based on 535,523 recent project-level residential developments from 2002 to 2013. We use the national-wide and million magnitude road junctions, points of interest and location based service records of 2014/2015 for measuring the morphological, functional and social vitality of each residential project. We then aggregate the project level evaluation results into the city level and thirty ghost cities are then identified by comparing the residential projects' vitality in the old (developed before or in 2000) and new (developed after 2000) urban areas in each city. Our profiling results illustrate the big picture of China's past residential developments, and then of ghost cities. We find the average vitality of residential projects in new urban areas is only 8.8% of that in old urban areas, denoting the potential existence of ghost cities in newly developed areas in Chinese cities. We have also benchmarked our identified ghost cities with existing rankings, the Baidu searching engine and night-time light images. Although we admit that ghost cities may exist in the particular urbanizing phase of China and that some ghost cities now may be well developed in the future, this study provides a thorough evaluation on the ghost city condition in China. This may shed light on policy implications for Chinese urban development.}
}
@article{COLAKOVIC201817,
title = {Internet of Things (IoT): A review of enabling technologies, challenges, and open research issues},
journal = {Computer Networks},
volume = {144},
pages = {17-39},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618305243},
author = {Alem Čolaković and Mesud Hadžialić},
keywords = {IoT (Internet of Things), IoT vision, IoT features, IoT enabling technologies, Open issues and challenges, Future research direction},
abstract = {IoT (Internet of Things) is a new paradigm which provides a set of new services for the next wave of technological innovations. IoT applications are nearly limitless while enabling seamless integration of the cyber-world with the physical world. However, despite the enormous efforts of standardization bodies, alliances, industries, researchers and others, there are still numerous problems to deal with in order to reach the full potential of IoT. These issues should be considered from various aspects such as enabling technologies, applications, business models, social and environmental impacts. In focus of this paper are open issues and challenges considered from the technological perspective. Just for clarification, we put in light different visions that stand behind this paradigm in order to facilitate a better understanding of the IoT's features. Furthermore, this exhaustive survey provides insights into the state-of-the-art of IoT enabling and emerging technologies. The most relevant among them are addressed with some details. The main scope is to deliver a comprehensive overview of open issues and challenges to be tackled by future research. We provide some insights into specific emerging ideas in order to facilitate future research. Also, this paper brings order in the existing literature by classifying contributions according to different research topics.}
}
@article{AMANI201732,
title = {Data mining applications in accounting: A review of the literature and organizing framework},
journal = {International Journal of Accounting Information Systems},
volume = {24},
pages = {32-58},
year = {2017},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1467089515300488},
author = {Farzaneh A. Amani and Adam M. Fadlalla},
keywords = {Data mining, Accounting, Literature review, Framework, Prospective, Retrospective},
abstract = {This paper explores the applications of data mining techniques in accounting and proposes an organizing framework for these applications. A large body of literature reported on specific uses of the important data mining paradigm in accounting, but research that takes a holistic view of these uses is lacking. To organize the literature on the applications of data mining in accounting, we create a framework that combines the two well-known accounting reporting perspectives (retrospection and prospection), and the three well-accepted goals of data mining (description, prediction, and prescription). The framework encapsulates a taxonomy of four categories (retrospective-descriptive, retrospective-prescriptive, prospective-prescriptive, and prospective-predictive) of data mining applications in accounting. The proposed framework revealed that the area of accounting that benefited the most from data mining is assurance and compliance, including fraud detection, business health and forensic accounting. The clear gaps seem to be in the two prescriptive application categories (retrospective-prescriptive and prospective-prescriptive), indicating opportunities for benefiting from data mining in these application categories. The framework presents a holistic view of the literature and systematically organizes it in a structurally logical and thematically coherent manner.}
}
@incollection{CORNEA20173,
title = {Chapter 1 - Cloud Services for Smart City Applications},
editor = {Mauro Migliardi and Alessio Merlo and Sherenaz {Al-Haj Baddar}},
booktitle = {Adaptive Mobile Computing},
publisher = {Academic Press},
address = {Boston},
pages = {3-28},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-804603-6},
doi = {https://doi.org/10.1016/B978-0-12-804603-6.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046036000012},
author = {Tudor Cornea and Catalin Gosman and Raluca Constanda and Ciprian Nuţescu and Ciprian Dobre},
keywords = {Intelligent transportation systems, Cloud services, Smart City},
abstract = {Intelligent transportation systems (ITS) are receiving increasing attention lately, due to the benefits that wireless devices, combined with sensing technologies and ICT smart services, bring. We present the MobiWay project, leading to the development of a collaborative platform designed to support ITS applications by acting as a middleware connection hub. The chapter presents both the theoretical model being proposed by MobiWay, and its implementation for aggregating traffic data from large sets of users. We propose a scalable platform that is capable of storing and processing a large number of user supplied data.}
}
@article{MENG201851,
title = {Conducting highly principled data science: A statistician’s job and joy},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {51-57},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.053},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300981},
author = {Xiao-Li Meng},
keywords = {Astrostatistics, Computational efficiency, Principled corner cutting, Scientific justification},
abstract = {Highly Principled Data Science insists on methodologies that are: (1) scientifically justified; (2) statistically principled; and (3) computationally efficient. An astrostatistics collaboration, together with some reminiscences, illustrates the increased roles statisticians can and should play to ensure this trio, and to advance the science of data along the way.}
}
@article{SOUZA20181,
title = {Towards a proper service placement in combined Fog-to-Cloud (F2C) architectures},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {1-15},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17323051},
author = {V.B. Souza and X. Masip-Bruin and E. Marín-Tordera and S. Sànchez-López and J. Garcia and G.J. Ren and A. Jukan and A. {Juan Ferrer}},
keywords = {Service placement and execution, Resource allocation, Cloud & fog computing, Distributed systems, Quality of service},
abstract = {The Internet of Things (IoT) has empowered the development of a plethora of new services, fueled by the deployment of devices located at the edge, providing multiple capabilities in terms of connectivity as well as in data collection and processing. With the inception of the Fog Computing paradigm, aimed at diminishing the distance between edge-devices and the IT premises running IoT services, the perceived service latency and even the security risks can be reduced, while simultaneously optimizing the network usage. When put together, Fog and Cloud computing (recently coined as fog-to-cloud, F2C) can be used to maximize the advantages of future computer systems, with the whole greater than the sum of individual parts. However, the specifics associated with cloud and fog resource models require new strategies to manage the mapping of novel IoT services into the suitable resources. Despite few proposals for service offloading between fog and cloud systems are slowly gaining momentum in the research community, many issues in service placement, both when the service is ready to be executed admitted as well as when the service is offloaded from Cloud to Fog, and vice-versa, are new and largely unsolved. In this paper, we provide some insights into the relevant features about service placement in F2C scenarios, highlighting main challenges in current systems towards the deployment of the next-generation IoT services.}
}
@article{ALWOSHEEL2018167,
title = {Is your dataset big enough? Sample size requirements when using artificial neural networks for discrete choice analysis},
journal = {Journal of Choice Modelling},
volume = {28},
pages = {167-182},
year = {2018},
issn = {1755-5345},
doi = {https://doi.org/10.1016/j.jocm.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1755534518300058},
author = {Ahmad Alwosheel and Sander {van Cranenburgh} and Caspar G. Chorus},
abstract = {Artificial Neural Networks (ANNs) are increasingly used for discrete choice analysis. But, at present, it is unknown what sample size requirements are appropriate when using ANNs in this particular context. This paper fills this knowledge gap: we empirically establish a rule-of-thumb for ANN-based discrete choice analysis based on analyses of synthetic and real data. To investigate the effect of complexity of the data generating process on the minimum required sample size, we conduct extensive Monte Carlo analyses using a series of different model specifications with different levels of model complexity, including RUM and RRM models, with and without random taste parameters. Based on our analyses we advise to use a minimum sample size of fifty times the number of weights in the ANN; it should be noted, that the number of weights is generally much larger than the number of parameters in a discrete choice model. This rule-of-thumb is considerably more conservative than the rule-of-thumb that is most often used in the ANN community, which advises to use at least ten times the number of weights.}
}
@article{BANERJEE201811,
title = {Radiology report annotation using intelligent word embeddings: Applied to multi-institutional chest CT cohort},
journal = {Journal of Biomedical Informatics},
volume = {77},
pages = {11-20},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417302575},
author = {Imon Banerjee and Matthew C. Chen and Matthew P. Lungren and Daniel L. Rubin},
keywords = {Information extraction, Word embedding, Pulmonary embolism, Report annotation},
abstract = {We proposed an unsupervised hybrid method – Intelligent Word Embedding (IWE) that combines neural embedding method with a semantic dictionary mapping technique for creating a dense vector representation of unstructured radiology reports. We applied IWE to generate embedding of chest CT radiology reports from two healthcare organizations and utilized the vector representations to semi-automate report categorization based on clinically relevant categorization related to the diagnosis of pulmonary embolism (PE). We benchmark the performance against a state-of-the-art rule-based tool, PeFinder and out-of-the-box word2vec. On the Stanford test set, the IWE model achieved average F1 score 0.97, whereas the PeFinder scored 0.9 and the original word2vec scored 0.94. On UPMC dataset, the IWE model’s average F1 score was 0.94, when the PeFinder scored 0.92 and word2vec scored 0.85. The IWE model had lowest generalization error with highest F1 scores. Of particular interest, the IWE model (trained on the Stanford dataset) outperformed PeFinder on the UPMC dataset which was used originally to tailor the PeFinder model.}
}
@article{MIKALEF20171,
title = {Information technology-enabled dynamic capabilities and their indirect effect on competitive performance: Findings from PLS-SEM and fsQCA},
journal = {Journal of Business Research},
volume = {70},
pages = {1-16},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316305690},
author = {Patrick Mikalef and Adamantia Pateli},
keywords = {Fuzzy-set qualitative comparative analysis, Competitive advantage, Dynamic capabilities, Organizational agility, Empirical study, Environmental uncertainty},
abstract = {A central question for researchers and practitioners is whether and how IT (information technology) can help build a competitive advantage in uncertain environments. To address this question, the present study seeks to empirically explore the relationship between IT-enabled dynamic capabilities and competitive performance. By drawing upon recent thinking in the strategy and IT management literatures, this paper argues that the impact of IT-enabled dynamic capabilities on competitive performance is mediated by organizational agility. Using survey data from 274 international firms and by applying structural equation modelling (SEM), outcomes suggest that IT-enabled dynamic capabilities facilitate two types of agility, market capitalizing and operational adjustment agility, which in sequence enhance competitive performance. The confluence of environmental factors is examined by fuzzy-set qualitative comparative analysis (fsQCA). The results of fsQCA reinforce and refine findings of the PLS analysis concerning the limits and conditions to which IT-enabled dynamic capabilities add value.}
}
@article{LONG2018281,
title = {Understanding uneven urban expansion with natural cities using open data},
journal = {Landscape and Urban Planning},
volume = {177},
pages = {281-293},
year = {2018},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2017.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0169204617301123},
author = {Ying Long and Weixin Zhai and Yao Shen and Xinyue Ye},
keywords = {Urban expansion, Social media, Head/tail division, New data, Open data, China},
abstract = {The last several decades have witnessed a rapid yet uneven urban expansion in developing countries. The existing studies rely heavily on official statistical yearbooks and remote sensing images. However, the former data sources have been criticized due to its non-objectivity and low quality, while the latter is labor and cost consuming in most cases. Recent efforts made by fractal analyses provide alternatives to scrutinize the corresponding “natural urban area”. In our proposed framework, the dynamics of internal urban contexts is reflected in a quasi-real-time manner using emerging new data and the expansion is a fractal concept instead of an absolute one based on the conventional Euclidean method. We then evaluate the magnitude and pattern of natural cities and their expansion in size and space. It turns out that the spatial expansion rate of official cities (OCs) in our study area China has been largely underestimated when compared with the results of natural cities (NCs). The perspective of NCs also provides a novel way to understanding the quality of uneven urban expansion. We detail our analysis for the 23 urban agglomerations in China, especially paying more attention to the three most dominating urban agglomerations of China: Beijing-Tianjin-Hebei (BTH), Yangtze River Delta (YRD) and Pearl River Delta (PRD). The findings from the OC method are not consistent with the NC method. The distinctions may arise from the definition of a city, and the bottom-up NC method contributes to our comprehensive understanding of uneven urban expansion.}
}
@article{LEE2018678,
title = {Manufacturer’s printing forecast, reprinting decision, and contract design in the educational publishing industry},
journal = {Computers & Industrial Engineering},
volume = {125},
pages = {678-687},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.05.049},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218302584},
author = {Chia-Yen Lee and Chia-Lung Liang},
keywords = {Demand forecasting, Capacity planning, Inventory management, Contract design, Educational publishing industry},
abstract = {An educational publishing industry generally builds lots of inventory for make-to-stock production; however, the frequent revision causes the obsolescence problem. This study proposes two models to address different but related problems, inventory scrap and contract design. The industry uses prediction modeling to forecast demand and manage inventory of diverse print products. The printing decision module is developed to improve the accuracy of the demand forecast and reduce the inventory scrap problem. In addition, there exists asymmetric information in a two-echelon supply chain and contract design favors educational publishing retailers; thus the profitability of the overall supply chain is not maximized and manufacturers’ profits are limited. This study suggests a contract design module to encourage retailers to provide true information for improving profitability in the overall supply chain. An empirical study of Taiwan’s leading educational publisher validates the proposed models. The results show that the proposed printing decision model improved forecast accuracy by 3.7%, reduced cost by 8.3%, and the contract design enhanced overall supply chain and manufacturer profitability by 0.5% and 2.7%, respectively.}
}
@article{KHARLAMOV201754,
title = {Semantic access to streaming and static data at Siemens},
journal = {Journal of Web Semantics},
volume = {44},
pages = {54-74},
year = {2017},
note = {Industry and In-use Applications of Semantic Technologies},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300124},
author = {Evgeny Kharlamov and Theofilos Mailis and Gulnar Mehdi and Christian Neuenstadt and Özgür Özçep and Mikhail Roshchin and Nina Solomakhina and Ahmet Soylu and Christoforos Svingos and Sebastian Brandt and Martin Giese and Yannis Ioannidis and Steffen Lamparter and Ralf Möller and Yannis Kotidis and Arild Waaler},
keywords = {Ontology based data access, Data integration, Streaming data, Static data, Predictive and reactive analytics, Optimisations, Siemens},
abstract = {We present a description and analysis of the data access challenge in Siemens Energy. We advocate Ontology Based Data Access (OBDA) as a suitable Semantic Web driven technology to address the challenge. We derive requirements for applying OBDA in Siemens, review existing OBDA systems and discuss their limitations with respect to the Siemens requirements. We then introduce the Optique platform as a suitable OBDA solution for Siemens. The platform is based on a number of novel techniques and components including a deployment module, BootOX for ontology and mapping bootstrapping, a query language STARQL that allows for a uniform querying of both streaming and static data, a highly optimised backend, ExaStream, for processing such data, and a query formulation interface, OptiqueVQS, that allows to formulate STARQL queries without prior knowledge of its formal syntax. Finally, we describe our installation and evaluation of the platform in Siemens.}
}
@article{BAI201766,
title = {Toward a systematic exploration of nano-bio interactions},
journal = {Toxicology and Applied Pharmacology},
volume = {323},
pages = {66-73},
year = {2017},
issn = {0041-008X},
doi = {https://doi.org/10.1016/j.taap.2017.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0041008X17301151},
author = {Xue Bai and Fang Liu and Yin Liu and Cong Li and Shenqing Wang and Hongyu Zhou and Wenyi Wang and Hao Zhu and David A. Winkler and Bing Yan},
abstract = {Many studies of nanomaterials make non-systematic alterations of nanoparticle physicochemical properties. Given the immense size of the property space for nanomaterials, such approaches are not very useful in elucidating fundamental relationships between inherent physicochemical properties of these materials and their interactions with, and effects on, biological systems. Data driven artificial intelligence methods such as machine learning algorithms have proven highly effective in generating models with good predictivity and some degree of interpretability. They can provide a viable method of reducing or eliminating animal testing. However, careful experimental design with the modelling of the results in mind is a proven and efficient way of exploring large materials spaces. This approach, coupled with high speed automated experimental synthesis and characterization technologies now appearing, is the fastest route to developing models that regulatory bodies may find useful. We advocate greatly increased focus on systematic modification of physicochemical properties of nanoparticles combined with comprehensive biological evaluation and computational analysis. This is essential to obtain better mechanistic understanding of nano-bio interactions, and to derive quantitatively predictive and robust models for the properties of nanomaterials that have useful domains of applicability.}
}
@article{BRUNO2018256,
title = {Historic Building Information Modelling: performance assessment for diagnosis-aided information modelling and management},
journal = {Automation in Construction},
volume = {86},
pages = {256-276},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517301164},
author = {Silvana Bruno and Mariella {De Fino} and Fabio Fatiguso},
keywords = {BIM (Building Information Modelling), Historic building, HBIM (Historic Building Information Modelling), FM (Facility Management), Refurbishment, Energy retrofitting, Structural assessment, Diagnostics, Diagnosis-aided HBIMM (DA-HBIMM)},
abstract = {Building Information Modelling, new paradigm of digital design and management, shows great potential for the refurbishment process, as it represents a possible way out of criticalities that occur in documentation and preservation of existing assets, if connected to cognitive automation. The combination of BIM with automation systems improves the quality control during diagnosis, design and work execution, and the labour savings, which is particularly relevant for rapid intervention in case of hazardous conditions. Therefore, the paper is going to address a methodological discussion concerning complete “as-built” parametric models of historical buildings, supporting the design of refurbishment and conservation interventions. Although some reviews of the state of the art exist on the topic of Historic Building Information Modelling, the present research introduces a different perspective on HBIM modelling, with diagnosis and performance assessment as key-aspects, in terms of automating performance assessment. Specifically, from the data collection of contributions regarding HBIM/BIM, diagnostics and monitoring on existing buildings and infrastructures, a critical review by selected criteria is developed. Nevertheless, general methods and tools for information management and exchange tasks in BIM are briefly described as well, since they are considered useful for future developments of HBIM approach. The core of the critical analysis is focused on the scientific and technical relations among HBIM models, diagnosis and performance assessment features. In addition, the review identifies specific activities and relative tools and methods for knowledge acquisition and semantic enrichment. Finally, gaps in knowledge of the current literature are outlined and discussed, with specific focus on performance assessment in HBIM. In this regard, a new methodology toward Diagnosis-Aided Historic Building Information Modelling and Management (DA-HBIMM) is proposed as a framework to be developed in order to address smart knowledge acquisition, collection and notification of assessed performances and eventual risks, by cognitive automation and artificial intelligence, in the near future.}
}
@article{RICHARD2018383,
title = {French household travel survey: The next generation},
journal = {Transportation Research Procedia},
volume = {32},
pages = {383-393},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.065},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518302199},
author = {Olivier Richard and Mathieu Rabaud},
keywords = {Household travel survey, transport data collection, smartphone, GPS, Web survey},
abstract = {The French household travel survey (HTS) methodology “Cerema standard” is currently the standard data source for local mobility knowledge in France. However, the major changes in the context led Cerema to begin in 2015 a review of the current methodology and of new data sources which could replace or complete it. This article describes the evaluating process of new survey techniques (Web, GPS) and data sources (mobile phone, smartphone) and its results. For next generation of HTS in the French context, Cerema proposes a modular mechanism with a core and standardized options as a new framework for observing local mobility.}
}
@article{PATRICIAN2017S120,
title = {Twenty years of staffing, practice environment, and outcomes research in military nursing},
journal = {Nursing Outlook},
volume = {65},
number = {5, Supplement },
pages = {S120-S129},
year = {2017},
note = {Supplement Issue: Military Nursing Research},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2017.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0029655417301847},
author = {Patricia A. Patrician and Lori A. Loan and Mary S. McCarthy and Pauline Swiger and Sara Breckenridge-Sproat and Laura Ruse Brosch and Bonnie Mowinski Jennings},
keywords = {Nursing sensitive indicators, Outcomes, Practice environment, Staffing},
abstract = {Background
Two decades ago, findings from an Institute of Medicine (IOM) report sparked the urgent need for evidence supporting relationships between nurse staffing and patient outcomes.
Purpose
This article provides an overview of nurse staffing, practice environment, and patient outcomes research, with an emphasis on findings from military studies. Lessons learned also are enumerated.
Method
This study is a review of the entire Military Nursing Outcomes Database (MilNOD) program of research.
Discussion
The MilNOD, in combination with evidence from other health care studies, provides nurses and leaders with information about the associations between staffing, patient outcomes, and the professional practice environment of nursing in the military. Leaders, therefore, have useful empirical evidence to make data-driven decisions. The MilNOD studies are the basis for the current Army nursing dashboard, and care delivery framework, called the Patent CaringTouch System.
Conclusion
Future research is needed to identify ideal staffing based on workload demands, and provide leaders with factors to consider when operationalizing staffing recommendations.}
}
@article{STALLABOURDILLON2018784,
title = {The GDPR: A game changer for electronic identification schemes? The case study of Gov.UK Verify},
journal = {Computer Law & Security Review},
volume = {34},
number = {4},
pages = {784-805},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0267364918301997},
author = {Sophie Stalla-Bourdillon and Henry Pearce and Niko Tsakalakis},
keywords = {Data protection, Electronic identification, GDPR, Gov.UK Verify, Joint controllership, Legal bases},
abstract = {This article offers an interdisciplinary analysis of the General Data Protection Regulation (GDPR) in the context of electronic identification schemes. Gov.UK Verify, the UK Government's electronic identification scheme, and its compatibility with some important aspects of EU data protection law are reviewed. An in-depth examination of Gov.UK Verify's architecture and the most significant constituent elements of both the Data Protection Directive and the imminent GDPR – notably the legitimising grounds for the processing of personal data and the doctrine of joint controllership – highlight several flaws inherent in the Gov.UK Verify's development and mode of operation. This article advances the argument that Gov.UK Verify is incompatible with some major substantive provisions of the EU Data Protection Framework. It also provides some general insight as to how to interpret the requirement of a legitimate legal basis and the doctrine of joint controllership. It ultimately suggests that the choice of the appropriate legal basis should depend upon a holistic approach to the relationship between the actors involved in the processing activities.}
}
@article{SUN201854,
title = {Local spatial obesity analysis and estimation using online social network sensors},
journal = {Journal of Biomedical Informatics},
volume = {83},
pages = {54-62},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300522},
author = {Qindong Sun and Nan Wang and Shancang Li and Hongyi Zhou},
keywords = {Online social networks, Internet of things (IoT), Obesity, Health informatics, Bioinformatics, Public health},
abstract = {Recently, the online social networks (OSNs) have received considerable attentions as a revolutionary platform to offer users massive social interaction among users that enables users to be more involved in their own healthcare. The OSNs have also promoted increasing interests in the generation of analytical, data models in health informatics. This paper aims at developing an obesity identification, analysis, and estimation model, in which each individual user is regarded as an online social network ‘sensor’ that can provide valuable health information. The OSN-based obesity analytic model requires each sensor node in an OSN to provide associated features, including dietary habit, physical activity, integral/incidental emotions, and self-consciousness. Based on the detailed measurements on the correlation of obesity and proposed features, the OSN obesity analytic model is able to estimate the obesity rate in certain urban areas and the experimental results demonstrate a high success estimation rate. The measurements and estimation experimental findings created by the proposed obesity analytic model show that the online social networks could be used in analyzing the local spatial obesity problems effectively.}
}
@article{JANUSZ201783,
title = {Predicting seismic events in coal mines based on underground sensor measurements},
journal = {Engineering Applications of Artificial Intelligence},
volume = {64},
pages = {83-94},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2017.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0952197617301215},
author = {Andrzej Janusz and Marek Grzegorowski and Marcin Michalak and Łukasz Wróbel and Marek Sikora and Dominik Ślęzak},
keywords = {Decision support systems, Time series data, Seismic events prediction, Cold-start problem, Predictive analytics, Feature engineering},
abstract = {In this paper, we address the problem of safety monitoring in underground coal mines. In particular, we investigate and compare practical methods for the assessment of seismic hazards using analytical models constructed based on sensory data and domain knowledge. For our case study, we use a rich data set collected during a period of over five years from several active Polish coal mines. We focus on comparing the prediction quality between expert methods which serve as a standard in the coal mining industry and state-of-the-art machine learning methods for mining high-dimensional time series data. We describe an international data mining challenge organized to facilitate our study. We also demonstrate a technique which we employed to construct an ensemble of regression models able to outperform other approaches used by participants of the challenge. Finally, we explain how we utilized the data obtained during the competition for the purpose of research on the cold start problem in deploying decision support systems at new mining sites.}
}
@incollection{KUMUTHINI2018179,
title = {Chapter 9 - Minimum Information Required for Pharmacogenomics Experiments},
editor = {Christophe G. Lambert and Darrol J. Baker and George P. Patrinos},
booktitle = {Human Genome Informatics},
publisher = {Academic Press},
pages = {179-193},
year = {2018},
isbn = {978-0-12-809414-3},
doi = {https://doi.org/10.1016/B978-0-12-809414-3.00009-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094143000097},
author = {J. Kumuthini and L. Zass and Emile R. Chimusa and Melek Chaouch and Collen Masimiremwa},
keywords = {Bioinformatics, Bioinformatics standardization, Data standardization, DMET, FAIR principles, MIDE, Minimum requirement reporting, Pharmacogenomics, Pharmacogenetics, Personalized medicine},
abstract = {Pharmacogenomics studies the impact of genetic variation on drug response. The discipline is crucial in order to improve healthcare worldwide and an important stepping stone toward a future of personalized medicine. Pharmacogenomics generally involves single- to multigene variation investigations, and, although a great deal of progress has been made in the last decades, significant barriers prevent the utilization of pharmacogenomics to its full potential. These barriers include both financial and technical/analytical considerations, which are further complicated by the varied reporting methods employed by pharmacogenomics researchers and clinicians. Therefore, standardizing the manner in which pharmacogenomics investigations are reported can significantly contribute to resolving the existing analytical concerns, and thus, the Minimum Information required for a DMET experiment (MIDE) pharmacogenomics was designed to challenge the reporting variance in experiments employing microarray technology, specifically with regard to pharmacogenomics. The following chapter provides a brief overview of pharmacogenomics research, standardization within the field, and an in-depth look into the MIDE pharmacogenomics standard. Notably, the MIDE standard is adaptable to other pharmacogenomics applications and is available along with several other pharmacogenomics standards on the FAIRsharing (Findable, Accessible, Interoperable, Reproducible Sharing) resource. Standardization across pharmacogenomics applications is crucial in order to enhance the findability, accessibility, interoperability, and reusability of pharmacogenomics studies, ultimately enhancing the quality of such studies and promoting collaboration between independent bodies involved in the science.}
}
@article{FRANK2018235,
title = {LSane: Collaborative Validation and Enrichment of Heterogeneous Observation Streams},
journal = {Procedia Computer Science},
volume = {137},
pages = {235-241},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316272},
author = {Matthias T. Frank and Sebastian Bader and Viliam Simko and Stefan Zander},
keywords = {Semantic Sensors, Semantic Stream Processing, Collaborative Shape Constraints, Collaborative Reasoning},
abstract = {The increasing amount of publicly available data streams of environmental observation stations opens up new opportunities: domain experts are provided with an extensive amount of observations covering large areas with high density of environmental sensors, which could hardly ever be provided by a single organization. However, these opportunities come at the cost of new challenges regarding trustworthiness and comparability of such observations. In this paper, we address the challenges of semantic validation and enrichment of heterogeneous observation streams by exploiting collaboratively created and curated annotations. For this purpose, we introduce and discuss the Linked Stream Annotation Engine (LSane) to validate observation messages from heterogeneous sensors. We enrich these observation messages with provenance information derived from annotations. We present an implementation of LSane with messages from public and private environmental observation stations, which are mapped to explicit semantics, and validate and enrich the mapped messages based on annotations from the LSane collaboration platform.}
}
@article{MEJRI201746,
title = {Crisis information to support spatial planning in post disaster recovery},
journal = {International Journal of Disaster Risk Reduction},
volume = {22},
pages = {46-61},
year = {2017},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2017.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212420916303144},
author = {Ouejdane Mejri and Scira Menoni and Kyla Matias and Negar Aminoltaheri},
keywords = {Crisis information, Spatial planning, Resilience, Knowledge management, Ontologies, Crowdsourcing},
abstract = {In this paper we propose to explore the complex node of post disaster reconstruction, knowledge and data necessary to support spatial planning, and new information technologies. The methodology that is illustrated assumes that post-event damage assessments are useful to verify to what extent hazard and risk assessments that were available to planners to make decisions before the disaster were correct and if they were actually used as a basis for locational and zoning choices. Our contribution is aimed at the creation and design of knowledge bases accounting for the dynamic evolution of disasters. New web based technologies provide the opportunity to collect and analyse dynamic territorial crisis data using crowdsourcing and crowdmapping platforms. The proposed methodology permits to sort and classify a very large set of different types of data generated through the web. Semantic conceptualization using ontologies is performed to identify and select the information produced during the emergency that can support spatial planning in the post disaster reconstruction. The city of Tacloban in the Philippines, affected by the Super Typhoon Haiyan in November 2013 constitutes the test case for applying the methodology that has been developed.}
}
@incollection{MACAULAY2017279,
title = {Chapter 13 - RIoT Control},
editor = {Tyson Macaulay},
booktitle = {RIoT Control},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {279-368},
year = {2017},
isbn = {978-0-12-419971-2},
doi = {https://doi.org/10.1016/B978-0-12-419971-2.00013-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124199712000133},
author = {Tyson Macaulay},
keywords = {Internet of Things (IoT), risk in the Internet of Things (RIoT), security, risk management, enterprise risk management, smart cities, automotive security},
abstract = {This chapter is not about all risks in the Internet of Things (IoT). Rather, it focuses on just what is different about risk management in the IoT, and what has changed in the IoT.}
}
@article{STINGONE2017730,
title = {Using machine learning to identify air pollution exposure profiles associated with early cognitive skills among U.S. children},
journal = {Environmental Pollution},
volume = {230},
pages = {730-740},
year = {2017},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2017.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0269749117313027},
author = {Jeanette A. Stingone and Om P. Pandey and Luz Claudio and Gaurav Pandey},
keywords = {Multiple exposures, Mixtures, Machine learning, Neurodevelopment},
abstract = {Data-driven machine learning methods present an opportunity to simultaneously assess the impact of multiple air pollutants on health outcomes. The goal of this study was to apply a two-stage, data-driven approach to identify associations between air pollutant exposure profiles and children's cognitive skills. Data from 6900 children enrolled in the Early Childhood Longitudinal Study, Birth Cohort, a national study of children born in 2001 and followed through kindergarten, were linked to estimated concentrations of 104 ambient air toxics in the 2002 National Air Toxics Assessment using ZIP code of residence at age 9 months. In the first-stage, 100 regression trees were learned to identify ambient air pollutant exposure profiles most closely associated with scores on a standardized mathematics test administered to children in kindergarten. In the second-stage, the exposure profiles frequently predicting lower math scores were included within linear regression models and adjusted for confounders in order to estimate the magnitude of their effect on math scores. This approach was applied to the full population, and then to the populations living in urban and highly-populated urban areas. Our first-stage results in the full population suggested children with low trichloroethylene exposure had significantly lower math scores. This association was not observed for children living in urban communities, suggesting that confounding related to urbanicity needs to be considered within the first-stage. When restricting our analysis to populations living in urban and highly-populated urban areas, high isophorone levels were found to predict lower math scores. Within adjusted regression models of children in highly-populated urban areas, the estimated effect of higher isophorone exposure on math scores was −1.19 points (95% CI −1.94, −0.44). Similar results were observed for the overall population of urban children. This data-driven, two-stage approach can be applied to other populations, exposures and outcomes to generate hypotheses within high-dimensional exposure data.}
}
@incollection{GROOT201765,
title = {Chapter 3 - Information as the Fuel for Financial Services’ Business Processes},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {65-106},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809776200003X},
author = {Martijn Groot},
keywords = {financial services processes, business data architecture, trade life cycle, regulatory reporting},
abstract = {This chapter summarizes the primary (trading, investing, lending, insuring, payment services, distributing, intermediary role) and secondary (clearing and settlement, custody, fund administration, regulatory) business processes in financial services from an information management perspective. We specifically zoom in on the instrument and transaction life cycle and the specific information management challenges in the various stages of that life cycle. We also address the changing requirements from a customer interaction and regulatory reporting perspective and discuss the notion of business data architecture as a bridge between business requirements and information technology.}
}
@article{HAVERMANS201796,
title = {Forecasting European trade mark and design filings: An innovative approach including exogenous variables and IP offices' events},
journal = {World Patent Information},
volume = {48},
pages = {96-108},
year = {2017},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S017221901730008X},
author = {Quirinus A. Havermans and Samuel Gabaly and Antonio Hidalgo},
keywords = {Trade mark filings, Design filings, Artificial intelligent forecasting techniques, Advanced data analysis, Exogenous variables},
abstract = {Both national and international Intellectual Property (IP) offices need to adopt and use more reliable and efficient forecasting systems to improve their strategic planning and budgetary outlook. The European Union Intellectual Property Office (EUIPO), through the European Observatory on Infringements of Intellectual Property Rights, has conducted a research to evaluate the forecasting methodologies currently used at IP offices. Novel forecasting approaches for trade marks and designs have also been analysed. This paper discusses the classic forecasting techniques and shows the improved results that are obtained when innovative techniques based on artificial intelligence with the inclusion of exogenous variables are used.}
}
@article{HENRIQUES2018638,
title = {Predictive Modelling: Flight Delays and Associated Factors, Hartsfield–Jackson Atlanta International Airport},
journal = {Procedia Computer Science},
volume = {138},
pages = {638-645},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.085},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918317319},
author = {Roberto Henriques and Inês Feiteira},
keywords = {Data Mining, Predictive Analysis, Flight Delays, Hartsfield–Jackson International Airport, Atlanta International Airport},
abstract = {Nowadays, a downside to traveling is the delays that are constantly being advertised to passengers resulting in a decrease in customer satisfaction and causing costs. Consequently, there is a need to anticipate and mitigate the existence of delays helping airlines and airports improving their performance or even take consumer-oriented measures that can undo or attenuate the effect that these delays have on their passengers. This study has as main objective to predict the occurrence of delays in arrivals at the international airport of Hartsfield-Jackson. A Knowledge Discovery Database (KDD) methodology was followed, and several Data Mining techniques were applied. Historical data of the flight and weather, information of the airplane and propagation of the delay were gathered to train the model. To overcome the problem of unbalanced datasets, we applied different sampling techniques. To predict delays in individual flights we used Decision Trees, Random Forest and Multilayer Perceptron. Finally, each model’s performance was evaluated and compared. The best model proved to be the Multilayer Perceptron with 85% of accuracy.}
}
@article{SMALL201777,
title = {Text mining applied to electronic cardiovascular procedure reports to identify patients with trileaflet aortic stenosis and coronary artery disease},
journal = {Journal of Biomedical Informatics},
volume = {72},
pages = {77-84},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417301387},
author = {Aeron M. Small and Daniel H. Kiss and Yevgeny Zlatsin and David L. Birtwell and Heather Williams and Marie A. Guerraty and Yuchi Han and Saif Anwaruddin and John H. Holmes and Julio A. Chirinos and Robert L. Wilensky and Jay Giri and Daniel J. Rader},
keywords = {Valvular heart disease, Coronary artery disease, Text mining, Administrative, Billing codes},
abstract = {Background
Interrogation of the electronic health record (EHR) using billing codes as a surrogate for diagnoses of interest has been widely used for clinical research. However, the accuracy of this methodology is variable, as it reflects billing codes rather than severity of disease, and depends on the disease and the accuracy of the coding practitioner. Systematic application of text mining to the EHR has had variable success for the detection of cardiovascular phenotypes. We hypothesize that the application of text mining algorithms to cardiovascular procedure reports may be a superior method to identify patients with cardiovascular conditions of interest.
Methods
We adapted the Oracle product Endeca, which utilizes text mining to identify terms of interest from a NoSQL-like database, for purposes of searching cardiovascular procedure reports and termed the tool “PennSeek”. We imported 282,569 echocardiography reports representing 81,164 individuals and 27,205 cardiac catheterization reports representing 14,567 individuals from non-searchable databases into PennSeek. We then applied clinical criteria to these reports in PennSeek to identify patients with trileaflet aortic stenosis (TAS) and coronary artery disease (CAD). Accuracy of patient identification by text mining through PennSeek was compared with ICD-9 billing codes.
Results
Text mining identified 7115 patients with TAS and 9247 patients with CAD. ICD-9 codes identified 8272 patients with TAS and 6913 patients with CAD. 4346 patients with AS and 6024 patients with CAD were identified by both approaches. A randomly selected sample of 200–250 patients uniquely identified by text mining was compared with 200–250 patients uniquely identified by billing codes for both diseases. We demonstrate that text mining was superior, with a positive predictive value (PPV) of 0.95 compared to 0.53 by ICD-9 for TAS, and a PPV of 0.97 compared to 0.86 for CAD.
Conclusion
These results highlight the superiority of text mining algorithms applied to electronic cardiovascular procedure reports in the identification of phenotypes of interest for cardiovascular research.}
}
@article{CUIJUAN201815,
title = {Implementation of a Linked Data-Based Genealogy Knowledge Service Platform for Digital Humanities},
journal = {Data and Information Management},
volume = {2},
number = {1},
pages = {15-26},
year = {2018},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2018-0005},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000869},
author = {Xia Cuijuan and Liu Wei and Zhang Lei},
keywords = {Digital Humanities, Linked Data, Genealogy},
abstract = {Abstract
Linked data is becoming a mature technology as a lightweight realization of the Semantic Web, as well as a way of facilitating knowledge reorganization and discovery. As a use case and start point, based on linked data technology, a genealogy knowledge service platform was implemented by the Shanghai Library for providing knowledge discovery and open data services. This article explains the design and development of the Genealogy Knowledge Service Platform, describes the method and process of the implementation, and introduces four examples of how the platform helps users to discover questions, raise questions, and solve questions for their research, to explain how Linked Data can be used in Digital Humanities.}
}
@article{RODRIGUEZALVAREZ201812,
title = {Body condition estimation on cows from depth images using Convolutional Neural Networks},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {12-22},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.09.039},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917315788},
author = {Juan {Rodríguez Alvarez} and Mauricio Arroqui and Pablo Mangudo and Juan Toloza and Daniel Jatip and Juan M. Rodríguez and Alfredo Teyseyre and Carlos Sanz and Alejandro Zunino and Claudio Machado and Cristian Mateos},
keywords = {Precision livestock, Body condition score, Image analysis, Convolutional Neural Networks},
abstract = {BCS (“Body Condition Score”) is a method used to estimate body fat reserves and accumulated energy balance of cows. BCS heavily influences milk production, reproduction, and health of cows. Therefore, it is important to monitor BCS to achieve better animal response, but this is a time-consuming and subjective task performed visually by expert scorers. Several studies have tried to automate BCS of dairy cows by applying image analysis and machine learning techniques. This work analyzes these studies and proposes a system based on Convolutional Neural Networks (CNNs) to improve overall automatic BCS estimation, whose use might be extended beyond dairy production. The developed system has achieved good estimation results in comparison with other systems in the area. Overall accuracy of BCS estimations within 0.25 units of difference from true values was 78%, while overall accuracy within 0.50 units was 94%. Similarly, weighted precision and recall, which took into account imbalance BCS distribution in the built dataset, show similar values considering those error ranges.}
}
@article{KUDO201814,
title = {An exhaustive classification for the seasonal variation of organic peaks in the atmospheric fine particles obtained by a gas chromatography/mass spectrometry},
journal = {Environmental Technology & Innovation},
volume = {12},
pages = {14-26},
year = {2018},
issn = {2352-1864},
doi = {https://doi.org/10.1016/j.eti.2018.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S2352186417303152},
author = {Shinji Kudo and Akihiro Iijima and Kimiyo Kumagai and Hiroshi Tago and Miwako Ichijo},
keywords = {Fine particles (), Full scan GC/MS, Chromatogram, Unquantitative peaks, Cluster analysis, Dicarboxylic acids},
abstract = {For organic compounds in fine particles (PM2.5), the knowledge on their sources and formation processes are still rather limited because of labile characteristics. To assess the contribution of primary and secondary organic aerosols, it is necessary to utilize the behavior of organic markers. In this study, cluster analysis was applied to the seasonal variations of peaks in chromatogram for unquantitative organic compounds in PM2.5 obtained by a gas chromatography/mass spectrometry (GC/MS) analysis after derivatization method. We used the PM2.5 samples collected at Maebashi in Japan for four seasons. Based on cluster analysis, total 115 peaks were divided into nine clusters. The combination effects of the primary and secondary particles transported plume from urban atmosphere were found in two clusters for spring and summer. For dicarboxylic acids, C3−C5 diacids were in same cluster, whereas C2 and C6 diacids were separated with the other cluster. These results show different trends of secondary formation for dicarboxylic acids. This approach might help to find a group of markers in organic aerosols and to classify potential source of secondary organic aerosols.}
}
@article{LATOMBE2017295,
title = {A vision for global monitoring of biological invasions},
journal = {Biological Conservation},
volume = {213},
pages = {295-308},
year = {2017},
note = {SI:Measures of biodiversity},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2016.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0006320716302373},
author = {Guillaume Latombe and Petr Pyšek and Jonathan M. Jeschke and Tim M. Blackburn and Sven Bacher and César Capinha and Mark J. Costello and Miguel Fernández and Richard D. Gregory and Donald Hobern and Cang Hui and Walter Jetz and Sabrina Kumschick and Chris McGrannachan and Jan Pergl and Helen E. Roy and Riccardo Scalera and Zoe E. Squires and John R.U. Wilson and Marten Winter and Piero Genovesi and Melodie A. McGeoch},
keywords = {Essential Biodiversity Variables, Alien species, Species distribution, Occurrence, Alien impact, Alien listing},
abstract = {Managing biological invasions relies on good global coverage of species distributions. Accurate information on alien species distributions, obtained from international policy and cross-border co-operation, is required to evaluate trans-boundary and trading partnership risks. However, a standardized approach for systematically monitoring alien species and tracking biological invasions is still lacking. This Perspective presents a vision for global observation and monitoring of biological invasions. We show how the architecture for tracking biological invasions is provided by a minimum information set of Essential Variables, global collaboration on data sharing and infrastructure, and strategic contributions by countries. We show how this novel, synthetic approach to an observation system for alien species provides a tangible and attainable solution to delivering the information needed to slow the rate of new incursions and reduce the impacts of invaders. We identify three Essential Variables for Invasion Monitoring; alien species occurrence, species alien status and alien species impact. We outline how delivery of this minimum information set by joint, complementary contributions from countries and global community initiatives is possible. Country contributions are made feasible using a modular approach where all countries are able to participate and strategically build their contributions to a global information set over time. The vision we outline will deliver wide-ranging benefits to countries and international efforts to slow the rate of biological invasions and minimize their environmental impacts. These benefits will accrue over time as global coverage and information on alien species increases.}
}
@article{SILVA2018323,
title = {A semi-supervised Genetic Programming method for dealing with noisy labels and hidden overfitting},
journal = {Swarm and Evolutionary Computation},
volume = {39},
pages = {323-338},
year = {2018},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2210650217302730},
author = {Sara Silva and Leonardo Vanneschi and Ana I.R. Cabral and Maria J. Vasconcelos},
keywords = {Data errors, Noisy labels, Classification, Hidden overfitting, Semi-supervised learning, Genetic Programming},
abstract = {Data gathered in the real world normally contains noise, either stemming from inaccurate experimental measurements or introduced by human errors. Our work deals with classification data where the attribute values were accurately measured, but the categories may have been mislabeled by the human in several sample points, resulting in unreliable training data. Genetic Programming (GP) compares favorably with the Classification and Regression Trees (CART) method, but it is still highly affected by these errors. Despite consistently achieving high accuracy in both training and test sets, many classification errors are found in a later validation phase, revealing a previously hidden overfitting to the erroneous data. Furthermore, the evolved models frequently output raw values that are far from the expected range. To improve the behavior of the evolved models, we extend the original training set with additional sample points where the class label is unknown, and devise a simple way for GP to use this additional information and learn in a semi-supervised manner. The results are surprisingly good. In the presence of the exact same mislabeling errors, the additional unlabeled data allowed GP to evolve models that achieved high accuracy also in the validation phase. This is a brand new approach to semi-supervised learning that opens an array of possibilities for making the most of the abundance of unlabeled data available today, in a simple and inexpensive way.}
}
@article{YANG2018407,
title = {Towards sustainable and resilient high density cities through better integration of infrastructure networks},
journal = {Sustainable Cities and Society},
volume = {42},
pages = {407-422},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S2210670718300210},
author = {Yifan Yang and S. Thomas Ng and Frank J. Xu and Martin Skitmore},
keywords = {High density cities, Sustainability, Resilience, Infrastructure system, Asset management},
abstract = {Many developed high density cities around the world are facing unprecedented challenges as their infrastructure facilities are aging while citizen demands are ever surging. The concerted efforts of different infrastructure stakeholders are indispensable to elevate the quality, reliability, and capacity of infrastructure systems to make high-density cities more sustainable and resilient against increasing climate change and manmade threats. To address these challenges, this paper proposes an integrated framework for multisector infrastructure asset management. For deriving the framework, case studies are conducted first on the best infrastructure asset management (IAM) practices of different countries in such diverse aspects as core process integration, contingency management, climate change response and adaptation, program coordination and orchestration, social value creation and sharing, risk management, resilience and sustainability. By using the criteria and a list of questions obtained from the case studies, interviews with different stakeholders in selected infrastructure sectors in Hong Kong are subsequently carried out to identify the barriers and possible solutions to enhancing the integrated management of multisector infrastructure assets in high-density cities. To facilitate such municipalities in managing their infrastructure assets effectively and efficiently, the proposed multisector integrated IAM framework is established from the holistic perspectives of information integration, process integration, collective decision, and harmonization between interdependent infrastructure systems.}
}
@article{CONGRAM2017260,
title = {Grave mapping in support of the search for missing persons in conflict contexts},
journal = {Forensic Science International},
volume = {278},
pages = {260-268},
year = {2017},
issn = {0379-0738},
doi = {https://doi.org/10.1016/j.forsciint.2017.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0379073817302803},
author = {Derek Congram and Michael Kenyhercz and Arthur Gill Green},
keywords = {Forensic anthropology, Forensic archaeology, Spatial analysis, Geographic Information Science, Forensic Humanitarian Action},
abstract = {We review the current and potential uses of Geographic Information Software (GIS) and “spatial thinking” for understanding body disposal behaviour in times of mass fatalities, particularly armed conflict contexts. The review includes observations made by the authors during the course of their academic research and professional consulting on the use of spatial analysis and GIS to support Humanitarian Forensic Action (HFA) to search for the dead, theoretical and statistical considerations in modelling grave site locations, and suggestions on how this work may be advanced further.}
}
@article{PATONAI201777,
title = {Aggregation of incomplete food web data may help to suggest sampling strategies},
journal = {Ecological Modelling},
volume = {352},
pages = {77-89},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2017.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S0304380016305488},
author = {Katalin Patonai and Ferenc Jordán},
keywords = {Food web, Aggregation, Regular equivalence, Taxonomy, Incomplete data},
abstract = {Aggregation of data and incomplete sampling are two notoriuos problems of food web research. We suggest to look at them in parallel since their effects are interdependent. Different aggregation methods are not equally sensitive to missing data and they lead to different biases in describing food web structure. In this paper, we construct a low-quality food web of Lake Balaton (based only on high-quality literature), aggregate it in several ways, compare the different versions of the food web by network analysis and discuss how the results can help future sampling, field work and data management. We identify groups where resolution or aggregation should be increased.}
}
@article{DING2018800,
title = {Detecting the urban traffic network structure dynamics through the growth and analysis of multi-layer networks},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {503},
pages = {800-817},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.02.059},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118301316},
author = {Rui Ding and Norsidah Ujang and Hussain bin Hamid and Mohd Shahrudin Abd Manan and Yuou He and Rong Li and Jianjun Wu},
keywords = {Complex network, Multi-layer network growth, Traffic network accessibility, Network centrality, Community partition, Urban structure dynamics},
abstract = {Research into multi-layer network growth in the detection of urban dynamics provides scholars a new way to discuss the structure changing trends and related impacts. The quantitative research method is applied to examine, the network centrality, network accessibility and network community partition focusing on the upper-layer (rail network) network growth process. We based on the case study of Kuala Lumpur and found that when a rail network grows with a simple tree-like network to a more intricate form, the network diameter and the average shortest path length of multi-layer networks decrease dramatically. The network expansion ability keeps changing and more rail stations in the city centre have higher ability for future expansion. Changes in betweenness centrality and closeness centrality of multi-layer networks essentially hinge on the growth of rail network, with the highest change rate of closeness centrality at around 211.48%. The growth of network allows the remainder of the network to be easily visited, with the highest change rate of network accessibility around 12%. Different performances of these nodes added in the multi-layer network are discussed to show their impact on the repartition of network communities and the number of communities is decreasing. We believe this research can benefit scholars to easily understand and apply these network dynamic computational techniques.}
}
@article{BARANN201786,
title = {An open-data approach for quantifying the potential of taxi ridesharing},
journal = {Decision Support Systems},
volume = {99},
pages = {86-95},
year = {2017},
note = {Location Analytics and Decision Support},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2017.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923617300866},
author = {Benjamin Barann and Daniel Beverungen and Oliver Müller},
keywords = {Taxi ridesharing, Collaborative consumption, Transportation, Open data, Sustainability, Shared mobility},
abstract = {Taxi ridesharing11Taxi ridesharing (TRS), also known as shared taxi or collective taxi, is an advanced form of public transportation with flexible routing and scheduling that matches at least two separate ride requests with similar spatio-temporal characteristics in real-time to a jointly used taxi, driven by an employed driver without own destination. TRS, therefore, differs from private ridesharing, which refers to sharing of rides among private people. TRS is a more restricted dynamic dial-a-ride problem, which considers the requirements of both multiple passengers and the service provider. Because of the pooled simultaneous utilization of a taxi, TRS is collaborative consumption.[This definition has been pasted from the paper, Section 2.2. References are provided there] (TRS) is an advanced form of urban transportation that matches separate ride requests with similar spatio-temporal characteristics to a jointly used taxi. As collaborative consumption, TRS saves customers money, enables taxi companies to economize use of their resources, and lowers greenhouse gas emissions. We develop a one-to-one TRS approach that matches rides with similar start and end points. We evaluate our approach by analyzing an open dataset of >5 million taxi trajectories in New York City. Our empirical analysis reveals that the proposed approach matches up to 48.34% of all taxi rides, saving 2,892,036km of travel distance, 231,362.89l of gas, and 532,134.64kg of CO2 emissions per week. Compared to many-to-many TRS approaches, our approach is competitive, simpler to implement and operate, and poses less rigid assumptions on data availability and customer acceptance.}
}
@article{MCHANEY20181,
title = {Using LIWC to choose simulation approaches: A feasibility study},
journal = {Decision Support Systems},
volume = {111},
pages = {1-12},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300691},
author = {Roger McHaney and Antuela Tako and Stewart Robinson},
keywords = {Linguistic inquiry and word count(LIWC), Text analytics, Problem solving, Tool choice, Discrete event simulation, System dynamics},
abstract = {Can language usage help determine which model approach is best suited to provide decision makers with desired insights? This research addresses that question through an investigation of Linguistic Inquiry and Word Count (LIWC), which calculates the presence of >80 language dimensions in text samples, and permits construction of custom dictionaries. This article demonstrates use of LIWC to ensure better problem/model fit within the context of selecting a decision support tool. We selected two simulation tools as research instruments to investigate a broader question on the usefulness of LIWC to guide choice of DSS tool. The tools selected were System Dynamics (SD) and Discrete Event Simulation (DES). First, we tested LIWC to analyze practitioners' language use when developing models. LIWC pointed out significant linguistic differences consistent with prior theoretical work, based on model development approach in a number of dimensions. These differences provided a basis for developing a custom dictionary for use on the second part of our study. The second part of the study focused on language used by decision makers in problem statements and used the linguistic clues identified in the first part of the study to ensure problem/model fit. Results indicated problem statements contained linguistic clues related to the type of information desired by problem solvers. The article concludes with a discussion about how LIWC and similar tools can help determine which DSS tools are suited to particular applications.}
}
@article{DEDUMAST201845,
title = {A web-based system for neural network based classification in temporomandibular joint osteoarthritis},
journal = {Computerized Medical Imaging and Graphics},
volume = {67},
pages = {45-54},
year = {2018},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2018.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0895611118302805},
author = {Priscille {de Dumast} and Clément Mirabel and Lucia Cevidanes and Antonio Ruellas and Marilia Yatabe and Marcos Ioshida and Nina Tubau Ribera and Loic Michoud and Liliane Gomes and Chao Huang and Hongtu Zhu and Luciana Muniz and Brandon Shoukri and Beatriz Paniagua and Martin Styner and Steve Pieper and Francois Budin and Jean-Baptiste Vimort and Laura Pascal and Juan Carlos Prieto},
keywords = {Neural network, Web-Based system, Osteoarthritis},
abstract = {Objective
The purpose of this study is to describe the methodological innovations of a web-based system for storage, integration and computation of biomedical data, using a training imaging dataset to remotely compute a deep neural network classifier of temporomandibular joint osteoarthritis (TMJOA).
Methods
This study imaging dataset consisted of three-dimensional (3D) surface meshes of mandibular condyles constructed from cone beam computed tomography (CBCT) scans. The training dataset consisted of 259 condyles, 105 from control subjects and 154 from patients with diagnosis of TMJ OA. For the image analysis classification, 34 right and left condyles from 17 patients (39.9 ± 11.7 years), who experienced signs and symptoms of the disease for less than 5 years, were included as the testing dataset. For the integrative statistical model of clinical, biological and imaging markers, the sample consisted of the same 17 test OA subjects and 17 age and sex matched control subjects (39.4 ± 15.4 years), who did not show any sign or symptom of OA. For these 34 subjects, a standardized clinical questionnaire, blood and saliva samples were also collected. The technological methodologies in this study include a deep neural network classifier of 3D condylar morphology (ShapeVariationAnalyzer, SVA), and a flexible web-based system for data storage, computation and integration (DSCI) of high dimensional imaging, clinical, and biological data.
Results
The DSCI system trained and tested the neural network, indicating 5 stages of structural degenerative changes in condylar morphology in the TMJ with 91% close agreement between the clinician consensus and the SVA classifier. The DSCI remotely ran with a novel application of a statistical analysis, the Multivariate Functional Shape Data Analysis, that computed high dimensional correlations between shape 3D coordinates, clinical pain levels and levels of biological markers, and then graphically displayed the computation results.
Conclusions
The findings of this study demonstrate a comprehensive phenotypic characterization of TMJ health and disease at clinical, imaging and biological levels, using novel flexible and versatile open-source tools for a web-based system that provides advanced shape statistical analysis and a neural network based classification of temporomandibular joint osteoarthritis.}
}
@article{WONG2018312,
title = {Digitisation in facilities management: A literature review and future research directions},
journal = {Automation in Construction},
volume = {92},
pages = {312-326},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517309020},
author = {Johnny Kwok Wai Wong and Janet Ge and Sean Xiangjian He},
keywords = {Digital technology, Facilities management, Building information modelling, Radio frequency identification, Point cloud, Laser scanning},
abstract = {Research into digital technology (DT) in construction practices has gained widespread attention. While the application of different DTs in facility management (FM) has been growing, to date, there is no holistic review of the various DT developments and research into FM. A total of 120 academic journal papers, conference proceedings and other technical notes published on the subject, mainly between 2004 and 2017, were reviewed in this paper. The applications of various major DTs, including 1) building information modelling (BIM), 2) reality capture technology (including 3D laser scanning, point cloud), 3) the Internet of Things (IoT) (including radio frequency identification (RFID) and sensor network technologies) and 4) geographic information system (GIS), were reviewed and scrutinised. The review identified a number of possibilities for future research into DT in FM, including, enhancing the interoperability of data, improving the accuracy of point cloud data for developing as-built models for existing facilities, and generating effective BIM/GIS asset database integration. It is hoped that this review and the future directions highlighted in this paper will assist researchers in identifying the areas where further research efforts are most required and in identifying which future directions would be most helpful for digital FM research.}
}
@article{BEIER2017466,
title = {Multicenter data sharing for collaboration in sleep medicine},
journal = {Future Generation Computer Systems},
volume = {67},
pages = {466-480},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16300693},
author = {Maximilian Beier and Christoph Jansen and Geert Mayer and Thomas Penzel and Andrea Rodenbeck and René Siewert and Michael Witt and Jie Wu and Dagmar Krefting},
keywords = {Biosignal, Polysomnography, EDF, XNAT, OpenStack, Docker},
abstract = {Sleep is a fundamental biological process crucial for survival and health of (not only) humans. But many circumstances like physiological and mental disorders, environment and lifestyle may affect healthy sleep. To date, 88 different sleep disorders are internationally recognized. They cover a broad field of medical areas. Analysis of human sleep is typically based on multidimensional biosignal recordings, so called polysomnographies (PSG). Therefore research often includes digital signal processing. Clinical sleep research is an inherent multidisciplinary field. Inter-institutional and interdisciplinary collaborations are required to address the complexity of sleep regulation and disturbance. But to date, collaborative sleep research is poorly supported by IT systems. In particular, the management and processing of PSGs is challenging. A large variety of PSG devices, data formats, measurement procedures and quality variations impedes consistent biosignal data processing. In this manuscript we introduce a virtual research platform supporting inter-institutional data sharing and processing. The infrastructure is based on XNAT—a free and open source neuroimaging research platform, a loosely coupled service oriented architecture and scalable virtualization in the back end. The system is capable of local pseudonymization of biosignal data, mapping to a standardized set of parameters and automatic quality assessment. Terms and quality measures are derived from the “Manual for the Scoring of Sleep and Associated Events” of the American Academy of Sleep Medicine (AASM), the de facto standard for diagnostic biosignal analysis in sleep medicine.}
}
@article{CETTO2018275,
title = {“Thanks for sharing”—Identifying users’ roles based on knowledge contribution in Enterprise Social Networks},
journal = {Computer Networks},
volume = {135},
pages = {275-288},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618300793},
author = {Alexandra Cetto and Mathias Klier and Alexander Richter and Jan Felix Zolitschka},
keywords = {Enterprise Social Networks, User roles, Knowledge contribution, Knowledge sharing, Knowledge seeking},
abstract = {While ever more companies use Enterprise Social Networks for knowledge management, there is still a lack of understanding of users’ knowledge exchanging behavior. In this context, it is important to be able to identify and characterize users who contribute and communicate their knowledge in the network and help others to get their work done. In this paper, we propose a new methodological approach consisting of three steps, namely “message classification”, “identification of users’ roles” as well as “characterization of users’ roles”. We apply the approach to a dataset from a multinational consulting company, which allows us to identify three user roles based on their knowledge contribution in messages: givers, takers, and matchers. Going beyond this categorization, our data shows that whereas the majority of messages aims to share knowledge, matchers, that means people that give and take, are a central element of the network. In conclusion, the development and application of a new methodological approach allows us to contribute to a more refined understanding of users’ knowledge exchanging behavior in Enterprise Social Networks which can ultimately help companies to take measures to improve their knowledge management.}
}
@article{HAMPSON20171104,
title = {Improving the selection and development of influenza vaccine viruses – Report of a WHO informal consultation on improving influenza vaccine virus selection, Hong Kong SAR, China, 18–20 November 2015},
journal = {Vaccine},
volume = {35},
number = {8},
pages = {1104-1109},
year = {2017},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2017.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X17300373},
author = {Alan Hampson and Ian Barr and Nancy Cox and Ruben O. Donis and Hirve Siddhivinayak and Daniel Jernigan and Jacqueline Katz and John McCauley and Fernando Motta and Takato Odagiri and John S. Tam and Anthony Waddell and Richard Webby and Thedi Ziegler and Wenqing Zhang},
keywords = {Influenza vaccines, Surveillance, Antigenic drift, Pandemic influenza},
abstract = {Since 2010 the WHO has held a series of informal consultations to explore ways of improving the currently highly complex and time-pressured influenza vaccine virus selection and development process. In November 2015 experts from around the world met to review the current status of efforts in this field. Discussion topics included strengthening influenza surveillance activities to increase the availability of candidate vaccine viruses and improve the extent, timeliness and quality of surveillance data. Consideration was also given to the development and potential application of newer laboratory assays to better characterize candidate vaccine viruses, the potential importance of antibodies directed against influenza virus neuraminidase, and the role of vaccine effectiveness studies. Advances in next generation sequencing and whole genome sequencing of influenza viruses were also discussed, along with associated developments in synthetic genomics technologies, evolutionary analysis and predictive mathematical modelling. Discussions were also held on the late emergence of an antigenic variant influenza A(H3N2) virus in mid-2014 that could not be incorporated in time into the 2014–15 northern hemisphere vaccine. There was broad recognition that given the current highly constrained influenza vaccine development and production timeline it would remain impossible to incorporate any variant virus which emerged significantly long after the relevant WHO biannual influenza vaccine composition meetings. Discussions were also held on the development of pandemic and broadly protective vaccines, and on associated regulatory and manufacturing requirements and constraints. With increasing awareness of the health and economic burdens caused by seasonal influenza, the ever-present threat posed by zoonotic influenza viruses, and the significant impact of the 2014–15 northern hemisphere seasonal influenza vaccine mismatch, this consultation provided a very timely opportunity to share developments and exchange views. In all areas, a renewed and strengthened emphasis was placed on developing concrete and measurable actions and identifying the key stakeholders responsible for their implementation.}
}
@article{MEYER2018905,
title = {Machine learning for real-time prediction of complications in critical care: a retrospective study},
journal = {The Lancet Respiratory Medicine},
volume = {6},
number = {12},
pages = {905-914},
year = {2018},
issn = {2213-2600},
doi = {https://doi.org/10.1016/S2213-2600(18)30300-X},
url = {https://www.sciencedirect.com/science/article/pii/S221326001830300X},
author = {Alexander Meyer and Dina Zverinski and Boris Pfahringer and Jörg Kempfert and Titus Kuehne and Simon H Sündermann and Christof Stamm and Thomas Hofmann and Volkmar Falk and Carsten Eickhoff},
abstract = {Summary
Background
The large amount of clinical signals in intensive care units can easily overwhelm health-care personnel and can lead to treatment delays, suboptimal care, or clinical errors. The aim of this study was to apply deep machine learning methods to predict severe complications during critical care in real time after cardiothoracic surgery.
Methods
We used deep learning methods (recurrent neural networks) to predict several severe complications (mortality, renal failure with a need for renal replacement therapy, and postoperative bleeding leading to operative revision) in post cardiosurgical care in real time. Adult patients who underwent major open heart surgery from Jan 1, 2000, to Dec 31, 2016, in a German tertiary care centre for cardiovascular diseases formed the main derivation dataset. We measured the accuracy and timeliness of the deep learning model's forecasts and compared predictive quality to that of established standard-of-care clinical reference tools (clinical rule for postoperative bleeding, Simplified Acute Physiology Score II for mortality, and the Kidney Disease: Improving Global Outcomes staging criteria for acute renal failure) using positive predictive value (PPV), negative predictive value, sensitivity, specificity, area under the curve (AUC), and the F1 measure (which computes a harmonic mean of sensitivity and PPV). Results were externally retrospectively validated with 5898 cases from the published MIMIC-III dataset.
Findings
Of 47 559 intensive care admissions (corresponding to 42 007 patients), we included 11 492 (corresponding to 9269 patients). The deep learning models yielded accurate predictions with the following PPV and sensitivity scores: PPV 0·90 and sensitivity 0·85 for mortality, 0·87 and 0·94 for renal failure, and 0·84 and 0·74 for bleeding. The predictions significantly outperformed the standard clinical reference tools, improving the absolute complication prediction AUC by 0·29 (95% CI 0·23–0·35) for bleeding, by 0·24 (0·19–0·29) for mortality, and by 0·24 (0·13–0·35) for renal failure (p<0·0001 for all three analyses). The deep learning methods showed accurate predictions immediately after patient admission to the intensive care unit. We also observed an increase in performance in our validation cohort when the machine learning approach was tested against clinical reference tools, with absolute improvements in AUC of 0·09 (95% CI 0·03–0·15; p=0·0026) for bleeding, of 0·18 (0·07–0·29; p=0·0013) for mortality, and of 0·25 (0·18–0·32; p<0·0001) for renal failure.
Interpretation
The observed improvements in prediction for all three investigated clinical outcomes have the potential to improve critical care. These findings are noteworthy in that they use routinely collected clinical data exclusively, without the need for any manual processing. The deep machine learning method showed AUC scores that significantly surpass those of clinical reference tools, especially soon after admission. Taken together, these properties are encouraging for prospective deployment in critical care settings to direct the staff's attention towards patients who are most at risk.
Funding
No specific funding.}
}
@incollection{VALLERO201825,
title = {Chapter 2 - The Environmental Knowledge Cascade},
editor = {Daniel Vallero},
booktitle = {Translating Diverse Environmental Data into Reliable Information},
publisher = {Academic Press},
pages = {25-41},
year = {2018},
isbn = {978-0-12-812446-8},
doi = {https://doi.org/10.1016/B978-0-12-812446-8.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124468000025},
author = {Daniel Vallero},
keywords = {Alternative assessment, Decision space, Decision-making, Knowledge cascade, Knowledge-building, Life cycle, Multi-criteria decision analysis (MCDA), Reproducibility},
abstract = {The underlying premise of the knowledge cascade, introduced in Chapter 1, is that data themselves only become useful to decision-making after they have undergone a transformation into information. The information is then transformed into knowledge as it is applied to specific needs. The extent of application depends on the type of decision. For example, a theoretical scientist may be completely satisfied by adding new insights into an area of research, such as how an atom's spin is changed when exposed to a particular range of electromagnetic energy. To this scientist, the data gathered from experiments are now useful information in helping to explain the atom. However, another scientist whose interests lie further detached from the “pure” research, e.g., one who is trying to enhance chemical analysis, may only be satisfied when the same data are transformed into improving chemical analytical equipment, such as improving retention times for a gas chromatograph's column or a better detection using mass spectroscopy.}
}
@article{GALLINELLI2017391,
title = {CityFeel - micro climate monitoring for climate mitigation and urban design},
journal = {Energy Procedia},
volume = {122},
pages = {391-396},
year = {2017},
note = {CISBAT 2017 International ConferenceFuture Buildings & Districts – Energy Efficiency from Nano to Urban Scale},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.07.427},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217332605},
author = {Peter Gallinelli and Reto Camponovo and Victor Guillot},
keywords = {climate mitigation, micro climate monitoring, urban design, open data},
abstract = {While a significant part of the population is concentrated in urban areas, the influence of cityscape parameters on human heat stress remain poorly understood. Yet we agree to develop urban spaces (street, square, district ...) in a way to provide best possible quality of life. In order to do so, quantitative and qualitative references are required. To fill this gap the HES-SO††University of Applied Sciences and Arts of Western Switzerland - hepia/leea‡‡Haute école du paysage, d’ingénierie et d’architecture de Genève / Laboratory for energy, environment and architecture has developed an innovative portable monitoring system that can be easily deployed in various outdoor and indoor environments. The monitoring equipment is embedded into a backpack that is carried during ‘climatic urban walks’ that can be reproduced at different times of the day or seasons so to yield a detailed and dynamic description of the climatic context of a portion of the city from the pedestrian point of view.}
}
@incollection{2018167,
title = {Chapter 5 - Analytical Approaches for Post-Authorization Safety Studies},
editor = {Ayad K. Ali and Abraham G. Hartzema},
booktitle = {Post-Authorization Safety Studies of Medicinal Products},
publisher = {Academic Press},
pages = {167-221},
year = {2018},
isbn = {978-0-12-809217-0},
doi = {https://doi.org/10.1016/B978-0-12-809217-0.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092170000052}
}
@article{TANG2018147,
title = {Image hashing with color vector angle},
journal = {Neurocomputing},
volume = {308},
pages = {147-158},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.04.057},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218304971},
author = {Zhenjun Tang and Xuelong Li and Xianquan Zhang and Shichao Zhang and Yumin Dai},
keywords = {Image hashing, Hashing function, Color vector angle, Color histogram},
abstract = {Color vector angle (CVA) is an important feature of processing color images and has been successfully developed and used in real applications, such as edge detection, indexing and retrieval of images. However, it is unsolved how to apply the CVA to efficiently generating an image hash. Also, most image hashing algorithms choose luminance component of color image for hash generation and cannot well capture the color information of images. To tackle these issues, we study efficient image hashing algorithms with the histogram of CVAs, called HCVA hashing. The histogram is first extracted from those angles that are in the biggest circle inscribed inside the normalized image. And then, it is compressed to make a short hash. We conducted some experiments to assess the performance, and illustrated that the DCT (Discrete Cosine Transform) is the best one of that cooperating with HCVA at generating hashes, as well as the HCVA hashing is robust and promising.}
}
@article{WANG201832,
title = {Truthful incentive mechanism with location privacy-preserving for mobile crowdsourcing systems},
journal = {Computer Networks},
volume = {135},
pages = {32-43},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618300756},
author = {Yingjie Wang and Zhipeng Cai and Xiangrong Tong and Yang Gao and Guisheng Yin},
keywords = {Mobile crowdsourcing, Incentive mechanism, Auction algorithm, -anonymity, Differential privacy},
abstract = {With the rapid development of mobile devices, mobile crowdsourcing has become an important research focus. In order to improve the efficiency and truthfulness of mobile crowdsourcing systems, this paper proposes a truthful incentive mechanism with location privacy-preserving for mobile crowdsourcing systems. The improved two-stage auction algorithm based on trust degree and privacy sensibility (TATP) is proposed. In addition, the k−ɛ-differential privacy-preserving is proposed to prevent users’ location information from being leaked. Through comparison experiments, the effectiveness of the proposed incentive mechanism is verified. The proposed incentive mechanism with location privacy-preserving can inspire users to participate sensing tasks, and protect users’ location privacy effectively.}
}
@article{WANG2017165,
title = {Regional eco-efficiency prediction with Support Vector Spatial Dynamic MIDAS},
journal = {Journal of Cleaner Production},
volume = {161},
pages = {165-177},
year = {2017},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.077},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617310089},
author = {Xianning Wang and Zhi Xiao},
keywords = {Regional eco-efficiency, Prediction, Spatial mixed-frequency panel data, Support Vector Spatial Dynamic MIDAS},
abstract = {To obtain good eco-efficiency prediction with factors accompanied by spatial relationship, mixed frequency data and nonlinearity, based on the existing spatial panel data forecasting models and MIxed DAta Sampling (MIDAS), we established Support Vector Spatial Dynamic MIDAS to incorporate the spatial interaction, different frequencies of sampling data, and non-linear relationship between the eco-efficiency and various factors. Further to testify the effectiveness, we applied the new model to regional eco-efficiency prediction in China. Prediction Error of the Last Year, Mean Percentage Error, Mean Square of Prediction Error and Standard Deviation of Prediction Error were utilized to measure prediction accuracy. Results showed SVSD-MIDAS effectively considered the mixed frequency factors--Financial Development Level, Foreign Direct Investment, Urbanization Level, Price Index, Fixed Asset Investment and their spatial interaction. Prediction performances of 30 regions are very good, with low prediction error below 1% or smaller. And regional prediction characteristics in the eastern, central, western and northeast regions were compared. The different spatial weights impacted the prediction no matter in individual province or the whole 4 areas. Accurate prediction by SVSD-MIDAS can save costs of collecting and calculating indicators, and guide the formulation of regional sustainable development strategies of residents, business managers, government departments in advance.}
}
@article{RAMIREZ201743,
title = {Evaluating the benefits of combined and continuous Fog-to-Cloud architectures},
journal = {Computer Communications},
volume = {113},
pages = {43-52},
year = {2017},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2017.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0140366417301792},
author = {W. Ramirez and X. Masip-Bruin and E. Marin-Tordera and V.B.C. Souza and A. Jukan and G-J. Ren and O. {Gonzalez de Dios}},
abstract = {The need to extend the features of Cloud computing to the edge of the network has fueled the development of new computing architectures, such as Fog computing. When put together, the combined and continuous use of fog and cloud computing, lays the foundation for a new and highly heterogeneous computing ecosystem, making the most out of both, cloud and fog. Incipient research efforts are devoted to propose a management architecture to properly manage such combination of resources, such as the reference architecture proposed by the OpenFog Consortium or the recent Fog-to-Cloud (F2C). In this paper, we pay attention to such a combined ecosystem and particularly evaluate the potential benefits of F2C in dynamic scenarios, considering computing resources mobility and different traffic patterns. By means of extensive simulations we specifically study the aspects of service response time, network bandwidth occupancy, power consumption and service disruption probability. The results indicate that a combined fog-to-cloud architecture brings significant performance benefits in comparison with the traditional standalone Cloud, e.g., over 50% reduction in terms of power consumption.}
}
@incollection{KATSILA2018195,
title = {Chapter 10 - Human Genomic Databases in Translational Medicine},
editor = {Christophe G. Lambert and Darrol J. Baker and George P. Patrinos},
booktitle = {Human Genome Informatics},
publisher = {Academic Press},
pages = {195-222},
year = {2018},
isbn = {978-0-12-809414-3},
doi = {https://doi.org/10.1016/B978-0-12-809414-3.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094143000103},
author = {Theodora Katsila and Emmanouil Viennas and Marina Bartsakoulia and Aggeliki Komianou and Konstantinos Sarris and Giannis Tzimas and George P. Patrinos},
keywords = {Database management systems, Genomic databases, Genomic variants, Locus-specific databases, Microattribution, National/ethnic mutation databases},
abstract = {Genomic databases are integral parts of human genome informatics, which enjoyed an exponential growth in the postgenomic era, as a result of the understanding of the genetic etiology of human disorders and the identification of numerous genomic variants. These resources organize this knowledge and variants such that it could be eventually useful not only for molecular diagnosis, but also for clinicians and researchers. Human genomic databases are referred to as online repositories of genomic variants, mainly described for a single or more genes or specifically for a population or ethnic group, aiming to facilitate diagnosis at the DNA level and to correlate genomic variants with specific phenotypic patterns and clinical features. Here, the key features of the main types of human genomic databases that are frequently used in genomic and translational medicine will be summarized, namely locus-specific and national/ethnic genetic databases. In particular, the main activities relating to these genetic database types will be highlighted to describe the existing and emerging database types in this domain and emphasize their potential applications in modern medical genetics, while the key elements that are still missing and holding back the field will be critically discussed.}
}
@article{JADOT2017194,
title = {Accounting for Protein Subcellular Localization: A Compartmental Map of the Rat Liver Proteome*},
journal = {Molecular & Cellular Proteomics},
volume = {16},
number = {2},
pages = {194-212},
year = {2017},
issn = {1535-9476},
doi = {https://doi.org/10.1074/mcp.M116.064527},
url = {https://www.sciencedirect.com/science/article/pii/S1535947620324361},
author = {Michel Jadot and Marielle Boonen and Jaqueline Thirion and Nan Wang and Jinchuan Xing and Caifeng Zhao and Abla Tannous and Meiqian Qian and Haiyan Zheng and John K. Everett and Dirk F. Moore and David E. Sleat and Peter Lobel},
abstract = {Accurate knowledge of the intracellular location of proteins is important for numerous areas of biomedical research including assessing fidelity of putative protein-protein interactions, modeling cellular processes at a system-wide level and investigating metabolic and disease pathways. Many proteins have not been localized, or have been incompletely localized, partly because most studies do not account for entire subcellular distribution. Thus, proteins are frequently assigned to one organelle whereas a significant fraction may reside elsewhere. As a step toward a comprehensive cellular map, we used subcellular fractionation with classic balance sheet analysis and isobaric labeling/quantitative mass spectrometry to assign locations to >6000 rat liver proteins. We provide quantitative data and error estimates describing the distribution of each protein among the eight major cellular compartments: nucleus, mitochondria, lysosomes, peroxisomes, endoplasmic reticulum, Golgi, plasma membrane and cytosol. Accounting for total intracellular distribution improves quality of organelle assignments and assigns proteins with multiple locations. Protein assignments and supporting data are available online through the Prolocate website (http://prolocate.cabm.rutgers.edu). As an example of the utility of this data set, we have used organelle assignments to help analyze whole exome sequencing data from an infant dying at 6 months of age from a suspected neurodegenerative lysosomal storage disorder of unknown etiology. Sequencing data was prioritized using lists of lysosomal proteins comprising well-established residents of this organelle as well as novel candidates identified in this study. The latter included copper transporter 1, encoded by SLC31A1, which we localized to both the plasma membrane and lysosome. The patient harbors two predicted loss of function mutations in SLC31A1, suggesting that this may represent a heretofore undescribed recessive lysosomal storage disease gene.}
}
@article{BZDOK2018223,
title = {Machine Learning for Precision Psychiatry: Opportunities and Challenges},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {3},
number = {3},
pages = {223-230},
year = {2018},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2017.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S2451902217302069},
author = {Danilo Bzdok and Andreas Meyer-Lindenberg},
keywords = {Artificial intelligence, Endophenotypes, Machine learning, Null-hypothesis testing, Personalized medicine, Predictive analytics, Research Domain Criteria (RDoC), Single-subject prediction},
abstract = {The nature of mental illness remains a conundrum. Traditional disease categories are increasingly suspected to misrepresent the causes underlying mental disturbance. Yet psychiatrists and investigators now have an unprecedented opportunity to benefit from complex patterns in brain, behavior, and genes using methods from machine learning (e.g., support vector machines, modern neural-network algorithms, cross-validation procedures). Combining these analysis techniques with a wealth of data from consortia and repositories has the potential to advance a biologically grounded redefinition of major psychiatric disorders. Increasing evidence suggests that data-derived subgroups of psychiatric patients can better predict treatment outcomes than DSM/ICD diagnoses can. In a new era of evidence-based psychiatry tailored to single patients, objectively measurable endophenotypes could allow for early disease detection, individualized treatment selection, and dosage adjustment to reduce the burden of disease. This primer aims to introduce clinicians and researchers to the opportunities and challenges in bringing machine intelligence into psychiatric practice.}
}
@article{BEDNARZ201850,
title = {Advances, challenges, and promises in pediatric neuroimaging of neurodevelopmental disorders},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {90},
pages = {50-69},
year = {2018},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2018.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S014976341730711X},
author = {Haley M. Bednarz and Rajesh K. Kana},
keywords = {Neurodevelopmental disorders, Autism spectrum disorder, Attention-deficit/hyperactivity disorder, Tourette’s syndrome, Neuroimaging, Magnetic resonance imaging},
abstract = {Recent years have witnessed the proliferation of neuroimaging studies of neurodevelopmental disorders (NDDs), particularly of children with autism spectrum disorder (ASD), attention-deficit/hyperactivity disorder (ADHD), and Tourette’s syndrome (TS). Neuroimaging offers immense potential in understanding the biology of these disorders, and how it relates to clinical symptoms. Neuroimaging techniques, in the long run, may help identify neurobiological markers to assist clinical diagnosis and treatment. However, methodological challenges have affected the progress of clinical neuroimaging. This paper reviews the methodological challenges involved in imaging children with NDDs. Specific topics include correcting for head motion, normalization using pediatric brain templates, accounting for psychotropic medication use, delineating complex developmental trajectories, and overcoming smaller sample sizes. The potential of neuroimaging-based biomarkers and the utility of implementing neuroimaging in a clinical setting are also discussed. Data-sharing approaches, technological advances, and an increase in the number of longitudinal, prospective studies are recommended as future directions. Significant advances have been made already, and future decades will continue to see innovative progress in neuroimaging research endeavors of NDDs.}
}
@article{KALLAB2017895,
title = {HIT2GAP: Towards a better building energy management},
journal = {Energy Procedia},
volume = {122},
pages = {895-900},
year = {2017},
note = {CISBAT 2017 International ConferenceFuture Buildings & Districts – Energy Efficiency from Nano to Urban Scale},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.07.399},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217330035},
author = {Lara Kallab and Richard Chbeir and Pierre Bourreau and Pascale Brassier and Michael Mrissa},
keywords = {Energy Efficiency, Smart Buildings, Web-based Framework, Ontological Data Model, Web-service Composition},
abstract = {Recent studies show that the Energy Performance Gap (EPGap), defined as the difference between the estimated and actual energy consumption of a building, is significantly high. This is due to various factors encountered in the different phases of the building life cycle, i.e., inaccuracy of the specifications used in the simulation tools during design phase, poor quality of the on-site practices conducted throughout the construction, inadequate verification of the equipment installed in the building during commissioning phase, and limited analysis of the data collected from the equipment during the operational phase. With the aim of reducing the EPGap, we present an energy management framework defined in the context of an EU-funded H2020 project, HIT2GAP1. The proposed solution provides several services from collecting heterogeneous on-site data, to advanced data analysis and visualization tools designed for the different actors of a building (e.g., building/facility/energy manager, occupants, etc.), for building energy performance optimization. In this paper, we give an overview on the proposed framework architecture and detail its different functionalities, with a particular focus on the solution Core Platform, which is in charge of orchestrating the different components, storing and structuring the data, and providing pre-processing services.}
}
@article{SHAN20181,
title = {Integrated SWATH-based and targeted-based proteomics provide insights into the retinal emmetropization process in guinea pig},
journal = {Journal of Proteomics},
volume = {181},
pages = {1-15},
year = {2018},
issn = {1874-3919},
doi = {https://doi.org/10.1016/j.jprot.2018.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S1874391918301246},
author = {Sze Wan Shan and Dennis Yan-yin Tse and Bing Zuo and Chi Ho To and Quan Liu and Sally A. McFadden and Rachel Ka-Man Chun and Jingfang Bian and King Kit Li and Thomas Chuen Lam},
keywords = {Emmetropization, Retina, SWATH-MS, Myopia, Guinea pigs},
abstract = {Myopia is generally regarded as a failure of normal emmetropization process, however, its underlying molecular mechanisms are unclear. To investigate the retinal protein profile changes during emmetropization, we studied differential protein expressions of ocular growth in young guinea pigs at 3 and 21 days old respectively, when significant axial elongation was detected (P < 0.001, n = 10). Independent pooled retinal samples of both eyes were subjected to SWATH mass spectrometry (MS) followed by bioinformatics analysis using cloud-based platforms. A comprehensive retina SWATH ion-library consisting of 3138 (22,871) unique proteins (peptides) at 1% FDR was constructed. 40 proteins were found to be significantly up-regulated and 8 proteins down-regulated during emmetropization (≥log2 of 0.43 with ≥2 peptides matched per protein; P < 0.05). Using pathway analysis, the most significant pathway identifiable was ‘phototransduction’ (P = 1.412e−4). Expression patterns of 7 proteins identified in this pathway were further validated and confirmed (P < 0.05) with high-resolution Multiple Reaction Monitoring (MRM-HR) MS. Combining discovery and targeted proteomics approaches, this study for the first time comprehensively profiled protein changes in the guinea pig retina during normal emmetropization-associated eye growth. The findings of this study are also relevant to the myopia development, which is the result of failed emmetropization.
Significance
Myopia is considered as a failure of emmetropization. However, the underlying biochemical mechanism of emmetropization, a visually guided process in which eye grows towards the optimal optical state of clear vision during early development, is not well understood. Retina is known as the key tissue to regulate this active eye growth. we studied eye growth of young guinea pigs and harvested their retinal tissues. A comprehensive SWATH ion library with identification of a total 3138 unique proteins were established, in which 48 proteins exhibited significant differential expressions between 3 and 21 days old. After MRM-HR confirmation, ‘phototransduction’ were found as the most active pathway during emmetropic eye growth. This study is the first in discovering key retinal protein players and pathways which are presumably orchestrated by biological mechanism(s) underlying emmetropization.}
}
@article{KIRBY20171,
title = {Advances in spatial epidemiology and geographic information systems},
journal = {Annals of Epidemiology},
volume = {27},
number = {1},
pages = {1-9},
year = {2017},
note = {GIS and Spatial Methods in Epidemiology Symposium},
issn = {1047-2797},
doi = {https://doi.org/10.1016/j.annepidem.2016.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1047279716304951},
author = {Russell S. Kirby and Eric Delmelle and Jan M. Eberth},
keywords = {Spatial epidemiology, Geographic information systems (GIS), Geocoding, Geographic mapping, Health services accessibility, Spatial regression, Spatial analysis, Spatiotemporal analysis},
abstract = {The field of spatial epidemiology has evolved rapidly in the past 2 decades. This study serves as a brief introduction to spatial epidemiology and the use of geographic information systems in applied research in epidemiology. We highlight technical developments and highlight opportunities to apply spatial analytic methods in epidemiologic research, focusing on methodologies involving geocoding, distance estimation, residential mobility, record linkage and data integration, spatial and spatio-temporal clustering, small area estimation, and Bayesian applications to disease mapping. The articles included in this issue incorporate many of these methods into their study designs and analytical frameworks. It is our hope that these studies will spur further development and utilization of spatial analysis and geographic information systems in epidemiologic research.}
}
@incollection{VALLERO20189,
title = {Chapter 1 - Building a New Environmental Knowledgebase},
editor = {Daniel Vallero},
booktitle = {Translating Diverse Environmental Data into Reliable Information},
publisher = {Academic Press},
pages = {9-23},
year = {2018},
isbn = {978-0-12-812446-8},
doi = {https://doi.org/10.1016/B978-0-12-812446-8.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124468000013},
author = {Daniel Vallero},
keywords = {Data, Environmental knowledgebase, Environmental protection, Information, Information technology, Precaution, Scientific method, Uncertainty},
abstract = {This chapter explains how data and information are gathered and used to build environmental knowledge. Environmental protection and ecology are not synonymous, nor are environmental protection and public health. Environmental protection embodies ecosystems and human populations, as well as other resources, such as material integrity of monuments and other cultural resources. In addition, environmental studies and assessments often address societal needs, such as economics and justice. This means that the knowledgebases that underpin these studies and the decisions they support are diverse.}
}
@article{SHARMA2018446,
title = {eFeed-Hungers.com: Mitigating global hunger crisis using next generation technologies},
journal = {Telematics and Informatics},
volume = {35},
number = {2},
pages = {446-456},
year = {2018},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2018.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0736585317308468},
author = {Sugam Sharma and Ritu Shandilya and U. Sunday Tim and Johnny Wong},
keywords = {Hunger, Feed, Food waste, Mitigate, Information technology},
abstract = {Hunger is a global crisis and impacting the world for a very long time. Significant efforts have been made in the past by various organizations employing various means to address this persisting problem. Despite this, it still remains far from being resolved. Recently, the United Nations has published a report on global hunger, which claims that the global food production is fairly enough to feed the entire world with a population of about 7.3 billion. However, a major quantity of the food grown is not channeled appropriately and effectively to reach the needy and thus gets wasted unfortunately. Using the advanced computer technologies, we devise and develop a web-based computational framework (eFeed-Hungers.com) that serves as a bridge between the food waste and the hunger to mitigate the hunger issue; the food waste is the excess, unused, edible food, which otherwise is destined to the dumpster unfortunately. The eFeed-Hungers.com encourages and assists the food waste donation announcements with the least minimal efforts with the best possible outreach. Through the eFeed-Hungers.com, the food donations are globally searchable by the needy, with enough additional information imparted for quick decisions making to fetch the appropriate donated food. The eFeed-Hungers.com is envisioned to be a fully functional organization eventually with global outreach.}
}
@incollection{LAURINI2017223,
title = {11 - Geovisualization and Chorems},
editor = {Robert Laurini},
booktitle = {Geographic Knowledge Infrastructure},
publisher = {Elsevier},
pages = {223-246},
year = {2017},
isbn = {978-1-78548-243-4},
doi = {https://doi.org/10.1016/B978-1-78548-243-4.50011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781785482434500116},
author = {Robert Laurini},
keywords = {Cartograms, Chorems, Dashboards, DB access, Elementary chorems, Geovisualization, Maps, Smart cities, Visual analytics},
abstract = {Abstract:
The objective of this chapter will be to give some elements regarding the way to visualize geographic data and knowledge for decision-makers. Indeed, conventional cartography presents some limits and sometimes decision-makers require for novel types of visualization which can help them efficiently. Under the banner of visual analytics and geovisualization, new approaches are emerging, sometimes far from classical mapping.}
}
@article{KSHETRI201880,
title = {1 Blockchain’s roles in meeting key supply chain management objectives},
journal = {International Journal of Information Management},
volume = {39},
pages = {80-89},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217305248},
author = {Nir Kshetri},
keywords = {Auditability, Blockchain, Internet of things, Network effects, Supply chain, Sustainability},
abstract = {Arrival of blockchain is set to transform supply chain activities. Scholars have barely begun to systematically assess the effects of blockchain on various organizational activities. This paper examines how blockchain is likely to affect key supply chain management objectives such as cost, quality, speed, dependability, risk reduction, sustainability and flexibility. We present early evidence linking the use of blockchain in supply chain activities to increase transparency and accountability. Case studies of blockchain projects at various phases of development for diverse purposes are discussed. This study illustrates the various mechanisms by which blockchain help achieve the above supply chain objectives. Special emphasis has been placed on the roles of the incorporation of the IoT in blockchain-based solutions and the degree of deployment of blockchain to validate individuals’ and assets’ identities.}
}
@article{SOVACOOL201812,
title = {Promoting novelty, rigor, and style in energy social science: Towards codes of practice for appropriate methods and research design},
journal = {Energy Research & Social Science},
volume = {45},
pages = {12-42},
year = {2018},
note = {Special Issue on the Problems of Methods in Climate and Energy Research},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S2214629618307230},
author = {Benjamin K. Sovacool and Jonn Axsen and Steve Sorrell},
keywords = {Validity, Research methods, Research methodology, Interdisciplinary research, Research excellence},
abstract = {A series of weaknesses in creativity, research design, and quality of writing continue to handicap energy social science. Many studies ask uninteresting research questions, make only marginal contributions, and lack innovative methods or application to theory. Many studies also have no explicit research design, lack rigor, or suffer from mangled structure and poor quality of writing. To help remedy these shortcomings, this Review offers suggestions for how to construct research questions; thoughtfully engage with concepts; state objectives; and appropriately select research methods. Then, the Review offers suggestions for enhancing theoretical, methodological, and empirical novelty. In terms of rigor, codes of practice are presented across seven method categories: experiments, literature reviews, data collection, data analysis, quantitative energy modeling, qualitative analysis, and case studies. We also recommend that researchers beware of hierarchies of evidence utilized in some disciplines, and that researchers place more emphasis on balance and appropriateness in research design. In terms of style, we offer tips regarding macro and microstructure and analysis, as well as coherent writing. Our hope is that this Review will inspire more interesting, robust, multi-method, comparative, interdisciplinary and impactful research that will accelerate the contribution that energy social science can make to both theory and practice.}
}
@article{BRENTAN201877,
title = {Hybrid SOM+k-Means clustering to improve planning, operation and management in water distribution systems},
journal = {Environmental Modelling & Software},
volume = {106},
pages = {77-88},
year = {2018},
note = {Special Issue on Environmental Data Science. Applications to Air quality and Water cycle},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217301834},
author = {Bruno Brentan and Gustavo Meirelles and Edevar Luvizotto and Joaquin Izquierdo},
keywords = {Water supply systems, Classification, Self-organizing maps, -means clustering},
abstract = {With the advance of new technologies and emergence of the concept of the smart city, there has been a dramatic increase in available information. Water distribution systems (WDSs) in which databases can be updated every few minutes are no exception. Suitable techniques to evaluate available information and produce optimized responses are necessary for planning, operation, and management. This can help identify critical characteristics, such as leakage patterns, pipes to be replaced, and other features. This paper presents a clustering method based on self-organizing maps coupled with k-means algorithms to achieve groups that can be easily labeled and used for WDS decision-making. Three case-studies are presented, namely a classification of Brazilian cities in terms of their water utilities; district metered area creation to improve pressure control; and transient pressure signal analysis to identify burst pipes. In the three cases, this hybrid technique produces excellent results.}
}
@article{BORJI20181,
title = {Negative results in computer vision: A perspective},
journal = {Image and Vision Computing},
volume = {69},
pages = {1-8},
year = {2018},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2017.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0262885617301671},
author = {Ali Borji},
keywords = {Negative results, All results, Computer vision, Biological vision, Statistical testing},
abstract = {A negative result is when the outcome of an experiment or a model is not what is expected or when a hypothesis does not hold. Despite being often overlooked in the scientific community, negative results are results and they carry value. While this topic has been extensively discussed in other fields such as social sciences and biosciences, less attention has been paid to it in the computer vision community. The unique characteristics of computer vision, particularly its experimental aspect, call for a special treatment of this matter. In this manuscript, I will address what makes negative results important, how they should be disseminated and incentivized, and what lessons can be learned from cognitive vision research in this regard. Further, I will discuss matters such as experimental design, statistical hypothesis testing, explanatory versus predictive modeling, performance evaluation, model comparison, reproducibility of findings, the confluence of computer vision and human vision, as well as computer vision research culture.}
}
@article{MARKONIS20181,
title = {Global estimation of long-term persistence in annual river runoff},
journal = {Advances in Water Resources},
volume = {113},
pages = {1-12},
year = {2018},
issn = {0309-1708},
doi = {https://doi.org/10.1016/j.advwatres.2018.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0309170817303640},
author = {Y. Markonis and Y. Moustakis and C. Nasika and P. Sychova and P. Dimitriadis and M. Hanel and P. Máca and S.M. Papalexiou},
keywords = {River runoff, Long-term persistence, Long-range dependence, Self-Organizing Maps, Random forests, Catchment classification},
abstract = {Long-term persistence (LTP) of annual river runoff is a topic of ongoing hydrological research, due to its implications to water resources management. Here, we estimate its strength, measured by the Hurst coefficient H, in 696 annual, globally distributed, streamflow records with at least 80 years of data. We use three estimation methods (maximum likelihood estimator, Whittle estimator and least squares variance) resulting in similar mean values of H close to 0.65. Subsequently, we explore potential factors influencing H by two linear (Spearman's rank correlation, multiple linear regression) and two non-linear (self-organizing maps, random forests) techniques. Catchment area is found to be crucial for medium to larger watersheds, while climatic controls, such as aridity index, have higher impact to smaller ones. Our findings indicate that long-term persistence is weaker than found in other studies, suggesting that enhanced LTP is encountered in large-catchment rivers, were the effect of spatial aggregation is more intense. However, we also show that the estimated values of H can be reproduced by a short-term persistence stochastic model such as an auto-regressive AR(1) process. A direct consequence is that some of the most common methods for the estimation of H coefficient, might not be suitable for discriminating short- and long-term persistence even in long observational records.}
}
@article{KONDYLI2018215,
title = {Comparison of travel time measurement methods along freeway and arterial facilities},
journal = {Transportation Letters},
volume = {10},
number = {4},
pages = {215-228},
year = {2018},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2016.1245259},
url = {https://www.sciencedirect.com/science/article/pii/S1942786722001205},
author = {Alexandra Kondyli and Bryan {St. George} and Lily Elefteriadou},
keywords = {Travel time reliability, floating car, travel time accuracy},
abstract = {Travel time is an important performance measure used to assess traffic operational quality of various types of facilities. Previous efforts to estimate travel time have also compared model-estimated travel times to field-measured travel times using various sources of data. Given the variety and diversity of travel time measurement methods, it is important to evaluate the accuracy of the data obtained by each of them and to develop recommendations regarding their suitability in the validation of travel time estimation models. The research objective for this paper was to collect field data along several freeways and arterials and to evaluate the travel times obtained by STEWARD (Statewide Traffic Engineering Warehouse for Regionally Archived Data), INRIX, BlueTOAD (Bluetooth Travel-time Origination and Destination), and HERE. Benchmark data were collected with the use of an instrumented vehicle at five freeway segments and two arterial segments in Florida. The field-measured travel times were statistically compared with the travel times provided through various methods. The results suggest that the HERE traffic data provide better freeway travel time estimates compared to the remaining methods. In oversaturated conditions, STEWARD, INRIX and BlueTOAD data seem to underestimate travel times, while HERE data were found to be more accurate. For undersaturated freeways, STEWARD, INRIX and BlueTOAD were found to perform better than HERE. At the arterial sites BlueTOAD and HERE travel time data were analyzed and the analysis suggests that none of the methods is accurate, possibly due to the small sample size.}
}
@article{HARVEY2017152,
title = {Augmenting comprehension of geological relationships by integrating 3D laser scanned hand samples within a GIS environment},
journal = {Computers & Geosciences},
volume = {103},
pages = {152-163},
year = {2017},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417301772},
author = {A.S. Harvey and G. Fotopoulos and B. Hall and K. Amolins},
keywords = {3D laser scanner, Geology, GIS, Web GIS, Virtual database, Rocks, Minerals},
abstract = {Geological observations can be made on multiple scales, including micro- (e.g. thin section), meso- (e.g. hand-sized to outcrop) and macro- (e.g. outcrop and larger) scales. Types of meso-scale samples include, but are not limited to, rocks (including drill cores), minerals, and fossils. The spatial relationship among samples paired with physical (e.g. granulometric composition, density, roughness) and chemical (e.g. mineralogical and isotopic composition) properties can aid in interpreting geological settings, such as paleo-environmental and formational conditions as well as geomorphological history. Field samples are collected along traverses in the area of interest based on characteristic representativeness of a region, predetermined rate of sampling, and/or uniqueness. The location of a sample can provide relative context in seeking out additional key samples. Beyond labelling and recording of geospatial coordinates for samples, further analysis of physical and chemical properties may be conducted in the field and laboratory. The main motivation for this paper is to present a workflow for the digital preservation of samples (via 3D laser scanning) paired with the development of cyber infrastructure, which offers geoscientists and engineers the opportunity to access an increasingly diverse worldwide collection of digital Earth materials. This paper describes a Web-based graphical user interface developed using Web AppBuilder for ArcGIS for digitized meso-scale 3D scans of geological samples to be viewed alongside the macro-scale environment. Over 100 samples of virtual rocks, minerals and fossils populate the developed geological database and are linked explicitly with their associated attributes, characteristic properties, and location. Applications of this new Web-based geological visualization paradigm in the geosciences demonstrate the utility of such a tool in an age of increasing global data sharing.}
}
@article{ISMAIL201833,
title = {Mining productive-periodic frequent patterns in tele-health systems},
journal = {Journal of Network and Computer Applications},
volume = {115},
pages = {33-47},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518301474},
author = {Walaa N. Ismail and Mohammad Mehedi Hassan and Hessah A. Alsalamah and Giancarlo Fortino},
keywords = {Tele-health, Data mining, Productive periodic frequent patterns, Periodic patterns, Incremental database, Fp-growth},
abstract = {Recently, tele-health systems have gained attention from vast research fields because they facilitate remote monitoring of patients (e.g. vital sign data, physical activities, etc.) by utlizing various technologies such as body sensor network, wireless communications, multimedia and human-computer interactions without interrupting the quality of lifestyle. As tele-health generates a huge amount of healthcare data consisting of much useful information, finding hidden information from the data is an important task. The purpose of this work is to facilitate a real-time warning alarm in the context of tele-health remote monitoring using data mining techniques. This can be utilized for the e-wellbeing applications, for example, rehabilitation, early identification of therapeutic issues and emergency warning. In particular, we focus on mining Productive Periodic frequent patterns from incremental databases (such as vital sign data of patients) for various decision makings. Exploring the correlations between periodic frequent vital sign data or items is important since the inherent relationships between the items of patterns are relevant. To mine the correlated periodic frequent patterns from incremental databases, we introduce the productive (i.e. useful) periodic frequent patterns (PPFP) as the set of periodic frequent patterns with periodicities that result from the occurrence of correlated items. We finally design and develop an efficient PPFP mining technique that can mine the complete set of useful periodically occurring patterns in incremental databases. Numerous experiments were performed on both real and synthetic data set to judge the effectiveness of the proposed pattern mining procedure when contrasted with existing best in class approaches.}
}
@article{JIN2023104104,
title = {Machine learning techniques for pulmonary nodule computer-aided diagnosis using CT images: A systematic review},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104104},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104104},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422005638},
author = {Haizhe Jin and Cheng Yu and Zibo Gong and Renjie Zheng and Yinan Zhao and Quanwei Fu},
keywords = {Computed tomography, Pulmonary nodule, Computer-aided diagnosis, Machine learning, Deep learning},
abstract = {Objective
Early detection of pulmonary nodules is critical for the prevention and treatment of lung cancer.Concomitant with recent advancements in computer performance and intelligent algorithms, the efficacy of pulmonary nodule computer-aided diagnosis (CAD) has been continuously improving, and various algorithms have been proposed using different datasets. This study systematically analyzed and compared the performance of machine learning algorithms using the same dataset in the diagnosis of pulmonary nodules through a literature review.
Methods
The widely used LIDC-IDRI dataset and its subset LUNA16 were used as data objects.The SpringerLink, Science Direct, IEEE Xplore, and PubMed scientific databases were searched, and seventy-five papers were analyzed.
Results
Deep-learning-based CAD was found to be superior to conventional machine-learning-based CAD in terms of the number of published studies and algorithm performance.The best performances were as follows: feedforward neural network (FNN) and convolutional neural network (CNN) for detecting pulmonary nodules;region-based CNN (R-CNN) for the segmentation of pulmonary nodules;residual neural network (ResNet) for the classification of nodules and non-nodules; anddeep neural network (DNN) for the classification of benign and malignancy.
Conclusion
To further extend the application of CAD in clinical practice, the appropriate algorithm type should be used based on the characteristics of the task.The CAD process should be divided into logical stages and the optimal algorithm for each stage should be used to increase the reliability of the process.
Significance
The CAD performance of numerous algorithms on the same dataset is systematically compared and ideas for future exploration are provided.}
}
@article{HARSTINE2018s63,
title = {Review: Integrating a semen quality control program and sire fertility at a large artificial insemination organization},
journal = {Animal},
volume = {12},
pages = {s63-s74},
year = {2018},
issn = {1751-7311},
doi = {https://doi.org/10.1017/S1751731118000319},
url = {https://www.sciencedirect.com/science/article/pii/S1751731118000319},
author = {B.R. Harstine and M.D. Utt and J.M. DeJarnette},
keywords = {bull fertility, semen quality, bovine sperm, artificial insemination, spermatozoa},
abstract = {The technology available to assess sperm population characteristics has advanced greatly in recent years. Large artificial insemination (AI) organizations that sell bovine semen utilize many of these technologies not only for novel research purposes, but also to make decisions regarding whether to sell or discard the product. Within an AI organization, the acquisition, interpretation and utilization of semen quality data is often performed by a quality control department. In general, quality control decisions regarding semen sales are often founded on the linkages established between semen quality and field fertility. Although no one individual sperm bioassay has been successful in predicting sire fertility, many correlations to various in vivo fertility measures have been reported. The most powerful techniques currently available to evaluate semen are high-throughput and include computer-assisted sperm analysis and various flow cytometric analyses that quantify attributes of fluorescently stained cells. However, all techniques measuring biological parameters are subject to the principles of precision, accuracy and repeatability. Understanding the limitations of repeatability in laboratory analyses is important in a quality control and quality assurance program. Hence, AI organizations that acquire sizeable data sets pertaining to sperm quality and sire fertility are well-positioned to examine and comment on data collection and interpretation. This is especially true for sire fertility, where the population of AI sires has been highly selected for fertility. In the December 2017 sire conception rate report by the Council on Dairy Cattle Breeding, 93% of all Holstein sires (n=2062) possessed fertility deviations within 3% of the breed average. Regardless of the reporting system, estimates of sire fertility should be based on an appropriate number of services per sire. Many users impose unrealistic expectations of the predictive value of these assessments due to a lack of understanding for the inherent lack of precision in binomial data gathered from field sources. Basic statistical principles warn us of the importance of experimental design, balanced treatments, sampling bias, appropriate models and appropriate interpretation of results with consideration for sample size and statistical power. Overall, this review seeks to describe and connect the use of sperm in vitro bioassays, the reporting of AI sire fertility, and the management decisions surrounding the implementation of a semen quality control program.}
}