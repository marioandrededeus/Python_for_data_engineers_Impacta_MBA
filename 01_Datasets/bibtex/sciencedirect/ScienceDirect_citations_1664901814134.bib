@article{PAN2021103564,
title = {A BIM-data mining integrated digital twin framework for advanced project management},
journal = {Automation in Construction},
volume = {124},
pages = {103564},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103564},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000157},
author = {Yue Pan and Limao Zhang},
keywords = {Digital twin, Building information modeling (BIM), Process mining, Time series analysis},
abstract = {With the focus of smart construction project management, this paper presents a closed-loop digital twin framework under the integration of Building Information Modeling (BIM), Internet of Things (IoT), and data mining (DM) techniques. To be specific, IoT connects the physical and cyber world to capture real-time data for modeling and analyzing, and data mining methods incorporated in the virtual model aim to discover hidden knowledge in collected data. The proposed digital twin has been verified in a practical BIM-based project. Based on large inspection data from IoT devices, the 4D visualization and task-centered or worker-centered process model are built as the virtual model to simulate both the task execution and worker cooperation. Then, the high-fidelity virtual model is investigated by process mining and time series analysis. Results show that possible bottlenecks in the current process can be foreseen using the fuzzy miner, while the number of finished tasks in the next phase can be predicted by the multivariate autoregressive integrated moving average (ARIMAX) model. Consequently, tactic decision-making can realize to not only prevent possible failure in advance, but also arrange work and staffing reasonably to make the process adapt to changeable conditions. In short, the significance of this paper is to build a data-driven digital twin framework integrating with BIM, IoT, and data mining for advanced project management, which can facilitate data communication and exploration to better understand, predict, and optimize the physical construction operations. In future works, more complex cases with multiple data streams will be used to test the developed framework, and more detailed interpretations with the actual observations of construction activities will be given.}
}
@article{PAN2021103713,
title = {Automated process discovery from event logs in BIM construction projects},
journal = {Automation in Construction},
volume = {127},
pages = {103713},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103713},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001643},
author = {Yue Pan and Limao Zhang},
keywords = {Building information modeling, Process mining, Social network analysis, Performance evaluation, Construction management},
abstract = {To fully understand how a construction project actually proceeds, a novel framework for automated process discovery from building information modeling (BIM) event logs is developed. The significance of the work is to manage and optimize the complex construction process towards the ultimate goal of narrowing the gap between BIM and process mining. More specifically, meaningful information is retrieved from prepared event logs to build a participant-specific process model, and then the established model with executable semantics and fitness guarantees provides evidence in process improvement through identifying deviations, inefficiencies, and collaboration features. The proposed method has been validated in a case study, where the input is an as-planned event log from a real BIM construction project. The process model is created automatically by the inductive mining and fuzzy mining algorithms, which is then analyzed deeply under the joint use of conformance checking, frequency and bottleneck analysis, and social network analysis (SNA). The discovered knowledge contributes to revealing potential problems and evaluating the performance of workflows and participants objectively. In the discussion part, as-built data from the internet of things (IoT) deployment in construction site monitoring is automatically compared with the as-planned event log in the BIM platform to detect the actual delays. It turns out that the participant playing a central role in the network tends to overburden with heavier workloads, leading to more undesirable discrepancies and delays. As a result, extensive investigations based on process mining supports data-driven decision making to strategically smooth the construction process and increase collaboration opportunities, which also help in reducing the risk of project failure ahead of time.}
}
@article{SIWABESSY2021105121,
title = {Physical reality in planetary geomorphological inference and the pathway to a critical planetary geology},
journal = {Planetary and Space Science},
volume = {195},
pages = {105121},
year = {2021},
issn = {0032-0633},
doi = {https://doi.org/10.1016/j.pss.2020.105121},
url = {https://www.sciencedirect.com/science/article/pii/S0032063320303342},
author = {Andrew G. Siwabessy},
keywords = {Philosophy of geology, Philosophy of geography, Sociology of geology, Epistemology of geology, Critical planetary geology},
abstract = {I engage the philosophy of geology, and the philosophy of planetary geomorphology in particular, offering a perspective from the standpoint of a Mars geographer who is acquainted with both experimental and social research. I discuss the nature of geological reasoning and probe its somewhat paradoxical commitment to both a physical realism and to interpretative narrative construction, engaging particularly with how the interplay of these two factors are addressed in the works of the philosopher-geologist Victor Baker. I also draw attention to specific characteristics of knowledge construction in Martian geomorphology which systemically limit the attachment of our inferences to a physical reality. For instance I highlight a geospatial-geochemical dimorphism in the semantic content of available planetary data and several non-scientific controls on the knowledge construction process, including mission safety constraints and sociopolitical factors. Lastly, I find that physical geography has grown apart from planetary geomorphology in the decades since planetary geomorphology assumed its largely geological modern form. An academic siloing effect has followed suit. An immediate implication is that this contours the typical pathways of knowledge construction in our discipline, and I try to call attention to a few examples of this effect. As an effort to bridge this gap, I highlight the recent developments of the physical geographical subdisciplines of GIScience and critical physical geography. These disciplines offer opportunities for insight which might be fruitful for planetary geomorphologists, but their lines of inquiry have largely passed beneath our community’s radar. Specifically, in offering examples of what discourse in a critical planetary geology might look like, I hope to encourage the informal philosophical discussions already circulating in our community to pass into a formal space of debate.}
}
@article{NICOLAS2021102674,
title = {Natural language processing-based characterization of top-down communication in smart cities for enhancing citizen alignment},
journal = {Sustainable Cities and Society},
volume = {66},
pages = {102674},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102674},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720308891},
author = {Clément Nicolas and Jinwoo Kim and Seokho Chi},
keywords = {Smart city, Top-down communication, Citizen alignment, Natural language processing, Web scraping, Topic modeling, Latent Dirichlet allocation},
abstract = {Many city governments have implemented promising smart initiatives to make cities more efficient, livable, and ecological. To harness the full potential of smart city initiatives, it is vital for policymakers to align citizens with the project objectives. This study comprehensively characterizes and classifies top-down announcements formulated by city developers into six alignment categories (i.e., smart economy, smart people, smart governance, smart mobility, smart environment, and smart living) using natural language processing. The proposed framework consists of five main processes: (1) web scraping-based extraction of announcements of four smart cities – Boston, Helsinki, Seoul, and Taipei, (2) text data preprocessing, (3) latent Dirichlet allocation-based modeling of strategic topics, (4) quantification of inter-topic similarities using Hellinger distance, and (5) comparison of top-down communication trends with real-world levels of urban performance. Through the comparative analysis of top-down communication trends and actual urban performances, the top-down discourses of smart cities were deconstructed as a reflection of wider political programs developed to enhance citizen alignment and urban performances. Furthermore, inter-topic similarities were also quantified to reflect whether communication strategies are multidisciplinary and city-tailored. In conclusion, the findings of this study can enhance our understanding and provide workable guidance for future smart city development.}
}
@article{GOAD2021103292,
title = {Privacy and the Internet of Things−An experiment in discrete choice},
journal = {Information & Management},
volume = {58},
number = {2},
pages = {103292},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103292},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618310553},
author = {David Goad and Andrew T. Collins and Uri Gal},
keywords = {Internet of Things, IoT, Privacy, Discrete choice methods},
abstract = {The Internet of Things (IoT) is the concept that everyday devices are connected to the Internet generating data about us and the world around us. With the number of devices connected directly to the Internet expected to be three times the number of people by 2020, the potential for a reduction in personal privacy is evident. This research fills a gap in the literature by conducting a quantitative analysis of people’s privacy preferences as it relates to the IoT. Our findings provide potential guidance to practitioners in their IoT architectural design and increase our understanding of privacy preference overall.}
}
@article{FENG2021106033,
title = {A comprehensive review on recent applications of unmanned aerial vehicle remote sensing with various sensors for high-throughput plant phenotyping},
journal = {Computers and Electronics in Agriculture},
volume = {182},
pages = {106033},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106033},
url = {https://www.sciencedirect.com/science/article/pii/S016816992100051X},
author = {Lei Feng and Shuangshuang Chen and Chu Zhang and Yanchao Zhang and Yong He},
keywords = {Unmanned aerial vehicle, Remote sensing, High-throughput phenotyping, Sensors, Applications review},
abstract = {High-throughput phenotyping has been widely studied in plant science to monitor plant growth and analyze the influence of genotypes and environment on plant growth. To meet the demand of large-scale high-throughput phenotyping, unmanned aerial vehicles (UAVs) have been developed for near-ground remote sensing. UAVs based remote sensing has been used for high-throughput phenotyping of various traits of plants. This review focused on the applications of UAVs based remote sensing of different traits with different phenotyping sensors. In this review, the UAVs platforms and the phenotyping sensors were briefly introduced. The applications of UAVs to obtain and analyze plant phenotype traits were introduced and summarized by the traits in a more comprehensive way. A comparison of different phenotyping sensors was conducted. Furthermore, the challenges and future prospects of phenotype information acquisition and data analysis using UAVs as remote sensing platforms were also discussed. Since the current studies from various countries and researchers were fragmented to just explore the feasibility of UAVs based high-throughput phenotyping, this review aimed to provide the researchers and readers the current applications of UAVs for high-throughput phenotyping and how the studies were conducted, provide guidelines for future studies.}
}
@article{ABOELMAGED2021102247,
title = {Predicting subjective well-being among mHealth users: a readiness – value model},
journal = {International Journal of Information Management},
volume = {56},
pages = {102247},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102247},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220314468},
author = {Mohamed Aboelmaged and Gharib Hashem and Samar Mouakket},
keywords = {mHealth, Subjective well-being, Utilitarian value, Hedonic value, Technology readiness, Post-adoption},
abstract = {mHealth applications (MHA) have recently attracted great attention from various stakeholders as they are indeed important means to enhance users’ subjective well-being. While prior research has mainly focused on intention or adoption phase, little work has empirically examined the post-adoption effects of MHA with scarce attention given to the well-being outcome. Actual users are likely to conceive the values of MHA based mainly on their direct experience with it. In this paper, the dimensions of users’ technology readiness are regarded as major impetuses for perceived utilitarian and hedonic values, which in turn influence subjective well-being among MHA users. The proposed readiness-value model is analyzed using survey data collected from 731 users of MHA. The findings show that the model significantly predicts users’ subjective well-being considering that utilitarian value is more important for male users, whereas hedonic value has a more salient effect for female users. It also reveals that enablers of technology readiness (i.e., innovativeness and optimism) exert a stronger influence than that of inhibitors (i.e., discomfort and insecurity) on the perceived values of MHA. These results have essential implications for theory and practice.}
}
@article{2021105598,
title = {EU UPDATE OF THE COMPUTER LAW & SECURITY REVIEW},
journal = {Computer Law & Security Review},
volume = {42},
pages = {105598},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105598},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000716}
}
@article{MEHMOOD2021106845,
title = {Customizing SVM as a base learner with AdaBoost ensemble to learn from multi-class problems: A hybrid approach AdaBoost-MSVM},
journal = {Knowledge-Based Systems},
volume = {217},
pages = {106845},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106845},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001088},
author = {Zafar Mehmood and Sohail Asghar},
keywords = {Machine learning classifiers, Class overlapping, Imbalanced distribution of data, Imbalanced problem, Decomposition techniques},
abstract = {Learning from a multi-class problem has not been an easy task for most of the classifiers, because of multiple issues. In the complex multi-class scenarios, samples of different classes overlap with each other by sharing attribute, and hence the visibility of least represented samples decrease even more. Learning from imbalanced data studied extensively in the research community, however, the overlapping issues and the co-occurrence impact of overlapping with data imbalance have received comparatively less attention, even though their joint impact is more thoughtful on classifiers’ performance. In this paper, we introduce a modified SVM, MSVM to use as a base classifier with the AdaBoost ensemble classifier (MSVM-AdB) to enhance the learning capability of the ensemble classifier. To implement the proposed technique, we divide the multi-class dataset into overlapping and non-overlapping region. The overlapping region is further filter into the Critical and less Critical region depending upon their sample contribution in the overlapped region. The MSVM is designed to map the overlapped samples in a higher dimension by modifying the kernel mapping function of the standard SVM by using the mean distance of the Critical region samples. To highlight the learning enhancement of the MSVM-AdB, we use 20 real datasets with varying imbalance ratio and the overlapping degree to compare the significance of the AdaBoost-MSVM with the standard SVM, and AdaBoost with standard base classifiers. Experimental results show the superiority of the MSVM-AdB on a collection of benchmark datasets to its standard counterpart classifiers.}
}
@incollection{2021527,
title = {Index},
editor = {Lei Xing and Maryellen L. Giger and James K. Min},
booktitle = {Artificial Intelligence in Medicine},
publisher = {Academic Press},
pages = {527-544},
year = {2021},
isbn = {978-0-12-821259-2},
doi = {https://doi.org/10.1016/B978-0-12-821259-2.00030-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212592000302}
}
@article{THOMPSON2021101628,
title = {Platform, or technology project? A spectrum of six strategic ‘plays’ from UK government IT initiatives and their implications for policy},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101628},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101628},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000642},
author = {Mark Thompson and Will Venters},
keywords = {Platform innovation, Platform strategy, Government policy, Digital innovation, UK},
abstract = {There is a markedly broad range of definitions and illustrative examples of the role played by governments themselves within the literature on government platforms. In response we conduct an inductive and deductive qualitative review of the literature to clarify this landscape and so to develop a typology of six definitions of government platforms, organised within three genres along a spectrum from fully centralised, through to fully decentralised. For each platform definition we offer illustrative ‘mini-cases’ drawn from the UK government experience as well as further insights and implications for each genre, drawn from the broader information systems literature on platforms. A range of benefits, risks, governance challenges, policy recommendations, and suggestions for further research are then identified and discussed.}
}
@incollection{2021697,
title = {Index},
editor = {Anthony J. Martyr and David R. Rogers},
booktitle = {Engine Testing (Fifth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Fifth Edition},
address = {Oxford},
pages = {697-722},
year = {2021},
isbn = {978-0-12-821226-4},
doi = {https://doi.org/10.1016/B978-0-12-821226-4.00026-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212264000267}
}
@article{BREWIN2021103604,
title = {Sensing the ocean biological carbon pump from space: A review of capabilities, concepts, research gaps and future developments},
journal = {Earth-Science Reviews},
volume = {217},
pages = {103604},
year = {2021},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2021.103604},
url = {https://www.sciencedirect.com/science/article/pii/S0012825221001045},
author = {Robert J.W. Brewin and Shubha Sathyendranath and Trevor Platt and Heather Bouman and Stefano Ciavatta and Giorgio Dall'Olmo and James Dingle and Steve Groom and Bror Jönsson and Tihomir S. Kostadinov and Gemma Kulk and Marko Laine and Victor Martínez-Vicente and Stella Psarra and Dionysios E. Raitsos and Katherine Richardson and Marie-Hélène Rio and Cécile S. Rousseaux and Joe Salisbury and Jamie D. Shutler and Peter Walker},
keywords = {Ocean, Carbon cycle, Satellite, Biology},
abstract = {The element carbon plays a central role in climate and life on Earth. It is capable of moving among the geosphere, cryosphere, atmosphere, biosphere and hydrosphere. This flow of carbon is referred to as the Earth's carbon cycle. It is also intimately linked to the cycling of other elements and compounds. The ocean plays a fundamental role in Earth's carbon cycle, helping to regulate atmospheric CO2 concentration. The ocean biological carbon pump (OBCP), defined as a set of processes that transfer organic carbon from the surface to the deep ocean, is at the heart of the ocean carbon cycle. Monitoring the OBCP is critical to understanding how the Earth's carbon cycle is changing. At present, satellite remote sensing is the only tool available for viewing the entire surface ocean at high temporal and spatial scales. In this paper, we review methods for monitoring the OBCP with a focus on satellites. We begin by providing an overview of the OBCP, defining and describing the pools of carbon in the ocean, and the processes controlling fluxes of carbon between the pools, from the surface to the deep ocean, and among ocean, land and atmosphere. We then examine how field measurements, from ship and autonomous platforms, complement satellite observations, provide validation points for satellite products and lead to a more complete view of the OBCP than would be possible from satellite observations alone. A thorough analysis is then provided on methods used for monitoring the OBCP from satellite platforms, covering current capabilities, concepts and gaps, and the requirement for uncertainties in satellite products. We finish by discussing the potential for producing a satellite-based carbon budget for the oceans, the advantages of integrating satellite-based observations with ecosystem models and field measurements, and future opportunities in space, all with a view towards bringing satellite observations into the limelight of ocean carbon research.}
}
@article{OMETOV2021108074,
title = {A Survey on Wearable Technology: History, State-of-the-Art and Current Challenges},
journal = {Computer Networks},
volume = {193},
pages = {108074},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108074},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001651},
author = {Aleksandr Ometov and Viktoriia Shubina and Lucie Klus and Justyna Skibińska and Salwa Saafi and Pavel Pascacio and Laura Flueratoru and Darwin Quezada Gaibor and Nadezhda Chukhno and Olga Chukhno and Asad Ali and Asma Channa and Ekaterina Svertoka and Waleed Bin Qaim and Raúl Casanova-Marqués and Sylvia Holcer and Joaquín Torres-Sospedra and Sven Casteleyn and Giuseppe Ruggeri and Giuseppe Araniti and Radim Burget and Jiri Hosek and Elena Simona Lohan},
keywords = {Wearables, Communications, Standardization, Privacy, Security, Data processing, Interoperability, User adoption, Localization, Classification, Future perspective},
abstract = {Technology is continually undergoing a constituent development caused by the appearance of billions new interconnected “things” and their entrenchment in our daily lives. One of the underlying versatile technologies, namely wearables, is able to capture rich contextual information produced by such devices and use it to deliver a legitimately personalized experience. The main aim of this paper is to shed light on the history of wearable devices and provide a state-of-the-art review on the wearable market. Moreover, the paper provides an extensive and diverse classification of wearables, based on various factors, a discussion on wireless communication technologies, architectures, data processing aspects, and market status, as well as a variety of other actual information on wearable technology. Finally, the survey highlights the critical challenges and existing/future solutions.}
}
@incollection{2021769,
title = {Subject Index},
editor = {Marc Kéry and J. Andrew Royle},
booktitle = {Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in R and BUGS},
publisher = {Academic Press},
pages = {769-787},
year = {2021},
isbn = {978-0-12-823768-7},
doi = {https://doi.org/10.1016/B978-0-12-809585-0.20002-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095850200021}
}
@incollection{TAWHID2021215,
title = {Chapter 13 - Machine learning for optimizing healthcare resources},
editor = {Pardeep Kumar and Yugal Kumar and Mohamed A. Tawhid},
booktitle = {Machine Learning, Big Data, and IoT for Medical Informatics},
publisher = {Academic Press},
pages = {215-239},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-821777-1},
doi = {https://doi.org/10.1016/B978-0-12-821777-1.00020-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217771000203},
author = {Abdalrahman Tawhid and Tanya Teotia and Haytham Elmiligi},
keywords = {Chronic diseases, Public Health Agency of Canada, Diabetic patients, Cross-validation},
abstract = {Optimizing healthcare resources is a major goal for any healthcare administrator. The significance of such a goal becomes very clear during pandemics. Access to such resources at the right time affects the quality of healthcare services and provides alternative treatments that can save patients’ lives. Achieving this goal becomes more challenging when there are large number of patients who have chronic diseases. Machine learning algorithms provide a very promising solution that can help healthcare administrators make the right decision at the right time. Machine learning model can predict the progress of pandemics, classify a patient with well-defined symptoms as contagious or not, and can also predict the number of patients who will be hospitalized in the future. This chapter shows how to utilize machine learning algorithms to create a models that can predict some of the key issues in healthcare systems. The discussion in the chapter relates to COVID-19 pandemic and highlights the solutions offered by machine learning in such scenarios. The chapter also highlights the significance of feature engineering and its impact on the accuracy of machine learning models. The chapter ends with two case studies. The first case study shows how to build a prediction model that can predict the number of diabetic patients who will visit certain hospitals in a specific geographic location in future years. The second case study analyzes health records during the COVID-19 pandemic.}
}
@incollection{2021703,
title = {Index},
editor = {David Alderton and Scott A. Elias},
booktitle = {Encyclopedia of Geology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {703-796},
year = {2021},
isbn = {978-0-08-102909-1},
doi = {https://doi.org/10.1016/B978-0-08-102908-4.09958-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081029084099586}
}
@incollection{DONNELLY2021363,
title = {Chapter 17 - The role of future science and technologies in water management},
editor = {Barry T. Hart and Nick R. Bond and Neil Byron and Carmel A. Pollino and Michael J. Stewardson},
booktitle = {Murray-Darling Basin, Australia},
publisher = {Elsevier},
pages = {363-388},
year = {2021},
volume = {1},
series = {Ecohydrology from Catchment to Coast},
issn = {25892371},
doi = {https://doi.org/10.1016/B978-0-12-818152-2.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128181522000176},
author = {Chantal Donnelly and Leo Lymburner and Ulrike Bende-Michl and Andrew Frost and Eva Rodriguez},
keywords = {Future science, Technology, Water resources information, Management},
abstract = {Water decision-making in the Murray–Darling Basin by government agencies, utilities, water holders, and farmers is complex and involves use of technical, environmental, economic, social, and cultural information. Confidence in the decisions is dependent upon the quality of the information, the transparency of the process used to generate the information, the availability of the data and information used, and the quality and transparency of the models used to bring together different data and information sources. This chapter discusses likely scenarios for how current and future science and technology might improve information availability in the Murray–Darling Basin and contribute to improved decision-making. Possible future changes to information availability are discussed in three decision-making timeframes: near-term (now to next fortnight), seasonal (months to seasons ahead), and strategic (climatological timescales).}
}
@article{ABOUELKALAM2021100394,
title = {Securing SCADA and critical industrial systems: From needs to security mechanisms},
journal = {International Journal of Critical Infrastructure Protection},
volume = {32},
pages = {100394},
year = {2021},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2020.100394},
url = {https://www.sciencedirect.com/science/article/pii/S1874548220300585},
author = {Anas {Abou el Kalam}},
keywords = {SCADA, Protection, Security, Access control, Integrity, Intrusion tolerance, Self-healing, Availability, Threat, Risks, Security policies and models},
abstract = {Supervisory control and data acquisition (SCADA) systems are used in critical infrastructure to control vital sectors such as smart grids, oil pipelines, water treatment, chemical manufacturing plants, etc. Any malicious or accidental intrusion could cause dramatic human, material and economic damages. Thus, the security of the SCADA is very important, not only to keep the continuity of services (i.e., availability) against hostile and cyber-terrorist attacks, but also to ensure the resilience and integrity of processes and actions. Dealing with this issue, this paper discusses SCADA vulnerabilities and security threats, with a focus on recent ones. Then, we define a holistic methodology to derive the suitable security mechanisms for this kind of critical systems. Our methodology starts by identifying the security needs and objectives, specifying the security policies and models, deriving the adapted architecture and, finally, implementing the security mechanisms that satisfy the needs and cover the risks. We focus on the modelling step by proposing the new CI-OrBAC model. In this paper, we focused on securing communication and protecting SCADA against both internal and external threats while satisfying the self-healing, intrusion tolerance, integrity, scalability and collaboration needs.}
}
@article{JIANG2021103210,
title = {A selective ensemble model for cognitive cybersecurity analysis},
journal = {Journal of Network and Computer Applications},
volume = {193},
pages = {103210},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103210},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002125},
author = {Yuning Jiang and Yacine Atif},
keywords = {Information security, Vulnerability analysis, Data correlation, Machine learning, Ensemble, Data mining, Database management},
abstract = {Dynamic data-driven vulnerability assessments face massive heterogeneous data contained in, and produced by SOCs (Security Operations Centres). Manual vulnerability assessment practices result in inaccurate data and induce complex analytical reasoning. Contemporary security repositories’ diversity, incompleteness and redundancy contribute to such security concerns. These issues are typical characteristics of public and manufacturer vulnerability reports, which exacerbate direct analysis to root out security deficiencies. Recent advances in machine learning techniques promise novel approaches to overcome these notorious diversity and incompleteness issues across massively increasing vulnerability reports corpora. Yet, these techniques themselves exhibit varying degrees of performance as a result of their diverse methods. We propose a cognitive cybersecurity approach that empowers human cognitive capital along two dimensions. We first resolve conflicting vulnerability reports and preprocess embedded security indicators into reliable data sets. Then, we use these data sets as a base for our proposed ensemble meta-classifier methods that fuse machine learning techniques to improve the predictive accuracy over individual machine learning algorithms. The application and implication of this methodology in the context of vulnerability analysis of computer systems are yet to unfold the full extent of its potential. The proposed cognitive security methodology in this paper is shown to improve performances when addressing the above-mentioned incompleteness and diversity issues across cybersecurity alert repositories. The experimental analysis conducted on actual cybersecurity data sources reveals interesting tradeoffs of our proposed selective ensemble methodology, to infer patterns of computer system vulnerabilities.}
}
@article{ISLAM2021128297,
title = {A global review of consumer behavior towards e-waste and implications for the circular economy},
journal = {Journal of Cleaner Production},
volume = {316},
pages = {128297},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128297},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621025129},
author = {Md Tasbirul Islam and Nazmul Huda and Alex Baumber and Rezaul Shumon and Atiq Zaman and Forkan Ali and Rumana Hossain and Veena Sahajwalla},
keywords = {Consumer behavior, Waste electrical and electronic equipment (WEEE), Sustainable production and consumption, Disposal, Recycling, Literature review},
abstract = {To tackle the alarming increase in e-waste or end-of-life (EoL) electronic products, consumer behavior towards the end of their useful life needs to be thoroughly studied. End users or consumers are the starting point where e-waste starts its journey into several paths within the circular economy (CE), such as repair, reuse, remanufacturing, and recycling. E-waste often ends up in landfill due to improper disposal of e-waste with household waste by consumers. Studying consumer behavior allows for the identification of appropriate approaches to achieve CE. Numerous academic journal papers have been published concerning consumers' e-waste-related knowledge and awareness, and behavior on consumption, disposal, storage, recycling, and repair. Substantial knowledge gap exists around how understandings of consumer behavior around e-waste may be integrated into the CE model. This article aims to reduce this gap by reviewing 109 research papers published in international peer-reviewed journals identified in the Web of Science (WoS) core collection database, using content analysis methodology to analyze and review the articles. The study aims to provide invaluable input for developing a more consumer-centric CE framework for both policymakers and researchers seeking to advance knowledge and implementation strategies around e-waste. This is one of the earliest systematic reviews of studies on consumer behavior around e-waste. The study results show that consumers' disposal and recycling behaviors are the two main areas of research interest in the studies reviewed. In contrast, reuse and repair behavior were investigated to a lesser extent. In this study, several research gaps and areas for future research are identified, along with suggestions for a CE framework focusing on the e-waste sector that, encompasses policy initiatives and business model innovations. The identified studies presented here offer a valuable starting point for researchers who are starting to work on consumer behavior-related e-waste research.}
}
@article{DEBAUCHE2021100378,
title = {Data management and internet of things : A methodological review in smart farming},
journal = {Internet of Things},
volume = {14},
pages = {100378},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100378},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000226},
author = {Olivier Debauche and Jean-Philippe Trani and Saïd Mahmoudi and Pierre Manneback and Jérôme Bindelle and Sidi Ahmed Mahmoudi and Adriano Guttadauria and Frédéric Lebeau},
keywords = {Internet of things, Network protocols, Technological selection methodology, Cloud architecture, Security, Smart farming},
abstract = {Introduction. In the field of research, we are familiar to employ ready-to-use commercial solutions. This bibliographic review highlights the various technological paths that can be used in the context of agriculture digitalization and illuminates the reader on the capacities and limits of each one. Literature. Based on a literature review that we conducted, we describe the main components of the Internet of Things. Also, we analyzed the different technological pathways used by researchers to develop their projects. Finally, these versatile approaches are summarized in the form of tables and a methodological flowchart of communication protocols choices. Conclusions. In this article, we propose a methodology and a reflection on the technological choices and their implication on the durability and valorization of research projects in the field of smart agriculture.}
}
@incollection{2021681,
title = {Appendix},
editor = {M.Rafiqul Islam and M.Enamul Hossain},
booktitle = {Drilling Engineering},
publisher = {Gulf Professional Publishing},
pages = {681-740},
year = {2021},
series = {Sustainable Oil and Gas Development Series},
isbn = {978-0-12-820193-0},
doi = {https://doi.org/10.1016/B978-0-12-820193-0.15001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128201930150014}
}
@article{SKYDT2021108691,
title = {A probabilistic sequence classification approach for early fault prediction in distribution grids using long short-term memory neural networks},
journal = {Measurement},
volume = {170},
pages = {108691},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.108691},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120311994},
author = {Mathis Riber Skydt and Mads Bang and Hamid Reza Shaker},
keywords = {Fault prediction, Predictive maintenance, Grid management, Risk assessment, Neural networks, LSTM},
abstract = {As the global power grid must undergo a profound transformation in the coming decades to ensure reliable and cost-effective operation in a system with large shares of intermittent renewable energy generation, a critical element will be to leverage advanced data-driven predictive tools to optimise grid management activities. As it is expected that existing grids will be operated more to their limits, it is important to obtain better operational insights and estimations of the time to equipment failure to provide useful operational guidance and maintenance prioritisation support for grid operators. In this regard, this paper proposes a novel and real-time applicable method for fault prediction in 10 kV underground oil-insulated power cables using low-resolution data from a real case study from a Danish distribution system operator. The developed method is based on a sequence classification approach using long short-term memory neural networks where three different operational states are defined (Normal, Early warning, and Critical warning) to allow for prediction flexibility and better indication of the presence of systemic faults. Moreover, to enhance the data foundation, this paper investigates a Virtual Sample Generation method based on an adaptive Gaussian distribution. The capability of the proposed method yields satisfying results with prediction accuracy on the test set reaching as high as ~90%, hence proving the usefulness of the proposed approach and paving the way for smarter maintenance protocols.}
}
@article{YIN2021312,
title = {Field data analysis and risk assessment of gas kick during industrial deepwater drilling process based on supervised learning algorithm},
journal = {Process Safety and Environmental Protection},
volume = {146},
pages = {312-328},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2020.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0957582020316815},
author = {Qishuai Yin and Jin Yang and Mayank Tyagi and Xu Zhou and Xinxin Hou and Bohan Cao},
keywords = {Industrial deep-water drilling, Gas kick, Field data analysis, Risk assessment, Early gas, Kick detection, Supervised learning},
abstract = {During industrial offshore deep-water drilling process, gas kick event occurs frequently due to extremely narrow Mud Weight (MW) window (minimum 0.01sg) and negligible safety margins for the well control purposes. Further, traditional gas kick detection methods in such environments have significant time-lag and can often lead to severe well control issues, and occasionally to well blowouts or borehole abandonment. In this study, firstly, the raw field data is processed through data collection, data cleaning, feature scaling, outlier detection, data labeling and dataset splitting. Additionally, a novel data labeling criterion for gas kick risks is proposed where five kick risks (Indicated by different colors in this study) are defined based on three key indicators: differential flow out (DFO), kick gain volume (Vol), and kick duration time (Time). Kick risk status represents one of the following cases: Case 0 - No indicators are activated (Green), Case 1 - Multi-drilling parameters deviation or DFO is activated (Orange), Case 2 - DFO and Vol are simultaneously activated (Light Red), Case 3 - DFO and Time are simultaneously activated (Light Red), Case 4 - DFO, Vol and Time alarms are simultaneously activated (Dark Red). Then, a novel data mining method using Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) is presented for early detection of gas kick events by analyzing time series data from field drilling process. The network parameters such as number of hidden layers and number of neurons are initialized to build the LSTM network. The learned LSTM model is evaluated using the testing set, and the best LSTM model (six (6)-layers eighty (80)-nodes (6 L*80 N)) is optimally selected and deployed. The accuracy of deployed LSTM model is 87 % in the testing dataset, which is reliable enough to identify the kick fault during the deep-water drilling field operation. Lastly, the LSTM model detected the gas kick events earlier than the “Tank Volume” detection method in several representative case studies to conclude that the application of LSTM model can potentially improve well control safety in the deep-water wells with narrow MW windows.}
}
@article{WANG2021607,
title = {Knowledge graph quality control: A survey},
journal = {Fundamental Research},
volume = {1},
number = {5},
pages = {607-626},
year = {2021},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2021.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S2667325821001655},
author = {Xiangyu Wang and Lyuzhou Chen and Taiyu Ban and Muhammad Usman and Yifeng Guan and Shikang Liu and Tianhao Wu and Huanhuan Chen},
keywords = {Knowledge graph, Quality control, Quality evaluation, Quality enhancement},
abstract = {A knowledge graph (KG), a special form of semantic network, integrates fragmentary data into a graph to support knowledge processing and reasoning. KG quality control is important to the utility of KGs. It is essential to investigate KG quality and the parameters influencing KG quality to better understand its quality control. Although many works have been conducted to evaluate the dimensions of KG quality, quality control of the construction process, and enhancement methods for quality, a comprehensive literature review has not been presented on this topic. This paper intends to fill this research gap by presenting a comprehensive survey on the quality control of KGs. First, this paper defines six main evaluation dimensions of KG quality and investigates their correlations and differences. Second, quality control treatments during KG construction are introduced from the perspective of these dimensions of KG quality. Third, the quality enhancement of a constructed KG is described from various dimensions. This paper ultimately aims to promote the research and applications of KGs.}
}
@article{TSEKOS2021105180,
title = {Estimation of lignocellulosic biomass pyrolysis product yields using artificial neural networks},
journal = {Journal of Analytical and Applied Pyrolysis},
volume = {157},
pages = {105180},
year = {2021},
issn = {0165-2370},
doi = {https://doi.org/10.1016/j.jaap.2021.105180},
url = {https://www.sciencedirect.com/science/article/pii/S0165237021001662},
author = {C. Tsekos and S. Tandurella and W. {de Jong}},
keywords = {Pyrolysis, Artificial neural networks, Biomass modelling},
abstract = {As the push towards more sustainable ways to produce energy and chemicals intensifies, efforts are needed to refine and optimize the systems that can give an answer to these needs. In the present work, the use of neural networks as modelling tools for lignocellulosic biomass pyrolysis main products yields estimation was evaluated. In order to achieve this, the most relevant compositional and reaction parameters for lignocellulosic biomass pyrolysis were reviewed and their effect over the main products yields was assessed. Based on relevant literature data, a database was set up, containing parameters and experimental results from 32 published studies for a total of 482 samples, including both fast and slow pyrolysis experiments performed on a heterogeneous collection of lignocellulosic biomasses. The parameters that in the database configured as best predictors for the solid, liquid and gaseous products were determined through preliminary tests and were then used to build reduced models, one for each of the main products, which use five parameters instead of the full set for the estimation of yields. The procedures included hyperparameter optimizations steps. The performances of these reduced models were compared to those of the ones obtained using the full set of parameters as inputs by using the root mean squared error (RMSE) as metric. For both the char and gas products, the best results were consistently achieved by the reduced versions of the network (RMSE 5.1 wt% ar and 5.6 wt% ar respectively), while for the liquid product the best result was given by the full network (RMSE 6.9 wt% ar) indicating substantial value in proper selection of the input features. In general, the char models were the best performing ones. Additional models for the liquid and gas product featuring char as additional input to the system were also devised and obtained better performance (RMSE 5.5 wt% ar and 4.9 wt% ar respectively) compared to the original ones. Models based on single studies were also included in order to showcase both the capabilities of the tool and the challenges that arise when trying to build a generalizable model of this kind. Overall, artificial neural networks were shown to be an interesting tool for the construction of setup-unspecific biomass pyrolysis product yield models. The obstacles standing currently in the way of a more accurate modelling of the system were highlighted, along with certain literature discrepancies, which hinder reliable quantitative comparison of experimental conditions and results among separate studies.}
}
@article{GUO2021102219,
title = {A lightweight verifiable trust based data collection approach for sensor–cloud systems},
journal = {Journal of Systems Architecture},
volume = {119},
pages = {102219},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102219},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121001545},
author = {Jiawei Guo and Haoyang Wang and Wei Liu and Guosheng Huang and Jinsong Gui and Shaobo Zhang},
keywords = {Sensor–cloud systems, Trustworthiness, Data collection, Mobile vehicles, Unmanned aerial vehicles},
abstract = {A Lightweight Verifiable Trust based Data Collection (LVT-DC) approach is proposed to obtain credible data for mobile vehicles network, in which, a large number of IoT devices are deployed in smart city, and Mobile Vehicles (MVs) collect data from IoT devices and report data to cloud to construct various applications. First, some IoT devices are selected as core-IoT device whose data more than others. Then, Unmanned Aerial Vehicles (UAV) delivers a short verification code to each core-IoT devices and collects its data, and it embeds verification code into the data when the MVs collect it in order for cloud to check. Last, a verifiable trust inference method is proposed which includes two types of trust calculations. (a) Direct trust: MVs If the reported data cannot recover the verification code or is inconsistent with the UAV collection result, reduce the trust of the MVs, (b) Trust inference: If the data of a non-core-IoT devices reported by a MVs is inconsistent with the trusted MVs, then reduce its trust. After a large number of experimental results, the LVT-DC approach can quickly and accurately identify the credibility of the data collector and ensure credible data collection.}
}
@article{MOHAMMADI2021102983,
title = {A comprehensive survey and taxonomy of the SVM-based intrusion detection systems},
journal = {Journal of Network and Computer Applications},
volume = {178},
pages = {102983},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.102983},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000102},
author = {Mokhtar Mohammadi and Tarik A. Rashid and Sarkhel H.Taher Karim and Adil Hussain Mohammed Aldalwie and Quan Thanh Tho and Moazam Bidaki and Amir Masoud Rahmani and Mehdi Hosseinzadeh},
keywords = {SVM, Anomaly, Multi-class SVM, Feature selection, Intrusion detection, PCA},
abstract = {The increasing number of security attacks have inspired researchers to employ various classifiers, such as support vector machines (SVMs), to deal with them in Intrusion detection systems (IDSs). This paper presents a comprehensive study and investigation of the SVM-based intrusion detection and feature selection systems proposed in the literature. It first presents the essential concepts and background knowledge about security attacks, IDS, and SVM classifiers. It then provides a taxonomy of the SVM-based IDS schemes and describes how they have adapted numerous types of SVM classifiers in detecting various types of anomalies and intrusions. Moreover, it discusses the main contributions of the investigated schemes and highlights the algorithms and techniques combined with the SVM to enhance its detection rate and accuracy. Finally, different properties and limitations of the SVM-based IDS schemes are discussed.}
}
@article{JIANG2021105141,
title = {Utility of integrated IMERG precipitation and GLEAM potential evapotranspiration products for drought monitoring over mainland China},
journal = {Atmospheric Research},
volume = {247},
pages = {105141},
year = {2021},
issn = {0169-8095},
doi = {https://doi.org/10.1016/j.atmosres.2020.105141},
url = {https://www.sciencedirect.com/science/article/pii/S0169809520310772},
author = {Shanhu Jiang and Linyong Wei and Liliang Ren and Chong-Yu Xu and Feng Zhong and Menghao Wang and Linqi Zhang and Fei Yuan and Yi Liu},
keywords = {IMERG, GLEAM, Standardized Precipitation Evapotranspiration Index (SPEI), Drought monitoring, Mainland China},
abstract = {In this paper, we comprehensively evaluated the utility of integrated long-term satellite-based precipitation and evapotranspiration products for drought monitoring over mainland China. The latest Integrated Multi-satelliteE Retrievals for Global Precipitation Measurement V06 three Runs precipitation products, i.e., the near real-time Early Run (IMERG-E) and Late Run (IMERG-L) and the post-real time Final Run (IMERG-F), and the Global Land Evaporation Amsterdam Model V3.3a (GLEAM) potential evapotranspiration (PET) products from 2001 to 2017 were considered. The accuracy of IMERG precipitation and GLEAM PET products was first evaluated against observed precipitation and Penman-Monteith method estimated PET, respectively, based on dense meteorological station network. The Standard Precipitation Evapotranspiration Index (SPEI) calculated based on IMERG precipitation and GLEAM PET products (SPEIs, including SPEIE, SPEIL and SPEIF corresponding to IMERG-E, IMERG-L and IMERG-F, respectively) were then validated by using SPEI calculated based on meteorological data (SPEIm) at multiple temporal-spatial scales. Finally, four typical drought events were selected to analyse the ability of SPEIs to characterize the temporal-spatial evolution of drought situations. The results showed that the IMERG-F presents much better performance than IMERG-E and IMERG-L in terms of higher CC and smaller BIAS and RMSE values over mainland China. The GLEAM PET well simulated the change trend of reference PET, but generally underestimated reference PET in Northwest China (NW), Xinjiang (XJ) and Qinghai–Tibet plateau (TP). In general, the performances of SPEIs over eastern China and Southwest China (SW) were significantly superior to their performances in the NW, XJ, and TP regions. Even though the SPEIF performed the best, the SPEIE and SPEIL also performed reasonably well in some specific regions. SPEIs can well capture the temporal process and reasonably reflect the spatial characteristics for four typical drought events. It is thus highlighted that the latest IMERG precipitation (especially for IMERG-F) and GLEAM PET products could be used as alternative data sources for comprehensive drought monitoring, on account of the water balance principle over mainland China, particularly in eastern China and SW China. The outcomes of this study will provide valuable references for drought monitoring by integration of multi-source remote-sensing datasets in the GPM era.}
}
@incollection{ORLOWSKI20211,
title = {Chapter 1 - Smart Cities and open data},
editor = {Cezary Orlowski},
booktitle = {Management of IOT Open Data Projects in Smart Cities},
publisher = {Academic Press},
pages = {1-41},
year = {2021},
isbn = {978-0-12-818779-1},
doi = {https://doi.org/10.1016/B978-0-12-818779-1.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128187791000018},
author = {Cezary Orlowski},
keywords = {Smart Cities, open data, Internet of Things networks, business models, software project management},
abstract = {This chapter is devoted to Smart Cities and the importance of open data for their development. We, therefore, address the complexity of the Smart Cities environment for generating, collecting, and using open data from Internet of Things devices. The characteristics for the development of both Smart Cities and open data are discussed. We also present sample projects, whose product was embodied via open data. On this basis, we determine the business and legal conditions of Smart Cities for the acquisition, processing, and use of open data. In this way we intend to demonstrate the complexity of open data project management processes and to demonstrate the need to analyze existing managerial methods.}
}
@article{TANG2021106866,
title = {The exposome in practice: an exploratory panel study of biomarkers of air pollutant exposure in Chinese people aged 60–69 years (China BAPE Study)},
journal = {Environment International},
volume = {157},
pages = {106866},
year = {2021},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2021.106866},
url = {https://www.sciencedirect.com/science/article/pii/S0160412021004918},
author = {Song Tang and Tiantian Li and Jianlong Fang and Renjie Chen and Yu'e Cha and Yanwen Wang and Mu Zhu and Yi Zhang and Yuanyuan Chen and Yanjun Du and Tianwei Yu and David C. Thompson and Krystal J. {Godri Pollitt} and Vasilis Vasiliou and John S. Ji and Haidong Kan and Junfeng Jim Zhang and Xiaoming Shi},
keywords = {PM, Exposomics, Panel Study, Personal Exposure Monitoring, Metabolomics, Exposome-Wide Association Study},
abstract = {The exposome overhauls conventional environmental health impact research paradigms and provides a novel methodological framework that comprehensively addresses the complex, highly dynamic interplays of exogenous exposures, endogenous exposures, and modifiable factors in humans. Holistic assessments of the adverse health effects and systematic elucidation of the mechanisms underlying environmental exposures are major scientific challenges with widespread societal implications. However, to date, few studies have comprehensively and simultaneously measured airborne pollutant exposures and explored the associated biomarkers in susceptible healthy elderly subjects, potentially resulting in the suboptimal assessment and management of health risks. To demonstrate the exposome paradigm, we describe the rationale and design of a comprehensive biomarker and biomonitoring panel study to systematically explore the association between individual airborne exposure and adverse health outcomes. We used a combination of personal monitoring for airborne pollutants, extensive human biomonitoring, advanced omics analysis, confounding information, and statistical methods. We established an exploratory panel study of Biomarkers of Air Pollutant Exposure in Chinese people aged 60–69 years (China BAPE), which included 76 healthy residents from a representative community in Jinan City, Shandong Province. During the period between September 2018 and January 2019, we conducted prospective longitudinal monitoring with a 3-day assessment every month. This project: (1) leveraged advanced tools for personal airborne exposure monitoring (external exposures); (2) comprehensively characterized biological samples for exogenous and endogenous compounds (e.g., targeted and untargeted monitoring) and multi-omics scale measurements to explore potential biomarkers and putative toxicity pathways; and (3) systematically evaluated the relationships between personal exposure to air pollutants, and novel biomarkers of exposures and effects using exposome-wide association study approaches. These findings will contribute to our understanding of the mechanisms underlying the adverse health impacts of air pollution exposures and identify potential adverse clinical outcomes that can facilitate the development of effective prevention and targeted intervention techniques.}
}
@article{NINCEVICPASALIC2021102127,
title = {Smart city research advances in Southeast Europe},
journal = {International Journal of Information Management},
volume = {58},
pages = {102127},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102127},
url = {https://www.sciencedirect.com/science/article/pii/S026840121931477X},
author = {Ivana {Ninčević Pašalić} and Maja Ćukušić and Mario Jadrić},
keywords = {Smart cities, Southeast Europe, Descriptive literature review},
abstract = {Smart city (SC) research is an engaging research area as evidenced by a rising number of publications indexed in the most relevant global citation databases. However, research advances are not equally discussed and distributed within Europe. This study puts a focus on the specific geographic location of Southeast Europe (SEE), intending to fill the gap in understanding the research advances in this part of Europe. The aim of this descriptive review was to systematically investigate peer-reviewed publications focused on SC research in SEE in order to present the findings and the state-of-art in this research domain. Seventy-four papers were thoroughly studied, analysed and classified based on their focus on SC themes and common sub-themes. While smart governance had been studied extensively in the SEE region, topics related to the smart economy and smart people received low attention from researchers. Mapping the selected papers to the Plan-Do-Check-Act (PDCA) cycle showed that SC research in SEE is still in the conceptualising and planning stages, with very little evidence from the real implementation and follow-up activities. From the stakeholders’ perspective, the focus is on the institutional point of view as most of the papers present their findings in relation to (national or local) government bodies or policies, without balancing with corresponding businesses’ or individuals’ (users’) point of view. In general, user involvement was found to be very low in regards to current SC research in the SEE region.}
}
@article{YANG2021103346,
title = {Quantifying spatiotemporal patterns of shrinking cities in urbanizing China: A novel approach based on time-series nighttime light data},
journal = {Cities},
volume = {118},
pages = {103346},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2021.103346},
url = {https://www.sciencedirect.com/science/article/pii/S0264275121002468},
author = {Yang Yang and Jianguo Wu and Ying Wang and Qingxu Huang and Chunyang He},
keywords = {Shrinking cities, Urban shrinkage, Nighttime light, Urban sustainability, Urbanization, China},
abstract = {The shrinking of cities has become an increasingly global phenomenon, posing challenges for sustainable urban development. However, most focus remains on Europe and North America, and relatively little attention has been paid to the East Asia, especially the urbanizing China. Nighttime light (NL) dataset and its features (long-term time-series free access and large coverage) provide an alternative means to quantify shrinking cities. Here, we developed a new approach to identify shrinking cities and measure urban shrinkage, using corrected-integrated DMSP/OLS and NPP/VIIRS NL data. Based on this approach, we quantified the spatiotemporal patterns of shrinking cities in China from 1992 to 2019. Our study identified 153 shrinking cities in China during the study period, accounting for 23.39% of all 654 cities. These shrinking cities were widely distributed across eight economic regions and most provinces. The number of shrinking cities changed periodically and peaked following the Asian Financial Crisis in 1997 and again after the Global Economic Crisis in 2008. The cities that experienced the greatest shrinkage intensity were mainly distributed in northeast China, with severe urban shrinkage occurring between 2008 and 2013. The new approach proposed in this study can effectively identify shrinking city hotspots and key periods of urban shrinkage. Our findings suggest that sustainable urban development in China must consider shrinking cities, which are faced with challenging and urgent sustainability issues different from those by rapidly growing cities.}
}
@article{BERTRANDIAS2021120846,
title = {Delegating decision-making to autonomous products: A value model emphasizing the role of well-being},
journal = {Technological Forecasting and Social Change},
volume = {169},
pages = {120846},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120846},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100278X},
author = {Laurent BERTRANDIAS and Ben LOWE and Orsolya SADIK-ROZSNYAI and Manu CARRICANO},
keywords = {Perceived value, Autonomous cars, Autonomous products, Well-being, Consumers},
abstract = {Given the rapid growth of autonomous technologies it is important to understand how consumers attribute value to them. Such technologies require consumers to give up control to a machine by delegating decision-making power. To better understand value perceptions, and ultimately adoption, this paper proposes a conceptual model that explains the value attributed to autonomous cars as an archetypal consumer autonomous technology. The model is developed from literature around the theme of autonomy and two qualitative studies, which identify consumers’ perceived individual benefits (Freeing Time, Overcoming Human Weaknesses, Outperforming Human Capacities), risks (Loss of Competencies, Security and Privacy risk, Performance risk) and their proximal antecedents (Perceived Expertise, Attitude toward the Delegated Task, Previous Engagement in Delegation). The model is tested on a national sample of French drivers using a quantitative methodology and Structural Equation Modelling (SEM). This research contributes to literature on technological forecasting of autonomous technologies by developing and testing a conceptual model, which includes salient predictors of perceived value and highlights the mediating role of improvement in subjective well-being that consumers anticipate from adoption. The model can be used by managers to predict how users are likely to react to their products and communications about them.}
}
@article{2021503,
title = {Measuring routine childhood vaccination coverage in 204 countries and territories, 1980–2019: a systematic analysis for the Global Burden of Disease Study 2020, Release 1},
journal = {The Lancet},
volume = {398},
number = {10299},
pages = {503-521},
year = {2021},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(21)00984-3},
url = {https://www.sciencedirect.com/science/article/pii/S0140673621009843},
author = {Natalie C Galles and Patrick Y Liu and Rachel L Updike and Nancy Fullman and Jason Nguyen and Sam Rolfe and Alyssa N Sbarra and Megan F Schipp and Ashley Marks and Gdiom Gebreheat Abady and Kaja M Abbas and Sumra Wajid Abbasi and Hedayat Abbastabar and Foad Abd-Allah and Amir Abdoli and Hassan Abolhassani and Akine Eshete Abosetugn and Maryam Adabi and Abdu A Adamu and Olatunji O Adetokunboh and Qorinah Estiningtyas Sakilah Adnani and Shailesh M Advani and Saira Afzal and Seyed Mohammad Kazem Aghamir and Bright Opoku Ahinkorah and Sohail Ahmad and Tauseef Ahmad and Sepideh Ahmadi and Haroon Ahmed and Muktar Beshir Ahmed and Tarik {Ahmed Rashid} and Yusra {Ahmed Salih} and Yonas Akalu and Addis Aklilu and Chisom Joyqueenet Akunna and Hanadi {Al Hamad} and Fares Alahdab and Luciana Albano and Yosef Alemayehu and Kefyalew Addis Alene and Ayman Al-Eyadhy and Robert Kaba Alhassan and Liaqat Ali and Syed Mohamed Aljunid and Sami Almustanyir and Khalid A Altirkawi and Nelson Alvis-Guzman and Hubert Amu and Catalina Liliana Andrei and Tudorel Andrei and Adnan Ansar and Alireza Ansari-Moghaddam and Ippazio Cosimo Antonazzo and Benny Antony and Jalal Arabloo and Morteza Arab-Zozani and Kurnia Dwi Artanti and Judie Arulappan and Asma Tahir Awan and Mamaru Ayenew Awoke and Muluken Altaye Ayza and Ghasem Azarian and Ahmed Y Azzam and Darshan B B and Zaheer-Ud-Din Babar and Senthilkumar Balakrishnan and Maciej Banach and Simachew Animen Bante and Till Winfried Bärnighausen and Hiba Jawdat Barqawi and Amadou Barrow and Quique Bassat and Narantuya Bayarmagnai and Diana Fernanda {Bejarano Ramirez} and Tariku Tesfaye Bekuma and Habtamu Gebrehana Belay and Uzma Iqbal Belgaumi and Akshaya Srikanth Bhagavathula and Dinesh Bhandari and Nikha Bhardwaj and Pankaj Bhardwaj and Sonu Bhaskar and Krittika Bhattacharyya and Sadia Bibi and Ali Bijani and Antonio Biondi and Archith Boloor and Dejana Braithwaite and Danilo Buonsenso and Zahid A Butt and Paulo Camargos and Giulia Carreras and Felix Carvalho and Carlos A Castañeda-Orjuela and Raja Chandra Chakinala and Jaykaran Charan and Souranshu Chatterjee and Soosanna Kumary Chattu and Vijay Kumar Chattu and Fazle Rabbi Chowdhury and Devasahayam J Christopher and Dinh-Toi Chu and Sheng-Chia Chung and Paolo Angelo Cortesi and Vera Marisa Costa and Rosa A S Couto and Omid Dadras and Amare Belachew Dagnew and Baye Dagnew and Xiaochen Dai and Lalit Dandona and Rakhi Dandona and Jan-Walter {De Neve} and Meseret {Derbew Molla} and Behailu Tariku Derseh and Rupak Desai and Abebaw Alemayehu Desta and Deepak Dhamnetiya and Mandira Lamichhane Dhimal and Meghnath Dhimal and Mostafa Dianatinasab and Daniel Diaz and Shirin Djalalinia and Fariba Dorostkar and Bassey Edem and Hisham Atan Edinur and Sahar Eftekharzadeh and Iman {El Sayed} and Maysaa {El Sayed Zaki} and Muhammed Elhadi and Shaimaa I El-Jaafary and Aisha Elsharkawy and Shymaa Enany and Ryenchindorj Erkhembayar and Christopher Imokhuede Esezobor and Sharareh Eskandarieh and Ifeanyi Jude Ezeonwumelu and Sayeh Ezzikouri and Jawad Fares and Pawan Sirwan Faris and Berhanu Elfu Feleke and Tomas Y Ferede and Eduarda Fernandes and João C Fernandes and Pietro Ferrara and Irina Filip and Florian Fischer and Mark Rohit Francis and Takeshi Fukumoto and Mohamed M Gad and Shilpa Gaidhane and Silvano Gallus and Tushar Garg and Biniyam Sahiledengle Geberemariyam and Teshome Gebre and Birhan Gebresillassie Gebregiorgis and Ketema Bizuwork Gebremedhin and Berhe Gebremichael and Bradford D Gessner and Keyghobad Ghadiri and Mansour Ghafourifard and Ahmad Ghashghaee and Syed Amir Gilani and Ionela-Roxana Glăvan and Ekaterina Vladimirovna Glushkova and Mahaveer Golechha and Kebebe Bekele Gonfa and Sameer Vali Gopalani and Houman Goudarzi and Mohammed Ibrahim Mohialdeen Gubari and Yuming Guo and Veer Bala Gupta and Vivek Kumar Gupta and Reyna Alma Gutiérrez and Emily Haeuser and Rabih Halwani and Samer Hamidi and Asif Hanif and Shafiul Haque and Harapan Harapan and Arief Hargono and Abdiwahab Hashi and Shoaib Hassan and Mohamed H Hassanein and Soheil Hassanipour and Hadi Hassankhani and Simon I Hay and Khezar Hayat and Mohamed I Hegazy and Golnaz Heidari and Kamal Hezam and Ramesh Holla and Mohammad Enamul Hoque and Mostafa Hosseini and Mehdi Hosseinzadeh and Mihaela Hostiuc and Mowafa Househ and Vivian Chia-rong Hsieh and Junjie Huang and Ayesha Humayun and Rabia Hussain and Nawfal R Hussein and Segun Emmanuel Ibitoye and Olayinka Stephen Ilesanmi and Irena M Ilic and Milena D Ilic and Sumant Inamdar and Usman Iqbal and Lalu Muhammad Irham and Seyed Sina Naghibi Irvani and Sheikh Mohammed Shariful Islam and Nahlah Elkudssiah Ismail and Ramaiah Itumalla and Ravi Prakash Jha and Farahnaz Joukar and Ali Kabir and Zubair Kabir and Rohollah Kalhor and Zul Kamal and Stanley M Kamande and Himal Kandel and André Karch and Getinet Kassahun and Nicholas J Kassebaum and Patrick DMC Katoto and Bayew Kelkay and Andre Pascal Kengne and Yousef Saleh Khader and Himanshu Khajuria and Ibrahim A Khalil and Ejaz Ahmad Khan and Gulfaraz Khan and Junaid Khan and Maseer Khan and Moien AB Khan and Young-Ho Khang and Abdullah T Khoja and Jagdish Khubchandani and Gyu Ri Kim and Min Seo Kim and Yun Jin Kim and Ruth W Kimokoti and Adnan Kisa and Sezer Kisa and Vladimir Andreevich Korshunov and Soewarta Kosen and Barthelemy {Kuate Defo} and Vaman Kulkarni and Avinash Kumar and G Anil Kumar and Nithin Kumar and Alexander Kwarteng and Carlo {La Vecchia} and Faris Hasan Lami and Iván Landires and Savita Lasrado and Zohra S Lassi and Hankil Lee and Yeong Yeh Lee and Miriam Levi and Sonia Lewycka and Shanshan Li and Xuefeng Liu and Stany W Lobo and Platon D Lopukhov and Rafael Lozano and Ricardo {Lutzky Saute} and Muhammed {Magdy Abd El Razek} and Alaa Makki and Ahmad Azam Malik and Fariborz Mansour-Ghanaei and Mohammad Ali Mansournia and Lorenzo Giovanni Mantovani and Francisco Rogerlândio Martins-Melo and Philippa C Matthews and John Robert Carabeo Medina and Walter Mendoza and Ritesh G Menezes and Endalkachew Worku Mengesha and Tuomo J Meretoja and Amanual Getnet Mersha and Mohamed Kamal Mesregah and Tomislav Mestrovic and Bartosz Miazgowski and George J Milne and Andreea Mirica and Erkin M Mirrakhimov and Hamid Reza Mirzaei and Sanjeev Misra and Prasanna Mithra and Masoud Moghadaszadeh and Teroj Abdulrahman Mohamed and Karzan Abdulmuhsin Mohammad and Yousef Mohammad and Mokhtar Mohammadi and Abdollah Mohammadian-Hafshejani and Arif Mohammed and Shafiu Mohammed and Archisman Mohapatra and Ali H Mokdad and Mariam Molokhia and Lorenzo Monasta and Mohammad Ali Moni and Ahmed Al Montasir and Catrin E Moore and Ghobad Moradi and Rahmatollah Moradzadeh and Paula Moraga and Ulrich Otto Mueller and Sandra B Munro and Mohsen Naghavi and Mukhammad David Naimzada and Muhammad Naveed and Biswa Prakash Nayak and Ionut Negoi and Sandhya {Neupane Kandel} and Trang Huyen Nguyen and Rajan Nikbakhsh and Dina Nur Anggraini Ningrum and Molly R Nixon and Chukwudi A Nnaji and Jean Jacques Noubiap and Virginia Nuñez-Samudio and Vincent Ebuka Nwatah and Bogdan Oancea and Chimedsuren Ochir and Felix Akpojene Ogbo and Andrew T Olagunju and Babayemi Oluwaseun Olakunde and Obinna E Onwujekwe and Nikita Otstavnov and Stanislav S Otstavnov and Mayowa O Owolabi and Jagadish Rao Padubidri and Keyvan Pakshir and Eun-Cheol Park and Fatemeh {Pashazadeh Kan} and Mona Pathak and Rajan Paudel and Shrikant Pawar and Jeevan Pereira and Mario F P Peres and Arokiasamy Perianayagam and Marina Pinheiro and Majid Pirestani and Vivek Podder and Roman V Polibin and Richard Charles G Pollok and Maarten J Postma and Faheem Hyder Pottoo and Mohammad Rabiee and Navid Rabiee and Amir Radfar and Alireza Rafiei and Vafa Rahimi-Movaghar and Mosiur Rahman and Amir Masoud Rahmani and Setyaningrum Rahmawaty and Aashish Rajesh and Rebecca E Ramshaw and Priyanga Ranasinghe and Chythra R Rao and Sowmya J Rao and Priya Rathi and David Laith Rawaf and Salman Rawaf and Andre M N Renzaho and Negar Rezaei and Mohammad Sadegh Rezai and Maria Rios-Blancas and Emma L B Rogowski and Luca Ronfani and Godfrey M Rwegerera and Anas M Saad and Siamak Sabour and Basema Saddik and Mohammad Reza Saeb and Umar Saeed and Amirhossein Sahebkar and Mohammad Ali Sahraian and Nasir Salam and Hamideh Salimzadeh and Mehrnoosh Samaei and Abdallah M Samy and Juan Sanabria and Francesco Sanmarchi and Milena M Santric-Milicevic and Benn Sartorius and Arash Sarveazad and Brijesh Sathian and Monika Sawhney and Deepak Saxena and Sonia Saxena and Abdul-Aziz Seidu and Allen Seylani and Masood Ali Shaikh and Morteza Shamsizadeh and Pavanchand H Shetty and Mika Shigematsu and Jae Il Shin and Negussie Boti Sidemo and Ambrish Singh and Jasvinder A Singh and Smriti Sinha and Valentin Yurievich Skryabin and Anna Aleksandrovna Skryabina and Amin Soheili and Eyayou Girma Tadesse and Animut Tagele Tamiru and Ker-Kan Tan and Yohannes Tekalegn and Mohamad-Hani Temsah and Bhaskar Thakur and Rekha Thapar and Aravind Thavamani and Ruoyan Tobe-Gai and Hamid Reza Tohidinik and Marcos Roberto Tovani-Palone and Eugenio Traini and Bach Xuan Tran and Manjari Tripathi and Berhan Tsegaye and Gebiyaw Wudie Tsegaye and Anayat Ullah and Saif Ullah and Sana Ullah and Brigid Unim and Marco Vacante and Diana Zuleika Velazquez and Bay Vo and Sebastian Vollmer and Giang Thu Vu and Linh Gia Vu and Yasir Waheed and Andrea Sylvia Winkler and Charles Shey Wiysonge and Vahit Yiğit and Birhanu Wubale Yirdaw and Dong Keon Yon and Naohiro Yonemoto and Chuanhua Yu and Deniz Yuce and Ismaeel Yunusa and Mohammad Zamani and Maryam Zamanian and Dejene Tesfaye Zewdie and Zhi-Jiang Zhang and Chenwen Zhong and Alimuddin Zumla and Christopher J L Murray and Stephen S Lim and Jonathan F Mosser},
abstract = {Summary
Background
Measuring routine childhood vaccination is crucial to inform global vaccine policies and programme implementation, and to track progress towards targets set by the Global Vaccine Action Plan (GVAP) and Immunization Agenda 2030. Robust estimates of routine vaccine coverage are needed to identify past successes and persistent vulnerabilities. Drawing from the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2020, Release 1, we did a systematic analysis of global, regional, and national vaccine coverage trends using a statistical framework, by vaccine and over time.
Methods
For this analysis we collated 55 326 country-specific, cohort-specific, year-specific, vaccine-specific, and dose-specific observations of routine childhood vaccination coverage between 1980 and 2019. Using spatiotemporal Gaussian process regression, we produced location-specific and year-specific estimates of 11 routine childhood vaccine coverage indicators for 204 countries and territories from 1980 to 2019, adjusting for biases in country-reported data and reflecting reported stockouts and supply disruptions. We analysed global and regional trends in coverage and numbers of zero-dose children (defined as those who never received a diphtheria-tetanus-pertussis [DTP] vaccine dose), progress towards GVAP targets, and the relationship between vaccine coverage and sociodemographic development.
Findings
By 2019, global coverage of third-dose DTP (DTP3; 81·6% [95% uncertainty interval 80·4–82·7]) more than doubled from levels estimated in 1980 (39·9% [37·5–42·1]), as did global coverage of the first-dose measles-containing vaccine (MCV1; from 38·5% [35·4–41·3] in 1980 to 83·6% [82·3–84·8] in 2019). Third-dose polio vaccine (Pol3) coverage also increased, from 42·6% (41·4–44·1) in 1980 to 79·8% (78·4–81·1) in 2019, and global coverage of newer vaccines increased rapidly between 2000 and 2019. The global number of zero-dose children fell by nearly 75% between 1980 and 2019, from 56·8 million (52·6–60·9) to 14·5 million (13·4–15·9). However, over the past decade, global vaccine coverage broadly plateaued; 94 countries and territories recorded decreasing DTP3 coverage since 2010. Only 11 countries and territories were estimated to have reached the national GVAP target of at least 90% coverage for all assessed vaccines in 2019.
Interpretation
After achieving large gains in childhood vaccine coverage worldwide, in much of the world this progress was stalled or reversed from 2010 to 2019. These findings underscore the importance of revisiting routine immunisation strategies and programmatic approaches, recentring service delivery around equity and underserved populations. Strengthening vaccine data and monitoring systems is crucial to these pursuits, now and through to 2030, to ensure that all children have access to, and can benefit from, lifesaving vaccines.
Funding
Bill & Melinda Gates Foundation.}
}
@article{LI2021103221,
title = {An integrated multi-omic analysis of iPSC-derived motor neurons from C9ORF72 ALS patients},
journal = {iScience},
volume = {24},
number = {11},
pages = {103221},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.103221},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221011895},
author = {Hemali Phatnani and Justin Kwan and Dhruv Sareen and James R. Broach and Zachary Simmons and Ximena Arcila-Londono and Edward B. Lee and Vivianna M. {Van Deerlin} and Neil A. Shneider and Ernest Fraenkel and Lyle W. Ostrow and Frank Baas and Noah Zaitlen and James D. Berry and Andrea Malaspina and Pietro Fratta and Gregory A. Cox and Leslie M. Thompson and Steve Finkbeiner and Efthimios Dardiotis and Timothy M. Miller and Siddharthan Chandran and Suvankar Pal and Eran Hornstein and Daniel J. MacGowan and Terry Heiman-Patterson and Molly G. Hammell and Nikolaos.A. Patsopoulos and Oleg Butovsky and Joshua Dubnau and Avindra Nath and Robert Bowser and Matt Harms and Mary Poss and Jennifer Phillips-Cremins and John Crary and Nazem Atassi and Dale J. Lange and Darius J. Adams and Leonidas Stefanis and Marc Gotkine and Robert H. Baloh and Suma Babu and Towfique Raj and Sabrina Paganoni and Ophir Shalem and Colin Smith and Bin Zhang and Brent Harris and Iris Broce and Vivian Drory and John Ravits and Corey McMillan and Vilas Menon and Lani Wu and Steven Altschuler and Jonathan Li and Ryan G. Lim and Julia A. Kaye and Victoria Dardov and Alyssa N. Coyne and Jie Wu and Pamela Milani and Andrew Cheng and Terri G. Thompson and Loren Ornelas and Aaron Frank and Miriam Adam and Maria G. Banuelos and Malcolm Casale and Veerle Cox and Renan Escalante-Chong and J. Gavin Daigle and Emilda Gomez and Lindsey Hayes and Ronald Holewenski and Susan Lei and Alex Lenail and Leandro Lima and Berhan Mandefro and Andrea Matlock and Lindsay Panther and Natasha Leanna Patel-Murray and Jacqueline Pham and Divya Ramamoorthy and Karen Sachs and Brandon Shelley and Jennifer Stocksdale and Hannah Trost and Mark Wilhelm and Vidya Venkatraman and Brook T. Wassie and Stacia Wyman and Stephanie Yang and Jennifer E. {Van Eyk} and Thomas E. Lloyd and Steven Finkbeiner and Ernest Fraenkel and Jeffrey D. Rothstein and Dhruv Sareen and Clive N. Svendsen and Leslie M. Thompson},
keywords = {Biological sciences, Neuroscience, Systems neuroscience, Systems biology, Omics},
abstract = {Summary
Neurodegenerative diseases are challenging for systems biology because of the lack of reliable animal models or patient samples at early disease stages. Induced pluripotent stem cells (iPSCs) could address these challenges. We investigated DNA, RNA, epigenetics, and proteins in iPSC-derived motor neurons from patients with ALS carrying hexanucleotide expansions in C9ORF72. Using integrative computational methods combining all omics datasets, we identified novel and known dysregulated pathways. We used a C9ORF72 Drosophila model to distinguish pathways contributing to disease phenotypes from compensatory ones and confirmed alterations in some pathways in postmortem spinal cord tissue of patients with ALS. A different differentiation protocol was used to derive a separate set of C9ORF72 and control motor neurons. Many individual -omics differed by protocol, but some core dysregulated pathways were consistent. This strategy of analyzing patient-specific neurons provides disease-related outcomes with small numbers of heterogeneous lines and reduces variation from single-omics to elucidate network-based signatures.}
}
@article{OLIVER2021109119,
title = {4D seismic history matching},
journal = {Journal of Petroleum Science and Engineering},
volume = {207},
pages = {109119},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109119},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521007750},
author = {Dean S. Oliver and Kristian Fossum and Tuhin Bhakta and Ivar Sandø and Geir Nævdal and Rolf Johan Lorentzen},
keywords = {Time-lapse seismic, 4D seismic, History matching},
abstract = {Reservoir simulation models are used to forecast future reservoir behavior and to optimally manage reservoir production. These models require specification of hundreds of thousands of parameters, some of which may be determined from measurements along well paths, but the distance between wells can be large and the formations in which oil and gas are found are almost always heterogeneous with many geological complexities so many of the reservoir parameters are poorly constrained by well data. Additional constraints on the values of the parameters are provided by general geologic knowledge, and other constraints are provided by historical measurements of production and injection behavior. This type of information is often not sufficient to identify locations of either currently remaining oil, or to provide accurate forecasts where oil will remain at the end of project life. The repeated use of surface seismic surveys offers the promise of providing observations of locations of changes in physical properties between wells, thus reducing uncertainty in predictions of future reservoir behavior. Unfortunately, while methodologies for assimilation of 4D seismic data have demonstrated substantial value in synthetic model studies, the application to real fields has not been as successful. In this paper, we review the literature on 4D seismic history matching (SHM), focusing discussions on the aspects of the problem that make it more difficult than the more traditional production history matching. In particular, we discuss the possible choices for seismic attributes that can be used for comparison between observed or modeled attribute to determine the properties of the reservoir and the difficulty of estimating the magnitude of the noise or bias in the data. Depending on the level of matching, the bias may result from errors in the forward modeling, or errors in the inversion. Much of the practical literature has focused on methodologies for reducing the effect of bias or modeling error either through choice of attribute, or by appropriate weighting of data. Applications to field cases appear to have been at least partially successful, although quantitative assessment of the history matches and the improvements in forecast is difficult.}
}
@article{KWOK2021100785,
title = {Trends, topics, and lessons learnt from real case studies using mesoscale atmospheric models for urban climate applications in 2000–2019},
journal = {Urban Climate},
volume = {36},
pages = {100785},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100785},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521000158},
author = {Yu Ting Kwok and Edward Yan Yung Ng},
keywords = {Urban climate, Urban climate modelling, Urban climate application, Mesoscale atmospheric model, Urban parameterization},
abstract = {Researchers have made immense progress in understanding the urban-induced microclimate by numerical modelling. It has been around two decades since urban canopy models now commonly employed in mesoscale atmospheric models for operational and applied research purposes have emerged. To drive further advancement, it is timely to conduct a review of the state-of-the-art and lessons learnt from the relevant literature. In this paper, 102 urban climate real case modelling studies published in 2000–2019 are reviewed. Patterns and preferences in their study locations, periods, model choices, land cover databases, topics discussed, and scenarios investigated are holistically examined. There is an evident improvement in model complexity and urban surface data precision during the period reviewed. Most studies focus on the urban thermal climate and effects of urbanization. Based on the research gaps identified, more work is needed on the currently underrepresented but vulnerable cities in developing countries with tropical, arid, and cold climates. Collaborative field campaigns, initiatives to characterize cities in a consistent manner, and multi-scale modelling approaches have proven to benefit the progress in urban climate studies and should therefore be encouraged. More importantly, efforts should be invested in translating the science into information relevant to human well-being, urban planning, and policymaking.}
}
@incollection{KERY2021581,
title = {Chapter 10 - Integrated Models for Multiple Types of Data},
editor = {Marc Kéry and J. Andrew Royle},
booktitle = {Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in R and BUGS},
publisher = {Academic Press},
pages = {581-646},
year = {2021},
isbn = {978-0-12-823768-7},
doi = {https://doi.org/10.1016/B978-0-12-809585-0.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095850000107},
author = {Marc Kéry and J. Andrew Royle},
keywords = {Abundance, Citizen science, Data fusion, Distribution, IM, IPM, Population dynamics, SDM},
abstract = {Integrated models (IMs) represent one of the major research avenues in ecological statistics during the past couple of decades. They are hierarchical models for integrating two or more, disparate, data sets simultaneously. We cover a special case, integrated population models (IPMs), and emphasize the general principles of data integration, or fusion, which underlie all other types of IMs. These are powerful and sensible models, because they utilize all available information in an estimation problem, usually yield estimates with greater precision, and sometimes enable estimation of additional parameter that are not estimable with each individual data set alone. We discuss six widely different examples of IMs, which are useful by themselves and furthermore illustrate the very broad scope of IMs.}
}
@article{BOGDANOVIC2021100624,
title = {On revealing shared conceptualization among open datasets},
journal = {Journal of Web Semantics},
volume = {66},
pages = {100624},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100624},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300573},
author = {Miloš Bogdanović and Nataša Veljković and Milena {Frtunić Gligorijević} and Darko Puflović and Leonid Stoimenov},
keywords = {Open data, Formal concept analysis, Semantic similarity, Categorization, Natural language processing},
abstract = {Openness and transparency initiatives are not only milestones of science progress but have also influenced various fields of organization and industry. Under this influence, varieties of government institutions worldwide have published a large number of datasets through open data portals. Government data covers diverse subjects and the scale of available data is growing every year. Published data is expected to be both accessible and discoverable. For these purposes, portals take advantage of metadata accompanying datasets. However, a part of metadata is often missing which decreases users’ ability to obtain the desired information. As the scale of published datasets grows, this problem increases. An approach we describe in this paper is focused towards decreasing this problem by implementing knowledge structures and algorithms capable of proposing the best match for the category where an uncategorized dataset should belong to. By doing so, our aim is twofold: enrich datasets metadata by suggesting an appropriate category and increase its visibility and discoverability. Our approach relies on information regarding open datasets provided by users — dataset description contained within dataset tags. Since dataset tags express low consistency due to their origin, in this paper we will present a method of optimizing their usage through means of semantic similarity measures based on natural language processing mechanisms. Optimization is performed in terms of reducing the number of distinct tag values used for dataset description. Once optimized, dataset tags are used to reveal shared conceptualization originating from their usage by means of Formal Concept Analysis. We will demonstrate the advantage of our proposal by comparing concept lattices generated using Formal Concept Analysis before and after the optimization process and use generated structure as a knowledge base to categorize uncategorized open datasets. Finally, we will present a categorization mechanism based on the generated knowledge base that takes advantage of semantic similarity measures to propose a category suitable for an uncategorized dataset.}
}
@article{BAVARESCO2021107670,
title = {Internet of Things and occupational well-being in industry 4.0: A systematic mapping study and taxonomy},
journal = {Computers & Industrial Engineering},
volume = {161},
pages = {107670},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107670},
url = {https://www.sciencedirect.com/science/article/pii/S036083522100574X},
author = {Rodrigo Bavaresco and Helder Arruda and Eduarda Rocha and Jorge Barbosa and Guann-Pyng Li},
keywords = {Internet of Things, Industry 4.0, Occupational Health and Safety, Occupational Well-being, Human-Centered Industry},
abstract = {A high estimate of not only workplace fatal injuries but also nonfatal injuries and illnesses via overexertion, and contact with objects, equipment, and machinery workplace has been reported over the years. To address this issue, the Occupational Health and Safety (OHS) program has put an emphasis on policy and regulation for prevention, protection, and improvement of the individuals’ health related to the working conditions and industrial environment. In the last 10 years, the Internet of Things (IoT) has matured to seamlessly enable real-time communications and cooperation among machines, environments, and humans using data analytics. Thus, IoT offers potential technical solutions for the prevention and protection of workplace injuries and illnesses. Moreover, IoT invites opportunities for collaboration with OHS in various industries. This article presents a systematic mapping study of the literature to address the impact of IoT on occupational well-being, analyzing the progress of Industry 4.0 during the last decade. This study systematizes the literature providing a taxonomy of the area through the results of four general, four focused, and two statistical research questions. These questions outline industrial environments and aspects of health concerning workers’ well-being, concentrating on a human-centered approach leveraged by physiological measurements and psychological health. In addition, this paper explores questions regarding IoT’s technical components, such as sensors, devices, and communication technologies, investigating methods of data processing supported by the employment of classification algorithms and data fusion strategies. As a result, the systematic mapping process initially found 7515 articles from six academic databases in the period from 2009 to 2019. After the execution of filtering methods, a complete read of 67 articles allowed to answer quantitatively and qualitatively the research questions. The classification of the answers contributed to systematize the literature through the taxonomy and the relationships among the topics covered by the articles. Accordingly, this research produced theoretical benefits, mainly, a broad view of the state-of-the-art, a taxonomy to guide related researches, and guidelines for future works. Furthermore, this research would benefit management and corporations by shedding light on technologies explored in the literature and elucidating their feasibility in support of the workforce’s safety, psychological, and physical health.}
}
@article{LI202128,
title = {DNA methylation methods: Global DNA methylation and methylomic analyses},
journal = {Methods},
volume = {187},
pages = {28-43},
year = {2021},
note = {Advance Epigenetics Methods in Biomedicine},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2020.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1046202320302164},
author = {Shizhao Li and Trygve O. Tollefsbol},
keywords = {DNA methylation, DNA hydroxymethylation, Next-generation sequencing, Bisulfite conversion, Endonuclease digestion, Affinity enrichment, Microarray},
abstract = {DNA methylation provides a pivotal layer of epigenetic regulation in eukaryotes that has significant involvement for numerous biological processes in health and disease. The function of methylation of cytosine bases in DNA was originally proposed as a “silencing” epigenetic marker and focused on promoter regions of genes for decades. Improved technologies and accumulating studies have been extending our understanding of the roles of DNA methylation to various genomic contexts including gene bodies, repeat sequences and transcriptional start sites. The demand for comprehensively describing DNA methylation patterns spawns a diversity of DNA methylation profiling technologies that target its genomic distribution. These approaches have enabled the measurement of cytosine methylation from specific loci at restricted regions to single-base-pair resolution on a genome-scale level. In this review, we discuss the different DNA methylation analysis technologies primarily based on the initial treatments of DNA samples: bisulfite conversion, endonuclease digestion and affinity enrichment, involving methodology evolution, principles, applications, and their relative merits. This review may offer referable information for the selection of various platforms for genome-wide analysis of DNA methylation.}
}
@article{YU2021125976,
title = {Information disclosure decisions in an organic food supply chain under competition},
journal = {Journal of Cleaner Production},
volume = {292},
pages = {125976},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.125976},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621001967},
author = {Yanan Yu and Yong He},
keywords = {Asymmetric and private information, Production and pricing, Organic certification, Information disclosure, Competition, Food supply chain},
abstract = {This paper mainly investigates the information disclosure decisions in an organic food supply chain including one farmers’ organization and one small-scale producer (producer 2). We find that when demand competition is fierce, the organization is reluctant to disclose demand information. Producer 2 is willing to disclose product information facing a low portion of revenue sharing and a high safety perception. A “win-win” situation for two producers can occur by means of group certification with a rational portion of revenue sharing. Interestingly, a more transparent market is not necessarily desirable for customers and social welfare.}
}
@incollection{MAHLER2021237,
title = {Chapter 11 - Regulatory aspects of artificial intelligence and machine learning-enabled software as medical devices (SaMD)},
editor = {Michael Mahler},
booktitle = {Precision Medicine and Artificial Intelligence},
publisher = {Academic Press},
pages = {237-265},
year = {2021},
isbn = {978-0-12-820239-5},
doi = {https://doi.org/10.1016/B978-0-12-820239-5.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202395000103},
author = {Michael Mahler and Carolina Auza and Roger Albesa and Carlos Melus and Jungen Andrew Wu},
keywords = {SaMD, FDA, Regulatory, Artificial intelligence, Machine learning, Culture of quality and organizational excellence, Pre-Cert Program, MDR, NMPA, Cybersecurity},
abstract = {With the introduction of artificial intelligence (AI) and machine learning (ML) in healthcare and the development of an increasing number of commercial products based on software as medical device (SaMD), regulatory processes need to evolve in parallel. This book chapter aims to provide a high-level overview of the history of SaMD and reviews some of the various regulatory aspects associated with this group of medical devices. Although we aim to cover the regulations globally, the main focus is on markets dependent on the Food and Drug Administration (FDA) based review process mostly due to the early stages of regulations in other geographies.}
}
@article{DORFLEITNER2021111378,
title = {Blockchain applications for climate protection: A global empirical investigation},
journal = {Renewable and Sustainable Energy Reviews},
volume = {149},
pages = {111378},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111378},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121006638},
author = {Gregor Dorfleitner and Franziska Muck and Isabel Scheckenbach},
keywords = {Blockchain, Distributed ledger, Green finance, Consensus mechanisms, Peer-to-peer transactions, Sustainability goals},
abstract = {Our research consolidates the actual environment of blockchain applications that contribute in a certain way to climate protection. In view of the growing interest in climate change and the need to act on a global scale, knowledge about these applications enables investors, politicians, and citizens to drive this development forward through diverse support opportunities. This article provides an extensive overview of existing mitigation and adaptation measures based on blockchain technology. We collect data on 85 such applications and describe the empirical distributions of different attributes of these applications. In a logit regression, we analyze which application-specific and blockchain-specific characteristics determine the success of an application in the sense of an advanced operational status. We find evidence that applications of the type “energy trading” exhibit reduced chances of success, while green blockchain-based applications implementing a proof-of-stake consensus mechanism are more likely to become operational. Moreover, pursuing an initial coin offering has no significant effect on the success of an application. Our work provides the basis for a better understanding of the success factors of this new technology.}
}
@article{BRANCHER2021117153,
title = {Increased ozone pollution alongside reduced nitrogen dioxide concentrations during Vienna’s first COVID-19 lockdown: Significance for air quality management},
journal = {Environmental Pollution},
volume = {284},
pages = {117153},
year = {2021},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2021.117153},
url = {https://www.sciencedirect.com/science/article/pii/S0269749121007351},
author = {Marlon Brancher},
keywords = {COVID-19 lockdown, Air quality data, Atmospheric composition, Meteorology, Machine learning},
abstract = {Background
Lockdowns amid the COVID-19 pandemic have offered a real-world opportunity to better understand air quality responses to previously unseen anthropogenic emission reductions.
Methods and main objective
This work examines the impact of Vienna’s first lockdown on ground-level concentrations of nitrogen dioxide (NO2), ozone (O3) and total oxidant (Ox). The analysis runs over January to September 2020 and considers business as usual scenarios created with machine learning models to provide a baseline for robustly diagnosing lockdown-related air quality changes. Models were also developed to normalise the air pollutant time series, enabling facilitated intervention assessment.
Core findings
NO2 concentrations were on average −20.1% [13.7–30.4%] lower during the lockdown. However, this benefit was offset by amplified O3 pollution of +8.5% [3.7–11.0%] in the same period. The consistency in the direction of change indicates that the NO2 reductions and O3 increases were ubiquitous over Vienna. Ox concentrations increased slightly by +4.3% [1.8–6.4%], suggesting that a significant part of the drops in NO2 was compensated by gains in O3. Accordingly, 82% of lockdown days with lowered NO2 were accompanied by 81% of days with amplified O3. The recovery shapes of the pollutant concentrations were depicted and discussed. The business as usual-related outcomes were broadly consistent with the patterns outlined by the normalised time series. These findings allowed to argue further that the detected changes in air quality were of anthropogenic and not of meteorological reason. Pollutant changes on the machine learning baseline revealed that the impact of the lockdown on urban air quality were lower than the raw measurements show. Besides, measured traffic drops in major Austrian roads were more significant for light-duty than for heavy-duty vehicles. It was also noted that the use of mobility reports based on cell phone movement as activity data can overestimate the reduction of emissions for the road transport sector, particularly for heavy-duty vehicles. As heavy-duty vehicles can make up a large fraction of the fleet emissions of nitrogen oxides, the change in the volume of these vehicles on the roads may be the main driver to explain the change in NO2 concentrations.
Interpretation and implications
A probable future with emissions of volatile organic compounds (VOCs) dropping slower than emissions of nitrogen oxides could risk worsened urban O3 pollution under a VOC-limited photochemical regime. More holistic policies will be needed to achieve improved air quality levels across different regions and criteria pollutants.}
}
@article{ALLAN2021104885,
title = {Regulatory landscape of nanotechnology and nanoplastics from a global perspective},
journal = {Regulatory Toxicology and Pharmacology},
volume = {122},
pages = {104885},
year = {2021},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2021.104885},
url = {https://www.sciencedirect.com/science/article/pii/S0273230021000258},
author = {Jacqueline Allan and Susanne Belz and Arnd Hoeveler and Marta Hugas and Haruhiro Okuda and Anil Patri and Hubert Rauscher and Primal Silva and William Slikker and Birgit Sokull-Kluettgen and Weida Tong and Elke Anklam},
keywords = {Nanotechnology, Nanomaterials, Nanoplastics, Harmonisation, Regulatory science, Standards, GSRS},
abstract = {Nanotechnology and more particularly nanotechnology-based products and materials have provided a huge potential for novel solutions to many of the current challenges society is facing. However, nanotechnology is also an area of product innovation that is sometimes developing faster than regulatory frameworks. This is due to the high complexity of some nanomaterials, the lack of a globally harmonised regulatory definition and the different scopes of regulation at a global level. Research organisations and regulatory bodies have spent many efforts in the last two decades to cope with these challenges. Although there has been a significant advancement related to analytical approaches for labelling purposes as well as to the development of suitable test guidelines for nanomaterials and their safety assessment, there is a still a need for greater global collaboration and consensus in the regulatory field. Furthermore, with growing societal concerns on plastic litter and tiny debris produced by degradation of littered plastic objects, the impact of micro- and nanoplastics on humans and the environment is an emerging issue. Despite increasing research and initial regulatory discussions on micro- and nanoplastics, there are still knowledge gaps and thus an urgent need for action. As nanoplastics can be classified as a specific type of incidental nanomaterials, current and future scientific investigations should take into account the existing profound knowledge on nanotechnology/nanomaterials when discussing issues around nanoplastics. This review was conceived at the 2019 Global Summit on Regulatory Sciences that took place in Stresa, Italy, on 24–26 September 2019 (GSRS 2019) and which was co-organised by the Global Coalition for Regulatory Science Research (GCRSR) and the European Commission's (EC) Joint Research Centre (JRC). The GCRSR consists of regulatory bodies from various countries around the globe including EU bodies. The 2019 Global Summit provided an excellent platform to exchange the latest information on activities carried out by regulatory bodies with a focus on the application of nanotechnology in the agriculture/food sector, on nanoplastics and on nanomedicines, including taking stock and promoting further collaboration. Recently, the topic of micro- and nanoplastics has become a new focus of the GCRSR. Besides discussing the challenges and needs, some future directions on how new tools and methodologies can improve the regulatory science were elaborated by summarising a significant portion of discussions during the summit. It has been revealed that there are still some uncertainties and knowledge gaps with regard to physicochemical properties, environmental behaviour and toxicological effects, especially as testing described in the dossiers is often done early in the product development process, and the material in the final product may behave differently. The harmonisation of methodologies for quantification and risk assessment of nanomaterials and micro/nanoplastics, the documentation of regulatory science studies and the need for sharing databases were highlighted as important aspects to look at.}
}
@incollection{SUBASI2021209,
title = {Chapter 14 - COVID-19 detection from X-ray images using artificial intelligence},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {209-224},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000139},
author = {Abdulhamit Subasi and Saqib Ahmed Qureshi and Tayeb Brahimi and Akila Serireti},
keywords = {COVID-19 detection, Image classification, X-ray imaging, Artificial Intelligence, Deep learning},
abstract = {Currently, the most common disease is the new coronavirus disease identified as COVID-19. Various techniques to identifying the COVID-19 disease have been offered. Computer vision techniques are widely used to classify COVID-19 with the use of chest X-ray images. Rapid clinical results may prevent COVID-19 from spreading and help doctors treat patients under high workload conditions. As the normal diagnosis phase of illness with a laboratory test is time-consuming and requires a well-equipped laboratory, the X-ray imaging technique is a fast and cheap diagnostic tool for COVID-19. Machine learning methods can enhance the diagnosis of COVID-19 as a decision support platform for radiologists. This chapter utilizes various convolutional neural network (CNN) models, including pretrained models, to classify X-ray images into three classes: COVID-19, pneumonia, and normal. CNN, a form of deep neural networks that have become dominant in various computer vision tasks, attracts interest across various domains, including radiology. Pretrained models on ImageNet are good at detecting high-level features such as edges and patterns. These models understand certain representations of features, which can be reused. Also, deep classifiers have shown promising results in many kinds of work across various domains. We drew some useful results from these classifiers, which could be used faster when detecting COVID-19. Experimental results showed that the accuracy of the VGG19 classifier is 97.56%.}
}
@article{BILORIA20213,
title = {From smart to empathic cities},
journal = {Frontiers of Architectural Research},
volume = {10},
number = {1},
pages = {3-16},
year = {2021},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2020.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2095263520300698},
author = {Nimish Biloria},
keywords = {Empathic city, Smart city, Wellbeing, Neoliberalism, Regenerative model},
abstract = {This paper acknowledges the contemporary neoliberal mode of operation of Smart Cities. The pitfalls of Smart Cities concerning its propensity towards techno-centric and efficiency-focused governance are identified, with diminutive emphasis on social equity and human-centric urban growth. Thus, the paper elaborates upon an alternative mode of person-environment-interaction based approach towards placemaking: Empathic Cities. This approach implies embracing a shift from efficiency to sufficiency and wellbeing embedded regenerative perspective for conceiving the built environment. First, the variable dimensions of urban growth and governance, which gave rise to the smart city, are contextualized. The embedded neoliberal operational agenda of smart cities are established. On this basis, the underpinnings of an empathic city are established by acknowledging the shift from techno-centric to human-centric and from product-based to context-based smart city and wellbeing perspectives. Strategies toward urban development are proposed, such as embracing a regenerative perspective wherein the city and its constituents need to be understood as interdependent systemic elements while embracing a human-centric and ethical approach. Additionally, a transition from efficiency to sufficiency-oriented practices and a shift towards inclusive modes of participatory governance are proposed as fundamental principles for an empathic future of the built environment.}
}
@article{SHI2021103873,
title = {Working stage identification of excavators based on control signals of operating handles},
journal = {Automation in Construction},
volume = {130},
pages = {103873},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103873},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003241},
author = {Yupeng Shi and Yimin Xia and Lianglin Luo and Zhihong Xiong and Chengyu Wang and Laikuang Lin},
keywords = {Excavator, Working stage identification, Control signal, Operating handle, Long short-term memory (LSTM)},
abstract = {To improve the automated management level on construction sites, real-time monitoring of excavators' working stage for production efficiency and economic consumption analysis has been implemented in many projects, revealing great advantages. However, existing vision-based and non-vision-based working stage identification methods ignore the influence of response delay of hydraulic system on the recognition results. To overcome this problem, three machine learning algorithms, which select the control signals of operating handles that can reflect the actuator real-time operating status as segmentation marks, are used to establish the excavator working stage identification model in this study. The results show that the Long Short-Term Memory (LSTM) classifier has an accuracy of 93.21% and effectively reduces the lagging misidentification to 4.68%. This study contributes to automatic measurement of the excavator operational efficiency. For future work, the approach is combined with the adjustment strategy of engine working point to realize the staged energy-saving control of excavators.}
}
@article{2021I,
title = {Full Issue PDF},
journal = {JACC: Cardiovascular Imaging},
volume = {14},
number = {1},
pages = {I-CCCXXIII},
year = {2021},
issn = {1936-878X},
doi = {https://doi.org/10.1016/S1936-878X(20)31073-1},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20310731}
}
@article{WATERMANN2021103591,
title = {Predicting the self-regulated job search of mature-aged job seekers: The use of elective selection, loss-based selection, optimization, and compensation strategies},
journal = {Journal of Vocational Behavior},
volume = {128},
pages = {103591},
year = {2021},
issn = {0001-8791},
doi = {https://doi.org/10.1016/j.jvb.2021.103591},
url = {https://www.sciencedirect.com/science/article/pii/S0001879121000634},
author = {Henriette Watermann and Ulrike Fasbender and Ute-Christine Klehe},
keywords = {Aging, Job search, Mature-aged job seekers, Reemployment efficacy, Self-regulation, SOC strategies},
abstract = {Job search is a demanding and often demotivating process, challenging job-seekers' self-regulation. Particularly, mature-aged job seekers face lower reemployment chances – and may benefit from strategies known from the lifespan literature. The current study examined whether and when the use of aging strategies (elective selection, loss-based selection, optimization, and compensation; SOC strategies) can support mature-aged job seekers in their self-regulated job search process (goal establishment and goal pursuit). We collected data from 659 mature-aged job seekers in three countries (Germany, United Kingdom, and United States) at four different times over two months. Results of multi-level modeling showed no support for gain-oriented strategies, namely elective selection (prioritizing one instead of multiple goals) and optimization (investing every effort to reach one's goal). In contrast, loss-oriented strategies, namely loss-based selection (prioritizing or selecting a new goal after a setback) and compensation (using new or previously unused means in the face of obstacles), supported mature-aged job seekers' goal establishment and goal pursuit. Moreover, with increasing age, mature-aged job seekers reported lower reemployment efficacy (the confidence to find a new job), which moderated the relation between compensation with goal pursuit. Compensation was particularly helpful for mature-aged job seekers' goal pursuit in weeks in which they reported lower (vs. higher) reemployment efficacy. These findings highlight the importance of loss-oriented aging strategies as beneficial coping strategies. With regard to practice, the present study speaks to the benefits of SOC strategies and points to the development of interventions targeted toward mature-aged job seekers.}
}
@article{GIRAY2021111031,
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
journal = {Journal of Systems and Software},
volume = {180},
pages = {111031},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111031},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100128X},
author = {Görkem Giray},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review},
abstract = {Context:
Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems.
Objective:
The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems.
Method:
I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies.
Results:
The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions.
Conclusion:
The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.}
}
@article{KAVUR2021101950,
title = {CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation},
journal = {Medical Image Analysis},
volume = {69},
pages = {101950},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2020.101950},
url = {https://www.sciencedirect.com/science/article/pii/S1361841520303145},
author = {A. Emre Kavur and N. Sinem Gezer and Mustafa Barış and Sinem Aslan and Pierre-Henri Conze and Vladimir Groza and Duc Duy Pham and Soumick Chatterjee and Philipp Ernst and Savaş Özkan and Bora Baydar and Dmitry Lachinov and Shuo Han and Josef Pauli and Fabian Isensee and Matthias Perkonigg and Rachana Sathish and Ronnie Rajan and Debdoot Sheet and Gurbandurdy Dovletov and Oliver Speck and Andreas Nürnberger and Klaus H. Maier-Hein and Gözde {Bozdağı Akar} and Gözde Ünal and Oğuz Dicle and M. Alper Selver},
keywords = {Segmentation, Challenge, Abdomen, Cross-modality},
abstract = {Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative analysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS – Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Previous abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were designed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 ± 0.00 / 0.95 ± 0.01), but the best MSSD performance remains limited (21.89 ± 13.94 / 20.85 ± 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 ± 0.15 MSSD: 36.33 ± 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon.}
}
@article{MARTINEZRIOS2021102813,
title = {A review of machine learning in hypertension detection and blood pressure estimation based on clinical and physiological data},
journal = {Biomedical Signal Processing and Control},
volume = {68},
pages = {102813},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102813},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421004109},
author = {Erick Martinez-Ríos and Luis Montesinos and Mariel Alfaro-Ponce and Leandro Pecchia},
keywords = {Hypertension, Clinical data, Physiological data, Machine learning},
abstract = {The use of machine learning techniques in medicine has increased in recent years due to a rise in publicly available datasets. These techniques have been applied in high blood pressure studies following two approaches: hypertension stage classification based on clinical data and blood pressure estimation based on related physiological signals. This paper presents a literature review on such studies. We aimed to identify the best practices, challenges, and opportunities in developing machine learning models to detect hypertension or estimate blood pressure using clinical data and physiological signals. Hence, we identified and examined the machine learning techniques, publicly available datasets, and predictors used in previous studies. The feature selection techniques used to reduce model complexity are also reviewed. We found a lack of studies combining socio-demographic or clinical data with physiological signals, despite the correlation of blood pressure with photoplethysmography waveforms and variables such as age, gender, body mass index, and heart rate. Therefore, there is an opportunity to increase model performance by using both types of data for hypertension detection or blood pressure monitoring.}
}
@article{GREENLEAF2021105414,
title = {How far can Convention 108+ ‘globalise’? Prospects for Asian accessions},
journal = {Computer Law & Security Review},
volume = {40},
pages = {105414},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105414},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920300194},
author = {Graham Greenleaf},
abstract = {The ‘globalisation’ of Council of Europe data protection Convention 108 through non-European accessions has continued steadily, with eight such accessions since the first in 2013. The ‘modernisation’ of the Convention was completed on 10 October 2018 when the amending protocol for the new ‘Convention 108+’ became open for signature. Any new countries from outside Europe wishing to accede will have to accede to both Convention 108 and the amending Protocol (ie to 108+). The standards required of the laws of acceding countries by 108+ are higher than those required by 108, and are arguably mid-way between 108 and those of the European Union's General Data Protection Regulation (GDPR). This article examines to what extent each of the 26 ‘countries’ (separate jurisdictions) in Asia are likely to be able to accede to 108+, if they wish to. As yet, none have acceded to 108. It proposes an efficient way to consider such a question across such a complex set of jurisdictions. Fifteen of the 26 Asian countries already have data privacy laws, and two others have official Bills for such laws. An assessment of the prospects for accession can be done by considering in order the following grounds which may be impediments to accession: Jurisdictions which are not States; States which are not democratic; Laws of inadequate scope; Laws lacking an independent data protection authority; Laws with substantive provisions falling short of 108+ ‘accession standards’; States with proposed Bills only; and States with no relevant laws or proposed Bills. The most difficult step in this procedure is in deciding which of the substantive provisions of 108+ constitute its ‘accession standards’, or elements essential for accession to be invited. Neither the Convention, nor the guidelines issued by its Consultative Committee, shed much light on this question. However, previous practice under Convention 108, show there is some flexibility involved. The article concludes with suggestions as to how such flexibility can be made more transparent, and observations on which Asian countries, in light of the seven step assessment carried out in the article, are the most likely candidates to be able to accede to 108+, in both the short and medium terms.}
}
@article{JIN2021111345,
title = {Building occupancy forecasting: A systematical and critical review},
journal = {Energy and Buildings},
volume = {251},
pages = {111345},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111345},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821006290},
author = {Yuan Jin and Da Yan and Adrian Chong and Bing Dong and Jingjing An},
keywords = {Occupancy prediction, Forecast, Occupant behavior, Building, Operation, Energy conservation},
abstract = {Indoor environment construction for occupants has high energy consumption; as such, occupancy plays a noteworthy role in the complete life cycle phase of buildings, including design, operation, and retrofitting. In the past few years, building occupancy, which is considered the basis of occupant behavior, has attracted increasing attention from researchers. There are increasing requirements for buildings to be both comfortable and energy efficient; with the development of detection methods and analyzing algorithms, occupancy prediction has become a topic of interest for building automation and energy conservation. Therefore, this article reviews the literature regarding future building occupancy predictions (forecasting). This review is distinguished from occupancy simulation and detection research and focuses on the research purpose, physical routine, and complete methodology of occupancy forecasting. First, the research purposes, including the application field and detailed requirements for occupancy forecasting, are summarized and analyzed. Next, an overall methodology of occupancy forecasting, including data acquisition, modeling techniques, and evaluation, is discussed in terms of issues affecting prediction performance. Finally, the current challenges and perspectives of occupancy forecasting are highlighted, considering the insights of natural characteristics, on-site implementation, valid dataset sharing, and research techniques. Overall, accurate and robust future occupancy predictions will help to improve building system operations and energy conservation.}
}
@incollection{SAPIO2021595,
title = {Chapter 15 - Econometric modelling and forecasting of wholesale electricity prices},
editor = {Alessandro Rubino and Alessandro Sapio and Massimo {La Scala}},
booktitle = {Handbook of Energy Economics and Policy},
publisher = {Academic Press},
pages = {595-640},
year = {2021},
isbn = {978-0-12-814712-2},
doi = {https://doi.org/10.1016/B978-0-12-814712-2.00015-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128147122000154},
author = {Alessandro Sapio},
keywords = {Econometrics, Forecasting, Reduced-form models, Structural models, Stylised facts, Time series, Wholesale electricity prices},
abstract = {This chapter is an introduction to econometric modelling and forecasting electricity prices determined in liberalised wholesale power exchanges, focusing on a high frequency of observation (hourly, daily). Wholesale electricity prices pose a number of modelling and forecasting challenges because of the peculiar physical and economic features of electricity. After reviewing the stylised facts on wholesale electricity prices, such as seasonality, mean reversion, volatility clustering, spikes, and negative prices, the chapter illustrates the two main econometric approaches to modelling and forecasting: reduced-form models and structural models. The chapter overviews the empirical evidence on wholesale electricity prices; offers insights on how economic features of power generation and demand translate into empirically sound econometric models and provides ideas on comparison and selection of econometric models of wholesale electricity prices.}
}
@article{REN2021100178,
title = {Deep Learning-Based Weather Prediction: A Survey},
journal = {Big Data Research},
volume = {23},
pages = {100178},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100178},
url = {https://www.sciencedirect.com/science/article/pii/S2214579620300460},
author = {Xiaoli Ren and Xiaoyong Li and Kaijun Ren and Junqiang Song and Zichen Xu and Kefeng Deng and Xiang Wang},
keywords = {Deep learning, Big meteorological data, Weather forecasting, Spatio-temporal feature, Time series},
abstract = {Weather forecasting plays a fundamental role in the early warning of weather impacts on various aspects of human livelihood. For instance, weather forecasting provides decision making support for autonomous vehicles to reduce traffic accidents and congestions, which completely depend on the sensing and predicting of external environmental factors such as rainfall, air visibility and so on. Accurate and timely weather prediction has always been the goal of meteorological scientists. However, the conventional theory-driven numerical weather prediction (NWP) methods face many challenges, such as incomplete understanding of physical mechanisms, difficulties in obtaining useful knowledge from the deluge of observation data, and the requirement of powerful computing resources. With the successful application of data-driven deep learning method in various fields, such as computer vision, speech recognition, and time series prediction, it has been proven that deep learning method can effectively mine the temporal and spatial features from the spatio-temporal data. Meteorological data is a typical big geospatial data. Deep learning-based weather prediction (DLWP) is expected to be a strong supplement to the conventional method. At present, many researchers have tried to introduce data-driven deep learning into weather forecasting, and have achieved some preliminary results. In this paper, we survey the state-of-the-art studies of deep learning-based weather forecasting, in the aspects of the design of neural network (NN) architectures, spatial and temporal scales, as well as the datasets and benchmarks. Then we analyze the advantages and disadvantages of DLWP by comparing it with the conventional NWP, and summarize the potential future research topics of DLWP.}
}
@article{YANG2021115068,
title = {Artificial intelligence in ophthalmopathy and ultra-wide field image: A survey},
journal = {Expert Systems with Applications},
volume = {182},
pages = {115068},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115068},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421005091},
author = {Jie Yang and Simon Fong and Han Wang and Quanyi Hu and Chen Lin and Shigao Huang and Jian Shi and Kun Lan and Rui Tang and Yaoyang Wu and Qi Zhao},
keywords = {Ophthalmopathy, Ultra-wide field (UWF) imaging, Deep learning, Machine learning},
abstract = {Fundus digital photography and optical coherence tomography (OCT) are currently the primary imaging approaches for early diagnosis and treatment of eye diseases. In recent years, the significant development in artificial intelligence (AI), particularly in machine learning (ML) and deep learning (DL) are new and vital technical-driven motivations impacting on the traditional diagnosis and treatment methods. At the same time, the ultra-wide field (UWF) imaging technology is getting widely accepted and prevalent by its obvious advantageous features of non-dilate pupils, express-track result and the vast pool of fundus viewing angles. As a result, numerous research have been done to explore AI in ultra-wide field fundus imaging ophthalmology for joint diagnosis and treatment. However, the current review of this method is still in least ink. We first outlines the application and impact of AI technology in ophthalmic diseases in the past ten years. With the following part exclusively summarizing the technical integration of ultra-wide field fundus images and AI technology in the past four years, which has brought innovations to clinical treatment methods for the diagnosis and treatment of ophthalmic diseases; finally, we analyzed the application and implementation of the novel technology as well as the potential limitations and challenges, to predict the possibility of the technology’s further principles role and values in clinical ophthalmology.}
}
@article{GULER2021105115,
title = {A reformative framework for processes from building permit issuing to property ownership in Turkey},
journal = {Land Use Policy},
volume = {101},
pages = {105115},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2020.105115},
url = {https://www.sciencedirect.com/science/article/pii/S0264837720305044},
author = {Dogus Guler and Tahsin Yomralioglu},
keywords = {Building permit, Digitalization, Property ownership, 3D land administration, 3D city model, Turkey},
abstract = {A smart built environment has become necessary for ensuring social well-being due to uncontrolled population growth and unrestrainable urban sprawl. In this connection, effective land administration is a significant element to actualize sustainable development. Yet existing building permit procedures fail to satisfy the need for current construction demands because of the insufficient transparency and inefficient procedures. Two dimensional (2D) based systems also remain incapable to unambiguously delineate the property ownership related to complex buildings. Keeping up-to-date the three dimensional (3D) urban models is another key for smart cities but this issue has become difficult owing to the rapid changes in the cities. In this sense, this paper first examines thoroughly the current situation in Turkey in terms of the building permit procedures, land administration, and 3D city modeling. Then, the paper detailedly proposes a reformative framework. The framework consists of the use of digital building models for both building permit processes and 3D registration of property ownership, as well as updating the 3D city model databases. The proposed framework is evaluated in terms of its applicability with a discussion of the prospective directions.}
}
@article{HE2021113922,
title = {Artificial intelligence and machine learning assisted drug delivery for effective treatment of infectious diseases},
journal = {Advanced Drug Delivery Reviews},
volume = {178},
pages = {113922},
year = {2021},
issn = {0169-409X},
doi = {https://doi.org/10.1016/j.addr.2021.113922},
url = {https://www.sciencedirect.com/science/article/pii/S0169409X2100315X},
author = {Sheng He and Leon G. Leanse and Yanfang Feng},
keywords = {Artificial intelligence, Drug delivery, Infectious diseases, Antimicrobial resistance},
abstract = {In the era of antimicrobial resistance, the prevalence of multidrug-resistant microorganisms that resist conventional antibiotic treatment has steadily increased. Thus, it is now unquestionable that infectious diseases are significant global burdens that urgently require innovative treatment strategies. Emerging studies have demonstrated that artificial intelligence (AI) can transform drug delivery to promote effective treatment of infectious diseases. In this review, we propose to evaluate the significance, essential principles, and popular tools of AI in drug delivery for infectious disease treatment. Specifically, we will focus on the achievements and key findings of current research, as well as the applications of AI on drug delivery throughout the whole antimicrobial treatment process, with an emphasis on drug development, treatment regimen optimization, drug delivery system and administration route design, and drug delivery outcome prediction. To that end, the challenges of AI in drug delivery for infectious disease treatments and their current solutions and future perspective will be presented and discussed.}
}
@incollection{BAKER20211,
title = {1 - Future directions in digital information: Scenarios and themes},
editor = {David Baker and Lucy Ellis},
booktitle = {Future Directions in Digital Information},
publisher = {Chandos Publishing},
pages = {1-15},
year = {2021},
series = {Chandos Digital Information Review},
isbn = {978-0-12-822144-0},
doi = {https://doi.org/10.1016/B978-0-12-822144-0.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822144000001X},
author = {David Baker and Lucy Ellis},
keywords = {COVID-19, Public memory, Digital disruption, Delphi study, Digital education and training, Service innovation, Digital-first, Harvester paradigm, Digital literacy, Digital inclusiveness},
abstract = {This chapter introduces the purpose and aims of the book and provides an overview of each of the 19 chapters in terms of methodological approach and what they tell us about the future of digital information. Subheadings indicate the major areas of discussion that emerge from the whole and which reflect the issues of our time that occupy the minds and principles of scholars and practitioners. The chapter takes a look at the source of trends in digital information access and provision and considers the current issues and contexts for service innovation. The discussion of the key themes is reinforced and augmented by the results of the Delphi exercise and by selected thought pieces both of which are presented in text boxes.}
}
@article{ZHANG2021103372,
title = {A customized deep learning approach to integrate network-scale online traffic data imputation and prediction},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {132},
pages = {103372},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103372},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21003740},
author = {Zhengchao Zhang and Xi Lin and Meng Li and Yinhai Wang},
keywords = {Traffic prediction, Online data imputation, Deep learning, Bidirectional recurrent neural network, Graph convolution, 1 × 1 Convolution},
abstract = {Online data imputation and traffic prediction based on real-time data streams are essential for the intelligent transportation systems, particularly online navigation applications based on the real-time traffic information. However, the inevitable data missing problem caused by various disturbances undermines the information contained in such real-time data, thereby threatening the reliability of data acquisition as well as the prediction results. Such scenarios raise a strong need for integrating the tasks of network-scale online data imputation and traffic prediction, because the existing two-step approaches that separate the above procedures cannot be implemented in an online manner. In this paper, we propose a customized spatiotemporal deep learning architecture, named the graph convolutional bidirectional recurrent neural network (GCBRNN), to combine network-scale online data imputation and traffic prediction into an integrated task. The imputation mechanism and bidirectional framework are developed to cooperatively estimate missing entries and infer future values. We further design a network-scale graph convolutional gated recurrent unit (NGC-GRU) within the GCBRNN, which applies the graph convolution operation and 1×1 convolution module to capture the spatiotemporal dependencies in the traffic data. Experiments are carried out on two real-world traffic networks, including traffic speed and flow datasets. The comparison results demonstrate that our approach significantly outperforms several classical benchmark models with respect to both the imputation and prediction tasks on two datasets under various missing data rates.}
}
@article{MENTZELOPOULOS2021408,
title = {European Resuscitation Council Guidelines 2021: Ethics of resuscitation and end of life decisions},
journal = {Resuscitation},
volume = {161},
pages = {408-432},
year = {2021},
note = {European Resuscitation Council Guidelines for Resuscitation 2021},
issn = {0300-9572},
doi = {https://doi.org/10.1016/j.resuscitation.2021.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0300957221000708},
author = {Spyros D. Mentzelopoulos and Keith Couper and Patrick Van de Voorde and Patrick Druwé and Marieke Blom and Gavin D. Perkins and Ileana Lulic and Jana Djakow and Violetta Raffay and Gisela Lilja and Leo Bossaert},
abstract = {These European Resuscitation Council Ethics guidelines provide evidence-based recommendations for the ethical, routine practice of resuscitation and end-of-life care of adults and children. The guideline primarily focus on major ethical practice interventions (i.e. advance directives, advance care planning, and shared decision making), decision making regarding resuscitation, education, and research. These areas are tightly related to the application of the principles of bioethics in the practice of resuscitation and end-of-life care.}
}
@article{WIK2021100168,
title = {Proximity Extension Assay in Combination with Next-Generation Sequencing for High-throughput Proteome-wide Analysis},
journal = {Molecular & Cellular Proteomics},
volume = {20},
pages = {100168},
year = {2021},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2021.100168},
url = {https://www.sciencedirect.com/science/article/pii/S1535947621001407},
author = {Lotta Wik and Niklas Nordberg and John Broberg and Johan Björkesten and Erika Assarsson and Sara Henriksson and Ida Grundberg and Erik Pettersson and Christina Westerberg and Elin Liljeroth and Adam Falck and Martin Lundberg},
keywords = {biomarker, proteomics, next-generation sequencing, proximity extension assay, multiplex, immunoassay, plasma, serum, antibody},
abstract = {Understanding the dynamics of the human proteome is crucial for developing biomarkers to be used as measurable indicators for disease severity and progression, patient stratification, and drug development. The Proximity Extension Assay (PEA) is a technology that translates protein information into actionable knowledge by linking protein-specific antibodies to DNA-encoded tags. In this report we demonstrate how we have combined the unique PEA technology with an innovative and automated sample preparation and high-throughput sequencing readout enabling parallel measurement of nearly 1500 proteins in 96 samples generating close to 150,000 data points per run. This advancement will have a major impact on the discovery of new biomarkers for disease prediction and prognosis and contribute to the development of the rapidly evolving fields of wellness monitoring and precision medicine.}
}
@article{YU2021101136,
title = {Tracing the main path of interdisciplinary research considering citation preference: A case from blockchain domain},
journal = {Journal of Informetrics},
volume = {15},
number = {2},
pages = {101136},
year = {2021},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2021.101136},
url = {https://www.sciencedirect.com/science/article/pii/S1751157721000079},
author = {Dejian Yu and Tianxing Pan},
keywords = {Blockchain, Main path analysis, Discipline difference, Citation preference},
abstract = {Main path analysis has been widely used in various fields to detect their development trajectories. However, the previous methods treat every citation equally. In fact, it leaves a question open to scholars considering that there are different citation preferences in different disciplines and at different publication times. There are different citation preferences in different disciplines and at different periods, which are ignored by scholars. In order to deal with the problem in identifying development paths in interdisciplinary research areas, this paper proposes a new main path analysis method. The improved main path analysis considers two factors involved in citation preference, including discipline bias and time bias. An evidence analysis from blockchain domain is conducted to demonstrate the effectiveness of the proposed method. The research result shows that the proposed main path analysis method in this paper can resolve the problem of discipline bias and time bias in interdisciplinary research. Moreover, the improved method provides a more differentiated ranking for citation linkages in the network. Our research can enhance the objectivity of the resulting main paths and promote broader application of the main path analysis.}
}
@incollection{VALLERO2021293,
title = {Chapter 8 - Decision support tools},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {293-357},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000167},
author = {Daniel A. Vallero},
keywords = {Multicriteria decision analysis (MCDA), Bayesian belief network, Root cause analysis, Ishikawa diagram, Adverse outcome pathway (AOP), Big data, Informatics, Chemistry dashboard, Life cycle analysis (LCA), Integrated environmental modeling (IEM)},
abstract = {There are a growing number of models, modules, dashboards, and other tools that can be used to support sound risk assessments. This chapter discusses those that are most promising and applicable to environmental systems science. The chapter discusses approaches for evaluating complex systems. In addition, methods are recommended for analyzing projects and actions in terms of responsible and ethical content.}
}
@article{ALGAN2021106771,
title = {Image classification with deep learning in the presence of noisy labels: A survey},
journal = {Knowledge-Based Systems},
volume = {215},
pages = {106771},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106771},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121000344},
author = {Görkem Algan and Ilkay Ulusoy},
keywords = {Deep learning, Label noise, Classification with noise, Noise robust, Noise tolerant},
abstract = {Image classification systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or difficulty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in real-world datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overfit data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels. Differently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.}
}
@article{YAN2021101257,
title = {An Artificial Intelligence Model Considering Data Imbalance for Ship Selection in Port State Control Based on Detention Probabilities},
journal = {Journal of Computational Science},
volume = {48},
pages = {101257},
year = {2021},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101257},
url = {https://www.sciencedirect.com/science/article/pii/S187775032030555X},
author = {Ran Yan and Shuaian Wang and Chuansheng Peng},
keywords = {Port state control inspection, ship detention, machine learning in maritime transportation, artificial intelligence in maritime transportation, imbalanced data},
abstract = {Port state control inspection is seen as a safety net to guard marine safety, protect the marine environment, and guarantee decent onboard working and living conditions for seafarers. A substandard ship can be detained in an inspection if serious deficiencies are found onboard. Ship detention is regarded as a severe result in port state control inspection. However, developing accurate prediction models for ship detention based on ship’s generic factors (e.g. ship age, type, and flag), dynamic factors (e.g. times of ship flag changes), and inspection historical factors (e.g. total previous detentions in PSC inspection, last PSC inspection time, and last deficiency number in PSC inspection) before an inspection is conducted is not a trivial task as the low detention rate leads to a highly imbalanced inspection records dataset. To address this issue, this paper develops a classification model called balanced random forest (BRF) to predict ship detention by using 1,600 inspection records at the Hong Kong port for three years. Numerical experiments show that the proposed BRF model can identify 81.25% of all the ships with detention in the test set which contains another 400 inspection records. Compared with the current ship selection method at the Hong Kong port, the BRF model is much more efficient and can achieve an average improvement of 73.72% in detained ship identification.}
}
@article{BARROS2021100192,
title = {A systematic literature review about dimensioning safety stock under uncertainties and risks in the procurement process},
journal = {Operations Research Perspectives},
volume = {8},
pages = {100192},
year = {2021},
issn = {2214-7160},
doi = {https://doi.org/10.1016/j.orp.2021.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2214716021000142},
author = {Júlio Barros and Paulo Cortez and M. Sameiro Carvalho},
keywords = {Safety stocks, Inventory management, Procurement, Supply chain risk management, Uncertainty factors, Systematic literature review},
abstract = {This paper analyses literature contributions in the search for safety stock problem under uncertainties and risks in the procurement process, focusing on the dimensioning problem (determination of the safety stock level). We perform a systematic literature review (SLR) from 1995 to 2019 in relevant journals, covering 193 selected articles. These selected articles were classified into three safety stock main issues: safety stock dimensioning, safety stock management, and safety stock positioning, allocation or placement. The SLR analysis allowed the identification of literature gaps and research opportunities, thus providing a road map to guide future research on this topic.}
}
@article{BELLMAN202129,
title = {Self-improving system integration: Mastering continuous change},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {29-46},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330430},
author = {Kirstie Bellman and Jean Botev and Ada Diaconescu and Lukas Esterle and Christian Gruhl and Christopher Landauer and Peter R. Lewis and Phyllis R. Nelson and Evangelos Pournaras and Anthony Stein and Sven Tomforde},
keywords = {Self-integration, Self-improvement, Autonomous systems, Taxonomy, Organic computing, System engineering},
abstract = {The research initiative “self-improving system integration” (SISSY) was established with the goal to master the ever-changing demands of system organisation in the presence of autonomous subsystems, evolving architectures, and highly-dynamic open environments. It aims to move integration-related decisions from design-time to run-time, implying a further shift of expertise and responsibility from human engineers to autonomous systems. This introduces a qualitative shift from existing self-adaptive and self-organising systems, moving from self-adaptation based on predefined variation types, towards more open contexts involving novel autonomous subsystems, collaborative behaviours, and emerging goals. In this article, we revisit existing SISSY research efforts and establish a corresponding terminology focusing on how SISSY relates to the broad field of integration sciences. We then investigate SISSY-related research efforts and derive a taxonomy of SISSY technology. This is concluded by establishing a research road-map for developing operational self-improving self-integrating systems.}
}
@article{SUN20211865,
title = {In vivo structural characterization of the SARS-CoV-2 RNA genome identifies host proteins vulnerable to repurposed drugs},
journal = {Cell},
volume = {184},
number = {7},
pages = {1865-1883.e20},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421001586},
author = {Lei Sun and Pan Li and Xiaohui Ju and Jian Rao and Wenze Huang and Lili Ren and Shaojun Zhang and Tuanlin Xiong and Kui Xu and Xiaolin Zhou and Mingli Gong and Eric Miska and Qiang Ding and Jianwei Wang and Qiangfeng Cliff Zhang},
keywords = {SARS-CoV-2, RNA secondary structure, host factor, RBP binding prediction, drug reproposing},
abstract = {Summary
Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the cause of the ongoing coronavirus disease 2019 (COVID-19) pandemic. Understanding of the RNA virus and its interactions with host proteins could improve therapeutic interventions for COVID-19. By using icSHAPE, we determined the structural landscape of SARS-CoV-2 RNA in infected human cells and from refolded RNAs, as well as the regulatory untranslated regions of SARS-CoV-2 and six other coronaviruses. We validated several structural elements predicted in silico and discovered structural features that affect the translation and abundance of subgenomic viral RNAs in cells. The structural data informed a deep-learning tool to predict 42 host proteins that bind to SARS-CoV-2 RNA. Strikingly, antisense oligonucleotides targeting the structural elements and FDA-approved drugs inhibiting the SARS-CoV-2 RNA binding proteins dramatically reduced SARS-CoV-2 infection in cells derived from human liver and lung tumors. Our findings thus shed light on coronavirus and reveal multiple candidate therapeutics for COVID-19 treatment.}
}
@article{ORTMEIER2021163,
title = {Framework for the integration of Process Mining into Life Cycle Assessment},
journal = {Procedia CIRP},
volume = {98},
pages = {163-168},
year = {2021},
note = {The 28th CIRP Conference on Life Cycle Engineering, March 10 – 12, 2021, Jaipur, India},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121000470},
author = {Christian Ortmeier and Nadja Henningsen and Adrian Langer and Alexander Reiswich and Alexander Karl and Christoph Herrmann},
keywords = {process mining, process discovery, life cycle assesment, hotspot analyzes, life cycle inventory},
abstract = {An increasing product variance, shorter life cycles and the integration of new products and technologies into existing factories lead to a high complexity in today’s production systems. This makes it difficult to carry out Life Cycle Assessments (LCA) continuously and effectively. Major challenges of LCA are on the one hand a high expenditure of time and on the other hand a static evaluation of individual products. In order to perform dynamic and continuous process analyses, companies increasingly rely on new technologies and methods. Numerous studies have already been able to tap promising potentials by using Process Mining (PM). In contrast to traditional methods of process modeling, PM uses event log data to model the actual production processes. Based on this data, a real-time model is built to identify waste. In a holistic approach, PM is able to automatically uncover social and organizational networks and map them in a simulation model. According to the current state of research, there is no concept for integrating PM into LCA. Therefore, the present work focuses on the investigation of interfaces between PM and LCA in order to systematically identify and evaluate the potentials of using PM. The results are applied to a use case in the sector of commercial vehicles.}
}
@article{PAN2021103254,
title = {Discovering optimal strategies for mitigating COVID-19 spread using machine learning: Experience from Asia},
journal = {Sustainable Cities and Society},
volume = {75},
pages = {103254},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103254},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721005308},
author = {Yue Pan and Limao Zhang and Zhenzhen Yan and May O. Lwin and Miroslaw J. Skibniewski},
keywords = {COVID-19, Random forest regression, Feature importance analysis, Multi-objective optimization, Sustainability},
abstract = {To inform data-driven decisions in fighting the global pandemic caused by COVID-19, this research develops a spatiotemporal analysis framework under the combination of an ensemble model (random forest regression) and a multi-objective optimization algorithm (NSGA-II). It has been verified for four Asian countries, including Japan, South Korea, Pakistan, and Nepal. Accordingly, we can gain some valuable experience to better understand the disease evolution, forecast the prevalence of the disease, which can provide sustainable evidence to guide further intervention and management. Random forest with a proper rolling time-window can learn the combined effects of environmental and social factors to accurately predict the daily growth of confirmed cases and daily death rate on a national scale, which is followed by NSGA-II to find a range of Pareto optimal solutions for ensuring the minimization of the infection rate and mortality at the same time. Experimental results demonstrate that the predictive model can alert the local government in advance, allowing the accused time to put forward relevant measures. The temperature in the category of environment and the stringency index belonging to the social factor are identified as the top 2 important features to exert a greater impact on the virus transmission. Moreover, optimal solutions provide references to design the best control strategies towards pandemic containment and prevention that can accommodate the country-specific circumstance, which are possible to decrease the two objectives by more than 95%. In particular, appropriate adjustment of social-related features needs to take priority over others, since it can bring about at least 1.47% average improvement of two objectives compared to environmental factors.}
}
@incollection{GRASSO2021301,
title = {11 - Process monitoring of laser powder bed fusion},
editor = {Igor Yadroitsev and Ina Yadroitsava and Anton {du Plessis} and Eric MacDonald},
booktitle = {Fundamentals of Laser Powder Bed Fusion of Metals},
publisher = {Elsevier},
pages = {301-326},
year = {2021},
series = {Additive Manufacturing Materials and Technologies},
isbn = {978-0-12-824090-8},
doi = {https://doi.org/10.1016/B978-0-12-824090-8.00012-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240908000123},
author = {Marco Grasso and Bianca Maria Colosimo and Kevin Slattery and Eric MacDonald},
keywords = {Anomaly detection, In-situ inspection, In-situ monitoring, Process control, Process signatures},
abstract = {The layerwise production paradigm entailed by the Laser Powder Bed Fusion process makes potentially available a large amount of information on a layer-by-layer basis, to determine the stability of the process and anticipate quality inspections of the part while it is being produced. Such information can be gathered through a variety of sensors used in-situ and in-line, ranging from pyrometers to high spatial and/or temporal resolution cameras or acoustic emission sensors. This chapter provides an overview of the quantities that can be acquired as signatures of the process and proxies of the final quality of the part, the methods suitable to make sense of acquired signals to detect anomalies, and process control solutions for defect mitigation, correction, or avoidance. The chapter also provides an up-to-date perspective on the current open issues for a wider industrial adoption of these techniques and most promising future research directions.}
}
@article{WU2021112247,
title = {Does internet development improve green total factor energy efficiency? Evidence from China},
journal = {Energy Policy},
volume = {153},
pages = {112247},
year = {2021},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2021.112247},
url = {https://www.sciencedirect.com/science/article/pii/S0301421521001166},
author = {Haitao Wu and Yu Hao and Siyu Ren and Xiaodong Yang and Guo Xie},
keywords = {Internet development, Green total factor energy efficiency, Spatial durbin model, Dynamic threshold model},
abstract = {Information and communication technology supported by the internet has become an important driving force that promotes the intelligent development of environmental governance in China. Using Chinese provincial panel data for the period 2006–2017, this study investigates whether the internet has improved China's green total factor energy efficiency (GTFEE) using a dynamic spatial Durbin model, mediation effect model and dynamic threshold panel model. The empirical results indicate that the GTFEE has a significant positive spatial correlation. Internet development can not only directly improve local GTFEE but also improve GTFEE in neighboring regions. After accounting for potential endogeneity, this conclusion is still valid. Meanwhile, internet development can indirectly improve regional GTFEE by reducing the degree of resource mismatch while enhancing GTFEE by improving regional innovation capabilities and promoting industrial structure upgrades. In addition, the regression results of the dynamic threshold model show that there is a nonlinear relationship between the influence of the internet development and GTFEE. Specifically, due to an increase in the degree of labor resource mismatch and capital resource mismatch, the impact of the internet on GTFEE has gradually decreased, and this effect has gradually increased with the improvement of regional innovation capabilities and the industrial structure.}
}
@article{WILSON2021101526,
title = {Beyond the supply side: Use and impact of municipal open data in the U.S},
journal = {Telematics and Informatics},
volume = {58},
pages = {101526},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101526},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320301854},
author = {Bev Wilson and Cong Cong},
keywords = {Open data, Local government, Civic technology, Digital equity},
abstract = {While the number of open government data initiatives has increased considerably over the past decade, the impact of these initiatives remains uncertain. Recent studies have been critical of the “bias toward the supply side” and lack of “sufficient attention to the user perspective” in the way that open government data initiatives are implemented. This article asks: (1) who is using municipal open government data resources and for what purposes? and (2) what impact are municipal open government data having in cities where they have been implemented? We performed a qualitative analysis of 26 semi-structured telephone interviews conducted with government staff, civic technologists, and private sector stakeholders in nine cities around the United States. Each of these 30 to 45-minute telephone interviews were transcribed and analyzed to distill insights regarding the use and impact of municipal open government data in the nine cities considered. We find that the array of actors within open government data ecosystems at the local level is expanding as distinctions between the public and private sectors becomes increasingly blurred and that the demands of managing and sustaining these initiatives has led to changes in the services offered by local government, as well as in the duties of government staff. The impact of these data resources has been primarily felt within local government itself, although the lack of monitoring mechanisms makes it difficult to systematically evaluate their broader effects. We conclude that open government data initiatives should be coordinated and better integrated with digital equity and digital inclusion efforts in order to advance their political and social goals.}
}
@incollection{2021679,
title = {Index},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {679-692},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.09991-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539099918}
}
@article{CAESAR20212041,
title = {Metabolomics and genomics in natural products research: complementary tools for targeting new chemical entities},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {2041-2065},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00036e},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008820},
author = {Lindsay K. Caesar and Rana Montaser and Nancy P. Keller and Neil L. Kelleher},
abstract = {ABSTRACT
Covering: 2010 to 2021 Organisms in nature have evolved into proficient synthetic chemists, utilizing specialized enzymatic machinery to biosynthesize an inspiring diversity of secondary metabolites. Often serving to boost competitive advantage for their producers, these secondary metabolites have widespread human impacts as antibiotics, anti-inflammatories, and antifungal drugs. The natural products discovery field has begun a shift away from traditional activity-guided approaches and is beginning to take advantage of increasingly available metabolomics and genomics datasets to explore undiscovered chemical space. Major strides have been made and now enable -omics-informed prioritization of chemical structures for discovery, including the prospect of confidently linking metabolites to their biosynthetic pathways. Over the last decade, more integrated strategies now provide researchers with pipelines for simultaneous identification of expressed secondary metabolites and their biosynthetic machinery. However, continuous collaboration by the natural products community will be required to optimize strategies for effective evaluation of natural product biosynthetic gene clusters to accelerate discovery efforts. Here, we provide an evaluative guide to scientific literature as it relates to studying natural product biosynthesis using genomics, metabolomics, and their integrated datasets. Particular emphasis is placed on the unique insights that can be gained from large-scale integrated strategies, and we provide source organism-specific considerations to evaluate the gaps in our current knowledge.}
}
@incollection{2021567,
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {567-611},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-816077-0.09001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128160770090014}
}
@article{NGUYEN2021102121,
title = {Research manuscript: The Bullwhip Effect in rule-based supply chain planning systems–A case-based simulation at a hard goods retailer},
journal = {Omega},
volume = {98},
pages = {102121},
year = {2021},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2019.102121},
url = {https://www.sciencedirect.com/science/article/pii/S0305048318308855},
author = {Duy Tan Nguyen and Yossiri Adulyasak and Sylvain Landry},
keywords = {Flowcasting, Distribution, Logistics, Bullwhip Effect, Simulation, Factor analysis, Regression analysis},
abstract = {The vision of a well-integrated supply chain (SC) was developed as early as 1958 by Forrester, who addressed what would eventually be called the Bullwhip Effect (BWE). The Flowcasting concept, originally called Retail Resource Planning, was proposed to connect all SC upper tiers to the storefront through fulfillment logic based on the Distribution Resource Planning (DRP) system. This method can therefore be understood as fully or SC-wide integrated DRP with a focus on the role of retailers instead of that of vendors or distributors. We studied a Canadian retailer that implemented Flowcasting in order to gain insight into the benefits and operational logic of this system. Based on the data obtained from the company, we simulate Flowcasting operations across a 3-tier SC compared to the Reorder Point (ROP) system, which was previously used at the firm, as well as a combination of ROP and DRP (ROP/DRP or partially integrated DRP), which are some of the most common implementations in use. The simulation is configured based on the company's settings, including historical average demand, demand estimates, lead time, etc. Then, multivariate regression is deployed to statistically compare the efficacy of these methods in SC management using various assessment criteria, including BWE measures. The results show that the requirement calculation logic used in an SC-wide integrated DRP system (Flowcasting) generally outperforms the other two approaches, and its benefits in curtailing the BWE become more noticeable in the upper tiers of the SC. This paper indicates the enormous potential of SC-wide integrated DRP logic in rule-based replenishment planning systems.}
}
@article{DEFRAEYE2021245,
title = {Digital twins are coming: Will we need them in supply chains of fresh horticultural produce?},
journal = {Trends in Food Science & Technology},
volume = {109},
pages = {245-258},
year = {2021},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2021.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S092422442100025X},
author = {Thijs Defraeye and Chandrima Shrivastava and Tarl Berry and Pieter Verboven and Daniel Onwude and Seraina Schudel and Andreas Bühlmann and Paul Cronje and René M. Rossi},
keywords = {Postharvest, Physics-based, Virtual, Modeling, Simulation, Cyber-physical},
abstract = {Background
Digital twins have advanced fast in various industries, but are just emerging in postharvest supply chains. A digital twin is a virtual representation of a certain product, such as fresh horticultural produce. This twin is linked to the real-world product by sensors supplying data of the environmental conditions near the target fruit or vegetable. Statistical and data-driven twins quantify how quality loss of fresh horticultural produce occurs by grasping patterns in the data. Physics-based twins provide an augmented insight into the underlying physical, biochemical, microbiological and physiological processes, enabling to explain also why this quality loss occurs.
Scope and approach
We identify what the key advantages are of digital twins and how the supply chain of fresh horticultural produce can benefit from them in the future.
Key findings and conclusions
A digital twin has a huge potential to help horticultural produce to tell its history as it drifts along throughout its postharvest life. The reason is that each shipment is subject to a unique and unpredictable set of temperature and gas atmosphere conditions from farm to consumer. Digital twins help to identify the resulting, largely uncharted, postharvest evolution of food quality. The benefit of digital twins particularly comes forward for perishable species and at low airflow rates. Digital twins provide actionable data for exporters, retailers, and consumers, such as the remaining shelf life for each shipment, on which logistics decisions and marketing strategies can be based. The twins also help diagnose and predict potential problems in supply chains that will reduce food quality and induce food loss. Twins can even suggest preventive shipment-tailored measures to reduce retail and household food losses.}
}
@article{XU2021458,
title = {Anti-corruption, safety compliance and coal mine deaths: Evidence from China},
journal = {Journal of Economic Behavior & Organization},
volume = {188},
pages = {458-488},
year = {2021},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2021.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167268121001992},
author = {Gang Xu and Xue Wang and Ruiting Wang and Go Yano and Rong Zou},
keywords = {Anti-corruption, Safety compliance, Coal mine deaths, China},
abstract = {This study evaluates the impact of the anti-corruption campaign launched by President Xi since 2013 on coal mine mortality in China. Combining several unique provincial-level datasets on coal mines from 2004 to 2017, we find evidence that provinces with stronger exposure to the anti-corruption campaign have experienced a significantly larger decrease in coalmine death rates. This effect survives a vast array of robustness checks and also displays great heterogeneity. Further evidence suggests the campaign has led to fewer safety violations, more fixed investments and a decrease in profits accompanied by an increase in the costs of principal business in the coal mining industry. The above findings are most consistent with the interpretation that the campaign has made coal mining firms less likely to shirk on safety by curbing collusion between coal mines and local officials. We also rule out other channels such as intensified inspection and the change of employment composition in the industry.}
}
@article{MALAGNINO2021127716,
title = {Building Information Modeling and Internet of Things integration for smart and sustainable environments: A review},
journal = {Journal of Cleaner Production},
volume = {312},
pages = {127716},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127716},
url = {https://www.sciencedirect.com/science/article/pii/S095965262101934X},
author = {Ada Malagnino and Teodoro Montanaro and Mariangela Lazoi and Ilaria Sergi and Angelo Corallo and Luigi Patrono},
keywords = {Environmental sustainability, Smart environment, Internet of Things, BIM (Building Information Modeling), IoT},
abstract = {During the last decades, society has increasingly moved towards the adoption of digital solutions in almost every aspect of people's lives with the aim of enhancing daily activities. At the same time, the environmental impact of the built environment has attracted the attention of public opinion that is gradually perceiving the necessity of limiting its negative effects in order to safeguard the Earth and people's wellbeing. The Internet of Things is one of the biggest ecosystems that is bringing innovations encompassing digital solutions in almost every sector. On the other hand, the Building Information Modeling approach allows for data sharing among stakeholders, traceability, and the integrated management of the building or infrastructure life-cycle through a 3D informative virtual model. Our study reviews existing research works and technological solutions that integrate these two important topics to enhance the sustainability of the built environment, making it smarter. The presented review analyses the existing papers available in literature from January 2015 to December 2020, to present the best practices in this integration and discuss limitations of the identified solutions. Based on the outcomes of the analysis and aiming at the creation of a solid knowledge basis for the community interested in the sector, a comprehensive modular architecture has been proposed. Finally, new directions for future works are presented by discussing how the proposed architecture can actually facilitate the design and development phases.}
}
@article{KURNIAWAN2021195,
title = {Mobile computing and communications-driven fog-assisted disaster evacuation techniques for context-aware guidance support: A survey},
journal = {Computer Communications},
volume = {179},
pages = {195-216},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002802},
author = {Ibnu Febry Kurniawan and A. Taufiq Asyhari and Fei He and Ye Liu},
keywords = {Disaster recovery, Evacuation guidance, Fog, Fog computing, Fog communications, Collaborative analytics},
abstract = {The importance of an optimal solution for disaster evacuation has recently raised attention from researchers across multiple disciplines. This is not only a serious, but also a challenging task due to the complexities of the evacuees’ behaviors, route planning, and demanding coordination services. Although existing studies have addressed these challenges to some extent, mass evacuation in natural disasters tends to be difficult to predict and manage due to the limitation of the underlying models to capture realistic situations. It is therefore desirable to have on-demand mechanisms of locally-driven computing and data exchange services in order to enable near real-time capture of the disaster area during the evacuation. For this purpose, this paper comprehensively surveys recent advances in information and communication technology-enabled disaster evacuations, with the focus on fog computation and communication services to support a massive evacuation process. A numerous variety of tools and techniques are encapsulated within a coordinated on-demand strategy of an evacuation platform, which is aimed to provide a situational awareness and response. Herein fog services appear to be one of the viable options for responsive mass evacuation because they enable low latency data processing and dissemination. They can additionally provide data analytics support for autonomous learning for both the short-term guidance supports and long-term usages. This work extends the existing data-oriented framework by outlining comprehensive functionalities and providing seamless integration. We review the principles, challenges, and future direction of the state-of-the-art strategies proposed to sit within each functionality. Taken together, this survey highlights the importance of adaptive coordination and reconfiguration within the fog services to facilitate responsive mass evacuations as well as open up new research challenges associated with analytics-embedding network and computation, which is critical for any disaster recovery activities.}
}
@article{STEPANYAN2021104929,
title = {Multiple rotations of Gaussian quadratures: An efficient method for uncertainty analyses in large-scale simulation models},
journal = {Environmental Modelling & Software},
volume = {136},
pages = {104929},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104929},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220309865},
author = {Davit Stepanyan and Harald Grethe and Georg Zimmermann and Khalid Siddig and Andre Deppermann and Arndt Feuerbacher and Jonas Luckmann and Hugo Valin and Takamasa Nishizawa and Tatiana Ermolieva and Petr Havlik},
keywords = {Uncertainty analysis, Systematic sensitivity analysis, Stochastic modeling, Multiple rotations of Gaussian quadratures, Monte Carlo sampling, Computable general equilibrium models, Partial equilibrium models},
abstract = {Concerns regarding the impact of climate change, food price volatility, and weather uncertainty have motivated users of simulation models to consider uncertainty in their simulations. One way to do this is to integrate uncertainty components in the model equations, thus turning the model into a problem of numerical integration. Most of these problems do not have analytical solutions, and researchers, therefore, apply numerical approximation methods. This article presents a novel approach to conducting an uncertainty analysis as an alternative to the computationally burdensome Monte Carlo-based (MC) methods. The developed method is based on the degree three Gaussian quadrature (GQ) formulae and is tested using three large-scale simulation models. While the standard single GQ method often produces low-quality approximations, the results of this study demonstrate that the proposed approach reduces the approximation errors by a factor of nine using only 3.4% of the computational effort required by the MC-based methods in the most computationally demanding model.}
}
@article{LIANG2021107548,
title = {Industrial time series determinative anomaly detection based on constraint hypergraph},
journal = {Knowledge-Based Systems},
volume = {233},
pages = {107548},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107548},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008108},
author = {Zheng Liang and Hongzhi Wang and Xiaoou Ding and Tianyu Mu},
keywords = {Industrial time series, Anomaly detection, Constraint hypergraph},
abstract = {The explosive growth of time series captured by sensors in industrial pipelines gives rise to the flourish of intelligent industry. Exploiting the value of these time series is conductive to workload balancing and production optimization. Unfortunately, knowledge obtained from the mining process turns out to be insufficient for use due to widespread anomalies, indicating machine breakdown, sensor failure or working status shifts. To tackle this problem, we propose a constraint hypergraph-based method, combining multiple constraints for anomaly detection. We develop strategies for adaptive determinative anomaly detection and anomaly pattern mining. We also investigate the problem of Anomaly Pattern Matching, prove its NP-completeness, and propose algorithms to obtain its global and local optimum. Finally, we demonstrate our approach with three real world datasets from a real powerplant, a chemical production pipeline and a hydraulic system. The experimental results show that our approach can effectively and efficiently work under different circumstances.}
}
@article{CUGNO2021120756,
title = {Openness to Industry 4.0 and performance: The impact of barriers and incentives},
journal = {Technological Forecasting and Social Change},
volume = {168},
pages = {120756},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120756},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521001888},
author = {Monica Cugno and Rebecca Castagnoli and Giacomo Büchi},
keywords = {Industry 4.0, Openness, Performance, Barriers, Incentives, Mediators},
abstract = {The impact of barriers and incentives on the relationship between openness to Industry 4.0 and performance have so far received little scholarly attention. As a result, this paper explores this relationship by employing a mixed methods approach. A qualitative analysis using in-depth interviews and multiple case studies identifies prominent barriers and incentives, whilst a quantitative analysis on a representative sample of 500 local manufacturing units in Piedmont (a region of Northern Italy) is undertaken via an OLS regression-based path analysis. The results of the parallel-serial multiple mediation model show that: (1) greater openness to Industry 4.0 is related to better performance; (2) greater openness to Industry 4.0 leads to a higher perception of barriers; (3) greater knowledge-related and economic and financial barriers improve performance, abstracting from the adoption of incentives; and (4) greater openness to Industry 4.0 drives the adoption of incentives. However, perceived economic and financial barriers are found not to drive firms to adopt more incentives. The study contributes to the Industry 4.0 literature by identifying previously unidentified strengths and weaknesses to barriers and incentives, and highlights the necessity of policies that reflect real firms’ needs.}
}
@article{PHAOSATHIANPHAN2021100882,
title = {An intelligent travel technology assessment model for destination impacts of tourist adoption},
journal = {Tourism Management Perspectives},
volume = {40},
pages = {100882},
year = {2021},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2021.100882},
url = {https://www.sciencedirect.com/science/article/pii/S2211973621000957},
author = {Noppadol Phaosathianphan and Adisorn Leelasantitham},
keywords = {Technology Acceptance Model, IS Success, IS Continuance, Intelligent travel technology assessment model, Destination Impacts, Human-Computer Interaction, Socio-Technical System},
abstract = {As intelligent travel technology creates a business trend to support the travel and tourism industry, it is necessary to create a complete model to assess the users in terms of continuous user acceptance and destination impacts which are Competitiveness, Loyalty, and Sustainability with entire stakeholders (travellers, service providers, and destinations). The proposed conceptual model is formulated based on four studies: A Plenary Free Individual Traveler Life Cycle, IS Success, IS Continuance, and Destination Impacts of Travel and Tourism. The sample and data collections were done through the online questionnaire and survey via social media platforms such as Facebook and Line with individuals who have used intelligent personal assistants such as Google Assistant, Apple Siri, Microsoft Cortana, Amazon Alexa, and Samsung Bixby for travel and tourism. 400 respondents were analyzed with descriptive statistics and inferential statistics using the measurement model and the structural model by PASW Statistics and SmartPLS.}
}
@article{RALLO2021106645,
title = {Updated single and dual crop coefficients for tree and vine fruit crops},
journal = {Agricultural Water Management},
volume = {250},
pages = {106645},
year = {2021},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2020.106645},
url = {https://www.sciencedirect.com/science/article/pii/S0378377420321892},
author = {G. Rallo and T.A. Paço and P. Paredes and À. Puig-Sirera and R. Massai and G. Provenzano and L.S. Pereira},
keywords = {K and K values, Vineyards, Evergreen fruit trees, Deciduous fruit trees, Nut trees, Tropical fruit crops, Berries and hop},
abstract = {The present study reviews the research on the FAO56 crop coefficients of fruit trees and vines performed over the past twenty years. The main objective was to update information and extend tabulated single (Kc) and basal (Kcb) standard crop coefficients. The selection and analysis of the literature for this review have been done to consider only studies that adhere to FAO56 method, computing the reference ET with the FAO Penman–Monteith​ ETo equation and field measuring crop ET with proved accuracy. The crops considered refer to vine fruit crops, berries and hops, temperate climate evergreen fruit trees, temperate climate deciduous fruit trees and, tropical and subtropical fruit crops. Papers satisfying the conditions expressed above, and that studied the crops under pristine or appropriate eustress conditions, were selected to provide for standard Kc and Kcb data. Preference was given to studies reporting on the fraction of ground cover (fc), crop height (h), planting density, crop age and adopted training systems. The Kc and Kcb values obtained from the selected literature generally show coherence relative to the crop biophysical characteristics and reflect those characteristics, mainly fc, h and training systems. The ranges of reported Kc and Kcb values were grouped according to crop density, particularly fc and h, and were compared with FAO56 (Allen et al., 1998) previously tabulated Kc and Kcb values, as well as by Allen and Pereira (2009) and Jensen and Allen (2016), which lead to define update indicative standard Kc and Kcb values. These values are aimed for use in crop water requirement computations and modeling for irrigation planning and scheduling, thus also aimed at supporting improved water use and saving in orchards and vines.}
}
@article{BONESTROO2021104698,
title = {Diagnostic properties of milk diversion and farmer-reported mastitis to indicate clinical mastitis status in dairy cows using Bayesian latent class analysis},
journal = {Livestock Science},
volume = {253},
pages = {104698},
year = {2021},
issn = {1871-1413},
doi = {https://doi.org/10.1016/j.livsci.2021.104698},
url = {https://www.sciencedirect.com/science/article/pii/S1871141321003061},
author = {John Bonestroo and Nils Fall and Mariska {van der Voort} and Ilka Christine Klaas and Henk Hogeveen and Ulf Emanuelson},
keywords = {antibiotic treatment, proxy, Automatic milking system, Milk withdrawal, Latent class analysis},
abstract = {The development of digital farming gives bovine mastitis research and management tools access to large datasets. However, the quality of registered data on clinical mastitis cases or treatments may be inadequate (e.g. due to missing records). In automatic milking systems, the decision to divert milk from the bulk milk tank during milking is registered (i.e. milk diversion indicator) for every milking and could potentially indicate a clinical mastitis case. This study accordingly estimated the diagnostic performance of a milk diversion indicator in relation to farmer-recorded clinical mastitis cases in the absence of a “gold standard”. Data on milk diversion and farmer-reported clinical mastitis from 3,443 lactations in 13 herds were analyzed. Each cow lactation was split into 30-DIM periods in which it was registered whether milk was diverted and whether clinical mastitis was reported. One 30-DIM period was randomly sampled for each lactation and this was the unit of analysis, this procedure was repeated 300 times, resulting in 300 datasets to create autocorrelation-robust results during analysis. We used Bayesian latent class analysis to assess the diagnostic properties of milk diversion and farmer-reported clinical status. We analyzed different episode lengths of milk diversion of 1 or more milk diversion days until 10 or more milk diversion days for two scenarios: farmers with poor-quality (51% sensitivity, 99% specificity) and high-quality (90% sensitivity, 99% specificity) mastitis registrations. The analysis was done for all 300 datasets. The results showed that for the scenario where the quality of clinical mastitis reporting was high, the sensitivity was similar for milk-diversion threshold durations of 1–4 days (0.843 to 0.793 versus 0.893). Specificity increased when the number of days of milk diversion increased and was ≥98% at a milk-diversion threshold durations of 8 or more consecutive milk diversion days. In the scenario where the quality of clinical mastitis reporting was low, the sensitivity of milk diversion and reported clinical mastitis cases was similar at milk-diversion threshold durations of 1–7 days (0.687 to 0.448 versus 0.503 to 0.504) while specificity exceeded the 98% at milk-diversion threshold durations of 7 or more consecutive milk diversion days. In both scenarios, a milk diversion threshold duration of 4–7 days achieved the most desirable combined sensitivity and specificity. This study concluded that milk diversion can be a valid alternative to farmer-reported clinical mastitis as it performs similarly in indicating actual clinical mastitis.}
}
@article{HALAWA2021115696,
title = {Integrated framework of process mining and simulation–optimization for pod structured clinical layout design},
journal = {Expert Systems with Applications},
volume = {185},
pages = {115696},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115696},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421010800},
author = {Farouq Halawa and Sreenath {Chalil Madathil} and Mohammad T. Khasawneh},
keywords = {Process mining, Particle swarm optimization, Simulation-optimization, Facility layout, Healthcare},
abstract = {This paper proposes a three-phase framework to leverage hospital tracking data of patient visits while designing healthcare layouts with pod structures. The first phase proposes a process mining algorithm that modifies the Probabilistic Determining Finite Automata (PDFA) with Particle Swarm Optimization (PDFA-PSO) algorithm to predict the significant patient workflows from hospital historical data. The second phase employs simulation modeling to solve a right-sizing problem to determine the optimal size of the layout pods and the frequency of flows between the different clinical locations. The final phase uses an Unequal Area Facility Layout Problem (UAFLP) to determine the layout typology. The proposed process mining and simulation model are vital steps to measure the frequency between spaces and pod areas, which are needed to solve the UAFLP for outpatient settings. The proposed framework is validated using a case study for a renovation project of a large heart and vascular clinic in the US. The research shows that process mining is an efficient tool to extract a subset of significant patient pathways among 90 pathway variants and build a more realistic simulation that reflects behavioral and operational aspects. The research shows that the PSO algorithm is efficient in estimating the PDFA parameters and improving the prediction accuracy of the extracted patient pathways. In addition, the research shows that Genetic Algorithm with Placement Staretegy is an efficient algorithm for layout automation.}
}