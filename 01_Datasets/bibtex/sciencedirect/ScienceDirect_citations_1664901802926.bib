@article{KLAIB2021114037,
title = {Eye tracking algorithms, techniques, tools, and applications with an emphasis on machine learning and Internet of Things technologies},
journal = {Expert Systems with Applications},
volume = {166},
pages = {114037},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114037},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420308071},
author = {Ahmad F. Klaib and Nawaf O. Alsrehin and Wasen Y. Melhem and Haneen O. Bashtawi and Aws A. Magableh},
keywords = {Eye tracking techniques, Eye tracking applications, Electrooculography, Infrared oculography, Internet of Things, Machine learning, Scleral coil, Video oculography, Cloud computing, Fog computing, Choice modeling, Consumer psychology, Marketing},
abstract = {Eye tracking is the process of measuring where one is looking (point of gaze) or the motion of an eye relative to the head. Researchers have developed different algorithms and techniques to automatically track the gaze position and direction, which are helpful in different applications. Research on eye tracking is increasing owing to its ability to facilitate many different tasks, particularly for the elderly or users with special needs. This study aims to explore and review eye tracking concepts, methods, and techniques by further elaborating on efficient and effective modern approaches such as machine learning (ML), Internet of Things (IoT), and cloud computing. These approaches have been in use for more than two decades and are heavily used in the development of recent eye tracking applications. The results of this study indicate that ML and IoT are important aspects in evolving eye tracking applications owing to their ability to learn from existing data, make better decisions, be flexible, and eliminate the need to manually re-calibrate the tracker during the eye tracking process. In addition, they show that eye tracking techniques have more accurate detection results compared with traditional event-detection methods. In addition, various motives and factors in the use of a specific eye tracking technique or application are explored and recommended. Finally, some future directions related to the use of eye tracking in several developed applications are described.}
}
@article{WAGNER2021863,
title = {The Digital Twin in Order Processing},
journal = {Procedia CIRP},
volume = {104},
pages = {863-868},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.145},
url = {https://www.sciencedirect.com/science/article/pii/S221282712101043X},
author = {Sarah Bernadette Wagner and Michael Milde and Gunther Reinhart},
keywords = {digital twin, order processing, decision support, production, logistic network},
abstract = {Environmental factors, such as the high number of product variants, increase the complexity of order processing (OP). Holistic decision making presents a challenge due to the lack of transparency in OP, an inadequate data basis, and the unknown effects of decision alternatives. Today’s information systems and traditional calculation formulas for managing production only consider subsystems and isolated abstraction levels. As digital twins introduce new opportunities for decision support, their potential in OP is widely recognized, while their designing process still lacks research and methodologies. Consequently, we present a digital twin in OP, which enables efficient decision support.}
}
@incollection{ASHENDEN202127,
title = {Chapter 3 - Data types and resources},
editor = {Stephanie Kay Ashenden},
booktitle = {The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry},
publisher = {Academic Press},
pages = {27-60},
year = {2021},
isbn = {978-0-12-820045-2},
doi = {https://doi.org/10.1016/B978-0-12-820045-2.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200452000040},
author = {Stephanie Kay Ashenden and Sumit Deswal and Krishna C. Bulusu and Aleksandra Bartosik and Khader Shameer},
keywords = {Data, Omics, FAIR, Big data, SMILES, InChI},
abstract = {Recent innovation in the field of machine learning has been enabled by the confluence of three advances: rapid expansion of affordable computing power in the form of cloud computing environments, the accelerating pace of infrastructure associated with large-scale data collection and rapid methodological advancements, particularly neural network architecture improvements. Development and adoption of these advances have lagged in the health care domain largely due to restrictions around public use of data and siloed nature of these datasets with respect to providers, payers and clinical trial sponsors.}
}
@article{SCHARLER2021105020,
title = {Network construction, evaluation and documentation: A guideline},
journal = {Environmental Modelling & Software},
volume = {140},
pages = {105020},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105020},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221000633},
author = {U.M. Scharler and S.R. Borrett},
keywords = {Weighted networks, Ecosystem, Socio-economic, Best-practice, Plausibility, Sensitivity},
abstract = {Network analysis of complex systems is a rapidly growing field. Both theoretical and empirical network studies have permeated many different ecological, biological, social, and economic fields, investigating the interrelationships between nodes as structural and functional attributes in static, time-dynamic, or spatially explicit formats. We consider the network construction phase as a vital, but neglected component, and therefore provide recommended guidelines, describe how to evaluate the resulting network model quality, and highlight tools to assess their plausibility. Thereby we stress the importance of constructing multiple plausible networks to comply with basic scientific standards, and to pave the way for better informed evaluations. Finally, we provide recommendations for the management and policy arena where we advocate a thorough interrogation of network analyses outcomes (metrics) especially with regard to their sensitivity to the construction process, and a focus on relative changes between and within systems (e.g. as indication of vulnerability), rather than strict benchmarks.}
}
@article{WANG2021311,
title = {Estimating daily full-coverage near surface O3, CO, and NO2 concentrations at a high spatial resolution over China based on S5P-TROPOMI and GEOS-FP},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {175},
pages = {311-325},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000897},
author = {Yuan Wang and Qiangqiang Yuan and Tongwen Li and Liye Zhu and Liangpei Zhang},
keywords = {Full-coverage, Near surface concentrations, Air quality, S5P-TROPOMI, GEOS-FP, COVID-19},
abstract = {The Near Surface Concentrations (NSC) of O3, CO, and NO2 are crucial worldwide indicators of air quality. However, current frameworks devised for the estimation of the NSC of O3, CO, and NO2 have defects, such as coarse spatial resolution and large missing coverage. To address this issue, this study aims to estimate the daily (~13:30 local time) full-coverage NSC of O3, CO, and NO2 at a high spatial resolution (0.05° for O3 and NO2; 0.07° for CO) over China by using datasets from S5P-TROPOMI and GEOS-FP. In specific, the light gradient boosting machine is employed to train the estimation models. Validation results show that the NSC of O3, CO, and NO2 are well estimated, with the R2s of 0.91, 0.71, and 0.83 for the sample-based cross validation, respectively. Meanwhile, the proposed framework achieves a satisfactory performance in comparison to the latest related works, as reflected by the estimation accuracy and spatial resolution. As for the mapping, the estimated results show coherent spatial distribution and can accurately grasp the seasonal characteristics of each air pollutant. Finally, the estimated results are utilized to analyze the temporal variations of O3, CO, and NO2 during the COrona VIrus Disease 2019 (COVID-19) lockdown in China, which is an extend application for adopting the proposed framework in air quality monitoring. Results show that the estimated NSC of O3, CO, and NO2 in 2020 present significant variations during different periods of the COVID-19 lockdown in China compared to last year. In addition, the variations in the NSC of O3, CO, and NO2 during the COVID-19 lockdown in China possibly result from restrictions in the anthropogenic activities.}
}
@article{SOHRABPOUR2021120480,
title = {Export sales forecasting using artificial intelligence},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120480},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120480},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520313068},
author = {Vahid Sohrabpour and Pejvak Oghazi and Reza Toorajipour and Ali Nazarpour},
keywords = {Causal forecasting, Modeling, Export sales forecast, Genetic programming, Artificial intelligence},
abstract = {Sales forecasting is important in production and supply chain management. It affects firms’ planning, strategy, marketing, logistics, warehousing and resource management. While traditional time series forecasting methods prevail in research and practice, they have several limitations. Causal forecasting methods are capable of predicting future sales behavior based on relationships between variables and not just past behavior and trends. This research proposes a framework for modeling and forecasting export sales using Genetic Programming, which is an artificial intelligence technique derived from the model of biological evolution. Analyzing an empirical case of an export company, an export sales forecasting model is suggested. Moreover, a sales forecast for a period of six weeks is conducted, the output of which is compared with the real sales data. Finally, a variable sensitivity analysis is presented for the causal forecasting model.}
}
@article{FISCHER2021101689,
title = {On the composition of the long tail of business processes: Implications from a process mining study},
journal = {Information Systems},
volume = {97},
pages = {101689},
year = {2021},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101689},
url = {https://www.sciencedirect.com/science/article/pii/S030643792030137X},
author = {Marcus Fischer and Adrian Hofmann and Florian Imgrund and Christian Janiesch and Axel Winkelmann},
keywords = {Business Process Management, Long tail of business processes, Process mining, Process performance indicators},
abstract = {Digital transformation forces companies to rethink their processes to meet current customer needs. Business Process Management (BPM) can provide the means to structure and tackle this change. However, most approaches to BPM face restrictions on the number of processes they can optimize at a time due to complexity and resource restrictions. Investigating this shortcoming, the concept of the long tail of business processes suggests a hybrid approach that entails managing important processes centrally, while incrementally improving the majority of processes at their place of execution. This study scrutinizes this observation as well as corresponding implications. First, we define a system of indicators to automatically prioritize processes based on execution data. Second, we use process mining to analyze processes from multiple companies to investigate the distribution of process value in terms of their process variants. Third, we examine the characteristics of the process variants contained in the short head and the long tail to derive and justify recommendations for their management. Our results suggest that the assumption of a long-tailed distribution holds across companies and indicators and also applies to the overall improvement potential of processes and their variants. Across all cases, process variants in the long tail were characterized by fewer customer contacts, lower execution frequencies, and a larger number of involved stakeholders, making them suitable candidates for distributed improvement}
}
@article{ZHANG2021112265,
title = {An automated, generalized, deep-learning-based method for delineating the calving fronts of Greenland glaciers from multi-sensor remote sensing imagery},
journal = {Remote Sensing of Environment},
volume = {254},
pages = {112265},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112265},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720306386},
author = {Enze Zhang and Lin Liu and Lingcao Huang and Ka Shing Ng},
abstract = {In the past two decades, the data volume of remote sensing imagery in the polar regions has increased dramatically. The calving fronts of many Greenland glaciers have been undergoing substantial variations, and a comprehensive front dataset is necessary for better understanding such frontal dynamics. Therefore, there is a need for an automated approach to identifying glaciological features such as calving fronts. In 2019, three deep-learning-based methods were applied to calving front delineation, but were restricted to a specific area or dataset. Here, we develop a more generalized method that can be applied to a major outlet glacier or remote sensing datasets that are not included in the training. We integrate seven remote sensing datasets into a single deep learning network. The core datasets include optical (Landsat-8 and Sentinel-2) and synthetic aperture radar images (Envisat, ALOS-1 TerraSAR-X, Sentinel-1, and ALOS-2) taken over Jakobshavn Isbræ, Kangerlussuaq, and Helheim, spanning from 2002 to 2019. We evaluate four neural network architectures (e.g., U-Net, DeepLabv3+ with ResNet, DRN, and MobileNet as the backbones) and three histogram modification strategies (e.g., histogram normalization, linear stretching, and no histogram modification). We find that the combination of histogram normalization and DRN-DeepLabv3+ has the lowest test error, at 86 m. These promising results show that our method has a high generalization ability on various glaciers and data types.}
}
@article{ABDELRAHMAN2021110885,
title = {Data science for building energy efficiency: A comprehensive text-mining driven review of scientific literature},
journal = {Energy and Buildings},
volume = {242},
pages = {110885},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.110885},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821001699},
author = {Mahmoud M. Abdelrahman and Sicheng Zhan and Clayton Miller and Adrian Chong},
keywords = {Reference mining, Natural language processing, Data science, Built environment, Building energy efficiency, Word embeddings},
abstract = {The ever-changing data science landscape is fueling innovation in the built environment context by providing new and more effective means of converting large raw data sets into value for professionals in the design, construction and operations of buildings. The literature developed due to this convergence has rapidly increased in recent years, making it difficult for traditional review approaches to cover all related papers. Therefore, this paper applies a natural language processing (NLP) method to provide an exhaustive and quantitative review.Approximately 30,000 scientific publications were retrieved from the Elsevier API to extract the relationship between data sources, data science techniques, and building energy efficiency applications across the life cycle of buildings. The text-mining and NLP analysis reveals that data sciences techniques are applied more for operation phase applications such as fault detection and diagnosis (FDD), while being under-explored in design and commissioning phases. In addition, it is pointed out that more data science techniques that are to be investigated for various applications. For example, generative adversarial networks (GANs) has potential in facilitating parametric design; transfer learning is a promising path to promoting the application of optimal building operation;}
}
@incollection{LIN2021375,
title = {Chapter 14 - Machine learning and in silico methods},
editor = {Stavros Kassinos and Per Bäckman and Joy Conway and Anthony J. Hickey},
booktitle = {Inhaled Medicines},
publisher = {Academic Press},
pages = {375-390},
year = {2021},
isbn = {978-0-12-814974-4},
doi = {https://doi.org/10.1016/B978-0-12-814974-4.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128149744000134},
author = {Ching-Long Lin and Eric A. Hoffman and Stavros Kassinos},
keywords = {Computed tomography, Image registration, Airways, Lung, Asthma, COPD, Machine learning, Deep learning, Clustering, Clusters, Computational fluid and particle dynamics},
abstract = {This chapter reviews the techniques and strategies for identifying subpopulations (clusters) characterized by distinct lung features via machine learning and using cluster information to guide in silico computational fluid and particle dynamics (CFPD) analysis for the design of future inhaled drug delivery methods. We first review the collaborative efforts of collecting imaging, genetic, clinical and biological data sets for large cohorts of healthy, asthma and chronic obstructive pulmonary disease (COPD) subjects to investigate the heterogeneous nature of lung disease. We then focus on imaging-based phenotyping due to its quantitative nature that sensitively captures lung structural and functional alternations at both local (segmental/parenchymal) and global (lobar/lung) scales. Machine learning is then applied to identify imaging clusters for asthma and COPD patients. We select cluster archetypes to perform CFPD analysis and use CFPD-derived variables to interpret the link between cluster-specific alterations and particle depositions in the human lungs. Finally, we discuss the prospect of employing machine learning, physics-based learning and deep learning complementarily toward precision medicine.}
}
@article{NAKALEMBE2021100543,
title = {A review of satellite-based global agricultural monitoring systems available for Africa},
journal = {Global Food Security},
volume = {29},
pages = {100543},
year = {2021},
issn = {2211-9124},
doi = {https://doi.org/10.1016/j.gfs.2021.100543},
url = {https://www.sciencedirect.com/science/article/pii/S2211912421000523},
author = {Catherine Nakalembe and Inbal Becker-Reshef and Rogerio Bonifacio and Guangxiao Hu and Michael Laurence Humber and Christina Jade Justice and John Keniston and Kenneth Mwangi and Felix Rembold and Shraddhanand Shukla and Ferdinando Urbano and Alyssa Kathleen Whitcraft and Yanyun Li and Mario Zappacosta and Ian Jarvis and Antonio Sanchez},
keywords = {Satellite Earth Observations, Scalable, Operational agriculture monitoring, Open access, Africa},
abstract = {The increasing frequency and severity of extreme climatic events and their impacts are being realized in many regions of the world, particularly in smallholder crop and livestock production systems in Sub-Saharan Africa (SSA). These events underscore the need for timely early warning. Satellite Earth Observation (EO) availability, rapid developments in methodology to archive and process them through cloud services and advanced computational capabilities, continue to generate new opportunities for providing accurate, reliable, and timely information for decision-makers across multiple cropping systems and for resource-constrained institutions. Today, systems and tools that leverage these developments to provide open access actionable early warning information exist. Some have already been employed by early adopters and are currently operational in selecting national monitoring programs in Angola, Kenya, Rwanda, Tanzania, and Uganda. Despite these capabilities, many governments in SSA still rely on traditional crop monitoring systems, which mainly rely on sparse and long latency in situ reports with little to no integration of EO-derived crop conditions and yield models. This study reviews open-access operational agricultural monitoring systems available for Africa. These systems provide the best-available open-access EO data that countries can readily take advantage of, adapt, adopt, and leverage to augment national systems and make significant leaps (timeliness, spatial coverage and accuracy) of their monitoring programs. Data accessible (vegetation indices, crop masks) in these systems are described showing typical outputs. Examples are provided including crop conditions maps, and damage assessments and how these have integrated into reporting and decision-making. The discussion compares and contrasts the types of data, assessments and products can expect from using these systems. This paper is intended for individuals and organizations seeking to access and use EO to assess crop conditions who might not have the technical skill or computing facilities to process raw data into informational products.}
}
@article{ATTYE2021117927,
title = {TractLearn: A geodesic learning framework for quantitative analysis of brain bundles},
journal = {NeuroImage},
volume = {233},
pages = {117927},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.117927},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921002044},
author = {Arnaud Attyé and Félix Renard and Monica Baciu and Elise Roger and Laurent Lamalle and Patrick Dehail and Hélène Cassoudesalle and Fernando Calamante},
keywords = {Diffusion MRI, Fiber tractography, Precision medicine, Manifold learning},
abstract = {Deep learning-based convolutional neural networks have recently proved their efficiency in providing fast segmentation of major brain fascicles structures, based on diffusion-weighted imaging. The quantitative analysis of brain fascicles then relies on metrics either coming from the tractography process itself or from each voxel along the bundle. Statistical detection of abnormal voxels in the context of disease usually relies on univariate and multivariate statistics models, such as the General Linear Model (GLM). Yet in the case of high-dimensional low sample size data, the GLM often implies high standard deviation range in controls due to anatomical variability, despite the commonly used smoothing process. This can lead to difficulties to detect subtle quantitative alterations from a brain bundle at the voxel scale. Here we introduce TractLearn, a unified framework for brain fascicles quantitative analyses by using geodesic learning as a data-driven learning task. TractLearn allows a mapping between the image high-dimensional domain and the reduced latent space of brain fascicles using a Riemannian approach. We illustrate the robustness of this method on a healthy population with test-retest acquisition of multi-shell diffusion MRI data, demonstrating that it is possible to separately study the global effect due to different MRI sessions from the effect of local bundle alterations. We have then tested the efficiency of our algorithm on a sample of 5 age-matched subjects referred with mild traumatic brain injury. Our contributions are to propose: 1/ A manifold approach to capture controls variability as standard reference instead of an atlas approach based on a Euclidean mean. 2/ A tool to detect global variation of voxels’ quantitative values, which accounts for voxels’ interactions in a structure rather than analyzing voxels independently. 3/ A ready-to-plug algorithm to highlight nonlinear variation of diffusion MRI metrics. With this regard, TractLearn is a ready-to-use algorithm for precision medicine.}
}
@article{HE2021107889,
title = {Rapidly assessing earthquake-induced landslide susceptibility on a global scale using random forest},
journal = {Geomorphology},
volume = {391},
pages = {107889},
year = {2021},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2021.107889},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X2100297X},
author = {Qian He and Ming Wang and Kai Liu},
keywords = {Landslide susceptibility, Random forest model, Earthquake, Global scale},
abstract = {Earthquake-induced landslides (EQILs) are an incredibly destructive geological disaster. Rapid landslide susceptibility assessments are indispensable and critical for risk analysis and emergency management. Previous studies mainly focus on the regional-scale assessment of EQIL susceptibility, while the global analyses of that are lacking. In this study, we constructed a global model for rapidly assessing earthquake-induced landslide susceptibility based on the random forest (RF) algorithm using globally available data. In total, 288,114 landslides from 16 high-quality EQIL inventories were utilized to develop the global landslide model. We split the data into 70% training dataset for model training and 30% testing data for model evaluation. We also used three blind test events to validate the model performance. The model showed excellent performance on the testing data (accuracy = 0.945, and AUC = 0.985). The RF model exhibited strong spatial generalizability and robustness, with an AUC exceeding 0.8 for each landslide inventory and showing good performance on the blind test events. The resulting landslide susceptibility maps also match relatively well with the actual landslide locations. Among the conditioning factors, modified Mercalli intensity (MMI), elevation and slope are the three most important conditioning factors. The susceptibility maps for each landslide event were produced. The developed RF model would be useful in studies of earthquake-induced landslide susceptibility and emergency response after an earthquake.}
}
@article{ABDULJABBAR2021102734,
title = {The role of micro-mobility in shaping sustainable cities: A systematic literature review},
journal = {Transportation Research Part D: Transport and Environment},
volume = {92},
pages = {102734},
year = {2021},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2021.102734},
url = {https://www.sciencedirect.com/science/article/pii/S1361920921000389},
author = {Rusul L. Abduljabbar and Sohani Liyanage and Hussein Dia},
keywords = {Micro-mobility, Smart cities, Systematic literature review, Sustainable transport, Bibliometric networks, Co-citation analysis},
abstract = {Micro-mobility is increasingly recognised as a promising mode of urban transport, particularly for its potential to reduce private vehicle use for short-distance travel. Despite valuable research contributions that represent fundamental knowledge on this topic, today’s body of research appears quite fragmented in relation to the role of micro-mobility as a transformative solution for meeting sustainability outcomes in urban environments. This paper consolidates knowledge on the topic, analyses past and on-going research developments, and provides future research directions by using a rigorous and auditable systematic literature review methodology. To achieve these objectives, the paper analysed 328 journal publications from the Scopus database covering the period between 2000 and 2020. A bibliographic analysis was used to identify relevant publications and explore the changing landscape of micro-mobility research. The study constructed and visualised the literature’s bibliometric networks through citations and co-citations analyses for authors, articles, journals and countries. The findings showed a consistent spike in recent research outputs covering the sustainability aspects of micro-mobility reflecting its importance as a low-carbon and transformative mode of urban transport. The co-citation analysis, in particular, helped to categorise the literature into four main research themes that address benefits, technology, policy and behavioural mode-choice categories where the majority of research has been focused during the analysis period. For each cluster, inductive reasoning is used to discuss the emerging trends, barriers as well as pathways to overcome challenges to wide-scale deployment. This article provides a balanced and objective summary of research evidence on the topic and serves as a reference point for further research on micro-mobility for sustainable cities.}
}
@article{LAN2021107522,
title = {Trade-off between carbon sequestration and water loss for vegetation greening in China},
journal = {Agriculture, Ecosystems & Environment},
volume = {319},
pages = {107522},
year = {2021},
issn = {0167-8809},
doi = {https://doi.org/10.1016/j.agee.2021.107522},
url = {https://www.sciencedirect.com/science/article/pii/S0167880921002267},
author = {Xin Lan and Zhiyong Liu and Xiaohong Chen and Kairong Lin and Linying Cheng},
keywords = {Land use, Forest, Cropland, GPP, NPP, Evapotranspiration, PT-JPL model},
abstract = {Land use management of forests and croplands mainly drives the vegetation greening in China. Vegetation greening strongly modulates the trade-off between carbon sequestration via photosynthesis and water loss from evapotranspiration (ET) at the terrestrial ecosystem (representing by ecosystem water use efficiency, WUE). The function of vegetation greening in terrestrial carbon sequestration is well known, but the impacts of water loss from ET caused by vegetation greening on WUE are often neglected. Here, the GIS-based Priestley-Taylor Jet Propulsion Laboratory model was established to evaluate ET in China from 2001 to 2015, incorporating vegetation dynamics as a key component. To quantify the net effect of the ET caused by vegetation greening on WUE, we compared two different simulation scenarios: actual vegetation greening scenario and simulated without vegetation greening scenario. The results show that forests and croplands mainly contribute to the growth in GPP and NPP in China with annual rates of 2.53 gC·m−2 yr−2 and 1.59 gC·m−2 yr−2 from 2001 to 2015, respectively. With the increase of terrestrial carbon sequestration, the ET under actual vegetation greening scenario was generated 6.78 mm·yr−1 more than that under simulated without vegetation greening scenario. But as a result of the negative impacts of vegetation physiological effect (elevated CO2 concentration and the decreased VPD) on ET, values of ET under two different scenarios all exhibited a decline trend from 2001 to 2015 with rates of − 2.04% and − 3.63%, respectively. Consequently, although the WUE under two different scenarios exhibited increased trends (6.44%, actual vegetation dynamics scenario; 10.74%, simulated without vegetation greening scenario), the ET caused by vegetation greening led to an obvious divergence between the WUE under two different scenarios. For better understanding the impacts of human activities on carbon and water cycles at the terrestrial ecosystem, it is necessary to take the water loss from ET caused by vegetation greening into consideration, which is crucial for enhancing the sustainability of future vegetation-related projects.}
}
@article{KANN2021916,
title = {Artificial intelligence for clinical oncology},
journal = {Cancer Cell},
volume = {39},
number = {7},
pages = {916-927},
year = {2021},
issn = {1535-6108},
doi = {https://doi.org/10.1016/j.ccell.2021.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1535610821002105},
author = {Benjamin H. Kann and Ahmed Hosny and Hugo J.W.L. Aerts},
keywords = {artificial intelligence, clinical oncology, precision medicine, care pathway, clinical translation},
abstract = {Summary
Clinical oncology is experiencing rapid growth in data that are collected to enhance cancer care. With recent advances in the field of artificial intelligence (AI), there is now a computational basis to integrate and synthesize this growing body of multi-dimensional data, deduce patterns, and predict outcomes to improve shared patient and clinician decision making. While there is high potential, significant challenges remain. In this perspective, we propose a pathway of clinical cancer care touchpoints for narrow-task AI applications and review a selection of applications. We describe the challenges faced in the clinical translation of AI and propose solutions. We also suggest paths forward in weaving AI into individualized patient care, with an emphasis on clinical validity, utility, and usability. By illuminating these issues in the context of current AI applications for clinical oncology, we hope to help advance meaningful investigations that will ultimately translate to real-world clinical use.}
}
@article{WANG2021128952,
title = {How and when higher climate change risk perception promotes less climate change inaction},
journal = {Journal of Cleaner Production},
volume = {321},
pages = {128952},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128952},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621031449},
author = {Changcheng Wang and Liuna Geng and Julián D. Rodríguez-Casallas},
keywords = {Climate change risk perception, Sustainable development, Climate change inaction, Mindfulness, Climate change belief, Environmental efficacy},
abstract = {Climate change has been positioned as one of the most severe environmental threats facing us today. To address climate change, enhancing climate change risk perception and reducing climate change inaction are both critical. However, little research has touched on the issue of whether climate change risk perception is linked to climate change inaction in a negative manner. Moreover, there is still much unknown about the complex process behind this relationship, and the boundary conditions of this process awaits clarification. To address these gaps in the literature, two studies were conducted to first confirm the possible negative association between climate change risk perception and climate change inaction and, second, explore through a parallel mediational model whether climate change belief and environmental efficacy mediate simultaneously the relation between climate change risk perception and climate change inaction. Finally, a moderated sequential mediational model was used to investigate whether climate change risk perception is associated with climate change inaction through the sequential mediation of climate change belief and environmental efficacy, and to clarify underlying boundary conditions by analyzing the moderation of mindfulness as well. The results showed that, as expected, higher levels of climate change risk perception were related to less climate change inaction, and this relation was mediated by enhanced climate change belief and heightened environmental efficacy in a sequential manner. Furthermore, the sequential mediating effect of climate change belief and environmental efficacy was stronger among those who had a higher level of mindfulness. These findings advance the emerging research on climate change inaction by elucidating the mechanisms underlying the effect of climate change risk perception. Moreover, they extend the Domain-Context-Behavior (DCB) model and Gateway belief model (GBM). In practice, climate change education and climate change inaction interventions can be designed and implemented to nudge clean production, green supply chain and green consumption, which finally contribute to sustainable development and the ‘green transformation’ of society.}
}
@article{MA2021224,
title = {A novel rumor detection algorithm based on entity recognition, sentence reconfiguration, and ordinary differential equation network},
journal = {Neurocomputing},
volume = {447},
pages = {224-234},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221004392},
author = {Tinghuai Ma and Honghao Zhou and Yuan Tian and Najla Al-Nabhan},
keywords = {Rumor detection, Entity recognition, Sentence reconfiguration, ODE-net},
abstract = {Social media has recently become one of the most used media in the world. This has resulted in a great hotbed for the growth of rumors, as anyone can spread knowledge and opinions without confirmation. Previous works on rumor detection focused on hand-extracted features and spent less effort on text representation. In this research, a novel method for rumor detection on social media is proposed, which integrates entity recognition, sentence reconfiguration and ordinary differential equation network under a unified framework called ESODE. An entity recognition method to enhance the semantic understanding of rumor texts is used. Then, a sentence reconfiguration to improve the frequency of important words is designed. The complete feature map is established by further collecting statistical features from three aspects: linguistic features on the content of rumors, characteristics of users involved in rumor propagating, and propagation network structures. Finally, the ordinary differential equation network (ODEnet) is applied to detect rumors. Experimental results on datasets from Twitter and Weibo show that the proposed method achieves better performance than previous ones.}
}
@article{ALMEIDA2021113538,
title = {Tun-OCM: A model-driven approach to support database tuning decision making},
journal = {Decision Support Systems},
volume = {145},
pages = {113538},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113538},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621000488},
author = {Ana Carolina Almeida and Fernanda Baião and Sérgio Lifschitz and Daniel Schwabe and Maria Luiza M. Campos},
keywords = {Database systems, Tuning decision, Heuristics, Configuration management, Ontology pattern language},
abstract = {Database tuning is a task executed by Database Administrators (DBAs) based on their practical experience and on tuning systems, which support DBA actions towards improving the performance of a database system. It is notoriously a complex task that requires precise domain knowledge about possible database configurations. Ideally, a DBA should keep track of several Database Management Systems (DBMS) parameters, configure data structures, and must be aware about possible interferences among several database (DB) configurations. We claim that an automatic tuning system is a decision support system and DB tuning may also be seen as a configuration management task. Therefore, we may characterize it by means of a formal domain conceptualization, benefiting from existing control practices and computational support in the configuration management domain. This work presents Tun-OCM, a conceptual model represented as a well-founded ontology, that encompasses a novel characterization of the database tuning domain as a configuration management conceptualization to support decision making. We develop and represent Tun-OCM using the CM-OPL methodology and its underlying language. The benefits of Tun-OCM are discussed by instantiating it in a real scenario.}
}
@article{TERZIYAN2021676,
title = {Taxonomy of generative adversarial networks for digital immunity of Industry 4.0 systems},
journal = {Procedia Computer Science},
volume = {180},
pages = {676-685},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.290},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003392},
author = {Vagan Terziyan and Svitlana Gryshko and Mariia Golovianko},
keywords = {Industry 4.0, Generative Adversarial Networks, cybersecurity, artificial digital immunity},
abstract = {Industry 4.0 systems are extensively using artificial intelligence (AI) to enable smartness, automation and flexibility within variety of processes. Due to the importance of the systems, they are potential targets for attackers trying to take control over the critical processes. Attackers use various vulnerabilities of such systems including specific vulnerabilities of AI components. It is important to make sure that inappropriate adversarial content will not break the security walls and will not harm the decision logic of critical systems. We believe that the corresponding security toolset must be organized as a trainable self-protection mechanism similar to immunity. We found certain similarities between digital vs. biological immunity and we study the possibilities of Generative Adversarial Networks (GANs) to provide the basis for the digital immunity training. We suggest the taxonomy of GANs (including new architectures) suitable to simulate various aspects of the immunity for Industry 4.0 applications.}
}
@article{SADHANA2021255,
title = {AI-based Power Screening Solution for SARS-CoV2 Infection: A Sociodemographic Survey and COVID-19 Cough Detector},
journal = {Procedia Computer Science},
volume = {194},
pages = {255-271},
year = {2021},
note = {18th International Learning & Technology Conference 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.081},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921021220},
author = {S Sadhana and S Pandiarajan and E Sivaraman and D Daniel},
keywords = {Artificial Intelligence, Coronavirus (SARS –CoV2), Machine Learning, Pathomorphological Variations, Power Screening Solutions},
abstract = {Globally, the confirmed coronavirus (SARS-CoV2) cases are being increasing day by day. Coronavirus (COVID-19) causes an acute infection in the respiratory tract that started spreading in late 2019. Huge datasets of SARS-CoV2 patients can be incorporated and analyzed by machine learning strategies for understanding the pattern of pathological spread and helps to analyze the accuracy and speed of novel therapeutic methodologies, also detect the susceptible people depends on their physiological and genetic aspects. To identify the possible cases faster and rapidly, we propose the Artificial Intelligence (AI) power screening solution for SARS- CoV2 infection that can be deployable through the mobile application. It collects the details of the travel history, symptoms, common signs, gender, age and diagnosis of the cough sound. To examine the sharpness of pathomorphological variations in respiratory tracts induced by SARS-CoV2, that compared to other respiratory illnesses to address this issue. To overcome the shortage of SARS-CoV2 datasets, we apply the transfer learning technique. Multipronged mediator for risk-averse Artificial Intelligence Architecture is induced for minimizing the false diagnosis of risk-stemming from the problem of complex dimensionality. This proposed application provides early detection and prior screening for SARS-CoV2 cases. Huge data points can be processed through AI framework that can examine the users and classify them into “Probably COVID”, “Probably not COVID” and “Result indeterminate”.}
}
@incollection{EMINAGA2021309,
title = {Chapter 16 - Prospect and adversity of artificial intelligence in urology},
editor = {Lei Xing and Maryellen L. Giger and James K. Min},
booktitle = {Artificial Intelligence in Medicine},
publisher = {Academic Press},
pages = {309-337},
year = {2021},
isbn = {978-0-12-821259-2},
doi = {https://doi.org/10.1016/B978-0-12-821259-2.00016-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212592000168},
author = {Okyaz Eminaga and Joseph C. Liao},
keywords = {Urology, artificial intelligence, MRI, CT, urine analyses, AI-based solution, prostate cancer, bladder cancer, kidney cancer, ultrasound, diagnostic imaging, prediction models},
abstract = {The emergence of artificial intelligence (AI) has opened a new avenue for tackling existing challenges in clinical routine. This chapter will briefly introduce potential applications of AI in urology and focus on its benefits and barriers in solving real clinical problems. First, the introduction section will generally discuss AI and existing data resources. Then, the chapter will explain the potential application of AI in urological endoscopy, urine, stone and andrology, imaging and the robotic surgery. Further, this chapter will briefly discuss some tools of risk predictions for urological cancer. Finally, the author will discuss the potential future direction of AI in urology.}
}
@incollection{ORLOWSKI202187,
title = {Chapter 3 - Specification of architecture layers and reference models of Internet of Things systems},
editor = {Cezary Orlowski},
booktitle = {Management of IOT Open Data Projects in Smart Cities},
publisher = {Academic Press},
pages = {87-126},
year = {2021},
isbn = {978-0-12-818779-1},
doi = {https://doi.org/10.1016/B978-0-12-818779-1.00003-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128187791000031},
author = {Cezary Orlowski},
keywords = {IoT networks, sensors, communication layers and protocols, data collection},
abstract = {While the first chapter introduced open data and Smart Cities, the second chapter presented the environment for generating open data, in this chapter we discuss the layers of architecture of the Internet of Things systems. Characteristics of microcontrollers and sensors are presented in detail, and communication protocols are discussed. The purpose of this chapter is to familiarize the reader with the description of basic internet devices and methods of their communication.}
}
@article{ISLAM2021103008,
title = {Context-aware scheduling in Fog computing: A survey, taxonomy, challenges and future directions},
journal = {Journal of Network and Computer Applications},
volume = {180},
pages = {103008},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103008},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000357},
author = {Mir Salim Ul Islam and Ashok Kumar and Yu-Chen Hu},
keywords = {Fog computing, Context-awareness, Scheduling, Resource management, Resource estimation, Resource provisioning, Contextual information},
abstract = {Fog computing extends Cloud-based facilities and stays in the vicinity of the end-users to provide an attractive solution to a diverse range of latency-sensitive applications. The applications are becoming more sophisticated, context-aware, and computation-intensive due to varying situational and environmental conditions in order to meet the ever-increasing users’ demands. Further, resource heterogeneity, dynamic nature, resource limitations, and unpredictability of the Fog environment make scheduling of application tasks while satisfying Quality of Service (QoS) requirements a challenging job. To overcome these issues various scheduling strategies have been proposed considering contextual information of different entities involved in Fog computing. This survey represents a comprehensive literature analysis pertaining to context-aware scheduling in Fog computing. It provides detailed comparison of existing scheduling approaches based on important factors such as context-aware parameters, case studies, performance metrics, and evaluation tools along with advantages and limitations. It also presents detailed taxonomy, performance metrics, and context-aware parameter analysis. Further, it list several issues and challenges. This study will aid the research community in exploring future research directions and essential aspects of scheduling approaches using different types of contextual information.}
}
@article{TIAN2021102727,
title = {Using data monitoring algorithms to physiological indicators in motion based on Internet of Things in smart city},
journal = {Sustainable Cities and Society},
volume = {67},
pages = {102727},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102727},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721000226},
author = {Jian Tian and Lulu Gao},
keywords = {Internet of Things, Data fusion algorithm, Physiological indicators, Monitoring, Smart city},
abstract = {This article discusses the monitoring of physiological indicators during exercise, combined with the data fusion algorithm of the smart city Internet of Things health. We use the hash value of the tuple key to the corresponding data block of the node, use the data block record to obtain the response of the target node, and output the data tuple. It is used as a measure of the load balance of health data streams to determine whether load migration is needed and to determine the way and amount of migration tasks to make migration decisions. The simulation experiments show that the method has good computational performance and dynamic load balancing. A series of mean arterial pressure and heart rate of patients and non-stationary health data, and a series of blood pressure and heart rate of health individuals in different postures are selected to perform experiments to analyze the transfer function and power spectra in the model, validating that the model can be used to reveal the changes associated with severe systemic response syndrome (SIRS), providing a hypothesis for the decomposition of autoregulation of physiological control under health and disease conditions.}
}
@article{GALLIN2021101734,
title = {Measuring aggregate housing wealth: New insights from machine learning ☆},
journal = {Journal of Housing Economics},
volume = {51},
pages = {101734},
year = {2021},
issn = {1051-1377},
doi = {https://doi.org/10.1016/j.jhe.2020.101734},
url = {https://www.sciencedirect.com/science/article/pii/S105113772030070X},
author = {Joshua Gallin and Raven Molloy and Eric Nielsen and Paul Smith and Kamila Sommer},
keywords = {Residential real estate, Consumer economics and finance, Data collection and estimation, Flow of funds},
abstract = {We construct a new measure of aggregate housing wealth for the U.S. based on (1) home-value estimates derived from machine learning algorithms applied to detailed information on property characteristics and recent transaction prices, and (2) Census housing unit counts. According to our new measure, the timing and amplitude of the recent house-price cycle differs materially but plausibly from commonly-used measures, which are based on survey data or repeat-sales price indexes. Thus, our methodology generates estimates that should be of considerable value to researchers and policymakers interested in the dynamics of aggregate housing wealth.}
}
@article{AO2021,
title = {Automatic segmentation of stem and leaf components and individual maize plants in field terrestrial LiDAR data using convolutional neural networks},
journal = {The Crop Journal},
year = {2021},
issn = {2214-5141},
doi = {https://doi.org/10.1016/j.cj.2021.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2214514121002191},
author = {Zurui Ao and Fangfang Wu and Saihan Hu and Ying Sun and Yanjun Su and Qinghua Guo and Qinchuan Xin},
keywords = {Terrestrial LiDAR, Phenotype, Organ segmentation, Convolutional neural networks},
abstract = {High-throughput maize phenotyping at both organ and plant levels plays a key role in molecular breeding for increasing crop yields. Although the rapid development of light detection and ranging (LiDAR) provides a new way to characterize three-dimensional (3D) plant structure, there is a need to develop robust algorithms for extracting 3D phenotypic traits from LiDAR data to assist in gene identification and selection. Accurate 3D phenotyping in field environments remains challenging, owing to difficulties in segmentation of organs and individual plants in field terrestrial LiDAR data. We describe a two-stage method that combines both convolutional neural networks (CNNs) and morphological characteristics to segment stems and leaves of individual maize plants in field environments. It initially extracts stem points using the PointCNN model and obtains stem instances by fitting 3D cylinders to the points. It then segments the field LiDAR point cloud into individual plants using local point densities and 3D morphological structures of maize plants. The method was tested using 40 samples from field observations and showed high accuracy in the segmentation of both organs (F-score =0.8207) and plants (F-score =0.9909). The effectiveness of terrestrial LiDAR for phenotyping at organ (including leaf area and stem position) and individual plant (including individual height and crown width) levels in field environments was evaluated. The accuracies of derived stem position (position error =0.0141 m), plant height (R2 >0.99), crown width (R2 >0.90), and leaf area (R2 >0.85) allow investigating plant structural and functional phenotypes in a high-throughput way. This CNN-based solution overcomes the major challenges in organ-level phenotypic trait extraction associated with the organ segmentation, and potentially contributes to studies of plant phenomics and precision agriculture.}
}
@article{IMKER2021102369,
title = {An examination of data reuse practices within highly cited articles of faculty at a research university},
journal = {The Journal of Academic Librarianship},
volume = {47},
number = {4},
pages = {102369},
year = {2021},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2021.102369},
url = {https://www.sciencedirect.com/science/article/pii/S0099133321000604},
author = {Heidi J. Imker and Hoa Luong and William H. Mischo and Mary C. Schlembach and Chris Wiley},
keywords = {Data reuse, Data sharing, Data management, Data services, Scopus API},
abstract = {Data sharing and reuse are regarded as important components of the research workflow and key elements in open science. While reuse is well-documented in some circumstances, the utility of data sharing for all domains is less clear, and limited evidence of wide-spread demand can make it challenging to justify effort and funds required to format, document, share, and preserve data. This paper describes a project that: (1) surveyed authors of highly cited papers published in 2015 at the University of Illinois at Urbana-Champaign in nine STEM disciplines to determine if data were generated for their article and their knowledge of reuse by other researchers, and (2) surveyed authors who cited these 2015 articles to ascertain whether they reused data from the original article and how that data was obtained. The project goal was to better understand data reuse in practice and to explore if research data from an initial publication was reused in subsequent publications. While the results revealed reuse in many situations (and deemed important in these cases), the survey results and researcher supplied comments also indicated that data does not play the same role in all studies or even in studies that build on previous ones.}
}
@article{VANLOOY2021103413,
title = {A quantitative and qualitative study of the link between business process management and digital innovation},
journal = {Information & Management},
volume = {58},
number = {2},
pages = {103413},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103413},
url = {https://www.sciencedirect.com/science/article/pii/S0378720620303517},
author = {Amy {Van Looy}},
keywords = {Business process management, Process change management, Life cycle management, Adoption, Process innovation, Digital innovation, Digital transformation, Survey, Expert panel},
abstract = {The information revolution leaves its mark on businesses, resulting in organizations looking for digital innovation (DI) to apply to their business processes and anticipate competitors. Since the interplay between business process management (BPM) and DI has been underdeveloped, this mixed-methods article investigates the strength and nature of the relationship. We supplement the findings of an international survey (stage 1) with explanations from an expert panel (stage 2) to generalize a positive yet moderate link because of manifold contextual factors affecting strategic decision-making. We extend the technology–organization–environment (TOE) framework and profile organizations along their digital process innovation (DPI) mastery in a readiness matrix.}
}
@article{CZETANY2021111376,
title = {Development of electricity consumption profiles of residential buildings based on smart meter data clustering},
journal = {Energy and Buildings},
volume = {252},
pages = {111376},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111376},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821006605},
author = {László Czétány and Viktória Vámos and Miklós Horváth and Zsuzsa Szalay and Adrián Mota-Babiloni and Zsófia Deme-Bélafi and Tamás Csoknyai},
keywords = {Electricity consumption profile, Smart meter, Data clustering, K-means, Fuzzy k-means, Hierarchical, Residential buildings},
abstract = {In the present research, a high-resolution, detailed electric load dataset was assessed, collected by smart meters from nearly a thousand households in Hungary, many of them single-family houses. The objective was to evaluate this database in detail to determine energy consumption profiles from time series of daily and annual electric load. After representativity check of dataset daily and annual energy consumption profiles were developed, applying three different clustering methods (k-means, fuzzy k-means, agglomerative hierarchical) and three different cluster validity indexes (elbow method, silhouette method, Dunn index) in MATLAB environment. The best clustering method for our examination proved to be the k-means clustering technique. Analyses were carried out to identify different consumer groups, as well as to clarify the impact of specific parameters such as meter type in the housing unit (e.g. peak, off-peak meter), day of the week (e.g. weekend, weekday), seasonality, geographical location, settlement type and housing type (single-family house, flat, age class of the building). Furthermore, four electric user profile types were proposed, which can be used for building energy demand simulation, summer heat load and winter heating demand calculation.}
}
@article{BOCHIE2021103213,
title = {A survey on deep learning for challenged networks: Applications and trends},
journal = {Journal of Network and Computer Applications},
volume = {194},
pages = {103213},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103213},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002149},
author = {Kaylani Bochie and Mateus S. Gilbert and Luana Gantert and Mariana S.M. Barbosa and Dianne S.V. Medeiros and Miguel Elias M. Campista},
keywords = {Challenged networks, Internet of Things, Sensor networks, Industrial networks, Wireless mobile networks, Vehicular networks, Deep learning, Machine learning},
abstract = {Computer networks are dealing with growing complexity, given the ever-increasing volume of data produced by all sorts of network nodes. Performance improvements are a non-stop ambition and require tuning fine-grained details of the system operation. Analyzing such data deluge, however, is not straightforward and sometimes not supported by the system. There are often problems regarding scalability and the predisposition of the involved nodes to understand and transfer the data. This issue is at least partially circumvented by knowledge acquisition from past experiences, which is a characteristic of the herein called “challenged networks”. The addition of intelligence in these scenarios is fundamental to extract linear and non-linear relationships from the data collected by multiple sources. This is undoubtedly an invitation to machine learning and, more particularly, to deep learning. This paper identifies five different challenged networks: IoT and sensor, mobile, industrial, and vehicular networks as typical scenarios that may have multiple and heterogeneous data sources and face obstacles concerning connectivity. As a consequence, deep learning solutions can contribute to system performance by adding intelligence and the ability to interpret data. We start the paper by providing an overview of deep learning, further explaining this approach’s benefits over the cited scenarios. We propose a workflow based on our observations of deep learning applications over challenged networks, and based on it, we strive to survey the literature on deep-learning-based solutions at an application-oriented level using the PRISMA methodology. Afterward, we also discuss new deep learning techniques that show enormous potential for further improvements as well as transversal issues, such as security. Finally, we provide lessons learned raising trends linking all surveyed papers to deep learning approaches. We are confident that the proposed paper contributes to the state of the art and can be a piece of inspiration for beginners and also for enthusiasts on advanced networking research.}
}
@incollection{2021345,
title = {Index},
editor = {Shuai Li and John L. Hopper},
booktitle = {Twin and Family Studies of Epigenetics},
publisher = {Academic Press},
pages = {345-354},
year = {2021},
volume = {27},
series = {Translational Epigenetics},
isbn = {978-0-12-820951-6},
doi = {https://doi.org/10.1016/B978-0-12-820951-6.09992-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128209516099920}
}
@article{WANG20213829,
title = {Human population history at the crossroads of East and Southeast Asia since 11,000 years ago},
journal = {Cell},
volume = {184},
number = {14},
pages = {3829-3841.e21},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421006358},
author = {Tianyi Wang and Wei Wang and Guangmao Xie and Zhen Li and Xuechun Fan and Qingping Yang and Xichao Wu and Peng Cao and Yichen Liu and Ruowei Yang and Feng Liu and Qingyan Dai and Xiaotian Feng and Xiaohong Wu and Ling Qin and Fajun Li and Wanjing Ping and Lizhao Zhang and Ming Zhang and Yalin Liu and Xiaoshan Chen and Dongju Zhang and Zhenyu Zhou and Yun Wu and Hassan Shafiey and Xing Gao and Darren Curnoe and Xiaowei Mao and E. Andrew Bennett and Xueping Ji and Melinda A. Yang and Qiaomei Fu},
keywords = {ancient DNA, 12,000-year-old humans, deeply diverged ancestry, pre-farming, cross-interactions, admixture},
abstract = {Summary
Past human genetic diversity and migration between southern China and Southeast Asia have not been well characterized, in part due to poor preservation of ancient DNA in hot and humid regions. We sequenced 31 ancient genomes from southern China (Guangxi and Fujian), including two ∼12,000- to 10,000-year-old individuals representing the oldest humans sequenced from southern China. We discovered a deeply diverged East Asian ancestry in the Guangxi region that persisted until at least 6,000 years ago. We found that ∼9,000- to 6,000-year-old Guangxi populations were a mixture of local ancestry, southern ancestry previously sampled in Fujian, and deep Asian ancestry related to Southeast Asian Hòabìnhian hunter-gatherers, showing broad admixture in the region predating the appearance of farming. Historical Guangxi populations dating to ∼1,500 to 500 years ago are closely related to Tai-Kadai and Hmong-Mien speakers. Our results show heavy interactions among three distinct ancestries at the crossroads of East and Southeast Asia.}
}
@incollection{SCHMIDT2021283,
title = {Computational Toxicology},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {283-300},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11534-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801238311534X},
author = {Friedemann Schmidt},
keywords = {Applicability domain, Artificial intelligence, Classification, Computational toxicology, Data mining, Descriptor, Expert system, In silico, In silico profiling, Machine learning, Off target toxicity, QSAR, Rule-based, SAR, Toxicology},
abstract = {Powerful computer systems and growing amounts of curated data have rocketed the implementation of computational disciplines in all fields of drug discovery sciences, including toxicology. Computational toxicology feeds from the needs of researchers, manufacturers, regulators and patients to fully characterize the safety profiles of drugs to avoid the exposure of patients at risk. In this article, we introduce basic concepts of computational toxicology and discuss algorithms and tools that are widely employed to complement experimental laboratory work. Since more than two decades, lab work and particularly animal experimentation have found more and more counterparts by in-silico simulation and computational prediction. These methods are employed to inform scientists early in the drug discovery process, fill data gaps, explain causalities and ultimately reduce experimentation and cycle times. Since computational toxicology is applied throughout the full drug discovery value chain, from research to clinic, it has given rise to using an extraordinarily broad kit of tools employing data mining, curation of structural alerts, molecular fragment methods, quantitative structure-property relationships and machine learning. While the use of data-driven technologies is exploding, and multiple competing applications become available for predictive modeling, their underlying training datasets for individual endpoints may still be small—too small to allow for global applicability. Care must be taken therefore to ensure that predictive methods have been properly validated and are fit-for-purpose. We discuss such practice, as well as model interpretability and how computational methods can contribute to generate hypotheses and support the progression of new molecules.}
}
@article{GAITERO2021110960,
title = {System quality and security certification in seven weeks: A multi-case study in Spanish SMEs},
journal = {Journal of Systems and Software},
volume = {178},
pages = {110960},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110960},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000571},
author = {Domingo Gaitero and Marcela Genero and Mario Piattini},
keywords = {SME, Quality management system, Security management system, ISO 9001, ISO/IEC 27001, Multi-case study},
abstract = {Every company wishes to improve its system quality and security, all the more so in these times of digital transformation, since having a good quality and security management system is essential to any company’s commercial survival. Such needs are even more pressing for small and medium-sized enterprises (SMEs), given their limited time and resources. To address these needs, a Spanish company, Proceso Social, has developed an innovative method called “SevenWeeks” to allow SMEs to create or improve their quality and security management systems in just seven weeks, with a view to obtaining one or both of the ISO 9001 and ISO/IEC 27001 certifications. We have evaluated the effectiveness and usefulness of SevenWeeks by carrying out a multi-case study of 26 Spanish companies, based on independent sources of evidence. This allowed us to corroborate that SevenWeeks was indeed effective for and perceived as useful by all the companies, as it enabled them to create their own quality and security management systems in only seven weeks and to obtain the necessary ISO certification. The interviewees found SevenWeeks to be an agile and intuitive method, easy to implement, which reduces costs and effort. We also include some recommendations to improve and further develop the method.}
}
@article{PICCIALLI2021111,
title = {A survey on deep learning in medicine: Why, how and when?},
journal = {Information Fusion},
volume = {66},
pages = {111-137},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1566253520303651},
author = {Francesco Piccialli and Vittorio Di Somma and Fabio Giampaolo and Salvatore Cuomo and Giancarlo Fortino},
keywords = {Deep learning, Medicine, Artificial intelligence, Data science, Neural networks},
abstract = {New technologies are transforming medicine, and this revolution starts with data. Health data, clinical images, genome sequences, data on prescribed therapies and results obtained, data that each of us has helped to create. Although the first uses of artificial intelligence (AI) in medicine date back to the 1980s, it is only with the beginning of the new millennium that there has been an explosion of interest in this sector worldwide. We are therefore witnessing the exponential growth of health-related information with the result that traditional analysis techniques are not suitable for satisfactorily management of this vast amount of data. AI applications (especially Deep Learning), on the other hand, are naturally predisposed to cope with this explosion of data, as they always work better as the amount of training data increases, a phase necessary to build the optimal neural network for a given clinical problem. This paper proposes a comprehensive and in-depth study of Deep Learning methodologies and applications in medicine. An in-depth analysis of the literature is presented; how, where and why Deep Learning models are applied in medicine are discussed and reviewed. Finally, current challenges and future research directions are outlined and analysed.}
}
@incollection{2021311,
title = {Index},
editor = {Siddhartha Bhattacharyya and Naba Kumar Mondal and Jan Platos and Václav Snášel and Pavel Krömer},
booktitle = {Intelligent Environmental Data Monitoring for Pollution Management},
publisher = {Academic Press},
pages = {311-323},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-819671-7},
doi = {https://doi.org/10.1016/B978-0-12-819671-7.09991-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196717099917}
}
@article{JENKINS2021111239,
title = {Changing the approach to energy compliance in residential buildings – re-imagining EPCs},
journal = {Energy and Buildings},
volume = {249},
pages = {111239},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111239},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821005235},
author = {D.P. Jenkins and  S.Semple and S. Patidar and P. McCallum},
keywords = {EPC, Dynamic simulation, Urban energy modelling},
abstract = {As our need for energy information of buildings evolves, and the tools and methods at our disposal increase in scale and complexity, it is perhaps reasonable to expect a similar level of change in the way energy in buildings is assessed within national energy compliance frameworks. By comparing the available opportunities for building energy modelling with the current methodologies underlying Energy Performance Certificates, this study proposes future directions for standardised energy assessment of residential buildings and the impact this could have on different facets of energy policy. In carrying out this exercise, a number of criteria are proposed that could be used to appraise methodologies that align with future requirements of energy assessment, with two potential candidates for future energy assessment considered as part of this appraisal. An argument is thus proposed for better aligning future forms of standardised energy assessment with directions and requirements of future low-carbon energy policy.}
}
@incollection{FAKA2021199,
title = {Chapter 10 - Environmental sensing: a review of approaches using GPS/GNSS},
editor = {George p. Petropoulos and Prashant K. Srivastava},
booktitle = {GPS and GNSS Technology in Geosciences},
publisher = {Elsevier},
pages = {199-220},
year = {2021},
isbn = {978-0-12-818617-6},
doi = {https://doi.org/10.1016/B978-0-12-818617-6.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186176000135},
author = {Antigoni Faka and Konstantinos Tserpes and Christos Chalkias},
keywords = {Air pollution, Crowdsourcing, Environmental monitoring, Noise pollution, Smartphones},
abstract = {The condition of the environment is a critical factor for the quality of life and the well-being of humans. Environmental assessment ensures foreseeing and address of potential complications at an early stage in environmental planning and management. The recent technological advancements in spatiotemporal monitoring of environmental features provide to the scientific community the proper material for efficient and accurate assessment of the environment. This chapter aims to review the role of Global Positioning System (GPS)/Global Navigation Satellite System (GNSS) technologies in environmental sensing. The presentation focuses on the technologies and approaches followed in terms of data collection, analysis, and visualization and demonstrates the major categories of environmental sensing applications. The findings reveal a constantly gaining momentum of mobile monitoring and participatory systems. The adoption of GPS/GNSS enhancement techniques, including crowdsourcing methods, would assist environmental policymaking to establish targets and prioritize planned actions.}
}
@article{ALKHEDER2021120269,
title = {Taxi Ride sharing in Kuwait: Econ-enviro study},
journal = {Energy},
volume = {225},
pages = {120269},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120269},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221005181},
author = {Sharaf AlKheder},
keywords = {Traffic, Ridesharing, Passengers, Fuel consumption, Gas emissions},
abstract = {Traffic congestion had been the most important and sensitive problem facing Kuwait roads for decades. The objectives of this study were to: optimize the traffic flow on road networks by implementing shared rides, reduce vehicle emissions and minimize the ride cost. The concept concentrated on sharing the journeys starting and ending around the same place and time. The effect of ridesharing was studied according to the data collected for 3 months (July 2018, October 2018 and January 2019) from a taxi company. The data was then filtered so that the start point of all trips was South Surra. Three different scenarios were created: single passenger, two passengers, and more than two passengers. Java NetBeans was used to calculate the total distance for each month and the cost for each trip after applying the concept. “My driving” was used to obtain the fuel consumption to calculate the gas emissions for the three scenarios;CO, NOx and HC emissions. For the trip distance analysis, the results for July, October, and January decreased by 0.84%, 0.45%, and 1.25%, respectively. For gas emissions and by comparing the first scenario with the second one, the CO, NOx and HC emissions were reduced by 9.072%, 9.069%, and 9.074%, respectively. Finally, by comparing the first scenario with the third one, CO, NOx and HC emissions were lowered by 0.116%, 0.08%, and 0.108%, respectively.}
}
@article{KRAU2021150,
title = {Digital Manufacturing for Smart Small Satellites Systems},
journal = {Procedia Computer Science},
volume = {180},
pages = {150-161},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.138},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921001769},
author = {Markus Krauß and Florian Leutert and Markus R. Scholz and Michael Fritscher and Robin Heß and Christian Lilge and Klaus Schilling},
keywords = {digital manufacturing, demonstrator factory, new space, augmented reality user interfaces, human-robot collaboration, predictive maintenance, telemaintenance, quality of service, mobile robotics, intelligent material flow},
abstract = {The term Industry 4.0 – manufacturing with exploitation of digital technologies – comprises several challenging topics, among others intuitive machine programming, advanced maintenance-enabling technologies as well as flexible logistics. This paper gives an overview on recent research and development of our institute (Zentrum für Telematik, ZfT) in this field, the results of which are combined in an Industry 4.0 demonstration factory for the assembly of small satellites systems. We present a total of six tools to be used in such advanced manufacturing systems and their individual advantages. Based on our experience with industrial project partners, customers and visitors of our demonstration factory as well as on the evaluations of the jurors of several awards, we give a qualitative estimate of the effort required to port the individual tools to new production environments. Finally, utilizing our in-house expertise in the New-Space and Industry-4.0 sectors, we give an insight into the benefits achievable using digital manufacturing for small satellite assembly.}
}
@article{SINGH2021100489,
title = {Quantifying COVID-19 enforced global changes in atmospheric pollutants using cloud computing based remote sensing},
journal = {Remote Sensing Applications: Society and Environment},
volume = {22},
pages = {100489},
year = {2021},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2021.100489},
url = {https://www.sciencedirect.com/science/article/pii/S2352938521000252},
author = {Manmeet Singh and Bhupendra Bahadur Singh and Raunaq Singh and Badimela Upendra and Rupinder Kaur and Sukhpal Singh Gill and Mriganka Sekhar Biswas},
keywords = {COVID19, Google earth engine, PM, NO, AOD, Tropospheric ozone, Cloud computing},
abstract = {Global lockdowns in response to the COVID-19 pandemic have led to changes in the anthropogenic activities resulting in perceivable air quality improvements. Although several recent studies have analyzed these changes over different regions of the globe, these analyses have been constrained due to the usage of station based data which is mostly limited up to the metropolitan cities. Also the quantifiable changes have been reported only for the developed and developing regions leaving the poor economies (e.g. Africa) due to the shortage of in-situ data. Using a comprehensive set of high spatiotemporal resolution satellites and merged products of air pollutants, we analyze the air quality across the globe and quantify the improvement resulting from the suppressed anthropogenic activity during the lockdowns. In particular, we focus on megacities, capitals and cities with high standards of living to make the quantitative assessment. Our results offer valuable insights into the spatial distribution of changes in the air pollutants due to COVID-19 enforced lockdowns. Statistically significant reductions are observed over megacities with mean reduction by 19.74%, 7.38% and 49.9% in nitrogen dioxide (NO2), aerosol optical depth (AOD) and PM2.5 concentrations. Google Earth Engine empowered cloud computing based remote sensing is used and the results provide a testbed for climate sensitivity experiments and validation of chemistry-climate models. Additionally, Google Earth Engine based apps have been developed to visualize the changes in a real-time fashion.}
}
@article{SUN2021103372,
title = {Improvement of PM2.5 and O3 forecasting by integration of 3D numerical simulation with deep learning techniques},
journal = {Sustainable Cities and Society},
volume = {75},
pages = {103372},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103372},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721006466},
author = {Haochen Sun and Jimmy C.H. Fung and Yiang Chen and Wanying Chen and Zhenning Li and Yeqi Huang and Changqing Lin and Mingyun Hu and Xingcheng Lu},
keywords = {Deep learning, Community Multiscale Air Quality model, Spatial correction, PM, O},
abstract = {Air pollution is a major impediment to the sustainable development of cities and society. Governed by emission characteristics and meteorological conditions, the formation and destruction of fine particulate matter (PM2.5) and ozone (O3) are complicated, and accurate predictions of the concentrations of these two major secondary atmospheric pollutants remain challenging. In this study, by combining meteorological and air pollutant data from ground observations and the Weather Research and Forecasting (WRF)-Community Multiscale Air Quality (CMAQ) model simulations, a deep learning model structure based on long short-term memory layers (LSTM) was developed and applied to predict the PM2.5 and O3 concentrations in the future 48 h period. The forecasting improvement was extended to the whole Greater Bay Area by introducing a spatial correction (SC) method to the CMAQ simulation results. Compared with the original CMAQ forecast, the new method gained a 26% reduction in mean absolute error (MAE) and a 33% reduction in root mean square error (RMSE), respectively, in terms of PM2.5; it also achieved a 40% reduction in MAE and a 34% reduction in RMSE in terms of O3. SC method, applied to the whole GBA region, also reduced the overall MAE and RMSE by 10% and 17% in terms of PM2.5 and by 31% and 25% in terms of O3, respectively. Using an AI approach, our study provides new perspectives for further improving air quality forecasting from both temporal and spatial perspectives, thus increasing the smartness and resilience of the cities and promoting environmentally sustainable development in the area.}
}
@article{RAIJADA2021113857,
title = {Integration of personalized drug delivery systems into digital health},
journal = {Advanced Drug Delivery Reviews},
volume = {176},
pages = {113857},
year = {2021},
issn = {0169-409X},
doi = {https://doi.org/10.1016/j.addr.2021.113857},
url = {https://www.sciencedirect.com/science/article/pii/S0169409X21002490},
author = {Dhara Raijada and Katarzyna Wac and Emanuel Greisen and Jukka Rantanen and Natalja Genina},
keywords = {Personalized medicine, Pharmaceutical supply chain, Digital therapeutics, Smartphone, Internet of Things (IoT), 2D barcodes, Traceability, Anti-counterfeiting, Track and trace, Unique identifiers},
abstract = {Personalized drug delivery systems (PDDS), implying the patient-tailored dose, dosage form, frequency of administration and drug release kinetics, and digital health platforms for diagnosis and treatment monitoring, patient adherence, and traceability of drug products, are emerging scientific areas. Both fields are advancing at a fast pace. However, despite the strong complementary nature of these disciplines, there are only a few successful examples of merging these areas. Therefore, it is important and timely to combine PDDS with an increasing number of high-end digital health solutions to create an interactive feedback loop between the actual needs of each patient and the drug products. This review provides an overview of advanced design solutions for new products such as interactive personalized treatment that would interconnect the pharmaceutical and digital worlds. Furthermore, we discuss the recent advancements in the pharmaceutical supply chain (PSC) management and related limitations of the current mass production model. We summarize the current state of the art and envision future directions and potential development areas.}
}
@article{YAN2021102489,
title = {Data analytics for fuel consumption management in maritime transportation: Status and perspectives},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {155},
pages = {102489},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2021.102489},
url = {https://www.sciencedirect.com/science/article/pii/S1366554521002519},
author = {Ran Yan and Shuaian Wang and Harilaos N. Psaraftis},
keywords = {Maritime transportation, Ship fuel consumption prediction, Ship performance prediction, Ship energy efficiency optimization, Ship performance optimization},
abstract = {The shipping industry is associated with approximately three quarters of all world trade. In recent years, the sustainability of shipping has become a public concern, and various emissions control regulations to reduce pollutants and greenhouse gas (GHG) emissions from ships have been proposed and implemented globally. These regulations aim to drive the shipping industry in a low-carbon and low-pollutant direction by motivating it to switch to more efficient fuel types and reduce energy consumption. At the same time, the cyclical downturn of the world economy and high bunker prices make it necessary and urgent for the shipping industry to operate in a more cost-effective way while still satisfying global trade demand. As bunker fuel bunker (e.g., heavy fuel oil [HFO], liquified natural gas [LNG]) consumption is the main source of emissions and bunker fuel costs account for a large proportion of operating costs, shipping companies are making unprecedented efforts to optimize ship energy efficiency. It is widely accepted that the key to improving the energy efficiency of ships is the development of accurate models to predict ship fuel consumption rates under different scenarios. In this study, ship fuel consumption prediction models presented in the literature (including the academic literature and technical reports as a typical type of “grey literature”) are reviewed and compared, and models that optimize ship operations based on fuel consumption prediction results are also presented and discussed. Current research challenges and promising research questions on ship performance monitoring and operational optimization are identified.}
}
@article{LI2021105961,
title = {Evolution characteristics and displacement forecasting model of landslides with stair-step sliding surface along the Xiangxi River, three Gorges Reservoir region, China},
journal = {Engineering Geology},
volume = {283},
pages = {105961},
year = {2021},
issn = {0013-7952},
doi = {https://doi.org/10.1016/j.enggeo.2020.105961},
url = {https://www.sciencedirect.com/science/article/pii/S0013795220318585},
author = {Changdong Li and Robert E. Criss and Zhiyong Fu and Jingjing Long and Qinwen Tan},
keywords = {Reservoir landslide, Evolution process, Multi-step sliding surface, Displacement forecasting model, Lower reaches of Xiangxi River},
abstract = {Five large and many small landslides are developed in Jurassic strata along the lower reaches of Xiangxi River, where interbedded weak and hard bedrock layers foster the development of landslides with a “stair-step” sliding surface. The paper investigates the evolution characteristics of these landslides and presents a novel forecasting model for their displacements. The distribution characteristics and behavior of landslides developed along Xiangxi River is revealed by the database of landslides in the larger Zigui basin, of which this area is part. Most landslides occur at rather low elevations of <300 m and in areas of moderate rainfall. The geological evolution of landslides in the Xiangxi River valley can be divided into four stages, beginning with anticline formation, followed by valley incision, then by weathering and erosion, and culminating in formation of the colluvial landslides. The accumulative displacement curves of landslides with a stair-step sliding surface in Xiangxi River region also present obvious, step-like characteristics. A novel GA-CEEMD-RF algorithm was developed to predict the displacement of these stair-step landslides, which helps to define the combination of induced factors and weak stableness of prediction results using a single displacement prediction model and the multi-field monitoring data.}
}
@incollection{CHANAL2021111,
title = {Chapter 7 - Security and privacy in the internet of things: computational intelligent techniques-based approaches},
editor = {Siddhartha Bhattacharyya and Paramartha Dutta and Debabrata Samanta and Anirban Mukherjee and Indrajit Pan},
booktitle = {Recent Trends in Computational Intelligence Enabled Research},
publisher = {Academic Press},
pages = {111-127},
year = {2021},
isbn = {978-0-12-822844-9},
doi = {https://doi.org/10.1016/B978-0-12-822844-9.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228449000098},
author = {Poornima M. Chanal and Mahabaleshwar S. Kakkasageri and Sunil Kumar S. Manvi},
keywords = {Internet of things, security, privacy, authentication, integrity, confidentiality, availability, computational intelligence},
abstract = {The Internet of Things (IoT) is a network of universally interconnected devices via the Internet. The IoT requires the interconnection between billions or trillions of intelligent objects. IoT devices (nodes) are capable of capturing, preserving, analyzing, and sharing data about themselves and their physical world. Security and privacy are the major challenges in the implementation of IoT technology. Major privacy aspects in the IoT are stealing data, monitoring, and tracking, etc. Authentication, integrity, and confidentiality are major concerns for privacy and security preservation in the IoT. Computational intelligence (CI) focuses on the design and development of intelligent algorithms to solve real-time problems with minimum cost. The main goal of CI is to supplement natural and artificial intelligence to produce human-required competitive results. Computational intelligent mechanisms for providing privacy and security in the IoT include quantum cryptography, artificial intelligence, neural networks, natural computational techniques, bio-inspired computational techniques, fuzzy logic techniques, genetic algorithms, intelligent multiagents, etc.}
}
@article{DIAS2021104134,
title = {Criteria for selecting apps: Debating the perceptions of young children, parents and industry stakeholders},
journal = {Computers & Education},
volume = {165},
pages = {104134},
year = {2021},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2021.104134},
url = {https://www.sciencedirect.com/science/article/pii/S0360131521000117},
author = {Patrícia Dias and Rita Brito},
keywords = {Young children, Mobile media, Apps, Parents, Stakeholders},
abstract = {It is indisputable that young children are exposed to digital media since birth and start using them very early. This fuels debate that engages scholars and researchers, industry and brands, policymakers, and parents. Our study aimed to contrast these different perspectives, adding the view of children, who are frequently left out of this debate. Using an exploratory qualitative approach, we conducted interviews with children under 8 years old and their parents in 81 families, and with 17 expert stakeholders in different fields. We focused on their perceptions and practices regarding digital media, and specifically on how they assess and select apps, concluding that parents value safety and learning, children enjoy entertainment, and stakeholders highlight the importance of a good user experience.}
}
@article{YU2021103754,
title = {Identification of pediatric respiratory diseases using a fine-grained diagnosis system},
journal = {Journal of Biomedical Informatics},
volume = {117},
pages = {103754},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103754},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421000836},
author = {Gang Yu and Zhongzhi Yu and Yemin Shi and Yingshuo Wang and Xiaoqing Liu and Zheming Li and Yonggen Zhao and Fenglei Sun and Yizhou Yu and Qiang Shu},
keywords = {Respiratory diseases, Fine-grained diagnosis, Pediatric diagnosis, Clinical notes, Multi-modal},
abstract = {Respiratory diseases, including asthma, bronchitis, pneumonia, and upper respiratory tract infection (RTI), are among the most common diseases in clinics. The similarities among the symptoms of these diseases precludes prompt diagnosis upon the patients’ arrival. In pediatrics, the patients’ limited ability in expressing their situation makes precise diagnosis even harder. This becomes worse in primary hospitals, where the lack of medical imaging devices and the doctors’ limited experience further increase the difficulty of distinguishing among similar diseases. In this paper, a pediatric fine-grained diagnosis-assistant system is proposed to provide prompt and precise diagnosis using solely clinical notes upon admission, which would assist clinicians without changing the diagnostic process. The proposed system consists of two stages: a test result structuralization stage and a disease identification stage. The first stage structuralizes test results by extracting relevant numerical values from clinical notes, and the disease identification stage provides a diagnosis based on text-form clinical notes and the structured data obtained from the first stage. A novel deep learning algorithm was developed for the disease identification stage, where techniques including adaptive feature infusion and multi-modal attentive fusion were introduced to fuse structured and text data together. Clinical notes from over 12000 patients with respiratory diseases were used to train a deep learning model, and clinical notes from a non-overlapping set of about 1800 patients were used to evaluate the performance of the trained model. The average precisions (AP) for pneumonia, RTI, bronchitis and asthma are 0.878, 0.857, 0.714, and 0.825, respectively, achieving a mean AP (mAP) of 0.819. These results demonstrate that our proposed fine-grained diagnosis-assistant system provides precise identification of the diseases.}
}
@article{ACUTO2021105295,
title = {Mobilising urban knowledge in an infodemic: Urban observatories, sustainable development and the COVID-19 crisis},
journal = {World Development},
volume = {140},
pages = {105295},
year = {2021},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2020.105295},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X20304228},
author = {Michele Acuto and Ariana Dickey and Stephanie Butcher and Carla-Leanne Washbourne},
keywords = {Urban observatories, Infodemic, Boundary-spanning, Knowledge translation, Science-policy interface},
abstract = {Along with disastrous health and economic implications, COVID-19 has also been an epidemic of misinformation and rumours - an ‘infodemic’. The desire for robust, evidence-based policymaking in this time of disruption has been at the heart of the multilateral response to the crisis, not least in terms of supporting a continuing agenda for global sustainable development. The role of boundary-spanning knowledge institutions in this context could be pivotal, not least in cities, where much of the pandemic has struck. ‘Urban observatories’ have emerged as an example of such institutions; harbouring great potential to produce and share knowledge supporting sustainable and equitable processes of recovery. Building on four ‘live’ case studies during the crisis of institutions based in Johannesburg, Karachi, Freetown and Bangalore, our research note aims to capture the role of these institutions, and what it means to span knowledge boundaries in the current crisis. We do so with an eye towards a better understanding of their knowledge mobilisation practices in contributing towards sustainable urban development. We highlight that the crisis offers a key window for urban observatories to play a progressive and effective role for sustainable and inclusive development. However, we also underline continuing challenges in these boundary knowledge dynamics: including issues of institutional trust, inequality of voices, collective memory, and the balance between normative and advisory roles for observatories.}
}
@article{DAVID2021106630,
title = {Towards a comprehensive characterisation of the human internal chemical exposome: Challenges and perspectives},
journal = {Environment International},
volume = {156},
pages = {106630},
year = {2021},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2021.106630},
url = {https://www.sciencedirect.com/science/article/pii/S0160412021002555},
author = {Arthur David and Jade Chaker and Elliott J. Price and Vincent Bessonneau and Andrew J. Chetwynd and Chiara M. Vitale and Jana Klánová and Douglas I. Walker and Jean-Philippe Antignac and Robert Barouki and Gary W. Miller},
keywords = {Exposome, High-Resolution Mass Spectrometry, Internal chemical exposome, Non-targeted analysis, Suspect screening, EWAS},
abstract = {The holistic characterisation of the human internal chemical exposome using high-resolution mass spectrometry (HRMS) would be a step forward to investigate the environmental ætiology of chronic diseases with an unprecedented precision. HRMS-based methods are currently operational to reproducibly profile thousands of endogenous metabolites as well as externally-derived chemicals and their biotransformation products in a large number of biological samples from human cohorts. These approaches provide a solid ground for the discovery of unrecognised biomarkers of exposure and metabolic effects associated with many chronic diseases. Nevertheless, some limitations remain and have to be overcome so that chemical exposomics can provide unbiased detection of chemical exposures affecting disease susceptibility in epidemiological studies. Some of these limitations include (i) the lack of versatility of analytical techniques to capture the wide diversity of chemicals; (ii) the lack of analytical sensitivity that prevents the detection of exogenous (and endogenous) chemicals occurring at (ultra) trace levels from restricted sample amounts, and (iii) the lack of automation of the annotation/identification process. In this article, we discuss a number of technological and methodological limitations hindering applications of HRMS-based methods and propose initial steps to push towards a more comprehensive characterisation of the internal chemical exposome. We also discuss other challenges including the need for harmonisation and the difficulty inherent in assessing the dynamic nature of the internal chemical exposome, as well as the need for establishing a strong international collaboration, high level networking, and sustainable research infrastructure. A great amount of research, technological development and innovative bio-informatics tools are still needed to profile and characterise the “invisible” (not profiled), “hidden” (not detected) and “dark” (not annotated) components of the internal chemical exposome and concerted efforts across numerous research fields are paramount.}
}
@article{STOYKOVA2021105575,
title = {Digital evidence: Unaddressed threats to fairness and the presumption of innocence},
journal = {Computer Law & Security Review},
volume = {42},
pages = {105575},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105575},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000480},
author = {Radina Stoykova},
keywords = {Presumption of innocence, Digital evidence, Reliability, Digital forensics, Fair trial},
abstract = {Contemporary criminal investigation assisted by computing technology imposes challenges to the right to a fair trial and the scientific validity of digital evidence. This paper identifies three categories of unaddressed threats to fairness and the presumption of innocence during investigations – (i) the inappropriate and inconsistent use of technology; (ii) old procedural guarantees, which are not adapted to contemporary digital evidence processes and services; (iii) and the lack of reliability testing in digital forensics practice. Further, the solutions that have been suggested to overcome these issues are critically reviewed to identify their shortcomings. Ultimately, the paper argues for the need of legislative intervention and enforcement of standards and validation procedures for digital evidence in order to protect innocent suspects and all parties in the criminal proceedings from the negative consequences of technology-assisted investigations.}
}
@article{KARAGIANNIDIS2021108616,
title = {Data-driven modelling of ship propulsion and the effect of data pre-processing on the prediction of ship fuel consumption and speed loss},
journal = {Ocean Engineering},
volume = {222},
pages = {108616},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.108616},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821000512},
author = {Pavlos Karagiannidis and Nikos Themelis},
keywords = {Performance monitoring, Artificial neural network, Data processing, Propulsion modeling, Predictive analytics},
abstract = {Data-driven models for ship propulsion are presented while the effect of data pre-processing techniques is extensively examined. In this study, a large, automatically collected with high sampling frequency data set is exploited for training models that estimate the required shaft power or main engine fuel consumption of a container ship sailing under arbitrary conditions. Emphasis is given to the statistical evaluation and pre-processing of the data and two algorithms are presented for this scope. Additionally, state-of-the-art techniques for training and optimizing Feed-Forward Neural Networks (FNNs) are applied. The results indicate that with a delicate filtering and preparation stage it is possible to significantly increase the model's accuracy. Therefore, increase the prediction ability and awareness regarding the ship's hull and propeller actual condition. Furthermore, such models could be employed in studies targeting at the improvement of ship's operational energy efficiency.}
}
@incollection{2021399,
title = {Index},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {399-462},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10828-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717108280}
}
@article{TANG2021142007,
title = {Robustness analysis of storm water quality modelling with LID infrastructures from natural event-based field monitoring},
journal = {Science of The Total Environment},
volume = {753},
pages = {142007},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.142007},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720355364},
author = {Sijie Tang and Jiping Jiang and Yi Zheng and Yi Hong and Eun-Sung Chung and Asaad Y. Shamseldin and Yan Wei and Xiuheng Wang},
keywords = {Robustness analysis, Stormwater quality, SWMM, Sponge city, Nash-Sutcliffe efficiency coefficient (NSE)},
abstract = {Sponge city construction (SCC) in China, as a new concept and a practical application of low-impact development (LID), is gaining wide popularity. Modelling tools are widely used to evaluate the ecological benefits of SCC in stormwater pollution mitigation. However, the understanding of the robustness of water quality modelling with different LID design options is still limited due to the paucity of water quality data as well as the high cost of water quality data collection and model calibration. This study develops a new concept of ‘robustness’ measured by model calibration performances. It combines an automatic calibration technique with intensive field monitoring data to perform the robustness analysis of storm water quality modelling using the SWMM (Storm Water Management Model). One of the national pilot areas of SCC, Fenghuang Cheng, in Shenzhen, China, is selected as the study area. Five water quality variables (COD, NH3-N, TN, TP, and SS) and 13 types of LID/non-LID infrastructures are simulated using 37 rainfall events. The results show that the model performance is satisfactory for different water quality variables and LID types. Water quality modelling of greenbelts and rain gardens has the best performance, while the models of barrels and green roofs are not as robust as those of the other LID types. In urban runoff, three water quality parameters, namely, SS, TN and COD, are better captured by the SWMM models than NH3-N and TP. The modelling performance tends to be better under heavy rain and significant pollutant concentrations, denoting a potentially more stable and reliable design of infrastructures. This study helps to improve the current understanding of the feasibility and robustness of using the SWMM model in sponge city design.}
}
@incollection{AWOTUNDE2021235,
title = {Chapter Nine - Prediction and classification of diabetes mellitus using genomic data},
editor = {Arun Kumar Sangaiah and Subhas Mukhopadhyay},
booktitle = {Intelligent IoT Systems in Personalized Health Care},
publisher = {Academic Press},
pages = {235-292},
year = {2021},
series = {Cognitive Data Science in Sustainable Computing},
isbn = {978-0-12-821187-8},
doi = {https://doi.org/10.1016/B978-0-12-821187-8.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211878000095},
author = {Joseph Bamidele Awotunde and Femi Emmanuel Ayo and Rasheed Gbenga Jimoh and Roseline Oluwaseun Ogundokun and Opeyemi Emmanuel Matiluko and Idowu Dauda Oladipo and Muyideen Abdulraheem},
keywords = {Genomic data, Diabetes mellitus, Classification, Genetic algorithm, Deep neural networks},
abstract = {Diabetes mellitus (DM) is one of the chronic and debilitating diseases in modern society, hence the urgent need to prevent epidemic growth in society. This chapter is motivated by the studies of several scholars in the field of microarrays datasets for gene expression. Nonetheless, there are very few available gene signatures across datasets, thereby generating sample selection bias and over selection sets matching. Subjective selection of this gene and sample pairings could be addressed through large average submatrices and a unique method of biclustering using objective statistical assumptions to reconstruct robust signatures of expression. Hence, SWITCH (SupWald Identification of CHanges DNA copy) was created to label CNAs in platforms of aCGH and to connect them with subtypes. Therefore, the process of selecting the most informative gene biomarker was done using the genetic algorithm (GA) and deep neural networks (DNN) for biological sample classification. The simulated genomics datasets were divided into 95% training and 5% test samples and the DNN classifier is modified using these sets of SNPs and fine-tuned to classify type II DM analyses. The datasets are cleaned into four single-nucleotide polymorphism (SNP) function sets: 96 (P-value: 1×10−5), 214 (P-value: 1×10−4), 399 (P-value: 1×10−3), and 678 (P-value: 1×10−2) using P-value thresholds. The classifier was built using the training data while testing its efficiency on the test sample. MATLAB was used to implement the GA and DNN. The DNN model showed a significant predictive output with type II DM having AUC=0.9537 in male and AUC=0.9349 in female. An experimental test was carried out to determine the associations of all SNP datasets with the type II diabetes phenotype. The results from the test showed an enhanced model performance with 399 and 678 SNPs, respectively. The test result also showed that the higher the number of SNPs the better the predictive performance of the designed model. Finally, the result showed that DNNs can be used to predict type II diabetes using genomic-based data.}
}
@article{CHEN2021112348,
title = {Eddy morphology: Egg-like shape, overall spinning, and oceanographic implications},
journal = {Remote Sensing of Environment},
volume = {257},
pages = {112348},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112348},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721000663},
author = {Ge Chen and Jie Yang and Guiyan Han},
keywords = {Oceanic eddy, Morphology, Altimeter},
abstract = {Systematic tracking of individual eddies during their entire lifetimes marks a significant milestone in satellite oceanography over the first two decades of this century. Assuming that the geometry and properties of an oceanic eddy are orientation-sensitive, an angularly aligned composite analysis of over 40 million eddy-focused surface topographic “snapshots” obtained by merging tandem altimeter data from January 1993 through January 2019 reveals that oceanic vortices appear to have a characteristic surface shape of “egg” rather than circle or ellipse as previously understood. Consequently, a second-order moment in eddy morphology is revealed which leads to a ~ 10 km departure from a standard ellipse in terms of major axis. Furthermore, the sharp poles of oceanic eddies exhibit two quasi-orthogonal modes of orientation: primarily meridional and secondarily zonal. The additional submesoscale asymmetry in eddy shape is confirmed by a consistent anisotropy in a normalized eddy-centric velocity field derived from over 25 thousand drifters. The high-order moments in terms of geometric asymmetry and dynamic anisotropy associated with mesoscale eddies (which are found to be statistically significant at 99% level) may have profound geophysical and biological impacts on energy transport, substance entrainment, as well as ecosystem dynamics in the ocean. In particular, it is demonstrated that some of the previously identified patterns in eddy properties obtained by non-rotated normalization may be notably biased due to the ignorance of existing eddy orientation.}
}
@article{CHIOU2021100042,
title = {Travel pattern analytics driven by cellular signaling data},
journal = {Asian Transport Studies},
volume = {7},
pages = {100042},
year = {2021},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2021.100042},
url = {https://www.sciencedirect.com/science/article/pii/S2185556021000109},
author = {Yu-Chiun Chiou and Chih-Wei Hsieh},
keywords = {Cellular signaling data, Trip identification, Trip chaining pattern},
abstract = {Abstract
Cellular signaling data (CSD) could be one of the primary data sources for transportation planning and demand forecasting as a result of the rapid growth in triangulation and location techniques. Utilizing CSD to analyze travel patterns requires a meticulous process to overcome data oscillation and trajectory discontinuities. This study analyzes CSD and develops analytical models to enhance the applicability of CSD in transportation planning. For this study, we invite 30 volunteers to participate in a 30-day travel diary survey to collect data. In addition, based on CSD, we develop analytical algorithms to generate travel trajectories and analyze travel patterns, including the home and work location, trip chaining, and trip purpose of the user. Comparing the model results against the diary travel survey data indicates 84% accuracy for trip estimation and 89% accuracy for trip purpose identification, suggesting that the applicability of the proposed algorithm is satisfactory.}
}
@article{BASTIANELLI2021109239,
title = {Survival and cause-specific mortality of European wildcat (Felis silvestris) across Europe},
journal = {Biological Conservation},
volume = {261},
pages = {109239},
year = {2021},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2021.109239},
url = {https://www.sciencedirect.com/science/article/pii/S0006320721002913},
author = {Matteo Luca Bastianelli and Joseph Premier and Mathias Herrmann and Stefano Anile and Pedro Monterroso and Tobias Kuemmerle and Carsten F. Dormann and Sabrina Streif and Saskia Jerosch and Malte Götz and Olaf Simon and Marcos Moleón and José María Gil-Sánchez and Zsolt Biró and Jasja Dekker and Analena Severon and Axel Krannich and Karsten Hupe and Estelle Germain and Dominique Pontier and René Janssen and Pablo Ferreras and Francisco Díaz-Ruiz and José María López-Martín and Fermín Urra and Lolita Bizzarri and Elena Bertos-Martín and Markus Dietz and Manfred Trinzen and Elena Ballesteros-Duperón and José Miguel Barea-Azcón and Andrea Sforzi and Marie-Lazarine Poulle and Marco Heurich},
keywords = {Anthropogenic landscapes, European wildcat, Survival, Human-caused mortality, Roadkill, Road density},
abstract = {Humans have transformed most landscapes across the globe, forcing other species to adapt in order to persist in increasingly anthropogenic landscapes. Wide-ranging solitary species, such as wild felids, struggle particularly in such landscapes. Conservation planning and management for their long-term persistence critically depends on understanding what determine survival and what are the main mortality risks. We carried out the first study on annual survival and cause-specific mortality of the European wildcat with a large and unique dataset of 211 tracked individuals from 22 study areas across Europe. Furthermore, we tested the effect of environmental and human disturbance variables on the survival probability. Our results show that mortalities were mainly human-caused, with roadkill and poaching representing 57% and 22% of the total annual mortality, respectively. The annual survival probability of wildcat was 0.92 (95% CI = 0.87–0.98) for females and 0.84 (95% CI = 0.75–0.94) for males. Road density strongly impacted wildcat annual survival, whereby an increase in the road density of motorways and primary roads by 1 km/km2 in wildcat home-ranges increased mortality risk ninefold. Low-traffic roads, such as secondary and tertiary roads, did not significantly affect wildcat's annual survival. Our results deliver key input parameters for population viability analyses, provide planning-relevant information to maintain subcritical road densities in key wildcat habitats, and identify conditions under which wildcat-proof fences and wildlife crossing structures should be installed to decrease wildcat mortality.}
}
@article{LIN2021100894,
title = {Prevalence and intervention of preoperative anemia in Chinese adults: A retrospective cross-sectional study based on national preoperative anemia database},
journal = {EClinicalMedicine},
volume = {36},
pages = {100894},
year = {2021},
issn = {2589-5370},
doi = {https://doi.org/10.1016/j.eclinm.2021.100894},
url = {https://www.sciencedirect.com/science/article/pii/S2589537021001747},
author = {Jie Lin and Chao Wang and Junting Liu and Yang Yu and Shufang Wang and Aiqing Wen and Jufeng Wu and Long Zhang and Futing Sun and Xiaojun Guo and Fenghua Liu and Hailan Li and Na Li and Haibao Wang and Yi Lv and Zhonghua Jia and Xiaoyan Li and Jun Zhang and Zunyan Li and Shanshan Liu and Shuhuai Zhong and Jun Yang and Shuxuan Ma and Lingling Zhou and Xiaozhen Guan and Chunya Ma and Shijun Cheng and Shengxiong Chen and Zhenhua Xu and Gang Li and Deqing Wang},
keywords = {Preoperative anemia, Transfusion, Iron, Erythropoietin},
abstract = {Background
Preoperative anemia is an important pillar of perioperative patient blood management. However, there was no literature comprehensively described the current situation of preoperative anemia in China.
Methods
We conducted a national retrospective cross-sectional study to assess the prevalence and intervention of preoperative anemia in Chinese adults. Data were from the National Preoperative Anemia Database based on hospital administration data from January 1, 2013 to December 31, 2018.
Findings
A total of 797,002 patients were included for analysis. Overall, 27.57% (95% CI 27.47–27.67) of patients had preoperative anemia, which varied by gender, age, regions, and type of operation. Patients who were female, age over 60 years old, from South China, from provinces with lower per capita GDP, underwent operations on the lymphatic and hematopoietic system, with laboratory abnormalities were more likely to have a high risk of preoperative anemia. Among patients with preoperative anemia, 5.16% (95% CI 5.07–5.26) received red blood cell transfusion, 7.79% (95% CI 7.67–7.91) received anemia-related medications such as iron, erythropoietin, folic acid or vitamin B12, and 12.25% (95% CI 12.10–12.40) received anemia-related therapy (red blood cell transfusion or anemia-related medications) before operation. The probability of preoperative RBC transfusion decreased by 54.92% (OR 0.46, 95% CI 0.46–0.47) as each 10-g/L increase in preoperative hemoglobin. Patients with preoperative hemoglobin less than 130 g/L was associated with longer hospital stay and more hospital costs. Patients with severe preoperative anemia given iron preoperatively had lower intra/post-operative RBC transfusion rate, shorter length of stay and less hospitalization costs, but no similar correlation was found in patients with mild and moderate preoperative anemia and patients given erythropoietin preoperatively.
Interpretation
Our present study shows that preoperative anemia is currently a relatively prevalent problem that has not been fully appreciated in China. More researches will be required to optimize the treatment of preoperative anemia.
Funding
National Natural Science Foundation of China and the Logistics Support Department of the Central Military Commission.}
}
@incollection{CORRIGAN2021159,
title = {Chapter 9 - Image analysis in drug discovery},
editor = {Stephanie Kay Ashenden},
booktitle = {The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry},
publisher = {Academic Press},
pages = {159-189},
year = {2021},
isbn = {978-0-12-820045-2},
doi = {https://doi.org/10.1016/B978-0-12-820045-2.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200452000106},
author = {Adam M. Corrigan and Daniel Sutton and Johannes Zimmermann and Laura A.L. Dillon and Kaustav Bera and Armin Meier and Fabiola Cecchi and Anant Madabhushi and Günter Schmidt and Jason Hipp},
keywords = {Artificial intelligence, Computational pathology, Radiomics, Deep learning},
abstract = {Across all stages of drug discovery and development, experimental assays are performed to understand the effect of a drug or drug candidate—at the molecular, cellular, organ, or organism level. Imaging is a key technology in this process, and an imaging assay consisting of sample preparation, image acquisition, and image analysis provides a quantitative readout of a system. Historically, chemical assays have been the workhorse of early discovery, screening millions of compounds for a simple endpoint, whereas imaging was primarily used in lower throughput mechanistic studies. However, with development of high-throughput high-content microscopy platforms, the throughput of imaging assays now rivals chemical screens. Similarly, innovations in image analysis mean that robust quantitative conclusions can be derived from complex and multimodal image data, driving informed decision making later in the drug development process. For these reasons, imaging is widely used throughout the pharmaceutical industry and throughout multiple stages of the drug discovery process.}
}
@article{ZHU2021106519,
title = {DNN-based seabed classification using differently weighted MBES multifeatures},
journal = {Marine Geology},
volume = {438},
pages = {106519},
year = {2021},
issn = {0025-3227},
doi = {https://doi.org/10.1016/j.margeo.2021.106519},
url = {https://www.sciencedirect.com/science/article/pii/S0025322721001018},
author = {Zhengren Zhu and Xiaodong Cui and Kai Zhang and Bo Ai and Bo Shi and Fanlin Yang},
keywords = {MBES, Backscatter mosaic, Angular response, Seabed sediment classification, Deep neural networks, Weight coefficient},
abstract = {Seabed sediment classification has significance for the utilization of marine resources and marine scientific research. Currently, the multibeam echo sounder (MBES) is increasingly becoming the tool of choice for large-scale seabed sediment classification. To further explore the technology of seabed sediment classification, this paper proposes a new classification method. In addition to backscatter mosaic, the method also integrates three other different types of features, including texture features of backscatter mosaic, MBES bathymetry features, and backscatter angular response (AR) features, which are given different weights in the classification process. First, geographically weighted regression (GWR) analysis is performed between different types of features and seabed sediment types, and the normalized coefficient of determination (R2) is employed as the weight coefficient for the different types of features. Second, the backscatter mosaic is combined with features from different types to predict the seabed sediment types using a deep neural network (DNN) classifier. Third, the classification residuals of the features from these three different types are acquired through the above classification results. Last, the classification residuals of features from different types are added to the classification results of the backscatter mosaic according to the weights, thereby achieving seabed sediment classification based on MBES multifeatures with different weights. The results show that the overall classification accuracy of the seabed sediments can be significantly improved from 88.98%/85.14% to 93.43% when using the DNN classification model based on MBES multifeatures with different weights compared with the other two models (DNN classification model based on MBES multifeatures with equal weights and DNN classification model based on principal component analysis (PCA) dimensionality reduction). The kappa coefficient can also be significantly improved from approximately 0.85/0.80 to 0.91. Via analysis, the proposed method can reasonably assign the weights of the different features and take advantage of integrating MBES multifeatures for seabed sediment classification. This approach also provides an important reference for future research on seabed sediment classification.}
}
@article{HAN202126,
title = {A Bayesian LSTM model to evaluate the effects of air pollution control regulations in Beijing, China},
journal = {Environmental Science & Policy},
volume = {115},
pages = {26-34},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2020.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1462901120313538},
author = {Yang Han and Jacqueline CK Lam and Victor OK Li and David Reiner},
keywords = {Air pollution control regulations, Effects of regulatory interventions, Bayesian LSTM, Propensity score, Counterfactual analysis, Causal inference},
abstract = {Rapid socio-economic development and urbanization have resulted in serious deterioration in air-quality in many world cities, including Beijing, China. This study attempts to examine the effectiveness of air pollution control regulations implemented in Beijing during 2008–2019 through a data-driven regulatory intervention analysis. Our proposed Bayesian deep learning model utilizes proxy data including Aerosol Optical Depth (AOD) and meteorology as well as socio-economic data, while accounting for confounding effects via propensity score estimation. Our results show that air pollution control regulatory measures implemented in China and Beijing during 2008–2019 reduced PM2.5 pollution in Beijing by 11 % on average. After the introduction of Action Plan for Clean Air in China and Beijing in late 2013, as compared to the hypothetical PM2.5 concentration (without any regulatory interventions), the estimated PM2.5 reduction increased dramatically from 15 % in 2015 to 44 % in 2018. Our results suggest that Beijing’s air quality has improved gradually over the past decade, though the annual PM2.5 pollution still exceeds the WHO threshold. In this regard, the air pollution control regulations introduced in Beijing and China tend to become more effective after 2015, suggesting a 2-year time lag before the stringent air pollution control regulations starting from 2013 takes any strong positive effects. Moreover, as compared to the air pollution control regulations introduced before 2013, newly introduced policy-making governance, which couples the policy-makings of the local jurisdictions with that of the central government, and the new policy measures that tackle the vested interests of the local stakeholders in Beijing and its nearby cities, alongside with the stringent local and national air pollution control regulations and plans, should help reduce air pollution and promote healthy living in Beijing over the longer term.}
}
@article{HUSAK2021517,
title = {Predictive methods in cyber defense: Current experience and research challenges},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {517-530},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329836},
author = {Martin Husák and Václav Bartoš and Pavol Sokol and Andrej Gajdoš},
keywords = {Cybersecurity, Prediction, Forecasting, Data mining, Machine learning, Time series},
abstract = {Predictive analysis allows next-generation cyber defense that is more proactive than current approaches based on intrusion detection. In this paper, we discuss various aspects of predictive methods in cyber defense and illustrate them on three examples of recent approaches. The first approach uses data mining to extract frequent attack scenarios and uses them to project ongoing cyberattacks. The second approach uses a dynamic network entity reputation score to predict malicious actors. The third approach uses time series analysis to forecast attack rates in the network. This paper presents a unique evaluation of the three distinct methods in a common environment of an intrusion detection alert sharing platform, which allows for a comparison of the approaches and illustrates the capabilities of predictive analysis for current and future research and cybersecurity operations. Our experiments show that all three methods achieved a sufficient technology readiness level for experimental deployment in an operational setting with promising accuracy and usability. Namely prediction and projection methods, despite their differences, are highly usable for predictive blacklisting, the first provides a more detailed output, and the second is more extensible. Network security situation forecasting is lightweight and displays very high accuracy, but does not provide details on predicted events.}
}
@article{ZHAO2021102913,
title = {Impact of data processing on deriving micro-mobility patterns from vehicle availability data},
journal = {Transportation Research Part D: Transport and Environment},
volume = {97},
pages = {102913},
year = {2021},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2021.102913},
url = {https://www.sciencedirect.com/science/article/pii/S1361920921002121},
author = {Pengxiang Zhao and He Haitao and Aoyong Li and Ali Mansourian},
keywords = {Micro-mobility, E-scooter sharing, Data processing, Data sampling, Spatio-temporal patterns, Vehicle availability data, GPS, Trip identification},
abstract = {Vehicle availability data is emerging as a potential data source for micro-mobility research and applications. However, there is not yet research that systematically evaluates or validates the processing of this emerging mobility data. To fill this gap, we propose a generally applicable data processing framework and validate its related algorithms. The framework exploits micro-mobility vehicle availability data to identify individual trips and derive aggregate patterns by evaluating a range of temporal, spatial, and statistical mobility descriptors. The impact of data processing is systematically and rigorously investigated by applying the proposed framework with a case study dataset from Zurich, Switzerland. Our results demonstrate that the sampling rate used when collecting vehicle availability data has a significant and intricate impact on the derived micro-mobility patterns. This research calls for more attention to investigate various issues with emerging mobility data processing to ensure its validity for transportation research and practices.}
}
@article{GAO20211053,
title = {Hepatic transcriptomic adaptation from prepartum to postpartum in dairy cows},
journal = {Journal of Dairy Science},
volume = {104},
number = {1},
pages = {1053-1072},
year = {2021},
issn = {0022-0302},
doi = {https://doi.org/10.3168/jds.2020-19101},
url = {https://www.sciencedirect.com/science/article/pii/S0022030220309590},
author = {S.T. Gao and D.D. Girma and M. Bionaz and L. Ma and D.P. Bu},
keywords = {RNA sequencing, peripartum cow, metabolic adaptation, hepatic transcriptome},
abstract = {ABSTRACT
The transition from pregnancy to lactation is the most challenging period for high-producing dairy cows. The liver plays a key role in biological adaptation during the peripartum. Prior works have demonstrated that hepatic glucose synthesis, cholesterol metabolism, lipogenesis, and inﬂammatory response are increased or activated during the peripartum in dairy cows; however, those works were limited by a low number of animals used or by the use of microarray technology, or both. To overcome such limitations, an RNA sequencing analysis was performed on liver biopsies from 20 Holstein cows at 7 ± 5d before (Pre-P) and 16 ± 2d after calving (Post-P). We found 1,475 upregulated and 1,199 downregulated differently expressed genes (DEG) with a false discovery rate adjusted P-value < 0.01 between Pre-P and Post-P. Bioinformatic analysis revealed an activation of the metabolism, especially lipid, glucose, and amino acid metabolism, with increased importance of the mitochondria and a key role of several signaling pathways, chiefly peroxisome proliferators-activated receptor (PPAR) and adipocytokines signaling. Fatty acid oxidation and gluconeogenesis, with a likely increase in amino acid utilization to produce glucose, were among the most important functions revealed by the transcriptomic adaptation to lactation in the liver. Although gluconeogenesis was induced, data indicated decrease in expression of glucose transporters. The analysis also revealed high activation of cell proliferation but inhibition of xenobiotic metabolism, likely due to the liver response to inflammatory-like conditions. Co-expression network analysis disclosed a tight connection and coordination among genes driving biological processes associated with protein synthesis, energy and lipid metabolism, and cell proliferation. Our data confirmed the importance of metabolic adaptation to lipid and glucose metabolism in the liver of early Post-P cows, with a pivotal role of PPAR and adipocytokines.}
}
@article{BERRA2021118663,
title = {Remote sensing of temperate and boreal forest phenology: A review of progress, challenges and opportunities in the intercomparison of in-situ and satellite phenological metrics},
journal = {Forest Ecology and Management},
volume = {480},
pages = {118663},
year = {2021},
issn = {0378-1127},
doi = {https://doi.org/10.1016/j.foreco.2020.118663},
url = {https://www.sciencedirect.com/science/article/pii/S0378112720314328},
author = {Elias F. Berra and Rachel Gaulton},
keywords = {Satellite data, Land surface phenology, Phenometrics, Ground observations, SOS, EOS, Validation},
abstract = {Vegetation phenology is the study of recurring plant life cycle stages, seasonality which is linked to many ecosystem processes and is an important proxy of climate and environmental change. Remote sensing has been playing an important and increasing role in the monitoring and assessment of vegetation phenology. The aim of this review is to critically examine key studies related to remote sensing of vegetation phenology, with a special focus on temperate and boreal forests. Specifically, we focus on how the latest ground, near-surface and aerial data have been used to assess the satellite-derived Land Surface Phenology (LSP) metrics and the agreements that has been achieved in the last 15 years. Results demonstrated that the timing of satellite-derived LSP events can be detected, in the best-case scenarios, with a certainty of around half-week for spring metrics (e.g. Day of Year -DOY- of start of growing season) and around one week for autumn metrics (e.g. DOY of end of growing season). With expected shifts in plant phenology averaging <1 day per decade, such LSP uncertainties (in terms of absolute phenological dates) could greatly over- or under-estimate these species-level shifts; but the spatial variation in phenology can be consistently monitored. An increasing number of studies have investigated autumn phenology in the last decade, but autumn phenological dates continue to be more challenging to retrieve and interpret than spring dates. Emerging opportunities to further advance remote sensing of forest phenology is presented that includes synergetic use of multiple orbital sensors and its LSP evaluation with data from new sensors at a ground, near-surface and airborne level; yet traditional ground-based observations will continue to be highly useful to accurately record the timing of species-specific phenological events. This review might provide a guide for planning and managing remote sensing of forest phenology.}
}
@article{YIN2021100482,
title = {The data-intensive scientific revolution occurring where two-dimensional materials meet machine learning},
journal = {Cell Reports Physical Science},
volume = {2},
number = {7},
pages = {100482},
year = {2021},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2021.100482},
url = {https://www.sciencedirect.com/science/article/pii/S266638642100182X},
author = {Hang Yin and Zhehao Sun and Zhuo Wang and Dawei Tang and Cheng Heng Pang and Xuefeng Yu and Amanda S. Barnard and Haitao Zhao and Zongyou Yin},
keywords = {machine learning, 2D materials, materials preparation, structure analysis, property exploration},
abstract = {Summary
Machine learning (ML) has experienced rapid development in recent years and been widely applied to assist studies in various research areas. Two-dimensional (2D) materials, due to their unique chemical and physical properties, have been receiving increasing attention since the isolation of graphene. The combination of ML and 2D materials science has significantly accelerated the development of new functional 2D materials, and a timely review may inspire further ML-assisted 2D materials development. In this review, we provide a horizontal and vertical summary of the recent advances at the intersection of the fields of ML and 2D materials, discussing ML-assisted 2D materials preparation (design, discovery, and synthesis of 2D materials), atomistic structure analysis (structure identification and formation mechanism), and properties prediction (electronic properties, thermodynamic properties, mechanical properties, and other properties) and revealing their connections. Finally, we highlight current research challenges and provide insight into future research opportunities.}
}
@article{HOFFMANN2021102367,
title = {Improving the evidence base: A methodological review of the quantitative climate migration literature},
journal = {Global Environmental Change},
volume = {71},
pages = {102367},
year = {2021},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2021.102367},
url = {https://www.sciencedirect.com/science/article/pii/S0959378021001461},
author = {Roman Hoffmann and Barbora Šedová and Kira Vinke},
keywords = {Climate migration, Climate change, Systematic review, Meta-analysis, Methodology, Methods},
abstract = {The question whether and how climatic factors influence human migration has gained both academic and public interest in the past years. Based on two meta-analyses, this paper systematically reviews the quantitative empirical literature on climate-related migration from a methodological perspective. In total, information from 127 original micro- and macro-level studies is analyzed to assess how different concepts, research designs, and analytical methods shape our understanding of climate migration. We provide an overview of common methodological approaches and present evidence on their potential implications for the estimation of climatic impacts. We identify five key challenges, which relate to the i) measurement of migration and ii) climatic events, iii) the integration and aggregation of data, iv) the identification of causal relationships, and v) the exploration of contextual influences and mechanisms. Advances in research and modelling are discussed together with best practice cases to provide guidance to researchers studying the climate-migration nexus. We recommend for future empirical studies to employ approaches that are of relevance for and reflect local contexts, ensuring high levels of comparability and transparency.}
}
@article{KOUTSOUDIS20211,
title = {Multispectral aerial imagery-based 3D digitisation, segmentation and annotation of large scale urban areas of significant cultural value},
journal = {Journal of Cultural Heritage},
volume = {49},
pages = {1-9},
year = {2021},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1296207421000650},
author = {Anestis Koutsoudis and George Ioannakis and Petros Pistofidis and Fotis Arnaoutoglou and Nikolaos Kazakis and George Pavlidis and Chistodoulos Chamzas and Nestor Tsirliganis},
keywords = {multispectral, machine learning, 3D digitisation, 3D segmentation, annotation, structure from motion, urban, architecture, disaster management},
abstract = {Disaster risk management of movable and immovable cultural heritage is a highly significant research topic. In this work, we present a pipeline for 3D digitisation, segmentation and annotation of large scale urban areas in order to produce data that can be exploited in disaster management simulators (e.g fire spreading, crowd movement, firefighting training, evacuation planning, etc.). We have selected the old town of Xanthi (Greece) as a challenging case study. We developed a custom multispectral camera to be carried by a commercial drone. Using the structure from motion / multiview stereo (SFM/MVS) approach, we produced a 3D model of the urban area covering 0.5km2 that is followed by a multilayer texture map which carries information from visible and near-infrared regions of the electromagnetic spectrum. We developed a set of machine learning approaches based on logistic regression, support vector machines and artificial neural networks that allow 3D model segmentation by exploiting not only morphological and structural features but also the multispectral behaviour of different material surfaces. We objectively evaluate the performance of the proposed segmentation approaches on six significant material-based classes (cobbled-roads granite kilns, building walls, ceramic roof-tiles, low-vegetation, high-vegetation and metal surfaces) that are used in simulating fire propagation and crowd movement. The experiments revealed that the segmentation accuracy can be enhanced by taking into consideration surface material multispectral properties as well as morphological features. A Web-based multi-user annotation tool complements our proposed pipeline by enabling further 3D model segmentation, fine tuning and semantics annotation (e.g. usage-based building classification and evacuation priorities, escape paths and gathering points).}
}
@article{KRAJSIC202139,
title = {Semi-Supervised Anomaly Detection in Business Process Event Data using Self-Attention based Classification},
journal = {Procedia Computer Science},
volume = {192},
pages = {39-48},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014927},
author = {Philippe Krajsic and Bogdan Franczyk},
keywords = {anomaly detection, business process, classification, event data, process mining},
abstract = {The analysis of business processes has become increasingly important in recent years, not least due to the emergence of analysis tools that enable data-centric views of processes and thus provide increasingly operational support for process flows. In this work, a semi-supervised classification model is presented that takes into account different developments in deep learning (e.g., deep generative models), time series analysis (e.g., long short-term memory) and sequence processing (e.g., attention mechanism) and combines them in one approach. The results of the experimental implementation of the classification model show that it is able to filter activity-related and time-related anomalies from the event data and outperform existing approaches in its classification accuracy (F1 score). The classification model achieves an F1 score of up to 93%.}
}
@article{YU2021111191,
title = {Prioritizing urban planning factors on community energy performance based on GIS-informed building energy modeling},
journal = {Energy and Buildings},
volume = {249},
pages = {111191},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111191},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821004758},
author = {Hang Yu and Meng Wang and Xiaoyu Lin and Haijin Guo and He Liu and Yingru Zhao and Hongxin Wang and Chaoen Li and Rui Jing},
keywords = {Residential community, Energy use, Two-stage clustering process, Urban planning factor, Sensitivity analysis},
abstract = {The residential sector accounts for an increasing amount of global energy use with continued urbanization. Residential energy-informed urban planning offers an economical and easy-to-operate approach to achieve more efficient urban energy utilization. However, quantifying the interactions between residential energy and urban planning remains an open challenge. This study proposes a holistic approach integrating GIS techniques, building energy modeling, and a global sensitivity analysis to prioritize eight key urban planning factors on the community energy performance based on a building energy dataset. The dataset, including urban planning and building information, was first established using GIS techniques and validated using survey data. The residential energy performance model at the community scale was developed using the clustering tree structure of residential building prototypes and building performance simulations. A combined data-driven and global sensitivity analysis approach was further applied to prioritize the impacts of eight vital urban planning factors on energy use intensity and peak load intensity. A case study of 1963 communities in Shanghai revealed that, for the energy performance of residential communities, the floor area ratio and building coverage ratio are the most influential factors, followed by the maximum height and high-rise proportion having a relatively low impact but higher than other factors. Overall, the proposed holistic approach generates robust insights into urban-scale residential energy performance, which can effectively inform urban planners to achieve more energy-efficient regulatory planning.}
}
@article{ZHANG2021116641,
title = {Spatiotemporal wind field prediction based on physics-informed deep learning and LIDAR measurements},
journal = {Applied Energy},
volume = {288},
pages = {116641},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116641},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921001732},
author = {Jincheng Zhang and Xiaowei Zhao},
keywords = {Deep learning, LIDAR measurements, Physics-informed neural networks, Wind field prediction},
abstract = {Spatiotemporal wind field information is of great interest in wind industry e.g. for wind resource assessment and wind turbine/farm monitoring & control. However, its measurement is not feasible because only sparse point measurements are available with the current sensor technology such as LIDAR. This work fills the gap by developing a method that can achieve spatiotemporal wind field predictions by combining LIDAR measurements and flow physics. Specifically, a deep neural network is constructed and the Navier–Stokes equations, which provide a good description of atmospheric flows, are incorporated in the deep neural network by employing the physics-informed deep learning technique. The training of this physics-incorporated deep learning model only requires the sparse LIDAR measurement data while the spatiotemporal wind field in the whole domain (which cannot be measured) can be predicted after training. This study, which can discover complex wind patterns that do not present in the training dataset, is totally distinct from previous machine learning based wind prediction studies which treat machine learning models as “black-box” and require the corresponding input and target values to learn complex relations. The numerical results on the prediction of the wind field in front of a wind turbine show that the proposed method predicts the spatiotemporal flow velocity (including both downwind and crosswind components) in the whole domain very well for a wide range of scenarios (including various measurement noises, resolutions, LIDAR look directions, and turbulence levels), which is promising given that only line-of-sight wind speed measurements at sparse locations are used.}
}
@article{DEKEYSER202152,
title = {Opportunities and challenges of using biometrics for business: Developing a research agenda},
journal = {Journal of Business Research},
volume = {136},
pages = {52-62},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321005014},
author = {Arne {De Keyser} and Yakov Bart and Xian Gu and Stephanie Q. Liu and Stacey G. Robinson and P.K. Kannan},
keywords = {Biometrics, Technology, Ethics, Privacy, Security, AI, Bias},
abstract = {Recently, biometric data generated by fingerprints, hand geometry, heart rate, voice patterns, facial characteristics and expressions, brain activity and body movement has increased in both volume and prominence. Surprisingly, academic business literature has remained relatively silent on the immense potential of biometric data, as well as on the various dangers that come with its collection and usage. This article sets out to (1) detail what biometric data entails and how it may be used, (2) describe opportunities associated with using biometric data in various business applications, (3) discuss challenges related to biometric data collection and usage, privacy and security, storage and safety, and potential for reduced inclusiveness and enhanced biases, and (4) outline related directions for future research.}
}
@article{PRZYBYLAKASPEREK20213560,
title = {Stop Criterion in Building Decision Trees with Bagging Method for Dispersed Data},
journal = {Procedia Computer Science},
volume = {192},
pages = {3560-3569},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.129},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921018688},
author = {Małgorzata Przybyła-Kasperek and Samuel Aning},
keywords = {Ensemble of classifiers, Dispersed data, Stop criterion, Bagging method, Classification trees, Independent data sources},
abstract = {This article discusses issues related to decision making based on applying decision trees and bagging methods on dispersed knowledge. In dispersed knowledge, local decision tables possess data independently in fragments. In this study, sub-tables are further generated with bagging method for each local table, based on which the decision trees are built. These decision trees classify the test object, and a probability vector is defined over the decision classes for each local table. For each vector, decision classes with the maximum value of the coordinates are selected and final joint decisions for all local tables are made by majority voting. Quality of decision making has been observed to increase when bagging method as an ensemble method is combined with decision trees on independent dispersed data. An important criterion in building a decision tree is to know when to stop growing the tree (stop splitting). That is, at what minimum number of objects on a working node do we stop building the tree to ensure the best decision results. The contribution of the paper is to observe the influence a stop criterion (expressed in the number of objects in the node) for decision trees used in conjunction with bagging method on independent data sources. It can be concluded that in dispersed data set, the stop split criteria does not influence the classification quality much. The statistical significance of the difference in the mean classification error values was confirmed only for a very high stop criterion (0.1× number of objects in training set) and for a very low stop criterion (equal to two). There is no significant statistical difference in the classification quality obtained for the stop criterion values: 4, 6, 8 and 10. An interesting remark is that for some dispersed data sets, in the case of smaller number of local tables and larger number of bootstrap samples, better quality of classification is obtained for a small number of objects in the stop criterion (mostly for two objects). Only, at a significant increase in the minimum number of objects at which growth of trees is stopped is quality of classification affected. However, the gain in reducing the complexity for trees that we get when using the larger values of stop criterion is significant.}
}
@article{HALTAS2021102086,
title = {A comprehensive flood event specification and inventory: 1930–2020 Turkey case study},
journal = {International Journal of Disaster Risk Reduction},
volume = {56},
pages = {102086},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102086},
url = {https://www.sciencedirect.com/science/article/pii/S2212420921000522},
author = {Ismail Haltas and Enes Yildirim and Fatih Oztas and Ibrahim Demir},
keywords = {Flooding, Flood event inventory, Flood event specification, Data model},
abstract = {Flooding is one of the most frequent natural disasters that have significant impact on communities in terms of loss of life, direct and indirect economic losses, and disruption of daily life. Decision makers often depend on flood data inventories to make more informed decisions on the development of flood mitigation plans to protect flood prone communities. A comprehensive inventory that covers multiple aspects of a flood event is critical to identify vulnerable regions, historical trends, and mitigate possible flood impacts. This study proposes an integrated flood data specification to support multi-stakeholder use cases, community-based sustainable domain specific maintenance, and crowdsourced data collection and management. The specification is designed based on comprehensive review of existing global and national repositories, scientific studies and needs and requirements of stakeholders. The specification is designed to include metadata on environmental, economic, and demographic impact, hydraulic, hydrologic, and meteorological features, and detailed location information of a flood event. As a case study, a flood event inventory was compiled for Turkey between 1930 and 2020 using existing national and global data sources and digitized media archives. A total of 2101 flood events with 64 data attributes have been collected over the period of 90 years. An initial statistical analysis of the inventory is also presented for assessment of the seasonal and regional characteristics of flooding in Turkey.}
}
@article{WU2021275,
title = {Machine learning-based predictive control using noisy data: evaluating performance and robustness via a large-scale process simulator},
journal = {Chemical Engineering Research and Design},
volume = {168},
pages = {275-287},
year = {2021},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2021.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0263876221000551},
author = {Zhe Wu and Junwei Luo and David Rincon and Panagiotis D. Christofides},
keywords = {Machine learning, Long short-term memory neural networks, Noisy data, Model predictive control, Nonlinear systems, Chemical processes},
abstract = {Machine learning modeling of chemical processes using noisy data is a practically challenging task due to the occurrence of overfitting during learning. In this work, we propose a dropout method and a co-teaching learning algorithm that develop long short-term memory (LSTM) neural networks to capture the ground truth (i.e., underlying process dynamics) from noisy data. To evaluate the performance and robustness of the proposed modeling approaches, we consider an industrial chemical reactor example and use a large-scale process simulator, Aspen Plus Dynamics that does not employ assumptions on reactor properties typically made in the derivation of first-principles models, to generate process operational data that are corrupted by sensor noise which is determined using industrial data. The dropout method is first utilized to reduce the overfitting of LSTM models to noisy data. Then, another approach termed co-teaching method is used to train LSTM models with additional noise-free data generated from simulations of the reactor first-principles model that employs several standard modeling assumptions not made in the Aspen model. Through open-loop and closed-loop simulations, we demonstrate the improvement of model prediction accuracy and of the open- and closed-loop performances under model predictive controllers using dropout and co-teaching LSTM neural network models compared to the LSTM model developed from the standard training process from the noisy data.}
}
@article{ABAIMOV2021100077,
title = {A survey on the application of deep learning for code injection detection},
journal = {Array},
volume = {11},
pages = {100077},
year = {2021},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2021.100077},
url = {https://www.sciencedirect.com/science/article/pii/S2590005621000254},
author = {Stanislav Abaimov and Giuseppe Bianchi},
keywords = {Machine learning, Deep learning, Network intrusion detection, Code injection, Preprocessing},
abstract = {Code injection is one of the top cyber security attack vectors in the modern world. To overcome the limitations of conventional signature-based detection techniques, and to complement them when appropriate, multiple machine learning approaches have been proposed. While analysing these approaches, the surveys focus predominantly on the general intrusion detection, which can be further applied to specific vulnerabilities. In addition, among the machine learning steps, data preprocessing, being highly critical in the data analysis process, appears to be the least researched in the context of Network Intrusion Detection, namely in code injection. The goal of this survey is to fill in the gap through analysing and classifying the existing machine learning techniques applied to the code injection attack detection, with special attention to Deep Learning. Our analysis reveals that the way the input data is preprocessed considerably impacts the performance and attack detection rate. The proposed full preprocessing cycle demonstrates how various machine-learning-based approaches for detection of code injection attacks take advantage of different input data preprocessing techniques. The most used machine learning methods and preprocessing stages have been also identified.}
}
@article{SCHIRMER2021101972,
title = {Neuropsychiatric disease classification using functional connectomics - results of the connectomics in neuroimaging transfer learning challenge},
journal = {Medical Image Analysis},
volume = {70},
pages = {101972},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.101972},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521000189},
author = {Markus D. Schirmer and Archana Venkataraman and Islem Rekik and Minjeong Kim and Stewart H. Mostofsky and Mary Beth Nebel and Keri Rosch and Karen Seymour and Deana Crocetti and Hassna Irzan and Michael Hütel and Sebastien Ourselin and Neil Marlow and Andrew Melbourne and Egor Levchenko and Shuo Zhou and Mwiza Kunda and Haiping Lu and Nicha C. Dvornek and Juntang Zhuang and Gideon Pinto and Sandip Samal and Jennings Zhang and Jorge L. Bernal-Rusiel and Rudolph Pienaar and Ai Wern Chung},
keywords = {Functional connectomics, Disease classification, ADHD, Challenge},
abstract = {Large, open-source datasets, such as the Human Connectome Project and the Autism Brain Imaging Data Exchange, have spurred the development of new and increasingly powerful machine learning approaches for brain connectomics. However, one key question remains: are we capturing biologically relevant and generalizable information about the brain, or are we simply overfitting to the data? To answer this, we organized a scientific challenge, the Connectomics in NeuroImaging Transfer Learning Challenge (CNI-TLC), held in conjunction with MICCAI 2019. CNI-TLC included two classification tasks: (1) diagnosis of Attention-Deficit/Hyperactivity Disorder (ADHD) within a pre-adolescent cohort; and (2) transference of the ADHD model to a related cohort of Autism Spectrum Disorder (ASD) patients with an ADHD comorbidity. In total, 240 resting-state fMRI (rsfMRI) time series averaged according to three standard parcellation atlases, along with clinical diagnosis, were released for training and validation (120 neurotypical controls and 120 ADHD). We also provided Challenge participants with demographic information of age, sex, IQ, and handedness. The second set of 100 subjects (50 neurotypical controls, 25 ADHD, and 25 ASD with ADHD comorbidity) was used for testing. Classification methodologies were submitted in a standardized format as containerized Docker images through ChRIS, an open-source image analysis platform. Utilizing an inclusive approach, we ranked the methods based on 16 metrics: accuracy, area under the curve, F1-score, false discovery rate, false negative rate, false omission rate, false positive rate, geometric mean, informedness, markedness, Matthew’s correlation coefficient, negative predictive value, optimized precision, precision, sensitivity, and specificity. The final rank was calculated using the rank product for each participant across all measures. Furthermore, we assessed the calibration curves of each methodology. Five participants submitted their method for evaluation, with one outperforming all other methods in both ADHD and ASD classification. However, further improvements are still needed to reach the clinical translation of functional connectomics. We have kept the CNI-TLC open as a publicly available resource for developing and validating new classification methodologies in the field of connectomics.}
}
@article{SHRESTHA2021588,
title = {Augmenting organizational decision-making with deep learning algorithms: Principles, promises, and challenges},
journal = {Journal of Business Research},
volume = {123},
pages = {588-603},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306512},
author = {Yash Raj Shrestha and Vaibhav Krishna and Georg {von Krogh}},
keywords = {Case studies, Decision-making, Deep learning, Artificial intelligence},
abstract = {The current expansion of theory and research on artificial intelligence in management and organization studies has revitalized the theory and research on decision-making in organizations. In particular, recent advances in deep learning (DL) algorithms promise benefits for decision-making within organizations, such as assisting employees with information processing, thereby augment their analytical capabilities and perhaps help their transition to more creative work. We conceptualize the decision-making process in organizations augmented with DL algorithm outcomes (such as predictions or robust patterns from unstructured data) as deep learning–augmented decision-making (DLADM). We contribute to the understanding and application of DL for decision-making in organizations by (a) providing an accessible tutorial on DL algorithms and (b) illustrating DLADM with two case studies drawing on image recognition and sentiment analysis tasks performed on datasets from Zalando, a European e-commerce firm, and Rotten Tomatoes, a review aggregation website for movies, respectively. Finally, promises and challenges of DLADM as well as recommendations for managers in attending to these challenges are also discussed.}
}
@article{HASAN2021100799,
title = {Missing value imputation affects the performance of machine learning: A review and analysis of the literature (2010–2021)},
journal = {Informatics in Medicine Unlocked},
volume = {27},
pages = {100799},
year = {2021},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2021.100799},
url = {https://www.sciencedirect.com/science/article/pii/S2352914821002653},
author = {Md. Kamrul Hasan and Md. Ashraful Alam and Shidhartho Roy and Aishwariya Dutta and Md. Tasnim Jawad and Sunanda Das},
keywords = {Incomplete datasets, Imputation methods and evaluations, Machine learning classifiers and evaluations, PRISMA technique},
abstract = {Recently, numerous studies have been conducted on Missing Value Imputation (MVI), intending the primary solution scheme for the datasets containing one or more missing attribute’s values. The incorporation of MVI reinforces the Machine Learning (ML) models’ performance and necessitates a systematic review of MVI methodologies employed for different tasks and datasets. It will aid beginners as guidance towards composing an effective ML-based decision-making system in various fields of applications. This article aims to conduct a rigorous review and analysis of the state-of-the-art MVI methods in the literature published in the last decade. Altogether, 191 articles, published from 2010 to August 2021, are selected for review using the well-known Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) technique. We summarize those articles with relevant definitions, theories, and analyses to provide essential information for building a precise decision-making framework. In addition, the evaluation metrics employed for MVI methods and ML-based classification models are also discussed and explored. Remarkably, the trends for the MVI method and its evaluation are also scrutinized from the last twelve years’ data. To come up with the conclusion, several ML-based pipelines, where the MVI schemes are incorporated for performance enhancement, are investigated and reviewed for many different datasets. In the end, informative observations and recommendations are addressed for future research directions and trends in related fields of interest.}
}
@article{DIVAN2021106871,
title = {Metadata-based measurements transmission verified by a Merkle Tree},
journal = {Knowledge-Based Systems},
volume = {219},
pages = {106871},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106871},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001349},
author = {Mario José Diván and María Laura Sánchez-Reynoso},
keywords = {Measurement, Metadata-guided transmission, Brief data message, Integrity record, Data streaming},
abstract = {The Data Stream Processing Strategy (DSPS) is focused on the automatization of measurement projects based on a measurement framework. The measurement adapter (MA) is an architecture component located on mobile devices aims to integrate heterogeneous data sources (i.e., sensors). The Gathering Function (GF) is the component responsible for interacting and receiving measures from the MAs, and it resides on the Stream Processing Engine (SPE). MA and GF share the project definition based on a measurement framework to foster data interoperability, while MA regulates the frequency, size, and route related to data transmission. As contributions (i) The brief data message is introduced to optimize the data transmission keeping immutable the hierarchical data organization based on the project definition, and (ii) The integrity record for mobile and SPE environments is described based on a Merkle Tree. This allows optimizing each data transaction, incorporating a historical integrity record both MA and SPE. The proposals and simulations have been implemented on the cincamimis, cincamipd, mair, and pabmmcommons libraries, which are freely available on GitHub under the terms of the Apache 2.0 licence. Four simulations are explained to detail how to measures were obtained. Interesting results show that the brief data message consumes 17.50 KB to transmit 1000 measures (2.4 times smaller than JSON), while a message with 200 measures could be generated and compressed using GZIP in 25.12 ms (2.43 times faster than JSON). 196 KB is required to keep 17 min of the integrity history in a MA, being created in 4.85 ms.}
}
@article{CHEN2021103226,
title = {Scalable low-rank tensor learning for spatiotemporal traffic data imputation},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {129},
pages = {103226},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103226},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21002400},
author = {Xinyu Chen and Yixian Chen and Nicolas Saunier and Lijun Sun},
keywords = {Spatiotemporal traffic data, High-dimensional data, Missing data imputation, Low-rank tensor completion, Linear unitary transformation, Quadratic variation},
abstract = {Missing value problem in spatiotemporal traffic data has long been a challenging topic, in particular for large-scale and high-dimensional data with complex missing mechanisms and diverse degrees of missingness. Recent studies based on tensor nuclear norm have demonstrated the superiority of tensor learning in imputation tasks by effectively characterizing the complex correlations/dependencies in spatiotemporal data. However, despite the promising results, these approaches do not scale well to large data tensors. In this paper, we focus on addressing the missing data imputation problem for large-scale spatiotemporal traffic data. To achieve both high accuracy and efficiency, we develop a scalable tensor learning model—Low-Tubal-Rank Smoothing Tensor Completion (LSTC-Tubal)—based on the existing framework of Low-Rank Tensor Completion, which is well-suited for spatiotemporal traffic data that is characterized by multidimensional structure of location × time of day × day. In particular, the proposed LSTC-Tubal model involves a scalable tensor nuclear norm minimization scheme by integrating linear unitary transformation. Therefore, tensor nuclear norm minimization can be solved by singular value thresholding on the transformed matrix of each day while the day-to-day correlation can be effectively preserved by the unitary transform matrix. Before setting up the experiment, we consider some real-world data sets, including two large-scale 5-min traffic speed data sets collected by the California PeMS system with 11160 sensors: 1) PeMS-4W covers the data over 4 weeks (i.e., 288×28 time points), and 2) PeMS-8W covers the data over 8 weeks (i.e., 288×56 time points). We compare LSTC-Tubal with some state-of-the-art baseline models, and find that LSTC-Tubal can achieve competitively accuracy with a significantly lower computational cost. In addition, the LSTC-Tubal will also benefit other tasks in modeling large-scale spatiotemporal traffic data, such as network-level traffic forecasting.}
}
@article{MELLIT2021110889,
title = {Artificial intelligence and internet of things to improve efficacy of diagnosis and remote sensing of solar photovoltaic systems: Challenges, recommendations and future directions},
journal = {Renewable and Sustainable Energy Reviews},
volume = {143},
pages = {110889},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.110889},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121001830},
author = {Adel Mellit and Soteris Kalogirou},
keywords = {Deep learning, Fault detection and diagnosis, Internet of things, Machine learning, Photovoltaic systems, Remote sensing, Smart monitoring},
abstract = {Currently, a huge number of photovoltaic plants have been installed worldwide and these plants should be carefully protected and supervised continually in order to be safe and reliable during their working lifetime. Photovoltaic plants are subject to different types of faults and failures, while available fault detection equipment are mainly used to protect and isolate the photovoltaic plants from some faults (such as arc fault, line-to-line, line-to-ground and ground faults). Although a good number of international standards (IEC, NEC, and UL) exists, undetectable faults continue to create serious problems in photovoltaic plants. Thus, designing smart equipment, including artificial intelligence and internet of things for remote sensing and fault detection and diagnosis of photovoltaic plants, will considerably solve the shortcomings of existing methods and commercialized equipment. This paper presents an overview of artificial intelligence and internet of things applications in photovoltaic plants. This research presents also the most advanced algorithms such as machine and deep learning, in terms of cost implementation, complexity, accuracy, software suitability, and feasibility of real-time applications. The embedding of artificial intelligence and internet of things techniques for fault detection and diagnosis into simple hardware, such as low-cost chips, may be economical and technically feasible for photovoltaic plants located in remote areas, with costly and challenging accessibility for maintenance. Challenging issues, recommendations, and trends of these techniques will also be presented in this paper.}
}
@article{MARQUES2021100037,
title = {Policy report on FinTech data gaps},
journal = {Latin American Journal of Central Banking},
volume = {2},
number = {3},
pages = {100037},
year = {2021},
issn = {2666-1438},
doi = {https://doi.org/10.1016/j.latcb.2021.100037},
url = {https://www.sciencedirect.com/science/article/pii/S266614382100017X},
author = {José Manuel Marqués and Fernando Ávila and Anahí Rodríguez-Martínez and Raúl Morales-Reséndiz and Antonio Marcos and Tamara Godoy and Pablo Villalobos and Andrea Ocontrillo and Valerie Ann Lankester and Clemente Blanco and Karla Reyes and Silvia Irina Lopez and Ana Fernández and Román Santos and Luis Ángel Maza and Manuel Sánchez and Carlos Domínguez and Natalie Haynes and Novelette Panton and Mario Griffiths and Kurt Murray and Michelle Doyle-Lowe and Leslie Ann {Des Vignes} and Michelle Francis-Pantor},
abstract = {This document aims to provide an overview of the main issues related to data gaps to facilitate monitoring of FinTech and overcome the significant challenges towards incorporating FinTech activities in regular statistics. Moreover, the document explains the implications of data gaps on some of the Central Banks’ main areas, in particular, monetary policy, financial stability, payment systems, and economic activity. Additionally, other implications related to the activity of BigTech companies, the impact of COVID-19 and Cybersecurity issues are explained, which represent an important challenge for data gathering at Central Banks. Also, it describes the main findings of the Irving Fisher Committee (IFC) survey “Central Banks and FinTech data” based on the answers provided by Latin American and Caribbean (LAC) countries, which identify their different positions regarding this topic and the current initiatives that each one is launching. Finally, a number of next steps are proposed based on a policy discussion and how LAC countries could overcome data gaps and improve data collection based on their current experience.}
}
@article{BECK2021117441,
title = {White matter microstructure across the adult lifespan: A mixed longitudinal and cross-sectional study using advanced diffusion models and brain-age prediction},
journal = {NeuroImage},
volume = {224},
pages = {117441},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117441},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920309265},
author = {Dani Beck and Ann-Marie G. {de Lange} and Ivan I. Maximov and Geneviève Richard and Ole A. Andreassen and Jan E. Nordvik and Lars T. Westlye},
keywords = {Ageing, White matter, Multi-shell, Longitudinal,, Diffusion, Brain age},
abstract = {The macro- and microstructural architecture of human brain white matter undergoes substantial alterations throughout development and ageing. Most of our understanding of the spatial and temporal characteristics of these lifespan adaptations come from magnetic resonance imaging (MRI), including diffusion MRI (dMRI), which enables visualisation and quantification of brain white matter with unprecedented sensitivity and detail. However, with some notable exceptions, previous studies have relied on cross-sectional designs, limited age ranges, and diffusion tensor imaging (DTI) based on conventional single-shell dMRI. In this mixed cross-sectional and longitudinal study (mean interval: 15.2 months) including 702 multi-shell dMRI datasets, we combined complementary dMRI models to investigate age trajectories in healthy individuals aged 18 to 94 years (57.12% women). Using linear mixed effect models and machine learning based brain age prediction, we assessed the age-dependence of diffusion metrics, and compared the age prediction accuracy of six different diffusion models, including diffusion tensor (DTI) and kurtosis imaging (DKI), neurite orientation dispersion and density imaging (NODDI), restriction spectrum imaging (RSI), spherical mean technique multi-compartment (SMT-mc), and white matter tract integrity (WMTI). The results showed that the age slopes for conventional DTI metrics (fractional anisotropy [FA], mean diffusivity [MD], axial diffusivity [AD], radial diffusivity [RD]) were largely consistent with previous research, and that the highest performing advanced dMRI models showed comparable age prediction accuracy to conventional DTI. Linear mixed effects models and Wilk's theorem analysis showed that the ‘FA fine’ metric of the RSI model and ‘orientation dispersion’ (OD) metric of the NODDI model showed the highest sensitivity to age. The results indicate that advanced diffusion models (DKI, NODDI, RSI, SMT mc, WMTI) provide sensitive measures of age-related microstructural changes of white matter in the brain that complement and extend the contribution of conventional DTI.}
}
@article{VANEM2021103158,
title = {Data-driven state of health modelling—A review of state of the art and reflections on applications for maritime battery systems},
journal = {Journal of Energy Storage},
volume = {43},
pages = {103158},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.103158},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21008598},
author = {Erik Vanem and Clara Bertinelli Salucci and Azzeddine Bakdi and Øystein Å sheim Alnes},
keywords = {Battery state of health, Degradation modelling, Capacity, Maritime battery systems, Data-driven modelling},
abstract = {Battery systems are becoming an increasingly attractive alternative for powering ocean going ships, and the number of fully electric or hybrid ships relying on battery power for propulsion and manoeuvring is growing. In order to ensure the safety of such electric ships, it is of paramount importance to monitor the available energy that can be stored in the batteries, and classification societies typically require that the state of health of the batteries can be verified by independent tests — annual capacity tests. However, this paper discusses data-driven state of health modelling for maritime battery systems based on operational sensor data collected from the batteries as an alternative approach. Thus, this paper presents a comprehensive review of different data-driven approaches to state of health modelling, and aims at giving an overview of current state of the art. More than 300 papers have been reviewed, most of which are referred to in this paper. Moreover, some reflections and discussions on what types of approaches can be suitable for modelling and independent verification of state of health for maritime battery systems are presented.}
}
@article{NIU2021102276,
title = {Incentive alignment for blockchain adoption in medicine supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {152},
pages = {102276},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2021.102276},
url = {https://www.sciencedirect.com/science/article/pii/S136655452100051X},
author = {Baozhuang Niu and Jian Dong and Yaoqi Liu},
keywords = {Blockchain technology, Social goods, Supply chain competition, Information sharing},
abstract = {In recent years, blockchain technology has been increasingly adopted in OTC medicine supply chains, enabling customers to track the entire process from raw material purchasing to finished medicine distribution. This improves the brand image and hence expands the market. With the use of blockchain, information transparency can be achieved because data are stored immutably and safely in a distributed database that is accessible by all supply chain members. However, will the incentives for supply chain members to participate in blockchain for larger-scale demand come at the cost of information disclosure? In this paper, to investigate the supply chain members’ incentive alignment opportunities towards the adoption of blockchain technology, we consider a two-stage supply chain comprising two medicine manufacturers and a common retailer that has more accurate demand information than the manufacturers have. We find that, interestingly, the retailer has incentives to participate in blockchain when the manufacturers’ competition is mild and the demand variance is low. We further investigate the impact of blockchain on total surplus and customer surplus and find that the adoption of blockchain always benefits customers and society; therefore, blockchain can be particularly useful for social goods such as OTC medicine.}
}
@article{ZHANG2021128801,
title = {Satellite-based ground PM2.5 estimation using a gradient boosting decision tree},
journal = {Chemosphere},
volume = {268},
pages = {128801},
year = {2021},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2020.128801},
url = {https://www.sciencedirect.com/science/article/pii/S0045653520329994},
author = {Tianning Zhang and Weihuan He and Hui Zheng and Yaoping Cui and Hongquan Song and Shenglei Fu},
keywords = {Aerosol optical depth, Air pollution, Machine learning, MODIS, Particulate matter},
abstract = {Fine particulate matter with an aerodynamic diameter less than 2.5 μm (PM2.5) is one of the major air pollutants risks to human health worldwide. Satellite-based aerosol optical depth (AOD) products are an effective metric for acquiring PM2.5 information, featuring broad coverage and high resolution, which compensate for the sparse and uneven distribution of existing monitoring stations. In this study, a gradient boosting decision tree (GBDT) model for estimating ground PM2.5 concentration directly from AOD products across China in 2017, integrating human activities and various natural variables was proposed. The GBDT model performed well in estimating temporal variability and spatial contrasts in daily PM2.5 concentrations, with relatively high fitted model (10-fold cross-validation) coefficients of determination of 0.98 (0.81), low root mean square errors of 3.82 (11.57) μg/m3, and mean absolute error of 1.44 (7.45) μg/m3. Seasonal examinations revealed that summer had the cleanest air with the highest estimation accuracies, whereas winter had the most polluted air with the lowest estimation accuracies. The model successfully captured the PM2.5 distribution pattern across China in 2017, showing high levels in southwest Xinjiang, the North China Plain, and the Sichuan Basin, especially in winter. Compared with other models, the GBDT model showed the highest performance in the estimation of PM2.5 with a 3-km resolution. This algorithm can be adopted to improve the accuracy of PM2.5 estimation with higher spatial resolution, especially in summer. In general, this study provided a potential method of improving the accuracy of satellite-based ground PM2.5 estimation.}
}
@article{SCHREIBER2021120894,
title = {Application of data-driven methods for energy system modelling demonstrated on an adaptive cooling supply system},
journal = {Energy},
volume = {230},
pages = {120894},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120894},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221011427},
author = {Thomas Schreiber and Christoph Netsch and Sören Eschweiler and Tianyuan Wang and Thomas Storek and Marc Baranski and Dirk Müller},
keywords = {Machine-learning, Supervised learning, Building automation and control, Data-driven modelling, Optimal control},
abstract = {The efficient and sustainable operation of building energy systems is playing an increasingly important role in most industrialized countries. At the same time, building energy systems are becoming increasingly complex; fault-free and optimal operation, under dynamic boundary conditions, is becoming more and more challenging. There are many approaches in research to address the optimal control problem of building energy systems, such as Rule-based Control, Model Predictive Control, or Adaptive Control. However, most methods rely on models of the system dynamics with high prediction accuracies. This is especially the case in Model Predictive Control, where the model is part of a continuously executed optimization problem; but models are also required when it comes to the optimal design of Rule-based Controllers, the safe pre-training of Adaptive Controllers, or model-based fault detection. A limiting factor for the manual development of physical models, for building energy systems, are the low monetary incentives for engineering services, due to the low energy prices in most countries. In addition, the creation of such models is time-consuming and error-prone, even for domain experts. Another weakness is that changes in the system dynamics are not automatically adapted within the models. These challenges are contrasted by an increasing availability of monitoring-data and computational power in recent years; with machine-learning algorithms, these resources are used in numerous application areas to achieve very promising results. Machine-learning methods can help to obtain data-driven, self-calibrating models, which can be learned from monitoring-data. In this paper, we apply methods for automated data-driven model generation. We demonstrate how machine-learning algorithms together with structured hyper-parameter tuning can be used to model individual subsystems as well as a complete energy supply system. To represent the dynamics of the supply system, it is first decomposed into simple functional relationships, which are aggregated into the overall system after training of the comparatively simple subsystem models. We evaluate the accuracy of the data-driven subsystem models using established metrics for the evaluation of regression models, namely the R2-score and the RMSE. The considered system is integrated into a district cooling network and consists of two compression chillers and an ice storage unit. Our investigations show that the dynamics of the subsystems can be learned with high accuracies, depending on the operation mode and the selected features. The prediction of the power demand of the compression chillers is learned with R2-scores between 0.94 and 0.99 and RMSE values between 2.02 kW and 3.51 kW. Also, the prediction of the percentage of ice formation within the ice storage is learned accurately with a R2-score of 1 and RMSE values between 0.08 % and 0.72 %. The dynamics of the aggregated system also show plausible behavior and can thus be used in future work. This work is part of an ongoing research project with the aim to optimize the operation of the entire campus cooling energy supply system. Our results show that, if detailed monitoring-data are available, data-driven modelling represents a viable alternative to the labor-intensive physical modelling approach. Furthermore, we emphasize the importance of structured hyper-parameter tuning, discuss the specifics of different machine-learning algorithms, and elaborate on possible future developments in this research area.}
}
@article{BOSSA2021100190,
title = {FAIRification of nanosafety data to improve applicability of (Q)SAR approaches: A case study on in vitro Comet assay genotoxicity data},
journal = {Computational Toxicology},
volume = {20},
pages = {100190},
year = {2021},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100190},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000384},
author = {Cecilia Bossa and Cristina Andreoli and Martine Bakker and Flavia Barone and Isabella {De Angelis} and Nina Jeliazkova and Penny Nymark and Chiara Laura Battistelli},
keywords = {Nanomaterials, FAIR principles, (Q)SAR approaches, Nanosafety data, Genotoxicity,  Comet assay},
abstract = {(Quantitative) structure-activity relationship ([Q]SAR) methodologies are widely applied to predict the (eco)toxicological effects of chemicals, and their use is envisaged in different regulatory frameworks for filling data gaps of untested substances. However, their application to the risk assessment of nanomaterials is still limited, also due to the scarcity of large and curated experimental datasets. Despite a great amount of nanosafety data having been produced over the last decade in international collaborative initiatives, their interpretation, integration and reuse has been hampered by several obstacles, such as poorly described (meta)data, non-standard terminology, lack of harmonized reporting formats and criteria. Recently, the FAIR (Findable, Accessible, Interoperable, and Reusable) principles have been established to guide the scientific community in good data management and stewardship. The EU H2020 Gov4Nano project, together with other international projects and initiatives, is addressing the challenge of improving nanosafety data FAIRness, for maximizing their availability, understanding, exchange and ultimately their reuse. These efforts are largely supported by the creation of a common Nanosafety Data Interface, which connects a row of project-specific databases applying the eNanoMapper data model. A wide variety of experimental data relating to characterization and effects of nanomaterials are stored in the database; however, the methods, protocols and parameters driving their generation are not fully mature. This article reports the progress of an ongoing case study in the Gov4nano project on the reuse of in vitro Comet genotoxicity data, focusing on the issues and challenges encountered in their FAIRification through the eNanoMapper data model. The case study is part of an iterative process in which the FAIRification of data supports the understanding of the phenomena underlying their generation and, ultimately, improves their reusability.}
}
@article{RANJAN2021960,
title = {Ocular artifact elimination from electroencephalography signals: A systematic review},
journal = {Biocybernetics and Biomedical Engineering},
volume = {41},
number = {3},
pages = {960-996},
year = {2021},
issn = {0208-5216},
doi = {https://doi.org/10.1016/j.bbe.2021.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0208521621000838},
author = {Rakesh Ranjan and Bikash {Chandra Sahana} and Ashish {Kumar Bhandari}},
keywords = {EEG signal, Ocular artifact, Artifact removal techniques, Hybrid method, Performance evaluation, Brain-computer interface},
abstract = {Electroencephalography (EEG) is the signal of intrigue that has immense application in the clinical diagnosis of various neurological, psychiatric, psychological, psychophysiological, and neurocognitive disorders. It is significantly crucial in neural communication, brain-computer interface, and other practical tasks. EEG signal is exceptionally susceptible to artifacts, which are external noise signals originated from non-cerebral regions. The interference of artifacts in EEG signals can potentially affect the original recorded EEG signal quality and pattern. Therefore, artifact removal from EEG signal is critically important before applying it to a specific task for accurate outcomes. Researchers have proposed numerous techniques to remove various artifacts present in the contaminated EEG signal. However, neither optimum method nor criterion stands standard for endorsement of clinically recorded EEG signals. Therefore, the research related to artifact elimination from EEG signal is challenging and perplexing task. This paper attempts to give an extensive outline of the advancement in methodologies to eliminate one of the most common artifacts, i.e., ocular artifact. It is anticipated that the study will enlighten the researchers on all the existing ocular artifact elimination techniques with a validated simulation model on the recorded EEG signal. In future advancements, Standard norms in artifact elimination techniques are expected to diminish the neurologist's load by substantiating the clinical diagnosis after gaining correct information from artifact-free EEG signals.}
}
@article{DAO2021103852,
title = {Semantic framework for interdependent infrastructure resilience decision support},
journal = {Automation in Construction},
volume = {130},
pages = {103852},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103852},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003034},
author = {Jicao Dao and S. Thomas Ng and Yifan Yang and Shenghua Zhou and Frank J. Xu and Martin Skitmore},
keywords = {Semantic Web, Ontology, Infrastructure systems, Interdependency, Data integration, Resilient decisions},
abstract = {The increasing need for interdependent infrastructure systems to withstand natural disasters has called for the co-creation of resilience decisions to minimize the impact on society. However, issues related to information integration across different infrastructure systems hamper decision making from a system-to-systems perspective. To resolve this problem, the Semantic Web technologies are presented in this paper to serve four functions: (i) linking cross domains through ontology development to represent different domain knowledge; (ii) integrating multiple-source heterogeneous data by a common data format; (iii) retrieving useful information using semantic query language; and (iv) deriving machine automatic logical reasoning by rule languages and logic engines to provide informed resilience decision making support. The proposed framework is tested by a case scenario involving intertwined drainage-transport-building systems under the influence of urban flooding. The result indicates that the framework effectively facilitates information integration between diverse infrastructure systems and helps decision-makers by providing resilience decision-making support.}
}
@incollection{2021361,
title = {Index},
editor = {Guy J-P. Schumann},
booktitle = {Earth Observation for Flood Applications},
publisher = {Elsevier},
pages = {361-370},
year = {2021},
series = {Earth Observation},
isbn = {978-0-12-819412-6},
doi = {https://doi.org/10.1016/B978-0-12-819412-6.00021-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194126000213}
}
@article{GRUNZE2021178,
title = {“Apples and pears are similar, but still different things.” Bipolar disorder and schizophrenia- discrete disorders or just dimensions ?},
journal = {Journal of Affective Disorders},
volume = {290},
pages = {178-187},
year = {2021},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2021.04.064},
url = {https://www.sciencedirect.com/science/article/pii/S0165032721003967},
author = {Heinz Grunze and Marcelo Cetkovich-Bakmas},
keywords = {Bipolar disorder, Schizophrenia, Dichotomy, Phenomenology, Neurobiology, Computational psychiatry},
abstract = {Starting with the dichotomous view of Kraepelin, schizophrenia and bipolar disorder have traditionally been considered as separate entities. More recent, this taxonomic view of illnesses has been challenged and a continuum psychosis has been postulated based on genetic and neurobiological findings suggestive of a large overlap between disorders. In this paper we will review clinical and experimental data from genetics, morphology, phenomenology and illness progression demonstrating what makes schizophrenia and bipolar disorder different conditions, challenging the idea of the obsolescence of the categorical approach. However, perhaps it is also time to move beyond DSM and search for more refined clinical descriptions that could uncover clinical invariants matching better with molecular data. In the future, computational psychiatry employing artificial intelligence and machine learning might provide us a tool to overcome the gap between clinical descriptions (phenomenology) and neurobiology.}
}
@article{LI2021105093,
title = {An overview of scientometric mapping for the safety science community: Methods, tools, and framework},
journal = {Safety Science},
volume = {134},
pages = {105093},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.105093},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520304902},
author = {Jie Li and Floris Goerlandt and Genserik Reniers},
keywords = {Scientometrics, Bibliometrics, Safety science, Mapping knowledge domains, Science mapping},
abstract = {Scientometrics analysis is increasingly applied across scientific domains to gain quantitative insights in the development of research on particular (sub-)domains of scientific inquiry. By visualizing metrics containing quantitative information about such a domain, scientometric mapping allows researchers to gain insights in aspects thereof. Methods have been developed to answer specific research questions, focusing e.g. on collaboration networks, thematic research clusters, historic evolution patterns, and trends in topics addressed. Several articles applying scientometric mapping to safety-related topics have been published. In context of the Special Issue ‘Mapping Safety Science – Reviewing Safety Research’, this article first reviews these, and subsequently provides an overview of key concepts, methods, and tools for scientometric mapping. Data sources and freely available tools are introduced, focusing on which research questions these are suited to answer. A brief tutorial-style description of a scientometrics research process is provided, guiding researchers new to this method how to engage with it. Finally, a discussion on best practices in scientometric mapping research is made, focusing on how to obtain reliable and valid results, and how to use the scientometric maps to gain meaningful insights. It is hoped that this work can advance the application of scientometric research within the safety science community.}
}
@article{DUPLESSIS2021116395,
title = {Short-term solar power forecasting: Investigating the ability of deep learning models to capture low-level utility-scale Photovoltaic system behaviour},
journal = {Applied Energy},
volume = {285},
pages = {116395},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116395},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920317657},
author = {A.A. {du Plessis} and J.M. Strauss and A.J. Rix},
keywords = {Short-term power forecasting, Machine learning, Deep learning, Photovoltaic, Long Short-Term Memory, Gated Recurrent Unit},
abstract = {Photovoltaic (PV) system power supply is characteristically intermittent. Therefore, PV forecasting is crucial for decision makers responsible for electrical grid stability. With forecast models traditionally trained as macro-level solutions, where a single model emulates the entire PV system, there is uncertainty regarding the ability of these macro-level models to capture the low-level power output dynamics of large multi-megawatt PV systems. Instead, an aggregated inverter-level forecasting methodology is proposed to obtain an enhanced forecasting accuracy. These macro-level and inverter-level forecasting methodologies are implemented with state-of-the-art deep learning based Feedforward neural network, Long Short-Term Memory and Gated Recurrent Unit recurrent neural network models. Results are generated for a real-world scenario, with multi-step forecasts delivered 1–6 h ahead for a 75 MW rated PV system. To ensure the scalability of the proposed methodology, a unique inverter-clustering technique is presented, which reduces the effort of optimising multiple low-level forecast models. A heuristic process of systematic hyperparameter optimisation is also proposed, which serves to guide future forecasting practitioners towards unbiased model development. From the deterministic and probabilistic confidence interval evaluations, overall results demonstrate a marginal increase in forecasting accuracy from the proposed aggregated inverter-level forecasts. The best performing macro-level model obtained Mean Absolute Percentage Error (MAPE) values ranging between 1.42%–8.13% for all weather types and forecast horisons. In comparison, the equivalent inverter-level forecasts delivered MAPE values ranging from 1.27%–8.29%. Finally, it is concluded that deep learning based macro-level forecast models have a sufficient ability to capture low-level PV system behaviour.}
}
@article{YU2021129386,
title = {Quantification and management of urban traffic emissions based on individual vehicle data},
journal = {Journal of Cleaner Production},
volume = {328},
pages = {129386},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129386},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621035708},
author = {Zhi Yu and Weichi Li and Yonghong Liu and Xuelan Zeng and Yongming Zhao and Kaiying Chen and Bin Zou and Jiajun He},
keywords = {Individual vehicle emissions, Emission quantification, Licence plate recognition data, Spatiotemporal emission characteristics, Traffic emission reduction policy},
abstract = {Urban traffic pollution poses a serious threat to the environment and human health, especially in urban centres with high population density. Traditional traffic pollution quantification and management methods can be improved based on fine-grained individual vehicle data provided by intelligent transportation systems. Traditional traffic emission quantification and management are often based on simulated or relatively coarse-grained measured data. Such data lack a comprehensive reflection of the actual conditions of all vehicles travelling on roads, which leads to deviations in emission quantification; thus, they cannot support the delicate control policy of traffic pollution. This paper presents a high-resolution individual vehicle emission quantification method based on real-time, real-world individual vehicle data, with a combination of automatic licence plate recognition data and vehicle registration data currently used for traffic management. In this study, we quantified the emissions of each vehicle driving in the urban centre of the case city and analysed regional traffic emission characteristics. We found that there was an apparent uneven distribution of vehicle emissions; that is, the emissions from a small number of high-emission vehicles accounted for a large proportion of the regional traffic emissions. Different pollutants and vehicle types had different emission distribution characteristics. Furthermore, we explored emission reduction policies based on the management of high-emission vehicles identified by individual vehicle data and conduct fine-scale analysis of the link-level hourly emission reduction effects. In addition, a comparison between traditional methods and the method used in this paper for emission quantification was performed. This paper provides a basis for the accurate analysis of regional traffic emission characteristics, individual-based emission reduction policy formulation, and refined policy effect analysis, which has great significance for the control of traffic pollution.}
}
@incollection{AWASTHI2021455,
title = {20 - Metagenomics: A computational approach in emergence of novel applications},
editor = {Maulin P. Shah and Susana Rodriguez-Couto},
booktitle = {Wastewater Treatment Reactors},
publisher = {Elsevier},
pages = {455-482},
year = {2021},
isbn = {978-0-12-823991-9},
doi = {https://doi.org/10.1016/B978-0-12-823991-9.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239919000046},
author = {Shruti Awasthi and Shubha Dwivedi and Naveen Dwivedi},
keywords = {Metagenomics, sequencing, computational approach, environment, ecosystem, statistical approaches},
abstract = {Metagenomics is a one of the fastest developing fields of research nowadays. It is a molecular tool used to study the genetic material obtained from environmental samples without culturing. Through culture-independent genomic sequencing, the metagenomics approach offers an exceptional analysis of the various microbial worlds in different environments, such as soil, air, ocean, and other water bodies, human and animal body sites, and many others. In the study of sequencing, metagenomics allows researcher to determine directly the collection of genes which are present in an environmental sample that are totally unknown organisms. It has been difficult to identify them, but now they can be analyzed by conducting biochemical tests and focusing on how they interact in the environment. Metagenomics data are growing at a fast pace so we need new infrastructure, methods, and technology so that this huge data can be analyzed and predicted; overcoming this problem with a computational approach can be a boon. A large volume of data could be managed by using a computational environment. This method collects, processes, and extracts useful biological information from samples and complex datasets, but genomics sequences need the integration of these computational methods. This chapter aims to focus on the detailed study of novel computational approach in metagenomics data analysis and the wide applications of sequencing methods.}
}
@article{RENGARAJAN2021120560,
title = {Strategy tools in dynamic environments – An expert-panel study},
journal = {Technological Forecasting and Social Change},
volume = {165},
pages = {120560},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120560},
url = {https://www.sciencedirect.com/science/article/pii/S004016252031386X},
author = {Srinath Rengarajan and Roger Moser and Gopalakrishnan Narayanamurthy},
keywords = {Strategy tools, Strategy frameworks, Dynamic environments, Information processing, Contextual fit},
abstract = {Strategy tools and frameworks are crucial for managers to navigate their business environment and formulate strategies. Extant research has focused on the characteristics, dimensions, applications, and impact of traditional tools. However, there are questions regarding the suitability of these tools to the increasingly dynamic environments faced by strategy practitioners characterized by blurring industry boundaries, uncertainty, and ambiguity. Using an expert-panel approach, we address this research gap by investigating how strategy experts from practice and academia assess established strategy tools in dynamic environment. We identify the characteristics of strategy tools that experts value in such contexts and which can inform future development of context-specific strategy tools. Additionally, we also investigate why experts select and apply specific tools and how they combine these tools. Our findings further allow us to explore the difference in perspectives of strategy scholars and practitioners, which is necessary to reconcile the gap between strategy theory and practice. Finally, we discuss implications of the study for strategy and management research, education, and practice.}
}