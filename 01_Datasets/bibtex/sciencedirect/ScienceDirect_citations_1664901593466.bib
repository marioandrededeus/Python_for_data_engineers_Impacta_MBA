@article{KJAERGAARD2020106848,
title = {Current practices and infrastructure for open data based research on occupant-centric design and operation of buildings},
journal = {Building and Environment},
volume = {177},
pages = {106848},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106848},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320302079},
author = {Mikkel B. Kjærgaard and Omid Ardakanian and Salvatore Carlucci and Bing Dong and Steven K. Firth and Nan Gao and Gesche Margarethe Huebner and Ardeshir Mahdavi and Mohammad Saiedur Rahaman and Flora D. Salim and Fisayo Caleb Sangogboye and Jens Hjort Schwee and Dawid Wolosiuk and Yimin Zhu},
keywords = {Open data, Data publishing, Data use, Occupant behavior, FAIR Data, Ontology, Anonymi, z, ation, Metadata schema},
abstract = {Many new tools for improving the design and operation of buildings try to realize the potential of big data. In particular, data is an important element for occupant-centric design and operation as occupants’ presence and actions are affected by a high degree of uncertainty and, hence, are hard to model in general. For such research, data handling is an important challenge, and following an open science paradigm based on open data can increase efficiency and transparency of scientific work. This article reviews current practices and infrastructure for open data-driven research on occupant-centric design and operation of buildings. In particular, it covers related work on open data in general and for the built environment in particular, presents survey results for existing scientific practices, reviews technical solutions for handling data and metadata, discusses ethics and privacy protection and analyses principles for the sharing of open data. In summary, this study establishes the status quo and presents an outlook on future work for methods and infrastructures to support the open data community within the built environment.}
}
@incollection{BUTKA20201,
title = {Chapter 1 - Methodologies for Knowledge Discovery Processes in Context of AstroGeoInformatics},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {1-20},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00010-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000102},
author = {Peter Butka and Peter Bednár and Juliana Ivančáková},
keywords = {Knowledge discovery process, Data mining, Methodology, Process modeling, Ontology, AstroGeoInformatics},
abstract = {Successful data science projects usually follow some methodology which can provide the data scientist with basic guidelines on how to challenge the problem and how to work with data, algorithms, or models. This methodology is then a structured way to describe the knowledge discovery process. Without a flexible structure of steps, data science projects can be unsuccessful, or at least it will be hard to achieve a result that can be easily applied and shared. Their better understanding is quite beneficial both to data scientists and to anyone who needs to discuss results or steps of the process. Moreover, in some domains, including those working with data from astronomy and geophysics, steps used in preprocessing and analysis of data are crucial to understanding provided data products. In this chapter, we provide an overview of knowledge discovery processes, selected methodologies, and their standardization and sharing using process languages and ontologies. At the end of the chapter, we also discuss these aspects according to the domain of astro/geo data.}
}
@article{SYSOEV2020507,
title = {Heterogeneous Data Aggregation Schemes to Determine Traffic Flow Parameters in Regional Intelligent Transportation Systems},
journal = {Transportation Research Procedia},
volume = {45},
pages = {507-513},
year = {2020},
note = {Transport Infrastructure and systems in a changing world. Towards a more sustainable, reliable and smarter mobility.TIS Roma 2019 Conference Proceedings},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.03.063},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520302271},
author = {Anton Sysoev and Elena Khabibullina and Dmitry Kadasev and Nikita Voronin},
keywords = {Intellegent Transportation System, Traffic Flow, Heterogeneous Data Sources},
abstract = {For optimal solution of intelligent transportation centers goals it’s necessary to collect and process information from a variety of sources to ensure a full understanding of the situation on roads in real-time. It is possible to improve traffic control and transportation corridor management tasks via analysis of heterogeneous sources such as GPS, metadata regarding device identity, status and location used by mobile devices to stay connected to various networks, data received from vehicles and road infrastructure devices, transport cards and terminals information in the public transport; e-mail messages, SMS, posts in messengers or social networks, metadata of phone calls of road users (drivers, pedestrians and passengers); video streams from surveillance cameras and drones, cameras to record violations; data from accelerometers, compasses, temperature sensors, infrared radiation, electromagnetic field, atmospheric pressure, etc.; data from radars, devices analyzing RFID-tags. In the paper the possibility of using heterogeneous sources to calculate the necessary characteristics of the traffic flow is discussed, the existing approaches to aggregate information obtained from heterogeneous sources are described, a conceptual scheme of the module to aggregate heterogeneous data based on BigData and Data Mining methods is suggested.}
}
@article{MBUNGE20201631,
title = {Integrating emerging technologies into COVID-19 contact tracing: Opportunities, challenges and pitfalls},
journal = {Diabetes & Metabolic Syndrome: Clinical Research & Reviews},
volume = {14},
number = {6},
pages = {1631-1636},
year = {2020},
issn = {1871-4021},
doi = {https://doi.org/10.1016/j.dsx.2020.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S1871402120303325},
author = {Elliot Mbunge},
keywords = {COVID-19, Contact tracing, Emerging technologies},
abstract = {Background and aims
With no approved vaccines for treating COVID-19 as of August 2020, many health systems and governments rely on contact tracing as one of the prevention and containment methods. However, there have been instances when the infected person forgets his/her contact-persons and does not have their contact details. Therefore, this study aimed at analyzing possible opportunities and challenges of integrating emerging technologies into COVID-19 contact tracing.
Methods
The study applied literature search from Google Scholar, Science Direct, PubMed, Web of Science, IEEE and WHO COVID-19 reports and guidelines analyzed.
Results
While the integration of technology-based contact tracing applications to combat COVID-19 and break transmission chains promise to yield better results, these technologies face challenges such as technical limitations, dealing with asymptomatic individuals, lack of supporting ICT infrastructure and electronic health policy, socio-economic inequalities, deactivation of mobile devices’ WIFI, GPS services, interoperability and standardization issues, security risks, privacy issues, political and structural responses, ethical and legal risks, consent and voluntariness, abuse of contact tracing apps, and discrimination.
Conclusion
Integrating emerging technologies into COVID-19 contact tracing is seen as a viable option that policymakers, health practitioners and IT technocrats need to seriously consider in mitigating the spread of coronavirus. Further research is also required on how best to improve efficiency and effectiveness in the utilisation of emerging technologies in contact tracing while observing the security and privacy of people in fighting the COVID-19 pandemic.}
}
@article{SCHRIJVERS2020104617,
title = {A review of methods and data to determine raw material criticality},
journal = {Resources, Conservation and Recycling},
volume = {155},
pages = {104617},
year = {2020},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2019.104617},
url = {https://www.sciencedirect.com/science/article/pii/S0921344919305233},
author = {Dieuwertje Schrijvers and Alessandra Hool and Gian Andrea Blengini and Wei-Qiang Chen and Jo Dewulf and Roderick Eggert and Layla {van Ellen} and Roland Gauss and James Goddin and Komal Habib and Christian Hagelüken and Atsufumi Hirohata and Margarethe Hofmann-Amtenbrink and Jan Kosmol and Maïté {Le Gleuher} and Milan Grohol and Anthony Ku and Min-Ha Lee and Gang Liu and Keisuke Nansai and Philip Nuss and David Peck and Armin Reller and Guido Sonnemann and Luis Tercero and Andrea Thorenz and Patrick A. Wäger},
keywords = {Critical raw materials, Material criticality, Critical resources, Strategic raw materials, Criticality assessment},
abstract = {The assessment of the criticality of raw materials allows the identification of the likelihood of a supply disruption of a material and the vulnerability of a system (e.g. a national economy, technology, or company) to this disruption. Inconclusive outcomes of various studies suggest that criticality assessments would benefit from the identification of best practices. To prepare the field for such guidance, this paper aims to clarify the mechanisms that affect methodological choices which influence the results of a study. This is achieved via literature review and round table discussions among international experts. The paper demonstrates that criticality studies are divergent in the system under study, the anticipated risk, the purpose of the study, and material selection. These differences in goal and scope naturally result in different choices regarding indicator selection, the required level of aggregation as well as the subsequent choice of aggregation method, and the need for a threshold value. However, this link is often weak, which suggests a lack of understanding of cause-and-effect mechanisms of indicators and outcomes. Data availability is a key factor that limits the evaluation of criticality. Furthermore, data quality, including both data uncertainty and data representativeness, is rarely addressed in the interpretation and communication of results. Clear guidance in the formulation of goals and scopes of criticality studies, the selection of adequate indicators and aggregation methods, and the interpretation of the outcomes, are important initial steps in improving the quality of criticality assessments.}
}
@article{LU202034,
title = {LSTM variants meet graph neural networks for road speed prediction},
journal = {Neurocomputing},
volume = {400},
pages = {34-45},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.03.031},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220303775},
author = {Zhilong Lu and Weifeng Lv and Yabin Cao and Zhipu Xie and Hao Peng and Bowen Du},
keywords = {Neural network, LSTM, LSTM Variant, GNN, Road speed prediction},
abstract = {Traffic flow prediction is a fundamental issue in smart cities and plays an important role in urban traffic planning and management. An accurate predictive model can help individuals make reliable travel plans and choose optimal routes while efficiently helping administrators maintain traffic order. Road speed prediction, which is a sub-task of traffic flow forecasting, is challenging due to the complicated spatial dependencies characterizing road networks and dynamic temporal traffic patterns. Given the power of recurrent neural networks (RNNs) in learning temporal relations and graph neural networks (GNNs) in integrating graph-structured and node-attributed features, in this paper, we design a novel graph LSTM (GLSTM) framework to capture spatial-temporal representations in road speed forecasting. More specifically, we first present a temporal directed attributed graph to model complex traffic flow. Then, to take advantage of the structure properties and graph features, we employ a message-passing mechanism for feature aggregation and updating. Finally, we further implement several variants of LSTMs with a GN block under the encoder-decoder framework to model spatial-temporal dependencies. The experiments show that our proposed model is able to fully utilize both the road latent graph structure and traffic speed to forecast the road state during future periods. The results on two real-world datasets show that our GLSTM can outperform state-of-the-art baseline methods by up to 32.8% in terms of MAE, 43.2% in terms of MAPE and 23.1% in terms of RMSE.}
}
@article{BOULTON2020111310,
title = {Use of public datasets in the examination of multimorbidity: Opportunities and challenges},
journal = {Mechanisms of Ageing and Development},
volume = {190},
pages = {111310},
year = {2020},
issn = {0047-6374},
doi = {https://doi.org/10.1016/j.mad.2020.111310},
url = {https://www.sciencedirect.com/science/article/pii/S0047637420301068},
author = {Christopher Boulton and J. Mark Wilkinson},
keywords = {Public datasets, Legislation, Information governance, Data curation, Multimorbidity},
abstract = {The interrogation of established, large-scale datasets presents great opportunities in health data science for the linkage and mining of potentially disparate resources to create new knowledge in a fast and cost-efficient manner. The number of datasets that can be queried in the field of multimorbidity is vast, ranging from national administrative and audit datasets, large clinical, technical and biological cohorts, through to more bespoke data collections made available by individual organisations and laboratories. However, with these opportunities also come technical and regulatory challenges that require an informed approach. In this review, we outline the potential benefits of using previously collected data as a vehicle for research activity. We illustrate the added value of combining potentially disparate datasets to find answers to novel questions in the field. We focus on the legal, governance and logistical considerations required to hold and analyse data acquired from disparate sources and outline some of the solutions to these challenges. We discuss the infrastructure resources required and the essential considerations in data curation and informatics management, and briefly discuss some of the analysis approaches currently used.}
}
@article{CERQUITELLI2020179,
title = {Enabling predictive analytics for smart manufacturing through an IIoT platform⁎⁎This research leading has been partially funded by the European Commission under the H2020-IND-CE-2016-17 program, FOF-09-2017, Grant agreement no. 767561 ”SERENA” project, VerSatilE plug-and-play platform enabling REmote predictive mainteNAnce.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {179-184},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301737},
author = {T. Cerquitelli and N. Nikolakis and P. Bethaz and S. Panicucci and F. Ventura and E. Macii and S. Andolina and A. Marguglio and K. Alexopoulos and P. Petrali and A. Pagani and P. {van Wilgen} and M. Ippolito},
keywords = {Data analytics, data management, analytics architecture, predictive analytics, production systems},
abstract = {In the last few years, manufacturing systems are getting gradually transformed into smart factories. In this context, an increasing number of information and communication technologies is incorporated towards facilitating management, production, and control processes. The introduction of advanced embedded systems with enhanced connectivity produces a vast amount of data, posing a challenge in terms of data analytics. However, the in-time collection and analysis of acquired data can create insight into the manufacturing process as well as its assets. One aspect of major importance for every production system is preserving its equipment in operational condition, and within those limits that could minimize unplanned breakdowns and production stoppages. This paper details the predictive analytics methodology integrated into the SERENA platform able to: (i) streamline the prognostics of the industrial components, (ii) characterize the health status of the monitored equipment, (iii) generate an early warning related to the condition of the equipment, and (iv) forecast the future evolution of the monitored equipment’s degradation. To demonstrate the effectiveness of the proposed methodology, different use cases are discussed with results obtained on real-data collected in real-time from the industrial environments.}
}
@article{JIN2020109567,
title = {Environmental performance of off-site constructed facilities: A critical review},
journal = {Energy and Buildings},
volume = {207},
pages = {109567},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.109567},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819319061},
author = {Ruoyu Jin and Jingke Hong and Jian Zuo},
keywords = {Off-site construction, Life cycle approach, Energy consumption, Carbon emissions, Environmental performance},
abstract = {During the recent decades, off-site construction (OSC) has gained a rapid growth worldwide. It has been reported that OSC, as an alternative construction method, has a variety of benefits. However, there is lack of critical review of the building performance (e.g. energy consumption and carbon emissions) of off-site built facilities. Life cycle approach and bibliometric analysis are adopted in this study to review existing research on the environmental performance of off-site built facilities. The results show that most existing studies chose to employ LCA method to systematically analyse carbon emissions and energy consumption of prefabricated residential buildings by using sub-assembly components as functional units. The detailed investigation of volumetric construction and building operational stage are rare. The sensitivity of thermal property caused by offsite manufacturing and onsite assembly in comparison to the traditional cast-in-situ method remains unexplored. It is encouraged to cover various environmental impacts in building performance assessment, to develop a sustainability rating system applied in OSC, to adopt Internet-of-Things in OSC monitoring by using real-time data, and to establish an indicator system for the evaluation of OSC performance. The findings of this study can facilitate the understanding of status quo and shed lights on future research direction in OSC.}
}
@article{CIMINI2020258,
title = {A human-in-the-loop manufacturing control architecture for the next generation of production systems},
journal = {Journal of Manufacturing Systems},
volume = {54},
pages = {258-271},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300029},
author = {Chiara Cimini and Fabiana Pirola and Roberto Pinto and Sergio Cavalieri},
keywords = {Industry 4.0, Human-in-the-loop, Cyber-physical production systems, Manufacturing control architecture},
abstract = {In recent years, the introduction of Industry 4.0 technologies in the manufacturing landscape promoted the development of smart factories characterised by relevant socio-technical interactions between humans and machines. In this context, understanding and modelling the role of humans turns out to be crucial to develop efficient manufacturing systems of the future. Grounding on previous researches in the field of Human-in-the-Loop and Human Cyber-Physical Systems, the paper aims at contributing to a deep reflection about human-machine interaction in the wider perspective of Social Human-in-the-Loop Cyber-Physical Production Systems, in which more agents collaborate and are socially connected. After presenting an evolution of manufacturing control organisations, an architecture to depict social interactions in smart factories is proposed. The proposed architecture contributes to the representation of different human roles in the smart factory and the exploration of both hierarchical and heterarchical data-driven decision-making processes in manufacturing.}
}
@article{BENKE2020114210,
title = {Development of pedotransfer functions by machine learning for prediction of soil electrical conductivity and organic carbon content},
journal = {Geoderma},
volume = {366},
pages = {114210},
year = {2020},
issn = {0016-7061},
doi = {https://doi.org/10.1016/j.geoderma.2020.114210},
url = {https://www.sciencedirect.com/science/article/pii/S001670611930610X},
author = {K.K. Benke and S. Norng and N.J. Robinson and K. Chia and D.B. Rees and J. Hopley},
keywords = {Artificial intelligence, Big data, Biostatistics, Carbon farming, Climate change, Predictive analytics, Regional analysis, Simulation},
abstract = {The pedotransfer function is a mathematical model used to convert direct soil measurements into known and unknown soil properties. It provides information for modelling and simulation in soil research, hydrology, environmental science and climate change impacts, including investigating the carbon cycle and the exchange of carbon between soils and the atmosphere to support carbon farming. In particular, the pedotransfer function can provide input parameters for landscape design, soil quality assessment and economic optimisation. The objective of the study was to investigate the feasibility of using a generalised pedotransfer function derived with a machine learning method to predict soil electrical conductivity (EC) and soil organic carbon content (OC) for different regional locations in the state of Victoria, Australia. This strategy supports a unified approach to the interpolation and population of a single regional soils database, in contrast to a range of pedotransfer functions derived from local databases with measurement sets that may have limited transferability. The pedotransfer function generation was based on a machine learning algorithm incorporating the Generalized Linear Mixed Model with interactions and nested terms, with Residual Maximum Likelihood estimation, and a predictor-frequency ranking system with step-wise reduction of predictors to evaluate the predictive errors in reduced models. The source of the data was the Victorian Soil Information System (VSIS), which is a database administered for soil information and mapping purposes. The database contains soil measurements and information from locations across Victoria and is a repository of historical data, including monitoring studies. In total, data from 93 projects were available for inputs to modelling and analysis, with 5158 samples used to derive predictors for EC and 1954 samples used to derive predictors for OC. Over 500 models were tested by systematically reducing the number of predictors from the full model. Five-fold cross-validation was used for estimation of model mean-squared prediction error (MSPE) and mean-absolute percentage error (MAPE). The results were statistically significant with only a gradual reduction in error for the top-ranked 50 models. The prediction errors (MSPE and MAPE) of the top ranked model for EC are 0.686 and 0.635, and 0.413 and 0.474 for OC respectively. The four most frequently occurring predictors both for EC and OC prediction across the full set of models were found to be soil depth, pH, particle size distribution and geomorphological mapping unit. The possible advantages and disadvantages of this approach were discussed with respect to other machine learning approaches.}
}
@article{ZUO2020106431,
title = {Geodata science and geochemical mapping},
journal = {Journal of Geochemical Exploration},
volume = {209},
pages = {106431},
year = {2020},
issn = {0375-6742},
doi = {https://doi.org/10.1016/j.gexplo.2019.106431},
url = {https://www.sciencedirect.com/science/article/pii/S0375674219304960},
author = {Renguang Zuo and Yihui Xiong},
keywords = {Geodata science, Data mining, Data insight and prediction, Geochemical exploration},
abstract = {Geodata science (GDS) is an interdisciplinary field in which geoscience data are mined for us to well understand the origin, evolution and future of our Earth and planet with prediction and assessment of its resources, environments, and natural hazards. The data chain of GDS involves collecting geosciences data, mining geoinformation, discovering geo-knowledge, and making spatial decisions. There are three groups of GDS methods for exploring and mining geoscience data including data statistics, data mining, and data insight and prediction. A case study on geochemical exploration data mapping was conducted to demonstrate the powerful use of GDS. The results show that GDS is a new research paradigm for exploring the spatial association of geochemical patterns, mining elemental association, and recognizing geochemical anomalies associated with mineralization via geo-computation and geo-visualization techniques in support of mineral exploration.}
}
@article{MOHAMMED2020106568,
title = {Training set selection and swarm intelligence for enhanced integration in multiple classifier systems},
journal = {Applied Soft Computing},
volume = {95},
pages = {106568},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106568},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620305068},
author = {Amgad M. Mohammed and Enrique Onieva and Michał Woźniak},
keywords = {Combination rule, Swarm intelligence, Optimization, Data reduction, Classifier integration, Multiple classifier systems, Big data, Machine learning, Classifier ensemble},
abstract = {Multiple classifier systems (MCSs) constitute one of the most competitive paradigms for obtaining more accurate predictions in the field of machine learning. Systems of this type should be designed efficiently in all of their stages, from data preprocessing to multioutput decision fusion. In this article, we present a framework for utilizing the power of instance selection methods and the search capabilities of swarm intelligence to train learning models and to aggregate their decisions. The process consists of three steps: First, the essence of the complete training data set is captured in a reduced set via the application of intelligent data sampling. Second, the reduced set is used to train a group of heterogeneous classifiers using bagging and distance-based feature sampling. Finally, swarm intelligence techniques are applied to identify a pattern among multiple decisions to enhance the fusion process by assigning class-specific weights for each classifier. The proposed methodology yielded competitive results in experiments that were conducted on 25 benchmark datasets. The Matthews correlation coefficient (MCC) is regarded as the objective to be maximized by various nature-inspired metaheuristics, which include the moth-flame optimization algorithm (MFO), the grey wolf optimizer (GWO) and the whale optimization algorithm (WOA).}
}
@incollection{LEWIS2020109,
title = {6 - Data openness and democratization in healthcare: an evaluation of hospital ranking methods},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Data Democracy},
publisher = {Academic Press},
pages = {109-126},
year = {2020},
isbn = {978-0-12-818366-3},
doi = {https://doi.org/10.1016/B978-0-12-818366-3.00006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818366300006X},
author = {Kelly Lewis and Chau Pham and Feras A. Batarseh},
keywords = {Data, Healthcare, Hospitals, Patient, Quality},
abstract = {The democratization of data in healthcare is subsequently becoming one of the most impacting hurdles in patient service, hospital operations, and the entire medical field. Controversy among healthcare data has brought challenges to many medical case studies including legal rights to data, data ownership, and data bias. The experiment performed in this chapter shows thorough hospital quality metrics and provides a new ranking system that compares hospitals and identifies which ones are best for a potential patient, based on their needs and medical status.}
}
@article{HUANG2020106437,
title = {Incomplete data classification with view-based decision tree},
journal = {Applied Soft Computing},
volume = {94},
pages = {106437},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106437},
url = {https://www.sciencedirect.com/science/article/pii/S156849462030377X},
author = {Hekai Huang and Hongzhi Wang and Ming Sun},
keywords = {Incomplete data, Missing value, Classification, Decision tree},
abstract = {Data quality issues may bring serious problems in data analysis. For instance, missing values could decrease the accuracy of the classification. As traditional classification approaches can only be applied to complete data sets, we present a generic classification model for incomplete data where existing classification methods can be effectively incorporated. Firstly, we generate complete views from the incomplete data by choosing proper subsets of attributes based on Information Gain measure. Then we use these selected views to obtain multiple base classifiers. Finally, the base classifies are effectively combined as a final classifier with a decision tree. Extensive experiments results on real data sets demonstrate that the proposed method outperforms existing approaches.}
}
@article{CHEN20201,
title = {AI-Skin: Skin disease recognition based on self-learning and wide data collection through a closed-loop framework},
journal = {Information Fusion},
volume = {54},
pages = {1-9},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519300867},
author = {Min Chen and Ping Zhou and Di Wu and Long Hu and Mohammad Mehedi Hassan and Atif Alamri},
keywords = {Skin disease recognition, Data width evolution, Self-learning process, Deep learning model},
abstract = {There are a lot of hidden dangers in the change of human skin conditions, such as the sunburn caused by long-time exposure to ultraviolet radiation, which not only has aesthetic impact causing psychological depression and lack of self-confidence, but also may even be life-threatening due to skin canceration. Current skin disease researches adopt the auto-classification system for improving the accuracy rate of skin disease classification. However, the excessive dependence on the image sample database is unable to provide individualized diagnosis service for different population groups. To overcome this problem, a medical AI framework based on data width evolution and self-learning is put forward in this paper to provide skin disease medical service meeting the requirement of real time, extendibility and individualization. First, the wide collection of data in the close-loop information flow of user and remote medical data center is discussed. Next, a data set filter algorithm based on information entropy is given, to lighten the load of edge node and meanwhile improve the learning ability of remote cloud analysis model. In addition, the framework provides an external algorithm load module, which can be compatible with the application requirements according to the model selected. Three kinds of deep learning model, i.e., LeNet-5, AlexNet and VGG16, are loaded and compared, which have verified the universality of the algorithm load module. The experiment platform for the proposed real-time, individualized and extensible skin disease recognition system is built. And the system’s computation and communication delay under the interaction scenario between tester and remote data center are analyzed. It is demonstrated that the system we put forward is reliable and effective.}
}
@incollection{20201,
title = {Author Index},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {1-10},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-444-64165-6.18002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444641656180024}
}
@article{JI20204,
title = {A review of measuring, assessing and mitigating heat stress in dairy cattle},
journal = {Biosystems Engineering},
volume = {199},
pages = {4-26},
year = {2020},
note = {Environmental stressors and animal production: improved performance through understanding and engineering advances},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2020.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1537511020302026},
author = {Boyu Ji and Thomas Banhazi and Kristen Perano and Afshin Ghahramani and Les Bowtell and Chaoyuan Wang and Baoming Li},
keywords = {precision livestock farming, dairy cow, heat stress, thermal index},
abstract = {Heat stress is a significant challenge in dairy farming systems. Dairy cows under heat stress will encounter impaired welfare leading to production losses. As the frequency and magnitude of heat stress events increase in the coming decades, a focus on heat stress reduction studies becomes important. Modelling and on-farm experiments have been used to assess the effects of heat stress on livestock over the last few decades. Mitigation solutions including optimal shed structure, ventilation, feeding regimes, farm management and genetic selection have all been explored. However, under different farm conditions, the heat tolerance and coping ability of dairy cows can vary significantly. Until now, the results from different mathematical models have provided a variety of heat stress thresholds for on-farm use. In practice, it is still costly to determine an accurate heat stress level in order to identify the mitigation requirements. This review summarises previous studies on the effects of heat stress on intensively reared dairy cows and different mitigation approaches. We have undertaken a comparative analysis of thermal indices, animal responses, and mitigation approaches. Recommendations are then given for developing a framework to enhance the measurement, assessment and mitigation of heat stress. Robust monitoring systems, big data analyses and artificial intelligence algorithms are needed for the future development of dynamic, self-calibrating model-based systems, which could provide real-time assessment and minimisation of heat stress.}
}
@article{LONGO2020899,
title = {Apollon: Towards a citizen science methodology for urban environmental monitoring},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {899-912},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20303940},
author = {Antonella Longo and Marco Zappatore and Mario A. Bochicchio},
keywords = {Mobile crowd sensing, Citizen science, Smart cities, Internet of people},
abstract = {The collaborative power of ICT systems is a key enabler of social and technological advances providing multiple opportunities for public involvement in participatory activities, thanks to novel paradigms like citizen science and mobile crowd sensing. These paradigms, if applied according to specific methodologies, promise to increase the pervasive observation of urban environmental pollution either directly by human observers, or by means of crowd-sourcing data measurement tasks using sensors in smart phones or other mobile devices. We propose a platform, named Apollon, to enable scientists and others to take part in citizen science projects based on the exploitation of mobile devices. The platform has been implemented and validated in an educational context, in which students participate in urban environmental monitoring activities. In the paper, we describe the platform and the approach developed to produce successful experiments.}
}
@article{BROSA2020S686,
title = {PNS270 Real World Evidence and Risk-sharing Agreements: Shaken, Not Stirred},
journal = {Value in Health},
volume = {23},
pages = {S686},
year = {2020},
note = {Virtual ISPOR Europe 2020},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2020.08.1714},
url = {https://www.sciencedirect.com/science/article/pii/S109830152033970X},
author = {M. Brosa and C. Avendaño-Solà and J. Espin and J. Mestre-Ferrandiz and C. Pinyol}
}
@article{LU202087,
title = {HAPE: A programmable big knowledge graph platform},
journal = {Information Sciences},
volume = {509},
pages = {87-103},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S002002551930800X},
author = {Ruqian LU and Chaoqun FEI and Chuanqing WANG and Shunfeng GAO and Han QIU and Songmao ZHANG and Cungen CAO},
keywords = {Big knowledge, Big knowledge system, Big knowledge graph, Knowledge graph browser, Knowledge graph operating system, Knowledge scripting language, Big knowledge security},
abstract = {Heaven Ape (HAPE) is an integrated big knowledge graph platform supporting the construction, management, and operation of large to massive scale knowledge graphs. Its current version described in this paper is a prototype, which consists of three parts: a big knowledge graph knowledge base, a knowledge graph browser on the client side, and a knowledge graph operating system on the server side. The platform is programmed in two high level scripting languages: JavaScript for programming the client side functions and Python for the server side functions. For making the programming more suitable for big knowledge processing and more friendly to knowledge programmers, we have developed two versions of knowledge scripting languages, namely K-script-c and K-script-s, for performing very high level knowledge programming of client resp. server side functions. HAPE borrows ideas from some well-known knowledge graph processing techniques and also invents some new ones as our creation. As an experiment, we transformed a major part of the DBpedia knowledge base and reconstructed it as a big knowledge graph. It works well in some application tests and provides acceptable efficiency.}
}
@article{GUPTA202026,
title = {Digital Analytics: Modeling for Insights and New Methods},
journal = {Journal of Interactive Marketing},
volume = {51},
pages = {26-43},
year = {2020},
note = {Special issue on Big Data, Technology-Driven CRM & Artificial Intelligence},
issn = {1094-9968},
doi = {https://doi.org/10.1016/j.intmar.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1094996820300840},
author = {Shaphali Gupta and Agata Leszkiewicz and V. Kumar and Tammo Bijmolt and Dmitriy Potapov},
keywords = {Digital analytics, Internet of things, Artificial intelligence, Drones, Blockchain, Firm capabilities},
abstract = {Firms are increasingly turning towards new-age technologies such as artificial intelligence (AI), the internet of things (IoT), blockchain, and drones, among others, to assist in interacting with their customers. Further, with the prominence of personalization and customer engagement as the go-to customer management strategies, it is essential for firms to understand how to integrate new-age technologies into their existing practices to aid seamlessly in the generation of actionable insights. Towards this end, this study proposes an organizing framework to understand how firms can use digital analytics, within the changing technology landscape, to generate consumer insights. The proposed framework begins by recognizing the forces that are external to the firm then lead to the generation of specific capabilities by the firm. Further, the firms capabilities can lead to the generation of insights for decision-making that can be data-driven and/or analytics-driven. Finally, the proposed framework identifies the creation of value-based outcomes for firms and customers resulting from the insights generated. Additionally, we identify moderators that influence: (a) the impact of external forces on the development of firm capabilities, and (b) the creation of insights and subsequent firm outcomes. This study also identifies questions for future research that combines the inclusion of new-age technologies, generation of strategic insights, and the achievement of established firm outcomes.}
}
@article{TETLEY2020265,
title = {Decoding earth's plate tectonic history using sparse geochemical data},
journal = {Geoscience Frontiers},
volume = {11},
number = {1},
pages = {265-276},
year = {2020},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1674987119300908},
author = {Michael G. Tetley and Zheng-Xiang Li and Kara J. Matthews and Simon E. Williams and R. Dietmar Müller},
keywords = {Plate tectonics, Geochemistry, Geodynamics, Supercontinents, Rodinia, Big data},
abstract = {Accurately mapping plate boundary types and locations through time is essential for understanding the evolution of the plate-mantle system and the exchange of material between the solid Earth and surface environments. However, the complexity of the Earth system and the cryptic nature of the geological record make it difficult to discriminate tectonic environments through deep time. Here we present a new method for identifying tectonic paleo-environments on Earth through a data mining approach using global geochemical data. We first fingerprint a variety of present-day tectonic environments utilising up to 136 geochemical data attributes in any available combination. A total of 38301 geochemical analyses from basalts aged from 5–0 Ma together with a well-established plate reconstruction model are used to construct a suite of discriminatory models for the first order tectonic environments of subduction and mid-ocean ridge as distinct from intraplate hotspot oceanic environments, identifying 41, 35, and 39 key discriminatory geochemical attributes, respectively. After training and validation, our model is applied to a global geochemical database of 1547 basalt samples of unknown tectonic origin aged between 1000–410 Ma, a relatively ill-constrained period of Earth's evolution following the breakup of the Rodinia supercontinent, producing 56 unique global tectonic environment predictions throughout the Neoproterozoic and Early Paleozoic. Predictions are used to discriminate between three alternative published Rodinia configuration models, identifying the model demonstrating the closest spatio-temporal consistency with the basalt record, and emphasizing the importance of integrating geochemical data into plate reconstructions. Our approach offers an extensible framework for constructing full-plate, deep-time reconstructions capable of assimilating a broad range of geochemical and geological observations, enabling next generation Earth system models.}
}
@article{BARMPOUNAKIS202050,
title = {On the new era of urban traffic monitoring with massive drone data: The pNEUMA large-scale field experiment},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {111},
pages = {50-71},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X19310320},
author = {Emmanouil Barmpounakis and Nikolas Geroliminis},
keywords = {Unmanned aerial systems, Swarm of drones, Experiment, Traffic monitoring, Traffic flow modeling, Multimodal systems},
abstract = {The new era of sharing information and “big data” has raised our expectations to make mobility more predictable and controllable through a better utilization of data and existing resources. The realization of these opportunities requires going beyond the existing traditional ways of collecting traffic data that are based either on fixed-location sensors or GPS devices with low spatial coverage or penetration rates and significant measurement errors, especially in congested urban areas. Unmanned Aerial Systems (UAS) or simply “drones” have been proposed as a pioneering tool of the Intelligent Transportation Systems (ITS) infrastructure due to their unique characteristics, but various challenges have kept these efforts only at a small size. This paper describes the system architecture and preliminary results of a first-of-its-kind experiment, nicknamed pNEUMA, to create the most complete urban dataset to study congestion. A swarm of 10 drones hovering over the central business district of Athens over multiple days to record traffic streams in a congested area of a 1.3 km2 area with more than 100 km-lanes of road network, around 100 busy intersections (signalized or not), many bus stops and close to half a million trajectories. The aim of the experiment is to record traffic streams in a multi-modal congested environment over an urban setting using UAS that can allow the deep investigation of critical traffic phenomena. The pNEUMA experiment develops a prototype system that offers immense opportunities for researchers many of which are beyond the interests and expertise of the authors. This open science initiative creates a unique observatory of traffic congestion, a scale an-order-of-magnitude higher than what was available till now, that researchers from different disciplines around the globe can use to develop and test their own models.}
}
@article{SALUVEER2020102895,
title = {Methodological framework for producing national tourism statistics from mobile positioning data},
journal = {Annals of Tourism Research},
volume = {81},
pages = {102895},
year = {2020},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2020.102895},
url = {https://www.sciencedirect.com/science/article/pii/S0160738320300396},
author = {Erki Saluveer and Janika Raun and Margus Tiru and Laura Altin and Jaanus Kroon and Tarass Snitsarenko and Anto Aasa and Siiri Silm},
keywords = {Tourism statistics, Mobile positioning, Cross-border tourism, Accommodation statistics, Estonia},
abstract = {Due to the ongoing increase in daily mobility, reductions in border controls, and new trends in tourism, it is important to find new ways to record comprehensively the growing number of tourists. This paper describes a method of extracting cross-border statistics on tourism from roaming call activities found within passive mobile positioning data. Eesti Pank (the central bank of Estonia) has been using these data and methodology since 2008 to calculate the national balance of payments and publish tourism statistics. Statistics obtained from mobile positioning data are herein compared with statistics on accommodation. Results indicate that positioning data enables the generation of detailed statistics on tourism, and for inbound visits, there is a strong correlation with official statistics on accommodation.}
}
@article{AHMAD2020117283,
title = {Smart energy forecasting strategy with four machine learning models for climate-sensitive and non-climate sensitive conditions},
journal = {Energy},
volume = {198},
pages = {117283},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.117283},
url = {https://www.sciencedirect.com/science/article/pii/S036054422030390X},
author = {Tanveer Ahmad and Chen Huanxin and Dongdong Zhang and Hongcai Zhang},
keywords = {Supervised learning, Utilities and building consumption, Energy forecasting, Imbalanced data handling, Performance correlation, Energy benchmark},
abstract = {Developing a reliable and robust algorithm for accurate energy demand prediction is indispensable for utility companies for various applications, e.g., power dispatching, market participation and infrastructure planning. However, this is challenging because the performance of a forecasting algorithm may be affected by various factors, such as data quality, geographic diversity, forecast horizon, customer segmentation and the forecast origin. Furthermore, an approach that performs well in one region may fail in other regions, and similarly, a model that forecasts accurately in one horizon may fail to produce an accurate prediction for other horizons. To overcome the above challenges such as rough data quality, different forecasting horizons, different kinds of loads and forecasting for different regions, this study proposes four machine learning/supervised learning models. These models are applied to improve the generalization of the network and reduce forecasting. These models are intended to simplify or demystify terms, complex concepts and data granularity used in energy forecasting. Two different data sites and four forecasting horizons are used to validate the proposed models. The coefficient of variation and mean absolute percentage error are 50% higher as compared with the existing model. The proposed supervised learning models ensure a generalization ability, robustness and high accuracy for building and utilities energy consumption forecasting. The forecasting results help to improve and automate the predictive modeling process while covering the knowledge-gaps between machine learning and conventional forecasting models.}
}
@article{WANG20201,
title = {GuardHealth: Blockchain empowered secure data management and Graph Convolutional Network enabled anomaly detection in smart healthcare},
journal = {Journal of Parallel and Distributed Computing},
volume = {142},
pages = {1-12},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519308470},
author = {Ziyu Wang and Nanqing Luo and Pan Zhou},
keywords = {Blockchain, Smart healthcare, Security and privacy, Graph Convolutional Network, Trust assessment},
abstract = {The paradox between the dramatic development of medical data privacy demand and years of bureaucratic regulation has slowed innovation for electronic medical records (EMRs). We are at a historical point for such innovation to prompt patients data autonomy. In this paper, we propose GuardHealth: an efficient, secure and decentralized Blockchain system for data privacy preserving and sharing. GuardHealth manages confidentiality, authentication, data preserving and data sharing when handling sensitive information. We exploit consortium Blockchain and smart contract to achieve secure data storage and sharing, which prevents data sharing without permission. A trust model is utilized for precisely managing trust of users with the implementation of the state-of-art Graph Neural Network (GNN) for malicious node detection. Security analysis and experiment results show that the proposed scheme is applicable for smart healthcare system.}
}
@article{DEIST2020189,
title = {Distributed learning on 20 000+ lung cancer patients – The Personal Health Train},
journal = {Radiotherapy and Oncology},
volume = {144},
pages = {189-200},
year = {2020},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2019.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167814019334899},
author = {Timo M. Deist and Frank J.W.M. Dankers and Priyanka Ojha and M. {Scott Marshall} and Tomas Janssen and Corinne Faivre-Finn and Carlotta Masciocchi and Vincenzo Valentini and Jiazhou Wang and Jiayan Chen and Zhen Zhang and Emiliano Spezi and Mick Button and Joost {Jan Nuyttens} and René Vernhout and Johan {van Soest} and Arthur Jochems and René Monshouwer and Johan Bussink and Gareth Price and Philippe Lambin and Andre Dekker},
keywords = {Lung cancer, Big data, Distributed learning, Federated learning, Machine learning, Survival analysis, Prediction modeling, FAIR data},
abstract = {Background and purpose
Access to healthcare data is indispensable for scientific progress and innovation. Sharing healthcare data is time-consuming and notoriously difficult due to privacy and regulatory concerns. The Personal Health Train (PHT) provides a privacy-by-design infrastructure connecting FAIR (Findable, Accessible, Interoperable, Reusable) data sources and allows distributed data analysis and machine learning. Patient data never leaves a healthcare institute.
Materials and methods
Lung cancer patient-specific databases (tumor staging and post-treatment survival information) of oncology departments were translated according to a FAIR data model and stored locally in a graph database. Software was installed locally to enable deployment of distributed machine learning algorithms via a central server. Algorithms (MATLAB, code and documentation publicly available) are patient privacy-preserving as only summary statistics and regression coefficients are exchanged with the central server. A logistic regression model to predict post-treatment two-year survival was trained and evaluated by receiver operating characteristic curves (ROC), root mean square prediction error (RMSE) and calibration plots.
Results
In 4 months, we connected databases with 23 203 patient cases across 8 healthcare institutes in 5 countries (Amsterdam, Cardiff, Maastricht, Manchester, Nijmegen, Rome, Rotterdam, Shanghai) using the PHT. Summary statistics were computed across databases. A distributed logistic regression model predicting post-treatment two-year survival was trained on 14 810 patients treated between 1978 and 2011 and validated on 8 393 patients treated between 2012 and 2015.
Conclusion
The PHT infrastructure demonstrably overcomes patient privacy barriers to healthcare data sharing and enables fast data analyses across multiple institutes from different countries with different regulatory regimens. This infrastructure promotes global evidence-based medicine while prioritizing patient privacy.}
}
@article{CHEN2020102839,
title = {Blockchain for Internet of things applications: A review and open issues},
journal = {Journal of Network and Computer Applications},
volume = {172},
pages = {102839},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102839},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520303076},
author = {Fei Chen and Zhe Xiao and Laizhong Cui and Qiuzhen Lin and Jianqiang Li and Shui Yu},
keywords = {Blockchain, IoT, Access control, Data security, Trusted third party, Automatic payment, Usage paradigm},
abstract = {Blockchain and the Internet of things (IoT) systems are attracting more and more research efforts from both academia and industry. Blockchain is rapidly evolving to be a new infrastructure for building robust distributed applications. Similarly, Internet of things is getting increasing deployment in the context of smart city, smart home, smart healthcare, etc. On the intersection of the two emerging areas, researchers are proposing to use blockchain to build more dependable IoT systems. This paper reviews the most recent research advances in this direction during the past four years. Specifically, we review, summarize, and categorize existing research works. We divide the research works into four groups according to the roles that the blockchain plays in IoT systems, i.e., access control platform, data security platform, trusted third party, and automatic payment platform. For each group, we also discuss future research challenges. From the review, we further summarize the usage paradigms and open issues on using blockchain to build dependable IoT systems. We hope this work serves as a reference of existing models for both researchers and engineers that are interested to leverage blockchain to build future IoT systems.}
}
@article{VICTORELLI202013,
title = {Understanding human-data interaction: Literature review and recommendations for design},
journal = {International Journal of Human-Computer Studies},
volume = {134},
pages = {13-32},
year = {2020},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1071581919301193},
author = {Eliane Zambon Victorelli and Julio Cesar {Dos Reis} and Heiko Hornung and Alysson Bolognesi Prado},
keywords = {Human-Data Interaction, Literature review, Research challenges, Data deluge},
abstract = {The trend of collecting information about human activities to inform and influence actions and decisions poses a series of challenges to analyze this data deluge. The lack of ability to understand and interact with this amount of data prevents people and organizations from taking the best of this information. To investigate how people interact with data, a new area of study called “Human-Data Interaction” (HDI) is emerging. In this article, we conduct a thorough literature review to create the big picture about the subject. We carry out a variety of analyses and visual examinations to understand the characteristics of existing publications, detecting the most frequently addressed research topics and consolidating the research challenges. Based on the needs of HDI we found in the analyzed publications, we organize a set of recommendations and evaluate online systems that demand intensive human-data interaction. The obtained results indicate there are still many open questions for this interesting area, which is maturing with an increase number of publications in the last years, and that systems with large amount of data openly available poorly meet the proposed recommendations.}
}
@article{KANKANAMGE2020101360,
title = {Determining disaster severity through social media analysis: Testing the methodology with South East Queensland Flood tweets},
journal = {International Journal of Disaster Risk Reduction},
volume = {42},
pages = {101360},
year = {2020},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2019.101360},
url = {https://www.sciencedirect.com/science/article/pii/S2212420919307940},
author = {Nayomi Kankanamge and Tan Yigitcanlar and Ashantha Goonetilleke and Md. Kamruzzaman},
keywords = {Social media, Data analytics, Big data, Crowdsourcing, Volunteered geographic information, South East Queensland Floods},
abstract = {Social media was underutilised in disaster management practices, as it was not seen as a real-time ground level information harvesting tool during a disaster. In recent years, with the increasing popularity and use of social media, people have started to express their views, experiences, images, and video evidences through different social media platforms. Consequently, harnessing such crowdsourced information has become an opportunity for authorities to obtain enhanced situation awareness data for efficient disaster management practices. Nonetheless, the current disaster-related Twitter analytics methods are not versatile enough to define disaster impacts levels as interpreted by the local communities. This paper contributes to the existing knowledge by applying and extending a well-established data analysis framework, and identifying highly impacted disaster areas as perceived by the local communities. For this, the study used real-time Twitter data posted during the 2010–2011 South East Queensland Floods. The findings reveal that: (a) Utilising Twitter is a promising approach to reflect citizen knowledge; (b) Tweets could be used to identify the fluctuations of disaster severity over time; (c) The spatial analysis of tweets validates the applicability of geo-located messages to demarcate highly impacted disaster zones.}
}
@article{FISHBEIN2020100458,
title = {The Longitudinal Epidemiologic Assessment of Diabetes Risk (LEADR): Unique 1.4 M patient Electronic Health Record cohort},
journal = {Healthcare},
volume = {8},
number = {4},
pages = {100458},
year = {2020},
issn = {2213-0764},
doi = {https://doi.org/10.1016/j.hjdsi.2020.100458},
url = {https://www.sciencedirect.com/science/article/pii/S2213076420300579},
author = {Howard A. Fishbein and Rebecca Jeffries Birch and Sunitha M. Mathew and Holly L. Sawyer and Gerald Pulver and Jennifer Poling and David Kaelber and Russell Mardon and Maurice C. Johnson and Wilson Pace and Keith D. Umbel and Xuanping Zhang and Karen R. Siegel and Giuseppina Imperatore and Sundar Shrestha and Krista Proia and Yiling Cheng and Kai {McKeever Bullard} and Edward W. Gregg and Deborah Rolka and Meda E. Pavkov},
keywords = {Chronic disease, Diabetes mellitus, Epidemiologic methods, Epidemiologic research design, Big data, Electronic health records, Public health informatics, Public health practice},
abstract = {Background
The Longitudinal Epidemiologic Assessment of Diabetes Risk (LEADR) study uses a novel Electronic Health Record (EHR) data approach as a tool to assess the epidemiology of known and new risk factors for type 2 diabetes mellitus (T2DM) and study how prevention interventions affect progression to and onset of T2DM. We created an electronic cohort of 1.4 million patients having had at least 4 encounters with a healthcare organization for at least 24-months; were aged ≥18 years in 2010; and had no diabetes (i.e., T1DM or T2DM) at cohort entry or in the 12 months following entry. EHR data came from patients at nine healthcare organizations across the U.S. between January 1, 2010–December 31, 2016.
Results
Approximately 5.9% of the LEADR cohort (82,922 patients) developed T2DM, providing opportunities to explore longitudinal clinical care, medication use, risk factor trajectories, and diagnoses for these patients, compared with patients similarly matched prior to disease onset.
Conclusions
LEADR represents one of the largest EHR databases to have repurposed EHR data to examine patients’ T2DM risk. This paper is first in a series demonstrating this novel approach to studying T2DM.
Implications
Chronic conditions that often take years to develop can be studied efficiently using EHR data in a retrospective design.
Level of evidence
While much is already known about T2DM risk, this EHR's cohort's 160 M data points for 1.4 M people over six years, provides opportunities to investigate new unique risk factors and evaluate research hypotheses where results could modify public health practice for preventing T2DM.}
}
@article{GANDHI2020100132,
title = {An automated review of body sensor networks research patterns and trends},
journal = {Journal of Industrial Information Integration},
volume = {18},
pages = {100132},
year = {2020},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2020.100132},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X20300078},
author = {Vidhyotma Gandhi and Jaiteg Singh},
keywords = {Body sensor network, Survey, Latent semantic analysis, IoT, cloud computing},
abstract = {The root of contemporary biomedical engineering and research is the amalgamation of Body Sensor Network (BSN) with the Internet of Things (IoT) and cloud computing. It has resulted in a lot of research articles from reputed journals by renowned researchers. semiautomatic technique, Latent Semantic Algorithm (LSA) is a tried and tested machine learning concept to find out the latest research trend in the specific area. Here, we apply the same to a dataset of 927 research titles and abstracts for finding research trends pertaining to BSN. The literature published from 2004 till 2018 was analyzed during this study. In this study, 5-core research areas and 100 research trends were identified. On the basis of these findings, future directions with potential to steer future research were also given.}
}
@article{MESSINGER2020476,
title = {The doctor will see you now: How machine learning and artificial intelligence can extend our understanding and treatment of asthma},
journal = {Journal of Allergy and Clinical Immunology},
volume = {145},
number = {2},
pages = {476-478},
year = {2020},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2019.12.898},
url = {https://www.sciencedirect.com/science/article/pii/S0091674919326053},
author = {Amanda I. Messinger and Gang Luo and Robin R. Deterding},
keywords = {Asthma, machine learning, artificial intelligence, digital health}
}
@article{20201221,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {10},
pages = {1221},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820305437}
}
@article{CHU202028,
title = {Neighborhood rough set-based three-way clustering considering attribute correlations: An approach to classification of potential gout groups},
journal = {Information Sciences},
volume = {535},
pages = {28-41},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520304497},
author = {Xiaoli Chu and Bingzhen Sun and Xue Li and Keyu Han and JiaQi Wu and Yan Zhang and Qingchun Huang},
keywords = {Three-way clustering, Heterogeneous information system, Medical decision making, Classification of potential gout groups, Neighborhood rough set},
abstract = {Using modern information theory to classify and identify high-risk disease groups is one of the research concerns in medical decision-making. The early diagnosis of gout is missing a single indicator, and relying on artificial labeling of disease characteristics is not only costly for decision-making, but also has a high misdiagnosis rate. Aiming at incomplete and attribute-related random large sample data, we propose a three-way clustering algorithm based on neighborhood rough sets, which is used to initially label the data, reduce the rate of misdiagnosis, and improve decision-making efficiency. Firstly, a neighborhood rough set theory in a heterogeneous information system is established. Secondly, the Best-Worst method-based neighborhood rough set attribute reduction model considering attribute correlation is constructed. Thirdly, a neighborhood rough set-based three-way clustering method for heterogeneous information system is proposed. Finally, we use 2,683 random samples and the proposed model to identify and classify potential gout patients in the samples. The results show that the proposed model can be used to mark and cluster potential gout groups in random samples without prior probability and with fuzzy decision rules, which is helpful for clinical decision-making.}
}
@article{KILKIS2020113410,
title = {Advances in integration of energy, water and environment systems towards climate neutrality for sustainable development},
journal = {Energy Conversion and Management},
volume = {225},
pages = {113410},
year = {2020},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2020.113410},
url = {https://www.sciencedirect.com/science/article/pii/S0196890420309456},
author = {Şiir Kılkış and Goran Krajačić and Neven Duić and Marc A. Rosen and Moh'd Ahmad Al-Nimr},
keywords = {Energy, Water, Environment, Sustainable development, System integration, Climate mitigation},
abstract = {The integration of energy, water and environment systems represents important opportunities for addressing the urgent imperative of climate neutrality. The 29 original papers in the virtual special issue of the 14th Conference on Sustainable Development of Energy, Water and Environment Systems exemplify multiple advances in integrated approaches. This editorial provides a review of recent scientific contributions in energy system integration, urban synergies in the energy transition, integration of energy and water systems as well as valorization of waste heat. Advances that relate to sustainable combustion, biomass, and managing emissions provide further perspectives. All seven themes contain new research directions in such areas as solar energy technologies, thermal energy storage, power-to-X technologies, district heating and cooling networks, wastewater treatment plants, water desalination, and salinity gradient technologies. Advanced optimization approaches, big data analytics for cogeneration, thermal management applications, pollution minimization, lignocellulosic biomass, catalysts for alternative fuels as well as carbon capture, storage and utilization are described among other scientific contributions. Across the world, the focus on integration is gaining prominence, especially with the European Union Strategy on Energy System Integration that recognizes the role of a coordinated approach for planning and operating the energy system as a whole. The research advances that are contained in this editorial will support the realization of a coherent approach on the path towards sustaining the life-support systems of the planet and thereby support sustainable development.}
}
@incollection{PENZA2020235,
title = {Chapter 12 - Low-cost sensors for outdoor air quality monitoring},
editor = {Eduard Llobet},
booktitle = {Advanced Nanomaterials for Inexpensive Gas Microsensors},
publisher = {Elsevier},
pages = {235-288},
year = {2020},
series = {Micro and Nano Technologies},
isbn = {978-0-12-814827-3},
doi = {https://doi.org/10.1016/B978-0-12-814827-3.00012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128148273000128},
author = {Michele Penza},
keywords = {Chemical sensor materials, Air quality sensors, Gas sensors, Particulate matter sensors, Sensor nodes, Wireless sensor networks, Mobile sensing, Demonstration pilots, Air pollution},
abstract = {This chapter reviews the air quality sensors distributed in urban areas for wireless node network applications at the current state of art. The functional materials such as metal oxides, carbon nanomaterials, and hybrid composites are enabling technologies for chemical sensing. Low-cost sensors integrated in low-power sensor systems are operated in wireless sensor networks for urban air quality monitoring of toxic gases and particulate matter at outdoor level with high spatial and temporal resolution. Examples of stationary sensor networks and mobile sensing on ground vehicles and unmanned aerial vehicles (UAV) are presented in terms of smart and sustainable cities and Internet-of-Things (IoT) applications as supplement of the expensive air quality monitoring stations. The air quality sensor performance of the existing worldwide-operated wireless sensor networks is described with emphasis to the data quality objectives. The article concludes on challenging applications and promising perspectives of the air quality sensors in the rapidly emerging fields.}
}
@article{SCOTT202071,
title = {The adaptive immune receptor repertoire community as a model for FAIR stewardship of big immunology data},
journal = {Current Opinion in Systems Biology},
volume = {24},
pages = {71-77},
year = {2020},
note = {Systems immunology & host-pathogen interaction (2020)},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2020.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452310020300433},
author = {Jamie K. Scott and Felix Breden},
keywords = {Adaptive immunity, B-cell and T-cell receptor repertoires, FAIR Principles, Open source, Adaptive immune receptor repertoire (AIRR) community},
abstract = {Systems biology involves network-oriented, computational approaches to modeling biological systems through analysis of big biological data. To contribute maximally to scientific progress, big biological data should be FAIR: findable, accessible, interoperable, and reusable. Here, we describe high-throughput sequencing data that characterize the vast diversity of B- and T-cell clones comprising the adaptive immune receptor repertoire (AIRR-seq data) and its contribution to our understanding of COVID-19 (coronavirus disease 19). We describe the accomplishments of the AIRR community, a grass-roots network of interdisciplinary laboratory scientists, bioinformaticians, and policy wonks, in creating and publishing standards, software and repositories for AIRR-seq data based on the FAIR principles.}
}
@article{MONTERO2020593,
title = {Using GPS tracking data to validate route choice in OD trips within dense urban networks},
journal = {Transportation Research Procedia},
volume = {47},
pages = {593-600},
year = {2020},
note = {22nd EURO Working Group on Transportation Meeting, EWGT 2019, 18th – 20th September 2019, Barcelona, Spain},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.03.136},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520303355},
author = {Lídia Montero and Xavier Ros-Roca},
keywords = {Type your keywords here, separated by semicolons},
abstract = {There are presently several companies that provide processed or raw global positioning system (GPS) measurements generated by fleets of commercial vehicles, internet applications or automobile companies. The aim of this paper is to deepen our understanding of GPS data applicability in transportation modelling by providing systematic and quantified insights into the representativeness of collected data in describing individual’ route choices. Unfortunately, real data often contain noise, uncertainty, errors, redundancies or even irrelevant information. Useless models will be obtained when built over incorrect or incomplete data. This is why pre-processing is one of the most critical steps in data analysis. Yet, pre-processing has not been properly systematized, which is why this paper focuses on the necessary steps for pre-processing GPS tracking data together and puts forth a proposal for systematizing it. Furthermore, the aggregation level is at waypoint location for low latency GPS positions along the trip trajectory. Travel time reliability on OD paths is addressed, along with other OD path characteristics. Dense urban networks pose multiple possibilities for route choice, and data from new technologies offers an opportunity to understand route choice behaviour.}
}
@article{YOO2020117091,
title = {Adaptive spatial sampling design for environmental field prediction using low-cost sensing technologies},
journal = {Atmospheric Environment},
volume = {221},
pages = {117091},
year = {2020},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2019.117091},
url = {https://www.sciencedirect.com/science/article/pii/S1352231019307307},
author = {Eun-Hye Yoo and Andrew Zammit-Mangion and Michael G. Chipeta},
keywords = {Adaptive spatial sampling design, Change-of-support problem, Fixed rank kriging, Low-cost portable air sensors, Measurement uncertainty},
abstract = {The last decade has seen an explosion in data sources available for monitoring and prediction of environmental phenomena. While several inferential methods have been developed to make predictions on the underlying process by combining these data, an optimal sampling design for additional data collection in the presence of multiple heterogeneous sources has not yet been developed. Here, we provide an adaptive spatial design strategy based on a utility function that combines both prediction uncertainty and risk-factor criteria. Prediction uncertainty is obtained through a spatial data fusion approach based on fixed rank kriging that can tackle data with differing spatial supports and signal-to-noise ratios. We focus on the application of low-cost portable sensors, which tend to be relatively noisy, for air pollution monitoring, where data from regulatory stations as well as numeric modeling systems are also available. Although we find that spatial adaptive sampling designs can help to improve predictions and reduce prediction uncertainty, low-cost portable sensors are only likely to be beneficial if they are sufficient in number and quality. Our conclusions are based on a multi-factorial simulation experiment, and on a realistic simulation of pollutants in the Erie and Niagara counties in Western New York.}
}
@article{KRONBERGER2020528,
title = {Smart Manufacturing and Continuous Improvement and Adaptation of Predictive Models},
journal = {Procedia Manufacturing},
volume = {42},
pages = {528-531},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305862},
author = {Gabriel Kronberger and Florian Bachinger and Michael Affenzeller},
keywords = {Smart Manufacturing, Artificial Intelligence, Predictive Models, Concept Drift , 68T05, 68T20},
abstract = {Predictive models are an important success factor for smart manufacturing. Accordingly, purely data-driven models as well as hybrid models are increasingly deployed within manufacturing environments for optimal control of plants. However, long-term monitoring and adaptation of predictive models has not been a focus of studies so far but will likely become increasingly more important as more and more predictive models are deployed. We give a number of recommendations for effectively managing predictive models in smart manufacturing environments.}
}
@article{CHOI2020101860,
title = {When blockchain meets social-media: Will the result benefit social media analytics for supply chain operations management?},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {135},
pages = {101860},
year = {2020},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.101860},
url = {https://www.sciencedirect.com/science/article/pii/S1366554519315339},
author = {Tsan-Ming Choi and Shu Guo and Suyuan Luo},
keywords = {Social media analytics, Methods, Blockchain technology, Operations management, Case studies},
abstract = {Social media analytics is very critical in modern supply chain operations management (SCOM). However, in terms of methods, conducting social media analytics (SMA) for SCOM faces many challenges. Problems such as data accuracy (e.g., fake data), user privacy, data security, etc. are all present. Recently, with the emergence of blockchain technology (BCT), many new social media apps and platforms are developed. Motivated by the fact that (i) SMA is important for SCOM, (ii) the traditional social media (TSM) has insufficiency, and (iii) the “blockchain technology supported social media” (BSM) platforms have emerged, we explore whether and how the BSM would enhance social media analytics for SCOM. To be specific, by reviewing papers published in leading SCOM journals, we identify the applications and limitations of SMA for SCOM. Then, we conduct real case studies to examine the shortcomings of the TSM platforms and reveal features of their BSM counterparts. We investigate how the blockchain technology would potentially improve the use of SMA for SCOM. Finally, a future research agenda is proposed.}
}
@article{BROO202090,
title = {Towards Data-centric Decision Making for Smart Infrastructure: Data and Its Challenges},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {90-94},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301580},
author = {Didem Gürdür Broo and Jennifer Schooling},
keywords = {smart infrastructure, cyber-physical systems, data analytics, data-centric, decision making},
abstract = {Smart infrastructure has the potential to revolutionise how infrastructure is delivered, managed and automatically controlled. Data and digital twins offer an opportunity for this purpose. However, before using data to aid stakeholders for their decisions, one should have a data plan, collect useful data, assess the quality of the data, have the language for enabling communication related to data, manage the data and have or acquire the skills that are needed to integrate and analyse data. To this end, this paper aims to provide an explanation and description of several challenges related to the data. The article concludes by suggesting that the industry addresses the challenges related to the availability, accessibility, quality, volume, variety and longevity of data by considering user-centric approaches, blending methodologies and identifying the organizational needs. The industry should plan and act on these challenges as part of its data analytics strategies of today to expedite the artificial intelligence applications of the future.}
}
@article{GRZENDA2020305,
title = {Hybrid short term prediction to address limited timeliness of public transport data streams},
journal = {Neurocomputing},
volume = {391},
pages = {305-317},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.100},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219316182},
author = {Maciej Grzenda and Karolina Kwasiborska and Tomasz Zaremba},
keywords = {IoT data streams, Hybrid algorithms, Stream mining, Multilayer perceptron, Stream processing architectures},
abstract = {In spite of the diversity of solutions developed in the Internet of Things (IoT) domain, some features are shared by numerous IoT deployments and the data they process. These include incompleteness and latency in data transmission from multiple distributed objects. Among others, the systems tracking the location of vehicles are affected by these problems. The primary objective of this work is to address the way the latency in location data acquisition, referred to also as timeliness, can be dealt with. We propose a hybrid method combining machine learning models such as multilayer perceptrons trained in batch mode and online learning methods to perform short-term prediction of vehicle delay data. In this way, stream instances that have not arrived yet from the sensors can be temporarily replaced with predicted values. The method we propose successfully integrates stream mining methods developed for stationary and non-stationary conditions i.e. also the methods developed for concept drifting data streams. For all examined reference data sets and hybridised stream methods, the method reduced prediction error and addressed the risk of using static prediction models not matching or no longer matching the evolving process for which the prediction is performed.}
}
@incollection{SKODA2020xiii,
title = {Preface},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {xiii-xvi},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00007-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000072},
author = {Petr Škoda and Fathalrahman Adam}
}
@article{LIU2020107582,
title = {PriDPM: Privacy-preserving dynamic pricing mechanism for robust crowdsensing},
journal = {Computer Networks},
volume = {183},
pages = {107582},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107582},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620312226},
author = {Yuxian Liu and Fagui Liu and Hao-Tian Wu and Xinglin Zhang and Bowen Zhao and Xingfu Yan},
keywords = {Crowdsensing, Privacy, Posted pricing, Data quality},
abstract = {Providing appropriate monetary incentives for participants is vital for crowdsensing to encourage their participation. Among all outstanding incentive mechanisms, posted pricing has been widely adopted because it is easy to implement and naturally achieves truthfulness and fairness. However, existing schemes either lack of privacy protection for the sensing data of participants, or fail to consider the diversity of sensing quality in crowdsensing systems. To address these critical problems, we propose a privacy-preserving dynamic pricing mechanism for robust crowdsensing, which only needs to spend a small amount of total payments to recruit a group of mobile users with reasonable sensing quality while protecting the sensing data privacy of each participant. Specifically, we first design an efficient secure aggregation algorithm through which the platform can compute the sum of sensing data from participants without learning each participant’s individual data. Then, we employ the aggregation algorithm to design a secure quality assessment algorithm to obtain the sensing quality levels of participants. Finally, according to the varying quality levels, we develop a model-free reinforcement learning based approach to optimize pricing policy to achieve lower total payments and robustness requirement. Through privacy analysis and extensive experiments, we demonstrate the effectiveness and efficiency of the proposed mechanism.}
}
@article{HONG2020109831,
title = {State-of-the-art on research and applications of machine learning in the building life cycle},
journal = {Energy and Buildings},
volume = {212},
pages = {109831},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.109831},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819337879},
author = {Tianzhen Hong and Zhe Wang and Xuan Luo and Wanni Zhang},
keywords = {Machine learning, Artificial intelligence, Buildings, Building life cycle, Building control, Building performance},
abstract = {Fueled by big data, powerful and affordable computing resources, and advanced algorithms, machine learning has been explored and applied to buildings research for the past decades and has demonstrated its potential to enhance building performance. This study systematically surveyed how machine learning has been applied at different stages of building life cycle. By conducting a literature search on the Web of Knowledge platform, we found 9579 papers in this field and selected 153 papers for an in-depth review. The number of published papers is increasing year by year, with a focus on building design, operation, and control. However, no study was found using machine learning in building commissioning. There are successful pilot studies on fault detection and diagnosis of HVAC equipment and systems, load prediction, energy baseline estimate, load shape clustering, occupancy prediction, and learning occupant behaviors and energy use patterns. None of the existing studies were adopted broadly by the building industry, due to common challenges including (1) lack of large scale labeled data to train and validate the model, (2) lack of model transferability, which limits a model trained with one data-rich building to be used in another building with limited data, (3) lack of strong justification of costs and benefits of deploying machine learning, and (4) the performance might not be reliable and robust for the stated goals, as the method might work for some buildings but could not be generalized to others. Findings from the study can inform future machine learning research to improve occupant comfort, energy efficiency, demand flexibility, and resilience of buildings, as well as to inspire young researchers in the field to explore multidisciplinary approaches that integrate building science, computing science, data science, and social science.}
}
@article{YAN2020103331,
title = {Data mining in the construction industry: Present status, opportunities, and future trends},
journal = {Automation in Construction},
volume = {119},
pages = {103331},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103331},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520309110},
author = {Hang Yan and Nan Yang and Yi Peng and Yitian Ren},
keywords = {Data mining, Construction industry, Knowledge discovery, Systematic review},
abstract = {The construction industry is experiencing remarkable growth in the data generation. Data mining (DM) from considerable amount of data in the construction industry has emerged as an important tool for knowledge discovery. Despite the remarkable growth of DM applications to the construction industry, a systematic review on DM applications in this field is still lacking. Therefore, this paper attempts to provide a comprehensive literature review of DM application articles published between 2001 and 2019 with the specificity of construction industry. The popularity of DM applications in the construction industry is increasing, especially after 2016, with a plurality emanating from China. The main data sources, DM functions, and frequently used DM techniques in the construction industry are discussed in detail. Nine major application fields are identified, with the primary research interests focusing on multiple dimensions of energy, safety management, building occupancy and occupant behavior, material performance, and textual knowledge discovery. Four major challenges and four future research directions are proposed by drawing on the research findings. This study provides academics and practitioners with a more comprehensive understanding of the state-of-the-art of DM applications and heuristic implications for future studies.}
}
@article{NGUYEN2020113670,
title = {Succinct contrast sets via false positive controlling with an application in clinical process redesign},
journal = {Expert Systems with Applications},
volume = {161},
pages = {113670},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113670},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420304942},
author = {Dang Nguyen and Wei Luo and Bay Vo and Witold Pedrycz},
keywords = {Data mining, Contrast set mining, Classification, False discovery rate, Emergency department, Length of stay (LOS)},
abstract = {Many applications of intelligent systems involve understanding a group of contrastively different outcome (e.g., all survivors of a deadly cancer, a top performing team in a large corporation). The intelligent system needs to identify attributes (features) which best describe or explain the group versus its alternatives. In data mining, this problem is studied under the framework of contrast set mining (CSM). Although CSM is not new, the era of big data has produced new computational and statistical challenges. In particular, existing algorithms fail (1) to perform efficiently in terms of runtime on large-scale datasets and (2) to accommodate simultaneous inference on an overwhelming array of features which are often repetitive and collinear. In this paper, we develop a CSM algorithm which addresses both challenges. The computational challenge is addressed with a tree structure and two theorems while the statistical challenge is addressed with the application of false discovery rate for multiple testing. The computational and statistical advantages of the proposed algorithm over three state-of-the-art algorithms are demonstrated with comprehensive experiments. In addition, we also show the effectiveness of our proposed method in an intelligence-system application involving hospital process redesign. The proposed method not only improves the performance of machine learning systems, but also generates succinct and insightful patterns directly relevant to clinical decision-making.}
}
@article{LI2020106287,
title = {A systematic review of unsupervised learning techniques for software defect prediction},
journal = {Information and Software Technology},
volume = {122},
pages = {106287},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106287},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300379},
author = {Ning Li and Martin Shepperd and Yuchen Guo},
keywords = {Unsupervised learning, Software defect prediction, Machine learning, Systematic review, Meta-analysis},
abstract = {Background
Unsupervised machine learners have been increasingly applied to software defect prediction. It is an approach that may be valuable for software practitioners because it reduces the need for labeled training data.
Objective
Investigate the use and performance of unsupervised learning techniques in software defect prediction.
Method
We conducted a systematic literature review that identified 49 studies containing 2456 individual experimental results, which satisfied our inclusion criteria published between January 2000 and March 2018. In order to compare prediction performance across these studies in a consistent way, we (re-)computed the confusion matrices and employed the Matthews Correlation Coefficient (MCC) as our main performance measure.
Results
Our meta-analysis shows that unsupervised models are comparable with supervised models for both within-project and cross-project prediction. Among the 14 families of unsupervised model, Fuzzy CMeans (FCM) and Fuzzy SOMs (FSOMs) perform best. In addition, where we were able to check, we found that almost 11% (262/2456) of published results (contained in 16 papers) were internally inconsistent and a further 33% (823/2456) provided insufficient details for us to check.
Conclusion
Although many factors impact the performance of a classifier, e.g., dataset characteristics, broadly speaking, unsupervised classifiers do not seem to perform worse than the supervised classifiers in our review. However, we note a worrying prevalence of (i) demonstrably erroneous experimental results, (ii) undemanding benchmarks and (iii) incomplete reporting. We therefore encourage researchers to be comprehensive in their reporting.}
}
@article{ZHANG20201,
title = {The Elements of Data Sharing},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {18},
number = {1},
pages = {1-4},
year = {2020},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2020.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1672022920300486},
author = {Zhang Zhang and Shuhui Song and Jun Yu and Wenming Zhao and Jingfa Xiao and Yiming Bao}
}
@article{PRATESI2020101786,
title = {PRIMULE: Privacy risk mitigation for user profiles},
journal = {Data & Knowledge Engineering},
volume = {125},
pages = {101786},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.101786},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18305342},
author = {Francesca Pratesi and Lorenzo Gabrielli and Paolo Cintia and Anna Monreale and Fosca Giannotti},
keywords = {Mobile phone data, Call detail record, Privacy, Anonymization},
abstract = {The availability of mobile phone data has encouraged the development of different data-driven tools, supporting social science studies and providing new data sources to the standard official statistics. However, this particular kind of data are subject to privacy concerns because they can enable the inference of personal and private information. In this paper, we address the privacy issues related to the sharing of user profiles, derived from mobile phone data, by proposing PRIMULE, a privacy risk mitigation strategy. Such a method relies on PRUDEnce (Pratesi et al., 2018), a privacy risk assessment framework that provides a methodology for systematically identifying risky-users in a set of data. An extensive experimentation on real-world data shows the effectiveness of PRIMULE strategy in terms of both quality of mobile user profiles and utility of these profiles for analytical services such as the Sociometer (Furletti et al., 2013), a data mining tool for city users classification.}
}
@article{GREGORIADES2020100986,
title = {Electronic word of mouth analysis for new product positioning evaluation},
journal = {Electronic Commerce Research and Applications},
volume = {42},
pages = {100986},
year = {2020},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2020.100986},
url = {https://www.sciencedirect.com/science/article/pii/S1567422320300636},
author = {Andreas Gregoriades and Maria Pampaka},
keywords = {Micro-blogs, Sentiment Analysis, Product positioning, Topic modelling},
abstract = {People increasingly choose to express themselves online through electronic word of mouth (eWOM), generating large amounts of data, making eWOM a valuable source of information through big data analytics. This enables organizations to gain insights directly from customers’ opinions for better decision making. This work presents a new methodology for evaluating an organisation’s product-positioning strategy through eWOM analytics. A product’s mispositioning has significant negative effects and there is strong interest in identifying ways to avoid it. Current methods that utilize eWOM for product positioning evaluation mostly use post-product release reviews and do not statistically evaluate the effect of time on the product positioning; nor do they provide any means to diagnose the cause of mispositioning. The temporal aspect of positioning, however, provides valuable insights into which product features are more time-invariant and accordingly makes it possible to plan for product redesign or repositioning to maximize profitability. A case study is presented in the context of smartphones using design science research, utilizing Twitter data regarding the release of a new product, collected using a custom Android application. The research questions addressed in this paper are: (1) How do consumers’ preferences change over time with regards to the product’s positioning? (2) Which product features positively influence product positioning and which negatively? To answer these questions, we compared the product-positioning strategy and consumers’ opinions before and after the release of a new product to identify possible discrepancies between expected and actual positioning of the product. This work constitutes a methodological contribution with demonstrated implications for new product positioning strategy evaluation using tweet analysis.}
}
@article{HAI202032,
title = {DependData: Data collection dependability through three-layer decision-making in BSNs for healthcare monitoring},
journal = {Information Fusion},
volume = {62},
pages = {32-46},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519303008},
author = {Tao Hai and Md Zakirul Alam Bhuiyan and Jing Wang and Tian Wang and D. Frank Hsu and Yafeng Li and Sinan Q Salih and Jie Wu and Penghui Liu},
keywords = {Body sensor networks, Healthcare monitoring, Decision-making, Dependability, Data dependability, Security and privacy, Data quality},
abstract = {Recently, there have been extensive studies on applying security and privacy protocols in Body Sensor Networks (BSNs) for patient healthcare monitoring (BSN-Health). Though these protocols provide adequate security to data packets, the collected data may still be compromised at the time of acquisition and before aggregation/storage in the severely resource-constrained BSNs. This leads to data collection frameworks being meaningless or undependable, i.e., an undependable BSN-Health. We study data dependability concerns in the BSN-Health and propose a data dependability verification framework named DependData with the objective of verifying data dependability through the decision-making in three layers. The 1st decision-making (1-DM) layer verifies signal-level data at each health sensor of the BSN locally to guarantee that collected signals ready for processing and transmission are dependable so that undependable processing and transmission in the BSN can be avoided. The 2nd decision-making (2-DM) layer verifies data before aggregation at each local aggregator (like clusterhead) of the BSN to guarantee that data received for aggregation is dependable so that undependable data aggregation can be avoided. The 3rd decision-making (3-DM) layer verifies the stored data before the data appears to a remote healthcare data user to guarantee that data available to the owner end (such as smartphone) is dependable so that undependable information viewing can be avoided. Finally, we evaluate the performance of DependData through simulations regarding 1-DM, 2-DM, and 3-DM and show that up to 92% of data dependability concerns can be detected in the three layers. To the best of our knowledge, DependData would be the first framework to address data dependability aside from current substantial studies of security and privacy protocols. We believe the three layers decision-making framework would attract a wide range of applications in the future.}
}
@article{KIM20201697,
title = {Considerations for generating meaningful HRA data: Lessons learned from HuREX data collection},
journal = {Nuclear Engineering and Technology},
volume = {52},
number = {8},
pages = {1697-1705},
year = {2020},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2020.01.034},
url = {https://www.sciencedirect.com/science/article/pii/S173857331930854X},
author = {Yochan Kim},
keywords = {Data analytics, Human reliability analysis, HuREX framework, Lesson learned, Simulation data},
abstract = {To enhance the credibility of human reliability analysis, various kinds of data have been recently collected and analyzed. Although it is obvious that the quality of data is critical, the practices or considerations for securing data quality have not been sufficiently discussed. In this work, based on the experience of the recent human reliability data extraction projects, which produced more than fifty thousand data-points, we derive a number of issues to be considered for generating meaningful data. As a result, thirteen considerations are presented here as pertaining to the four different data extraction activities: preparation, collection, analysis, and application. Although the lessons were acquired from a single kind of data collection framework, it is believed that these results will guide researchers to consider important issues in the process of extracting data.}
}
@article{EGU2020112,
title = {Investigating day-to-day variability of transit usage on a multimonth scale with smart card data. A case study in Lyon},
journal = {Travel Behaviour and Society},
volume = {19},
pages = {112-123},
year = {2020},
issn = {2214-367X},
doi = {https://doi.org/10.1016/j.tbs.2019.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214367X19302315},
author = {Oscar Egu and Patrick Bonnel},
keywords = {Public transit, Travel behaviour, Smart card data, Passenger clustering, Day-to-day variability, User segmentation},
abstract = {To examine the variability of travel behaviour over time, transportation researchers need to collect longitudinal data. The first studies around day-to-day variability of travel behaviour were based on surveys. Those studies have shown that there is considerable variation in individual travel behaviour. They have also discussed the implications of this variability in terms of modelling, policy evaluation or marketing. Recently, the multiplication of big data has led to an explosion in the number of studies about travel behaviour. This is because those new data sources collect lots of data, about lots of people over long periods. In the field of public transit, smart card data is one of those big data sources. They have been used by various authors to conduct longitudinal analyses of transit usage behaviour. However, researchers working with smart card data mostly rely on clustering techniques to measure variability, and they often use conceptual framework different from those of transportation researchers familiar with traditional data sources. In particular, there is no study based on smart card data that explicitly measure day-to-day intrapersonal variability of transit usage. Therefore, the purpose of this investigation is to address this gap. To do this, a clustering method and a similarity metric are combined to explore simultaneously interpersonal and intrapersonal variability of transit usage. The application is done with a rich dataset covering a 6 months period (181 days) and it contributes to the growing literature on smart card data. Results of this research confirm previous works based on survey data and show that there is no one size fits all approach to the problem of day-to-day variability of transit usage. They also prove that combining clustering algorithm with day-to-day intrapersonal similarity metric is a valuable tool to mine smart card data. The findings of this study can help in identifying new passenger segmentation and in tailoring information and services.}
}
@article{HILDEBRANDT2020103552,
title = {Rigor and reproducibility for data analysis and design in the behavioral sciences},
journal = {Behaviour Research and Therapy},
volume = {126},
pages = {103552},
year = {2020},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2020.103552},
url = {https://www.sciencedirect.com/science/article/pii/S0005796720300036},
author = {Tom Hildebrandt and Jason M. Prenoveau},
keywords = {Statistics, Big data, Reproducibility, Reliability, -hacking},
abstract = {The rigor and reproducibility of science methods depends heavily on the appropriate use of statistical methods to answer research questions and make meaningful and accurate inferences based on data. The increasing analytic complexity and valuation of novel statistical and methodological approaches to data place greater emphasis on statistical review. We will outline the controversies within statistical sciences that threaten rigor and reproducibility of research published in the behavioral sciences and discuss ongoing approaches to generate reliable and valid inferences from data. We outline nine major areas to consider for generally evaluating the rigor and reproducibility of published articles and apply this framework to the 116 Behaviour Research and Therapy (BRAT) articles published in 2018. The results of our analysis highlight a pattern of missing rigor and reproducibility elements, especially pre-registration of study hypotheses, links to statistical code/output, and explicit archiving or sharing data used in analyses. We recommend reviewers consider these elements in their peer review and that journals consider publishing results of these rigor and reproducibility ratings with manuscripts to incentivize authors to publish these elements with their manuscript.}
}
@article{STOCK20201490,
title = {Enabling a healthy start for vulnerable newborns},
journal = {The Lancet},
volume = {396},
number = {10261},
pages = {1490},
year = {2020},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(20)32235-2},
url = {https://www.sciencedirect.com/science/article/pii/S0140673620322352},
author = {Sarah J Stock and Meredith Brockway and Helga Zoega and Jasper V Been and Aziz Sheikh and Zulfiqar A Bhutta and David P Burgner and Meghan B Azad}
}
@article{ALTUNTASVURAL2020100525,
title = {Can digitalization mitigate barriers to intermodal transport? An exploratory study},
journal = {Research in Transportation Business & Management},
volume = {37},
pages = {100525},
year = {2020},
issn = {2210-5395},
doi = {https://doi.org/10.1016/j.rtbm.2020.100525},
url = {https://www.sciencedirect.com/science/article/pii/S2210539519302585},
author = {Ceren {Altuntaş Vural} and Violeta Roso and Árni Halldórsson and Gabriella Ståhle and Marina Yaruta},
keywords = {Internet of things, Blockchain, Cloud logistics, Digital tools, Transport networks},
abstract = {Although it offers11This paper is developed from a master thesis done at Chalmers University of Technology: Eriksson, G., & Yaruta, M. (2018). Mapping barriers in intermodal transportation both economic and environmental advantages particularly for long-distances, intermodal transport is still not utilized effectively, and it is challenged by many barriers to its utilization. On the other hand, the transport sector is influenced by recent trends, such as digitalization, Industry 4.0 and many technological innovations are introduced, aiming to transform the way of doing business. Therefore, understanding digitalization's potential to solve the transport sector's struggle for a higher degree of integration between different modes in intermodal transport is important. The purpose of this paper is to investigate the potential of different digital tools in mitigating barriers for increased utilization of intermodal transport. Semi-structured interviews and a policy Delphi study combined with a brainstorming session are conducted to classify various barriers for intermodal transport under certain categories and to understand if and how selected digital tools could help to mitigate these barriers. Findings indicate that the multi-actor nature of intermodal transport networks results in varying perceptions related with digital tools and technologies. This variance acts as a hindrance for digitalization within the conservative context of transportation industry and its cost-based competition structure. Adoption by market leaders, demand from intermodal transport buyers, vertical integration in intermodal transport chains are potential mitigation factors for elimination of barriers to digitalization in intermodal transportation.}
}
@article{SALING2020105,
title = {Leveraging People Analytics for an Adaptive Complex Talent Management System},
journal = {Procedia Computer Science},
volume = {168},
pages = {105-111},
year = {2020},
note = {“Complex Adaptive Systems”Malvern, PennsylvaniaNovember 13-15, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.269},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304087},
author = {Kristin C. Saling and Michael D. Do},
keywords = {People analytics, data analytics, predictive analytics, AI, marketplace, talent management},
abstract = {Data analytics inform many facets of our everyday life, from Netflix recommendations to the ads that pop up on our social media feeds. This same technology can make an enormous difference in human resource and talent management enabling individuals to market their skillsets and organizations to describe their job requirements down to a granular level of detail in the hopes that searches, optimization algorithms, and simple recommendation engines can guide them towards an optimal decision for talent management – the right person in the right job at the right time. While these analytic tools are important to optimizing decisions, it is not always evident where to apply them for the best possible effect. The Army advanced analytics in a way that allows them to forecast their ability to fill critical job requirements over time by forecasting new acquisitions, promotions, and losses at the aggregate level. However, that system falls far short of being able to match people to positions in an optimal manner and results in long lag times when it comes to meeting emerging requirements. A new data collection system identifying both unit-required and individual-possessed knowledge, skills, and behaviors (KSBs) will enable the Army to make forecasts and fill positions much more rapidly (along with assigning the best person to the position) provided the data is available to decision makers at the right time to best support talent management decisions. This paper outlines the new structure of this complex human resource (HR) system from data collection to analytic tools along with showing how modeling this system illustrates an adaptive complex system based on data engineering.}
}
@article{STURKENBOOM2020B8,
title = {ADVANCE database characterisation and fit for purpose assessment for multi-country studies on the coverage, benefits and risks of pertussis vaccinations},
journal = {Vaccine},
volume = {38},
pages = {B8-B21},
year = {2020},
note = {Monitoring of Vaccine Benefits and Risks in a Public-Private Collaboration across the EU-results of the ADVANCE System Testing},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2020.01.100},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X20301535},
author = {Miriam Sturkenboom and Toon Braeye and Lieke {van der Aa} and Giorgia Danieli and Caitlin Dodd and Talita Duarte-Salles and Hanne- Dorthe Emborg and Marius Gheorghe and Johnny Kahlert and Rosa Gini and Consuelo Huerta-Alvarez and Elisa Martín-Merino and Chris McGee and Simon {de Lusignan} and Gino Picelli and Giuseppe Roberto and Lara Tramontan and Marco Villa and Daniel Weibel and Lina Titievsky},
keywords = {Electronic health data, Big data, Fit-for-purpose, Vaccination, Real world evidence},
abstract = {Introduction
The public-private ADVANCE consortium (Accelerated development of vaccine benefit-risk collaboration in Europe) aimed to assess if electronic healthcare databases can provide fit-for purpose data for collaborative, distributed studies and monitoring of vaccine coverage, benefits and risks of vaccines.
Objective
To evaluate if European healthcare databases can be used to estimate vaccine coverage, benefit and/or risk using pertussis-containing vaccines as an example.
Methods
Characterisation was conducted using open-source Java-based (Jerboa) software and R scripts. We obtained: (i) The general characteristics of the database and data source (meta-data) and (ii) a detailed description of the database population (size, representatively of age/sex of national population, rounding of birth dates, delay between birth and database entry), vaccinations (number of vaccine doses, recording of doses, pattern of doses by age and coverage) and events of interest (diagnosis codes, incidence rates). A total of nine databases (primary care, regional/national record linkage) provided data on events (pertussis, pneumonia, death, fever, convulsions, injection site reactions, hypotonic hypo-responsive episode, persistent crying) and vaccines (acellular pertussis and whole cell pertussis) related to the pertussis proof of concept studies.
Results
The databases contained data for a total population of 44 million individuals. Seven databases had recorded doses of vaccines. The pertussis coverage estimates were similar to those reported by the World Health Organisation (WHO). Incidence rates of events were comparable in magnitude and age-distribution between databases with the same characteristics. Several conditions (persistent crying and somnolence) were not captured by the databases for which outcomes were restricted to hospital discharge diagnoses.
Conclusion
The database characterisation programs and workflows allowed for an efficient, transparent and standardised description and verification of electronic healthcare databases which may participate in pertussis vaccine coverage, benefit and risk studies. This approach is ready to be used for other vaccines/events to create readiness for participation in other vaccine related studies.}
}
@article{HANG2020105251,
title = {A secure fish farm platform based on blockchain for agriculture data integrity},
journal = {Computers and Electronics in Agriculture},
volume = {170},
pages = {105251},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105251},
url = {https://www.sciencedirect.com/science/article/pii/S016816991932006X},
author = {Lei Hang and Israr Ullah and Do-Hyeun Kim},
keywords = {Internet of Things, Agriculture data integrity, Blockchain, Permissioned network, Fish farm},
abstract = {Internet of Things (IoT) has opened up a new dimension for smart farming and agriculture because of the natural feature that makes it possible to assign tasks made by a user or that transfers agriculture data obtained through sensors to producers for analysis on various terminal devices. In recent years, heightened interest in agriculture data has arisen since the commercialization of precision agriculture technology. Agriculture data are known to be messy, especially from combine yield monitors, and analysts are concerned with the validity of data, especially given that other people may have impacted data quality at various steps along the data path. The blockchain can be a possible solution to the analyst’s problem of uncertain data quality from prior data manipulation since it ensures data have not been inappropriately manipulated or at the very least documents what changes have been made by specific individuals. This paper proposes a blockchain-based fish farm platform to ensure agriculture data integrity. The designed platform aims to provide fish farmers with secure storage for preserving the large amounts of agriculture data that cannot be tampered with. Diverse processes of the fish farm are executed automatically by using the smart contract to reduce the risk of error or manipulation. A proof of concept that integrates a legacy fish farm system with the Hyperledger Fabric blockchain is implemented on top of the proposed architecture. The efficiency and usability of the proposed platform are demonstrated through a series of experiments using various metrics.}
}
@article{GHORBANI2020106646,
title = {Repurposing legacy metallurgical data Part I: A move toward dry laboratories and data bank},
journal = {Minerals Engineering},
volume = {159},
pages = {106646},
year = {2020},
issn = {0892-6875},
doi = {https://doi.org/10.1016/j.mineng.2020.106646},
url = {https://www.sciencedirect.com/science/article/pii/S0892687520304660},
author = {Yousef Ghorbani and Glen T. Nwaila and Steven E. Zhang and Martyn P. Hay and Lunga C. Bam and Pratama Istiadi Guntoro},
keywords = {Data analytics, Dry laboratories, Data bank, Legacy metallurgical data},
abstract = {Advancements in modern mineral processing has been driven by technology and fuelled by market economics of supply and demand. Over the last three decades, the demand for various minerals has steadily increased, while the mineral processing industry has seen an unavoidable increase in the treatment of complex ores, continuous decline in plant feed grade and poor plant performance partly due to blending of ores with dissimilar properties. Despite these challenges, production plant data that are routinely generated are usually underutilised. In this contribution and aligned with the direction of the 4th industrial revolution, we highlight the value of legacy metallurgical plant data and the concept of a dry laboratory approach. This study is presented in two parts. In the current paper (Part I), a comprehensive review of the potential for the combination of modern analytical technology with data analytics to generate a new competence for process optimisation are provided. To demonstrate the value of data within the extractive metallurgy discipline, we employ data analytics and simulation to examine gold plant performance and the flotation process in two separate case studies in the second paper (Part II). This was done with the aim of showcasing relevant plant data insights, and extract parameters that should be targeted for plant design and performance optimisation. We identify several promising technologies that integrate well with existing mineral processing plants and testing laboratories to exploit the concept of a dry laboratory, in order to enhance pre-existing mineral processing chains. It also sets the passage in terms of the value of innovative analysis of existing and simulation data as part of the new world of data analytics. Using data- and technology-driven initiatives, we propose the establishment of dry laboratories and data banks to ultimately leverage integrated data, analytics and process simulation for effective plant design and improved performance.}
}
@article{YANG2020333,
title = {Teenager Health Oriented Data Security and Privacy Protection Research for Smart Wearable Device},
journal = {Procedia Computer Science},
volume = {174},
pages = {333-339},
year = {2020},
note = {2019 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.06.095},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031615X},
author = {Minghui Yang and Junqi Guo and Ziyun Zhao and Tianyou Xu and Ludi Bai},
keywords = {smart wearable device, data security, singular value Decomposition, k-anonymity},
abstract = {In recent years, the fitness problems of adolescents have attracted increasing attention from all walks of life. It has become an irreversible trend to use new technical means to supervise and guide young students’ behaviour. Smart wearable devices have become a better choice because of its simple usage and convenient wearing. But its lightness also brings the shortcoming of being vulnerable to attack. This research proposes a scheme to protect the privacy of smart wearable users. There are two main aspects: (1) Perturb the data collected by smart wearing devices before sending information to prevent third-party servers from leakage of users’ privacy; and a scheme of data reconstruction is proposed using SVD (Singular Value Decomposition) method to decompose eigenvalues; (2) During data publishing, we proposed a personalized k-anonymity publishing scheme based on entropy weight method, which has brilliant adaptability as well as preventing privacy leakage. Experiments show that the proposed method can effectively improve the safety of smart wearable devices, so that adolescents and parents can use it safely.}
}
@incollection{2020xv,
title = {Subject Classification},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {xv-xvii},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-444-64165-6.09001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444641656090017}
}
@article{ADELSERHANI2020583,
title = {Self-adapting cloud services orchestration for fulfilling intensive sensory data-driven IoT workflows},
journal = {Future Generation Computer Systems},
volume = {108},
pages = {583-597},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.02.066},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19316231},
author = {M. {Adel Serhani} and Hadeel T. El-Kassabi and Khaled Shuaib and Alramzana N. Navaz and Boualem Benatallah and Amine Beheshti},
keywords = {IoT, Workflow, Sensors, Orchestration, Adaptation, Health monitoring},
abstract = {Cloud computing has been adopted to support among others the storage and processing of complex Internet of Things (IoT) workflows handling sensory streamed time-series data. IoT workflow is often composed following a set of procedures which makes it hard to self-adapt, self-configure to react to runtime environment changes. Therefore, declarative data-driven workflow composition will provision self-learning and self-configurable workflows such as those of IoT. This paper proposes a comprehensive architecture to support end-to-end workflow management processes including declarative specification and composition, configuration deployment, orchestration, execution, adaptation, and quality enforcement. The later provision runtime intelligence for IoT workflow orchestration; this is achieved through the automated monitoring and analysis of runtime cloud resource orchestration, the monitoring of workflows tasks execution, as well as through cloud resource utilization prediction and workflow adaptation. In addition, it supports other intelligent features that include: (1) integration of edge computing (sensor edge) for local data processing which is very crucial for life-critical IoT workflows, (2) data compression for fast data transmission, and data storage adaptation, and (3) customization of data reporting and visualization. All these features have been evaluated through a set of experiments that proved a significant gain in terms of workflow execution time, cost and optimum usage of cloud resources compared to baseline adaptation strategy.}
}
@article{CASADOVARA2020965,
title = {IoT network slicing on virtual layers of homogeneous data for improved algorithm operation in smart buildings},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {965-977},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19304819},
author = {Roberto Casado-Vara and Angel {Martin-del Rey} and Soffiene Affes and Javier Prieto and Juan M. Corchado},
keywords = {IoT, Complex networks, Clustering, Layer slicing, Algorithm design, Data quality},
abstract = {With its strong coverage, low energy consumption, low cost and great connectivity, the Internet of Things technology has become the key technology in smart cities. However, faced with a large number of terminals, the rational allocation of limited resources, the topology and non-uniformity of smart buildings, the fusion of heterogeneous data become important trends in Internet of Things research. As a result, this paper proposes a novel technique for processing heterogeneous temperature data collected by an IoT network in a smart building and transforms them into homogeneous data that can be used as an input for monitoring and control algorithms in smart buildings, optimizing their performance. The proposed technique, called IoT slicing, combines complex networks and clusters in order to reduce algorithm input errors and improve the monitoring and control of a smart building. For validating the efficiency of the algorithm, it is proposed as a case study using the IoT slicing technique to improve the operation of an algorithm to self-correct outliers in data collected by IoT networks. The results of the case study confirm, irrefutably, the effectiveness of the proposed method.}
}
@article{FAROQI20202621,
title = {Investigating the Correlation between Activity Similarity and Trip Similarity of Public Transit Passengers Using Smart Card Data},
journal = {Transportation Research Procedia},
volume = {48},
pages = {2621-2637},
year = {2020},
note = {Recent Advances and Emerging Issues in Transport Research – An Editorial Note for the Selected Proceedings of WCTR 2019 Mumbai},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.08.249},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520306682},
author = {Hamed Faroqi and Mahmoud Mesbah and Jiwon Kim},
keywords = {Travel behavior, big data, data mining, trip purpose, space-time},
abstract = {The pattern of activities by transit passengers and the factors affecting this pattern are recently addressed in the literature. Also, the pattern of trips and the factors affecting trips are the subject of many studies. However, little is known about the correlation between the activity and trip of passengers in the public transit network. This study investigates how similar are activities of passengers if they have similar trips (or vice versa)? And what factors impact the similarity of the activity and trip of passengers? Answering these questions is useful in understanding the mobilization patterns of passengers and developing group-based transit services. Also, smart card data have provided an opportunity, which was not available before, to analyze the activity and trip of the passengers in a large scale network. In this paper, the correlation between activity similarity and trip similarity of public transit passengers is investigated. The correlation between the activity and trip of the passengers is analyzed using histograms, Pearson correlation coefficient, conditional probabilities, and hexagonal binning technique. In addition, the impact of trip length and duration on the activity and trip similarity are examined using histograms and hexagonal binning diagrams. The proposed methodology is implemented for two-day smart card data in Brisbane, Australia. Results show that it is more likely to have the activity similarity when there is a trip similarity than having the trip similarity when there is activity similarity. Also, there is a nonlinear correlation between the activity and trip similarity with the trip length and duration.}
}
@article{PERAKA2020103336,
title = {Pavement asset management systems and technologies: A review},
journal = {Automation in Construction},
volume = {119},
pages = {103336},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103336},
url = {https://www.sciencedirect.com/science/article/pii/S092658052030916X},
author = {Naga Siva Pavani Peraka and Krishna Prapoorna Biligiri},
keywords = {Pavement asset management System, Image processing, Artificial intelligence, Machine learning, Pavement condition index},
abstract = {Pavement asset management system (PAMS) assists agencies and decision makers to maintain deteriorating pavement assets with optimized budget allocation. The recent developments in pavement condition data collection and processing have significant effect on estimating remaining service life and selecting optimum maintenance strategies. Further, image processing (IP) and artificial intelligence (AI) tools have improved the overall performance of PAMS by helping analyze big data emanating from distress surveys. The objective of this review paper was to collect and report several current state-of-the-art developments in PAMS and the associated embedded processes, majorly focused on data collection procedures, analytical techniques, decision making tools, and processing methods. The shift from manual condition surveys to automated pavement condition surveys has profusely improved data collection rate. The wide-range of data collection methods, manual, automated vehicles, and cost-effective methods followed across the globe were reviewed. Further, the chronological development in data analysis, specifically, distress evaluation, homogeneous sectioning for selection of maintenance strategies, and prioritization and optimization of maintenance strategies were discussed while emphasizing the application of IP and AI in enhancing the efficacy of PAMS. In addition, this paper provided a narrative account of the interdisciplinary research and multi-scale developments that recognize the value-addition of cutting-edge technologies in AI and computer vision.}
}
@article{ZHANG2020102324,
title = {HKGB: An Inclusive, Extensible, Intelligent, Semi-auto-constructed Knowledge Graph Framework for Healthcare with Clinicians’ Expertise Incorporated},
journal = {Information Processing & Management},
volume = {57},
number = {6},
pages = {102324},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102324},
url = {https://www.sciencedirect.com/science/article/pii/S0306457320308190},
author = {Yong Zhang and Ming Sheng and Rui Zhou and Ye Wang and Guangjie Han and Han Zhang and Chunxiao Xing and Jing Dong},
keywords = {Health knowledge graph, clinician-in-the-loop, platform, knowledge island, healthcare, health informatics},
abstract = {Health knowledge graph provides an ideal technical means to integrate heterogeneous data resources and enhance knowledge-based services. There are many challenges for the construction of health knowledge graph such as complex concepts and relationships, various medical standards, heterogeneous data structures, poor data quality, highly accurate and interpretable services, etc. In this paper, firstly, we propose Health Knowledge Graph Builder (HKGB), an end-to-end platform which could be used to construct disease-specific and extensible health knowledge graphs from multiple sources. Secondly, we analyze the capabilities and requirements of clinicians, design the tasks to involve the clinicians and implement a clinician-in-the-loop toolset to integrate the clinicians prior knowledge into the construction of health knowledge graphs. Thirdly, we design an extensible mechanism to add new diseases to an existing knowledge graph. Fourthly, we present a quantitative effort estimation algorithm to quantitatively evaluate the effort of clinicians during the construction, and use it to calculate the workloads such as 44.27 person days for knee osteoarthritis domain. Finally, we have developed several knowledge graph based tools to facilitate real applications.}
}
@article{ANDERSON2020587,
title = {COVID-19 spread in the UK: the end of the beginning?},
journal = {The Lancet},
volume = {396},
number = {10251},
pages = {587-590},
year = {2020},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(20)31689-5},
url = {https://www.sciencedirect.com/science/article/pii/S0140673620316895},
author = {Roy M Anderson and T Déirdre Hollingsworth and Rebecca F Baggaley and Rosie Maddren and Carolin Vegvari}
}
@article{COMBITANINO2020405,
title = {Business intelligence governance framework in a university: Universidad de la costa case study},
journal = {International Journal of Information Management},
volume = {50},
pages = {405-412},
year = {2020},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217310320},
author = {Harold Arturo {Combita Niño} and Johana Patricia {Cómbita Niño} and Roberto {Morales Ortega}},
keywords = {Business intelligence, Governance, University, Analytics, Decision making},
abstract = {Universities and companies have decision-making processes that allow to achieve institutional objectives. Currently, data analysis has an important role in generating knowledge, obtaining important patterns and predictions for formulating strategies. This article presents the design of a business intelligence governance framework for the Universidad de la Costa, easily replicable in other institutions. For this purpose, a diagnosis was made to identify the level of maturity in analytics. From this baseline, a model was designed to strengthen organizational culture, infrastructure, data management, data analysis and governance. The proposal contemplates the definition of a governance framework, guiding principles, strategies, policies, processes, decision-making body and roles. Therefore, the framework is designed to implement effective controls that ensure the success of business intelligence projects, achieving an alignment of the objectives of the development plan with the analytical vision of the institution.}
}
@article{STAHL2020274,
title = {Summary perioperative risk metrics within the electronic medical record predict patient-level cost variation in pancreaticoduodenectomy},
journal = {Surgery},
volume = {168},
number = {2},
pages = {274-279},
year = {2020},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2020.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0039606020301173},
author = {Christopher C. Stahl and Patrick B. Schwartz and Glen E. Leverson and James R. Barrett and Taylor Aiken and Alexandra W. Acher and Sean M. Ronnekleiv-Kelly and Rebecca M. Minter and Sharon M. Weber and Daniel E. Abbott},
abstract = {Background
Automated data extraction from the electronic medical record is fast, scalable, and inexpensive compared with manual abstraction. However, concerns regarding data quality and control for underlying patient variation when performing retrospective analyses exist. This study assesses the ability of summary electronic medical record metrics to control for patient-level variation in cost outcomes in pancreaticoduodenectomy.
Methods
Patients that underwent pancreaticoduodenectomy from 2014 to 2018 at a single institution were identified within the electronic medical record and linked with the National Surgical Quality Improvement Program. Variables in both data sets were compared using interrater reliability. Logistic and linear regression modelling of complications and costs were performed using combinations of comorbidities/summary metrics. Models were compared using the adjusted R2 and Akaike information criterion.
Results
A total of 117 patients populated the final data set. A total of 31 (26.5%) patients experienced a complication identified by the National Surgical Quality Improvement Program. The median direct variable cost for the encounter was US$14,314. Agreement between variables present in the electronic medical record and the National Surgical Quality Improvement Program was excellent. Stepwise linear regression models of costs, using only electronic medical record–extractable variables, were non-inferior to those created with manually abstracted individual comorbidities (R2 = 0.67 vs 0.30, Akaike information criterion 2,095 vs 2,216). Model performance statistics were minimally impacted by the addition of comorbidities to models containing electronic medical record summary metrics (R2 = 0.67 vs 0.70, Akaike information criterion 2,095 vs 2,088).
Conclusion
Summary electronic medical record perioperative risk metrics predict patient-level cost variation as effectively as individual comorbidities in the pancreaticoduodenectomy population. Automated electronic medical record data extraction can expand the patient population available for retrospective analysis without the associated increase in human and fiscal resources that manual data abstraction requires.}
}
@article{HINDLE2020483,
title = {Business analytics: Defining the field and identifying a research agenda},
journal = {European Journal of Operational Research},
volume = {281},
number = {3},
pages = {483-490},
year = {2020},
note = {Featured Cluster: Business Analytics: Defining the field and identifying a research agenda},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719308173},
author = {Giles Hindle and Martin Kunc and Michael Mortensen and Asil Oztekin and Richard Vidgen},
keywords = {Business analytics, Practice of OR, Data science, Topic models, Computational literature review},
abstract = {The special issue on business analytics has been a great endeavor with more than 100 papers received. The call for papers highlighted that business analytics has a clear role to generate competitive advantage in organizations and our focus has been to demonstrate this role through the papers finally selected for the special issue. The editorial aims to provide not only a summary of the papers but also presents our perspective on the current situation of the field through a computational literature review and comparison with the papers in the special issue. Our findings, and discussions on the papers included in the special issue, suggest that business analytics is maturing as a field with significant synergies and opportunities for the operational research community.}
}
@article{KU2020106297,
title = {Digital transformation to empower smart production for Industry 3.5 and an empirical study for textile dyeing},
journal = {Computers & Industrial Engineering},
volume = {142},
pages = {106297},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106297},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220300310},
author = {Chien-Chun Ku and Chen-Fu Chien and Kang-Ting Ma},
keywords = {Smart production, Digital transformation, Textile dyeing machine scheduling, Industry 3.5, Decision support system, Traditional industry},
abstract = {Most of traditional industries in emerging countries may not be ready to migrate for Industry 4.0 directly. There is a need of effective solutions to support digital transformation of traditional industries. Textile industry is facing global competition for mass customization to address dynamic customer demands. To enable the challenge from mass production to build-on-demand with small lot size and diversified product mixes, this study aims to develop a solution to support traditional industries to adopt smart manufacturing and empower digital transformation. Following a framework as systematic approach to collect, identify, and analyze related steps and decisions for an organization, a decision support system for dyeing machine scheduling is developed to empower smart manufacturing and break down information silos. In particular, setup time for textile dyeing operations is sequence-dependent, and products of different types and colors require setups for tank cleaning. The results have shown the practical viability of the proposed approach and Industry 3.5. Indeed, the developed solution has been implemented in a textile company in Taiwan.}
}
@article{SANA2020103996,
title = {Artificial intelligence in celiac disease},
journal = {Computers in Biology and Medicine},
volume = {125},
pages = {103996},
year = {2020},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2020.103996},
url = {https://www.sciencedirect.com/science/article/pii/S0010482520303279},
author = {Muhammad Khawar Sana and Zeshan M. Hussain and Pir Ahmad Shah and Muhammad Haisum Maqsood},
keywords = {Artificial intelligence, Machine learning, Deep learning, Celiac disease, Systematic review},
abstract = {Celiac disease (CD) has been on the rise in the world and a large part of it remains undiagnosed. Novel methods are required to address the gaps in prompt detection and management. Artificial intelligence (AI) has seen an exponential surge in the last decade worldwide. With the advent of big data and powerful computational ability, we now have self-driving cars and smart devices in our daily lives. Huge databases in the form of electronic medical records and images have rendered healthcare a lucrative sector where AI can prove revolutionary. It is being used extensively to overcome the barriers in clinical workflows. From the perspective of a disease, it can be deployed in multiple steps i.e. screening tools, diagnosis, developing novel therapeutic agents, proposing management plans, and defining prognostic indicators, etc. We review the areas where it may augment physicians in the delivery of better healthcare by summarizing current literature on the use of AI in healthcare using CD as a model. We further outline major barriers to its large-scale implementations and prospects from the healthcare point of view.}
}
@article{ZHANG2020121774,
title = {Exploring the influencing factors of public environmental satisfaction based on socially aware computing},
journal = {Journal of Cleaner Production},
volume = {266},
pages = {121774},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.121774},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620318217},
author = {Qiang Zhang and Tianze Gao and Xueyan Liu and Yun Zheng},
keywords = {Atmospheric environment, Socially aware computing, Participatory sensing, Analysis of correlation},
abstract = {As the problem of urban environment and air pollution continues to intensify, the public has developed a very strong perception of the surrounding environment. With this growing perception, their emotions and satisfaction about the environment may also be greatly affected. This article explores the main factors that affect the public’s environmental satisfaction with two main novelties. The first is to develop a data acquisition platform. Based on socially aware computing and a participatory sensing method, the public’s subjective environmental satisfaction is measured by quantifying the collected sensing data. The second is to establish a multiple regression model and a correlation model to analyze the relationship between public’s environmental satisfaction and atmospheric environmental factors, and to compare the influence of each factor. Lanzhou City was taken as a case study, and a total of 33505 data points was acquired over a year of experiment. The experimental results show: (1) Air pollution in Lanzhou affects public environmental satisfaction more strangely than meteorological factors. (2) There is a negative correlation between public environmental satisfaction and different air pollutants in Lanzhou. (3) PM2.5, PM10 and NO2 are the main influencing factors of the public’s environmental satisfaction in Lanzhou. Overall, our study demonstrates a viable method for capturing public environmental satisfaction and identifying probable drivers of it, and our results provide important information for policy makers to prioritize areas that can more effectively improve public satisfaction.}
}
@article{VILORIA202096,
title = {Segmentation process and spectral characteristics in the determination of musical genres},
journal = {Procedia Computer Science},
volume = {175},
pages = {96-101},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920316951},
author = {Amelec Viloria and Omar Bonerge {Pineda Lezama} and Danelys Cabrera},
keywords = {Supervised learning algorithms, Music genres classification, Centroid (SC), Flatness (SF), Spread (SS)},
abstract = {Over the past few years there has been a tendency to store audio tracks for later use on CD-DVDs, HDD-SSDs as well as on the internet, which makes it challenging to classify the information either online or offline. For this purpose, the audio tracks must be tagged. Tags are said to be texts based on the semantic information of the sound [1]. Thus, music analysis can be done in several ways [2] since music is identified by its genre, artist, instruments and structure, by a tagging system that can be manual or automatic. The manual tagging allows the visualization of the behavior of an audio track either in time domain or in frequency domain as in the spectrogram, making it possible to classify the songs without listening to them. However, this process is very time consuming and labor intensive, including health problems [3] which shows that "the volume, sound sensitivity, time and cost required for a manual labeling process is generally prohibitive. Three fundamental steps are required to carry out automatic labelling: pre-processing, feature extraction and classification [4]. The present study developed an algorithm for performing automatic classification of music genres using a segmentation process employing spectral characteristics such as centroid (SC), flatness (SF) and spread (SS), as well as a time spectral characteristic.}
}
@incollection{VASILEIOU2020229,
title = {10 - Analytic methods for systems medicine},
editor = {Stephen T. Sonis and Allessandro Villa},
booktitle = {Translational Systems Medicine and Oral Disease},
publisher = {Academic Press},
pages = {229-248},
year = {2020},
isbn = {978-0-12-813762-8},
doi = {https://doi.org/10.1016/B978-0-12-813762-8.00010-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128137628000104},
author = {Panagiotis V.S. Vasileiou and Gkikas Magiorkinis and Pagona Lagiou and Vassilis Gorgoulis},
keywords = {Analytics, Big data, Data mining, Data preprocessing, Integration, Systems medicine},
abstract = {The outburst of computing power and the advent of high-throughput technologies facilitated medicine to enter the era of systems approach. The basic principle of systems medicine is that the human organism is a mosaic of interacting subnetworks. In this regard, analytic methods should account for this multidimensional and dynamic nature of interdependence. Based on various computational models and novel mathematical methodologies, analytic methods in systems medicine are designed to handle input from all relevant levels within the patient's system, from the microscopic to the macroscopic. This chapter introduces data sources used in systems medicine and challenges that are raised on all aspects of workflow due to inherent quality issues, high dimensionality, and heterogeneity of biomedical data. It then explains the sequential steps followed during analysis, namely data preprocessing and data mining.}
}
@article{BROUS2020101952,
title = {The dual effects of the Internet of Things (IoT): A systematic review of the benefits and risks of IoT adoption by organizations},
journal = {International Journal of Information Management},
volume = {51},
pages = {101952},
year = {2020},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218309022},
author = {Paul Brous and Marijn Janssen and Paulien Herder},
keywords = {Internet of things, IoT, Adoption, Big and open linked data, Case study, Asset management, Smart cities, Duality of technology, Structuration theory},
abstract = {The Internet of Things (IoT) might yield many benefits for organizations, but like other technology adoptions may also introduce unforeseen risks and requiring substantial organizational transformations. This paper analyzes IoT adoption by organizations, and identifies IoT benefits and risks. A Big, Open, Linked Data (BOLD) categorization of the expected benefits and risks of IoT is made by conducting a comprehensive literature study. In-depth case studies in the field of asset management were then executed to examine the actual experienced, real world benefits and risks. The duality of technology is used as our theoretical lens to understand the interactions between organization and technology. The results confirm the duality that gaining the benefits of IoT in asset management produces unexpected social changes that lead to structural transformation of the organization. IoT can provide organizations with many benefits, after having dealt with unexpected risks and making the necessary organizational changes. There is a need to introduce changes to the organization, processes and systems, to develop capabilities and ensure that IoT fits the organization’s purposes.}
}
@article{ZAMANI2020107,
title = {An edge-aware autonomic runtime for data streaming and in-transit processing},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {107-118},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19307265},
author = {Ali Reza Zamani and Daniel Balouek-Thomert and J.J. Villalobos and Ivan Rodero and Manish Parashar},
keywords = {Stream-oriented workflows, Edge processing, In-transit processing, Wide-area analytics},
abstract = {One of the major endeavors of modern cyberinfrastructure (CI) is to carry content produced on remote data sources, such as sensors and scientific instruments, and to deliver it to end users and workflow applications. Maintaining data quality, data resolution, and on-time data delivery and considering the increasing number of computing, storage, and network resources are challenging tasks that require a receptive system able to adapt to ever-changing demands. In this paper, we propose a mathematical model of such system by expressing the dynamic stages of different resources in the context of edge and in-transit computing. By considering resource utilization, approximation techniques, and user constraints, our proposed model generates mappings of different workflow stages on heterogeneous geographically distributed resources. Specifically, we propose an autonomic runtime management layer that adapts the data resolution being delivered to the users by implementing feedback loops over the resources involved in the delivery and processing of data streams. The implementation of our model is based on a subscription-based data streaming framework that enables the integration of large facilities and advanced CI. Moreover, the idea of stream or request aggregation is incorporated into our framework, which eliminates redundant data streams to save bandwidth. Experimental results show that dynamically adapting data resolution and stream aggregation can overcome bandwidth limitations in wide-area streaming analytics by leveraging the resources at the edge and in-transit.}
}
@incollection{PEZOULAS2020337,
title = {Chapter 9 - Conclusions and future trends},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {337-344},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000098},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Conclusions, Data analytics, Data harmonization, Data sharing, Future trends},
abstract = {This chapter provides a comprehensive summary of what has been presented so far regarding the current advances and future trends in medical data sharing, harmonization, and analytics. The overall workflow of a federated health research and innovation cloud platform, which constitutes the backbone of this book, is summarized starting from medical data protection and data sharing to medical data harmonization and data analytics. A short description of the highlights and the main key points is provided for each individual chapter along with a brief description of the scientific impact and value of this book. The chapter concludes with future trends in the exciting but challenging domain of medical data sharing, harmonization, and analytics.}
}
@article{KANG2020103556,
title = {Creating a database for health IT events via a hybrid deep learning model},
journal = {Journal of Biomedical Informatics},
volume = {110},
pages = {103556},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103556},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301842},
author = {Hong Kang and Yang Gong},
keywords = {Patient safety, Medical error, Health information technology, Deep learning},
abstract = {Objective
The use of poorly designed and improperly implemented health information technology (HIT) may compound risks because it can disrupt established work patterns and encourage workarounds. Analyzing and learning from HIT events could reduce the risks and improve safety but are limited by accessible HIT event reports. In this study, we propose a hybrid deep learning model to identify HIT event reports from the FDA resource and thus establish the first publicly accessible database for HIT event reports.
Materials and methods
6994 samples (3521 HIT and 3473 non-HIT events) extracted from the FDA MAUDE database were employed to assess nine individual and 120 hybrid models on the task of HIT identification. The optimal model was evaluated on an independent dataset prior to its application for establishing the HIT event database.
Results
The hybrid model consisting of logistic regression, CNN, and Hierarchical RNN (ACC = 0.903, AUC = 0.954, F1 score = 0.876) is superior to all the other models. The causes of errors include lack of root cause (72.3%), short descriptions (19.7%), and model undertrained (8.0%). The accuracy of the hybrid model on an independent dataset is reported as 0.862. We applied the optimal model to the entire MAUDE database (1991–2018) and generated an HIT event database with 48,997 reports.
Conclusion
The first HIT event database contains 48,997 reports with an annual growth rate of 10% (~5000 reports). The strategy of HIT event identification and establishment of the database could help healthcare professionals describe, understand, integrate the events and propose solutions in the context of a fuller spectrum of HIT events.}
}
@incollection{PEZOULAS20201,
title = {Chapter 1 - Introduction},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {1-18},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000013},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Cloud infrastructures, Data analytics, Data harmonization, Data mining, Data protection, Data sharing, Federated platform},
abstract = {This chapter introduces the fundamental principles behind medical data sharing and data governance, data protection, data harmonization, cloud infrastructure, and data analytics. Initially, the different origins of medical data are presented. The main idea of medical data sharing is then described followed by the data protection legislation to present key regulatory aspects of data sharing. The concept of data harmonization is also presented. Cloud infrastructure is also discussed to enable data sharing and at the same time to ensure the privacy of the data. Emphasis is given on data mining and machine learning tools to analyze the data in a federated manner. The book's structure and contents per chapter are finally provided.}
}
@article{AYDOGAN2020123288,
title = {Improving the accuracy using pre-trained word embeddings on deep neural networks for Turkish text classification},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {541},
pages = {123288},
year = {2020},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2019.123288},
url = {https://www.sciencedirect.com/science/article/pii/S0378437119318436},
author = {Murat Aydoğan and Ali Karci},
keywords = {Deep learning, Word embedding, Turkish text classification, Text processing},
abstract = {Today, extreme amounts of data are produced, and this is commonly referred to as Big Data. A significant amount of big data is composed of textual data, and as such, text processing has correspondingly increased in its importance. This is especially valid to the development of word embedding and other groundbreaking advancements in this field. However, When studies on text processing and word embedding are examined, it can be seen that while there have been many world language-oriented studies, especially for the English language, there has been an insufficient level of study undertaken specific to the Turkish language. As a result, Turkish was chosen as the target language for the current study. Two Turkish datasets were created for this study. Word vectors were trained using the Word2Vec method on an unlabeled large corpus of approximately 11 billion words. Using these word vectors, text classification was applied with deep neural networks on a second dataset of 1.5 million examples and 10 classes. The current study employed the Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and the Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) methods – other types of this architecture – and their variations as deep neural network architectures. The performances of the embedding methods for the words used in this study, their effects on the rate of accuracy, and the success of the deep neural network architectures were then analyzed in detail. When studying the experimental results, it was determined that the GRU and LSTM methods were more successful compared to the other deep neural network models used in this study. The results showed that the pre-trained word vectors’ (PWVs) accuracy on deep neural networks improved at rates of approximately 5% and 7%. The datasets and word vectors of the current study will be shared in order to contribute to the Turkish language literature in this field.}
}
@article{GUO2020106630,
title = {The internet of things-based decision support system for information processing in intelligent manufacturing using data mining technology},
journal = {Mechanical Systems and Signal Processing},
volume = {142},
pages = {106630},
year = {2020},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2020.106630},
url = {https://www.sciencedirect.com/science/article/pii/S0888327020300169},
author = {Yuan Guo and Nan Wang and Ze-Yin Xu and Kai Wu},
keywords = {Intelligent decision support system, Data mining, Decision tree},
abstract = {To comprehensively understand the decision information system for the information processing of the intelligent manufacturing under Internet of Things, an intelligent decision support system (DSS) based on data mining technology is applied to enterprises to establish an Internet of Things-based intelligent DSS for manufacturing industry, thereby supporting the decision-makers in making intelligent decisions through the intelligent DSS. The research results show that data mining technology can analyze the statistical data from multiple angles and perspectives by modeling, classifying, and clustering a large amount of data, as well as discovering the correlations between the data. Also, in statistical work, the data are counted, and their correlations are utilized to support the decision analysis. Therefore, it can be concluded that the establishment of intelligent DSS for enterprises in manufacturing industry and the utilization of data mining technology as the key technology to achieve the system can make the decision-making of the manufacturing enterprises more effective and scientific. Eventually, the satisfactory decision-making results can be obtained.}
}
@article{SCHAEFFER2020101918,
title = {Forecasting client retention — A machine-learning approach},
journal = {Journal of Retailing and Consumer Services},
volume = {52},
pages = {101918},
year = {2020},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2019.101918},
url = {https://www.sciencedirect.com/science/article/pii/S0969698919302668},
author = {Satu Elisa Schaeffer and Sara Veronica {Rodriguez Sanchez}},
keywords = {Client retention, Sales forecasting, Machine learning, Prepaid unitary services},
abstract = {In the age of big data, companies store practically all data on any client transaction. Making use of this data is commonly done with machine-learning techniques so as to turn it into information that can be used to drive business decisions. Our interest lies in using data on prepaid unitary services in a business-to-business setting to forecast client retention: whether a particular client is at risk of being lost before they cease being clients. The purpose of such a forecast is to provide the company with an opportunity to reach out to such clients as an effort to ensure their retention. We work with monthly records of client transactions: each client is represented as a series of purchases and consumptions. We vary (1) the length of the time period used to make the forecast, (2) the length of a period of inactivity after which a client is assumed to be lost, and (3) how far in advance the forecast is made. Our experimental work finds that current machine-learning techniques able to adequately predict, well in advance, which clients will be lost. This knowledge permits a company to focus marketing efforts on such clients as early as three months in advance.}
}
@article{ZIAKOPOULOS2020203,
title = {A critical overview of driver recording tools},
journal = {Journal of Safety Research},
volume = {72},
pages = {203-212},
year = {2020},
issn = {0022-4375},
doi = {https://doi.org/10.1016/j.jsr.2019.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0022437519306796},
author = {Apostolos Ziakopoulos and Dimitrios Tselentis and Armira Kontaxi and George Yannis},
keywords = {Driver behavior, Driver recording tools, Tool comparison, Smartphone data, Naturalistic driving},
abstract = {Introduction: Technological advancements during recent decades have led to the development of a wide array of tools and methods in order to record driving behavior and measure various aspects of driving performance. The aim of the present study is to present and comparatively assess the various driver recording tools that researchers have at their disposal. Method: In order to achieve this aim, a multitude of published studies from the international literature have been examined based on the driver recording methodologies that have been implemented. An examination of more traditional survey methods (questionnaires, police reports, and direct observer methods) is initially conducted, followed by investigating issues pertinent to the use of driving simulators. Afterwards, an extensive section is provided for naturalistic driving data tools, including the utilization of on-board diagnostics (OBD) and in-vehicle data recorders (IVDRs). Lastly, in-depth incident analysis and the exploitation of smartphone data are discussed. Results: A critical synthesis of the results is conducted, providing the advantages and disadvantages of utilizing each tool and including additional knowledge regarding ease of experimental implementation, data handling issues, impacts on subsequent analyses, as well as the respective cost parameters. Conclusions: New technologies provide undeniably powerful tools that allow for seamless data handling, storage, and analysis, such as smartphones and in-vehicle data recorders. However, this sometimes comes at considerable costs (which may or may not pay off at a later stage), while legacy driver recording methods still have their own niches to fill in research. Practical Applications: The present research supports researchers when designing driver behavior monitoring studies. The present work enables better scheduling and pacing of research activities, but can also provide insights for the distribution of research funds.}
}
@article{GARCIALOZANO2020113132,
title = {Veracity assessment of online data},
journal = {Decision Support Systems},
volume = {129},
pages = {113132},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113132},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301617},
author = {Marianela {García Lozano} and Joel Brynielsson and Ulrik Franke and Magnus Rosell and Edward Tjörnhammar and Stefan Varga and Vladimir Vlassov},
keywords = {Veracity assessment, Credibility, Data quality, Online data, Social media, Fake news},
abstract = {Fake news, malicious rumors, fabricated reviews, generated images and videos, are today spread at an unprecedented rate, making the task of manually assessing data veracity for decision-making purposes a daunting task. Hence, it is urgent to explore possibilities to perform automatic veracity assessment. In this work we review the literature in search for methods and techniques representing state of the art with regard to computerized veracity assessment. We study what others have done within the area of veracity assessment, especially targeted towards social media and open source data, to understand research trends and determine needs for future research. The most common veracity assessment method among the studied set of papers is to perform text analysis using supervised learning. Regarding methods for machine learning much has happened in the last couple of years related to the advancements made in deep learning. However, very few papers make use of these advancements. Also, the papers in general tend to have a narrow scope, as they focus on solving a small task with only one type of data from one main source. The overall veracity assessment problem is complex, requiring a combination of data sources, data types, indicators, and methods. Only a few papers take on such a broad scope, thus, demonstrating the relative immaturity of the veracity assessment domain.}
}
@article{COITO2020103329,
title = {A Middleware Platform for Intelligent Automation: An Industrial Prototype Implementation},
journal = {Computers in Industry},
volume = {123},
pages = {103329},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103329},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305637},
author = {Tiago Coito and Miguel S.E. Martins and Joaquim L. Viegas and Bernardo Firme and João Figueiredo and Susana M. Vieira and João M.C. Sousa},
keywords = {Intelligent Automation, Interoperability, Middleware, Fog computing, Data Preparation, Industry 4.0},
abstract = {The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by artificial intelligence, delivering smart manufacturing and mass customization while improving resource efficiency. However, challenges towards the development of intelligent automation architectures include the lack of interoperability between systems, complex data preparation steps, and the inability to deal with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial frameworks focused on the development of standardized system architectures for Industry 4.0, closing the gap between generic architectures and physical realizations. It proposes a platform for intelligent automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of the challenges are handled using fog computing and a multi-workload database. This paper presents an implementation of the platform in the pharmaceutical industry, providing interoperability and real-time reaction capability to changes to an industrial prototype using dynamic scheduling algorithms.}
}
@article{WELTE2020909,
title = {A Method for Implementation of Machine Learning Solutions for Predictive Maintenance in Small and Medium Sized Enterprises},
journal = {Procedia CIRP},
volume = {93},
pages = {909-914},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.052},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120306223},
author = {Rebecca Welte and Manfred Estler and Dominik Lucke},
keywords = {Predicitve Maintenance, Machine Learning, Implementation Method},
abstract = {In recent years, machine learning algorithms have made a huge development in performance and applicability in industry and especially maintenance. Their application enables predictive maintenance and thus offers significant efficiency increases. However, a successful implementation of such solutions still requires high effort in data preparation to obtain the right information, interdisciplinarity in teams as well as a good communication to employees. Here, small and medium sized enterprises (SME) often lack in experience, competence and capacity. This paper presents a systematic and practice-oriented method for an implementation of machine learning solutions for predictive maintenance in SME, which has already been validated.}
}
@article{THAKURIAH2020101427,
title = {Integrated Multimedia City Data (iMCD): A composite survey and sensing approach to understanding urban living and mobility},
journal = {Computers, Environment and Urban Systems},
volume = {80},
pages = {101427},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2019.101427},
url = {https://www.sciencedirect.com/science/article/pii/S0198971519301929},
author = {Piyushimita (Vonu) Thakuriah and Katarzyna Sila-Nowicka and Jinhyun Hong and Christina Boididou and Michael Osborne and Catherine Lido and Andrew McHugh},
keywords = {Wearable sensors, Image data, Urban metabolism, Travel behavior, Social media, Smart cities},
abstract = {We describe the Integrated Multimedia City Data (iMCD), a data platform involving detailed person-level self-reported and sensed information, with additional Internet, remote sensing, crowdsourced and environmental data sources that measure the wider social, economic and physical context of the participant. Selected aspects of the platform, which covers the Glasgow, UK, city-region, are available to other researchers, and allows knowledge discovery on critical urban living themes, for example in transportation, lifelong learning, sustainable behavior, social cohesion, ways of being in a digital age, and other topics. It further allows research into the technological and methodological aspects of emerging forms of urban data. Key highlights of the platform include a multi-topic household and person-level survey; travel and activity diaries; a privacy and personal device sensitivity survey; a rich set of GPS trajectory data; accelerometer, light intensity and other personal environment sensor data from wearable devices; an image data collection at approximately 5-second resolution of participants’ daily lives; multiple forms of text-based and multimedia Internet data; high resolution satellite and LiDAR data; and data from transportation, weather and air quality sensors. We demonstrate the power of the platform in understanding personal behavior and urban patterns by means of three examples: an examination of the links between mobility and literacy/learning using the household survey, a social media analysis of urban activity patterns, and finally, the degree of physical isolation levels using deep learning algorithms on image data. The analysis highlights the importance of purposefully designed multi-construct and multi-instrument data collection approaches that are driven by theoretical frameworks underpinning complex urban challenges, and the need to link to policy frameworks (e.g., Smart Cities, Future Cities, UNESCO Learning Cities agendas) that have the potential to translate data to impactful decision-making.}
}
@article{XIANGDONG2020715,
title = {Asset management of oil and gas pipeline system Based on Digital Twin},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {715-719},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.163},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321003037},
author = {Xue Xiangdong and Li Bo and Gai Jiannan},
keywords = {Digital Twin, Asset Management, Intelligent Pipeline, Oil, Gas Pipeline System, Equipment},
abstract = {With the rapid development of IT technologies, digital transformation is reshaping the world in multiple scales. Digital Twin, as an important enabling technology and driving force of digital transformation, has been widely used in various fields, which has played a good exemplary role in the application and development of digital twin technology in the pipeline industry. This paper discusses the background, definition and framework of the digital twin in oil and gas pipeline system, analyzes the existing problems in pipeline asset management and equipment asset management, and analyzes the application prospect of digital twin in oil and gas pipeline asset management. In the end, it is pointed out that the construction of digital twin cannot be accomplished overnight. There is still a long way to go before it can be applied and popularized in the industry. Finally, it can help the safe, efficient and environmental protection operation of oil and gas pipeline network.}
}
@incollection{POORTHUIS2020137,
title = {Geotag},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {137-140},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10551-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105517},
author = {Ate Poorthuis},
keywords = {Big data, Data science, Digital geography, Geocomputation, Georeferencing, Geotag, Geoweb, Internet geography, Social media data, Volunteered geographic information},
abstract = {The geotag is a geographic identifier that can be added to any digital object (e.g., a photo or a message). Enabled by a host of new technologies that emerged around the turn of the century, today much digital content has such a geotag attached. These large, geotagged datasets, ranging from simple websites and Wikipedia to social media platforms and transportation records, affect the world around us, as well as how we conduct geographic research about that world. Although a relatively humble concept, the geotag has enabled an entire wave of new types of geographic research that are often labeled as “digital geography.”}
}
@article{ZHUANG2020245,
title = {The Next Breakthroughs of Artificial Intelligence: The Interdisciplinary Nature of AI},
journal = {Engineering},
volume = {6},
number = {3},
pages = {245-247},
year = {2020},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2020.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S209580992030028X},
author = {Yueting Zhuang and Ming Cai and Xuelong Li and Xiangang Luo and Qiang Yang and Fei Wu}
}
@article{HOSSAIN2020102187,
title = {Revisiting customer analytics capability for data-driven retailing},
journal = {Journal of Retailing and Consumer Services},
volume = {56},
pages = {102187},
year = {2020},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2020.102187},
url = {https://www.sciencedirect.com/science/article/pii/S0969698919314523},
author = {Md Afnan Hossain and Shahriar Akter and Venkata Yanamandram},
keywords = {Customer analytics capability, Data-driven retailing, Outside-in marketing, Systematic literature review, Thematic analysis},
abstract = {Customer analytics is one of the most dominant strategic weapons in today's competitive retail environment. In spite of its strategic importance, there is scant attention to investigating customer analytics capabilities in the retail context. Drawing on a systematic literature review and thematic analysis, this study proposes a multidimensional customer analytics capability model by identifying relevant dimensions and sub-dimensions in retail settings. The principal contribution of this study is that the model links a customer analytics perspective to a resource-based view (RBV)-capability of the retailers by proposing six customer analytics capability dimensions and twelve sub-dimensions in the spectrum of market orientation and technology orientation. The customer analytics capability dimensions depict three crucial themes of marketing, such as value creation (offering capability and personalization capability), value delivery (distribution capability and communication capability), and value management (data management capability and data protection capability). By incorporating this capability dimensions, practitioners will likely be able to engage customers and enhance customer equity.}
}
@article{GASSER2020e425,
title = {Digital tools against COVID-19: taxonomy, ethical challenges, and navigation aid},
journal = {The Lancet Digital Health},
volume = {2},
number = {8},
pages = {e425-e434},
year = {2020},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(20)30137-0},
url = {https://www.sciencedirect.com/science/article/pii/S2589750020301370},
author = {Urs Gasser and Marcello Ienca and James Scheibner and Joanna Sleigh and Effy Vayena},
abstract = {Summary
Data collection and processing via digital public health technologies are being promoted worldwide by governments and private companies as strategic remedies for mitigating the COVID-19 pandemic and loosening lockdown measures. However, the ethical and legal boundaries of deploying digital tools for disease surveillance and control purposes are unclear, and a rapidly evolving debate has emerged globally around the promises and risks of mobilising digital tools for public health. To help scientists and policy makers to navigate technological and ethical uncertainty, we present a typology of the primary digital public health applications that are in use. These include proximity and contact tracing, symptom monitoring, quarantine control, and flow modelling. For each, we discuss context-specific risks, cross-sectional issues, and ethical concerns. Finally, recognising the need for practical guidance, we propose a navigation aid for policy makers and other decision makers for the ethical development and use of digital public health tools.}
}
@article{RUMSON2020105004,
title = {The role of data within coastal resilience assessments: an East Anglia, UK, case study},
journal = {Ocean & Coastal Management},
volume = {185},
pages = {105004},
year = {2020},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2019.105004},
url = {https://www.sciencedirect.com/science/article/pii/S0964569119306738},
author = {Alexander G. Rumson and Andres Payo Garcia and Stephen H. Hallett},
keywords = {Coastal management, Resilience metrics, Geospatial data, Open source data, Big data},
abstract = {Embracing the concept of resilience within coastal management marks a step change in thinking, building on the inputs of more traditional risk assessments, and further accounting for capacities to respond, recover and implement contingency measures. Nevertheless, many past resilience assessments have been theoretical and have failed to address the requirements of practitioners. Assessment methods can also be subjective, relying on opinion-based judgements, and can lack empirical validation. Scope exists to address these challenges through drawing on rapidly emerging sources of data and smart analytics. This, alongside the careful selection of the metrics used in assessment of resilience, can facilitate more robust assessment methods. This work sets out to establish a set of core metrics, and data sources suitable for inclusion within a data-driven coastal resilience assessment. A case study region of East Anglia, UK, is focused on, and data types and sources associated with a set of proven assessment metrics were identified. Virtually all risk-specific metrics could be satisfied using available or derived data sources. However, a high percentage of the resilience-specific metrics would still require human input. This indicates that assessment of resilience is inherently more subjective than assessment of risk. Yet resilience assessments incorporate both risk and resilience specific variables. As such it was possible to link 75% of our selected metrics to empirical sources. Through taking a case study approach and discussing a set of requirements outlined by a coastal authority, this paper reveals scope for the incorporation of rapidly progressing data collection, dissemination, and analytical methods, within dynamic coastal resilience assessments. This could facilitate more sustainable evidence-based management of coastal regions.}
}
@article{LIU2020113,
title = {Machine learning in materials genome initiative: A review},
journal = {Journal of Materials Science & Technology},
volume = {57},
pages = {113-122},
year = {2020},
issn = {1005-0302},
doi = {https://doi.org/10.1016/j.jmst.2020.01.067},
url = {https://www.sciencedirect.com/science/article/pii/S1005030220303327},
author = {Yingli Liu and Chen Niu and Zhuo Wang and Yong Gan and Yan Zhu and Shuhong Sun and Tao Shen},
keywords = {Materials genome initiative (MGI), Materials database, Machine learning, Materials properties prediction, Materials design and discovery},
abstract = {Discovering new materials with excellent performance is a hot issue in the materials genome initiative. Traditional experiments and calculations often waste large amounts of time and money and are also limited by various conditions. Therefore, it is imperative to develop a new method to accelerate the discovery and design of new materials. In recent years, material discovery and design methods using machine learning have attracted much attention from material experts and have made some progress. This review first outlines available materials database and material data analytics tools and then elaborates on the machine learning algorithms used in materials science. Next, the field of application of machine learning in materials science is summarized, focusing on the aspects of structure determination, performance prediction, fingerprint prediction, and new material discovery. Finally, the review points out the problems of data and machine learning in materials science and points to future research. Using machine learning algorithms, the authors hope to achieve amazing results in material discovery and design.}
}