@article{BAKER201732,
title = {Process mining routinely collected electronic health records to define real-life clinical pathways during chemotherapy},
journal = {International Journal of Medical Informatics},
volume = {103},
pages = {32-41},
year = {2017},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2017.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1386505617300722},
author = {Karl Baker and Elaine Dunwoodie and Richard G. Jones and Alex Newsham and Owen Johnson and Christopher P. Price and Jane Wolstenholme and Jose Leal and Patrick McGinley and Chris Twelves and Geoff Hall},
keywords = {Electronic health records, Process mining, Episode of care, Care pathways, Drug therapy, Neoplasms},
abstract = {Background
There is growing interest in the use of routinely collected electronic health records to enhance service delivery and facilitate clinical research. It should be possible to detect and measure patterns of care and use the data to monitor improvements but there are methodological and data quality challenges. Driven by the desire to model the impact of a patient self-test blood count monitoring service in patients on chemotherapy, we aimed to (i) establish reproducible methods of process-mining electronic health records, (ii) use the outputs derived to define and quantify patient pathways during chemotherapy, and (iii) to gather robust data which is structured to be able to inform a cost-effectiveness decision model of home monitoring of neutropenic status during chemotherapy.
Methods
Electronic Health Records at a UK oncology centre were included if they had (i) a diagnosis of metastatic breast cancer and received adjuvant epirubicin and cyclosphosphamide chemotherapy or (ii) colorectal cancer and received palliative oxaliplatin and infusional 5-fluorouracil chemotherapy, and (iii) were first diagnosed with cancer between January 2004 and February 2013. Software and a Markov model were developed, producing a schematic of patient pathways during chemotherapy.
Results
Significant variance from the assumed care pathway was evident from the data. Of the 535 patients with breast cancer and 420 with colorectal cancer there were 474 and 329 pathway variants respectively. Only 27 (5%) and 26 (6%) completed the planned six cycles of chemotherapy without having unplanned hospital contact. Over the six cycles, 169 (31.6%) patients with breast cancer and 190 (45.2%) patients with colorectal cancer were admitted to hospital.
Conclusion
The pathways of patients on chemotherapy are complex. An iterative approach to addressing semantic and data quality issues enabled the effective use of routinely collected patient records to produce accurate models of the real-life experiences of chemotherapy patients and generate clinically useful information. Very few patients experience the idealised patient pathway that is used to plan their care. A better understanding of real-life clinical pathways through process mining can contribute to care and data quality assurance, identifying unmet needs, facilitating quantification of innovation impact, communicating with stakeholders, and ultimately improving patient care and outcomes.}
}
@article{DONASCIMENTO2018335,
title = {Heuristic-based approaches for speeding up incremental record linkage},
journal = {Journal of Systems and Software},
volume = {137},
pages = {335-354},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.11.074},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302972},
author = {Dimas Cassimiro {do Nascimento} and Carlos Eduardo {Santos Pires} and Demetrio {Gomes Mestre}},
keywords = {Record linkage, Deduplication, Incremental clustering, Heuristics},
abstract = {Record Linkage is the task of processing a dataset in order to identify which records refer to the same real world entity. The intrinsic complexity of this task brings many challenges to traditional or naive approaches, especially in contexts such as Big Data, unstructured data and frequent data increments over the dataset. To deal with these contexts, especially the latter, an incremental record linkage approach may be employed in order to avoid (re)processing the entire dataset to update the deduplication results. For doing so, different classification techniques can be employed to identify duplicate entities. Recently, many algorithms have been proposed to combine collective classification, which employs clustering algorithms, together with the incremental principle. In this article, we propose new metrics for incremental record linkage using collective classification and new heuristics (which combine clustering, coverage component filters and a greedy approach) to speed up even more a solution to incremental record linkage. These heuristics have been evaluated using three different scale datasets and the results were analyzed and discussed based on both classical and the newly proposed metrics. The experiments present different trade-offs, regarding efficacy and efficiency results, which are generated by the considered heuristics. Also, the results indicate that, for large and frequent data increments, it is possible to slightly reduce efficacy results by employing a coverage filter-based heuristic that is reasonably faster than the current state-of-the-art approach. In turn, it is also possible to employ single-pass clustering algorithms, which are able to execute significantly faster than the state-of-the-art approach at the cost of sacrificing precision results.}
}
@article{SICARI201739,
title = {A policy enforcement framework for Internet of Things applications in the smart health},
journal = {Smart Health},
volume = {3-4},
pages = {39-74},
year = {2017},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352648316300435},
author = {S. Sicari and A. Rizzardi and L.A. Grieco and G. Piro and A. Coen-Porisini},
keywords = {Internet of Things, Smart health, Security, Policy enforcement},
abstract = {Internet of Things (IoT) is characterized by heterogeneous technologies, which concur to the provisioning of innovative services in different application domains. Introducing efficient mechanisms for collecting, processing, and delivering data generated by sensors, medical equipment, wearable devices, and humans, is a key enabling factor for advanced healthcare services. The adoption of IoT in smart health, however, opens the doors to some security concerns. In fact, by considering the confidentiality and sensitivity of medical data, a healthcare system must fulfill advanced access control procedures with strict security and data quality requirements. To this end, a flexible policy enforcement framework, based on the IoT paradigm, is defined hereby. It is able to face security and quality threats in dynamic large scale and heterogeneous smart hearth environments. As a key feature of the proposed framework, cross-domain policies have been defined using a specification language based on XML. In this way, it becomes possible to ease the management of interactions across different realms and policy conflicts. Moreover, to demonstrate the usefulness of the proposed approach, a running example, based on a smart health application, is detailed throughout the manuscript. This helps to highlight the different facets of the conceived enforcement framework. A preliminary performance analysis also demonstrates its feasibility in large scale environments.}
}
@article{REHMAN20171,
title = {Towards next-generation heterogeneous mobile data stream mining applications: Opportunities, challenges, and future research directions},
journal = {Journal of Network and Computer Applications},
volume = {79},
pages = {1-24},
year = {2017},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.11.031},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516302995},
author = {Muhammad Habib ur Rehman and Chee Sun Liew and Teh Ying Wah and Muhammad Khurram Khan},
keywords = {Frequent pattern mining, Classification, Clustering, Mobile computing, Cloud computing, Edge computing},
abstract = {The convergence of Internet of Things (IoTs), mobile computing, cloud computing, edge computing and big data has brought a paradigm shift in computing technologies. New computing systems, application models, and application areas are emerging to handle the massive growth of streaming data in mobile environments such as smartphones, IoTs, body sensor networks, and wearable devices, to name a few. However, the challenge arises about how and where to process the data streams in order to perform analytic operations and uncover useful knowledge patterns. The mobile data stream mining (MDSM) applications involve a number of operations for, 1) data acquisition from heterogeneous data sources, 2) data preprocessing, 3) data fusion, 4) data mining, and 5) knowledge management. This article presents a thorough review of execution platforms for MDSM applications. In addition, a detailed taxonomic discussion of heterogeneous MDSM applications is presented. Moreover, the article presents detailed literature review of methods that are used to handle heterogeneity at application and platform levels. Finally, the gap analysis is articulated and future research directions are presented to develop next-generation MDSM applications.}
}
@article{KALANTARI20182,
title = {Computational intelligence approaches for classification of medical data: State-of-the-art, future challenges and research directions},
journal = {Neurocomputing},
volume = {276},
pages = {2-22},
year = {2018},
note = {Machine Learning and Data Mining Techniques for Medical Complex Data Analysis},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.01.126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217315436},
author = {Ali Kalantari and Amirrudin Kamsin and Shahaboddin Shamshirband and Abdullah Gani and Hamid Alinejad-Rokny and Anthony T. Chronopoulos},
keywords = {Computational intelligence, Medical application, Big data, Detection, Ensemble algorithm},
abstract = {The explosive growth of data in volume, velocity and diversity that are produced by medical applications has contributed to abundance of big data. Current solutions for efficient data storage and management cannot fulfill the needs of heterogeneous data. Therefore, by applying computational intelligence (CI) approaches in medical data helps get better management, faster performance and higher level of accuracy in detection. This paper aims to investigate the state-of-the-art of computational intelligence approaches in medical data and to categorize the existing CI techniques, used in medical fields, as single and hybrid. In addition, the techniques and methodologies, their limitations and performances are presented in this study. The limitations are addressed as challenges to obtain a set of requirements for Computational Intelligence Medical Data (CIMD) in establishing an efficient CIMD architectural design. The results show that on the one hand Support Vector Machine (SVM) and Artificial Immune Recognition System (AIRS) as a single based computational intelligence approach were the best methods in medical applications. On the other hand, the hybridization of SVM with other methods such as SVM-Genetic Algorithm (SVM-GA), SVM-Artificial Immune System (SVM-AIS), SVM-AIRS and fuzzy support vector machine (FSVM) had great performances achieving better results in terms of accuracy, sensitivity and specificity.}
}
@article{STEPPE2017768,
title = {Online price discrimination and personal data: A General Data Protection Regulation perspective},
journal = {Computer Law & Security Review},
volume = {33},
number = {6},
pages = {768-785},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917301656},
author = {Richard Steppe},
keywords = {Online price discrimination, Data processing, Big data, General Data Protection Regulation (GDPR)},
abstract = {The General Data Protection Regulation (GDPR) contains various provisions with relevance to online price discrimination. This article, which analyses a number of essential elements on this junction, aims to provide a theory on whether, and, if so, how the GDPR affects price discrimination based on the processing of personal data. First, the contribution clarifies the concept of price discrimination, as well as its typology and relevance for big data settings. Subsequent to studying this topic in the context of the Commission's Digital Single Market strategy, the article tests the applicability of the GDPR to online price personalisation practices by applying criteria as ‘personal data’ and ‘automated processing’ to several discriminatory pricing cases and examples. Secondly, the contribution evaluates the possible lawfulness of price personalisation under the GDPR on the basis of consent, the necessity for pre-contractual or contractual measures, and the data controller's legitimate interests. The paper concludes by providing a capita selecta of rights and obligations pertinent to online discriminatory pricing, such as transparency obligations and the right to access, as well as the right to rectify the data on which price discrimination is based, and the right not to be subject to certain discriminatory pricing decisions.}
}
@article{RAMIREZGALLEGO201859,
title = {Online entropy-based discretization for data streaming classification},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {59-70},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17325815},
author = {S. Ramírez-Gallego and S. García and F. Herrera},
keywords = {Data stream, Concept drift, Data preprocessing, Data reduction, Discretization, Online learning},
abstract = {Data quality is deemed as determinant in the knowledge extraction process. Low-quality data normally imply low-quality models and decisions. Discretization, as part of data preprocessing, is considered one of the most relevant techniques for improving data quality. In static discretization, output intervals are generated at once, and maintained along the whole process. However, many contemporary problems demands rapid approaches capable of self-adapting their discretization schemes to an ever-changing nature. Other major issues for stream-based discretization such as interval definition, labeling or how is implemented the interaction between learning and discretization components are also discussed in this paper. In order to address all the aforementioned problems, we propose a novel, online and self-adaptive discretization solution for streaming classification which aims at reducing the negative impact of fluctuations in evolving intervals. Experiments with a long list of standard streaming datasets and discretizers have demonstrated that our proposal performs significantly more accurately than the other alternatives. In addition, our scheme is able to leverage from class information without incurring in an overweight cost, being ranked as one of the most rapid supervised options.}
}
@article{DASKIVICH2017311,
title = {Online Ratings Systems for Physicians and Institutions: Limitations of the Current State of the Art},
journal = {European Urology},
volume = {71},
number = {3},
pages = {311-312},
year = {2017},
issn = {0302-2838},
doi = {https://doi.org/10.1016/j.eururo.2016.08.025},
url = {https://www.sciencedirect.com/science/article/pii/S0302283816304924},
author = {Timothy J. Daskivich and Brennan Spiegel and Hyung L. Kim}
}
@article{JANSSEN2017200,
title = {Towards a new generation of agricultural system data, models and knowledge products: Information and communication technology},
journal = {Agricultural Systems},
volume = {155},
pages = {200-212},
year = {2017},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2016.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16305637},
author = {Sander J.C. Janssen and Cheryl H. Porter and Andrew D. Moore and Ioannis N. Athanasiadis and Ian Foster and James W. Jones and John M. Antle},
keywords = {Agricultural models, ICT, Linked data, Big data, Open science, Sensing, Visualization},
abstract = {Agricultural modeling has long suffered from fragmentation in model implementation. Many models are developed, there is much redundancy, models are often poorly coupled, model component re-use is rare, and it is frequently difficult to apply models to generate real solutions for the agricultural sector. To improve this situation, we argue that an open, self-sustained, and committed community is required to co-develop agricultural models and associated data and tools as a common resource. Such a community can benefit from recent developments in information and communications technology (ICT). We examine how such developments can be leveraged to design and implement the next generation of data, models, and decision support tools for agricultural production systems. Our objective is to assess relevant technologies for their maturity, expected development, and potential to benefit the agricultural modeling community. The technologies considered encompass methods for collaborative development and for involving stakeholders and users in development in a transdisciplinary manner. Our qualitative evaluation suggests that as an overall research challenge, the interoperability of data sources, modular granular open models, reference data sets for applications and specific user requirements analysis methodologies need to be addressed to allow agricultural modeling to enter in the big data era. This will enable much higher analytical capacities and the integrated use of new data sources. Overall agricultural systems modeling needs to rapidly adopt and absorb state-of-the-art data and ICT technologies with a focus on the needs of beneficiaries and on facilitating those who develop applications of their models. This adoption requires the widespread uptake of a set of best practices as standard operating procedures.}
}
@article{VANDENHOMBERG201860,
title = {Bridging the information gap of disaster responders by optimizing data selection using cost and quality},
journal = {Computers & Geosciences},
volume = {120},
pages = {60-72},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S009830041730571X},
author = {Marc {van den Homberg} and Robert Monné and Marco Spruit},
keywords = {Disaster management, Data fusion, Big data analytics, Decision making, Natural disaster, Information requirements, Humanitarian response, Data preparedness, Integer linear programming, Decision making},
abstract = {Natural disasters are chaotic and disruptive events, with compressed timelines and high levels of uncertainty. Comprehensive data on the impact becomes only available well into the response phase and data is scattered across organizations. Data heterogeneity issues are common. Consequently, responding organizations have difficulties finding data that match their information needs. We investigated the information needs of and the disaster management data available to both national and local decision makers during the 2014 floods in Bangladesh. We conducted 13 semi-structured interviews and three focus group discussions, collecting in this way input from 51 people, transcribed and coded them so that themes of information needs emerged. We mapped the information needs on the available data sets and determined which needs were not, partially or completely covered. We identified seven themes of in total 71 information needs and 15 data sets. The mapping revealed a significant information gap of timely and location-based data. Only 40% of the information needs are covered in time and 75% if no time constraints are considered. Instead of using all data sets, we optimized for coverage -with Integer Linear Programming-combinations of data sets against the costs of extracting data from structured versus unstructured data and against the quality in terms of timeliness, source and content rating and granularity. Without time constraints, three data sets yield already a coverage of 68%, whereas adding five extra data sets only gives an improvement of 7%. We recommend executing identification and mapping of available data sets on the information needs as part of Data Preparedness. Determination of the optimal combination of data sets can be used to extract data on information needs more efficiently. Currently, we did this manually, but future research will investigate automatic matching of information needs on data sets, by applying intelligent querying and semantic data matching.}
}
@article{RIVERA2018447,
title = {Towards a Predictive Maintenance System of a Hydraulic Pump⁎⁎This work is funded by the Bayerisches Staatsministerium fur Wirtschaft und Medien, Energie und Technologie in its R&D program Bayern Digital.},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {447-452},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.346},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314708},
author = {Domingo Llorente Rivera and Markus R. Scholz and Michael Fritscher and Markus Krauss and Klaus Schilling},
keywords = {industrial analytics, predictive maintenance, hydraulic, hydraulic pump},
abstract = {In this contribution we present a model based approach towards predictive maintenance of the hydraulic pump of an injection-moulding machine for a system that remains in production. We present a methodology to iteratively gain a detailed understanding of a production facility. We describe our approach from problem evaluation, gathering of expert knowledge, data access, exploration of and knowledge mapping to the data, towards a first model. We discuss the advantages of the implementation of expert knowledge to define normal machine behaviour in terms of data-quality control and resource-friendly modelling. We present first results from a physical model of a subprocess and from a model based on vibration analysis and discuss their interplay and possible benefits from this different models.}
}
@article{ABELLA201747,
title = {A model for the analysis of data-driven innovation and value generation in smart cities' ecosystems},
journal = {Cities},
volume = {64},
pages = {47-53},
year = {2017},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2017.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0264275116303845},
author = {Alberto Abella and Marta Ortiz-de-Urbina-Criado and Carmen De-Pablos-Heredero},
keywords = {Smart cities, Public-private collaboration, Data reusability, Open government data, Data-driven innovation},
abstract = {Smart cities are key elements to cope with certain of the largest challenges facing society, such as overpopulation, transport, pollution, sustainability, security, health, and the creation of new firms. Smart cities' portals offer a great amount of data that can be used by the private and public entities to create new services. These data are also a valuable source for the deployment of big data businesses. In this article, a model is presented demonstrating how the data released by the smart cities creates value for the citizens and society. The model operates using three stages. The first one shapes the release of data by the smart city, and it includes several of the dimensions that make data appealing for reuse. The second stage analyses the mechanisms to create innovative products and services. The last stage explains how these products and services impact its society.}
}
@article{TRIEU2017111,
title = {Getting value from Business Intelligence systems: A review and research agenda},
journal = {Decision Support Systems},
volume = {93},
pages = {111-124},
year = {2017},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616301683},
author = {Van-Hau Trieu},
keywords = {Business Intelligence, Analytics, Big data, Data mining, Data warehousing, Business value},
abstract = {Much of the research on Business Intelligence (BI) has examined the ability of BI systems to help organizations address challenges and opportunities. However, the literature is fragmented and lacks an overarching framework to integrate findings and systematically guide research. Moreover, researchers and practitioners continue to question the value of BI systems. This study reviews and synthesizes empirical Information System (IS) studies to learn what we know, how well we know, and what we need to know about the processes of organizations obtaining business value from BI systems. The study aims to identify which parts of the BI business value process have been studied and are still most in need of research, and to propose specific research questions for the future. The findings show that organizations appear to obtain value from BI systems according to the process suggested by Soh and Markus (1995), as a chain of necessary conditions from BI investments to BI assets to BI impacts to organizational performance; however, researchers have not sufficiently studied the probabilistic processes that link the necessary conditions together. Moreover, the research has not sufficiently covered all relevant levels of analysis, nor examined how the levels link up. Overall, the paper identified many opportunities for researchers to provide a more complete picture of how organizations can and do obtain value from BI.}
}
@article{SHARMA20182680,
title = {Using Digital Health Technology to Better Generate Evidence and Deliver Evidence-Based Care},
journal = {Journal of the American College of Cardiology},
volume = {71},
number = {23},
pages = {2680-2690},
year = {2018},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2018.03.523},
url = {https://www.sciencedirect.com/science/article/pii/S0735109718344139},
author = {Abhinav Sharma and Robert A. Harrington and Mark B. McClellan and Mintu P. Turakhia and Zubin J. Eapen and Steven Steinhubl and James R. Mault and Maulik D. Majmudar and Lothar Roessig and Karen J. Chandross and Eric M. Green and Bakul Patel and Andrew Hamer and Jeffrey Olgin and John S. Rumsfeld and Matthew T. Roe and Eric D. Peterson},
keywords = {clinical trial conduct, digital health technology, healthcare delivery, think tank meeting},
abstract = {As we enter the information age of health care, digital health technologies offer significant opportunities to optimize both clinical care delivery and clinical research. Despite their potential, the use of such information technologies in clinical care and research faces major data quality, privacy, and regulatory concerns. In hopes of addressing both the promise and challenges facing digital health technologies in the transformation of health care, we convened a think tank meeting with academic, industry, and regulatory representatives in December 2016 in Washington, DC. In this paper, we summarize the proceedings of the think tank meeting and aim to delineate a framework for appropriately using digital health technologies in healthcare delivery and research.}
}
@article{GAI2018262,
title = {A survey on FinTech},
journal = {Journal of Network and Computer Applications},
volume = {103},
pages = {262-273},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517303247},
author = {Keke Gai and Meikang Qiu and Xiaotong Sun},
keywords = {FinTech, Cloud computing, Cyber security, Big data, Financial computing, Data-driven framework},
abstract = {As a new term in the financial industry, FinTech has become a popular term that describes novel technologies adopted by the financial service institutions. This term covers a large scope of techniques, from data security to financial service deliveries. An accurate and up-to-date awareness of FinTech has an urgent demand for both academics and professionals. This work aims to produce a survey of FinTech by collecting and reviewing contemporary achievements, by which a theoretical data-driven FinTech framework is proposed. Five technical aspects are summarized and involved, which include security and privacy, data techniques, hardware and infrastructure, applications and management, and service models. The main findings of this work are fundamentals of forming active FinTech solutions.}
}
@article{LOEKEN201862,
title = {Design Principles Behind the Construction of an Autonomous Laboratory-Scale Drilling Rig},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {8},
pages = {62-69},
year = {2018},
note = {3rd IFAC Workshop on Automatic Control in Offshore Oil and Gas Production OOGP 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.356},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318306864},
author = {Erik A. Loeken and Alexander Trulsen and Andrew M. Holsaeter and Ekaterina Wiktorski and Dan Sui and Robert Ewald},
keywords = {Drilling Automation, ROP Optimization, Modeling, Fault Detection, Drill String Dynamics},
abstract = {In recent years, hot topics such as digitalization, machine learning, digital twin and big data have evolved from being envisions on the paper to state of art solutions, expected to revolutionize drilling efficiency in the industry. Drilling automation tomorrow is all about exploiting the current state of technologies available to the entire operation of drilling a well. Not only can drilling automation limit costs and reduce the risk to rig personnel and the environment, but they also give access to locations of considerable potential that previously have been regarded unsafe or uneconomical to operate in. There are however some challenges in keeping up with the ever-increasing pace of the development. For one, testing of novel and innovative solutions is often very expensive because of non-productive rig time during implementation, trial runs and data evaluation. Also, the modern technologies require extensive R&D before on-site testing can even commence. While on land-rigs, some of these costs and risks can be greatly minimized, many offshore solutions lack that luxury. This paper presents an overview of the design principles that go into the construction of a fully autonomous laboratory-scale drilling rig at the University of Stavanger. It aims at describing 1) the engineering principles involved to resemble full-scale drilling operations on the laboratory scale, 2) design considerations and components, 3) component requirements for the rig, 4) control system algorithms for real-time optimization of drilling parameters and detection and handling of drilling anomalies, 5) development of drilling models (drill string dynamics, bit-vibration, etc.) and 6) benefits and future work with the laboratory-scale system. Some of the concepts that are presented in this paper have yet to be implemented during 2018.}
}
@incollection{ZHOU2018423,
title = {5.11 Smart Energy Management},
editor = {Ibrahim Dincer},
booktitle = {Comprehensive Energy Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {423-456},
year = {2018},
isbn = {978-0-12-814925-6},
doi = {https://doi.org/10.1016/B978-0-12-809597-3.00525-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095973005253},
author = {Kaile Zhou and Shanlin Yang},
keywords = {Demand side management, Energy big data, Energy consumption behavior, Energy informatics, Energy social informatics, Smart energy management, Smart energy system},
abstract = {Smart energy management is the path to achieve the management and operational objectives of smart energy systems (SESs). First, some related concepts of smart energy management are introduced. The evolution of energy systems in four stages and the three dimensions of smart energy management are also proposed. Then the overall structure and key technologies of SESs are provided, followed by the introduction of the composition of energy big data and its application in demand side management (DSM). Furthermore, the Ubiquitous Energy Internet in China, the smart energy management in smart buildings, smart manufacturing, and smart transportation are discussed as case studies of smart energy management. Finally, the research paradigms of smart energy management are presented and future directions are pointed out.}
}
@article{FAHEEM20181,
title = {Smart grid communication and information technologies in the perspective of Industry 4.0: Opportunities and challenges},
journal = {Computer Science Review},
volume = {30},
pages = {1-30},
year = {2018},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1574013718300856},
author = {M. Faheem and S.B.H. Shah and R.A. Butt and B. Raza and M. Anwar and M.W. Ashraf and Md.A. Ngadi and V.C. Gungor},
keywords = {Internet of Things, Industry 4.0, Big data, Cloud computing, Smart grid, Wireless sensor network},
abstract = {The fourth industrial revolution known as Industry 4.0 has paved the way for a systematical deployment of the modernized power grid (PG) to manage continuously growing energy demand by integrating renewable energy resources. In the context of Industry 4.0, a smart grid (SG) by employing advanced Information and Communication Technologies (ICTs), intelligent information processing (IIP) and future-oriented techniques (FoT) allows energy utilities to monitor and control power generation, transmission and distribution processes in more efficient, flexible, reliable, sustainable, decentralized, secure and economic manners. Despite providing immense opportunities, SG has many challenges in the context of Industry 4.0 (I 4.0). To this end, this paper presents a comprehensive presentation on critical smart grid components with international standards and information technologies in the context of Industry 4.0. In addition, this study gives an overview of different smart grid applications, their benefits, characteristics, and requirements. Also, this research investigates and explores different wired and wireless communication technologies used in smart grid with their benefits and characteristics. Finally, this article discusses a number of critical challenges and open issues and future research directions.}
}
@incollection{CHEN2018353,
title = {2.16 - GIS and Placemaking Using Social Media Data},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {353-370},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09648-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489096482},
author = {Yan Chen},
keywords = {GIS, Placemaking, Social media, Point visualization, Vibrant center},
abstract = {Where is more popular and vibrant in the city? Analyzing human activity pattern in space has always been the key part of data analysis in urban studies. With the rise of big data era, there are more and more available data could be utilized for this topic. In this chapter, the article introduces a set of newly emerged methods to analyze the spatial distribution of human activity using social media data to identify where is the most vibrant place in the city. Using Twitter data in Chicago downtown as an example, this chapter will discuss how to collect social media data, how to visualize the data and how to identify the most vibrant region in the city using it. The process is replicable for the most area in the world as long as the social media service is viable, therefore provide the urban analysts a new tool to have a better understanding of the city.}
}
@article{ASHWINKUMAR2017213,
title = {Content sensitivity based access control framework for Hadoop},
journal = {Digital Communications and Networks},
volume = {3},
number = {4},
pages = {213-225},
year = {2017},
note = {Big Data Security and Privacy},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352864817300858},
author = {T.K. {Ashwin Kumar} and Hong Liu and Johnson P. Thomas and Xiaofeh Hou},
keywords = {Access control, Data driven information sensitivity, Privacy, Information value},
abstract = {Big data technologies have seen tremendous growth in recent years. They are widely used in both industry and academia. In spite of such exponential growth, these technologies lack adequate measures to protect data from misuse/abuse. Corporations that collect data from multiple sources are at risk of liabilities due to the exposure of sensitive information. In the current implementation of Hadoop, only file-level access control is feasible. Providing users with the ability to access data based on the attributes in a dataset or the user’s role is complicated because of the sheer volume and multiple formats (structured, unstructured and semi-structured) of data. In this paper, we propose an access control framework, which enforces access control policies dynamically based on the sensitivity of the data. This framework enforces access control policies by harnessing the data context, usage patterns and information sensitivity. Information sensitivity changes over time with the addition and removal of datasets, which can lead to modifications in access control decisions. The proposed framework accommodates these changes. The proposed framework is automated to a large extent as the data itself determines the sensitivity with minimal user intervention. Our experimental results show that the proposed framework is capable of enforcing access control policies on non-multimedia datasets with minimal overhead.}
}
@article{ACETO2018125,
title = {The role of Information and Communication Technologies in healthcare: taxonomies, perspectives, and challenges},
journal = {Journal of Network and Computer Applications},
volume = {107},
pages = {125-154},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518300456},
author = {Giuseppe Aceto and Valerio Persico and Antonio Pescapé},
keywords = {Healthcare, ICTs, e-health, m-Health, Pervasive health, WBAN, Cloud computing, Internet of Things, Fog, Big data, Genomics, Health monitoring, Privacy, Security, Interoperability},
abstract = {Progress in Information and Communication Technologies (ICTs) is shaping more and more the healthcare domain. ICTs adoption provides new opportunities, as well as discloses novel and unforeseen application scenarios. As a result, the overall health sector is potentially benefited, as the quality of medical services is expected to be enhanced and healthcare costs are reduced, in spite of the increasing demand due to the aging population. Notwithstanding the above, the scientific literature appears to be still quite scattered and fragmented, also due to the interaction of scientific communities with different background, skills, and approaches. A number of specific terms have become of widespread use (e.g., regarding ICTs-based healthcare paradigms as well as at health-related data formats), but without commonly-agreed definitions. While scientific surveys and reviews have also been proposed, none of them aims at providing a holistic view of how today ICTs are able to support healthcare. This is the more and more an issue, as the integrated application of most if not all the main ICTs pillars is the most agreed upon trend, according to the Industry 4.0 paradigm about ongoing and future industrial revolution. In this paper we aim at shedding light on how ICTs and healthcare are related, identifying the most popular ICTs-based healthcare paradigms, together with the main ICTs backing them. Studying more than 300 papers, we survey outcomes of literature analyses and results from research activities carried out in this field. We characterize the main ICTs-based healthcare paradigms stemmed out in recent years fostered by the evolution of ICTs. Dissecting the scientific literature, we also identify the technological pillars underpinning the novel applications fueled by these technological advancements. Guided by the scientific literature, we review a number of application scenarios gaining momentum thanks to the beneficial impact of ICTs. As the evolution of ICTs enables to gather huge and invaluable data from numerous and highly varied sources in easier ways, here we also focus on the shapes that this healthcare-related data may take. This survey provides an up-to-date picture of the novel healthcare applications enabled by the ICTs advancements, with a focus on their specific hottest research challenges. It helps the interested readership (from both technological and medical fields) not to lose orientation in the complex landscapes possibly generated when advanced ICTs are adopted in application scenarios dictated by the critical healthcare domain.}
}
@article{LIU201811,
title = {Review and Prospects of Hydrological Component Development of ESCAP/WMO Typhoon Committee},
journal = {Tropical Cyclone Research and Review},
volume = {7},
number = {1},
pages = {11-22},
year = {2018},
issn = {2225-6032},
doi = {https://doi.org/10.6057/2018TCRR01.02},
url = {https://www.sciencedirect.com/science/article/pii/S2225603219300256},
author = {Jinping Liu and Yoshio Tokunaga and Hyoseob Cho and Jinxing Wang},
keywords = {Typhoon Committee, Hydrological Component, development, review, prospect},
abstract = {ABSTRACT
Hydrological Component has been one of three essential parts of ESCAP/WMO Typhoon Committee. The Working Group on Hydrology (WGH) established at TC 33rd Session in 2000 is the first working group among three basic components of the Committee, which has been repeatedly recognized as the most active component in the Committee who has integrated conducted successfully a number of cooperation projects and integrated actions among Members. In the past decade, WGH has implemented wide range activities under the framework of Strategic Plan of the Committee. The achievement of those cooperation projects played very important roles in promoting the capacity building on hydrological monitoring, forecasting and early warning among Members. This paper introduced the Term of Reference for WGH and its high priorities, and summarized the activities conducted in recent decade. The paper also reviewed the progresses in TC Members on hydrological observation and monitoring network, hydrological data collection and transmission, hydrological information and forecasting, and establishment of flood forecasting system. The paper also pointed out the development direction and the area to be enhanced for hydrological component in future, including: (1) application of QPE/QPF in flood forecasting and establishment of the coupled hydro-meteorological modeling; (2) development of impact-based, risk-based and community-based flood forecasting and warning system, including storm surge, urban flood, sediment disaster (flash flood, landslide and mudflow); and (3) application of internet of things (IOT), big data, cloud computing, and mobile internet in flood monitoring, forecasting and early warning, and better response, particularly in flood inundation mapping and QPE/QPF products application.}
}
@article{CHATFIELD2017231,
title = {A longitudinal cross-sector analysis of open data portal service capability: The case of Australian local governments},
journal = {Government Information Quarterly},
volume = {34},
number = {2},
pages = {231-243},
year = {2017},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2017.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X16302283},
author = {Akemi Takeoka Chatfield and Christopher G. Reddick},
keywords = {Longitudinal cross-sector analysis, Open data portal, Service capability, Open data, Big data, Data analytics, Open innovation, Open data policy intensity, Citizens/portal users},
abstract = {While open government partnerships and open government data initiatives around the world have proliferated in practice, empirical research is required to better understand open data policy and open data portal capability which would spur meaningful citizen engagement towards co-production of open services innovation through open data reuse. Specifically, relatively little has been empirically investigated about open data portal as supply-side service capabilities at the local government level. In this longitudinal research on twenty open data portals in Australia's largest cities, cross-sector analysis results find large variation in open data portal service capabilities, which are measured by open data policy intensity, open data provision, data format variety, and entrepreneurial data services, including analytics tools, data modeling, and hackathon idea competitions. Longitudinal cross-sector analysis results also find the important roles played by open data policy and dedicated open data portal investment as predictors of open data portal service capability improvements over time.}
}
@article{LEIGHTLEY201817,
title = {Integrating electronic healthcare records of armed forces personnel: Developing a framework for evaluating health outcomes in England, Scotland and Wales},
journal = {International Journal of Medical Informatics},
volume = {113},
pages = {17-25},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S138650561830090X},
author = {Daniel Leightley and Zoe Chui and Margaret Jones and Sabine Landau and Paul McCrone and Richard D. Hayes and Simon Wessely and Nicola T. Fear and Laura Goodwin},
keywords = {Hospital episode statistics, Electronic health records, Hospital admission, Secondary care, Big data, Data linkage},
abstract = {Background
Electronic Healthcare Records (EHRs) are created to capture summaries of care and contact made to healthcare services. EHRs offer a means to analyse admissions to hospitals for epidemiological research. In the United Kingdom (UK), England, Scotland and Wales maintain separate data stores, which are administered and managed exclusively by devolved Government. This independence results in harmonisation challenges, not least lack of uniformity, making it difficult to evaluate care, diagnoses and treatment across the UK. To overcome this lack of uniformity, it is important to develop methods to integrate EHRs to provide a multi-nation dataset of health.
Objective
To develop and describe a method which integrates the EHRs of Armed Forces personnel in England, Scotland and Wales based on variable commonality to produce a multi-nation dataset of secondary health care.
Methods
An Armed Forces cohort was used to extract and integrate three EHR datasets, using commonality as the linkage point. This was achieved by evaluating and combining variables which shared the same characteristics. EHRs representing Accident and Emergency (A&E), Admitted Patient Care (APC) and Outpatient care were combined to create a patient-level history spanning three nations. Patient-level EHRs were examined to ascertain admission differences, common diagnoses and record completeness.
Results
A total of 6,336 Armed Forces personnel were matched, of which 5,460 personnel had 7,510 A&E visits, 9,316 APC episodes and 45,005 Outpatient appointments. We observed full completeness for diagnoses in APC, whereas Outpatient admissions were sparsely coded; with 88% of diagnoses coded as “Unknown/unspecified cause of morbidity”. In addition, A&E records were sporadically coded; we found five coding systems for identifying reason for admission.
Conclusion
At present, EHRs are designed to monitor the cost of treatment, enable administrative oversight, and are not currently suited to epidemiological research. However, only small changes may be needed to take advantage of what should be a highly cost-effective means of delivering important research for the benefit of the NHS.}
}
@article{FAN20171070,
title = {Assessment of Building Operational Performance Using Data Mining Techniques: A Case Study},
journal = {Energy Procedia},
volume = {111},
pages = {1070-1078},
year = {2017},
note = {8th International Conference on Sustainability in Energy and Buildings, SEB-16, 11-13 September 2016, Turin, Italy},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.03.270},
url = {https://www.sciencedirect.com/science/article/pii/S187661021730303X},
author = {Cheng Fan and Fu Xiao},
keywords = {Building Energy Conservation, Building Automation System, Data Mining, Big Data, Intelligent Building},
abstract = {Today's buildings are not only energy intensive, but also information intensive. Massive amounts of operational data are available for knowledge discovery. Data mining (DM) has excellent ability in extracting insights from massive data. This paper performs a case study on the assessment of building operational performance using DM techniques. Typical DM techniques are compared and considerations for choosing specific DM techniques for the case study are presented. The methodology developed has been applied to analyze the data retrieved from a university building in Hong Kong. Useful insights have been obtained to identify typical operation patterns and energy conservation opportunities.}
}
@article{BRICKA2018195,
title = {Workshop Synthesis: Increasing survey participation levels without changing travel behavior},
journal = {Transportation Research Procedia},
volume = {32},
pages = {195-199},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.035},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301893},
author = {Stacey G. Bricka and Stephen P. Greaves},
keywords = {travel surveys, incentives, gamification, survey methods, survey technology},
abstract = {The evolution of travel survey methods is linked to technological advances. New technology makes data collection more efficient. New technology also challenges us to explore concepts such as ‘gamification’, which is anticipated to improve the survey experience, response rates and data quality. Gamification offers possibilities around the structuring of incentive systems (e.g., points for completeness/timeliness of trip reporting). This leads into the second workshop theme which identifies the opportunities technology brings for incentivizing participants to improve survey results. Workshop attendees explored advances in gamification and incentives to identify how to improve participation levels and data quality without changing travel behaviour.}
}
@article{MOTAI2017612,
title = {Heterogeneous data analysis: Online learning for medical-image-based diagnosis},
journal = {Pattern Recognition},
volume = {63},
pages = {612-624},
year = {2017},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2016.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0031320316302977},
author = {Yuichi Motai and Nahian Alam Siddique and Hiroyuki Yoshida},
keywords = {Online learning, Computed tomographic colonography, Heterogeneous data analysis, Kernel feature analysis, Computer-aided detection, Principal composite kernel feature analysis},
abstract = {Heterogeneous Data Analysis (HDA) is proposed to address a learning problem of medical image databases of Computed Tomographic Colonography (CTC). The databases are generated from clinical CTC images using a Computer-aided Detection (CAD) system, the goal of which is to aid radiologists' interpretation of CTC images by providing highly accurate, machine-based detection of colonic polyps. We aim to achieve a high detection accuracy in CAD in a clinically realistic context, in which additional CTC cases of new patients are added regularly to an existing database. In this context, the CAD performance can be improved by exploiting the heterogeneity information that is brought into the database through the addition of diverse and disparate patient populations. In the HDA, several quantitative criteria of data compatibility are proposed for efficient management of these online images. After an initial supervised offline learning phase, the proposed online learning method decides whether the online data are heterogeneous or homogeneous. Our previously developed Principal Composite Kernel Feature Analysis (PC-KFA) is applied to the online data, managed with HDA, for iterative construction of a linear subspace of a high-dimensional feature space by maximizing the variance of the non-linearly transformed samples. The experimental results showed that significant improvements in the data compatibility were obtained when the online PC-KFA was used, based on an accuracy measure for long-term sequential online datasets. The computational time is reduced by more than 93% in online training compared with that of offline training.}
}
@article{GIULIANI2017222,
title = {Live Monitoring of Earth Surface (LiMES): A framework for monitoring environmental changes from Earth Observations},
journal = {Remote Sensing of Environment},
volume = {202},
pages = {222-233},
year = {2017},
note = {Big Remotely Sensed Data: tools, applications and experiences},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2017.05.040},
url = {https://www.sciencedirect.com/science/article/pii/S0034425717302481},
author = {Gregory Giuliani and Hy Dao and Andrea {De Bono} and Bruno Chatenoux and Karin Allenbach and Pierric {De Laborie} and Denisa Rodila and Nikos Alexandris and Pascal Peduzzi},
keywords = {Earth Observations, Big Data, Monitoring, Environmental changes, Sustainable development, Interoperability},
abstract = {Global environmental changes are mostly induced by human activities (e.g., food and energy production, urbanization, mining activities). To assess and understand these changes that are occurring all around the planet, regular and continuous monitoring is an essential condition. However, due to the potentially large area spread over numerous locations that need to be followed, this usually leads to a low frequency of monitoring of environmental changes of only a few selected sites at best. With the increasing number of freely and openly accessible big remotely-sensed Earth Observations (EO) Data repositories and the increasing capabilities of open and interoperable software solutions it is now possible to automate various EO data processing tasks to monitor environmental changes at large scale. This paper presents the Live Monitoring of Earth Surface (LiMES) framework that helps to automate image processing tasks in transforming raw data into information and knowledge through workflows using interoperable processing service chains for monitoring environmental changes. Both benefits and limitations are demonstrated and discussed through the implementation of a prototype to facilitate the update on the status of some of the 278 UNEP Environmental Hotspots. We believe that such a framework can help to reduce the gap between massive volumes of EO data and the users such as International Organizations (IO) in order to help them better fulfil their environmental monitoring mandates by bringing raw data to a level which can be used by non-remote sensing experts for basic impacts assessments.}
}
@article{RIKHARDSSON201837,
title = {Business intelligence & analytics in management accounting research: Status and future focus},
journal = {International Journal of Accounting Information Systems},
volume = {29},
pages = {37-58},
year = {2018},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2018.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300616},
author = {Pall Rikhardsson and Ogan Yigitbasioglu},
keywords = {Business intelligence, Management accounting, Big data, Analytics},
abstract = {Executives see technology, data and analytics as a transforming force in business. Many organizations are therefore implementing business intelligence & analytics (BI&A) technologies to support reporting and decision-making. Traditionally, management accounting is the primary support for decision-making and control in an organization. As such, it has clear links to and can benefit from applying BI&A technologies. This indicates an interesting research area for accounting and AIS researchers. However, a review of the literature in top accounting and information systems journals indicates that to date, little research has focused on this link. This article reviews the literature, points to several research gaps and proposes a framework for studying the relationship between BI&A and management accounting.}
}
@incollection{YANG201783,
title = {Chapter 4 - Efficient Nonlinear Regression-Based Compression of Big Sensing Data on Cloud},
editor = {Hui-Huang Hsu and Chuan-Yu Chang and Ching-Hsien Hsu},
booktitle = {Big Data Analytics for Sensor-Network Collected Intelligence},
publisher = {Academic Press},
pages = {83-98},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-809393-1},
doi = {https://doi.org/10.1016/B978-0-12-809393-1.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093931000040},
author = {Chi Yang and Jinjun Chen},
keywords = {Big sensing data, Cloud computing, Data compression, Nonlinear regression, Scalability},
abstract = {With the advance of modern technology, big sensing data is commonly encountered in every aspect of industrial activity, scientific research and people's lives. In order to process that big sensing data with the computational power of the cloud effectively, different compression strategies have been proposed including data trend-based approaches and linear regression-based approaches. However, in lots of real-world applications, the incoming big sensing data can be extremely bumpy and discrete. Thus, at the big data processing steps of data collection and data preparation, the above compression techniques may lose effect in terms of scalability and compression due to the inner constraints of their predicting models. To improve the effectiveness and efficiency for processing those real-world big sensing data, in this chapter, a novel nonlinear regression prediction model is introduced. The related details, including regression design, least squares, and triangular transform, are also discussed. To explore fully the power and resource offered by the cloud, the proposed nonlinear compression is implemented with MapReduce for achieving scalability. Through our experiment based on real-world earthquake big sensing data, we demonstrate that the compression based on the proposed nonlinear regression model can achieve significant storage and time performance gains compared to previous compression models when processing similar big sensing data on the cloud.}
}
@article{FAN2018296,
title = {Unsupervised data analytics in mining big building operational data for energy efficiency enhancement: A review},
journal = {Energy and Buildings},
volume = {159},
pages = {296-308},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817326671},
author = {Cheng Fan and Fu Xiao and Zhengdao Li and Jiayuan Wang},
keywords = {Unsupervised data mining, Big data, Building operational performance, Building energy management, Building energy efficiency},
abstract = {Building operations account for the largest proportion of energy use throughout the building life cycle. The energy saving potential is considerable taking into account the existence of a wide variety of building operation deficiencies. The advancement in information technologies has made modern buildings to be not only energy-intensive, but also information-intensive. Massive amounts of building operational data, which are in essence the reflection of actual building operating conditions, are available for knowledge discovery. It is very promising to extract potentially useful insights from big building operational data, based on which actionable measures for energy efficiency enhancement are devised. Data mining is an advanced technology for analyzing big data. It consists of two main types of data analytics, i.e., supervised and unsupervised analytics. Despite of the power of supervised analytics in predictive modeling, unsupervised analytics are more practical and promising in discovering novel knowledge given limited prior knowledge. This paper provides a comprehensive review on the current utilization of unsupervised data analytics in mining massive building operational data. The commonly used unsupervised analytics are summarized according to their knowledge representations and applications. The challenges and opportunities are elaborated as guidance for future research in this multi-disciplinary field.}
}
@incollection{GUARASCIO2019336,
title = {Knowledge Discovery in Databases},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {336-341},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20456-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204561},
author = {Massimo Guarascio and Giuseppe Manco and Ettore Ritacco},
keywords = {Anomaly detection, Associations rules, Classification, Clustering, CRISP-DM methodology, Data mining, Knowledge discovery, Machine learning, Model evaluation, Pattern recognition},
abstract = {The huge amount of data, generated by daily-life data sources, represents a big opportunity for the development and advancement in several fields: scientific research, social life and industry. At the same time, analyzing these big repositories is a hard challenge, since the overload of information can overwhelm our capability of reading and understanding data, making finding useful pieces of information a difficult task. In this discussion we give a general overview about Knowledge Discovery in Databases as a scientific discipline that provides methodologies, techniques and tools for dealing with Big Data in order to find underlying knowledge that can be exploited in decision making processes.}
}
@article{JU2023108747,
title = {A novel subspace pursuit of residual correlation step algorithm for distributed compressed sensing},
journal = {Signal Processing},
volume = {202},
pages = {108747},
year = {2023},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2022.108747},
url = {https://www.sciencedirect.com/science/article/pii/S0165168422002869},
author = {Mingchi Ju and Man Zhao and Tailin Han and Hong Liu and Bo Xu and Xuan Liu},
keywords = {Distributed compressed sensing, Sparse reconstruction, Mixed-support set model},
abstract = {Multi-signal joint reconstruction is critical in distributed compressed sensing (DCS). We propose a joint reconstruction algorithm for subspace pursuit of residual correlation steps to balance reconstruction efficiency and data quality. The algorithm improves on two aspects. Firstly, the residuals are used as the step feedback condition to achieve dynamic step adjustment, reducing the iteration’s overall number. Secondly, based on the mixed-support set model (MSM), the residual non-decreasing surplus condition is set to reconstruct the common and the innovation components of the target signal in groups, which reduces the mixing error in joint reconstruction. This paper compares the performance of six algorithms of the same type under the conditions of Gaussian sparse signal and measured shock wave field sensor network data. The results show that the proposed algorithm can effectively reduce the number of measurements required for reconstruction and improve efficiency while maintaining accuracy.}
}
@article{BOSELLI2018319,
title = {Classifying online Job Advertisements through Machine Learning},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {319-328},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17321830},
author = {Roberto Boselli and Mirko Cesarini and Fabio Mercorio and Mario Mezzanzanica},
keywords = {Machine learning, Text classification, Big data, NLP},
abstract = {The rapid growth of Web usage for advertising job positions provides a great opportunity for real-time labour market monitoring. This is the aim of Labour Market Intelligence (LMI), a field that is becoming increasingly relevant to EU Labour Market policies design and evaluation. The analysis of Web job vacancies, indeed, represents a competitive advantage to labour market stakeholders with respect to classical survey-based analyses, as it allows for reducing the time-to-market of the analysis by moving towards a fact-based decision making model. In this paper, we present our approach for automatically classifying million Web job vacancies on a standard taxonomy of occupations. We show how this problem has been expressed in terms of text classification via machine learning. We also show how our approach has been applied to certain real-life projects and we discuss the benefits provided to end users.}
}
@article{INDRA2023118747,
title = {An improved flood forecasting system with cluster based visualization and analyzing using GK-ANFIS and CGDNN},
journal = {Expert Systems with Applications},
volume = {212},
pages = {118747},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118747},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422017651},
author = {G. Indra and N. Duraipandian},
keywords = {Forecasting, Feature extraction, levy flight k means clustering (LF-K-Means), Gaussian Kernel Adaptive Neuro Fuzzy Interface System (GK-ANFIS), Conjugate gradient deep neural network (CGDNN), K-Fold Cross Validation},
abstract = {Flooding has been a major causative aimed at social and also economic losses, and also human life’s loss. Flood flow’s accurate forecasting is a vital necessity to decrement the flooding’s risk and is essential to plan and manage the water resources systems, however, due to water flood stage analysis that is an intricate dynamic procedure characterized by temporal and spatial differences it remains to be quite challenging. Aimed at overcoming the challenge, the work has proposed a two-mode operation i.e. Data Visualization (DV) and Data Analysis (DA) for forecasting flood aimed at maintaining an accurate prediction. The main motive of the proposed framework is to forecast the flood in location wise by using the location features. Initially, DV is processed, which is initiated with the data’s quality enhancement for decrementing the error. In aid to fully utilize the database with important data, this work has initiated feature extraction (FE), and thereafter, the features are clustered into a group aimed at evading data misallocation utilizing the levy flight K-means clustering (LF-K-Means Clustering) technique. The clustered data’s visualization is executed utilizing Gaussian Kernel-Adaptive Neuro-Fuzzy Interface System (GK-ANFIS). After that, DA is executed and is initiated with data’s preprocessing followed by flood forecasting (FF) utilizing Conjugate gradient deep neural network (CGDNN) centred on K-Fold cross-validation train test split. Experiential outcomes exhibited that the framework proposed yielded 96.66% prediction accuracy and outshined the existent top-notch method.}
}
@article{KWON201856,
title = {Toward data-driven idea generation: Application of Wikipedia to morphological analysis},
journal = {Technological Forecasting and Social Change},
volume = {132},
pages = {56-80},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517300859},
author = {Heeyeul Kwon and Yongtae Park and Youngjung Geum},
keywords = {Idea generation, Ideation, Morphological analysis, Wikipedia, Big data},
abstract = {The generation of new and creative ideas is vital to stimulating innovation. Morphological analysis is one appropriate method given its objective, impersonal, and systematic nature. However, how to build a morphological matrix is a critical problem, especially in the big data era. This research focuses on Wikipedia's case-specific characteristics and well-coordinated knowledge structure and attempts to integrate the platform with morphological analysis. In details, several methodological options are explored to implement Wikipedia data into morphological analysis. We then propose a Wikipedia-based approach to the development of morphological matrix, which incorporates the data on table of contents, hyperlinks, and categories. Its feasibility was demonstrated through a case study of drone technology, and its validity and effectiveness was shown based on a comparative analysis with a conventional discussion-based approach. The methodology is expected to be served as an essential supporting tool for generating creative ideas that could spark innovation.}
}
@article{WEI201810,
title = {Prediction model of outpatient flow based on behaviour data of outpatients in a Chinese tertiary hospital},
journal = {Computers in Industry},
volume = {97},
pages = {10-16},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517303974},
author = {Lan Wei and Shanshan Li and Yifang Yuan and Zheng Yao and Yue Huang and Dan Yue and Xin Shi and Qing Wang and Xiaolu Fei},
keywords = {Outpatients’ selection after payment, Patient queue information, Data analysis, Prediction model},
abstract = {Objective
Outpatients at Chinese tertiary hospitals are always over-crowded due to the “walk in” mode and all of the services provided within the hospital, which include patient interviews, lab workups, imaging examinations and prescription fill. The aim of this study was to build a model to predict sequential patterns of the services that the patients sought to use and then provide advice for what services they can choose and in which order to avoid long waiting times.
Method
Data collected from outpatient information systems were used to construct a data warehouse. Using the Hadoop distributed platform, outpatient data were processed and analysed using SparkR. The algorithms used included exploratory data, correlation analysis, and machine learning algorithms to analyse the patient flow data.
Results
Approximately 2 hundred thousand qualified records were used for the training set, and 89 thousand records were used for the test set. A prediction model for patient flow was built to predict a patient’s selection from the patients who utilized more than one service in a single outpatient visit. This model can predict the patient’s behaviour of filling prescriptions before going to other services (lab tests or imaging examination), with accuracy rates of 80.94 and 73%, respectively. Diagnosis classification, insurance type, gender and the other three attributes were considered key factors affecting the patient’s selection.
Conclusions
This model calculates the selection likelihood of each patient after seeing a doctor and then estimates the number of patients waiting at the in-hospital pharmacy, laboratory and radiology services within a time interval (e.g., half an hour). In addition, it can be used as a guide for outpatient services in Chinese tertiary hospitals after further optimization.}
}
@article{ZHAO20181,
title = {Tweets or nighttime lights: Comparison for preeminence in estimating socioeconomic factors},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {146},
pages = {1-10},
year = {2018},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0924271618302375},
author = {Naizhuo Zhao and Guofeng Cao and Wei Zhang and Eric L. Samson},
keywords = {Nighttime lights imagery, Twitter, Socioeconomic factors, Location-based social media, The United States},
abstract = {Nighttime lights (NTL) imagery is one of the most commonly used tools to quantitatively study socioeconomic systems over large areas. In this study we aim to use location-based social media big data to challenge the primacy of NTL imagery on estimating socioeconomic factors. Geo-tagged tweets posted in the contiguous United States in 2013 were retrieved to produce a tweet image with the same spatial resolution of the NTL imagery (i.e., 0.00833° × 0.00833°). Sum tweet (the total number of tweets) and sum light (summed DN value of the NTL image) of each state or county were obtained from the tweets and the NTL images, respectively, to estimate three important socioeconomic factors: personal income, electric power consumption, and fossil fuel carbon dioxide emissions. Results show that sum tweet is a better measure of personal income and electric power consumption while carbon dioxide emissions can be more accurately estimated by sum light. We further exploited that African-Americans adults are more likely than White seniors to post geotagged tweets in the US, yet did not find any significant correlations between proportions of the subpopulations and the estimation accuracy of the socioeconomic factors. Existence of saturated pixels and blooming effects and failure to remove gas flaring reduce quality of NTL imagery in estimating socioeconomic factors, however, such problems are nonexistent in the tweet images. This study reveals that the number of geo-tagged tweets has great potential to be deemed as a substitute of brightness of NTL to assess socioeconomic factors over large geographic areas.}
}
@article{RAI2017105,
title = {Data for development: The case for an Indian energy information administration},
journal = {Energy Research & Social Science},
volume = {25},
pages = {105-109},
year = {2017},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214629617300026},
author = {Varun Rai and Rahul Tongia and Gireesh Shrimali and Nikit Abhyankar},
keywords = {India, Energy data, Energy governance, Energy institutions},
abstract = {Energy in India has a big data problem – the world’s second most populous and fastest growing large economy does not have a singular central body in charge of maintaining and disseminating India’s energy data, let alone analyzing it. Different pockets of data come from different bodies, some statutory, but these lack granularity and timeliness, even before we get to the challenges of data completeness, accuracy, and differences in methodology and assumptions. We propose that India should create a national Energy Information Agency – an Indian EIA or “indEIA.” India urgently needs a dedicated, central agency to collect, collate, disseminate, and facilitate the analysis of all essential energy-related data. An entity like indEIA will be critical in helping India leverage the creative powers of the national and international analyst community to provide reliable, cost-effective, and clean energy to its citizens. Our proposal envisions indEIA as the primary vehicle for curating and maintaining India’s energy data.}
}
@article{POWER2017150,
title = {A simple but useful way to assess fMRI scan qualities},
journal = {NeuroImage},
volume = {154},
pages = {150-158},
year = {2017},
note = {Cleaning up the fMRI time series: Mitigating noise with advanced acquisition and correction strategies},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2016.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1053811916303871},
author = {Jonathan D. Power},
abstract = {This short “how to” article describes a plot I find useful for assessing fMRI data quality. I discuss the reasoning behind the plot and how it is constructed. I create the plot in scans from several publicly available datasets to illustrate different kinds of fMRI signal variance, ranging from thermal noise to motion artifacts to respiratory-related signals. I also show how the plot can be used to understand the variance removed during denoising. Code to make the plot is provided with the article, and supplemental movies show plots for hundreds of additional subjects.}
}
@incollection{GREIS201775,
title = {Chapter 5 - A Data-Driven Approach to Food Safety Surveillance and Response},
editor = {S. Kennedy},
booktitle = {Food Protection and Security},
publisher = {Woodhead Publishing},
pages = {75-99},
year = {2017},
series = {Woodhead Publishing Series in Food Science, Technology and Nutrition},
isbn = {978-1-78242-251-8},
doi = {https://doi.org/10.1016/B978-1-78242-251-8.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781782422518000059},
author = {N.P. Greis and M.L. Nogueira},
keywords = {Food safety, big data, predictive analytics, situational awareness, visualization},
abstract = {Increasingly, new techniques of big data and predictive analytics are being marshaled to reduce the time, scale and scope of foodborne contamination events. Contamination of food can occur at any point across increasingly complicated and intersecting global food chains. By the time a food safety problem is suspected, several weeks may have passed since first contact with the tainted product. And only after a foodborne outbreak has been confirmed by laboratory tests can the responsible products be identified and recalled. The entire process from detection to product recall can take weeks, even months. Dealing with the twin problems of early detection and rapid response are the core challenges of food safety today. In this chapter we describe a prototype informatics tool called NCFEDA (North Carolina Foodborne Events Data Integration and Analysis) that builds situational awareness of emerging contamination events by fusing traditional and nontraditional data sources, predictive analytics, visualization tools, and real-time collaboration across stakeholders to reduce the latency in detecting and responding to emerging contamination events.}
}
@incollection{MANNING2018119,
title = {3.09 - Collaborative Historical Information Analysis},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {119-144},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09658-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489096585},
author = {Patrick Manning and Pieter François and Daniel Hoyer and Vladimir Zadorozhny},
keywords = {Big historical data, Human system, Longue durée history, Metadata, Ontology, Social science analysis},
abstract = {This article addresses the task of building world-historical data resources. The group that has formed, Big Data in Human History, is a collaboration of several social science research groups using advanced information technology to document the characteristics of human society at multiple scales from the present back to early human times. The article underscores the need for global social science analysis but also the major scientific and organizational challenges of such analysis. The article introduces the five social science groups working on parallel and interactive projects at the scale of humanity over historical time, building interconnections while remaining distinct. For instance, researchers working on times 5000 years ago must use different data and techniques than those working on the period since 1800, though many of the analytical questions are similar. The first half of the article reviews the global framework, showing categories of scale and theory on a global level, and then describes how the research projects of the groups are distributed over time and analytical focus. The second half of the article provides more detailed exploration of the research process in large-scale social science analysis, especially through examples from the Collaborative for Historical Information and Analysis as well as Seshat: Global History Databank. These sections begin with the information infrastructure—data collection, archiving, documentation, linking, analysis, and visualization—to be developed by the groups. Details on data and especially on metadata are central to the aggregation of data to a global level. Closely related are the formal ontologies that must be developed in space, time, topics, and levels of aggregation. Analysis relies on established methods of social science analysis, but especially on new techniques. The concluding section explores prospects for deeper collaboration in quantitative analysis of the characteristics of human society.}
}
@article{MESTRE201727,
title = {Towards the efficient parallelization of multi-pass adaptive blocking for entity matching},
journal = {Journal of Parallel and Distributed Computing},
volume = {101},
pages = {27-40},
year = {2017},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2016.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0743731516301459},
author = {Demetrio Gomes Mestre and Carlos Eduardo Santos Pires and Dimas Cassimiro Nascimento},
keywords = {Entity matching, Indexing, Blocking, Adaptive windowing},
abstract = {Modern parallel computing programming models, such as MapReduce (MR), have proven to be powerful tools for efficient parallel execution of data-intensive tasks such as Entity Matching (EM) in the era of Big Data. For this reason, studies about challenges and possible solutions of how EM can benefit from this well-known cloud computing programming model have become an important demand nowadays. Furthermore, the effectiveness and scalability of MR-based implementations for EM depend on how well the workload distribution is balanced among all reduce tasks. In this article, we investigate how MapReduce can be used to perform efficient (load balanced) parallel EM using a variation of the multi-pass Sorted Neighborhood Method (SNM) that uses a varying size (adaptive) window. We propose Multi-pass MapReduce Duplicate Count Strategy (MultiMR-DCS++), a MR-based approach for multi-pass adaptive SNM, aiming to increase even more the performance of the SNM. The evaluation results based on real-world datasets and cluster infrastructure show that our approach increases the performance of MapReduce-based SNM regarding the EM execution time and detection quality.}
}
@article{SAEZ2018109,
title = {Kinematics of Big Biomedical Data to characterize temporal variability and seasonality of data repositories: Functional Data Analysis of data temporal evolution over non-parametric statistical manifolds},
journal = {International Journal of Medical Informatics},
volume = {119},
pages = {109-124},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S138650561830563X},
author = {Carlos Sáez and Juan M García-Gómez},
keywords = {Temporal stability, Data quality, Time series, Data reuse, Big data, Seasonality, Coordinate-free, Trajectories, Functional data analysis, Statistical manifolds},
abstract = {Aim
The increasing availability of Big Biomedical Data is leading to large research data samples collected over long periods of time. We propose the analysis of the kinematics of data probability distributions over time towards the characterization of data temporal variability.
Methods
First, we propose a kinematic model based on the estimation of a continuous data temporal trajectory, using Functional Data Analysis over the embedding of a non-parametric statistical manifold which points represent data temporal batches, the Information Geometric Temporal (IGT) plot. This model allows measuring the velocity and acceleration of data changes. Next, we propose a coordinate-free method to characterize the oriented seasonality of data based on the parallelism of lagged velocity vectors of the data trajectory throughout the IGT space, the Auto-Parallelism of Velocity Vectors (APVV) and APVVmap. Finally, we automatically explain the maximum variance components of the IGT space coordinates by means of correlating data points with known temporal factors from the domain application.
Materials
Methods are evaluated on the US National Hospital Discharge Survey open dataset, consisting of 3,25M hospital discharges between 2000 and 2010.
Results
Seasonal and abrupt behaviours were present on the estimated multivariate and univariate data trajectories. The kinematic analysis revealed seasonal effects and punctual increments in data celerity, the latter mainly related to abrupt changes in coding. The APVV and APVVmap revealed oriented seasonal changes on data trajectories. For most variables, their distributions tended to change to the same direction at a 12-month period, with a peak of change of directionality at mid and end of the year. Diagnosis and Procedure codes also included a 9-month periodic component. Kinematics and APVV methods were able to detect seasonal effects on extreme temporal subgrouped data, such as in Procedure code, where Fourier and autocorrelation methods were not able to. The automated explanation of IGT space coordinates was consistent with the results provided by the kinematic and seasonal analysis. Coordinates received different meanings according to the trajectory trend, seasonality and abrupt changes.
Discussion
Treating data as a particle moving over time through a multidimensional probabilistic space and studying the kinematics of its trajectory has turned out to a new temporal variability methodology. Its results on the NHDS were aligned with the dataset and population descriptions found in the literature, contributing with a novel temporal variability characterization. We have demonstrated that the APVV and APVVmat are an appropriate tool for the coordinate-free and oriented analysis of trajectories or complex multivariate signals.
Conclusion
The proposed methods comprise an exploratory methodology for the characterization of data temporal variability, what may be useful for a reliable reuse of Big Biomedical Data repositories acquired over long periods of time.}
}
@article{RAJABIASADABADI2018292,
title = {Letter: The concept of stratification and future applications},
journal = {Applied Soft Computing},
volume = {66},
pages = {292-296},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618300942},
author = {Mehdi {Rajabi Asadabadi} and Morteza Saberi and Elizabeth Chang},
keywords = {Concept of stratification (CST), Fuzzy logic, Granulation, Artificial intelligence (AI)},
abstract = {The main purpose of this letter is to draw attention to a recent concept, namely Concept of Stratification (CST) developed by Zadeh [1]. CST describes a system that transitions through a number of states in order to arrive at a desired state. CST is a problem-solving approach, which is easy while effective. Therefore, CST seems very likely to emerge in coming years as a major interest area in areas such as soft computing, Artificial Intelligence (AI), robotics, Natural Language Processing (NLP), and big data. In this expository letter, the advantages and the main shortcoming of CST are reviewed. The concept is explained and areas that the concept is likely to be applied are discussed. Considering the generality of the original CST proposed by Zadeh, it is possible to consider different versions for CST to be applied in future studies. Hence, versions of CST including fuzzy CST, a 3DCST, and multiple systems and multiple CSTs are presented. This work is a first step in a vast range of applications of CST. Researchers, especially those applying soft computing tools such as fuzzy sets theory and granulation, are encouraged to examine the capability of CST in addressing significant real-world problems.}
}
@article{KRAMER2018154,
title = {Market power, regulatory convergence, and the role of data in digital markets},
journal = {Telecommunications Policy},
volume = {42},
number = {2},
pages = {154-171},
year = {2018},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2017.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0308596117302744},
author = {Jan Krämer and Michael Wohlfarth},
keywords = {Market power, Market definition, Level-playing field, Data-driven business models, Big data analysis, Data protection, GDPR, Over-the-top services, Regulatory framework, Policy},
abstract = {The increased economic importance of digital services has profoundly changed the power structure in telecommunications and media markets. Although these services sometimes directly compete with traditional telecommunications services, the regulatory obligations for both players differ significantly. This article discusses three important areas deemed relevant in order to define a coherent regulatory framework and to account for the specific peculiarities of digital markets: First, challenges associated with assessing market power in digital markets. Second, challenges in harmonizing different regulatory obligations for digital services, and third, the vital role of data and data protection in the context of data-driven business models.}
}
@article{JIANG201850,
title = {A survey of real-time approximate nearest neighbor query over streaming data for fog computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {116},
pages = {50-62},
year = {2018},
note = {Towards the Internet of Data: Applications, Opportunities and Future Challenges},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518300182},
author = {Xiaohui Jiang and Peng Hu and Yanchao Li and Chi Yuan and Isma Masood and Hamed Jelodar and Mahdi Rabbani and Yongli Wang},
keywords = {Fog computing, Streaming data, Approximate nearest neighbor query, Real-time analysis, Hashing learning, Quantization method},
abstract = {Real-time approximate nearest neighbor (ANN) query over streaming data in fog computing environment is the fundamental problem of real-time analysis of big data. As the fog computing paradigm needs to provide real-time and low latency services, and traditional streaming data ANN query technology cannot be directly applied. Exploring the basic theory, querying framework and technology of real-time ANN query over streaming data for fog computing becomes one of the current research hotspots. This paper summarizes the related ANN query technology based on random hash, learning-to-hash and synopses, analyzes the problems and challenges of real-time ANN query in resource-limited fog computing environment, and finally discusses in detail the basic theory and method of the query, the dimension reduction and encoding method based on learning-to-hash, the generating synopses method for ANN query over streaming data from Internet of Thing, and the future related research directions of ANN query framework and others. Additionally, we propose a Dynamic Adaptive Quantization (DAQ) method for learning-to-hash. Experiments show that DAQ outperformed other quantization methods.}
}
@article{GAO2017172,
title = {Constructing gazetteers from volunteered Big Geo-Data based on Hadoop},
journal = {Computers, Environment and Urban Systems},
volume = {61},
pages = {172-186},
year = {2017},
note = {Geospatial Cloud Computing and Big Data},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0198971514000209},
author = {Song Gao and Linna Li and Wenwen Li and Krzysztof Janowicz and Yue Zhang},
keywords = {Gazetteers, Volunteered geographic information, Hadoop, Scalable geoprocessing workflow, Big Geo-Data, CyberGIS},
abstract = {Traditional gazetteers are built and maintained by authoritative mapping agencies. In the age of Big Data, it is possible to construct gazetteers in a data-driven approach by mining rich volunteered geographic information (VGI) from the Web. In this research, we build a scalable distributed platform and a high-performance geoprocessing workflow based on the Hadoop ecosystem to harvest crowd-sourced gazetteer entries. Using experiments based on geotagged datasets in Flickr, we find that the MapReduce-based workflow running on the spatially enabled Hadoop cluster can reduce the processing time compared with traditional desktop-based operations by an order of magnitude. We demonstrate how to use such a novel spatial-computing infrastructure to facilitate gazetteer research. In addition, we introduce a provenance-based trust model for quality assurance. This work offers new insights on enriching future gazetteers with the use of Hadoop clusters, and makes contributions in connecting GIS to the cloud computing environment for the next frontier of Big Geo-Data analytics.}
}
@article{WANG20171945,
title = {Risk Assessment and Online Forewarning of Oil & Gas Storage and Transportation Facilities Based on Data Mining},
journal = {Procedia Computer Science},
volume = {112},
pages = {1945-1953},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917313947},
author = {Tianteng Wang and Tong Li and Yingzhi Xia and Zexi Zhang and Shikai Jin},
keywords = {oil & gas storage, transportation, risk assessment, online forewarning, data mining},
abstract = {In this paper, we focus on oil & gas storage and transportations facilities (OSTF), and research on the risk assessment and the hysteretic nature of contingency plans. First, with principal component analysis to select forewarning indicators, we structure a risk assessment model based on logistic regression, and the model has strong robustness. What’s more, we link multidimensional data with different contingency plans by K-means Cluster Analysis. In conclusion, we realize the accuracy of assessment of OSTF from the view of data mining, and come up with a new management mode including “Big data monitoring---Intelligent assessment---Automatically forewarning---The advance of emergency measures” with the help of MIS to realize the timeliness of contingency management.}
}
@article{ARISTODEMOU201837,
title = {The state-of-the-art on Intellectual Property Analytics (IPA): A literature review on artificial intelligence, machine learning and deep learning methods for analysing intellectual property (IP) data},
journal = {World Patent Information},
volume = {55},
pages = {37-51},
year = {2018},
note = {Advanced Analytics of Intellectual Property Information for TechMining},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0172219018300103},
author = {Leonidas Aristodemou and Frank Tietze},
keywords = {Intellectual property analytics, Patent analytics, Machine learning, Deep learning, Artificial intelligence},
abstract = {Big data is increasingly available in all areas of manufacturing and operations, which presents an opportunity for better decision making and discovery of the next generation of innovative technologies. Recently, there have been substantial developments in the field of patent analytics, which describes the science of analysing large amounts of patent information to discover trends. We define Intellectual Property Analytics (IPA) as the data science of analysing large amount of IP information, to discover relationships, trends and patterns for decision making. In this paper, we contribute to the ongoing discussion on the use of intellectual property analytics methods, i.e artificial intelligence methods, machine learning and deep learning approaches, to analyse intellectual property data. This literature review follows a narrative approach with search strategy, where we present the state-of-the-art in intellectual property analytics by reviewing 57 recent articles. The bibliographic information of the articles are analysed, followed by a discussion of the articles divided in four main categories: knowledge management, technology management, economic value, and extraction and effective management of information. We hope research scholars and industrial users, may find this review helpful when searching for the latest research efforts pertaining to intellectual property analytics.}
}
@incollection{SALIMI2018279,
title = {Chapter 5 - Modeling and Simulation: The Essential Tools to Manage the Complexities},
editor = {Fabienne Salimi and Frederic Salimi},
booktitle = {A Systems Approach to Managing the Complexities of Process Industries},
publisher = {Elsevier},
pages = {279-407},
year = {2018},
isbn = {978-0-12-804213-7},
doi = {https://doi.org/10.1016/B978-0-12-804213-7.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042137000050},
author = {Fabienne Salimi and Frederic Salimi},
keywords = {Model Based System Engineering (MBSE), IIoT, Cybersecurity, Big Data management, Cloud computing, Virtual & Augmented reality, Industry 4.0, interoperability, Integration of PSM-OTS-RBI, ADEPP},
abstract = {This chapter discusses the modeling and simulations that are needed to understand and manage complex systems such as a process plant and its supply chain values. Recent progress in web technology, connectivity and interoperability of the automation provides the required tools to implement effectively process safety management system as a part of the operational excellence on an IIoT platform. A background discussion on industry 4.0, IIoT, Cybersecurity, cloud and fog computing, “Big Data Management,” virtual and augmented reality is given. The PLM and IIoT software are discussed and concept of Installation Lifecycle Management (ILM) for process industry is introduced. Chapter concluded the discussion by presentation of ADEPP approach to integrate Process Safety Management (PSM), Operator Training Simulator (OTS) and Risk Based Inspection (RBI) softwares on a user-friendly web based platform. Thanks to OPC connection facilities of Ignition software, the process simulations, Safety models and RBI interfaces are interoperated on visualised on a common User Graphic Interface.}
}
@incollection{ZHAO2017183,
title = {Chapter 11 - A Taxonomy and Survey of Stream Processing Systems},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {183-206},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000119},
author = {Xinwei Zhao and Saurabh Garg and Carlos Queiroz and Rajkumar Buyya},
keywords = {Stream processing platforms, CEP, Data stream management systems, Distributed systems},
abstract = {In the era of big data, an unprecedented amount of data is generated every second. The real time analytics has become a force for transforming organizations which are looking for increasing their consumer base and profit. Therefore, the real time stream processing systems have gained a lot of attention, particularly within social media companies such as Twitter and LinkedIn. To identify the open challenges in the area of stream processing and facilitate future advancements, it is essential to synthesize and categorize current stream processing systems. In this chapter, we propose a taxonomy that characterizes and classifies various stream systems. Based on the taxonomy we present a survey and comparison study of the state-of-the-art open source stream computing platforms. The taxonomy and survey is intended to help researchers by providing insights into capabilities of existing stream platforms and businesses by providing criteria that can be leveraged to identify the most suitable stream processing solution that can be adopted for developing their domain-specific applications.}
}
@article{LIM2018121,
title = {From data to value: A nine-factor framework for data-based value creation in information-intensive services},
journal = {International Journal of Information Management},
volume = {39},
pages = {121-135},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217300816},
author = {Chiehyeon Lim and Ki-Hun Kim and Min-Jun Kim and Jun-Yeon Heo and Kwang-Jae Kim and Paul P. Maglio},
keywords = {Big data, Data-based value creation, Information-intensive service, Factor, Data–Value Chain},
abstract = {Service is a key context for the application of IT, as IT digitizes information interactions in service and facilitates value creation, thereby contributing to service innovation. The recent proliferation of big data provides numerous opportunities for information-intensive services (IISs), in which information interactions exert the greatest effect on value creation. In the modern data-rich economy, understanding mechanisms and related factors of data-based value creation in IISs is essential for using IT to improve such services. This study identified nine key factors that characterize this data-based value creation: (1) data source, (2) data collection, (3) data, (4) data analysis, (5) information on the data source, (6) information delivery, (7) customer (information user), (8) value in information use, and (9) provider network. These factors were identified and defined through six action research projects with industry and government that used specific datasets to design new IISs and by analyzing data usage in 149 IIS cases. This paper demonstrates the usefulness of these factors for describing, analyzing, and designing the entire value creation chain, from data collection to value creation, in IISs. The main contribution of this study is to provide a simple yet comprehensive and empirically tested basis for the use and management of data to facilitate service value creation.}
}
@article{JING201715,
title = {Quick attribute reduction with generalized indiscernibility models},
journal = {Information Sciences},
volume = {397-398},
pages = {15-36},
year = {2017},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.02.032},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517305297},
author = {Fan Jing and Jiang Yunliang and Liu Yong},
keywords = {Generalized indiscernibility relation, Attribute reduction, Granular structure, Acceleration policy},
abstract = {The efficiency of attribute reduction is one of the important challenges being faced in the field of Big Data processing. Although many quick attribute reduction algorithms have been proposed, they are tightly coupled with their corresponding indiscernibility relations, and it is difficult to extend specific acceleration policies to other reduction models. In this paper, we propose a generalized indiscernibility reduction model(GIRM) and a concept of the granular structure in GIRM, which is a quantitative measurement induced from multiple indiscernibility relations and which can be used to represent the computation cost of varied models. Then, we prove that our GIRM is compatible with three typical reduction models. Based on the proposed GIRM, we present a generalized attribute reduction algorithm and a generalized positive region computing algorithm. We perform a quantitative analysis of the computation complexities of two algorithms using the granular structure. For the generalized attribute reduction, we present systematic acceleration policies that can reduce the computational domain and optimize the computation of the positive region. Based on the granular structure, we propose acceleration policies for the computation of the generalized positive region, and we also propose fast positive region computation approaches for three typical reduction models. Experimental results for various datasets prove the efficiency of our acceleration policies in those three typical reduction models.}
}
@article{JAFFEE2017e653,
title = {Future cancer research priorities in the USA: a Lancet Oncology Commission},
journal = {The Lancet Oncology},
volume = {18},
number = {11},
pages = {e653-e706},
year = {2017},
issn = {1470-2045},
doi = {https://doi.org/10.1016/S1470-2045(17)30698-8},
url = {https://www.sciencedirect.com/science/article/pii/S1470204517306988},
author = {Elizabeth M Jaffee and Chi Van Dang and David B Agus and Brian M Alexander and Kenneth C Anderson and Alan Ashworth and Anna D Barker and Roshan Bastani and Sangeeta Bhatia and Jeffrey A Bluestone and Otis Brawley and Atul J Butte and Daniel G Coit and Nancy E Davidson and Mark Davis and Ronald A DePinho and Robert B Diasio and Giulio Draetta and A Lindsay Frazier and Andrew Futreal and Sam S Gambhir and Patricia A Ganz and Levi Garraway and Stanton Gerson and Sumit Gupta and James Heath and Ruth I Hoffman and Cliff Hudis and Chanita Hughes-Halbert and Ramy Ibrahim and Hossein Jadvar and Brian Kavanagh and Rick Kittles and Quynh-Thu Le and Scott M Lippman and David Mankoff and Elaine R Mardis and Deborah K Mayer and Kelly McMasters and Neal J Meropol and Beverly Mitchell and Peter Naredi and Dean Ornish and Timothy M Pawlik and Jeffrey Peppercorn and Martin G Pomper and Derek Raghavan and Christine Ritchie and Sally W Schwarz and Richard Sullivan and Richard Wahl and Jedd D Wolchok and Sandra L Wong and Alfred Yung},
abstract = {Summary
We are in the midst of a technological revolution that is providing new insights into human biology and cancer. In this era of big data, we are amassing large amounts of information that is transforming how we approach cancer treatment and prevention. Enactment of the Cancer Moonshot within the 21st Century Cures Act in the USA arrived at a propitious moment in the advancement of knowledge, providing nearly US$2 billion of funding for cancer research and precision medicine. In 2016, the Blue Ribbon Panel (BRP) set out a roadmap of recommendations designed to exploit new advances in cancer diagnosis, prevention, and treatment. Those recommendations provided a high-level view of how to accelerate the conversion of new scientific discoveries into effective treatments and prevention for cancer. The US National Cancer Institute is already implementing some of those recommendations. As experts in the priority areas identified by the BRP, we bolster those recommendations to implement this important scientific roadmap. In this Commission, we examine the BRP recommendations in greater detail and expand the discussion to include additional priority areas, including surgical oncology, radiation oncology, imaging, health systems and health disparities, regulation and financing, population science, and oncopolicy. We prioritise areas of research in the USA that we believe would accelerate efforts to benefit patients with cancer. Finally, we hope the recommendations in this report will facilitate new international collaborations to further enhance global efforts in cancer control.}
}
@article{KAVAKIOTIS2017146,
title = {FIFS: A data mining method for informative marker selection in high dimensional population genomic data},
journal = {Computers in Biology and Medicine},
volume = {90},
pages = {146-154},
year = {2017},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2017.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0010482517303189},
author = {Ioannis Kavakiotis and Patroklos Samaras and Alexandros Triantafyllidis and Ioannis Vlahavas},
keywords = {Bioinformatics, Machine learning, Data mining, Feature selection, Frequent pattern mining, Single nucleotide polymorphism, Population genomics, Ancestry informative marker, Big data},
abstract = {Background and objective
Single Nucleotide Polymorphism (SNPs) are, nowadays, becoming the marker of choice for biological analyses involving a wide range of applications with great medical, biological, economic and environmental interest. Classification tasks i.e. the assignment of individuals to groups of origin based on their (multi-locus) genotypes, are performed in many fields such as forensic investigations, discrimination between wild and/or farmed populations and others. Τhese tasks, should be performed with a small number of loci, for computational as well as biological reasons. Thus, feature selection should precede classification tasks, especially for Single Nucleotide Polymorphism (SNP) datasets, where the number of features can amount to hundreds of thousands or millions.
Methods
In this paper, we present a novel data mining approach, called FIFS – Frequent Item Feature Selection, based on the use of frequent items for selection of the most informative markers from population genomic data. It is a modular method, consisting of two main components. The first one identifies the most frequent and unique genotypes for each sampled population. The second one selects the most appropriate among them, in order to create the informative SNP subsets to be returned.
Results
The proposed method (FIFS) was tested on a real dataset, which comprised of a comprehensive coverage of pig breed types present in Britain. This dataset consisted of 446 individuals divided in 14 sub-populations, genotyped at 59,436 SNPs. Our method outperforms the state-of-the-art and baseline methods in every case. More specifically, our method surpassed the assignment accuracy threshold of 95% needing only half the number of SNPs selected by other methods (FIFS: 28 SNPs, Delta: 70 SNPs Pairwise FST: 70 SNPs, In: 100 SNPs.)
Conclusion
Our approach successfully deals with the problem of informative marker selection in high dimensional genomic datasets. It offers better results compared to existing approaches and can aid biologists in selecting the most informative markers with maximum discrimination power for optimization of cost-effective panels with applications related to e.g. species identification, wildlife management, and forensics.}
}
@article{FAN2017396,
title = {The moderating effect of external pressure on the relationship between internal organizational factors and the quality of open government data},
journal = {Government Information Quarterly},
volume = {34},
number = {3},
pages = {396-405},
year = {2017},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2017.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17300801},
author = {Bo Fan and Yupan Zhao},
keywords = {Open government data, Data quality, Institutional capacity, External pressure},
abstract = {Open government data (OGD) have important political, economic, and social values that are highly valued by many countries around the world. These data represent the cross-boundary information sharing practices between governments and the public. Therefore, this paper investigates OGD by following the theory and practice of cross-boundary information sharing. Most of the current studies have focused on OGD platform, whereas this study has focused on individual government departments and datasets. From the perspective of public data users, we constructed an evaluation index to measure OGD quality, which reflected the degree of cross-boundary information sharing between the governments and the public. We specifically concentrated on the external environment of OGD and selected 128 government departments in Shanghai, Beijing, and Wuhan in China as samples. We found that institutional capacity is an important factor for OGD quality of individual government departments. However, technology capacity and organization arrangement demonstrated small significance for OGD quality. The pressure from the public and higher-level government departments can moderate the relationship between institutional capacity and OGD quality. These results can guide governments in prioritizing the improvement of OGD quality and implementing an OGD project.}
}
@article{KOZJEK2017214,
title = {Interpretative identification of the faulty conditions in a cyclic manufacturing process},
journal = {Journal of Manufacturing Systems},
volume = {43},
pages = {214-224},
year = {2017},
note = {High Performance Computing and Data Analytics for Cyber Manufacturing},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2017.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612517300304},
author = {Dominik Kozjek and Rok Vrabič and David Kralj and Peter Butala},
keywords = {Production process, Fault identification, Root cause analysis, Decision support, Big Data},
abstract = {The intensive development of information and communication technologies in recent years has led to an increase in data size and complexity. Conventional approaches, with associated methods of analysis based on descriptive and inductive statistics, may no longer be suitable for extracting the valuable information that is hidden in the available data. Computer-controlled manufacturing systems are becoming rich sources of data. Plastic injection moulding and die casting systems are typical examples of such manufacturing systems where the parts are produced by repeating the same sequence of steps that make up a manufacturing cycle. For each cycle, similarly structured data is generated. In this work a method for systematic data analysis for cyclic manufacturing processes is presented. The proposed data-analysis method integrates well-known heuristic algorithms, i.e., decision trees and clustering, with the purpose of identifying types of faulty operating conditions. The result of the analysis is an interpretable model for decision support that can be used for fault identification, to search for root causes, and to develop prognostic systems. A holistic approach of applying the proposed data-analysis method, along with suggestions and guidelines for implementation, is presented. A case study is presented in which the proposed method is applied to real industrial data from a plastic injection-moulding process.}
}
@article{ALSHDIFAT201831,
title = {Development of a Context-aware framework for the Integration of Internet of Things and Cloud Computing for Remote Monitoring Services},
journal = {Procedia Manufacturing},
volume = {16},
pages = {31-38},
year = {2018},
note = {Proceedings of the 7th International Conference on Through-life Engineering Services},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.10.155},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918312769},
author = {Ali Al-Shdifat and Christos Emmanouilidis},
keywords = {Internet of Things, Context Management, Cloud Computing, Remote Monitoring Services},
abstract = {With global competition and technological progress, there have been growing demands by industry for more efficiency in monitoring the health status of the manufacturing equipment in real time. Remote monitoring services in the era of Industry 4.0 are nonetheless faced some challenges such as big data’s 4V challenges (volume, velocity, variety, veracity), scalability, data heterogeneity, as well as relevant to integrating data with domain knowledge. While all these pose problems in conventional monitoring, they become even more challenges when integrating IoT and cloud computing to deliver advanced services to offer infrastructure availability and ubiquitous accessibility. Although it offers many benefits and solution enablers, substantial effort is required to manage and exploit the data generated by "things". Among the key instruments to tackle such complexity is the concept of context information management. This paper proposes a conceptual context-aware framework for the integration of Internet of Things and cloud computing for remote monitoring services.}
}
@article{SCHOVSBO2018203,
title = {Oil production monitoring and optimization from produced water analytics; a case study from the Halfdan chalk oil field, Danish North Sea},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {8},
pages = {203-210},
year = {2018},
note = {3rd IFAC Workshop on Automatic Control in Offshore Oil and Gas Production OOGP 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.378},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318307080},
author = {Niels H. Schovsbo and Sofie N. Gottfredsen and Karen G. Schmidt and Thomas M. Jørgensen},
keywords = {Big Data, History Matching, Reservoir Simulation Optimization, Management, Production Monitoring, Automation, Optimization},
abstract = {Produced water analysis is a direct source of information to the subsurface processes active in an oil field. The information is, however, complex and requires a multidisciplinary approach and access to multiple data types and sources to successfully unlock and decode the processes. We apply data analytics on a combined data set of water chemistry and oil and gas production data measured in the production stream from five wells in the Halfdan field. The field is produced applying extensive water injection to ensure the most efficient water sweep of the reservoir. Relationships between daily production data and water chemistry are examined with Principal Component Analysis (PCA), and systematics with respect to predictability of daily changes in the oil production from water chemistry are examined with partial least square (PLS) regression models. For each well, the water chemistry provides a high degree of predictability with respect to daily oil cut in the production stream. The results have potential for application within prediction of sweep efficiency, by-passed oil and for prediction of water break-through. Full potential, however, depend on successful implementation of water chemistry-oil production analytics into other data domains such as seismic (4D) data and well work-over data.}
}
@article{VENKATRAMANAN201843,
title = {Using data-driven agent-based models for forecasting emerging infectious diseases},
journal = {Epidemics},
volume = {22},
pages = {43-49},
year = {2018},
note = {The RAPIDD Ebola Forecasting Challenge},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2017.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1755436517300221},
author = {Srinivasan Venkatramanan and Bryan Lewis and Jiangzhuo Chen and Dave Higdon and Anil Vullikanti and Madhav Marathe},
keywords = {Emerging infectious diseases, Agent-based models, Simulation optimization, Bayesian calibration, Ebola},
abstract = {Producing timely, well-informed and reliable forecasts for an ongoing epidemic of an emerging infectious disease is a huge challenge. Epidemiologists and policy makers have to deal with poor data quality, limited understanding of the disease dynamics, rapidly changing social environment and the uncertainty on effects of various interventions in place. Under this setting, detailed computational models provide a comprehensive framework for integrating diverse data sources into a well-defined model of disease dynamics and social behavior, potentially leading to better understanding and actions. In this paper, we describe one such agent-based model framework developed for forecasting the 2014–2015 Ebola epidemic in Liberia, and subsequently used during the Ebola forecasting challenge. We describe the various components of the model, the calibration process and summarize the forecast performance across scenarios of the challenge. We conclude by highlighting how such a data-driven approach can be refined and adapted for future epidemics, and share the lessons learned over the course of the challenge.}
}
@article{RECUEROVIRTO201847,
title = {A preliminary assessment of the indicators for Sustainable Development Goal (SDG) 14 “Conserve and sustainably use the oceans, seas and marine resources for sustainable development”},
journal = {Marine Policy},
volume = {98},
pages = {47-57},
year = {2018},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2018.08.036},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X18301131},
author = {Laura {Recuero Virto}},
keywords = {Q01, Q20, Q30, Oceans, Sustainable development goal, Indicators},
abstract = {The SDGs are intended to address sustainable development processes in both developed and developing countries, and to facilitate action at all levels and with all actors, including government, civil society, the private sector and the science community. The SDG 14 covers, among other features, economic pressures on the marine environment and takes into account the specificities of coastal communities. This paper reviews the rational for the SDG 14, as well as the framework for the SDG 14 indicators including (i) some basic concepts such as the role of uncertainty, irreversibility and thresholds in the marine context, and the multidimensionality of the indicators; (ii) synergies and trade-offs among the SDG 14 targets, and between SDG 14 and other SDGs targets, and how to track progress on policy coherence at the national level; (iii) synergies between SDG 14 indicators, and Millennium Development Goals’ and Multilateral Environmental Agreements’ targets and indicators; and (iv) the role of big data. Indicators at the global and national scales (France) are also explored. To conclude, there are challenges and opportunities for future research in this area such as the development of indicators building on the frontiers of ocean science, the development of innovative approaches for data collection, the development of common approaches in valuing marine ecosystem services and national accounting, the provision of incentives for best practice and peer-learning, the harmonisation of measurement methodologies and the selection of SDG 14 indicators according to the geographical level of intervention.}
}
@article{MONTEIRO2018244,
title = {An urban building database (UBD) supporting a smart city information system},
journal = {Energy and Buildings},
volume = {158},
pages = {244-260},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817319485},
author = {Claudia Sousa Monteiro and Carlos Costa and André Pina and Maribel Y. Santos and Paulo Ferrão},
keywords = {Urban database, Building stock data, Energy data, Urban energy consumption, Smart city},
abstract = {Urban energy modelling requires a large amount of detailed data to perform systematic dynamic simulations of a large number of buildings, where the adoption of energy efficiency strategies is an important concern for sustainable urban planning. National statistical datasets collect important aggregated data regarding building construction, energy consumption and occupants, and cities are making a significant effort to update spatial referenced data of their territory. However, these data is generally not detailed enough, being available at different scales and in different formats. The integrated use of these data is critical to validate different methods to predict and model energy consumption in cities, as well in addressing its energy saving potential. Furthermore, scenario analysis for retrofit or new design is only possible at building scale, highlighting the potential of a bottom-up database. This paper presents the process of collecting, mapping, cleansing and integrating urban data resulting in an UBD to support an information system for Smart Cities. The goal is to reduce the gap between the available urban data and the specific data required to run a complete urban building energy simulation. Key characteristics of an UBD are explored and applied to a case study in Lisbon, Portugal. As a result, a Buildings Dashboard is developed, materializing the UBD user interface. This dashboard allows the interactive visualization and data exploration of the building stock at multiple scales. Future work includes the development of an Urban Scenario category, bringing new insights on urban energy simulation and scenario evaluation to this platform.}
}
@article{KIRCHDOERFER2017622,
title = {Data Driven Computing with noisy material data sets},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {326},
pages = {622-641},
year = {2017},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2017.07.039},
url = {https://www.sciencedirect.com/science/article/pii/S0045782517304012},
author = {T. Kirchdoerfer and M. Ortiz},
keywords = {Data science, Big data, Approximation theory, Scientific computing},
abstract = {We formulate a Data Driven Computing paradigm, termed max-ent Data Driven Computing, that generalizes distance-minimizing Data Driven Computing and is robust with respect to outliers. Robustness is achieved by means of clustering analysis. Specifically, we assign data points a variable relevance depending on distance to the solution and on maximum-entropy estimation. The resulting scheme consists of the minimization of a suitably-defined free energy over phase space subject to compatibility and equilibrium constraints. Distance-minimizing Data Driven schemes are recovered in the limit of zero temperature. We present selected numerical tests that establish the convergence properties of the max-ent Data Driven solvers and solutions.}
}
@article{CHEN2018307,
title = {Water quality monitoring in smart city: A pilot project},
journal = {Automation in Construction},
volume = {89},
pages = {307-316},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517305988},
author = {Yiheng Chen and Dawei Han},
keywords = {Water quality monitoring, High-frequency, Real-time, Internet of Things, Smart city},
abstract = {A smart city is an urban development vision to integrate multiple information and communication technology (ICT), “Big Data” and Internet of Things (IoT) solutions in a secure fashion to manage a city's assets for sustainability, resilience and liveability. Meanwhile, water quality monitoring has been evolving to the latest wireless sensor network (WSN) based solutions in recent decades. This paper presents a multi-parameter water quality monitoring system of Bristol Floating Harbour which has successfully demonstrated the feasibility of collecting real-time high-frequency water quality data and displayed the real-time data online. The smart city infrastructure – Bristol Is Open was utilised to provide a plug & play platform for the monitoring system. This new system demonstrates how a future smart city can build the environment monitoring system benefited by the wireless network covering the urban area. The system can be further integrated in the urban water management system to achieve improved efficiency.}
}
@article{SAMMER2018627,
title = {Workshop Synthesis: Validation under "ground truth" in surveys},
journal = {Transportation Research Procedia},
volume = {32},
pages = {627-633},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301601},
author = {Gerd Sammer and Alfred Chu},
keywords = {Survey method, quality of survey, validation, weighing, uncertainty, ground truth},
abstract = {Travel surveys as well as freight transport surveys are the basis for any fact-based transport policy. It needs to be stressed that the collection of data and description of the existing mobility behaviour and transport demand is linked to a considerable amount of uncertainty. Therefore, it is greatly important to assure the quality of the collection of travel behaviour data. If one considers the current guidelines for planning and surveys, from that point of view, it is amazing how little attention is paid to the disclosure of the data quality and uncertainty directly. Therefore, the standardized definition of requirements to achieve high quality for survey results is essential. This workshop tried to further develop validation procedures in order to come closer to the "ground truth" in surveys.}
}
@article{MCCARTY2017142,
title = {Extracting smallholder cropped area in Tigray, Ethiopia with wall-to-wall sub-meter WorldView and moderate resolution Landsat 8 imagery},
journal = {Remote Sensing of Environment},
volume = {202},
pages = {142-151},
year = {2017},
note = {Big Remotely Sensed Data: tools, applications and experiences},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2017.06.040},
url = {https://www.sciencedirect.com/science/article/pii/S0034425717302997},
author = {J.L. McCarty and C.S.R. Neigh and M.L. Carroll and M.R. Wooten},
keywords = {Big data, WorldView-1, WorldView-2, Landsat 8, Agriculture, Smallholder farms, Crop area, Image segmentation, Ethiopia},
abstract = {Very high resolution (VHR) satellite data is experiencing rapid annual growth, producing petabytes of remotely sensed data per year. The WorldView constellation, operated by DigitalGlobe, images over 1.2billionkm2 annually at <2m spatial resolution. Due to computation, data cost, and methodological concerns, VHR satellite data has mainly been used to produce needed geospatial information for site-specific phenomena. This project produced a VHR spatiotemporally explicit wall-to-wall cropland area map for the rainfed residential cropland mosaic of the Tigray Region, Ethiopia, which is comprised mostly of smallholder farms. Moderate resolution satellite data do not have adequate spatial resolution to capture the total area occupied by smallholder farms, i.e., farms with agricultural fields of ≤45×45m in dimension. In order to accurately map smallholder cropped area over a large region, hundreds of VHR images spanning two or more years are needed. Sub-meter WorldView-1 and WorldView-2 segmentation results were combined with median phenology amplitude from Landsat 8 data to map cropped area. Over 2700 VHR WorldView-1, -2 data were obtained from the U.S. National Geospatial-Intelligence Agency (NGA) via the NextView license agreement and were processed from raw imagery to produce a smallholder crop map in ~1week using a semi-automated method with the large computing capacity of the Advanced Data Analytics Platform. We estimated cropped area in Tigray to be 46% with a commission error of 5%±10% and omission error of 15%±12%. This methodology is extensible to other regions with similar vegetation texture and can easily be expanded to run on much larger regions.}
}
@article{XIANG201751,
title = {A comparative analysis of major online review platforms: Implications for social media analytics in hospitality and tourism},
journal = {Tourism Management},
volume = {58},
pages = {51-65},
year = {2017},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0261517716301807},
author = {Zheng Xiang and Qianzhou Du and Yufeng Ma and Weiguo Fan},
keywords = {Online reviews, Hotel industry, Information quality, Social media analytics, Text analytics, Machine learning},
abstract = {Online consumer reviews have been studied for various research problems in hospitality and tourism. However, existing studies using review data tend to rely on a single data source and data quality is largely anecdotal. This greatly limits the generalizability and contribution of social media analytics research. Through text analytics this study comparatively examines three major online review platforms, namely TripAdvisor, Expedia, and Yelp, in terms of information quality related to online reviews about the entire hotel population in Manhattan, New York City. The findings show that there are huge discrepancies in the representation of the hotel industry on these platforms. Particularly, online reviews vary considerably in terms of their linguistic characteristics, semantic features, sentiment, rating, usefulness as well as the relationships between these features. This study offers a basis for understanding the methodological challenges and identifies several research directions for social media analytics in hospitality and tourism.}
}
@article{BIBRI2017183,
title = {Smart sustainable cities of the future: An extensive interdisciplinary literature review},
journal = {Sustainable Cities and Society},
volume = {31},
pages = {183-212},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716304073},
author = {Simon Elias Bibri and John Krogstie},
keywords = {Smart cities, Sustainable cities, Smart sustainable cities, Sustainable urban forms, Urban sustainability, Sustainable development goals, ICT, Computing, Planning, Big data analytics},
abstract = {In recent years, the concept of smart sustainable cities has come to the fore. And it is rapidly gaining momentum and worldwide attention as a promising response to the challenge of urban sustainability. This pertains particularly to ecologically and technologically advanced nations. This paper provides a comprehensive overview of the field of smart (and) sustainable cities in terms of its underlying foundations and assumptions, state–of–the art research and development, research opportunities and horizons, emerging scientific and technological trends, and future planning practices. As to the design strategy, the paper reviews existing sustainable city models and smart city approaches. Their strengths and weaknesses are discussed with particular emphasis being placed on the extent to which the former contributes to the goals of sustainable development and whether the latter incorporates these goals. To identify the related challenges, those models and approaches are evaluated and compared against each other in line with the notion of sustainability. The gaps in the research within the field of smart sustainable cities are identified in accordance with and beyond the research being proposed. As a result, an integrated approach is proposed based on an applied theoretical perspective to align the existing problems and solutions identification for future practices in the area of smart sustainable urban planning and development. As to the findings, the paper shows that critical issues remain unsettled, less explored, largely ignored, and theoretically underdeveloped for applied purposes concerning existing models of sustainable urban form as to their contribution to sustainability, among other things. It also reveals that numerous research opportunities are available and can be realized in the realm of smart sustainable cities. Our perspective on the topic in this regard is to develop a theoretically and practically convincing model of smart sustainable city or a framework for strategic smart sustainable urban development. This model or framework aims to address the key limitations, uncertainties, paradoxes, and fallacies pertaining to existing models of sustainable urban form—with support of ICT of the new wave of computing and the underlying big data and context–aware computing technologies and their advanced applications. We conclude that the applied theoretical inquiry into smart sustainable cities of the future is deemed of high pertinence and importance—given that the research in the field is still in its early stages, and that the subject matter draws upon contemporary and influential theories with practical applications. The comprehensive overview of and critique on existing work on smart (and) sustainable cities provide a valuable and seminal reference for researchers and practitioners in related research communities and the necessary material to inform these communities of the latest developments in the area of smart sustainable urban planning and development. In addition, the proposed holistic approach is believed to be the first of its kind. That is, it has not been, to the best of one’s knowledge, investigated or produced elsewhere.}
}
@article{JERNIGAN20181,
title = {Introduction},
journal = {Developmental Cognitive Neuroscience},
volume = {32},
pages = {1-3},
year = {2018},
note = {The Adolescent Brain Cognitive Development (ABCD) Consortium: Rationale, Aims, and Assessment Strategy},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1878929317301883},
author = {Terry L. Jernigan and Sandra A. Brown},
abstract = {The Adolescent Brain Cognitive Development (ABCD) Study is a longitudinal, observational study of over 10,000 youth recruited at 21 sites throughout the United States. Comprehensive biennial assessments and more limited interim assessments measure health, mental health, neurocognition, family, cultural and environmental variables, substance use, genetic and other biomarkers, and structural and functional brain development. Within this Special Issue, readers will find much information about the rationale and objectives of the study, the broad ranging assessment protocols and new as well as traditional methodologies applied at baseline, the recruitment and retention strategies, and the anticipated final composition of the cohort. Information is also provided about how the study is coordinated and conducted, how decisions are made, how data quality is monitored, and how ethical standards are protected. In this introduction we will focus instead on the position of the ABCD Study in the changing landscape of biomedical research.}
}
@article{SINGH2018398,
title = {Social media data analytics to improve supply chain management in food industries},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {398-415},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2017.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1366554516303817},
author = {Akshit Singh and Nagesh Shukla and Nishikant Mishra},
keywords = {Beef supply chain, Twitter data, Sentiment analysis},
abstract = {This paper proposes a big-data analytics-based approach that considers social media (Twitter) data for the identification of supply chain management issues in food industries. In particular, the proposed approach includes text analysis using a support vector machine (SVM) and hierarchical clustering with multiscale bootstrap resampling. The result of this approach included a cluster of words which could inform supply-chain (SC) decision makers about customer feedback and issues in the flow/quality of food products. A case study in the beef supply chain was analysed using the proposed approach, where three weeks of data from Twitter were used.}
}
@article{HU2018225,
title = {Framework for a smart data analytics platform towards process monitoring and alarm management},
journal = {Computers & Chemical Engineering},
volume = {114},
pages = {225-244},
year = {2018},
note = {FOCAPO/CPC 2017},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2017.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098135417303551},
author = {Wenkai Hu and Sirish L. Shah and Tongwen Chen},
keywords = {Analytics, Big data, Performance monitoring, Process monitoring, Alarm systems, Process data analytics, Fault detection and diagnosis},
abstract = {The fusion of information from disparate sources of data is the key step in devising strategies for a smart analytics platform. In the context of the application of analytics in the process industry, this paper provides a framework for seamless integration of information from process and alarm databases complimented with process connectivity information. The discovery of information from such diverse data sources can be subsequently used for process and performance monitoring including alarm rationalization, root cause diagnosis of process faults, hazard and operability analysis, safe and optimal process operation. The utility of the proposed framework is illustrated by several successful industrial case studies.}
}
@article{THORISSON2017255,
title = {Multiscale identification of emergent and future conditions along corridors of transportation networks},
journal = {Reliability Engineering & System Safety},
volume = {167},
pages = {255-263},
year = {2017},
note = {Special Section: Applications of Probabilistic Graphical Models in Dependability, Diagnosis and Prognosis},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2017.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0951832016304604},
author = {Heimir Thorisson and James H. Lambert},
keywords = {Risk analysis, Transportation planning, Data visualization, Corridor trace analysis, Systems engineering},
abstract = {Transportation agencies are challenged with managing large-scale networks of roads, spanning geographic areas diverse in terms of jurisdictions, topography, demographics, economy, organizations, and others. Risk management, asset management, and resource allocation must be approached from a holistic perspective without compromising specific regional needs. Agencies collect a large amount of data, the utilization of which should be transparent and consistent with their stated aims. The innovation of this paper is a corridor trace analysis, a method for identifying anomalies of hazard intensity, exposure, and vulnerability along many thousands of kilometers of a transportation network by integrating key road characteristic and performance metrics to straight-line diagrams of corridor sections. Road segments under stress are identified by searching for one or more characteristics that are outliers with respect to the contextual data. The paper includes demonstrations of this method for big-data integration on a real-world system, focusing on how the method is useful to shift among geographic scales. The demonstrations suggest the efficacy of the approach to sustain the efficient, reliable, and safe movement of passengers and freight.}
}
@article{GHASEMAGHAEI2018101,
title = {Data analytics competency for improving firm decision making performance},
journal = {The Journal of Strategic Information Systems},
volume = {27},
number = {1},
pages = {101-113},
year = {2018},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2017.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717300768},
author = {Maryam Ghasemaghaei and Sepideh Ebrahimi and Khaled Hassanein},
keywords = {Data analytics competency, Data quality, Bigness of data, Analytical skills, Domain knowledge, Tools sophistication, Decision making performance},
abstract = {This study develops and validates the concept of Data Analytics Competency as a five multidimensional formative index (i.e., data quality, bigness of data, analytical skills, domain knowledge, and tools sophistication) and empirically examines its impact on firm decision making performance (i.e., decision quality and decision efficiency). The findings based on an empirical analysis of survey data from 151 Information Technology managers and data analysts demonstrate a large, significant, positive relationship between data analytics competency and firm decision making performance. The results reveal that all dimensions of data analytics competency significantly improve decision quality. Furthermore, interestingly, all dimensions, except bigness of data, significantly increase decision efficiency. This is the first known empirical study to conceptualize, operationalize and validate the concept of data analytics competency and to study its impact on decision making performance. The validity of the data analytics competency construct as conceived and operationalized, suggests the potential for future research evaluating its relationships with possible antecedents and consequences. For practitioners, the results provide important guidelines for increasing firm decision making performance through the use of data analytics.}
}
@incollection{FERSTER201826,
title = {1.04 - Current Themes in Volunteered Geographic Information},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {26-41},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09620-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489096202},
author = {Colin J. Ferster and Trisalyn Nelson and Colin Robertson and Rob Feick},
keywords = {Active transportation, Citizen science, Climate change, Cycling, Data quality, Outdoor skating, Social media, Wildfire},
abstract = {Volunteered geographic information (VGI) is the use of digital tools to collect, analyze, and share geographic information that was provided by individuals. Recent advances in digital communication tools and applications have led to unprecedented advances for VGI. Because official credentials are not required, VGI can include the experiences and perspectives of many people, respond rapidly to local needs, and be more spatially and temporally extensive than traditional approaches. VGI encompasses a wide range of activities, from simple and possibly unintentional actions, such as georeferencing messages or pictures shared on the Internet, to the intentional involvement of volunteers in scientific research. The motivations of volunteers and researchers for these activities are wide and ranging and relate to the products that are generated. This article focuses on intentional efforts by geographers to collect VGI. Two main barriers to adoption of VGI for decision making are concerns about data quality and incomplete representation. Strategies to address these concerns can lead to new discoveries and improved decision making in governance. In this article we introduce VGI, discuss theoretical developments, and relate them to three recent experiences.}
}
@article{PORCIUNCULA2018S220,
title = {Wearable Movement Sensors for Rehabilitation: A Focused Review of Technological and Clinical Advances},
journal = {PM&R},
volume = {10},
number = {9, Supplement 2},
pages = {S220-S232},
year = {2018},
note = {Innovations Influencing Physical Medicine and Rehabilitation},
issn = {1934-1482},
doi = {https://doi.org/10.1016/j.pmrj.2018.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1934148218303630},
author = {Franchino Porciuncula and Anna Virginia Roto and Deepak Kumar and Irene Davis and Serge Roy and Conor J. Walsh and Louis N. Awad},
abstract = {Recent technologic advancements have enabled the creation of portable, low-cost, and unobtrusive sensors with tremendous potential to alter the clinical practice of rehabilitation. The application of wearable sensors to track movement has emerged as a promising paradigm to enhance the care provided to patients with neurologic or musculoskeletal conditions. These sensors enable quantification of motor behavior across disparate patient populations and emerging research shows their potential for identifying motor biomarkers, differentiating between restitution and compensation motor recovery mechanisms, remote monitoring, telerehabilitation, and robotics. Moreover, the big data recorded across these applications serve as a pathway to personalized and precision medicine. This article presents state-of-the-art and next-generation wearable movement sensors, ranging from inertial measurement units to soft sensors. An overview of clinical applications is presented across a wide spectrum of conditions that have potential to benefit from wearable sensors, including stroke, movement disorders, knee osteoarthritis, and running injuries. Complementary applications enabled by next-generation sensors that will enable point-of-care monitoring of neural activity and muscle dynamics during movement also are discussed.}
}
@article{SOLIS2018143,
title = {Engaging global youth in participatory spatial data creation for the UN sustainable development goals: The case of open mapping for malaria prevention},
journal = {Applied Geography},
volume = {98},
pages = {143-155},
year = {2018},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0143622818300456},
author = {Patricia Solís and Brent McCusker and Nwasinachi Menkiti and Nuala Cowan and Chad Blevins},
keywords = {Sustainable development goals, Remote sensing, Participatory, Open data, Malaria},
abstract = {Practitioners bemoan lack of data as one of the biggest obstacles to progress towards global sustainable development goals. This paper explores a scaled-up participatory method developed by YouthMappers, for creating missing geospatial data derived from remotely sensed imagery in order to contribute to persistent data needs in the context of the United Nations Sustainable Development Goals (SDGs). We explore the application of this method to a case related to SDG 3 on Health. We document how our approach centered on creating a global academic network designed to engage and empower university students and their faculty mentors to participate in broader efforts to create open, free spatial data on open platforms to inform humanitarian and development objectives outlined by the funding agency, the United States Agency for International Development (USAID). This approach expressly links supply and demand for geospatial knowledge by connecting specific needs for geographic information to specific development objectives in targeted places where USAID works to end extreme poverty. We discuss the rationale and context for the methodology as it draws from and builds upon prominent literature of participatory GIS (PGIS) and volunteered geographic information (VGI). We demonstrate how the mapping of building and road infrastructure in Mozambique and Kenya was carried out in order to provide information for an insecticide spray campaign to prevent malaria and protect public health. Throughout these efforts, steps are taken to ensure spatial data quality and to offer opportunities for youth volunteer embeddedness in mapping tasks and themes in places where students otherwise would not engage with real world data or connect with peers from different countries. We reflect on the opportunities and challenges for how this scaled-up “remote participatory sensing” approach to spatial data creation can inform development projects in the context of the SDGs.}
}
@article{ALFAROALMAGRO2018400,
title = {Image processing and Quality Control for the first 10,000 brain imaging datasets from UK Biobank},
journal = {NeuroImage},
volume = {166},
pages = {400-424},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917308613},
author = {Fidel Alfaro-Almagro and Mark Jenkinson and Neal K. Bangerter and Jesper L.R. Andersson and Ludovica Griffanti and Gwenaëlle Douaud and Stamatios N. Sotiropoulos and Saad Jbabdi and Moises Hernandez-Fernandez and Emmanuel Vallee and Diego Vidaurre and Matthew Webster and Paul McCarthy and Christopher Rorden and Alessandro Daducci and Daniel C. Alexander and Hui Zhang and Iulius Dragonu and Paul M. Matthews and Karla L. Miller and Stephen M. Smith},
keywords = {Epidemiological studies, Image analysis pipeline, Multi-modal data integration, Quality control, Big data imaging, Machine learning},
abstract = {UK Biobank is a large-scale prospective epidemiological study with all data accessible to researchers worldwide. It is currently in the process of bringing back 100,000 of the original participants for brain, heart and body MRI, carotid ultrasound and low-dose bone/fat x-ray. The brain imaging component covers 6 modalities (T1, T2 FLAIR, susceptibility weighted MRI, Resting fMRI, Task fMRI and Diffusion MRI). Raw and processed data from the first 10,000 imaged subjects has recently been released for general research access. To help convert this data into useful summary information we have developed an automated processing and QC (Quality Control) pipeline that is available for use by other researchers. In this paper we describe the pipeline in detail, following a brief overview of UK Biobank brain imaging and the acquisition protocol. We also describe several quantitative investigations carried out as part of the development of both the imaging protocol and the processing pipeline.}
}
@article{OLARU201854,
title = {Workshop Synthesis: Passive and sensor data - potential and application},
journal = {Transportation Research Procedia},
volume = {32},
pages = {54-61},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301613},
author = {Doina Olaru and Alejandro Tudela},
keywords = {passive data, big data, sensor, GPS, Wi-Fi, smartcard, transport, travel surveys},
abstract = {The workshop on technology, tools and applications around passive and sensor travel data is summarized in this paper. Such data requires protocols for collection, storing/retrieving, sharing and processing; as well as the need for validation methods and multi-disciplinary work. Traditional surveys can complement passive and sensor data to aid a deeper understanding of travel behaviour. Passive data is particularly beneficial for planning long-distance travel and freight transport. While access to massive data collected by licensed operators should be guaranteed, maintaining data privacy and cultural sensitivity is a priority.}
}
@article{KAMBLE2018408,
title = {Sustainable Industry 4.0 framework: A systematic literature review identifying the current trends and future perspectives},
journal = {Process Safety and Environmental Protection},
volume = {117},
pages = {408-425},
year = {2018},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2018.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0957582018301629},
author = {Sachin S. Kamble and Angappa Gunasekaran and Shradha A. Gawankar},
keywords = {Industry 4.0, Smart manufacturing, Internet of things, Process safety, Augmented reality, Sustainability, Big data},
abstract = {Industry 4.0 and its other synonyms like Smart Manufacturing, Smart Production or Internet of Things, have been identified as major contributors in the context of digital and automated manufacturing environment. The term industry 4.0 comprises a variety of technologies to enable the development of the value chain resulting in reduced manufacturing lead times, and improved product quality and organizational performance. Industry 4.0 has attracted much attention in the recent literature, however there are very few systematic and extensive review of research that captures the dynamic nature of this topic. The rapidly growing interest from both academics and practitioners in Industry 4.0 has urged the need for review of up-to-date research and development to develop a new agenda. Selected 85 papers were classified in five research categories namely conceptual papers on Industry 4.0, human-machine interactions, machine-equipment interactions, technologies of Industry 4.0 and sustainability. The review primarily attempted to seek answers to the following two questions: (1) What are different research approaches used to study Industry 4.0? and (2) What is the current status of research in the domains of Industry 4.0?. We propose a sustainable Industry 4.0 framework based on the findings of the review with three critical components viz., Industry 4.0 technologies, process integration and sustainable outcomes. Finally, the scope of future research is discussed in detail.}
}
@article{EFROS201784,
title = {How to Quantify the Impact of Lossy Transformations on Event Detection},
journal = {Big Data Research},
volume = {9},
pages = {84-97},
year = {2017},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579616300855},
author = {Pavel Efros and Erik Buchmann and Adrian Englhardt and Klemens Böhm},
keywords = {Time series, Lossy transformations, Event detection, Change detection},
abstract = {To ease the proliferation of big data, it frequently is transformed, be it by compression, be it by anonymization. Such transformations however modify characteristics of the data. In the case of time series, important characteristics are the occurrence of certain changes or patterns in the data, also referred to as events. Clearly, the less transformations modify events, the better for subsequent analyses. More specifically, the severity of those modifications depends on the application scenario, and quantifying it is far from trivial. In this paper, we propose MILTON, a flexible and robust Measure for quantifying the Impact of Lossy Transformations on subsequent event detectiON. MILTON is applicable to any lossy transformation technique on time-series data and to any general-purpose event-detection approach. We have evaluated it with several real-world use cases. Our evaluation shows that MILTON allows to quantify the impact of lossy transformations and to choose the best one from a class of transformation techniques for a given application scenario.}
}
@incollection{BEDIA20181,
title = {Chapter One - Introduction to the Data Analysis Relevance in the Omic Era},
editor = {Joaquim Jaumot and Carmen Bedia and Romà Tauler},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {82},
pages = {1-12},
year = {2018},
booktitle = {Data Analysis for Omic Sciences: Methods and Applications},
issn = {0166-526X},
doi = {https://doi.org/10.1016/bs.coac.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X18300758},
author = {Carmen Bedia and Romà Tauler and Joaquim Jaumot},
keywords = {Data analysis, Workflow, Chemometrics, Omics, Transcriptomics, Metabolomics},
abstract = {We are witnesses of a revolution in a variety of research fields due to the development of high-throughput analytical techniques that have caused the emergence of omic sciences. In all these new research fields, the primary goal is to obtain the maximum information from the different biological levels to understand their structure, function or dynamics. This is particularly true in the case of nontargeted studies in which all the possible compounds are evaluated without following an a priori hypothesis. However, retrieving the relevant biological or chemical information from these vast data sets is not straightforward and, for many years, the data analysis has been considered as one of the major bottlenecks in omics. For these reasons, several data analysis strategies have been proposed for dealing with these omic data sets that, in some cases, can be considered as a type of scientific Big Data. In this introductory chapter, we highlight the importance of these data processing strategies taking as a reference the traditional omic workflow. A brief introduction to the major issues and current solutions to perform the data analysis in omic sciences is presented.}
}
@article{STIEGLITZ2018156,
title = {Social media analytics – Challenges in topic discovery, data collection, and data preparation},
journal = {International Journal of Information Management},
volume = {39},
pages = {156-168},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217308526},
author = {Stefan Stieglitz and Milad Mirbabaie and Björn Ross and Christoph Neuberger},
keywords = {Social media analytics, Social media, Information systems, Big data},
abstract = {Since an ever-increasing part of the population makes use of social media in their day-to-day lives, social media data is being analysed in many different disciplines. The social media analytics process involves four distinct steps, data discovery, collection, preparation, and analysis. While there is a great deal of literature on the challenges and difficulties involving specific data analysis methods, there hardly exists research on the stages of data discovery, collection, and preparation. To address this gap, we conducted an extended and structured literature analysis through which we identified challenges addressed and solutions proposed. The literature search revealed that the volume of data was most often cited as a challenge by researchers. In contrast, other categories have received less attention. Based on the results of the literature search, we discuss the most important challenges for researchers and present potential solutions. The findings are used to extend an existing framework on social media analytics. The article provides benefits for researchers and practitioners who wish to collect and analyse social media data.}
}
@article{SAMMER2018649,
title = {The dilemma of systematic underreporting of travel behavior when conducting travel diary surveys – A meta-analysis and methodological considerations to solve the problem},
journal = {Transportation Research Procedia},
volume = {32},
pages = {649-658},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301571},
author = {Gerd Sammer and Christian Gruber and Gerald Roeschel and Rupert Tomschy and Max Herry},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Travel behaviour surveys are an essential basis for the implementation of a transport policy in line with society’s targets. Therefore, the assurance of the quality of the collection of data about travel behaviour is enormously important. A key problem is the fact that transport demand is underreported and biased in transport surveys. This is true for both passenger traffic and goods transport, if one uses the indicator ‛trip between origin-destination’. Depending on the mode of transport and mobility indicator, such as length of trip, duration of trip, modal split, travel purpose or share of mobile persons, the underreporting of trips has different implications. It is generally true that the published results of travel behaviour surveys, both at a national and local level, hardly ever address this underreporting of trips which, according to estimates, amounts to up to -30% of the total. The reasons for the underreporting of travel behaviour are complex. The survey method itself is one key cause. Suitable weighting and imputation processes for trips which were not recorded might be a way to avoid any bias due to underreporting. By using a passive GPS survey in combination with traditional travel diary survey and in-depth interviews, trips missing from a complete mobility pattern of individual interviewees can be identified and used in an imputation model. Another promising solution is the use of weighting with independently collected “Big Data” of the transport networks.}
}
@article{SOAR2017182,
title = {Estimating bedload transport rates in a gravel-bed river using seismic impact plates: Model development and application},
journal = {Environmental Modelling & Software},
volume = {90},
pages = {182-200},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217300877},
author = {Philip J. Soar and Peter W. Downs},
keywords = {Coarse bedload transport, Fluvial geomorphology, Monte Carlo simulation, Sediment monitoring, Seismic impact plate, Uncertainty},
abstract = {A data-driven, uncertainty-bound estimation technique for bedload transport rates is developed based on passive sensing devices. The model converts sediment samples to a mass in transit for each instantaneous discharge according to impacts detected and a Monte Carlo simulation of the load determined at random from the particle size distribution. Using impact count data autogenically produces a supply-limited, location-specific and high-resolution time-series of bedload rates, while the probabilistic approach inherently accommodates the stochastic nature of bedload transport. Application to the River Avon (Devon, U.K.) provides cross-sectional bedload rate estimates within the bounds of experimental data and calibrated to observed field behaviour. This new procedure offers an alternative ‘class’ of bedload estimation to existing approaches and has the potential for wide-ranging applications in river management and restoration, while contributing to the integration of ‘big data’ into a progressive agenda for hydrogeomorphology research.}
}
@incollection{ZUFFEREY2018107,
title = {Chapter 6 - Unsupervised Learning Methods for Power System Data Analysis},
editor = {Reza Arghandeh and Yuxun Zhou},
booktitle = {Big Data Application in Power Systems},
publisher = {Elsevier},
pages = {107-124},
year = {2018},
isbn = {978-0-12-811968-6},
doi = {https://doi.org/10.1016/B978-0-12-811968-6.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119686000061},
author = {Thierry Zufferey and Andreas Ulbig and Stephan Koch and Gabriela Hug},
keywords = {Data visualization, Distribution grid transparency, K-Means clustering, Smart meter data},
abstract = {This chapter focuses on the use of the K-Means clustering algorithm for an enhanced visibility of the electrical distribution system which can be provided by advanced metering infrastructure and supported by big data technologies and parallel cloud computing environments such as Spark and H2O. Based on smart meter data of more than 30,000 loads in the City of Basel, Switzerland, and thanks to an appropriate cluster analysis, it is shown that useful knowledge of the grid state can be gained without any further information concerning the type of consumer and their habits. Once energy data is judiciously prepared, the features extraction is an important step. A graphical user interface is presented which illustrates the potentially great flexibility in the choice of features according to the needs of distribution system operators (DSOs). For example, the distribution of the various types of customers across the power system is of interest to DSOs. This chapter presents thus some pertinent examples of clustering outcomes that are visualized on the map of Basel, which notably enables to easily identify heating and cooling demand or gain insight into the energy consumption throughout the day for different neighborhoods.}
}
@article{SAPOUNTZI2018893,
title = {Social networking data analysis tools & challenges},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {893-913},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1630423X},
author = {Androniki Sapountzi and Kostas E. Psannis},
keywords = {Sentiment analysis, Topic detection, Social network analysis, Collaborative recommendation, Computational intelligence, Online Social Networks},
abstract = {Online Social Network’s (OSN) considered a spark that burst the Big Data era. The unfolding of every event, breaking new or trend flows in real time inside OSN triggering a surge of opinionated networked content. An unprecedented scale of social relationships also diffuses across this vastly interconnected system affecting public behaviors and knowledge construction. Extracting intelligence from such data has becoming a quickly widening multidisciplinary area that demands the synergy of scientific tools and expertise. Key analysis practices include social network analysis, sentiment analysis, trend analysis and collaborative recommendation. Though, both their recent advent and the fact that science is still in the frontiers of processing human-generated data, provokes the need for an update and comprehensible taxonomy of the related research. In response to this chaotic emerging science of social data, this paper provides a sophisticated classification of state-of the-art frameworks considering the diversity of practices, methods and techniques. To the best of our knowledge, this is the first attempt that illustrated the entire spectrum of social data networking analysis and their associated frameworks. The survey demonstrates challenges and future directions with a focus on text mining and the promising avenue of computational intelligence.}
}
@article{ZHANG2017464,
title = {China's new environmental protection regulatory regime: Effects and gaps},
journal = {Journal of Environmental Management},
volume = {187},
pages = {464-469},
year = {2017},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2016.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0301479716308854},
author = {Bo Zhang and Cong Cao and Robert M. Hughes and Wayne S. Davis},
keywords = {Air pollution, Water pollution, Law enforcement, Challenges}
}
@article{BRIGHT2018130,
title = {OpenStreetMap data for alcohol research: Reliability assessment and quality indicators},
journal = {Health & Place},
volume = {50},
pages = {130-136},
year = {2018},
issn = {1353-8292},
doi = {https://doi.org/10.1016/j.healthplace.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1353829217305804},
author = {Jonathan Bright and Stefano {De Sabbata} and Sumin Lee and Bharath Ganesh and David K. Humphreys},
keywords = {Alcohol availability, Alcohol licensing, Alcohol-related harm, OpenStreetMap, Big data},
abstract = {There is a growing interest in using OpenStreetMap [OSM] data in health research. We evaluate the usefulness of OSM data for researching the spatial availability of alcohol, a field which has been hampered by data access difficulties. We find OSM data is about 50% complete, which appears adequate for replicating findings from other studies using alcohol licensing data. Further, we show how OSM quality metrics can be used to select areas with more complete alcohol data. The ease of access and use may create opportunities for analysts and researchers seeking to understand broad patterns of alcohol availability.}
}
@article{SPRUIT2018643,
title = {Applied data science in patient-centric healthcare: Adaptive analytic systems for empowering physicians and patients},
journal = {Telematics and Informatics},
volume = {35},
number = {4},
pages = {643-653},
year = {2018},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0736585318303666},
author = {Marco Spruit and Miltiadis Lytras},
keywords = {Applied data science, Knowledge discovery process, Patient-centric healthcare, Adaptive analytic system, Meta-algorithmic modelling, Big data analytics, Natural language processing},
abstract = {We define the emerging research field of applied data science as the knowledge discovery process in which analytic systems are designed and evaluated to improve the daily practices of domain experts. We investigate adaptive analytic systems as a novel research perspective of the three intertwining aspects within the knowledge discovery process in healthcare: domain and data understanding for physician- and patient-centric healthcare, data preprocessing and modelling using natural language processing and (big) data analytic techniques, and model evaluation and knowledge deployment through information infrastructures. We align these knowledge discovery aspects with the design science research steps of problem investigation, treatment design, and treatment validation, respectively. We note that the adaptive component in healthcare system prototypes may translate to data-driven personalisation aspects including personalised medicine. We explore how applied data science for patient-centric healthcare can thus empower physicians and patients to more effectively and efficiently improve healthcare. We propose meta-algorithmic modelling as a solution-oriented design science research framework in alignment with the knowledge discovery process to address the three key dilemmas in the emerging “post-algorithmic era” of data science: depth versus breadth, selection versus configuration, and accuracy versus transparency.}
}
@article{YIN201840,
title = {Active learning based support vector data description method for robust novelty detection},
journal = {Knowledge-Based Systems},
volume = {153},
pages = {40-52},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118301850},
author = {Lili Yin and Huangang Wang and Wenhui Fan},
keywords = {Active learning, SVDD, Robust novelty detection, TEP},
abstract = {Practical industrial data usually has non-Gaussian data distribution and nonlinear variable correlation. Because support vector data description (SVDD) has no Gaussian limitations and can be extended to the nonlinear case by applying the kernel trick, it is one of the most widely used novelty detection methods. However, there is a great deal of actual industrial data that are mixed with much noise and uncertainty. Furthermore, SVDD may perform worse when the amount of data is too large and the data quality is poor. Describing the whole distribution with a small number of labeled samples has great practical significance and research value. This paper proposed an active learning-based SVDD method for robust novelty detection. It can reduce the amount of labeled data using an active learning framework, generalize the distribution of data and reduce the impact of noise by using the local density to guide the selection process. Experiments on two-dimensional synthetic distributions, UCI datasets and the Tennessee Eastman Process (TEP) show the effectiveness of the proposed method.}
}
@article{SERNA20171,
title = {Sustainability analysis on Urban Mobility based on Social Media content},
journal = {Transportation Research Procedia},
volume = {24},
pages = {1-8},
year = {2017},
note = {3rd Conference on Sustainable Urban Mobility, 3rd CSUM 2016, 26 – 27 May 2016, Volos, Greece},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2017.05.059},
url = {https://www.sciencedirect.com/science/article/pii/S235214651730340X},
author = {Ainhoa Serna and Jon Kepa Gerrikagoitia and Unai Bernabé and Tomás Ruiz},
keywords = {Urban mobility, Transportation, Social Media, Sustainable Urban Transport, Sentiment Analysis, Natural Language Processing},
abstract = {Urban transport became an important element in the promotion of strategies towards sustainability, in fact one of the challenges posed by booming urban populations is the question of mobility. Traditional travel survey methods used to study urban mobility are very expensive, and the data collected are of poor quality. This is mainly explained because of the difficulty of getting a representative sample of the population, and the lack of motivated participants. Therefore, travel surveys are carried out less and less frequently, and the result is that good travel data is not available to develop mobility and travel behaviour studies. Information and Communication Technologies (ICT) offer the opportunity to improve traditional travel survey methods, decreasing bias in the data, reducing respondent burden, and increasing data quality. On the other hand, nowadays the User Generated Content (UGC) is growing very fast in Internet. Social media have become a valuable source for knowledge but there is a big gap in the automatic Sentiment Analysis with Semantic taxonomy annotation of online textual content. The aim of this research is to identify sustainability issues related to urban mobility based in the perceptions and experiences that underlie in the UGC. The methodology follows a quantitative and qualitative content analysis using Sentiment Analysis techniques. This paper demonstrates empirically the feasibility of the automatic identification of the Sustainable Urban Mobility problems in the discourses generated by the UGC, through a powerful ad-hoc software combining Natural Language Processing and Sentiment Analysis field tools. The main contribution of this work is the development of a tool and methodology on sustainability analysis on urban environment. Our approach enriches the data of the traditional surveys, extends traditional analysis with Big-Data methods, using data mining algorithms and Natural Language Processing techniques to extract urban mobility information from Social Media data. These data include important information about activities and travels, and can help to improve our understanding of urban mobility.}
}
@article{LEWIS2017276,
title = {The Australian Geoscience Data Cube — Foundations and lessons learned},
journal = {Remote Sensing of Environment},
volume = {202},
pages = {276-292},
year = {2017},
note = {Big Remotely Sensed Data: tools, applications and experiences},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2017.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0034425717301086},
author = {Adam Lewis and Simon Oliver and Leo Lymburner and Ben Evans and Lesley Wyborn and Norman Mueller and Gregory Raevksi and Jeremy Hooke and Rob Woodcock and Joshua Sixsmith and Wenjun Wu and Peter Tan and Fuqin Li and Brian Killough and Stuart Minchin and Dale Roberts and Damien Ayers and Biswajit Bala and John Dwyer and Arnold Dekker and Trevor Dhu and Andrew Hicks and Alex Ip and Matt Purss and Clare Richards and Stephen Sagar and Claire Trenham and Peter Wang and Lan-Wei Wang},
keywords = {Landsat, Time-series, Big data, Data cube, High performance computing, High performance data, Collection management, Geometric correction, Pixel quality, Australian Geoscience Data Cube},
abstract = {The Australian Geoscience Data Cube (AGDC) aims to realise the full potential of Earth observation data holdings by addressing the Big Data challenges of volume, velocity, and variety that otherwise limit the usefulness of Earth observation data. There have been several iterations and AGDC version 2 is a major advance on previous work. The foundations and core components of the AGDC are: (1) data preparation, including geometric and radiometric corrections to Earth observation data to produce standardised surface reflectance measurements that support time-series analysis, and collection management systems which track the provenance of each Data Cube product and formalise re-processing decisions; (2) the software environment used to manage and interact with the data; and (3) the supporting high performance computing environment provided by the Australian National Computational Infrastructure (NCI). A growing number of examples demonstrate that our data cube approach allows analysts to extract rich new information from Earth observation time series, including through new methods that draw on the full spatial and temporal coverage of the Earth observation archives. To enable easy-uptake of the AGDC, and to facilitate future cooperative development, our code is developed under an open-source, Apache License, Version 2.0. This open-source approach is enabling other organisations, including the Committee on Earth Observing Satellites (CEOS), to explore the use of similar data cubes in developing countries.}
}
@article{YUE2018316,
title = {Using Taiwan National Health Insurance Database to model cancer incidence and mortality rates},
journal = {Insurance: Mathematics and Economics},
volume = {78},
pages = {316-324},
year = {2018},
note = {Longevity risk and capital markets: The 2015–16 update},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2017.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167668717304304},
author = {Jack C. Yue and Hsin-Chung Wang and Yin-Yee Leong and Wei-Ping Su},
keywords = {Cancer insurance, Longevity risk, Big data, Stochastic models, National Health Insurance},
abstract = {The increasing cancer incidence and decreasing mortality rates in Taiwan worsened the loss ratio of cancer insurance products and created a financial crisis for insurers. In general, the loss ratio of long-term health products seems to increase with the policy year. In the present study, we used the data from Taiwan National Health Insurance Research Database to evaluate the challenge of designing cancer products. We found that the Lee–Carter and APC models have the smallest estimation errors, and the CBD and Gompertz models are good alternatives to explore the trend of cancer incidence and mortality rates, especially for the elderly people. The loss ratio of Taiwan’s cancer products is to grow and this can be deemed as a form of longevity risk. The longevity risk of health products is necessary to face in the future, similar to the annuity products.}
}
@article{AGRAFIOTIS20181204,
title = {Risk-based Monitoring of Clinical Trials: An Integrative Approach},
journal = {Clinical Therapeutics},
volume = {40},
number = {7},
pages = {1204-1212},
year = {2018},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2018.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0149291818302339},
author = {Dimitris K. Agrafiotis and Victor S. Lobanov and Michael A. Farnum and Eric Yang and Joseph Ciervo and Michael Walega and Adam Baumgart and Aaron J. Mackey},
keywords = {clinical informatics, RBM, risk-based monitoring, statistical monitoring, Xcellerate},
abstract = {Purpose
Clinical trial monitoring is an essential component of drug development aimed at safeguarding subject safety, data quality, and protocol compliance by focusing sponsor oversight on the most important aspects of study conduct. In recent years, regulatory agencies, industry consortia, and nonprofit collaborations between industry and regulators, such as TransCelerate and International Committee for Harmonization, have been advocating a new, risk-based approach to monitoring clinical trials that places increased emphasis on critical data and processes and encourages greater use of centralized monitoring. However, how best to implement risk-based monitoring (RBM) remains unclear and subject to wide variations in tools and methodologies. The nonprescriptive nature of the regulatory guidelines, coupled with limitations in software technology, challenges in operationalization, and lack of robust evidence of superior outcomes, have hindered its widespread adoption.
Methods
We describe a holistic solution that combines convenient access to data, advanced analytics, and seamless integration with established technology infrastructure to enable comprehensive assessment and mitigation of risk at the study, site, and subject level.
Findings
Using data from completed RBM studies carried out in the last 4 years, we demonstrate that our implementation of RBM improves the efficiency and effectiveness of the clinical oversight process as measured on various quality, timeline, and cost dimensions.
Implications
These results provide strong evidence that our RBM methodology can significantly improve the clinical oversight process and do so at a lower cost through more intelligent deployment of monitoring resources to the sites that need the most attention.}
}
@article{REALI201827,
title = {Genomics as a service: A joint computing and networking perspective},
journal = {Computer Networks},
volume = {145},
pages = {27-51},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618307205},
author = {G. Reali and M. Femminella and E. Nunzi and D. Valocchi},
keywords = {Genomic, Pipeline, Cloud computing, Big data, Network virtualization},
abstract = {This paper shows a global picture of the deployment of networked processing services for genomic data sets. Many current research and medical activities make an extensive use of genomic data, which are massive and rapidly increasing over time. They are typically stored in remote databases, accessible by using Internet connections. For this reason, the quality of the available network services could be a significant issue for effectively handling genomic data through networks. A first contribution of this paper consists in identifying the still unexploited features of genomic data that could allow optimizing their networked management. The second and main contribution is a methodological classification of computing and networking alternatives, which can be used to deploy what we call the Genomics-as-a-Service (GaaS) paradigm. In more detail, we analyze the main genomic processing applications, and classify both the computing alternatives to run genomics workflows, in either a local machine or a distributed cloud environment, and the main software technologies available to develop genomic processing services. Since an analysis encompassing only the computing aspects would provide only a partial view of the issues for deploying GaaS systems, we present also the main networking technologies that are available to efficiently support a GaaS solution. We first focus on existing service platforms, and analyze them in terms of service features, such as scalability, flexibility, and efficiency. Then, we present a taxonomy for both wide area and datacenter network technologies that may fit the GaaS requirements. It emerges that virtualization, both in computing and networking, is the key for a successful large-scale exploitation of genomic data, by pushing ahead the adoption of the GaaS paradigm. Finally, the paper illustrates a short and long-term vision on future research challenges in the field.}
}
@article{WALLER2017371,
title = {Evaluating the replicability, specificity, and generalizability of connectome fingerprints},
journal = {NeuroImage},
volume = {158},
pages = {371-377},
year = {2017},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917305840},
author = {Lea Waller and Henrik Walter and Johann D. Kruschwitz and Lucia Reuter and Sabine Müller and Susanne Erk and Ilya M. Veer},
abstract = {Establishing reliable, robust, and unique brain signatures from neuroimaging data is a prerequisite for precision psychiatry, and therefore a highly sought-after goal in contemporary neuroscience. Recently, the procedure of connectome fingerprinting, using brain functional connectivity profiles as such signatures, was shown to be able to accurately identify individuals from a group of 126 subjects from the Human Connectome Project (HCP). However, the specificity and generalizability of this procedure were not tested. In this replication study, we show both for the original and an extended HCP data set (n = 900 subjects), as well as for an additional data set of more commonly acquired imaging quality (n = 84) that (i) although the high accuracy can be replicated for the larger HCP 900 data set, accuracy is (ii) lower for standard neuroimaging data, and, that (iii) connectome fingerprinting may not be specific enough to distinguish between individuals. In addition, both accuracy and specificity are projected to drop considerably as the size of a data set increases. Although the moderate-to-high accuracies do suggest there is a portion of unique variance, our results suggest that connectomes may actually be quite similar across individuals. This outcome may be relevant to how precision psychiatry could benefit from inferences based on functional connectomes.}
}
@article{WANG2023104206,
title = {Arrhythmia classification algorithm based on multi-head self-attention mechanism},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104206},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104206},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006607},
author = {Yue Wang and Guanci Yang and Shaobo Li and Yang Li and Ling He and Dan Liu},
keywords = {Arrhythmia classification, Electrocardiogram (ECG), Attention mechanism, Feature extraction},
abstract = {Cardiovascular disease is a major illness that causes human death, especially in the elderly. Timely and accurate diagnosis of arrhythmia types is the key to early prevention and diagnosis of cardiovascular diseases. This paper proposed an arrhythmia classification algorithm based on multi-head self-attention mechanism (ACA-MA). First, an ECG signal preprocessing algorithm based on wavelet transform is put forward and implemented using db6 wavelet transform to focus on improving the data quality of ECG signals and reduce the noise of ECG signals. Second, a linear projection layer for acquiring semantic features of ECG signals is designed using the matching relationship between ECG tag and segmented ECG signals. Third, a position encoding-based spatiotemporal characterization method of ECG signal sequences is designed to integrate time series information into a matrix operation. Fourth, a multi-head self-attentive mechanism capable of capturing global contextual information is proposed to extract relationships and semantic features between ECG segments and achieve semantic association and information stitching of nonadjacent ECG signals. Finally, experimental results on the arrhythmia dataset MIT/BIH show that ACA-MA outperforms other state-of-the-art methods with an overall classification accuracy of 99.4%, a specific rate of 99.41%, and a sensitivity of 97.36%.}
}
@article{TALEBI2018155,
title = {Sensitive association rules hiding using electromagnetic field optimization algorithm},
journal = {Expert Systems with Applications},
volume = {114},
pages = {155-172},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418304548},
author = {Behnam Talebi and Mohammad Naderi Dehkordi},
keywords = {Privacy preserving data mining, Association rules hiding, Data distortion technique, Electromagnetic field optimization algorithm},
abstract = {Privacy preserving data mining has been a major research subject in recent years. The most important goal of this area is to protect personal information and prevent disclosure of this information during the data mining process. There are various techniques in the field of privacy preserving data mining. One of these techniques is association rules mining. The main purpose of association rules mining is to hide sensitive association rules. So far, various algorithms have been presented to this field in order to reach the purpose of sensitive association rules hiding. Each algorithm has its own specific functions and methods. To hide sensitive association rules, this paper presents an electromagnetic field optimization algorithm (EFO4ARH). This algorithm utilizes the data distortion technique to hide the sensitive association rules. In this algorithm, two fitness functions are used to reach the solution with the least side effects. Also, in this algorithm, the runtime has been reduced. This algorithm consists of a technique for exiting from local optima point and moving toward global optimal points. The performance of the proposed algorithm is evaluated by doing experiments on both real-world and synthetic datasets. Compared to four reference algorithms, the proposed algorithm shows a reduction in the side effects and better preservation of data quality. The performance of EFO4ARH is tested by standard deviation and mean Friedman ranks of error for standard functions (CEC benchmarks). In addition, hiding experiments show that our proposed algorithm outperforms existing hiding algorithms.}
}
@article{MEUNIER201832,
title = {Image analysis to refine measurements of dairy cow behaviour from a real-time location system},
journal = {Biosystems Engineering},
volume = {173},
pages = {32-44},
year = {2018},
note = {Advances in the Engineering of Sensor-based Monitoring and Management Systems for Precision Livestock Farming},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2017.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S1537511017302179},
author = {Bruno Meunier and Philippe Pradel and Karen H. Sloth and Carole Cirié and Eric Delval and Marie M. Mialon and Isabelle Veissier},
keywords = {RTLS, Image analysis, Dairy cows, Behaviour, Precision livestock farming, Time-budget},
abstract = {Long-term monitoring of animal activity can yield key information for both researchers in ethology and engineers in charge of developing precision livestock farming tools. First, a barn is segmented into delimited areas (e.g. cubicles) with which an activity can be associated (e.g. resting), then a real-time location system (RTLS) can be used to automatically convert cow position into behaviour. Working within the EU-PLF project, we tested a system already able to determine basic activities (resting, moving, eating…) and logged a “big data” set of billions of data points (123 days × 190 cows × 1 location-per-second readings). We then focused on integrating image analysis techniques to help visualise and analyse the dataset, first to validate the data and then to enrich the information extracted. The algorithm developed using freely available tools quickly confirmed the ability of the system to determine cows' main activities (except drinking behaviour), even with 11% of positions missing. The good localisation precision (16 cm) made it possible to enrich the time-budget with new activities such as using brushes and licking mineral blocks. For both activities, using visual observations as gold standard, activity profiles with excellent sensitivity (nearly 80%) were extracted. This validation procedure is both necessary and generalisable to other situations. The improvement of biological information contained in such data holds promise for people designing alarm devices and health and welfare indicators for farmers and/or vets.}
}