@article{SHEN2019759,
title = {White Matter Microstructure and Its Relation to Longitudinal Measures of Depressive Symptoms in Mid- and Late Life},
journal = {Biological Psychiatry},
volume = {86},
number = {10},
pages = {759-768},
year = {2019},
note = {Cortical Pathology and Depression},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2019.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0006322319314507},
author = {Xueyi Shen and Mark J. Adams and Tuula E. Ritakari and Simon R. Cox and Andrew M. McIntosh and Heather C. Whalley},
keywords = {Big data, Depression, Longitudinal, Neuroimaging, UK Biobank, White matter microstructure},
abstract = {Background
Studies of white matter microstructure in depression typically show alterations in individuals with depression, but they are frequently limited by small sample sizes and the absence of longitudinal measures of depressive symptoms. Depressive symptoms are dynamic, however, and understanding the neurobiology of different trajectories could have important clinical implications.
Methods
We examined associations between current and longitudinal measures of depressive symptoms and white matter microstructure (fractional anisotropy and mean diffusivity [MD]) in the UK Biobank Imaging Study. Depressive symptoms were assessed on two to four occasions over 5.9 to 10.7 years (n = 18,959 individuals on at least two occasions, n = 4444 on four occasions), from which we derived four measures of depressive symptomatology: cross-sectional measure at the time of scan and three longitudinal measures, namely trajectory and mean and intrasubject variance over time.
Results
Decreased white matter microstructure in the anterior thalamic radiation demonstrated significant associations across all four measures of depressive symptoms (MD: βs = .020–.029, pcorr < .030). The greatest effect sizes were seen between white matter microstructure and longitudinal progression (MD: βs = .030–.040, pcorr < .049). Cross-sectional symptom severity was particularly associated with decreased white matter integrity in association fibers and thalamic radiations (MD: βs = .015–.039, pcorr < .041). Greater mean and within-subject variance were mainly associated with decreased white matter microstructure within projection fibers (MD: βs = .019–.029, pcorr < .044).
Conclusions
These findings indicate shared and differential neurobiological associations with severity, course, and intrasubject variability of depressive symptoms. This enriches our understanding of the neurobiology underlying dynamic features of the disorder.}
}
@article{SIMKO2019891,
title = {Challenges to the Standardization of Trauma Data Collection in Burn, Traumatic Brain Injury, Spinal Cord Injury, and Other Trauma Populations: A Call for Common Data Elements for Acute and Longitudinal Trauma Databases},
journal = {Archives of Physical Medicine and Rehabilitation},
volume = {100},
number = {5},
pages = {891-898},
year = {2019},
issn = {0003-9993},
doi = {https://doi.org/10.1016/j.apmr.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S000399931831400X},
author = {Laura C. Simko and Liang Chen and Dagmar Amtmann and Nicole Gibran and David Herndon and Karen Kowalske and A. Cate Miller and Eileen Bulger and Ryan Friedman and Audrey Wolfe and Kevin K. Chung and Michael Mosier and James Jeng and Joseph Giacino and Ross Zafonte and Lewis E. Kazis and Jeffrey C. Schneider and Colleen M. Ryan},
keywords = {Common data elements, Database, Rehabilitation, Rehabilitation research},
abstract = {Objective
Common data elements (CDEs) promote data sharing, standardization, and uniform data collection, which facilitate meta-analyses and comparisons of studies. Currently, there is no set of CDEs for all trauma populations, but their creation would allow researchers to leverage existing databases to maximize research on trauma outcomes. The purpose of this study is to assess the extent of common data collection among 5 trauma databases.
Design
The data dictionaries of 5 trauma databases were examined to determine the extent of common data collection. Databases included 2 acute care databases (American Burn Association’s National Burn Data Standard and American College of Surgeons’ National Trauma Data Standard) and 3 longitudinal trauma databases (Burn, Traumatic Brain Injury, Spinal Cord Injury Model System National Databases). Data elements and data values were compared across the databases. Quantitative and qualitative variations in the data were identified to highlight meaningful differences between datasets.
Setting
N/A.
Participants
N/A.
Interventions
N/A.
Main Outcome Measures
N/A.
Results
Of the 30 data elements examined, 14 (47%) were present in all 5 databases. Another 9 (30%) elements were present in 4 of the 5 databases. The number of elements present in each database ranged from 23 (77%) to 26 (86%). There were inconsistencies in the data values across the databases. Twelve of the 14 data elements present in all 5 databases exhibited differences in data values.
Conclusions
This study demonstrates inconsistencies in the documentation of data elements in 5 common trauma databases. These discrepancies are a barrier to database harmonization and to maximizing the use of these databases through linking, pooling, and comparing data. A collaborative effort is required to develop a standardized set of elements for trauma research.}
}
@article{BOYLES201967,
title = {Ontology-based data integration for advancing toxicological knowledge},
journal = {Current Opinion in Toxicology},
volume = {16},
pages = {67-74},
year = {2019},
note = {Systems Toxicology},
issn = {2468-2020},
doi = {https://doi.org/10.1016/j.cotox.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2468202019300051},
author = {R.R. Boyles and A.E. Thessen and A. Waldrop and M.A. Haendel},
abstract = {Modern toxicology is evolving to leverage data science approaches to better address complex public health concerns. Understanding the adverse health impacts of exposure is a multifactorial endeavor, which requires working across a variety of data types. These data are often siloed, and integration has required manual curation and extraction. In this review, we present the utility of adopting ontologies and semantic methods to bring disparate data into a scientifically meaningful context to drive novel scientific insights. Existing semantic standards have not been widely utilized in toxicology. Broader adoption of ontologies, together with increased data sharing, will improve a researcher's ability to integrate, navigate, and analyze vast amounts of heterogeneous data—allowing for more rapid assessment of chemical(s) safety and biological mechanisms. Recent efforts have aimed to define and realize the establishment of a data ecosystem or “commons” whereby data are shared for use by all in a common infrastructure, thereby increasing the value of government-funded data sets. Investment in making data “born interoperable” using common semantics could bring computational resources to bear on issues that are solely reliant on manual and expert assessment. Here, we introduce the concept of ontologies and present our vision for computationally enabled semantic toxicology.}
}
@article{BAUER2019338,
title = {Automation to optimise physician treatment of individual patients: examples in psychiatry},
journal = {The Lancet Psychiatry},
volume = {6},
number = {4},
pages = {338-349},
year = {2019},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(19)30041-0},
url = {https://www.sciencedirect.com/science/article/pii/S2215036619300410},
author = {Michael Bauer and Scott Monteith and John Geddes and Michael J Gitlin and Paul Grof and Peter C Whybrow and Tasha Glenn},
abstract = {Summary
There is widespread agreement by health-care providers, medical associations, industry, and governments that automation using digital technology could improve the delivery and quality of care in psychiatry, and reduce costs. Many benefits from technology have already been realised, along with the identification of many challenges. In this Review, we discuss some of the challenges to developing effective automation for psychiatry to optimise physician treatment of individual patients. Using the perspective of automation experts in other industries, three examples of automation in the delivery of routine care are reviewed: (1) effects of electronic medical records on the patient interview; (2) effects of complex systems integration on e-prescribing; and (3) use of clinical decision support to assist with clinical decision making. An increased understanding of the experience of automation from other sectors might allow for more effective deployment of technology in psychiatry.}
}
@article{JIANG2019215,
title = {Construction of data resource sharing center of the Puguang Intelligent Gas Field},
journal = {Natural Gas Industry B},
volume = {6},
number = {3},
pages = {215-219},
year = {2019},
issn = {2352-8540},
doi = {https://doi.org/10.1016/j.ngib.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S2352854019300452},
author = {Yiwei Jiang and Jinxian Li and Hanwei Zhang and Qingyin Wang and Yanqiu Yu and Chunguang He and Meisheng Liang},
keywords = {Intelligent gas field, Puguang gas field, Standard and specification, Post data, Real time data, Video data, Distributed storage, Data sharing service},
abstract = {During the initial construction of the Puguang Gas Field, information infrastructure was built. Due to the absence of unified planning and deployment, however, many “isolated information islands” were formed in data systems, and the data resources cannot meet the construction requirements of intelligent gas field. In this paper, the status quo and problems of data resources in the Puguang Gas Field were analyzed, and a data resource sharing center was constructed according to the overall architecture design of the Puguang Intelligent Gas Field. Based on the architecture design of data resource sharing center, the overall construction conception of data resource sharing center was put forward and the business data model was designed. Finally, the integrated data collection, storage, calculation and utilization was realized by establishing data standard, combing data sources and designing data services, and then it was applied on site. The following research results were obtained. First, the data resource sharing center is an important foundation for the construction of this project, and its overall architecture is divided into three layers from bottom to top, i.e., data specification and standard, data collection, storage, calculation and utilization, and data control. Second, the data resource sharing center achieves the one-time collection, centralized storage, shared use and unified management of exploration & development, gathering & purification, production & operation and safety & environmental protection data, and provides an important data base for the construction of business system of the intelligent gas field and a comprehensive, reliable and effective data support for the intelligent and mobile application in the Puguang Gas Field.}
}
@article{KURTZ20198,
title = {Linking Scattered Stem Cell-Based Data to Advance Therapeutic Development},
journal = {Trends in Molecular Medicine},
volume = {25},
number = {1},
pages = {8-19},
year = {2019},
issn = {1471-4914},
doi = {https://doi.org/10.1016/j.molmed.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1471491418302041},
author = {Andreas Kurtz and Magdi Elsallab and Ralf Sanzenbacher and Mohamed Abou-El-Enein},
keywords = {big data, stem cells, regulation, clinical trials, cell lines, registries, databases, translation},
abstract = {The biomedical field has witnessed remarkable advances in analytical tools and technologies that have expanded our understanding of healthy and diseased human tissue and, at the same time, enable extensive molecular characterization of living cells. The volume of scientific data generated is expanding in an unprecedented manner; however, these data remain scattered across research groups worldwide. Access to various data sources in a systematic fashion could hugely benefit the progress of nascent fields such as stem cell-based therapeutics. We explore here the currently available databases for stem cell research, and we propose creating a common portal to access these different databases that could act as a translational link between basic and clinical research to advance stem cell-based therapeutic development.}
}
@article{CANDON20192488,
title = {Implementing Intelligent Asset Management Systems (IAMS) within an Industry 4.0 Manufacturing Environment},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {2488-2493},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.580},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319315678},
author = {E. Candón and P. Martínez-Galán and A. {De la Fuente} and V. González-Prida and A. {Crespo Márquez} and J. Gómez and A. Sola and M. Macchi},
keywords = {Asset management, Operation, Maintenance, Decision making, Value management, Risk management},
abstract = {This paper aims to define the different considerations and results obtained in the implementation in an Intelligent Maintenance System of a laboratory designed based on basic concepts of Industry 4.0. The Intelligent Maintenance System uses asset monitoring techniques that allow, on-line digital modelling and automatic decision making. The three fundamental premises used for the development of the management system are the structuring of information, value identification and risk management.}
}
@article{GITTLER2019586,
title = {A fundamental approach for data acquisition on machine tools as enabler for analytical Industrie 4.0 applications},
journal = {Procedia CIRP},
volume = {79},
pages = {586-591},
year = {2019},
note = {12th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 18-20 July 2018, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.02.088},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119302057},
author = {Thomas Gittler and Adam Gontarz and Lukas Weiss and Konrad Wegener},
keywords = {Machine tools, ICT, Data analysis, Data acquisition, Industrie 4.0, Efficiency improvements, Statistical analysis, Machine monitoring},
abstract = {Decreasing ICT-costs propel connectivity and storage solutions for data generated, harvested and analyzed in machine tools. To acquire the necessary reliable, comprehensive and structured data for analytical applications, data from multiple sources must be acquired and combined. Many approaches for data acquisition either fail to cover all relevant data or cannot be put into action due to limited access on numerical controls. The following paper demonstrates the use of a multi-channel measurement application of a machine tool including its auxiliaries. The given approach was applied and verified on prototype machines. As a result, the application in current and future use-cases is discussed.}
}
@incollection{MORENOIBARRA201931,
title = {Chapter 3 - Civic participation in smart cities: the role of social media},
editor = {Anna Visvizi and Miltiadis D. Lytras},
booktitle = {Smart Cities: Issues and Challenges},
publisher = {Elsevier},
pages = {31-46},
year = {2019},
isbn = {978-0-12-816639-0},
doi = {https://doi.org/10.1016/B978-0-12-816639-0.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012816639000003X},
author = {Marco Moreno-Ibarra and Miguel Torres-Ruiz},
keywords = {Civic participation, GIS, Smart cities, Social media, Urban analysis, VGI},
abstract = {Nowadays, the perspective of smart cities is addressed in the social media. Big cities are facing problems in which the information lack is an important requirement to find solutions. Thus, social media has become a very prominent data source to obtain data and contextual information. Recently, cities are exposed to a variety of challenges such as health care, environment, energy consumption, traffic congestion, housing, education, public safety, budgets, economic development, demographics, diversity, and inclusiveness. So, technological advances bear the promise of addressing many of these challenges. At this point, the feedback from the citizens and city inhabitants is needed to collaborate voluntarily with observations about their surrounding environment. Thus, the technology plays an important role to offer solutions essentially fostering the civic participation. This chapter is focused on examining this particular issue, taking into consideration data from social media that can assist governments and organizations to better understand various phenomena massively and nearly to real time. A framework for urban analysis based on civic participation is proposed, in which social media posts are used to explicitly describe geographic features and events as well as spatial relationships that include time stamps and geolocation data. On the other hand, to highlight the advantage of the framework, a geographic information system–approach is considered, emphasizing aspects related to the data quality, where the diversity of criteria applied by users in the data acquisition is presented. As a case study, Twitter was chosen as the social media for its common use in scientific and commercial data analysis.}
}
@article{ATHINARAYANAN201991,
title = {Learning in Context with Horizontally & Vertically Integrated Curriculum in a Smart Learning Factory},
journal = {Procedia Manufacturing},
volume = {31},
pages = {91-96},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919303786},
author = {Ragu Athinarayanan and Brittany Newell and Jose Garcia and Jason Ostanek and Xiumin Diao and Raji Sundararajan and Henry Zhang and Grant Richards},
keywords = {Horizontally, Vertically Integrated Curriculum, Smart Factory, Cyber-Physical Production},
abstract = {In this work we present the development of a Smart Learning Factory (SLF) at Purdue University for preparing students with the skills, capabilities, and technological experiences necessary to excel in an Industry 4.0 environment. Functionally, the SLF is a replica of an actual cyber-physical production factory designed to intentionally foster collaboration between courses from multiple disciplinary areas, particularly mechanical, electrical, mechatronics, and robotics. Our objective is to reduce course silos by deliberately fusing the interconnection between courses by using SLF as the common unifying platform. Activities in design, manufacturing processes, production, production management, automation, energy, information, and communications, teamwork and collaboration are integrated using a vertical and horizontal integration framework. Unifying project activities were designed and introduced into these courses using this framework. As a result, 22 courses were integrated and a total of 26 vertical and horizontal integrated projects developed. This integration also facilitated the development of an energy credential using the SLF. Students progressing from freshman through senior year in college will better understand the interconnection of content between different courses, apply their learning to a manufacturing environment, gain a holistic perspective of the interdependent structures of the cyber-physical system, the connected enterprise, and the manufacturing ecosystem.}
}
@article{MARTINS201989,
title = {Reverse engineering database queries from examples: State-of-the-art, challenges, and research opportunities},
journal = {Information Systems},
volume = {83},
pages = {89-100},
year = {2019},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437918300978},
author = {Denis Mayr Lima Martins},
keywords = {Reverse engineering database queries, Databases, Query discovery, Query synthesis, Query learning},
abstract = {With the popularization of data access and usage, an increasing number of users without expert knowledge of databases is required to perform data interactions. Often, these users face the challenges of writing and reformulating database queries, which consume a considerable amount of time and frequently yield unsatisfactory results. To facilitate this human–database interaction, researchers have investigated the Query By Example (QBE) paradigm in which database queries are (semi) automatically discovered from data examples given by users. This paradigm allows non-database experts to formulate queries without relying on complex query languages. In this context, this work aims to present a systematic review of the recent developments, open challenges, and research opportunities of the QBE reported in the literature. This work also describes strategies employed to leverage efficient example acquisition and query reverse engineering. The obtained results show that recent research developments have focused on enhancing the expressiveness of produced queries, minimizing user interaction, and enabling efficient query learning in the context of data retrieval, exploration, integration, and analytics. Our findings indicate that future research should concentrate efforts to provide innovative solutions to the challenges of improving controllability and transparency, considering diverse user preferences in the processes of learning personalized queries, ensuring data quality, and improving the support of additional SQL features and operators.}
}
@article{GUILLAUMET2019230,
title = {Analyzing a CRIS: From data to insight in university research},
journal = {Procedia Computer Science},
volume = {146},
pages = {230-240},
year = {2019},
note = {14th International Conference on Current Research Information Systems, CRIS2018, FAIRness of Research Information},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919301024},
author = {Anna Guillaumet and Francesc García and Oscar Cuadrón},
keywords = {Data Analytics, Current Research Informations Systems},
abstract = {The exponential growth of data in digital environments has highlighted the emergence of developing analytics processes for data visualization and evaluation. The same happens at the university research level. Therefore, universities need to have specific analytical processes for research evaluation. The aim of our study is to find the way to apply general data analytical methods and technologies, specifically for university research environments, to help improve the results of their research. We have found that universities that maintain a Current Research Information System, CRIS (CERIF Compliant), containing high quality data, are able to implement these analytical processes and improve their results in the assessment and evaluation of their research data. This paper explains the process to implement such methodologies and techniques. Some results are also explained.}
}
@article{LIN2019103988,
title = {Applying density-based outlier identifications using multiple datasets for validation of stroke clinical outcomes},
journal = {International Journal of Medical Informatics},
volume = {132},
pages = {103988},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.103988},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619306732},
author = {Ching-Heng Lin and Kai-Cheng Hsu and Kory R. Johnson and Marie Luby and Yang C. Fann},
keywords = {Stroke outcome, modified Rankin Scale, Barthel Index, Outlier detection},
abstract = {Introduction
Clinicians commonly use the modified Rankin Scale (mRS) and the Barthel Index (BI) to measure clinical outcome after stroke. These are potential targets in machine learning models for stroke outcome prediction. Therefore, the quality of the measurements is crucial for training and validation of these models. The objective of this study was to apply and evaluate density-based outlier detection methods for identifying potentially incorrect measurements in multiple large stroke datasets to assess the measurement quality.
Method
We applied three density-based outlier detection methods including density-based spatial clustering of applications (DBSCAN), hierarchical DBSCAN (HDBSCAN) and local outlier factor (LOF) based on a large dataset obtained from a nationwide prospective stroke registry in Taiwan. The testing of each method was done by using four different NINDS funded stroke datasets.
Result
The DBSCAN achieved a high performance across all mRS values where the highest average accuracy was 99.2 ± 0.7 at mRS of 4 and the lowest average accuracy was 92.0 ± 4.6 at mRS of 3. The LOF also achieved similar performance, however, the HDBSCAN with default parameters setting required further tuning improvement.
Conclusion
The density-based outlier detection methods were proven to be promising for validation of stroke outcome measures. The outlier detection algorithm developed from a large prospective registry dataset was effectively applied in four different NINDS stroke datasets with high performance results. The tool developed from this detection algorithm can be further applied to real world datasets to increase the data quality in stroke outcome measures.}
}
@article{HALGAMUGE2019108634,
title = {Lessons learned from the application of machine learning to studies on plant response to radio-frequency},
journal = {Environmental Research},
volume = {178},
pages = {108634},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.108634},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119304311},
author = {Malka N. Halgamuge and Devra Davis},
keywords = {Radio-frequency, Machine learning, Plant, RF-EMF},
abstract = {This paper applies Machine Learning (ML) algorithms to peer-reviewed publications in order to discern whether there are consistent biological impacts of exposure to non-thermal low power radio-frequency electromagnetic fields (RF-EMF). Expanding on previous analysis that identified sensitive plant species, we extracted data from 45 articles published between 1996 and 2016 that included 169 experimental case studies of plant response to RF-EMF. Raw-data from these case studies included six different attributes: frequency, specific absorption rate (SAR), power flux density, electric field strength, exposure time and plant type (species). This dataset has been tested with two different classification algorithms: k-Nearest Neighbor (kNN) and Random Forest (RF). The outputs are estimated using k-fold cross-validation method to identify and compare classifier mean accuracy and computation time. We also developed an optimization technique to distinguish the trade-off between prediction accuracy and computation time based on the classification algorithm. Our analysis illustrates kNN (91.17%) and RF (89.41%) perform similarly in terms of mean accuracy, nonetheless, kNN takes less computation time (3.38 s) to train a model compared to RF (248.12 s). Very strong correlations were observed between SAR and frequency, and SAR with power flux density and electric field strength. Despite the low sample size (169 reported experimental case studies), that limits statistical power, nevertheless, this analysis indicates that ML algorithms applied to bioelectromagnetics literature predict impacts of key plant health parameters from specific RF-EMF exposures. This paper addresses both questions of the methodological importance and relative value of different methods of ML and the specific finding of impacts of RF-EMF on specific measures of plant growth and health. Recognizing the importance of standardizing nomenclature for EMF-RF, we conclude that Machine Learning provides innovative and efficient RF-EMF exposure prediction tools, and we propose future applications in occupational and environmental epidemiology and public health.}
}
@article{MCARTHUR2019233,
title = {Visualising where commuting cyclists travel using crowdsourced data},
journal = {Journal of Transport Geography},
volume = {74},
pages = {233-241},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2018.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317307445},
author = {David Philip McArthur and Jinhyun Hong},
keywords = {Cycling, Crowdsourced data, Volunteered geographic data (VGI), Strava},
abstract = {Encouraging more cycling is increasingly seen as an important way to create more sustainable cities and to improve public health. Understanding how cyclists travel and how to encourage cycling requires data; something which has traditionally been lacking. New sources of data are emerging which promise to reveal new insights. In this paper, we use data from the activity tracking app Strava to examine where people in Glasgow cycle and how new forms of data could be utilised to better understand cycling patterns. We propose a method for augmenting the data by comparing the observed link flows to the link flows which would have resulted if people took the shortest route. Comparing these flows gives some expected results, for example, that people like to cycle along the river, as well as some unexpected results, for example, that some routes with cycling infrastructure are avoided by cyclists. This study proposes a practical approach that planners can use for cycling plans with new/emerging cycling data.}
}
@article{STRINGER201922,
title = {Computational processing of neural recordings from calcium imaging data},
journal = {Current Opinion in Neurobiology},
volume = {55},
pages = {22-31},
year = {2019},
note = {Machine Learning, Big Data, and Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0959438818300977},
author = {Carsen Stringer and Marius Pachitariu},
abstract = {Electrophysiology has long been the workhorse of neuroscience, allowing scientists to record with millisecond precision the action potentials generated by neurons in vivo. Recently, calcium imaging of fluorescent indicators has emerged as a powerful alternative. This technique has its own strengths and weaknesses and unique data processing problems and interpretation confounds. Here we review the computational methods that convert raw calcium movies to estimates of single neuron spike times with minimal human supervision. By computationally addressing the weaknesses of calcium imaging, these methods hold the promise of significantly improving data quality. We also introduce a new metric to evaluate the output of these processing pipelines, which is based on the cluster isolation distance routinely used in electrophysiology.}
}
@article{WANYONYI201988,
title = {Electronic primary dental care records in research: A case study of validation and quality assurance strategies},
journal = {International Journal of Medical Informatics},
volume = {127},
pages = {88-94},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618308657},
author = {Kristina L. Wanyonyi and David R. Radford and Jennifer E. Gallagher},
keywords = {Electronic dental records, Primary dental care, Dental informatics, Dental research, Clinical research, Electronic health records, Health service research},
abstract = {Background
In dentistry, the use of electronic patient records for research is underexplored. The aim of this paper is to describe a case study process of obtaining research data (sociodemographic, clinical and workforce) from electronic primary care dental records, and outlining data cleaning and validation strategies. This study was undertaken at the University of Portsmouth Dental Academy (UPDA), which is a centre of education, training and provision of state funded services (National Health Services). UPDA’s electronic patient management system is R4/Clinical +. This is a widely used system in general dental practices in the UK.
Method
A two-phase process, involving first Pilot and second Main data extraction were undertaken. Using System Query Language (SQL), data extracts containing variables related to patients’ demography, socio-economic status and dental care received were generated. A data cleaning and validation exercise followed, using a combination of techniques including Maletic and Marcus’s (2000) general framework for data cleaning and Rahm and Haido’s (2010) principles of data cleaning.
Results
The findings of the case study support the use of a two-phase data extraction process. The data validation processes highlighted the need for both manual and analytical strategies when cleaning these data. Finally, the process demonstrated that electronic dental records can be validated and used for epidemiological and heath service research. The potential to generalise findings is great due to the large number of records. There are, however, limitations to the data which need to be considered, relating to quality (data input), database structure and interpretation of data codes.
Conclusion
Electronic dental records are useful in health service research, epidemiological studies and skill mix research. Researchers should work closely with clinicians, managers and software developers to ensure that the data generated are accurate, valid and generalisable. Following data extraction the researchers need to adapt stringent validation and data cleaning strategies to guarantee that the extracted electronic data are accurate.}
}
@article{PEREIRA2019100809,
title = {Using clinical data to effect practice change},
journal = {International Emergency Nursing},
volume = {46},
pages = {100809},
year = {2019},
issn = {1755-599X},
doi = {https://doi.org/10.1016/j.ienj.2019.100809},
url = {https://www.sciencedirect.com/science/article/pii/S1755599X19300953},
author = {Katherine Pereira and Victoria Goode and Petra Brysiewicz}
}
@article{GOEL20191831,
title = {Noninfectious transfusion-associated adverse events and their mitigation strategies},
journal = {Blood},
volume = {133},
number = {17},
pages = {1831-1839},
year = {2019},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood-2018-10-833988},
url = {https://www.sciencedirect.com/science/article/pii/S0006497120426035},
author = {Ruchika Goel and Aaron A.R. Tobian and Beth H. Shaz},
abstract = {Abstract
Blood transfusions are life-saving therapies; however, they can result in adverse events that can be infectious or, more commonly, noninfectious. The most common noninfectious reactions include febrile nonhemolytic transfusion reactions, allergic transfusion reactions, transfusion-associated circulatory overload, transfusion-related acute lung injury, and acute and delayed hemolytic transfusion reactions. These reactions can be asymptomatic, mild, or potentially fatal. There are several new methodologies to diagnose, treat, and prevent these reactions. Hemovigilance systems for monitoring transfusion events have been developed and demonstrated decreases in some adverse events, such as hemolytic transfusion reactions. Now vein-to-vein databases are being created to study the interactions of the donor, product, and patient factors in the role of adverse outcomes. This article reviews the definition, pathophysiology, management, and mitigation strategies, including the role of the donor, product, and patient, of the most common noninfectious transfusion-associated adverse events. Prevention strategies, such as leukoreduction, plasma reduction, additive solutions, and patient blood management programs, are actively being used to enhance transfusion safety. Understanding the incidence, pathophysiology, and current management strategies will help to create innovative products and continually hone in on best transfusion practices that suit individualized patient needs.}
}
@article{ZHANG2019102497,
title = {Biking islands in cities: An analysis combining bike trajectory and percolation theory},
journal = {Journal of Transport Geography},
volume = {80},
pages = {102497},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2019.102497},
url = {https://www.sciencedirect.com/science/article/pii/S0966692319301802},
author = {Yongping Zhang and Diao Lin and Xiaoyue Cathy Liu},
keywords = {Biking islands, Dockless, Bike-sharing, Trajectory, Usage pattern, Big data},
abstract = {Bike trajectories generated by the dockless bike-sharing service provides a great opportunity to explore users' travel behavior within the shared mobility transportation ecosystem. This paper proposes a new concept, namely biking islands, defined as geographical areas of interest with a high concentration of bike usage. Leveraging high-resolution trajectory data, biking islands are identified via percolation theory because of its suitability in describing the formation of clusters and critical road segments that have significant influence on biking behavior in an urban context. We showcase our methodology using the bike trajectory dataset provided by a market-leading company and use Shanghai, China as the study area. Results reveal a hierarchical structure of biking islands. With the increase of threshold, the biking islands start to shrink and split into various smaller ones. Larger biking islands are usually located in the central urban area of Shanghai and the Huangpu River acts as a natural barrier that impedes biking continuity across the region. Besides, the formation of biking islands is highly influenced by the surrounding land uses. The proposed concept and methods are not only helpful to understand travel behavior of cyclists and urban structures used for cycling, but also has the potential to support relevant urban and transportation planning, such as identifying designated non-motorized areas for cycling and biking facilities and pinpointing critical road segments that can improve the cycling efficiency of the entire network. Biking islands could be designed as designated areas for cycling where sufficient bike facilities are provided, and/or motorized transport modes are restricted or even prohibited, so as to ensure the convenience and safety of cyclists and support the development of bike-friendly cities.}
}
@incollection{HOMAYOUNI2019223,
title = {Chapter Five - Data Warehouse Testing},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {112},
pages = {223-273},
year = {2019},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300578},
author = {Hajar Homayouni and Sudipto Ghosh and Indrakshi Ray},
keywords = {Data warehouse, Extract-Transform-Load, Functional testing, Performance testing, Security testing, Systematic testing},
abstract = {Enterprises use data warehouses to accumulate data from multiple sources for data analysis and research. Since organizational decisions are often made based on the data stored in a data warehouse, all its components must be rigorously tested. Researchers have proposed a number of approaches and tools to test and evaluate different components of data warehouse systems. In this chapter, we present a comprehensive survey of data warehouse testing techniques. We define a classification framework that can categorize the existing testing approaches. We also discuss open problems and propose research directions.}
}
@article{NEMETH2019645,
title = {A Maturity Assessment Procedure Model for Realizing Knowledge-Based Maintenance Strategies in Smart Manufacturing Enterprises},
journal = {Procedia Manufacturing},
volume = {39},
pages = {645-654},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.439},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305114},
author = {Tanja Nemeth and Fazel Ansari and Wilfried Sihn},
keywords = {Knowledge-based maintenance, maturity model, quality indicators},
abstract = {The digital transformation of manufacturing industries currently re-invents conventional production paradigms through the creation of cyber-physical production systems (CPPS) and smart manufacturing networks. Novel knowledge-based maintenance (KBM) strategies and models are regarded as a key enabler in order to manage the increasing complexity and automatization of CPPS. Thus, securing and improving machine availability and process stability are accomplished. Although industrial decision makers are willing to invest in renovating and enhancing their companies’ maintenance strategy, they lack knowledge regarding their readiness levels in realizing and deploying KBM. In particular, they doubt whether their companies hold fundamental competence and capacity (i.e. appropriate methods for data analytics and systematic guidance) towards realizing KBM. In this paper, the authors present a holistic procedure model that assesses a company´s individual status quo in KBM and enables the identification of strengths and weaknesses on operative, tactical and strategic level following a multidimensional analytical approach. The model thereby builds on the assessment of more than 35 quality indicators assigned to a maintenance execution and data management dimension. The indicators feed into a mathematical calculation of data, information, knowledge and maintenance quality factors. The authors present the model’s development and content. Applying the model to an Austrian manufacturing company, its KBM readiness and maturity level for the creation of a predictive maintenance strategy under the premise of supporting prompt and efficient decision making is assessed. Feedback and validation of the results reveals both scientifically valuable, systematic and transparent applicability in real production environments.}
}
@article{KAN2019229,
title = {Traffic congestion analysis at the turn level using Taxis' GPS trajectory data},
journal = {Computers, Environment and Urban Systems},
volume = {74},
pages = {229-243},
year = {2019},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518301741},
author = {Zihan Kan and Luliang Tang and Mei-Po Kwan and Chang Ren and Dong Liu and Qingquan Li},
keywords = {Traffic congestion, GPS trajectories, Intersection, Turn-level congestion, Big data},
abstract = {Sensing turn-level or lane-level traffic conditions not only enables navigation systems to provide users with more detailed and finer-grained information, it can also improve the accuracy in the search for the fastest routes and in short-term predictions of traffic conditions. The widespread collection and application of taxis' GPS data enable us to sense urban traffic flow on a large scale. Since current GPS positional accuracy cannot reach the lane level, existing approaches using GPS trajectory data only analyze traffic conditions at the road level. Whereas some studies attempted to detect lane-level traffic conditions using lane-level data, the high cost of data collection considerably limits their practical application. To address this limitation, this article proposes an approach for detecting traffic congestion from taxis' GPS trajectories at the turn level. Based on analyzing features of GPS trajectories and identifying valid trajectory segments, the proposed approach detects congested trajectory segments of three different intensities. It then identifies congestion events in each turning direction through a clustering approach. Finally, congestion intensity, time of the day when congestion occurred and queue length in each turning direction at a road intersection in Wuhan, China are explored and analyzed. The results support the feasibility of this approach for detecting and analyzing traffic congestion at the turn level. Compared with other approaches that detect traffic congestion using GPS trajectory data, the proposed approach analyzes congestion at a finer-grained level (the turn level). Compared with other approaches that detect traffic congestion at the lane level, the proposed approach can sense traffic congestion over a larger area and at a lower cost.}
}
@article{XU2019345,
title = {On predictability of time series},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {523},
pages = {345-351},
year = {2019},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378437119301591},
author = {Paiheng Xu and Likang Yin and Zhongtao Yue and Tao Zhou},
keywords = {Predictability, Human mobility, Time series},
abstract = {The method to estimate the predictability of human mobility was proposed in Song et al. (2010), which is extensively followed in exploring the predictability of disparate time series. However, the ambiguous description in the original paper leads to some misunderstandings, including the inconsistent logarithm bases in the entropy estimator and the entropy-predictability-conversion equation, as well as the details in the calculation of the Lempel–Ziv estimator, which further results in remarkably overestimated predictability. This paper demonstrates the degree of overestimation by four different types of theoretically generated time series and an empirical data set, and shows the intrinsic deviation of the Lempel–Ziv estimator for highly random time series. This work provides a clear picture on this issue and thus helps researchers in correctly estimating the predictability of time series.}
}
@article{GUTIERREZ2019104844,
title = {A review of visualisations in agricultural decision support systems: An HCI perspective},
journal = {Computers and Electronics in Agriculture},
volume = {163},
pages = {104844},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918319069},
author = {Francisco Gutiérrez and Nyi Nyi Htun and Florian Schlenz and Aikaterini Kasimati and Katrien Verbert},
keywords = {Decision support systems, Human-computer interaction, Information visualisation, Precision agriculture},
abstract = {Decision Support Systems (DSSs) are used in precision agriculture to provide feedback to a variety of stakeholders, including farmers, advisers, researchers and policymakers. However, increments in the amount of data might lead to data quality issues, and as these applications scale into big, real-time monitoring systems the problem gets even more challenging. Visualisation is a powerful technique used in these systems that provides an indispensable step in assisting end-users to understand and interpret the data. In this paper, we present a systematic review to synthesise literature related to the use of visualisation techniques in the domain of agriculture. The search identified 61 eligible articles, from which we established end-users, visualisation techniques and data collection methods across different application domains. We found visualisation techniques used in various areas of agriculture, including viticulture, dairy farming, wheat production and irrigation management. Our results show that the majority of DSSs utilise maps, together with satellite imagery, as the central visualisation. Also, we observed that there is an excellent opportunity for dashboards to enable end-users with better interaction support to understand the uncertainty of data. Based on this analysis, we provide design guidelines towards the implementation of more interactive and visual DSSs.}
}
@article{WASESA2019797,
title = {Open Data Visual Analytics to Support Decisions on Physical Investments},
journal = {Procedia Computer Science},
volume = {161},
pages = {797-804},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.185},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919318964},
author = {Meditya Wasesa and M. Mashuri and Putri Handayani and Utomo S. Putro},
keywords = {Open Data, Visual Analytics, Descriptive Analytics, Spatiotemporal Data, Physical Investment},
abstract = {Before making decisions about the location of physical investments (e.g. factories, warehouses, stores, etc.), investors have to conduct thorough profiling on the prospective locations of the physical facilities. For this profiling purpose, investors consider a number of important aspects such as the location’s local resources, market size, economic growth, regional minimum wage (salary) standard, land acquisition cost, human development index, and gross domestic product (GDP). While conducting manual research on those decision variables can cost extensive time and efforts, in this article, we present an open data visual analytics which will help investors in making decisions on the prospective location of physical investments. Investors can use the web-based application to easily gather information and do a quick comparison among prospective investment locations in terms of the selected decision variables.}
}
@incollection{2019409,
title = {Index},
editor = {W.H. Inmon and Daniel Linstedt and Mary Levins},
booktitle = {Data Architecture (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {409-416},
year = {2019},
isbn = {978-0-12-816916-2},
doi = {https://doi.org/10.1016/B978-0-12-816916-2.09992-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128169162099927}
}
@article{HYNDS2019851,
title = {Evaluation of hydrometric network efficacy and user requirements in the Republic of Ireland via expert opinion and statistical analysis},
journal = {Journal of Hydrology},
volume = {574},
pages = {851-861},
year = {2019},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2019.04.086},
url = {https://www.sciencedirect.com/science/article/pii/S0022169419304263},
author = {P.D. Hynds and A.E. Nasr and J. O'Dwyer},
keywords = {Hydrometric measurement, Network design, Optimisation, Cross-sectional survey},
abstract = {Decreased funding and shifting governmental priorities have resulted in a contraction of hydrometric measurement in many regions over the past two decades. Moreover, concerns exist with respect to appropriate data usage and (transboundary) exchange, in addition to the compatibility and extent of existing hydrometric datasets. These issues are undoubtedly magnified due to enhanced data demands and increased financial pressures on network managers, thus requiring new approaches to optimising the societal benefits and overall efficacy of hydrometric information for future socio-hydrological resilience. The current study employed a quantitative cross-sectional expert elicitation of 203 respondents to collate, analyse and assess hydrometric network users’ opinions, knowledge and experience. Current usage patterns, perceived network strengths, requirements, and limitations have been identified and discussed within the context of hydrometric resilience in a changing social, economic and natural environment. Findings indicate that small (<30 km2) catchment data are most frequently employed in the Republic of Ireland, particularly with respect to extreme event prediction and flood management. Similarly, small catchments and areas characterised by previous/recent flooding were prioritised for resilience management via network amendment. Over half of those surveyed (50.5%) reported the current network as inadequate for their professional requirements. Conversely, respondents indicated network efficacy has improved (53.2%) or remained stable (26.6%) over the course of their professional career, however, improvements (as defined by individual respondents i.e. network density, data quality, data availability) have not occurred at a sufficient rate. User-defined efficacy (adequacy, resilience) was found to be a somewhat vague, multivariate concept, with no individual predictor identified, however, general data quality, network density, and urban catchment data were the most significant issues among respondents. A significant majority (85.4%) of respondents indicate that future resilience would be best achieved via network density amendment, with over 60% favouring geographically and/or categorically focused network increases, as opposed to more general national increases.}
}
@article{ENGELGAU201975,
title = {Predictive Analytics: Helping Guide the Implementation Research Agenda at the National Heart, Lung, and Blood Institute},
journal = {Global Heart},
volume = {14},
number = {1},
pages = {75-79},
year = {2019},
issn = {2211-8160},
doi = {https://doi.org/10.1016/j.gheart.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211816019300134},
author = {Michael M. Engelgau and Muin J. Khoury and Rebecca A. Roper and Jennifer S. Curry and George A. Mensah}
}
@article{HAUSMANN2019617,
title = {Assessing global popularity and threats to Important Bird and Biodiversity Areas using social media data},
journal = {Science of The Total Environment},
volume = {683},
pages = {617-623},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.05.268},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719323095},
author = {Anna Hausmann and Tuuli Toivonen and Christoph Fink and Vuokko Heikinheimo and Henrikki Tenkanen and Stuart H.M. Butchart and Thomas M. Brooks and Enrico {Di Minin}},
keywords = {Social media, Biodiversity, Big data, Ecotourism, IBAs, Threat},
abstract = {Understanding worldwide patterns of human use of sites of international significance for biodiversity conservation is crucial for meeting global conservation targets. However, robust global datasets are scarce. In this study, we used social media data, mined from Flickr and Twitter, geolocated in Important Bird and Biodiversity Areas (IBAs) to assess i) patterns of popularity; ii) relationships of this popularity with geographical and biological variables; and iii) identify sites under high pressure from visitors. IBAs located in Europe and Asia, and in temperate biomes, had the highest density of users. Sites of importance for congregatory species, which were also more accessible, more densely populated and provided more tourism facilities, received higher visitation than did sites richer in bird species. We found 17% of all IBAs assessed to be under very high threat also received high visitation. Our results show in which IBAs enhanced monitoring should be implemented to reduce potential visitation risks to sites of conservation concern for birds, and to harness the potential benefits of tourism for conservation.}
}
@article{WANG2019135,
title = {Network data management model based on Naïve Bayes classifier and deep neural networks in heterogeneous wireless networks},
journal = {Computers & Electrical Engineering},
volume = {75},
pages = {135-145},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S004579061831766X},
author = {Kangyi Wang},
keywords = {Network management system (NM), Intrusion detection, Deep neural network (DNN), Data mining},
abstract = {A key purpose of a data management system is to monitor and control the large volume of data and other relevant information. Focusing on the data management issues and developing a detection system is necessary for resolving attacks or intrusion in a network. This network instruction detection system helps to identify unseen and unpredictable attacks in management station via loophole to break network security. Conventional instruction detection system has complexities in exploiting, enhancing the security features and this research work focuses on solving above issue. The proposed research work is designed for efficient and flexible network intrusion detection system using Naïve Bayes classifier and deep neural networks. The experimental results show that proposed Deep Neural Network-based Intrusion Detection System is suitable for classification with high accuracy and precision in both binary and multiclass, recall and f- measure values. Compared with other state-of-the-art approaches, the analytic accuracy has been improved.}
}
@article{WANG2019117965,
title = {Earthquake emergency response framework on campus based on multi-source data monitoring},
journal = {Journal of Cleaner Production},
volume = {238},
pages = {117965},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.117965},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619328355},
author = {Tao Wang and Shaya Guomai and Limao Zhang and Guijun Li and Yulong Li and Junhua Chen},
keywords = {University campus, Earthquake, Emergency response, Data monitoring, Decision making},
abstract = {Earthquakes can have a tremendous impact on the operations of university campuses. Since campus buildings are used for teaching, living, and entertaining, it is necessary to establish timely and effective emergency response strategies that accord with the functions of each building. Since there is currently no emergency response system that can provide real-time feedback and enable decision making during an earthquake, this paper proposes a university emergency response system based on multi-source data monitoring. The proposed system offers priorities for rescuing people and maintaining facilities according to five indices: the number of people in the building, the building's command role, the potential danger of hazardous chemicals in the building, the extent of damage to the building, and the lifeline role of facilities in the building. Several methods are used in the process of decision making, including the analytic hierarchy process (AHP), linear regression, earthquake disaster simulation, and expert evaluation. The emergency response system is then applied to a case study of the Shahe Campus of the Central University of Finance and Economics in Beijing, where simulations of three typical scenarios under fixed seismic parameters are carried out and compared. The system proposed in this paper makes the campus's earthquake emergency response more rapid and reasonable, providing new ideas for promoting the development of emergency management systems and sustainable campus operations at colleges and universities.}
}
@article{XU2019513,
title = {Quantitative credibility evaluation of Global Energy Interconnection data},
journal = {Global Energy Interconnection},
volume = {2},
number = {6},
pages = {513-520},
year = {2019},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2020.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096511720300050},
author = {Xinzhi Xu and Xingyuan Zhao and Jun Li and Yi Gao and Ping Yan and Fang Chen},
keywords = {Global Energy Interconnection, Data credibility evaluation, Cloud Model},
abstract = {The development of Global Energy Interconnection (GEI) is essential for supporting a wide range of basic data resources. The Global Energy Interconnection Development and Cooperation Organization has established a comprehensive data center covering six major systems. However, methods for accurately describing and scientifically evaluating the credibility of the massive amount of GEI data remain underdeveloped. To address this lack of such methods, a GEI data credibility quantitative evaluation model is proposed here. An evaluation indicator system is established to evaluate data credibility from multiple perspectives and ensure the comprehensiveness and impartiality of evaluation results. The Cloud Model abandons the hard division of comments to ensure objectivity and accuracy in evaluation results. To evaluate the suitability of the proposed method, a case analysis is conducted, wherein the proposed method demonstrates sufficient validity and feasibility.}
}
@article{SRINIVASAN2019911,
title = {Modelling food sourcing decisions under climate change: A data-driven approach},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {911-919},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.10.048},
url = {https://www.sciencedirect.com/science/article/pii/S036083521830528X},
author = {Rengarajan Srinivasan and Vaggelis Giannikas and Mukesh Kumar and Renaud Guyot and Duncan McFarlane},
keywords = {Climate change, Food supply chain, Open data, Sourcing decision, Sourcing risk},
abstract = {Changes in climate conditions are expected to pose significant challenges to the food industry, as it is very likely that they will affect the production of various crops. As a consequence, decisions associated with the sourcing of food items will need to be reconsidered in the years to come. In this paper, we investigate how environmental changes are likely to affect the suitability and risk of different regions—in terms of growing certain food items—and whether companies should adapt their sourcing decisions due to these changes. In particular, we propose a three-stage approach that guides food sourcing decisions by incorporating climate change data. The methodology utilises environmental data from several publicly available databases and models weather uncertainties to calculate the suitability and risk indices associated with growing a crop in a particular geographical area. The estimated suitability and risk parameters are used in a mean-variance analysis to calculate the optimal sourcing decision. Results from a case example indicate that sourcing decisions of popular food items are likely to require significant adaptations due to changes to the suitability of certain regions.}
}
@article{LAI2019117,
title = {Study on microclimate observation network for urban unit: A case study in a campus of Shenzhen, China},
journal = {Physics and Chemistry of the Earth, Parts A/B/C},
volume = {110},
pages = {117-124},
year = {2019},
note = {Sensing and Sensor Systems for Urban Environmental Studies},
issn = {1474-7065},
doi = {https://doi.org/10.1016/j.pce.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1474706518301608},
author = {Xin Lai and Yang Tang and Lei Li and PakWai Chan and Qingfeng Zeng},
keywords = {Urban unit, Microclimate, Indoor climate, Observation},
abstract = {Urban units are city components that have relatively pure functions while carry a large number of urban residents. The quality of an urban unit's microclimate environment is closely related to the health and comfort of its population. In this study, we develop a technology on microclimate observation for urban units, which is experimentally applied in a campus in Shenzhen, China. We have arrived at the following conclusions: (1) The application of precision integration technology and small sensors greatly reduces the physical size of the microclimate observation instrument. With the help of advanced algorithms for data quality control, the cost of detection and measuring instruments can be greatly reduced even when accuracy is maintained so that the instruments can be densely deployed in urban units. (2) The urban unit microclimate observation network, as a supplement to the traditional meteorological observation system, makes up for the deficiencies of coarse resolution and limited capability of the traditional observation system on describing urban heterogeneity, and can be used to more precisely depict the heterogeneity of the microclimate within the urban units. Utilizing the technology of Internet of things, it provides data support for people, who work, live and study in the urban units, to take appropriate measures to regulate microclimate environment, and also provides external parameters for automatic adjustment and control of smart air conditioners and dehumidifiers. (3) Temperature monitoring data show that the detected daily temperature ranges of the indoor stations within urban unit are relatively small, and the daily average temperatures observed by the indoor stations are quite different from those observed by the automatic weather stations located in open spaces, indicating that conventional meteorological observation has little reference value to the indoor environment of urban units. At the same time, the observed data of the different indoor monitoring points are quite different from each other, which are closely related to their specific environments. (4) Analysis of relative humidity data shows that the rules of variation of indoor relative humidity are quite different from those of open space, and indoor crowd and monitoring point environment may affect the indoor relative humidity.}
}
@article{WANG2019167,
title = {Delineating urbanization “source-sink” regions in China: Evidence from mobile app data},
journal = {Cities},
volume = {86},
pages = {167-177},
year = {2019},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2018.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0264275118307984},
author = {Yuxia Wang and Fahui Wang and Yi Zhang and Yu Liu},
keywords = {Mobile app data, Labor source-sink areas, Annual spatial mismatch, Urbanization, Regionalization, China},
abstract = {Traditional census data are ill-suited for uncovering the true population patterns and underlying social and economic dynamics in China as the census relies on information of population with registered household status. A large number of migrant workers are registered rural residents but spend most of a year working in cities that are hundreds or even thousands of miles away. It is termed “annual spatial mismatch” here for the separation of registered residence and workplace in China, in contrast to “spatial mismatch” that is known in the urban commuting literature in the west but on a daily basis. Big geo-data, such as the mobile app data, afford us a rare opportunity to examine this unique phenomenon. Specifically, this research uses a mobile app dataset of two epochs, i.e., prior to and during the Chinese Spring Festival, to capture the population patterns before and after the migrant workers return home, respectively. The difference between them reflects distinctive roles of an area plays in labor market, termed “source-sink areas”. A GIS-automated regionalization method is used to delineate China into hierarchical “source-sink” areas, characterizing various urbanization levels or distinctive roles in labor market. The study demonstrates the value of using human mobility data in urban and regional analysis on issues that were previously infeasible, especially in study areas without reliable data.}
}
@article{CAI2019S389,
title = {RW4 DEVELOPMENT OF REAL-WORLD DATA AND THE USE OF RWD FOR EVIDENCE GENERATION IN CHINA},
journal = {Value in Health},
volume = {22},
pages = {S389-S390},
year = {2019},
note = {ISPOR 2019: Rapid. Disruptive. Innovative: A New Era in HEOR},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2019.04.1903},
url = {https://www.sciencedirect.com/science/article/pii/S1098301519320959},
author = {B. Cai and Y. Xie and Y. Gong and W. Luo and H. Qu and J. Liu}
}
@article{CECCHINEL2019225,
title = {Leveraging live machine learning and deep sleep to support a self-adaptive efficient configuration of battery powered sensors},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {225-240},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18305740},
author = {Cyril Cecchinel and François Fouquet and Sébastien Mosser and Philippe Collet},
abstract = {Sensor networks empower Internet of Things (IoT) applications by connecting them to physical world measurements. However, the necessary use of limited bandwidth networks and battery-powered devices makes their optimal configuration challenging. An over-usage of periodic sensors (i.e. too frequent measurements) may easily lead to network congestion or battery drain effects, and conversely, a lower usage is likely to cause poor measurement quality. In this paper we propose a middleware that continuously generates and exposes to the sensor network an energy-efficient sensors configuration based on data live observations. Using a live learning process, our contributions dynamically act on two configuration points: (i) sensors sampling frequency, which is optimized based on machine-learning predictability from previous measurements, (ii) network usage optimization according to the frequency of requests from deployed software applications. As a major outcome, we obtain a self-adaptive platform with an extended sensors battery life while ensuring a proper level of data quality and freshness. Through theoretical and experimental assessments, we demonstrate the capacity of our approach to constantly find a near-optimal tradeoff between sensors and network usage, and measurement quality. In our experimental validation, we have successfully scaled up the battery lifetime of a temperature sensor from a monthly to a yearly basis.}
}
@article{ELKASSABI2019462,
title = {Trust enforcement through self-adapting cloud workflow orchestration},
journal = {Future Generation Computer Systems},
volume = {97},
pages = {462-481},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18313529},
author = {Hadeel T. El-Kassabi and M. {Adel Serhani} and Rachida Dssouli and Alramzana N. Navaz},
keywords = {, Reconfiguration, Self-adapt system, State machine, Trust assessment, Workflow},
abstract = {Providing runtime intelligence of a workflow in a highly dynamic cloud execution environment is a challenging task due the continuously changing cloud resources. Guaranteeing a certain level of workflow Quality of Service (QoS) during the execution will require continuous monitoring to detect any performance violation due to resource shortage or even cloud service interruption. Most of orchestration schemes are either configuration, or deployment dependent and they do not cope with dynamically changing environment resources. In this paper, we propose a workflow orchestration, monitoring, and adaptation model that relies on trust evaluation to detect QoS performance degradation and perform an automatic reconfiguration to guarantee QoS of the workflow. The monitoring and adaptation schemes are able to detect and repair different types of real time errors and trigger different adaptation actions including workflow reconfiguration, migration, and resource scaling. We formalize the cloud resource orchestration using state machine that efficiently captures different dynamic properties of the cloud execution environment. In addition, we use validation model checker to validate our model in terms of reachability, liveness, and safety properties. Extensive experimentation is performed using a health monitoring workflow we have developed to handle dataset from Intelligent Monitoring in Intensive Care III (MIMICIII) and deployed over Docker swarm cluster. A set of scenarios were carefully chosen to evaluate workflow monitoring and the different adaptation schemes we have implemented. The results prove that our automated workflow orchestration model is self-adapting, self-configuring, react efficiently to changes and adapt accordingly while supporting high level of Workflow QoS.}
}
@article{TALUKDER2019147,
title = {Determinants of user acceptance and use of open government data (OGD): An empirical investigation in Bangladesh},
journal = {Technology in Society},
volume = {56},
pages = {147-156},
year = {2019},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X18301738},
author = {Md Shamim Talukder and Liang Shen and Md Farid {Hossain Talukder} and Yukun Bao},
keywords = {Open government data, Open data, Technology adoption, UTAUT, IS success model, Developing countries},
abstract = {Despite the benefits of transparency, accountability, and participation of open government data (OGD), low acceptance and use of OGD have been observed. However, the acceptance and use of OGD has not been adequately addressed in existing literature. Therefore, this study aimed to synthesize the strengths of two well-established theories; the unified theory of acceptance and use of technology (UTAUT) and IS success model as there is a need to develop a parsimonious model. The research model was empirically validated using 385 responses from a developing Country-Bangladesh. Data were analyzed using partial least square (PLS) technique, a statistical tools based on structured equation modeling (SEM). The results revealed that performance expectancy, effort expectancy, social influence, system quality, and information quality had significant direct and indirect effects on OGD adoption. Theoretically, this research provides a basis for advance enhancement of individual models of technology acceptance. Practically, this research provides several valuable policy guidelines to carry out the better policies and strategies to endorse the acceptance and use of OGD.}
}
@article{2019iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {160},
pages = {iii-x},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(19)31798-3},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317983}
}
@article{SAEED2019115,
title = {Enhanced Heartbeat Graph for emerging event detection on Twitter using time series networks},
journal = {Expert Systems with Applications},
volume = {136},
pages = {115-132},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419304051},
author = {Zafar Saeed and Rabeeh Ayaz Abbasi and Imran Razzak and Onaiza Maqbool and Abida Sadaf and Guandong Xu},
keywords = {Event detection, Twitter, Text stream, Emerging trends, Dynamic graph, Time series network, Big data},
abstract = {With increasing popularity of social media, Twitter has become one of the leading platforms to report events in real-time. Detecting events from Twitter stream requires complex techniques. Event-related trending topics consist of a group of words which successfully detect and identify events. Event detection techniques must be scalable and robust, so that they can deal with the huge volume and noise associated with social media. Existing event detection methods mostly rely on burstiness, mainly the frequency of words and their co-occurrences. However, burstiness sometimes dominates other relevant details in the data which could be equally significant. Besides, the topological and temporal relationships in the data are often ignored. In this work, we propose a novel graph-based approach, called the Enhanced Heartbeat Graph (EHG), which detects events efficiently. EHG suppresses dominating topics in the subsequent data stream, after their first detection. Experimental results on three real-world datasets (i.e., Football Association Challenge Cup Final, Super Tuesday, and the US Election 2012) show superior performance of the proposed approach in comparison to the state-of-the-art techniques.}
}
@article{BELLAVISTA201971,
title = {A survey on fog computing for the Internet of Things},
journal = {Pervasive and Mobile Computing},
volume = {52},
pages = {71-99},
year = {2019},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1574119218301111},
author = {Paolo Bellavista and Javier Berrocal and Antonio Corradi and Sajal K. Das and Luca Foschini and Alessandro Zanni},
keywords = {Fog computing, Internet of Things, IoT case studies, Survey, Taxonomy},
abstract = {Fog computing has emerged to support the requirements of IoT applications that could not be met by today’s solutions. Different initiatives have been presented to drive the development of fog, and much work has been done to improve certain aspects. However, an in-depth analysis of the different solutions, detailing how they can be integrated and applied to meet specific requirements, is still required. In this work, we present a unified architectural model and a new taxonomy, by comparing a large number of solutions. Finally, we draw some conclusions and guidelines for the development of IoT applications based on fog.}
}
@article{VANRENSBURG2019101125,
title = {Society 4.0 applied in Africa: Advancing the social impact of technology},
journal = {Technology in Society},
volume = {59},
pages = {101125},
year = {2019},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X18303361},
author = {Nickey Janse {Van Rensburg} and Arnesh Telukdarie and Pavitra Dhamija},
keywords = {Industry 4.0, Society 4.0, Social impact of technology, South Africa, Mobile technologies, Youth},
abstract = {Global technological advancement as aligned to the fourth industrial revolution supports various first world development needs. Within the African context, and specifically in developing countries like South Africa (SA), the benefit from technology innovation can significantly impact on socio-economic issues like unemployment and skills development. Technological development is identified as a crucial driver for new growth and a means to develop and outperform global competitors. Human capital, specifically young people, supports inclusive and sustainable economic growth. South Africa as per current classification, remains a twofold economy with among the greatest inequality rates in the world, continuing both disparity and segregation. Employment remains at more than one in three youth unemployed. A group of academics and commercial partners working across disciplines collaborate to create opportunities for local young people to participate in research and data gathering at the University of Johannesburg (UJ). The need for the enablement of technology especially among the youth of the African society has initiated a research question, which determines whether technological instruments can be developed and deployed in Africa to facilitate youth. A transformative but collaborative (researcher and community team), approach is the focus with youth employment as a key objective. Collaboration with local communities to gather household and sector specific data, UJ mobilizes cross-disciplinary research networks to drive social innovation. Joining strategies and technologies to develop appropriate, smart digital tools to support technological enablement for survey data collection in the informal sector in South Africa. The team develop a research methodology and digital tools, which enables geographic information systems (GIS) survey data collection over large geographical areas, at scale, which relies on rigorous data quality controls. Providing various opportunities to local young people in the digital, gig-economy through data collection and digital networking. The major perspective of this paper is to study the aspect of technology and the way it empowers young people on the African continent. The paper presents the design, evolution and results of the digital platform as developed for two major South African Projects.}
}
@article{KHURSHID2019148,
title = {Analyzing diffusion patterns of big open data as policy innovation in public sector},
journal = {Computers & Electrical Engineering},
volume = {78},
pages = {148-161},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618330635},
author = {Muhammad Mahboob Khurshid and Nor Hidayati Zakaria and Ammar Rashid and Rafaqat Kazmi and Muhammad Noman Shafique and Mohammad {Nazir Ahmad}},
keywords = {Big Open Data, Open Government Data, Policy Innovation, Diffusion, Adoption, Knowledge-based Systems, Developing country, Pakistan},
abstract = {The Fourth Industrial Revolution begins, and policies are being formulated by economies as well as efforts are being made to meet the needs of the current times. Among other technologies which are part of Fourth Industrial Revolution, big open data is also the part of Pakistan’s digital policy framework. Scholarship is available to explore and examine the determinants of BOD diffusion and adoption, a little literature is available to understand big open data as policy innovation and its diffusion. Therefore, this study drew on Diffusion of Innovation (DOI) theory and its application to public policy innovation research to examine patterns of BOD policy innovation diffusion at government and public bodies’ level in Pakistan. These patterns are based on policy declaration timing by the governments, policy adoption timing, development of technological applications, and proactively released datasets statistics in public bodies. An Event History Analysis was carried out to examine BOD policy innovation diffusion patterns. Results showed that Federal government is the innovator for the policy innovation diffusion across different governments and subsequent public bodies. It was also found that efficacy ranking of public bodies is quite low in terms of developing BOD technological platforms and proactive release of datasets in large quantities. Therefore, government and policy-makers should focus on effective implementation of big open data policy innovation in Pakistan to equip with 21st century needs.}
}
@article{JEWELL20198,
title = {The analytics lifecycle and the age of innovation},
journal = {Network Security},
volume = {2019},
number = {4},
pages = {8-11},
year = {2019},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(19)30047-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485819300479},
author = {Nick Jewell},
abstract = {A huge number of businesses and organisations of all sizes, from the world's biggest, to small, local companies manage themselves using data to direct their energies. Analytics is at the forefront of modern business management. These are the thoughts that Dave Wells, practice director for data management at Eckerson Group, puts forward in the report ‘The New Analytics Lifecycle’.1 Organisations of all sizes use data to direct their energies. Analytics is at the forefront of modern business management. Yet with the continuing snowballing of data resources, access privileges and the technology stack to support them, security and governance have not been kept top of mind within the analytics lifecycle. An organisation needs to understand from top to bottom the data owners, flows and processes it has and then map onto these new ways of doing business that will keep them compliant and customer-centric, says Nick Jewell of Alteryx.}
}
@article{QUON2019809,
title = {Needs and Challenges for Radiation Oncology in the Era of Precision Medicine},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {103},
number = {4},
pages = {809-817},
year = {2019},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2018.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0360301618340033},
author = {Harry Quon and Todd McNutt and Junghoon Lee and Michael Bowers and Wei Jiang and Pranav Lakshminarayanan and Zhi Cheng and Peijin Han and Xuan Hui and Veeraj Shah and Joseph Moore and Minoru Nakatsugawa and Scott Robertson and Emilie Cecil and Brandi Page and Ana Kiess and John Wong and Theodore DeWeese},
abstract = {Modern medicine, including the care of the cancer patient, has significantly advanced, with the evidence-based medicine paradigm serving to guide clinical care decisions. Yet we now also recognize the tremendous heterogeneity not only of disease states but of the patient and his or her environment as it influences treatment outcomes and toxicities. These reasons and many others have led to a reevaluation of the generalizability of randomized trials and growing interest in accounting for this heterogeneity under the rubric of precision medicine as it relates to personalizing clinical care predictions, decisions, and therapy for the disease state. For the cancer patient treated with radiation therapy, characterizing the spatial treatment heterogeneity has been a fundamental tenet of routine clinical care facilitated by established database and imaging platforms. Leveraging these platforms to further characterize and collate all clinically relevant sources of heterogeneity that affect the longitudinal health outcomes of the irradiated cancer patient provides an opportunity to generate a critical informatics infrastructure on which precision radiation therapy may be realized. In doing so, data science–driven insight discoveries, personalized clinical decisions, and the potential to accelerate translational efforts may be realized ideally within a network of institutions with locally developed yet coordinated informatics infrastructures. The path toward realizing these goals has many needs and challenges, which we summarize, with many still to be realized and understood. Early efforts by our group have identified the feasibility of this approach using routine clinical data sets and offer promise that this transformation can be successfully realized in radiation oncology.}
}
@article{ZHANG2019425,
title = {Towards a unified multi-source-based optimization framework for multi-label learning},
journal = {Applied Soft Computing},
volume = {76},
pages = {425-435},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618307051},
author = {Jia Zhang and Candong Li and Zhenqiang Sun and Zhiming Luo and Changen Zhou and Shaozi Li},
keywords = {Multi-label learning, Multi-source fusion, Optimization framework},
abstract = {In the era of Big Data, a practical yet challenging task is to make learning techniques more universally applicable in dealing with the complex learning problem, such as multi-source multi-label learning. While some of the early work have developed many effective solutions for multi-label classification and multi-source fusion separately, in this paper we learn the two problems together, and propose a novel method for the joint learning of multiple class labels and data sources, in which an optimization framework is constructed to formulate the learning problem, and the result of multi-label classification is induced by the weighted combination of the decisions from multiple sources. The proposed method is responsive in exploiting the label correlations and fusing multi-source data, especially in the fusion of long-tail data. Experiments on various multi-source multi-label data sets reveal the advantages of the proposed method.}
}
@article{BURKE2019869,
title = {The use of machine learning in the study of suicidal and non-suicidal self-injurious thoughts and behaviors: A systematic review},
journal = {Journal of Affective Disorders},
volume = {245},
pages = {869-884},
year = {2019},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2018.11.073},
url = {https://www.sciencedirect.com/science/article/pii/S0165032718317506},
author = {Taylor A. Burke and Brooke A. Ammerman and Ross Jacobucci},
keywords = {Machine learning, Suicide, Suicide attempt, Suicide risk, Suicidal ideation, Non-suicidal self-injury, Big data, Pattern recognition, Exploratory data mining},
abstract = {Background
Machine learning techniques offer promise to improve suicide risk prediction. In the current systematic review, we aimed to review the existing literature on the application of machine learning techniques to predict self-injurious thoughts and behaviors (SITBs).
Method
We systematically searched PsycINFO, PsycARTICLES, ERIC, CINAHL, and MEDLINE for articles published through February 2018.
Results
Thirty-five articles met criteria to be included in the review. Included articles were reviewed by outcome: suicide death, suicide attempt, suicide plan, suicidal ideation, suicide risk, and non-suicidal self-injury. We observed three general aims in the use of SITB-focused machine learning analyses: (1) improving prediction accuracy, (2) identifying important model indicators (i.e., variable selection) and indicator interactions, and (3) modeling underlying subgroups. For studies with the aim of boosting predictive accuracy, we observed greater prediction accuracy of SITBs than in previous studies using traditional statistical methods. Studies using machine learning for variable selection purposes have both replicated findings of well-known SITB risk factors and identified novel variables that may augment model performance. Finally, some of these studies have allowed for subgroup identification, which in turn has helped to inform clinical cutoffs.
Limitations
Limitations of the current review include relatively low paper sample size, inconsistent reporting procedures resulting in an inability to compare model accuracy across studies, and lack of model validation on external samples.
Conclusions
We concluded that leveraging machine learning techniques to further predictive accuracy and identify novel indicators will aid in the prediction and prevention of suicide.}
}
@article{TARIQ2019745,
title = {Accurate detection of sitting posture activities in a secure IoT based assisted living environment},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {745-757},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1731333X},
author = {Muhammad Tariq and Hammad Majeed and Mirza Omer Beg and Farrukh Aslam Khan and Abdelouahid Derhab},
keywords = {Social Internet of Things (SIoT), Information assurance and security, Kinect, Hidden Markov Model, Human robot interaction, Activity recognition, Neural network},
abstract = {In this work, we present a technique as well as a dataset for improving daily life assistive activities in a smart Internet of Things (IoT) driven environment. We propose that augmenting data from multiple sensing devices such as Microsoft Kinect and Smartwatches can significantly improve the detection performance once incorporated in the context of an IoT framework. Kinect, being the feature-wise richest input device in the IoT world, is a favorite pick of most of the researchers for detecting postural activities. However, there are certain activity classes in IoT based smart environments on which Kinect based solutions result in high misclassifications. This is due to the similarities in 3D joint position space. For such scenarios, Kinect must be augmented with additional sensor(s) to achieve the desired level of accuracy. In this work, we improve the detection of the assistive activities related to sitting posture in general and dining-related activities in particular. Our research focus is to enable a robot to understand the activities of a human at the dining table and plan the assistive tasks accordingly. This is a two-step process; in the first step, Kinect sensor data is augmented with a collection of motion sensors’ data. Then, this data is analyzed for discrimination power through cross-validation of the Hidden Markov Model (HMM). In addition, we propose a two-level security scheme consisting of key establishment and two-factor authentication for the IoT based activity recognition environment. Our experiments show that Kinect, when complemented by motion sensors’ data, reduces the confusion instances by up to 12% on average. Moreover, we demonstrate the data quality through clustering properties of the data using an unsupervised neural network.}
}
@article{KONOVALENKO2019229,
title = {Event processing in supply chain management – The status quo and research outlook},
journal = {Computers in Industry},
volume = {105},
pages = {229-249},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518303166},
author = {Iurii Konovalenko and André Ludwig},
keywords = {Logistics, Supply chain event management, Event processing, Decision support, Literature review, Research agenda},
abstract = {Increasing supply chain complexity poses new challenges to managers. On the other hand, evolving information and communication technology offers ample opportunity for more reliable supply chain management practices. Event processing has established itself in many applications in logistics. Although the topic has enjoyed increasing popularity, there is no study taking stock of prior developments and guiding future research. Therefore, a systematic literature review on the topic of event processing in supply chain management from 2005 until the present is undertaken. Extant literature is synthesized and analyzed from technological and supply chain management perspectives to inform scholars and practitioners of existing field developments. Additionally, to guide future scholarly endeavors, a research agenda is derived from promising topics raised in papers and unfulfilled practical requirements. We find that current solutions primarily focus on a limited number of supply chain core processes and a restricted number of supply chain actors. The majority of publications focused on time-temperature sensitive products. Additionally, the domination of road transportation can be observed, while other modes of transport are often ignored in solution implementations. Decision support in terms of object traceability within the supply chain is found in most articles. RFID, typically accompanied by the Electronic Product Code Information Services standard, is the dominant enabling technology. Future research should focus on the topics of standardization, granularity, data sources, and cooperation. Moreover, holistic event processing supported by big data and machine learning techniques could create interfaces with other legacy business intelligence applications. Another promising area includes the exploration of new technologies, i.e. IoT, to enable new smart solutions.}
}
@article{WANG201974,
title = {Trajectory analysis for on-demand services: A survey focusing on spatial-temporal demand and supply patterns},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {108},
pages = {74-99},
year = {2019},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X19306278},
author = {Shuofeng Wang and Li Li and Wanjing Ma and Xiqun Chen},
keywords = {On-demand services, Trajectory analysis, Big data},
abstract = {With the development of information technique and wireless communication, a vast number of taxis' and ride-sharing cars' trajectory data that provide a rich and detailed source to study on-demand services have been collected. The increasing available trajectory data bring benefits and new challenges to the studies of on-demand services. To provide an overview of the benefits and challenges brought by the trajectory data, we provide a survey on recent studies of trajectory analysis (refer to analyzing trajectory datasets) for on-demand services in this paper. Our purposes are at least trifold. First, we highlight the value of trajectory data in understanding on-demand services and discuss the procedures of retrieving information for the demand part and the supply part from raw trajectory data. Second, we categorize related studies into three parts (the demand part, the supply part, and the mixed part) and review the significant findings. For the demand part, we focus on the models proposed for describing and explaining the spatial-temporal characteristics of observed trips. Methods or models proposed for describing trip statistics, scaling laws of trips, and dynamics of ridership are reviewed. We summarize four types of factors that influence the spatial-temporal patterns of demands. For the supply part, we focus on the models proposed for describing the spatial-temporal characteristics of available taxis/ride-sharing cars and modeling the behavior of drivers (i.e., passenger-search behavior and route choice behavior) to explain the spatial-temporal patterns of taxi/ride-sharing supplies. For the mixed part, we focus on studies that apply the uncovered demands/supplies patterns to design recommendation systems and pricing strategies. Third, we discuss the future directions on collecting/releasing trajectory data and future research directions to advance the understanding of on-demand services.}
}
@incollection{TSIATSIS2019279,
title = {Chapter 14 - Smart Cities},
editor = {Vlasios Tsiatsis and Stamatis Karnouskos and Jan Höller and David Boyle and Catherine Mulligan},
booktitle = {Internet of Things (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {279-287},
year = {2019},
isbn = {978-0-12-814435-0},
doi = {https://doi.org/10.1016/B978-0-12-814435-0.00027-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128144350000274},
author = {Vlasios Tsiatsis and Stamatis Karnouskos and Jan Höller and David Boyle and Catherine Mulligan},
keywords = {cities, urban development, Context Information Management, citizens, data privacy, data security, physical security},
abstract = {Smart Cities are a concept that has gained a lot of attention over the last decade – from solutions that ensure more efficient use and monitoring of air quality, lighting, and traffic to concepts that are designed to ensure citizen engagement and “fun” technology – and has been seen as an opportunity to redefine how we live, work, and play in our urban environments. Cities, however are complex environments and house not just companies, but also schools, hospitals, and green areas. Just exactly what a smart city is and how much citizen data is appropriately used within such systems is a key question that has been raised in many different areas. This chapter investigates these areas and outlines different use cases and business models for this promising and complex area of IoT.}
}
@article{FRAHM2019101957,
title = {Introducing the Peabody-Yale Reference Obsidians (PYRO) sets: Open-source calibration and evaluation standards for quantitative X-ray fluorescence analysis},
journal = {Journal of Archaeological Science: Reports},
volume = {27},
pages = {101957},
year = {2019},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2019.101957},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X19303323},
author = {Ellery Frahm},
keywords = {Obsidian sourcing, EDXRF, Portable XRF, Accuracy, Reproducibility, Transparency, Collaboration},
abstract = {A series of well-characterized specimens, known as the Peabody-Yale Reference Obsidians (PYRO) sets, has been designed to aid with calibrating and assessing X-ray fluorescence analysis (XRF) data, including portable XRF (pXRF) measurements, for obsidian sourcing. Each of these ten matched sets consists of 35 specimens: 20 specimens for calibration and 15 specimens for evaluation. These sets include not only obsidians with common geochemical compositions (i.e., alkaline rhyolites) but also rarer ones (i.e., peralkaline rhyolitic, trachytic, and andesitic specimens, including East African Rift obsidians). When used as described, the PYRO sets are suitable to calibrate and evaluate XRF data for obsidians worldwide. A set can be borrowed following loan policies of the Peabody Museum of Natural History, which will also accession a set. Publishing all source information – from their names and GPS coordinates to the datasets used for the recommended values – not only allows the sets to be replicated by others but also fulfills the demands of scientific transparency. Their main purpose is facilitating collaborations and “big data” projects, and the PYRO sets were designed to complement existing protocols for calibration and evaluation. In short, the sets are intended as a tool for almost anyone (e.g., a student who borrows an instrument to source artifacts) to meet – and even exceed – experts' practices involving transparency, accuracy, and reproducibility.}
}
@article{BOYNE201911,
journal = {Tourism Management},
volume = {70},
pages = {11-12},
year = {2019},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2018.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0261517718301547},
author = {Steven Boyne}
}
@article{LI2019109254,
title = {Data-driven health estimation and lifetime prediction of lithium-ion batteries: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {113},
pages = {109254},
year = {2019},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2019.109254},
url = {https://www.sciencedirect.com/science/article/pii/S136403211930454X},
author = {Yi Li and Kailong Liu and Aoife M. Foley and Alana Zülke and Maitane Berecibar and Elise Nanini-Maury and Joeri {Van Mierlo} and Harry E. Hoster},
keywords = {Lithium-ion battery, Data-driven approach, Ageing mechanism, Battery health diagnostics and prognostics, Electric vehicle, Sustainable energy},
abstract = {Accurate health estimation and lifetime prediction of lithium-ion batteries are crucial for durable electric vehicles. Early detection of inadequate performance facilitates timely maintenance of battery systems. This reduces operational costs and prevents accidents and malfunctions. Recent advancements in “Big Data” analytics and related statistical/computational tools raised interest in data-driven battery health estimation. Here, we will review these in view of their feasibility and cost-effectiveness in dealing with battery health in real-world applications. We categorise these methods according to their underlying models/algorithms and discuss their advantages and limitations. In the final section we focus on challenges of real-time battery health management and discuss potential next-generation techniques. We are confident that this review will inform commercial technology choices and academic research agendas alike, thus boosting progress in data-driven battery health estimation and prediction on all technology readiness levels.}
}
@article{QIN20191,
title = {Enactment of adaptation in data stream processing with latency implications—A systematic literature review},
journal = {Information and Software Technology},
volume = {111},
pages = {1-21},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300539},
author = {Cui Qin and Holger Eichelberger and Klaus Schmid},
keywords = {Stream processing, Big data, Runtime adaptation, Enactment, Latency, Systematic literature review},
abstract = {Context
Stream processing is a popular paradigm to continuously process huge amounts of data. Runtime adaptation plays a significant role in supporting the optimization of data processing tasks. In recent years runtime adaptation has received significant interest in scientific literature. However, so far no categorization of the enactment approaches for runtime adaptation in stream processing has been established.
Objective
This paper identifies and characterizes different approaches towards the enactment of runtime adaptation in stream processing with a main focus on latency as quality dimension.
Method
We performed a systematic literature review (SLR) targeting five main research questions. An automated search, resulting in 244 papers, was conducted. 75 papers published between 2006 and 2018 were finally included. From the selected papers, we extracted data like processing problems, adaptation goals, enactment approaches of adaptation, enactment techniques, evaluation metrics as well as evaluation parameters used to trigger the enactment of adaptation in their evaluation.
Results
We identified 17 different enactment approaches and categorized them into a taxonomy. For each, we extracted the underlying technique used to implement this enactment approach. Further, we identified 9 categories of processing problems, 6 adaptation goals, 9 evaluation metrics and 12 evaluation parameters according to the extracted data properties.
Conclusion
We observed that the research interest on enactment approaches to the adaptation of stream processing has significantly increased in recent years. The most commonly applied enactment approaches are parameter adaptation to tune parameters or settings of the processing, load balancing used to re-distribute workloads, and processing scaling to dynamically scale up and down the processing. In addition to latency, most adaptations also address resource fluctuation / bottleneck problems. For presenting a dynamic environment to evaluate enactment approaches, researchers often change input rates or processing workloads.}
}
@article{SIVAKUMAR2019347,
title = {New and emerging methods for the improved modelling of travel behaviour: Transportation Research Part B: Special issue of papers from the IATBR 2015 conference},
journal = {Transportation Research Part B: Methodological},
volume = {123},
pages = {347-348},
year = {2019},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2018.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0191261518306581},
author = {Aruna Sivakumar and Stephane Hess}
}
@article{PARKINSON201915,
title = {Continuous IEQ monitoring system: Context and development},
journal = {Building and Environment},
volume = {149},
pages = {15-25},
year = {2019},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0360132318307467},
author = {Thomas Parkinson and Alex Parkinson and Richard {de Dear}},
keywords = {Indoor environmental quality, Continuous monitoring, Building performance, Sensors, Analytics, Measurement},
abstract = {Addressing two common challenges for building performance – reducing the carbon footprint attached to the provision of comfortable indoor environments, and improving the health and wellbeing of occupants – requires a more comprehensive understanding of how the indoor environments of buildings are operated. This paper introduces SAMBA, a state-of-the-art monitoring station for continuous, real-time measurements of indoor environmental quality (IEQ) parameters from occupants’ work desks. It combines a hardware solution that integrates a low-cost suite of sensors with a software platform designed to automatically analyse and visualize data for quick interpretation of IEQ performance by non-scientist. In addition to feeding a massive IEQ database for research, the resulting data may be used to better inform the metrological requirements for popular international IEQ rating schemes. This new era of indoor environmental monitoring based upon systems such as SAMBA affords a fundamentally new approach to built environmental field research that holds significant promise to improve building performance and indoor environmental quality and occupant satisfaction, health, wellbeing and performance.}
}
@article{ZHOU2019102043,
title = {Why do we hardly see people with visual impairments in the street? A case study of Changsha, China},
journal = {Applied Geography},
volume = {110},
pages = {102043},
year = {2019},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2019.102043},
url = {https://www.sciencedirect.com/science/article/pii/S0143622818311172},
author = {Kaichun Zhou and Chengfeng Hu and Honghui Zhang and Yulong Hu and Binggeng Xie},
keywords = {Tactile paving surfaces, Obstructions, Connectivity, Network},
abstract = {Although China has the longest and widest range of tactile paving surfaces in the world, we hardly see people with visual impairments on city streets. A main reason may be the poor state of these tactile paving surfaces. In this study, we evaluate the effectiveness of tactile paving surfaces in Changsha city by measuring their covering capacity and connectivity. We first collect the obstruction data with smart phones. Then, the obstruction data are transformed and matched with road networks. Tactile paving surface networks are constructed later. Finally, the shortest distances between public service facilities and tactile paving surface networks are analysed, and several indicators are calculated to measure the connectivity of the tactile paving surface networks. The results show that the state of tactile paving is worse than it should be, thus leading to poor effectiveness of the tactile paving surfaces.}
}
@article{GHASEMAGHAEI201914,
title = {Does data analytics use improve firm decision making quality? The role of knowledge sharing and data analytics competency},
journal = {Decision Support Systems},
volume = {120},
pages = {14-24},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300429},
author = {Maryam Ghasemaghaei},
keywords = {Data analytics use, Knowledge sharing, Decision making quality, Data analytics competency},
abstract = {Despite the huge investment in data analytics tools, the necessary conditions required to obtain benefit from such investment deserves close investigation. In this study we utilize the knowledge-based view and data analytics competency literature to address two important research questions: (1) Does knowledge sharing have a mediating role on the impact of data analytics usage on the quality of firm decisions?; and (2) What is the role of data analytics competency in enhancing the quality of firm decisions through increasing knowledge sharing? Survey data collected from top and middle-level managers from 133 U.S.-based firms indicates that: the impact of data analytics use on the quality of firm decisions is fully mediated by knowledge sharing; the impact of knowledge sharing on firm decision quality is not significant and it is moderated by data analytics competency; and data analytics competency does not moderate the impact of data analytics use on knowledge sharing. Collectively, the findings provide a theory-based understanding of how data analytics use improves firm decision quality. The results also provide actionable guidelines for firms regarding the critical resources they need to invest in order to obtain benefits from using data analytics tools.}
}
@article{BIRKIN2019245,
title = {Spatial data analytics of mobility with consumer data},
journal = {Journal of Transport Geography},
volume = {76},
pages = {245-253},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2018.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300212},
author = {Mark Birkin},
keywords = {Consumer data, Attitudes, Lifestyle, Consumer behaviour, Personal mobility},
abstract = {Consumer data arising from the interaction between customers and service providers are becoming ubiquitous. These data are appealing for research because they are frequently collected and quickly released; they cover a wide variety of attitudes, lifestyles and behavioural characteristics; and they are often dynamically replenished and longitudinal. It is demonstrated that consumer data can make important contributions to understanding problems in transport geography and in solving applied problems ranging from migration, infrastructure investment and retail service provision to commuting and individual mobility. However more effective exploitation of these data depends on the construction of bridges to allow greater freedom in the transfer of data from the commercial to the academic sector; it requires development of frameworks for privacy and ethics in the secondary use of personal data; and it is contingent on the emergence of effective strategies for the amelioration of selection bias which impairs the quality of many consumer data sources.}
}
@article{RIESENER2019729,
title = {The Digital Shadow as Enabler for Data Analytics in Product Life Cycle Management},
journal = {Procedia CIRP},
volume = {80},
pages = {729-734},
year = {2019},
note = {26th CIRP Conference on Life Cycle Engineering (LCE) Purdue University, West Lafayette, IN, USA May 7-9, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.01.083},
url = {https://www.sciencedirect.com/science/article/pii/S221282711930085X},
author = {Michael Riesener and Günther Schuh and Christian Dölle and Christian Tönnes},
keywords = {Product information, Data analytics, Life cycle management},
abstract = {The high availability of data as well as intelligent analysis methods are central points of “Industrie 4.0”. However, companies often miss their opportunities because they are not able use available information. In times of a dynamic market environment, it is essential for companies to build up a detailed understanding of the influences on their business based on data. Especially in product development and life cycle management, the use of data-generated knowledge can significantly increase the productivity. The problem is not a lack of information but an information overload, as companies are generating more data than they are actually able to use. At the same time, it is crucial that decision-makers have access to the right information for the right purpose at the right time. Which means, the primarily reason for a lack of data-based decisions is the availability of suitable data. Especially the identification and preparation of data for certain analyses causes a high delay and therefore a decrease of decision quality. A multi-perspective and database-overarching information model of products is necessary to overcome incomprehensibility of information, undefined data structures and inflexibility of systems. The methodology presented in this paper describes the derivation of such an information model, which is collecting and merging selected information from existing databases for business intelligence applications. Key element of the method is the choice of a suitable data source for the required information. Therefore, a data-information-fit-indicator is introduced to enable the assignment of data sources to information of the information model.}
}
@article{HAEGEMANS20191,
title = {A theoretical framework to improve the quality of manually acquired data},
journal = {Information & Management},
volume = {56},
number = {1},
pages = {1-14},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617309801},
author = {Tom Haegemans and Monique Snoeck and Wilfried Lemahieu},
keywords = {Data quality, Data entry, Manually acquired data, Information quality},
abstract = {We present a framework for organisations to prevent errors in data entry. It states that data entry errors can be prevented by a strong intention of data producers to enter data correctly and by a high task-technology fit. Two empirical studies support the framework and demonstrate that a high task-technology fit is relatively more important than the data producers’ intention. The framework refines the theory of planned behaviour, and extends the explanatory domain of the task-technology fit construct. The empirical evidence underlines the importance of the task-technology fit construct, an often-neglected construct in information systems research.}
}
@article{PENCE2019240,
title = {Data-theoretic methodology and computational platform to quantify organizational factors in socio-technical risk analysis},
journal = {Reliability Engineering & System Safety},
volume = {185},
pages = {240-260},
year = {2019},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2018.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S095183201830053X},
author = {Justin Pence and Tatsuya Sakurahara and Xuefeng Zhu and Zahra Mohaghegh and Mehmet Ertem and Cheri Ostroff and Ernie Kee},
keywords = {Probabilistic Risk Assessment (PRA), Organizational factors, Human Reliability Analysis (HRA), Text mining, Causal modeling, Big data analytics},
abstract = {Organizational factors, as literature indicates, are significant contributors to risk in high-consequence industries. Therefore, building a theoretical framework equipped with reliable modeling techniques and data analytics to quantify the influence of organizational performance on risk scenarios is important for improving realism in Probabilistic Risk Assessment (PRA). The Socio-Technical Risk Analysis (SoTeRiA) framework theoretically connects the structural (e.g., safety practices) and behavioral (e.g., safety culture) aspects of an organization with PRA. An Integrated PRA (I-PRA) methodological framework is introduced to operationalize SoTeRiA in order to quantify the incorporation of underlying organizational failure mechanisms into risk scenarios. This research focuses on the Data-Theoretic module of I-PRA, which has two sub-modules: (i) DT-BASE: developing detailed causal relationships in SoTeRiA, grounded on theories and equipped with a semi-automated baseline quantification utilizing information extracted from academic articles, industry procedures, and regulatory standards, and (ii) DT-SITE: conducting automated data extraction and inference methods to quantify SoTeRiA causal elements based on site-specific event databases and by Bayesian updating of the DT-BASE baseline quantification. A case study demonstrates the quantification of a nuclear power plant's organizational “training” causal model, which is associated with the training/experience in Human Reliability Analysis, along with a sensitivity analysis to identify critical factors.}
}
@article{GRIMALDI2019e02195,
title = {Data maturity analysis and business performance. A Colombian case study},
journal = {Heliyon},
volume = {5},
number = {8},
pages = {e02195},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02195},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019358554},
author = {Didier Grimaldi and Javier Diaz and Hugo Arboleda and Vicenc Fernandez},
keywords = {Business, Computer science, Information science, Economics, Colombia, Data science, Competitive analytics, Qualitative comparative analysis, Business performance},
abstract = {Context
Colombia over the last decade has experienced a historic economic boom and Information Technology (IT) has been emerging as a tool to enable the competitiveness of companies. The last government (2014–2018) took different actions to explain how the use of data science and open data improve the business activity. The question to identify if there is a relationship between IT capacities, the organizational structure and the performance of the companies remains unresolved and is certainly an urgent issue for new government of Iván Duque.
Purpose
Our study analyses the relationship between data structure and business performance measured through the efficiency of customer experience and provider operations processes.
Methodology
our methodology is novel compared to previous researches which develop linear regression. It is based on the use of a fuzzy-set qualitative comparative analysis (fsQCA).
Originality/value
our method allows to reveal multiple and complementary paths to achieve possible correlations between data and business performance.
Findings
Our results show that data consistency, data usage and data protection are the three more frequent conditions to a better customer experience and provider operations efficiency. Surprisingly, data-driven profile is a necessary but not sufficient condition.
Practical implications
our conclusions allow practitioners to uncover the strength of the data to orientate their digital strategy. Our recommendations could be used for the new governmental program of digital revival for Small and Medium Enterprises.}
}
@article{NAM2019233,
title = {Business analytics use in CRM: A nomological net from IT competence to CRM performance},
journal = {International Journal of Information Management},
volume = {45},
pages = {233-245},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217308812},
author = {Dalwoo Nam and Junyeong Lee and Heeseok Lee},
keywords = {Business analytics, Dynamic capability, Data management capability, Customer response capability, Customer relationship management},
abstract = {Business analytics (BA) becomes increasingly important under rapidly changing business environment. A research challenge is that BA use is not fully understood. We tackle this challenge from the perspective of dynamic capability by using an empirical model with the emphasis on BA use in customer relationship management (CRM). Based on 170 samples from firm-level survey, we analyze the nomological linkage from IT competence to CRM performance. The results show data management capability fully mediates between IT competence and BA use, while customer response capability partially mediates between BA use and CRM performance.}
}
@article{JAMALUDIN2019496,
title = {An integrated carbon footprint accounting and sustainability index for palm oil mills},
journal = {Journal of Cleaner Production},
volume = {225},
pages = {496-509},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.03.312},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619310285},
author = {Nabila Farhana Jamaludin and Zarina Ab Muis and Haslenda Hashim},
keywords = {Carbon accounting, Carbon footprint, Carbon footprint accounting, Sustainability assessment, Integrated assessment, Palm oil mill, Big data analysis},
abstract = {Palm oil industry has received criticism from various parties on the issue of sustainability and the greenhouse gases. Carbon footprint accounting are widely used as a metric of climate change impacts and the main focus of many sustainability policies among companies and authorities. However, carbon footprint accounting has limitation to represent sustainability as a whole and may resulting inaccurate selection of further mitigation. This paper evaluates sustainability and greenhouse gases simultaneously using an integrated palm oil mill carbon footprint accounting (POMCFA) and palm oil mill sustainability index (POMSI) method. The integration was performed via the adoption of data synchronization of the carbon footprint accounting and sustainability assessment. The analysis shows that highest carbon dioxide equivalent emission was contributed by palm oil mill effluent followed by diesel consumption and water consumption. In terms of sustainability scoring, the results show that the environmental aspect achieved the lowest scores compared to other aspects (social and economy). Weaknesses identified include diesel consumption, palm oil mill effluent and boiler emission. The assessment analysed in terms of carbon dioxide equivalent and sustainability scoring demonstrates its potential to provide comprehensive mitigation selection purposes.}
}
@article{SPENCE2019801,
title = {The Future Directions of Research in Cardiac Anesthesiology},
journal = {Anesthesiology Clinics},
volume = {37},
number = {4},
pages = {801-813},
year = {2019},
note = {Cardiothoracic Anesthesia and Critical Care},
issn = {1932-2275},
doi = {https://doi.org/10.1016/j.anclin.2019.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1932227519300655},
author = {Jessica Spence and C. David Mazer},
keywords = {Cardiothoracic anesthesia, Research, Trial design, Future}
}
@article{KUANG2019161,
title = {Predicting duration of traffic accidents based on cost-sensitive Bayesian network and weighted K-nearest neighbor},
journal = {Journal of Intelligent Transportation Systems},
volume = {23},
number = {2},
pages = {161-174},
year = {2019},
note = {Vehicle Sensor Data-based Transportation Research: Modelling, Analysis, And Management},
issn = {1547-2450},
doi = {https://doi.org/10.1080/15472450.2018.1536978},
url = {https://www.sciencedirect.com/science/article/pii/S1547245022011513},
author = {Li Kuang and Han Yan and Yujia Zhu and Shenmei Tu and Xiaoliang Fan},
keywords = {Accident duration prediction, Bayesian network, cost-sensitive, KNN regression},
abstract = {With the development of urbanization, road congestion has become increasingly serious, and an important cause is the traffic accidents. In this article, we aim to predict the duration of traffic accidents given a set of historical records and the feature of the new accident, which can be collected from the vehicle sensors, in order to help guide the congestion and restore the road. Existing work on predicting the duration of accidents seldom consider the imbalance of samples, the interaction of attributes, and the cost-sensitive problem sufficiently. Therefore, in this article, we propose a two-level model, which consists of a cost-sensitive Bayesian network and a weighted K-nearest neighbor model, to predict the duration of accidents. After data preprocessing and variance analysis on the traffic accident data of Xiamen City in 2015, the model uses some important discrete attributes for classification, and then utilizes the remaining attributes for K-nearest neighbor regression prediction. The experiment results show that our proposed approach to predicting the duration of accidents achieves higher accuracy compared with classical models.}
}
@article{WOLFERS2019240,
title = {From pattern classification to stratification: towards conceptualizing the heterogeneity of Autism Spectrum Disorder},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {104},
pages = {240-254},
year = {2019},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0149763419303197},
author = {Thomas Wolfers and Dorothea L. Floris and Richard Dinga and Daan {van Rooij} and Christina Isakoglou and Seyed Mostafa Kia and Mariam Zabihi and Alberto Llera and Rajanikanth Chowdanayaka and Vinod J. Kumar and Han Peng and Charles Laidi and Dafnis Batalle and Ralica Dimitrova and Tony Charman and Eva Loth and Meng-Chuan Lai and Emily Jones and Sarah Baumeister and Carolin Moessnang and Tobias Banaschewski and Christine Ecker and Guillaume Dumas and Jonathan O’Muircheartaigh and Declan Murphy and Jan K. Buitelaar and Andre F. Marquand and Christian F. Beckmann},
keywords = {Autism spectrum disorder, Machine learning, Pattern recognition, Classification, Clustering, Stratification, Biotypes, Precision medicine},
abstract = {Pattern classification and stratification approaches have increasingly been used in research on Autism Spectrum Disorder (ASD) over the last ten years with the goal of translation towards clinical applicability. Here, we present an extensive scoping literature review on those two approaches. We screened a total of 635 studies, of which 57 pattern classification and 19 stratification studies were included. We observed large variance across pattern classification studies in terms of predictive performance from about 60% to 98% accuracy, which is among other factors likely linked to sampling bias, different validation procedures across studies, the heterogeneity of ASD and differences in data quality. Stratification studies were less prevalent with only two studies reporting replications and just a few showing external validation. While some identified strata based on cognition and intelligence reappear across studies, biology as a stratification marker is clearly underexplored. In summary, mapping biological differences at the level of the individual with ASD is a major challenge for the field now. Conceptualizing those mappings and individual trajectories that lead to the diagnosis of ASD, will become a major challenge in the near future.}
}
@article{GOLD20193100,
title = {Electrocardiogram in Cardiac Resynchronization Therapy: Is Non–Left Bundle Branch Block an Oversimplification?∗},
journal = {Journal of the American College of Cardiology},
volume = {73},
number = {24},
pages = {3100-3101},
year = {2019},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2019.03.516},
url = {https://www.sciencedirect.com/science/article/pii/S0735109719349265},
author = {Michael R. Gold and Scott M. Koerber},
keywords = {cardiac resynchronization therapy defibrillator, National Cardiovascular Data Registry, non–left bundle branch block, nonspecific interventricular conduction delay, right bundle branch block}
}
@article{DURANRODAS2019688,
title = {Automated open-source data collection and processing: an example of OpenStreetMap and bike-sharing},
journal = {Transportation Research Procedia},
volume = {41},
pages = {688-693},
year = {2019},
note = {Urban Mobility – Shaping the Future Together mobil.TUM 2018 – International Scientific Conference on Mobility and Transport Conference Proceedings},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2019.09.117},
url = {https://www.sciencedirect.com/science/article/pii/S2352146519305344},
author = {David Duran-Rodas and Emmanouil Chaniotakis and Constantinos Antoniou},
keywords = {bike-sharing, influencing factors, data mining, automated}
}
@article{WILJER2019S8,
title = {Developing an Artificial Intelligence–Enabled Health Care Practice: Rewiring Health Care Professions for Better Care},
journal = {Journal of Medical Imaging and Radiation Sciences},
volume = {50},
number = {4, Supplement 2},
pages = {S8-S14},
year = {2019},
note = {Artificial Intelligence in Medical Imaging and Radiation Sciences},
issn = {1939-8654},
doi = {https://doi.org/10.1016/j.jmir.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1939865419305430},
author = {David Wiljer and Zaki Hakim},
keywords = {Artificial intelligence, digital healthcare, AI-enabled health professions education, professional development, AI literacy},
abstract = {Artificial intelligence (AI) has the potential to impact almost every aspect of health care, from detection to prediction and prevention. The adoption of new technologies in health care, however, lags far behind the emergence of new technologies. Health care professionals and organizations must be prepared to change and evolve to adopt these new technologies. A basic understanding of emerging AI technologies will be essential for all health care professionals. These technologies include expert systems, robotic process automation, natural language processing, machine learning, and deep learning. Health care professionals and organizations must build their capacity and capabilities to understand and appropriately adopt these technologies. This understanding starts with basic AI literacy, including data governance principles, basic statistics, data visualization, and the impact on clinical processes. Health care professionals and organizations will need to overcome several challenges and tackle core structural issues, such as access to data and the readiness of algorithms for clinical practice. However, health care professionals have an opportunity to shape the way that AI will be used and the outcomes that will be achieved. There is an urgent and emerging need for education and training so that appropriate technologies can be rapidly adopted, resulting in a healthier world for our patients and our communities.}
}
@article{LI2019109244,
title = {Review of building energy performance certification schemes towards future improvement},
journal = {Renewable and Sustainable Energy Reviews},
volume = {113},
pages = {109244},
year = {2019},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2019.109244},
url = {https://www.sciencedirect.com/science/article/pii/S1364032119304447},
author = {Y. Li and S. Kubicki and A. Guerriero and Y. Rezgui},
keywords = {Energy performance certificate, Building energy demand, Building renovation, Energy planning, Database, Sustainable development, Smart building},
abstract = {The building sector accounts for 40% of the total energy consumption in the EU. It faces great challenges to meet the goal of transforming the existing building stocks into near zero-energy buildings by 2050. The development of Energy Performance Certificate (EPC) schemes in the EU provides a powerful and comprehensive information tool to quantitatively predict annual energy demand from the building stock, creating a demand-driven market for energy-effective buildings. Properties with improved energy rating have had a positive impact on property investments and rental return because of the reduced energy bills. In addition, the EPC databases have been applied to energy planning and building renovations. However, it should be mentioned that the current evaluation system faces problems, such as not being fully implemented, delivering low quality and insufficient information to stimulate renovation, therefore requiring improvements to be made. This paper provides a review of the current EPC situations in the EU and discusses the direction of future improvements. The next generation EPC should rely on BIM technology, benefit from big data techniques and use building smart-readiness indicators to create a more reliable, affordable, comprehensive and customer-tailored instrument, which could better represent energy efficiency, together with occupants’ perceived comfort, and air quality. Improved EPC schemes are expected to play an active role in monitoring building performance, future energy planning and quantifying building renovation rates, promoting energy conservation and sustainability.}
}
@incollection{MOYE201911,
title = {Chapter 2 - Statistical Interpretation of the Utility and Value of a Biomarker},
editor = {Vijay Nambi},
booktitle = {Biomarkers in Cardiovascular Disease},
publisher = {Elsevier},
pages = {11-20},
year = {2019},
isbn = {978-0-323-54835-9},
doi = {https://doi.org/10.1016/B978-0-323-54835-9.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323548359000028},
author = {Lem Moyé},
keywords = {Biomarkers, Biostatistics, Modeling, Relationship-building, Surrogacy},
abstract = {Representing the first link between pathology and a biomarker signal, biomarkers pose unique challenges in assessing their precision, accuracy and predictive values. Quality control is a key first step in assessing the utility of biomarkers. The notion of surrogacy is a continuing challenge. Established relationship building tools such as logistic regression and Cox regression have been particularly useful in understanding the strength of the biomarker-clinical endpoint relationship. Integrated discriminant improvement translates slope comparison to the incremental value of one predictive model over another. Net reclassification improvement (NCI) is an assessment of whether a new model changes the reclassification of patients from one model to a new model, demonstrating how many patients are shifted into different clinical risk categories (low, moderate, high) by using a different model. Relationships between biomarkers are readily assessed by commonly used relationship building models in biostatistics. Bayes analyses and machine learning offer real promise in this field. Finally, measure theory and big data concepts offer a firm foundation for future examination of the relationships biomarkers and both disease and clinical sequela.}
}
@article{KILINTZIS2019103179,
title = {Supporting integrated care with a flexible data management framework built upon Linked Data, HL7 FHIR and ontologies},
journal = {Journal of Biomedical Informatics},
volume = {94},
pages = {103179},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103179},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419300978},
author = {Vassilis Kilintzis and Ioanna Chouvarda and Nikolaos Beredimas and Pantelis Natsiavas and Nicos Maglaveras},
keywords = {Linked Data, Ontology, Telemonitoring, HL7 FHIR, OWL, Web services, Integrated care},
abstract = {In this paper we present the methodology and decisions behind an implementation of a telehealth data management framework, aiming to support integrated care services for chronic and multimorbid patients. The framework leverages an OWL ontology, built upon HL7 FHIR resources, to provide storage and representation of semantically enriched EHR data following Linked Data principles. This is presented along with the realization of the persistent storage solution and communication web services that allow the management of EHR data, ensuring the validity and integrity of the exchanged patient data as self-describing ontology instances. The framework concentrates on flexibility and reusability, which is addressed by regarding the aforementioned ontology as a single point of change. This solution has been implemented in the scope of the EU project WELCOME for managing data in a telemonitoring system for patients with COPD and co-morbidities and was also successfully deployed for the INLIFE EU project with minimal effort. The results of the two applications suggest it can be adopted and properly adapted in a series of integrated care scenarios with minimal effort.}
}
@article{HOWELL2019e02313,
title = {New Jersey's waste management data: retrospect and prospect},
journal = {Heliyon},
volume = {5},
number = {8},
pages = {e02313},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02313},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019359730},
author = {Jordan P. Howell and Katherine Schmidt and Brooke Iacone and Giavanni Rizzo and Christina Parrilla},
keywords = {Environmental science, Environmental assessment, Environmental management, Environmental economics, Operations management, Research and development, Political science, Human geography, Social responsibility, Conservation, Waste, Recycling, Environmental data, Solid waste management, New Jersey},
abstract = {Reliable data about collection, volume, tonnage, stream composition, and disposal price have long been described as key components of successful solid waste management planning. Yet, concerns about data quality and quantity have continued to limit even the most sincere, progressive waste management schemes. This paper examines solid waste management data that has been collected in the US state of New Jersey starting in the 1960s. We present the origins of waste management data collection in New Jersey and trace some of the applications that have been made with the data over time. We compare the New Jersey dataset to waste management data that has been collected in other US states. We then describe our work collecting, cleaning, and preparing for public dissemination and use in a geospatial visualization exercise a digital version of the data spanning approximately 1993 to 2016, before presenting some illustrations of the type of modeling and analysis that researchers or the concerned public would be able to undertake now that the dataset is available. (We are publishing the 1993–2016 dataset alongside this paper). We argue that the New Jersey waste management dataset is much better than most other waste datasets in the US, but despite this fairly high quality, there remain significant gaps which inhibit the ability of planners to design and implement comprehensive waste management plans. That there are limits inherent to the New Jersey dataset suggests, we argue, a ceiling to the usefulness of waste management data as a category of environmental knowledge with possible implications for ‘big’ environmental data more broadly.}
}
@article{HARDISTY201922,
title = {The Bari Manifesto: An interoperability framework for essential biodiversity variables},
journal = {Ecological Informatics},
volume = {49},
pages = {22-31},
year = {2019},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1574954118301961},
author = {Alex R. Hardisty and William K. Michener and Donat Agosti and Enrique {Alonso García} and Lucy Bastin and Lee Belbin and Anne Bowser and Pier Luigi Buttigieg and Dora A.L. Canhos and Willi Egloff and Renato {De Giovanni} and Rui Figueira and Quentin Groom and Robert P. Guralnick and Donald Hobern and Wim Hugo and Dimitris Koureas and Liqiang Ji and Wouter Los and Jeffrey Manuel and David Manset and Jorrit Poelen and Hannu Saarenmaa and Dmitry Schigel and Paul F. Uhlir and W. Daniel Kissling},
keywords = {Essential biodiversity variables, Cyberinfrastructure, E-infrastructure, Data products, Informatics, Interoperability},
abstract = {Essential Biodiversity Variables (EBV) are fundamental variables that can be used for assessing biodiversity change over time, for determining adherence to biodiversity policy, for monitoring progress towards sustainable development goals, and for tracking biodiversity responses to disturbances and management interventions. Data from observations or models that provide measured or estimated EBV values, which we refer to as EBV data products, can help to capture the above processes and trends and can serve as a coherent framework for documenting trends in biodiversity. Using primary biodiversity records and other raw data as sources to produce EBV data products depends on cooperation and interoperability among multiple stakeholders, including those collecting and mobilising data for EBVs and those producing, publishing and preserving EBV data products. Here, we encapsulate ten principles for the current best practice in EBV-focused biodiversity informatics as ‘The Bari Manifesto’, serving as implementation guidelines for data and research infrastructure providers to support the emerging EBV operational framework based on trans-national and cross-infrastructure scientific workflows. The principles provide guidance on how to contribute towards the production of EBV data products that are globally oriented, while remaining appropriate to the producer's own mission, vision and goals. These ten principles cover: data management planning; data structure; metadata; services; data quality; workflows; provenance; ontologies/vocabularies; data preservation; and accessibility. For each principle, desired outcomes and goals have been formulated. Some specific actions related to fulfilling the Bari Manifesto principles are highlighted in the context of each of four groups of organizations contributing to enabling data interoperability - data standards bodies, research data infrastructures, the pertinent research communities, and funders. The Bari Manifesto provides a roadmap enabling support for routine generation of EBV data products, and increases the likelihood of success for a global EBV framework.}
}
@article{SONG2019338,
title = {A review of research on tourism demand forecasting: Launching the Annals of Tourism Research Curated Collection on tourism demand forecasting},
journal = {Annals of Tourism Research},
volume = {75},
pages = {338-362},
year = {2019},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0160738318301312},
author = {Haiyan Song and Richard T.R. Qiu and Jinah Park},
keywords = {Tourism demand, Time series, Econometric model, Forecast combination, Artificial intelligence model, Judgment forecasts},
abstract = {This study reviews 211 key papers published between 1968 and 2018, for a better understanding of how the methods of tourism demand forecasting have evolved over time. The key findings, drawn from comparisons of method-performance profiles over time, are that forecasting models have grown more diversified, that these models have been combined, and that the accuracy of forecasting has been improved. Given the complexity of determining tourism demand, there is no single method that performs well for all situations, and the evolution of forecasting methods is still ongoing. This article also launches the Annals of Tourism Research Curated Collection on tourism demand forecasting, which contains past and hot off the press work on the topic and will continue to grow as new articles on the topic appear in Annals.}
}
@article{CARVALHO2019278,
title = {A health data analytics maturity model for hospitals information systems},
journal = {International Journal of Information Management},
volume = {46},
pages = {278-285},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830464X},
author = {João Vidal Carvalho and Álvaro Rocha and José Vasconcelos and António Abreu},
keywords = {Data analysis, Analytics, Maturity models, Hospital information systems},
abstract = {In the last five decades, maturity models have been introduced as reference frameworks for Information System (IS) management in organizations within different industries. In the healthcare domain, maturity models have also been used to address a wide variety of challenges and the high demand for hospital IS (HIS) implementations. The increasing volume of data, is exceeded the ability of health organizations to process it for improving clinical and financial efficiencies and quality of care. It is believed that careful and attentive use of Data Analytics in healthcare can transform data into knowledge that can improve patient outcomes and operational efficiency. A maturity model in this conjuncture, is a way of identifying strengths and weaknesses of the HIS maturity and thus, find a way for improvement and evolution. This paper presents a proposal to measure Hospitals Information Systems maturity with regard to Data Analytics. The outcome of this paper is a maturity model, which includes six stages of HIS growth and maturity progression.}
}
@article{BRENNAN2019100860,
title = {Corporate governance implications of disruptive technology: An overview},
journal = {The British Accounting Review},
volume = {51},
number = {6},
pages = {100860},
year = {2019},
note = {Innovative Governance and Sustainable Pathways in a Disruptive Environment},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2019.100860},
url = {https://www.sciencedirect.com/science/article/pii/S089083891930085X},
author = {Niamh M. Brennan and Nava Subramaniam and Chris J. {van Staden}},
keywords = {Corporate governance, Disruptive technologies, Innovation},
abstract = {This position paper introduces the special issue on “Innovative Governance and Sustainable Pathways in a Disruptive Environment”. The paper develops a framework to review the state of the art in disruptive technology and innovations (DTIs). Then the paper reviews the common characteristics of DTIs, and their implications for the principles and design of corporate governance and accounting mechanisms at the organisational level. Following on from that, the paper identifies the defining features of emergent DTI-related structural models that shape the demand for and changes to corporate governance and accounting mechanisms. The contributions of the three papers in the special issue are discussed. The paper concludes by proposing several research themes for future research on designing more innovative and sustainable governance systems, drawing on multidisciplinary theoretical and methodological perspectives. This complements calls for future research in accounting in our special-issue papers.}
}
@article{GU201953,
title = {A novel approach to intrusion detection using SVM ensemble with feature augmentation},
journal = {Computers & Security},
volume = {86},
pages = {53-62},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167404819301154},
author = {Jie Gu and Lihong Wang and Huiwen Wang and Shanshan Wang},
keywords = {Ensemble learning, Intrusion detection, Marginal density ratios transformation, Network security, Support vector machine},
abstract = {Network security has been a very important problem. Intrusion detection systems have been widely used to protect network security. Various machine learning techniques have been applied to improve the performance of intrusion detection systems, among which ensemble learning has received a growing interest and is considered as an effective method. Besides, the quality of training data is also an essential determinant that can greatly enhance the detection capability. Knowing that the marginal density ratios are the most powerful univariate classifiers. In this paper, we propose an effective intrusion detection framework based on SVM ensemble with feature augmentation. Specifically, the logarithm marginal density ratios transformation is implemented on the original features with the goal of obtaining new and better-quality transformed training data; SVM ensemble was then used to build the intrusion detection model. Experiment results show that our proposed method can achieve a good and robust performance, which possesses huge competitive advantages when compared to other existing methods in terms of accuracy, detection rate, false alarm rate and training speed.}
}
@article{KESERU2019485,
title = {Citizen observatory for mobility: a conceptual framework},
journal = {Transport Reviews},
volume = {39},
number = {4},
pages = {485-510},
year = {2019},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2018.1536089},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722001386},
author = {Imre Keseru and Nils Wuytens and Cathy Macharis},
keywords = {participatory sensing, citizen observatory, mobility surveys, smartphones, citizen science, citizen participation},
abstract = {ABSTRACT
Citizen observatories that incorporate participatory sensing can complement traditional and automated data collection methods for mobility planning and increase the level of participation of citizens in transport planning. The process of developing such an online environment is not only time-consuming and costly, but it would also require an extensive knowledge of computer programming. This is one of the main barriers to the proliferation of citizen observatories. Therefore, this paper develops a conceptual framework of a citizen observatory platform that does not require special skills or resources. It would enable the collection, analysis and exchange of quantitative and qualitative mobility-related data by citizens. We have reviewed 69 participatory sensing applications in the field of mobility to derive the essential building stones of such an observatory. We identified the requirements considering eight criteria: campaign management, objective, context, data types, sensing technology, motivation of data collectors, validation and representativeness, visualisation and reporting. Some concerns regarding representativeness of data, motivation of data collectors, accuracy of sensors and validated algorithms for indicators are also raised.}
}
@article{CHIANG2019856,
title = {Taiwan's Nationwide Cancer Registry System of 40 years: Past, present, and future},
journal = {Journal of the Formosan Medical Association},
volume = {118},
number = {5},
pages = {856-858},
year = {2019},
issn = {0929-6646},
doi = {https://doi.org/10.1016/j.jfma.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0929664618307629},
author = {Chun-Ju Chiang and Ying-Wei Wang and Wen-Chung Lee}
}
@article{PINTO2019709,
title = {Robot fault detection and remaining life estimation for predictive maintenance},
journal = {Procedia Computer Science},
volume = {151},
pages = {709-716},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.094},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305563},
author = {Riccardo Pinto and Tania Cerquitelli},
keywords = {Industry 4.0, machine learning, data analytics, robot fault forecasting},
abstract = {In this work some possible solutions to implement a Robotics-oriented predictive maintenance approach are discussed. The data-driven methodology is described from the data collection to the design of an appropriate dataset and finally to the use of some of the most promising algorithms in the field of machine learning. The whole process is composed by several building blocks that can be combined to realize a data analysis on industrial robots. Some of the most promising techniques in Predictive Maintenance for Industrial machines were included in the proposed methodology, together with a Survival Analysis study, and then evaluated with proper performance metrics. Experimenting this methodology on a real use-case with Comau industrial robots showed the validity of the approach and opened to the inclusion of such a process in a service-oriented solution.}
}
@article{CHANG2019559,
title = {Classification of machine learning frameworks for data-driven thermal fluid models},
journal = {International Journal of Thermal Sciences},
volume = {135},
pages = {559-579},
year = {2019},
issn = {1290-0729},
doi = {https://doi.org/10.1016/j.ijthermalsci.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1290072917317672},
author = {Chih-Wei Chang and Nam T. Dinh},
keywords = {Thermal fluid simulation, Closure relations, Multiscale modeling, Machine learning framework, Deep learning, Data driven, Convolutional neural networks, Classification},
abstract = {Thermal fluid processes are inherently multi-physics and multi-scale, involving mass-momentum-energy transport phenomena at multiple scales. Thermal fluid simulation (TFS) is based on solving conservative equations, for which – except for “first-principles” direct numerical simulation – closure relations (CRs) are required to provide microscopic interactions or so-called sub-grid-scale physics. In practice, TFS is realized through reduced-order modeling, and its CRs as low-fidelity models can be informed by observations and data from relevant and adequately evaluated experiments and high-fidelity simulations. This paper is focused on data-driven TFS models, specifically on their development using machine learning (ML). Five ML frameworks are introduced including physics-separated ML (PSML or Type I ML), physics-evaluated ML (PEML or Type II ML), physics-integrated ML (PIML or Type III ML), physics-recovered (PRML or Type IV ML), and physics-discovered ML (PDML or Type V ML). The frameworks vary in their performance for different applications depending on the level of knowledge of governing physics, source, type, amount and quality of available data for training. Notably, outlined for the first time in this paper, Type III models present stringent requirements on modeling, substantial computing resources for training, and high potential in extracting value from “big data” in thermal fluid research. The current paper demonstrates and investigates ML frameworks in three examples. First, we utilize the heat diffusion equation with a nonlinear conductivity model formulated by convolutional neural networks (CNNs) and feedforward neural networks (FNNs) to illustrate the applications of Type I, Type II, Type III, and Type V ML. The results indicate a preference for Type II ML under deficient data support. Type III ML can effectively utilize field data, potentially generating more robust predictions than Type I and Type II ML. CNN-based closures exhibit more predictability than FNN-based closures, but CNN-based closures require more training data to obtain accurate predictions. Second, we illustrate how to employ Type I ML and Type II ML frameworks for data-driven turbulence modeling using reference works. Third, we demonstrate Type I ML by building a deep FNN-based slip closure for two-phase flow modeling. The results show that deep FNN-based closures exhibit a bounded error in the prediction domain.}
}
@article{LIN201988,
title = {Adaptive security-related data collection with context awareness},
journal = {Journal of Network and Computer Applications},
volume = {126},
pages = {88-103},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303539},
author = {Huaqing Lin and Zheng Yan and Yulong Fu},
keywords = {Security-related data, Adaptive data collection, Heterogeneous network, Network context},
abstract = {The huge economic loss resulting from network attacks and intrusions has led to an intensive study on network security. The network security is usually reflected by some relevant data that can be collected in a network system. By learning and analyzing such data, which are called security-related data, we can detect the intrusions to the network system and further measure its security level. Clearly, the first step of detecting network intrusions is to collect security-related data. However, in the context of 5G and big data, there are a number of challenges in collecting these data due to the heterogeneity of network and ever-growing amount of data. Therefore, traditional data collection methods cannot be applied in the next generation network systems directly, especially for security-related data. This paper presents the design and implementation of an adaptive security-related data collector based on network context in heterogeneous networks. The proposed collector solves the issue of heterogeneity of network system by designing a Security-related Data Description Language (SDDL) to instruct security related data collection in various networking contexts. It also applies adaptive sampling algorithms to reduce the amount of collected data. Furthermore, performance evaluation based on a prototype implementation shows the effectiveness of the adaptive security-related data collector in terms of a number of pre-defined design requirements.}
}
@article{AVANESOVA2019179,
title = {Worldwide implementation of telemedicine programs in association with research performance and health policy},
journal = {Health Policy and Technology},
volume = {8},
number = {2},
pages = {179-191},
year = {2019},
issn = {2211-8837},
doi = {https://doi.org/10.1016/j.hlpt.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211883718302636},
author = {Anna A Avanesova and Tatyana A. Shamliyan},
keywords = {Telemedicine, Information and communication technologies, Health care policy, Legislation, Research performance, International collaboration},
abstract = {Objectives
We analyzed research performance, international collaboration, corporate contribution, country level economic factors, and legislative frameworks in association with worldwide implementation of telemedicine programs.
Methods
We identified telemedicine scholarly output in EMBASE and SCOPUS and SciVal and combine with Third Global Survey on eHealth available in the World Health Organization Global Observatory for eHealth.
Results
From 71245 telemedicine-related publications only 0.8% addressed a policy of tele-healthcare delivery. Scholarly output was positively associated with implementation of telemedicine, with legal framework supporting utilization of telemedicine and with country income and total health expenditure but not total population, physician, nurses or hospital bed density or life expectance. National eHealth policy, capacity building (training of medical students and staff), legislative regulations of electronic health records, and supply chain management information systems supported by information and communication technologies were associated with implementation of all examined telemedicine programs. 43 telecommunication technology or medical devices manufacturing companies contributed to more than 90% of scholarly output and implementation of telemedicine.
Conclusions
Research performance, training of medical students and healthcare professionals and collaboration with technology industry demonstrated the strongest association with implementation of telemedicine. Future efforts should be directed toward consistent collection and routine analysis of patient outcomes after telemedicine interventions. International legislation is needed for telemedicine capacity building, reimbursement policy, and secure data sharing policies.}
}
@article{SILVA20191219,
title = {Preservation of confidential information privacy and association rule hiding for data mining: a bibliometric review},
journal = {Procedia Computer Science},
volume = {151},
pages = {1219-1224},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.175},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919306428},
author = {Jesus Silva and Jenny Cubillos and Jesus Vargas Villa and Ligia Romero and Darwin Solano and Claudia Fernández},
keywords = {confidential information privacy preservation, approaches to hiding of association rules of data, bibliometric analysis, SCOPUS},
abstract = {In this era of technology, data of business organizations are growing with acceleration. Mining hidden patterns from this huge database would benefit many industries improving their decision-making processes. Along with the non-sensitive information, these databases also contain some sensitive information about customers. During the mining process, sensitive information about a person can get leaked, resulting in a misuse of the data and causing loss to an individual. The privacy preserving data mining can bring a solution to this problem, helping provide the benefits of mined data along with maintaining the privacy of the sensitive information. Hence, there is a growing interest in the scientific community for developing new approaches to hide the mined sensitive information. In this research, a bibliometric review is carried out during the period 2010 to 2018 to analyze the growth of studies regarding the confidential information privacy preservation through approaches addressed to the hiding of association rules of data.}
}
@article{KIM2019354,
title = {The Evolving Use of Electronic Health Records (EHR) for Research},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {354-361},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300426},
author = {Ellen Kim and Samuel M. Rubinstein and Kevin T. Nead and Andrzej P. Wojcieszynski and Peter E. Gabriel and Jeremy L. Warner},
abstract = {Electronic health records (EHR) have been implemented successfully in a majority of United States healthcare systems in some form. There has been a rise in secondary uses of EHR, especially for research. EHR data is large, heterogenous, incomplete, noisy, and primarily created for purposes other than research. This presents many challenges, many of which are beginning to be overcome with the application of computer science artificial intelligence techniques, such as natural language processing and machine learning. EHR are gradually being redesigned to facilitate future research, though we are still far from a “complete EHR.”}
}
@article{LI2019107,
title = {Using fine-tuned conditional probabilities for data transformation of nominal attributes},
journal = {Pattern Recognition Letters},
volume = {128},
pages = {107-114},
year = {2019},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167865519302399},
author = {Qiude Li and Qingyu Xiong and Shengfen Ji and Junhao Wen and Min Gao and Yang Yu and Rui Xu},
keywords = {Conditional probability transformation, Fine-tuning algorithm, MIC-based feature selection, Data transformation, Distance measure},
abstract = {Most of existing machine learning algorithms do not natively support nominal attributes, so it is essential to develop the data transformation of nominal attributes into high-quality numeric ones. Conditional Probability Transformation (CPT), using conditional probability terms to transform categories in nominal attributes, is competitive with state-of-the-art transformation methods such as One-Hot Encoding (OHE) and Separability Split Value Transformation (SSVT). However, it may be difficult to accurately estimate conditional probability terms when training data is insufficient or there exist strong dependencies among its attributes. Inspired by the fine-tuning method for improving conditional probability terms in distance measures, we proposed a Fine-Tuned Conditional Probability Transformation (FTCPT). In addition, we proposed an Improved SSV (ISSV) based on fine-tuned conditional probability terms, and used our Modified MIC-based Feature Selection method to further improve the performance of FTCPT. Experiment results show that the proposed methods can improve the quality of data transformation, thereby contribute to improving the classification performance of subsequent machine learning algorithm.}
}
@article{JUSTO2019739,
title = {Real-World Evidence in Healthcare Decision Making: Global Trends and Case Studies From Latin America},
journal = {Value in Health},
volume = {22},
number = {6},
pages = {739-749},
year = {2019},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2019.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S1098301519300774},
author = {Nahila Justo and Manuel A. Espinoza and Barbara Ratto and Martha Nicholson and Diego Rosselli and Olga Ovcinnikova and Sebastián {García Martí} and Marcos B. Ferraz and Martín Langsam and Michael F. Drummond},
keywords = {big data, epidemiology, health policy, health technology assessment, Latin America, real-world evidence},
abstract = {Background
Real-world evidence (RWE) is increasingly used to inform health technology assessments for resource allocation, which are valuable tools for emerging economies such as in America. Nevertheless, the characteristics and uses in South America are unknown.
Objectives
To identify sources, characteristics, and uses of RWE in Argentina, Brazil, Colombia, and Chile, and evaluate the context-specific challenges. The implications for future regulation and responsible management of RWE in the region are also considered.
Methods
A systematic literature review, database mapping, and targeted gray literature search were conducted to identify the sources and characteristics of RWE. Findings were validated by key opinion leaders attending workshops in 4 South American countries.
Results
A database mapping exercise revealed 407 unique databases. Geographic scope, database type, population, and outcomes captured were reported. Characteristics of national health information systems show efforts to collect interoperable data from service providers, insurers, and government agencies, but that initiatives are hampered by fragmentation, lack of stewardship, and resources. In South America, RWE is mainly used for pharmacovigilance and as pure academic research, but less so for health technology assessment decision making or pricing negotiations and not at all to inform early access schemes.
Conclusions
The quality of real-world data in the case study countries vary and RWE is not consistently used in healthcare decision making. Authors recommend that future studies monitor the impact of digitalization and the potential effects of access to RWE on the quality of patient care.}
}
@article{WEN2019178,
title = {A deep learning framework for road marking extraction, classification and completion from mobile laser scanning point clouds},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {147},
pages = {178-192},
year = {2019},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0924271618302855},
author = {Chenglu Wen and Xiaotian Sun and Jonathan Li and Cheng Wang and Yan Guo and Ayman Habib},
keywords = {Point cloud, Road marking, Extraction, Classification, Completion, Deep learning},
abstract = {Road markings play a critical role in road traffic safety and are one of the most important elements for guiding autonomous vehicles (AVs). High-Definition (HD) maps with accurate road marking information are very useful for many applications ranging from road maintenance, improving navigation, and prediction of upcoming road situations within AVs. This paper presents a deep learning-based framework for road marking extraction, classification and completion from three-dimensional (3D) mobile laser scanning (MLS) point clouds. Compared with existing road marking extraction methods, which are mostly based on intensity thresholds, our method is less sensitive to data quality. We added the step of road marking completion to further optimize the results. At the extraction stage, a modified U-net model was used to segment road marking pixels to overcome the intensity variation, low contrast and other issues. At the classification stage, a hierarchical classification method by integrating multi-scale clustering with Convolutional Neural Networks (CNN) was developed to classify different types of road markings with considerable differences. At the completion stage, a method based on a Generative Adversarial Network (GAN) was developed to complete small-size road markings first, then followed by completing broken lane lines and adding missing markings using a context-based method. In addition, we built a point cloud road marking dataset to train the deep network model and evaluate our method. The dataset contains urban road and highway MLS data and underground parking lot data acquired by our own assembled backpacked laser scanning system. Our experimental results obtained using the point clouds of different scenes demonstrated that our method is very promising for road marking extraction, classification and completion.}
}
@article{KURISON2019102976,
title = {Unlocking well productivity drivers in Eagle Ford and Utica unconventional resources through data analytics},
journal = {Journal of Natural Gas Science and Engineering},
volume = {71},
pages = {102976},
year = {2019},
issn = {1875-5100},
doi = {https://doi.org/10.1016/j.jngse.2019.102976},
url = {https://www.sciencedirect.com/science/article/pii/S1875510019302288},
author = {Clay Kurison and Huseyin Sadi Kuleli and Ahmed H. Mubarak},
keywords = {Unconventional resources, Statistical plays, Hydraulic fracturing stimulation, Well productivity, Data analytics},
abstract = {North America's unconventional resources are viewed as statistical plays because of the wide variability in the performance of hydraulically fractured wells. Attempts to unlock the primary drivers of productivity yield different interpretations because of the over-parameterized stimulation processes, diverse geoscience concepts and non-unique solutions of associated reservoir flow modeling. An opportunity to derive insights on the yet-to-be fully understood flow mechanisms lies in the interrogation of acquired stimulation and production data. In search of causation relationships, a data analytics approach was used to profile and associate well productivity with multiple hydraulic fracturing stimulation parameters for Eagle Ford and Utica horizontal wells in areas with similar geological settings. Data for the study was retrieved from public-accessible databases managed by the Rail Road Commission of Texas, Department of Natural Resources of Ohio and Chemical Disclosure Registry of FracFocus. Results from the approach show multiple apparent causal relationships between production and stimulation parameters with the latter evolving in tandem. As a result, a priori knowledge was incorporated in decoupling dependencies for sole contributions of parameters. Among the most critical trends, the amount of proppant and fracturing fluid volume correlated with productivity. The average proppant concentration, used to decouple this association, correlated poorly with cumulative hydrocarbon production. Better performing wells in which the amount of proppant was high coincidentally had been stimulated with large amounts of fracturing fluid; thus, motivating the latter to be interpreted as a primary driver for Eagle Ford and Utica production enhancement. It was speculated that the induced hydraulic fracture length is effective; thus, use of large amounts of high quality proppant to create conductive fractures might be yielding limited value. Additional results provided insights on interdependences between injected proppant, lateral length, fracturing stages, perforation clusters and fracturing fluid volume. The identification of stimulation parameters that correlated with well productivity provided ground for speculating that unconventional resources are not statistical as portrayed and repeatable well performance can be realized if stimulation operations are systematically constrained. Further reflection on the results backed postulations on attributes of reservoir flow physics such as matrix permeability, fracture conductivity and effective fracture length. In conclusion, data analytics shed light on dominant productivity drivers in the subject unconventional resources and provided ground for hypotheses that will improve knowledge-driven simulation models.}
}
@article{2019iii,
title = {Table of Contents},
journal = {Procedia Computer Science},
volume = {148},
pages = {iii-vii},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(19)30315-1},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919303151}
}
@article{JONGEN2019142,
title = {Observational designs in clinical multiple sclerosis research: Particulars, practices and potentialities},
journal = {Multiple Sclerosis and Related Disorders},
volume = {35},
pages = {142-149},
year = {2019},
issn = {2211-0348},
doi = {https://doi.org/10.1016/j.msard.2019.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2211034819303049},
author = {Peter Joseph Jongen},
keywords = {Multiple sclerosis, Observational, Effectiveness, Disease-modifying drug, Pharmacotherapy},
abstract = {Observational studies investigate a wide range of topics in multiple sclerosis research. This paper presents an overview of the various observational designs and their applications in clinical studies. Observational studies are well suited for making discoveries and assessing new explanations of phenomena, but less so for establishing causal relationships, due to confounding by indication (selection bias), co-morbidity, socio-economic or other factors. Whether observational findings are demonstrative, indicative or only suggestive, depends on the research question, whether and how the design fits this question, analytical techniques, and the quality of data. Observational studies may be cross-sectional vs. longitudinal, and prospective vs. retrospective. The term ‘retrograde’ is proposed to explicate that cross-sectional studies may obtain data that cover (long) preceding periods. Case reports and case series are usually based on accidental observations or routinely collected data. Cross-sectional studies, by simultaneously assessing clinical phenomena and external factors, enable the discovery and quantification of associations. In ecological studies the unit of analysis is population or group, and relationships on patient level cannot be established. A cohort study is a longitudinal study that investigates patients with a defining characteristic, e.g. diagnosis or specific treatment, by analyzing data acquired at various intervals. Prospective cohort studies use (some) data that are not yet available at the time the research is conceived, whereas in retrospective studies the data already exist. In a case-control study a representative group of patients with a specific clinical feature is compared with controls, and the frequencies at which an external factor, e.g. infection, has occurred in each group is compared; in a nested case-control study controls are drawn from a fully known cohort. Randomized controlled trial (RCT)-extension studies are informative because, due to RCT randomization, they are free from confounding by indication. Patient or disease registries are organised systems for the long-term collection of uniform data on a population that is defined by a particular disease, condition or exposure, with the purpose to study changes over time. In pharmacotherapeutic research, accidental observations of unexpected beneficial effects may lead to further research into a drug's efficacy in other conditions. Uncontrolled phase 1 studies investigate safety and dosing aspects. Observational studies are alternatives to RCTs when these are not feasible for ethical or practical reasons. Phase 4 observational studies play a crucial role in the evaluation of the effectiveness of treatments in daily practice, the validation of RCT-based side effect profiles, and the discovery of late occurring or rare, potentially life-threatening side effects. Combinations of multidisciplinary longitudinal data bases into large data sets enable the development of algorithms for personalized treatments. To improve the reporting of observational findings on treatment effectiveness, it is proposed that abstracts define the research question(s) the study was meant to answer, study design and analytical methods, and identify and quantify the patient population, treatment of interest, relevant outcomes and the study's strengths and limitations. The development of guidelines for Strengthening the Reporting of Observational Studies in Effectiveness Research (STROBER), as an extension of the guidelines used in epidemiology, is wanted.}
}
@article{TOIVONEN2019298,
title = {Social media data for conservation science: A methodological overview},
journal = {Biological Conservation},
volume = {233},
pages = {298-315},
year = {2019},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0006320718317609},
author = {Tuuli Toivonen and Vuokko Heikinheimo and Christoph Fink and Anna Hausmann and Tuomo Hiippala and Olle Järv and Henrikki Tenkanen and Enrico {Di Minin}},
keywords = {Social media, Nature conservation, Biodiversity, Spatial analysis, Content analysis, Machine learning, Artificial intelligence},
abstract = {Improved understanding of human-nature interactions is crucial to conservation science and practice, but collecting relevant data remains challenging. Recently, social media have become an increasingly important source of information on human-nature interactions. However, the use of advanced methods for analysing social media is still limited, and social media data are not used to their full potential. In this article, we present available sources of social media data and approaches to mining and analysing these data for conservation science. Specifically, we (i) describe what kind of relevant information can be retrieved from social media platforms, (ii) provide a detailed overview of advanced methods for spatio-temporal, content and network analyses, (iii) exemplify the potential of these approaches for real-world conservation challenges, and (iv) discuss the limitations of social media data analysis in conservation science. Combined with other data sources and carefully considering the biases and ethical issues, social media data can provide a complementary and cost-efficient information source for addressing the grand challenges of biodiversity conservation in the Anthropocene epoch.}
}
@article{GIROUX201957,
title = {A high-frequency mobile phone data collection approach for research in social-environmental systems: Applications in climate variability and food security in sub-Saharan Africa},
journal = {Environmental Modelling & Software},
volume = {119},
pages = {57-69},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218303207},
author = {Stacey A. Giroux and Inna Kouper and Lyndon D. Estes and Jacob Schumacher and Kurt Waldman and Joel T. Greenshields and Stephanie L. Dickinson and Kelly K. Caylor and Tom P. Evans},
keywords = {High frequency data, Farming, Food security, Short Message Service (SMS), Sub-Saharan Africa},
abstract = {Collecting high-frequency social-environmental data about farming practices in sub-Saharan Africa can provide new insight into environmental changes that farmers face and how they respond within smallholder agro-ecosystems. Traditional data collection methods such as agricultural censuses are costly and not useful for understanding intra-annual and real-time decisions. Short-message service (SMS) has the potential to transform the nature of data collection in coupled social-ecological systems. We present a system for collecting, managing, and synthesizing weekly data from farmers, including data infrastructure for management of big and heterogeneous datasets; probabilistic data quality assessment tools; and visualization and analysis tools such as mapping and regression techniques. We discuss limitations of collecting social-environmental data via SMS and data integration challenges that arise when linking these data with other social and environmental data. In combination with high-frequency environmental data, such data will help ameliorate issues of scale mismatch and build resilience in environmental systems.}
}
@article{CUI20191176,
title = {Assessing the effectiveness of artificial intelligence methods for melanoma: A retrospective review},
journal = {Journal of the American Academy of Dermatology},
volume = {81},
number = {5},
pages = {1176-1180},
year = {2019},
issn = {0190-9622},
doi = {https://doi.org/10.1016/j.jaad.2019.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S0190962219310412},
author = {Xiaoyu Cui and Ran Wei and Lixin Gong and Ruiqun Qi and Zeyin Zhao and Hongduo Chen and Kaixin Song and Amer A.A. Abdulrahman and Yining Wang and John Z.S. Chen and Shuo Chen and Yue Zhao and Xinghua Gao},
keywords = {artificial intelligence, classification, deep learning, melanoma diagnosis, segmentation, traditional machine learning},
abstract = {Background
Artificial intelligence methods for the classification of melanoma have been studied extensively. However, few studies compare these methods under the same standards.
Objective
To seek the best artificial intelligence method for diagnosis of melanoma.
Methods
The contrast test used 2200 dermoscopic images. Image segmentations, feature extractions, and classifications were performed in sequence for evaluation of traditional machine learning algorithms. The recent popular convolutional neural network frameworks were used for transfer learning training classification.
Results
The region growing algorithm has the best segmentation performance, with an intersection over union of 70.06% and a false-positive rate of 17.67%. Classification performance was better with logistic regression, with a sensitivity of 76.36% and a specificity of 87.04%. The Inception V3 model (Google, Mountain View, CA) worked best in deep learning algorithms: the accuracy was 93.74%, the sensitivity was 94.36%, and the specificity was 85.64%.
Limitations
There was no division in the severity of melanoma samples used in this experiment. The data set was relatively small for deep learning.
Conclusion
The performance of traditional machine learning is satisfactory for the small data set of melanoma dermoscopic images, and the potential for deep learning in the future big data era is enormous.}
}