@article{CEGUERRA2013224,
title = {The rise of computational techniques in atom probe microscopy},
journal = {Current Opinion in Solid State and Materials Science},
volume = {17},
number = {5},
pages = {224-235},
year = {2013},
note = {Atom Probe Tomography},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2013.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1359028613000776},
author = {Anna V. Ceguerra and Andrew J. Breen and Leigh T. Stephenson and Peter J. Felfer and Vicente J. Araullo-Peters and Peter V. Liddicoat and XiangYuan Cui and Lan Yao and Daniel Haley and Michael P. Moody and Baptiste Gault and Julie M. Cairney and Simon P. Ringer},
keywords = {Atom probe microscopy, Atom probe tomography, Materials informatics, Computational materials science},
abstract = {Much effort has been devoted to the development of computational techniques in atom probe microscopy over the past decade. There have been several drivers for this effort. Firstly, there has been effort devoted to addressing the challenges of discerning information from the increasingly large size of the data, and capturing the opportunities that this large data presents. Secondly, there has been significant new effort devoted to the simulation of atom probe data so that pristine datasets that contain microstructural features of increasing complexity can be generated in-silico, and subjected to complex data-mining algorithms. This has enabled the benchmarking of various algorithms, guided the setting of parameters for particular analyses, and exposed the effects of instrumentation parameters such as detector efficiency and aberrations in ionic flight path. The authors are especially interested in the prospects of converging atomic-scale microscopy with atomic-scale materials modelling via first principles approaches. This involves excising parts of the APM data and using these as super-cell inputs to calculations of materials properties via density functional theory. It is our opinion that this represents a major advance for materials science because it enables microscopy to advance microstructure–property relationships to the direct mapping of such relationships based on many-body interactions. As such, this approach has great potential for materials design and development. The final part of this paper focuses on how cloud-based computing represents an exciting frontier of the computational aspects of atom probe microscopy. We discuss the opportunities and the barriers for conducting new materials science through the analysis and visualisation of atom probe data via new generation tools that are cloud-based, and which are managed, curated and governed with significant user-community input and integrated with contemporary electronic laboratory notebook technology.}
}
@article{SHAPIRO2016216,
title = {Health Information Exchange in Emergency Medicine},
journal = {Annals of Emergency Medicine},
volume = {67},
number = {2},
pages = {216-226},
year = {2016},
issn = {0196-0644},
doi = {https://doi.org/10.1016/j.annemergmed.2015.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0196064415005247},
author = {Jason S. Shapiro and Diana Crowley and Shkelzen Hoxhaj and James Langabeer and Brian Panik and Todd B. Taylor and Arlo Weltge and Jeffrey A. Nielson},
abstract = {Emergency physicians often must make critical, time-sensitive decisions with a paucity of information with the realization that additional unavailable health information may exist. Health information exchange enables clinician access to patient health information from multiple sources across the spectrum of care. This can provide a more complete longitudinal record, which more accurately reflects the way most patients obtain care: across multiple providers and provider organizations. This information article explores various aspects of health information exchange that are relevant to emergency medicine and offers guidance to emergency physicians and to organized medicine for the use and promotion of this emerging technology. This article makes 5 primary emergency medicine–focused recommendations, as well as 7 additional secondary generalized recommendations, to health information exchanges, policymakers, and professional groups, which are crafted to facilitate health information exchange's purpose and demonstrate its value.}
}
@article{WANG2015198,
title = {Robust methods for assessing the accuracy of linear interpolated DEM},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {34},
pages = {198-206},
year = {2015},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2014.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0303243414001767},
author = {Bin Wang and Wenzhong Shi and Eryong Liu},
keywords = {DEM accuracy, Interpolation residuals, Robust estimation, Confidence interval, Monte Carlo simulation},
abstract = {Methods for assessing the accuracy of a digital elevation model (DEM) with emphasis on robust methods have been studied in this paper. Based on the squared DEM residual population generated by the bi-linear interpolation method, three average-error statistics including (a) mean, (b) median, and (c) M-estimator are thoroughly investigated for measuring the interpolated DEM accuracy. Correspondingly, their confidence intervals are also constructed for each average error statistic to further evaluate the DEM quality. The first method mainly utilizes the student distribution while the second and third are derived from the robust theories. These innovative robust methods possess the capability of counteracting the outlier effects or even the skew distributed residuals in DEM accuracy assessment. Experimental studies using Monte Carlo simulation have commendably investigated the asymptotic convergence behavior of confidence intervals constructed by these three methods with the increase of sample size. It is demonstrated that the robust methods can produce more reliable DEM accuracy assessment results compared with those by the classical t-distribution-based method. Consequently, these proposed robust methods are strongly recommended for assessing DEM accuracy, particularly for those cases where the DEM residual population is evidently non-normal or heavily contaminated with outliers.}
}
@article{VICIU2013161,
title = {Ongoing Economic Restructuring in the Wake of the Latest Economic Crisis: A Russian Perspective},
journal = {Procedia Economics and Finance},
volume = {6},
pages = {161-168},
year = {2013},
note = {International Economic Conference of Sibiu 2013 Post Crisis Economy: Challenges and Opportunities, IECS 2013},
issn = {2212-5671},
doi = {https://doi.org/10.1016/S2212-5671(13)00128-7},
url = {https://www.sciencedirect.com/science/article/pii/S2212567113001287},
author = {Tania Georgia Viciu and Mihaela Toma and Diana Larisa Ţâmpu},
keywords = {economic restructuring, economic crisis, tertiarization, developing economies, IT market.},
abstract = {The latest economic crisis has brought into focus long-standing discussions regarding the role played by services in contemporary economies. Whether viewed as a support for the well-established manufacturing sector, or growth engine in its own right, the third economic sector has become a focus point for policy makers worldwide. The paper follows the effects of the crisis on the tertiarization process, with an emphasis on developing economies, where this phenomenon is barely coming of age. The research is centered on the IT market, as this is seen as a comprehensive measure of the maturity and sophistication of the services sector.}
}
@incollection{PATRINOS2010379,
title = {Chapter 25 - Locus-Specific and National/Ethnic Mutation Databases: Emerging Tools for Molecular Diagnostics},
editor = {George P. Patrinos and Wilhelm J. Ansorge},
booktitle = {Molecular Diagnostics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {379-391},
year = {2010},
isbn = {978-0-12-374537-8},
doi = {https://doi.org/10.1016/B978-0-12-374537-8.00025-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123745378000250},
author = {George P. Patrinos},
abstract = {Publisher Summary
It rapidly has become clear that the knowledge and organization of alterations in structured repositories are of great importance not only for diagnosis but also for clinicians and researchers. The main applications of mutation databases are to facilitate diagnosis at the DNA level and to define an optimal strategy for mutation detection, to provide information about mutation-specific phenotypic patterns, and to correlate locus-specific variant information with genome-wide features, such as repetitive elements, gene structures, interspecies conservation, mutation hotspots, and recombination frequencies. This chapter describes the existing and emerging mutating database types and emphasizes their potential applications in modern medical genetics. The various depositories that fall under the banner of “mutation databases'’ are categorized into three types: general mutation databases (GMDs), locus-specific databases (LSDBs), and national/ethnic mutation databases (NEMDBs). LSDBs and NEMDBs are increasingly becoming valuable tools in molecular diagnostics. The key elements that are missing and holding back the field are discussed. The current array of GDBs, LSDBs, and NEMDBs are limited in number and their degree of interconnection to capture all that is known and being discovered regarding pathogenic DNA mutations. The main reason for this deficiency is that the modern research ethos fails to provide adequate incentives (i.e., publication options, peer recognition, funding) to encourage researchers to build new databases.}
}
@article{ZHANG2016175,
title = {Technology roadmapping for competitive technical intelligence},
journal = {Technological Forecasting and Social Change},
volume = {110},
pages = {175-186},
year = {2016},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0040162515003923},
author = {Yi Zhang and Douglas K.R. Robinson and Alan L. Porter and Donghua Zhu and Guangquan Zhang and Jie Lu},
keywords = {Technology roadmapping, Competitive technical intelligence, Text mining, Tech mining},
abstract = {Understanding the evolution and emergence of technology domains remains a challenge, particularly so for potentially breakthrough technologies. Though it is well recognized that emergence of new fields is complex and uncertain, to make decisions amidst such uncertainty, one needs to mobilize various sources of intelligence to identify known–knowns and known–unknowns to be able to choose appropriate strategies and policies. This competitive technical intelligence cannot rely on simple trend analyses because breakthrough technologies have little past to inform such trends, and positing the directions of evolution is challenging. Neither do qualitative tools, embracing the complexities, provide all the solutions, since transparent and repeatable techniques need to be employed to create best practices and evaluate the intelligence that comes from such exercises. In this paper, we present a hybrid roadmapping technique that draws on a number of approaches and integrates them into a multi-level approach (individual activities, industry evolutions and broader global changes) that can be applied to breakthrough technologies. We describe this approach in deeper detail through a case study on dye-sensitized solar cells. Our contribution to this special issue is to showcase the technique as part of a family of approaches that are emerging around the world to inform strategy and policy.}
}
@article{CLIFFORD2016272,
title = {Online dispute resolution: Settling data protection disputes in a digital world of customers},
journal = {Computer Law & Security Review},
volume = {32},
number = {2},
pages = {272-285},
year = {2016},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S026736491500179X},
author = {Damian Clifford and Yung Shin {Van Der Sype}},
keywords = {Online dispute resolution, Alternative dispute resolution, Data protection, Enforcement, European Union},
abstract = {In this article online dispute resolution (ODR) and alternative dispute resolution (ADR) are assessed in relation to the protection of personal data. ODR and ADR schemes are mechanisms to settle low-cost e-commerce disputes out-of-court. The purpose of this analysis is to examine the suitability of online dispute resolution as an additional means to the existing mechanisms for data protection enforcement. In this discussion particular attention is given to services offered to users as ‘free’, but which instead process personal data as a condition on access (e.g. social networking sites). The second section examines data protection in the digital age, highlighting the key principles of data protection and the challenges associated with the existing enforcement mechanisms. The third section questions the suitability of online dispute resolution as a solution for data protection enforcement in the European Union. In order to avail of the EU regulated ODR mechanism to resolve data protection issues, data protection disputes must fall under the scope of the Alternative Dispute Resolution Directive and the Online Dispute Resolution Regulation. Following an analysis of the applicability of the framework in this context, the final part of this article focuses on the challenges associated with the application of ODR schemes to the enforcement of online data protection disputes.}
}
@incollection{VANDERLANS201227,
title = {Chapter 2 - Business Intelligence and Data Warehousing},
editor = {Rick F. {van der Lans}},
booktitle = {Data Virtualization for Business Intelligence Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {27-57},
year = {2012},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-394425-2},
doi = {https://doi.org/10.1016/B978-0-12-394425-2.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123944252000022},
author = {Rick F. {van der Lans}}
}
@incollection{20159,
title = {Chapter Two - Database Management Systems},
editor = {Olivier Curé and Guillaume Blin},
booktitle = {RDF Database Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {9-40},
year = {2015},
isbn = {978-0-12-799957-9},
doi = {https://doi.org/10.1016/B978-0-12-799957-9.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012799957900002X},
keywords = {database management system, relational model, NoSQL, Structured Query Language, Index, ACID, Distribution},
abstract = {In this chapter, we present the main aspects and solutions of database management systems that have inspired or are currently influencing RDF stores. This ranges from systems based on the relational model to NoSQL and the recent NewSQL stores. It covers aspects such as storage solutions, efficient query processing through indexation, data and processing distribution and parallelism.}
}
@article{QAMAR2016324,
title = {Querying Medical Datasets While Preserving Privacy},
journal = {Procedia Computer Science},
volume = {98},
pages = {324-331},
year = {2016},
note = {The 7th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2016)/The 6th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2016)/Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916321949},
author = {Nafees Qamar and Yilong Yang and Andras Nadas and Zhiming Liu},
keywords = {Data disclosure, Data de-identification, Web Services, Data Privacy, Automated Software Engineering},
abstract = {This paper addresses the challenge of identifying clinically-relevant patterns in medical datasets without endangering patient privacy. To this end, we treat medical datasets as black box for both internal and external users of the data enabling a remote query mechanism to construct and execute database queries. The novelty of the approach lies in avoiding the complex data de-identification process which is often used to preserve patient privacy. The implemented toolkit combines software engineering technologies such as Java EE and RESTful web services, to allow exchanging medical data in an unidentifiable XML format along with restricting users to the need-to-know privacy principle. Consequently, the technique inhibits retrospective processing of data, such as attacks by an adversary on a medical dataset using advanced computational methods to reveal Protected Health Information (PHI). The approach is validated on an endoscopic reporting application based on openEHR and MST standards. The proposed approach is largely motivated by the issues related to querying datasets by clinical researchers, governmental or non-governmental organizations in monitoring health care services to improve quality of care.}
}
@article{WOODSMITH201434,
title = {Studying post-translational modifications with protein interaction networks},
journal = {Current Opinion in Structural Biology},
volume = {24},
pages = {34-44},
year = {2014},
note = {Folding and binding / Nucleic acids and their protein complexes},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2013.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X13002054},
author = {Jonathan Woodsmith and Ulrich Stelzl},
abstract = {At least 46 interactome studies, broad at proteome scale or biologically more focused, have together mapped about 75,000 human protein–protein interactions (PPIs). Many of the studies addressed local interactome data paucity analyzing specific homeostatic and regulatory systems, with recent focus demonstrating the involvement of post-translational protein modification (PTM) enzyme families in a wide range of cellular functions. These datasets provided insight into binding mechanisms, the dynamic modularity of complexes or delineated combinatorial enzymatic cascades. Furthermore, the combined study of PPI and PTM dynamics has begun to reveal conditional rewiring of molecular networks through PTM-mediated recognition events. Taken together these studies highlight the utility of local and global interaction networks to functionally prioritize the many changing PTMs mapped in human cells.}
}
@article{TANI20131194,
title = {Dealing with metadata quality: The legacy of digital library efforts},
journal = {Information Processing & Management},
volume = {49},
number = {6},
pages = {1194-1205},
year = {2013},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2013.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0306457313000526},
author = {Alice Tani and Leonardo Candela and Donatella Castelli},
keywords = {Quality, Digital Libraries, Metadata quality frameworks, Data infrastructures},
abstract = {In this work, we elaborate on the meaning of metadata quality by surveying efforts and experiences matured in the digital library domain. In particular, an overview of the frameworks developed to characterize such a multi-faceted concept is presented. Moreover, the most common quality-related problems affecting metadata both during the creation and the aggregation phase are discussed together with the approaches, technologies and tools developed to mitigate them. This survey on digital library developments is expected to contribute to the ongoing discussion on data and metadata quality occurring in the emerging yet more general framework of data infrastructures.}
}
@incollection{CHARLESWORTH201661,
title = {Chapter 3 - If You Find Yourself in a Hole, Stop Digging: Legal and Ethical Issues of Text/Data Mining in Research},
editor = {Emma L. Tonkin and Gregory J.L. Tourte},
booktitle = {Working with Text},
publisher = {Chandos Publishing},
pages = {61-88},
year = {2016},
series = {Chandos Information Professional Series},
isbn = {978-1-84334-749-1},
doi = {https://doi.org/10.1016/B978-1-84334-749-1.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781843347491000032},
author = {A. Charlesworth and E.L. Tonkin},
keywords = {text mining, law, ethics, data mining, intellectual property, research ethics framework},
abstract = {When considering the range of legal and ethical issues that can arise from text/data mining practices in academic research, the comparative paucity of literature addressing those issues, as well as the apparent lack of any community or discipline-generated ethical framework or initiative, is striking. It is suggested that while technical expertise in this space may be developing apace, and there is increasing recognition of its potential economic and commercial importance, that academic data/text mining researchers would be remiss not to seize the opportunity, as other research communities have done, to seek to ensure that the legal and ethics research paradigm within which their institutions want them to operate, appropriately reflects the contexts and risks actually applicable to their work.}
}
@incollection{HIRSCH2016145,
title = {Chapter 8 - Informatics Support Across the Cancer Continuum: Treatment},
editor = {Bradford W. Hesse and David K. Ahern and Ellen Beckjord},
booktitle = {Oncology Informatics},
publisher = {Academic Press},
address = {Boston},
pages = {145-157},
year = {2016},
isbn = {978-0-12-802115-6},
doi = {https://doi.org/10.1016/B978-0-12-802115-6.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021156000082},
author = {Bradford Hirsch and Amy P. Abernethy},
keywords = {Cancer treatment, standardization, guidelines, patient-generated data, patient-reported outcomes, quality of life, comparative effectiveness research, registries},
abstract = {Oncology is at a turning point in the treatment of disease. With an exponential increase in the number of treatments available for a given cancer type, it becomes critical that clinicians have the data they need to make decisions at the point of care. Randomized clinical trials cannot keep pace with all of the questions facing the field. Novel informatics approaches hold great promise as a means to bridge the gap but there must be a logical approach if its potential is to be realized, focusing on data aggregation, optimization, and tooling. Success will lead to better patient outcomes and quality of life.}
}
@incollection{WINTERSMINER201587,
title = {Chapter 6 - Open-Source EMR and Decision Management Systems},
editor = {Linda A. Winters-Miner and Pat S. Bolding and Joseph M. Hilbe and Mitchell Goldstein and Thomas Hill and Robert Nisbet and Nephi Walton and Gary D. Miner},
booktitle = {Practical Predictive Analytics and Decisioning Systems for Medicine},
publisher = {Academic Press},
pages = {87-95},
year = {2015},
isbn = {978-0-12-411643-6},
doi = {https://doi.org/10.1016/B978-0-12-411643-6.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124116436000065},
author = {Linda A. Winters-Miner and Pat Bolding and Thomas Hill and Bob Nisbet and Mitchell Goldstein and Joseph M. Hilbe and Nephi Walton and Gary Miner and Chris Papesh},
keywords = {open-source electronic medical record (EMR) systems, OSCAR EMR system, OpenEMR, OpenMRS by Partners in Health (PIH), Raxa Project, MOTECH, records, VistA},
abstract = {Open-source EMR systems are becoming very popular in less developed countries of the world. In the USA, however, many commercial EMR systems are vying for dominance in healthcare organizations that have the financial flexibility to afford them. This chapter contains discussions and summaries of a number of open-source EMR systems in use today. Each healthcare organization must evaluate the pros and cons of adoption of open-source EMR systems, and judge whether or not features of specific EMR packages suit the needs of the organization. The chapter will provide the “grist” for the decision “mill” for choosing the EMR package that is suited best to the organization.}
}
@article{BONNEL2015108,
title = {Workshop Synthesis: Comparing and Combining Survey Modes},
journal = {Transportation Research Procedia},
volume = {11},
pages = {108-117},
year = {2015},
note = {Transport Survey Methods: Embracing Behavioural and Technological Changes Selected contributions from the 10th International Conference on Transport Survey Methods 16-21 November 2014, Leura, Australia},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2015.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352146515003014},
author = {Patrick Bonnel and Caroline Bayart and Brett Smith},
keywords = {survey modes, combining survey data, travel survey, face-to-face survey, telephone survey, web survey},
abstract = {This paper summarizes the discussions held during an in-depth six-hour workshop on the challenges of combining data from different survey modes with the anticipated aim of identifying current research needs. The main theme of the workshop was mixing survey modes as a way to meet the challenge of low response rates. However, the use of multi-mode surveys introduces new sources of bias: not all households have access to certain survey media (coverage bias); the response rate using one or another of the survey modes is correlated with social demographics (non-response bias); the sampling frame is dependent on the mode (sampling bias) or the instrument itself may affect the responses (measurement bias). The aim of this report is present the workshop's discussion on the identification of research needs with related to combining data from different survey modes.}
}
@article{HSIUNG2015295,
title = {Improving surgical care for children through multicenter registries and QI collaboratives},
journal = {Seminars in Pediatric Surgery},
volume = {24},
number = {6},
pages = {295-306},
year = {2015},
note = {SI: Improving Pediatric Surgery Quality and Outcomes in the 21st Century},
issn = {1055-8586},
doi = {https://doi.org/10.1053/j.sempedsurg.2015.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1055858615000980},
author = {Grace E. Hsiung and Fizan Abdullah},
keywords = {Quality improvement, Health services research, Pediatric collaborative networks, Pediatric health, Learning healthcare systems},
abstract = {The role of the healthcare organization is shifting and must overcome the challenges of fragmented, costly care, and lack of evidence in practice, to reduce cost, ensure quality, and deliver high-value care. Notable gaps exist within the expected quality and delivery of pediatric healthcare, necessitating a change in the role of the healthcare organization. To realize these goals, the use of collaborative networks that leverage massive datasets to provide information for the development of learning healthcare systems will become increasingly necessary as efforts are made to narrow the gap in healthcare quality for children. By building upon the lessons learned from early collaborative efforts and other industries, operationalizing new technologies, encouraging clinical–community partnerships, and improving performance through transparent pursuit of meaningful goals, pediatric surgery can increase the adoption of best practices by developing collaborative networks that provide evidence-based clinical decision support and accelerate progress toward a new culture of delivering high-quality, high-value, and evidenced-based pediatric surgical care.}
}
@article{LEE2012492,
title = {An Open Government Maturity Model for social media-based public engagement},
journal = {Government Information Quarterly},
volume = {29},
number = {4},
pages = {492-503},
year = {2012},
note = {Social Media in Government - Selections from the 12th Annual International Conference on Digital Government Research (dg.o2011)},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2012.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X1200086X},
author = {Gwanhoo Lee and Young Hoon Kwak},
keywords = {Open government, Social media, Public engagement, Maturity model, Data transparency, Open participation, Open collaboration},
abstract = {Social media has opened up unprecedented new possibilities of engaging the public in government work. In response to the Open Government Directive, U.S. federal agencies developed their open government plan and launched numerous social media-based public engagement initiatives. However, we find that many of these initiatives do not deliver the intended outcomes due to various organizational, technological, and financial challenges. We propose an Open Government Maturity Model based on our field studies with U.S. federal healthcare administration agencies. This model is specifically developed to assess and guide open government initiatives which focus on transparent, interactive, participatory, collaborative public engagement that are largely enabled by emerging technologies such as social media. The model consists of five maturity levels: initial conditions (Level 1), data transparency (Level 2), open participation (Level 3), open collaboration (Level 4), and ubiquitous engagement (Level 5). We argue that there is a logical sequence for increasing social media-based public engagement and agencies should focus on achieving one maturity level at a time. The Open Government Maturity Model helps government agencies implement their open government initiatives effectively by building organizational and technological capabilities in an orderly manner. We discuss challenges and best practices for each maturity level and conclude by presenting recommendations.}
}
@article{STEENBRUGGEN201681,
title = {Traffic incidents in motorways: An empirical proposal for incident detection using data from mobile phone operators},
journal = {Journal of Transport Geography},
volume = {54},
pages = {81-90},
year = {2016},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2016.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0966692316302575},
author = {John Steenbruggen and Emmanouil Tranos and Piet Rietveld},
keywords = {Road traffic incident management, Mobile phone data, Data science, Collective sensing},
abstract = {This paper proves that mobile phone usage data is an easy to use, cheap and most importantly, reliable predictor of motorway incidents. Using econometric modelling, this paper provides a proof of concept of how mobile phone usage data can be utilised to detect motorway incidents. Greater Amsterdam is used here as a case study and the results suggest that mobile phone usage data can be utilised for the development of an early warning system to support road traffic incident management.}
}
@article{COMES201581,
title = {Understanding the Health Disaster: Research Design for the Response to the 2014 West African Ebola Outbreak},
journal = {Procedia Engineering},
volume = {107},
pages = {81-89},
year = {2015},
note = {Humanitarian Technology: Science, Systems and Global Impact 2015, HumTech2015},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.06.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815010140},
author = {Tina Comes and Bartel {Van de Walle} and Laura Laguna and Matthieu Lauras},
keywords = {Ebola Virus Disease, Health Disaster, Coordination and Information Sharing, Humanitarian Logistics, Decision Support},
abstract = {The 2014 Ebola outbreak in West Africa is the largest ever in history, affecting multiple countries and to this date, the World Health Organization has registered more than 6,500 deaths attributed to Ebola. The challenges arising from this outbreak to responders worldwide do not follow the standard characterisation or response patterns of natural sudden onset vs. conflict disasters. Rather, it is a medical emergency, which is intertwined with multiple challenges in the sectors decision-making, coordination, logistics and information management. In this paper, we present our research framework, which is based on desk research and initial interviews with responders. This framework guides on-going field research in Ghana (December2014), and Liberia (Spring 2015).}
}
@article{LEGG2014148,
title = {Standardisation of test requesting and reporting for the electronic health record},
journal = {Clinica Chimica Acta},
volume = {432},
pages = {148-156},
year = {2014},
note = {Harmonization of Laboratory Testing - A global activity},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cca.2013.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0009898113004919},
author = {Michael Legg},
keywords = {Pathology, Interoperability, Request, Report, Standardisation, Electronic health record},
abstract = {This paper is a review of the standardisation required to achieve interoperability for pathology test requesting and reporting. Interoperability is the ability of two parties, either human or machine, to exchange data or information in a manner that preserves shared meaning. This is needed to make healthcare safer, more efficient and more effective. Interoperability requires standardisation around: transmission of data; identification policies; information structures; common terminology; common understanding; and behavioural agreement. It is dependent on consensus. Each of these aspects is considered from the perspective of pathology requesting and reporting concluding that while much has been done, much remains to be done.}
}
@article{RIGBY2013209,
title = {Evidence for building a smarter health and wellness future—Key messages and collected visions from a Joint OECD and NSF workshop},
journal = {International Journal of Medical Informatics},
volume = {82},
number = {4},
pages = {209-219},
year = {2013},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2012.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1386505612001955},
author = {Michael Rigby and Elettra Ronchi and Susan Graham}
}
@article{LIU2016170,
title = {A comparative study of driving performance in metropolitan regions using large-scale vehicle trajectory data: Implications for sustainable cities},
journal = {International Journal of Sustainable Transportation},
volume = {11},
number = {3},
pages = {170-185},
year = {2016},
issn = {1556-8318},
doi = {https://doi.org/10.1080/15568318.2016.1230803},
url = {https://www.sciencedirect.com/science/article/pii/S1556831822006190},
author = {Jun Liu and Asad Khattak and Xin Wang},
keywords = {Driving indices, driving volatility, large-scale data, metropolitan region, mixed-effects model, sustainability},
abstract = {ABSTRACT
Volatile driving, characterized by hard accelerations and braking, can contribute substantially to higher energy consumption, tailpipe emissions, and crash risks. Drivers’ decisions to maintain speed, accelerate, brake rapidly, or jerk their vehicle are largely constrained by their unique regional and metropolitan contexts. These contexts may be characterized by their geography, roadway structure, traffic management, driving population, etc. This study captures how people generally drive in a region using large-scale vehicle trajectory data, implying how energy is consumed and how emissions are produced in regional transportation systems. Specifically, driving performance in four U.S. metropolitan areas (Los Angeles, San Francisco, Sacramento, and Atlanta) is compared, taking advantage of large-scale behavioral data (78.7 million seconds of speed records), collected by in-vehicle global positioning systems (GPSs) as part of regional surveys. Comparative analysis shows significant regional differences in terms of volatile driving and time spent to accelerate, brake, and jerk the vehicle during daily trips. Correlates of higher volatility are also explored, e.g., battery electric vehicles show low volatility, as expected. This study proposes a novel way to compare regional driving performance by successfully turning GPS driving data into valuable knowledge that can be applied in practice by developing regional driving performance indices. The new indices can also be used to compare regional performance over time and to imply the levels of sustainability of regional transportation systems. This study contributes by proposing a way to extract useful information from large-scale driving data.}
}
@incollection{RIDGE201516,
title = {Chapter 2 - Guerrilla Analytics: Challenges and Risks},
editor = {Enda Ridge},
booktitle = {Guerrilla Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {16-31},
year = {2015},
isbn = {978-0-12-800218-6},
doi = {https://doi.org/10.1016/B978-0-12-800218-6.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002186000023},
author = {Enda Ridge},
keywords = {Risks, Challenges, Management, Risk Mitigation},
abstract = {Summary
In this chapter, we describe Guerrilla Analytics projects in more detail. We will discuss the typical workflow and challenges encountered in Guerrilla Analytics projects. These challenges bring risks. We will discuss the risks inherent in a Guerrilla Analytics project and the effects and consequences of not managing these risks.}
}
@article{ASANO2016332,
title = {In Situ Cryo-Electron Tomography: A Post-Reductionist Approach to Structural Biology},
journal = {Journal of Molecular Biology},
volume = {428},
number = {2, Part A},
pages = {332-343},
year = {2016},
note = {Study of biomolecules and biological systems: Proteins},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2015.09.030},
url = {https://www.sciencedirect.com/science/article/pii/S0022283615005537},
author = {Shoh Asano and Benjamin D. Engel and Wolfgang Baumeister},
keywords = {cryo-EM, tomography, template matching, visual proteomics, subtomogram averaging},
abstract = {Cryo-electron tomography is a powerful technique that can faithfully image the native cellular environment at nanometer resolution. Unlike many other imaging approaches, cryo-electron tomography provides a label-free method of detecting biological structures, relying on the intrinsic contrast of frozen cellular material for direct identification of macromolecules. Recent advances in sample preparation, detector technology, and phase plate imaging have enabled the structural characterization of protein complexes within intact cells. Here, we review these technical developments and outline a detailed computational workflow for in situ structural analysis. Two recent studies are described to illustrate how this workflow can be adapted to examine both known and unknown cellular complexes. The stage is now set to realize the promise of visual proteomics—a complete structural description of the cell's native molecular landscape.}
}
@article{FAN201581,
title = {A framework for knowledge discovery in massive building automation data and its application in building diagnostics},
journal = {Automation in Construction},
volume = {50},
pages = {81-90},
year = {2015},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514002507},
author = {Cheng Fan and Fu Xiao and Chengchu Yan},
keywords = {Building Automation System, Data mining, Building energy performance, Building diagnostics},
abstract = {Building Automation System (BAS) plays an important role in building operation nowadays. A huge amount of building operational data is stored in BAS; however, the data can seldom be effectively utilized due to the lack of powerful tools for analyzing the large data. Data mining (DM) is a promising technology for discovering knowledge hidden in large data. This paper presents a generic framework for knowledge discovery in massive BAS data using DM techniques. The framework is specifically designed considering the low quality and complexity of BAS data, the diversity of advanced DM techniques, as well as the integration of knowledge discovered by DM techniques and domain knowledge in the building field. The framework mainly consists of four phases, i.e., data exploration, data partitioning, knowledge discovery, and post-mining. The framework is applied to analyze the BAS data of the tallest building in Hong Kong. The analysis of variance (ANOVA) method is adopted to identify the most significant time variables to the aggregated power consumption. Then the clustering analysis is used to identify the typical operation patterns in terms of power consumption. Eight operation patterns have been identified and therefore the entire BAS data are partitioned into eight subsets. The quantitative association rule mining (QARM) method is adopted for knowledge discovery in each subset considering most of BAS data are numeric type. To enhance the efficiency of the post-mining phase, two indices are proposed for fast and conveniently identifying and utilizing potentially interesting rules discovered by QARM. The knowledge discovered is successfully used for understanding the building operating behaviors, identifying non-typical operating conditions and detecting faulty conditions.}
}
@incollection{KOTU201517,
title = {Chapter 2 - Data Mining Process},
editor = {Vijay Kotu and Bala Deshpande},
booktitle = {Predictive Analytics and Data Mining},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {17-36},
year = {2015},
isbn = {978-0-12-801460-8},
doi = {https://doi.org/10.1016/B978-0-12-801460-8.00002-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128014608000021},
author = {Vijay Kotu and Bala Deshpande},
keywords = {CRISP, KDD, data mining process, prior knowledge, modeling, data preparation, evaluation, application},
abstract = {Successfully uncovering patterns using data mining is an iterative process. Chapter 2 provides a framework to solve the data mining problem. The five-step process outlined in this chapter provides guidelines on gathering subject matter expertise; exploring the data with statistics and visualization; building a model using data mining algorithms; testing the model and deploying it in a production environment; and finally reflecting on new knowledge gained in the cycle. Over the years of evolution of data mining practices, different frameworks for the data mining process have been put forward by various academic and commercial bodies, like the Cross Industry Standard Process for Data Mining, knowledge discovery in databases, etc. These data mining frameworks exhibit common characteristics and hence we will be using a generic framework closely resembling the CRISP process.}
}
@article{ALLEN20151,
title = {What do we do now? Workflows for an unpredictable world},
journal = {Future Generation Computer Systems},
volume = {42},
pages = {1-10},
year = {2015},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2014.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X14001526},
author = {M. David Allen and Adriane Chapman and Barbara Blaustein and Lisa Mak},
keywords = {Adaptive workflows, Usability, Exchanging pipelines, Dynamic workflows, Business process, Matching},
abstract = {Workflow systems permit organization of many individual subtasks into a cohesive whole, in order to accomplish a specific mission. For many government and business missions, these systems are used to manage repetitive processes, such as large data-processing and exploitation pipelines. Government missions with strong interactions with the real world are extremely dynamic, as are all missions dealing with error-prone or changing data streams. We contribute a vision for discovery of new steps in adaptive workflow systems, suitability functions that can discover candidate alternatives, and a way forward for sourcing options for decision-makers, without the strong assumptions required by previous work. As data-processing workflows are shared, the sharing entities may find that certain parts of the workflow must be adapted to the new environment of mission. Extremely dynamic environments call for capabilities that support agile operations and pipeline sharing by making it possible to choose relevant actions when a situation invalidates the assumptions of current execution. We adapt some work in schema matching towards this problem, citing key differences between the two sets of challenges.}
}
@incollection{2015137,
title = {Index},
editor = {Krish Krishnan and Shawn P. Rogers},
booktitle = {Social Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {137-142},
year = {2015},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397186-9},
doi = {https://doi.org/10.1016/B978-0-12-397186-9.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971869000212}
}
@incollection{SHAHBAZULHAQ201683,
title = {Chapter 12 - Data Mapping Scenarios},
editor = {Qamar {Shahbaz Ul Haq}},
booktitle = {Data Mapping for Data Warehouse Design},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {83-165},
year = {2016},
isbn = {978-0-12-805185-6},
doi = {https://doi.org/10.1016/B978-0-12-805185-6.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128051856000125},
author = {Qamar {Shahbaz Ul Haq}},
keywords = {history handling, data consolidation, mapping priority, master data management, data integration, data quality improvement, data warehouse performance improvement, denormalized mapping, aggregate, facts, dimensions, recursive query},
abstract = {After understanding source data, writing the transformation rule is a simple process in 90% of the time. However, there can be cases when special rules and handling are required. This chapter explains mapping scenarios that can be used to understand complex issues and their solutions.}
}
@article{TORRES20163513,
title = {Proteome-wide Structural Analysis of PTM Hotspots Reveals Regulatory Elements Predicted to Impact Biological Function and Disease*},
journal = {Molecular & Cellular Proteomics},
volume = {15},
number = {11},
pages = {3513-3528},
year = {2016},
issn = {1535-9476},
doi = {https://doi.org/10.1074/mcp.M116.062331},
url = {https://www.sciencedirect.com/science/article/pii/S1535947620333405},
author = {Matthew P. Torres and Henry Dewhurst and Niveda Sundararaman},
abstract = {Post-translational modifications (PTMs) regulate protein behavior through modulation of protein-protein interactions, enzymatic activity, and protein stability essential in the translation of genotype to phenotype in eukaryotes. Currently, less than 4% of all eukaryotic PTMs are reported to have biological function - a statistic that continues to decrease with an increasing rate of PTM detection. Previously, we developed SAPH-ire (Structural Analysis of PTM Hotspots) - a method for the prioritization of PTM function potential that has been used effectively to reveal novel PTM regulatory elements in discrete protein families (Dewhurst et al., 2015). Here, we apply SAPH-ire to the set of eukaryotic protein families containing experimental PTM and 3D structure data - capturing 1,325 protein families with 50,839 unique PTM sites organized into 31,747 modified alignment positions (MAPs), of which 2010 (∼6%) possess known biological function. Here, we show that using an artificial neural network model (SAPH-ire NN) trained to identify MAP hotspots with biological function results in prediction outcomes that far surpass the use of single hotspot features, including nearest neighbor PTM clustering methods. We find the greatest enhancement in prediction for positions with PTM counts of five or less, which represent 98% of all MAPs in the eukaryotic proteome and 90% of all MAPs found to have biological function. Analysis of the top 1092 MAP hotspots revealed 267 of truly unknown function (containing 5443 distinct PTMs). Of these, 165 hotspots could be mapped to human KEGG pathways for normal and/or disease physiology. Many high-ranking hotspots were also found to be disease-associated pathogenic sites of amino acid substitution despite the lack of observable PTM in the human protein family member. Taken together, these experiments demonstrate that the functional relevance of a PTM can be predicted very effectively by neural network models, revealing a large but testable body of potential regulatory elements that impact hundreds of different biological processes important in eukaryotic biology and human health.}
}
@article{RECALEGARI2016223,
title = {City data dating: Emerging affinities between diverse urban datasets},
journal = {Information Systems},
volume = {57},
pages = {223-240},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2015.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0306437915001362},
author = {Gloria {Re Calegari} and Irene Celino and Diego Peroni},
keywords = {Smart city, Data diversity, Spatio-temporal data resolution, Mobile data processing, Correlation analysis, Regression analysis, Clustering analysis, Information fusion},
abstract = {Cities are complex environments in which digital technologies are more and more pervasive; this digitization of the urban space has led to a rich ecosystem of data producers and data consumers. Moreover, heterogeneous sources differ in terms of data complexity, spatio-temporal resolution and curation/maintenance costs. Do those diverse urban sources reflect the same picture of the city? Do distinct perspectives share some commonalities? In this paper we present our data analytics/empirical experiments on a set of urban sources related to the city of Milano; our investigation is aimed at discovering “affinities” between datasets by means of different quantitative and qualitative correlation analyses. We also explore the influence of spatial resolution and data complexity on the dependence strength between heterogeneous urban sources, to pave the way to a meaningful information fusion.}
}
@incollection{2016395,
title = {Index},
editor = {Bradford W. Hesse and David K. Ahern and Ellen Beckjord},
booktitle = {Oncology Informatics},
publisher = {Academic Press},
address = {Boston},
pages = {395-407},
year = {2016},
isbn = {978-0-12-802115-6},
doi = {https://doi.org/10.1016/B978-0-12-802115-6.00034-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021156000343}
}
@incollection{KIM2016161,
title = {9 - Wearable body sensor network for health care applications},
editor = {Vladan Koncar},
booktitle = {Smart Textiles and their Applications},
publisher = {Woodhead Publishing},
address = {Oxford},
pages = {161-184},
year = {2016},
series = {Woodhead Publishing Series in Textiles},
isbn = {978-0-08-100574-3},
doi = {https://doi.org/10.1016/B978-0-08-100574-3.00009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081005743000096},
author = {Y.K. Kim and H. Wang and M.S. Mahmud},
keywords = {Biosensors, Electronic textiles (E-Tex), Internet of Things (IoT), Wearable body sensor networks (WBSNs), Wearable computing},
abstract = {“Wearable computing” refers to electronic systems that are directly integrated into garments and accessories for constant monitoring and easy accessibility, which is a wearable body sensor network (WBSN) based on smart textiles. WBSNs will be a great solution to enable the ubiquitous noninvasive health monitoring in people's daily life. However, developing smart textile-based body sensor networks poses significant technical challenges for the sensor and sensor network design such as miniaturization of sensors, imbedding sensors in noninvasive wearable structures, the integration of radio integrated circuits (ICs) and modules with body-worn antennas, energy consumption minimization, and data security. In this chapter, the challenges of designing electronic textiles-based body sensor networks are discussed, and strategies and solutions to overcome the problems are proposed. To demonstrate the applicability of smart textile-based body sensor networks, the Internet of Things (IoT) systems for health care and fitness applications will be reviewed.}
}
@article{COLOT201575,
title = {From Mobile Data Towards Better Customer Knowledge: Proposals for an Information Framework},
journal = {Procedia Computer Science},
volume = {52},
pages = {75-82},
year = {2015},
note = {The 6th International Conference on Ambient Systems, Networks and Technologies (ANT-2015), the 5th International Conference on Sustainable Energy Information Technology (SEIT-2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915008285},
author = {Christian Colot and Isabelle Linden},
keywords = {Mobile data, Mobile marketing, CRM, Customer model},
abstract = {Along with the development of internet connected devices, Mobile Marketing is expected to be ubiquitous in the near future. In order to deliver right mobile offers, the need to better know the customer is now crucial. This article develops a theoretical marketing framework to categorise the most relevant information linked to mobile data which complements traditional CRM data models. Three dimensions are identified: context, time line and source of the information. The matching of this framework with the current and future mobile technologies is then analysed.}
}
@incollection{SHERMAN2015425,
title = {Chapter 17 - People, Process and Politics},
editor = {Rick Sherman},
booktitle = {Business Intelligence Guidebook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {425-448},
year = {2015},
isbn = {978-0-12-411461-6},
doi = {https://doi.org/10.1016/B978-0-12-411461-6.00017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124114616000174},
author = {Rick Sherman},
keywords = {BI training, Business and IT, Data governance, People, Politics, Process, Project management team, Project team},
abstract = {People, process, and politics are at the heart of the stickiest business intelligence (BI) project issues. As with any relationship, the interactions between IT and business are vulnerable to misinterpretations, poor communication, and disagreements that can bog down a BI project. The groups need to learn how to communicate and engage in give-and-take. It's important to identify the roles and responsibilities clearly, and understand that some roles span both business and IT. When they start building the team, IT and the business should work together to sponsor and govern design, development, deployment, and ongoing support. With sponsorship and governance in place, they can build the project management and project-development teams. BI training should involve both business and IT people, and the content and methods should be tailored to specific needs. Training is far more than just learning tools; without the concepts and fundamentals of BI and DW, the tools won't have context. Data governance is a people-centric program that is critical for transforming data into actionable information. BI projects would be easy without the challenges of sorting through people, process, and political issues. These challenges are often harder than the technology issues.}
}
@incollection{WANG20151,
title = {Chapter 1 - A new information age},
editor = {Dong Wang and Tarek Abdelzaher and Lance Kaplan},
booktitle = {Social Sensing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-11},
year = {2015},
isbn = {978-0-12-800867-6},
doi = {https://doi.org/10.1016/B978-0-12-800867-6.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128008676000017},
author = {Dong Wang and Tarek Abdelzaher and Lance Kaplan},
keywords = {Social sensing, New information age, Introduction, Motivation, Challenges, State-of-the-art},
abstract = {Social sensing broadly refers to a set of sensing and data collection paradigms where data are collected from humans or devices on their behalf. In this chapter, we first give an overview of social sensing as an emerging research field and identify the data reliability problem as a fundamental research challenge in this field. This challenge, if successfully addressed, engenders a paradigm shift in social sensing by allowing development of dependable applications with guaranteed correctness properties that rely on the collective observations of untrained, average, and largely unreliable sources. Followed by the overview, we go over the motivations of social sensing applications and discuss several key challenges and state-of-the-art techniques centered on the data reliability problem. In the end of the chapter, we review the organization of the whole book chapter by chapter.}
}
@incollection{2013529,
editor = {A.J. Marian Walhout and Marc Vidal and Job Dekker},
booktitle = {Handbook of Systems Biology},
publisher = {Academic Press},
address = {San Diego},
pages = {529-538},
year = {2013},
isbn = {978-0-12-385944-0},
doi = {https://doi.org/10.1016/B978-0-12-385944-0.18001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859440180013}
}
@article{GOLIGHTLY201612,
title = {Manufacturing in the cloud: A human factors perspective},
journal = {International Journal of Industrial Ergonomics},
volume = {55},
pages = {12-21},
year = {2016},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2016.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0169814116300464},
author = {David Golightly and Sarah Sharples and Harshada Patel and Svetan Ratchev},
keywords = {Cloud manufacturing, Assembly, Production, Collaboration},
abstract = {Cloud manufacturing adopts a cloud computing paradigm as the basis for delivering shared, on-demand manufacturing services. The result is customer-centric supply chains that can be configured for cost, quality, speed and customisation. While the technical capabilities required for cloud manufacturing are a current focus, there are many emerging questions relating to the impact, both positive and negative, on the people consuming or supporting cloud manufacturing services. Human factors can have a pivotal role in enabling the success and adoption of cloud manufacturing, while ensuring the safety, well-being and optimum user experience of those involved in a cloud manufacturing environment. This paper presents these issues, structured around groups of users (service providers, application providers and consumers). We also consider the issues of collaboration that are likely to arise from the manufacturing cloud. From this analysis we discuss the central role of human factors as an enabler of cloud manufacturing, and the opportunities that emerge.}
}
@article{SYNTETOS20161,
title = {Supply chain forecasting: Theory, practice, their gap and the future},
journal = {European Journal of Operational Research},
volume = {252},
number = {1},
pages = {1-26},
year = {2016},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2015.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0377221715010231},
author = {Aris A. Syntetos and Zied Babai and John E. Boylan and Stephan Kolassa and Konstantinos Nikolopoulos},
keywords = {Supply chain forecasting, Forecasting software, Forecasting empirical research, Literature review},
abstract = {Supply Chain Forecasting (SCF) goes beyond the operational task of extrapolating demand requirements at one echelon. It involves complex issues such as supply chain coordination and sharing of information between multiple stakeholders. Academic research in SCF has tended to neglect some issues that are important in practice. In areas of practical relevance, sound theoretical developments have rarely been translated into operational solutions or integrated in state-of-the-art decision support systems. Furthermore, many experience-driven heuristics are increasingly used in everyday business practices. These heuristics are not supported by substantive scientific evidence; however, they are sometimes very hard to outperform. This can be attributed to the robustness of these simple and practical solutions such as aggregation approaches for example (across time, customers and products). This paper provides a comprehensive review of the literature and aims at bridging the gap between theory and practice in the existing knowledge base in SCF. We highlight the most promising approaches and suggest their integration in forecasting support systems. We discuss the current challenges both from a research and practitioner perspective and provide a research and application agenda for further work in this area. Finally, we make a contribution in the methodology underlying the preparation of review articles by means of involving the forecasting community in the process of deciding both the content and structure of this paper.}
}
@article{AKHTAR2016392,
title = {Data-driven and adaptive leadership contributing to sustainability: global agri-food supply chains connected with emerging markets},
journal = {International Journal of Production Economics},
volume = {181},
pages = {392-401},
year = {2016},
note = {Recent Development Of Sustainable Consumption And Production In Emerging Markets},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2015.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0925527315005125},
author = {Pervaiz Akhtar and Ying Kei Tse and Zaheer Khan and Rekha Rao-Nicholson},
keywords = {Data-driven and adaptive leadership, Sustainability, Global agrifood supply chains, Structural equation modeling, Endogeneity},
abstract = {Despite numerous promises on the links between data-driven and adaptive leadership, non-financial sustainability and financial sustainability, scholars have not conducted enough empirical research to test the links based on globally and massively connected emerging supply chains. This study therefore scrutinizes the interlocking links by applying the data collected from chief executive officers, managing directors and senior operations managers of such supply chains rooted in emerging markets. The possibly purified results raised from structural equation modeling indicate that data-driven and adaptive leadership is a key determinant for non-financial sustainability, which in turn contributes to financial sustainability. Directly, the leadership also plays a vital role for financial sustainability. Interaction effects further depict that the companies which apply more data-driven and adaptive leadership practices perform better compared to those which less focus on such practices. Consequently, the results provide the deeper understanding of the mechanism of how global supply chain leaders can use data-driven and adaptive leadership to co-create financial and non-financial sustainability.}
}
@incollection{2016351,
title = {Index},
editor = {Brian E. Dixon},
booktitle = {Health Information Exchange},
publisher = {Academic Press},
pages = {351-361},
year = {2016},
isbn = {978-0-12-803135-3},
doi = {https://doi.org/10.1016/B978-0-12-803135-3.00035-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128031353000359}
}
@article{KOVALCHUK2015532,
title = {Towards Ensemble Simulation of Complex Systems},
journal = {Procedia Computer Science},
volume = {51},
pages = {532-541},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.280},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915010881},
author = {Sergey V. Kovalchuk and Alexander V. Boukhanovsky},
keywords = {Ensemble, Complex system simulation, Assimilation, Diversity, Workflow},
abstract = {The paper presents an early-stage research which is aimed towards the development of comprehensive conceptual and technological framework for ensemble-based simulation of complex systems. The concept of multi-layer ensemble is presented as a background for further development of the framework to cover different kind of ensembles: ensemble of system's state, data ensemble, and models ensemble. Formal description of a hybrid model is provided as a core concept for ensemble-based complex system simulation. The example of water level forecasting application is used to show selected ensemble classes covered by the proposed framework.}
}
@incollection{KRISHNAN201561,
title = {Chapter 6 - Accessing the Data},
editor = {Krish Krishnan and Shawn P. Rogers},
booktitle = {Social Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {61-74},
year = {2015},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397186-9},
doi = {https://doi.org/10.1016/B978-0-12-397186-9.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971869000066},
author = {Krish Krishnan and Shawn P. Rogers},
keywords = {Acquire, Monitor, Store, Collect, Buy Data, Sentiment, Analysis, Algorithms},
abstract = {The biggest challenge in the world of Social media data is the data itself, which has a mix of noise and value as one angle of the triangle, privacy and compliance issues as the second angle, and third angle is the technology and process needed to harness the data. There are many techniques to accomplish all of these challenges, which is the best and long-standing solution? How to make this enterprise ready and secure?}
}
@incollection{2015283,
title = {Index},
editor = {Andrea Belgrano and Guy Woodward and Ute Jacob},
booktitle = {Aquatic Functional Biodiversity},
publisher = {Academic Press},
address = {San Diego},
pages = {283-289},
year = {2015},
isbn = {978-0-12-417015-5},
doi = {https://doi.org/10.1016/B978-0-12-417015-5.18001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124170155180016}
}
@article{GRIES2016201,
title = {Information management at the North Temperate Lakes Long-term Ecological Research site — Successful support of research in a large, diverse, and long running project},
journal = {Ecological Informatics},
volume = {36},
pages = {201-208},
year = {2016},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1574954116301273},
author = {Corinna Gries and Mark R. Gahler and Paul C. Hanson and Timothy K. Kratz and Emily H. Stanley},
keywords = {Ecological information management, Long-term ecological research, Database, Sensor data, Data life cycle},
abstract = {Information management has been an integral part of the research process at the North Temperate Lakes Long-term Ecological Research (NTL LTER) program for over 30years. A combination of factors has made the information management system (IMS) at NTL very successful. Significant resources have been invested in the IMS from the beginning, the Information Manager has been part of the leadership team at NTL and later in various roles at the LTER network level; the NTL IMS was a very early adopter of database systems, standardized metadata, and a data delivery system based on those metadata. This approach has made data easily accessible to NTL researchers and the broader scientific community. Data management workflows have become increasingly more automated with adoption of modern technologies as they became available, making the system efficient enough to handle core data as well as all one-time research data generated within NTL and several related projects. More than three decades of core data from eleven lakes are reused extensively as critical background information and as the limnological go-to site for many synthesis projects within and beyond LTER. The NTL IMS continues to implement new technologies for improving data management efficiency, discovery, access, integration, and synthesis. Accordingly, the functionality of the original online data access system programmed in Java and JavaServer Pages (JSP) was ported to the modern content management system, Drupal and integrated into LTER's Drupal Ecological Information Management System (DEIMS). NTL has invested in sensor technology for studying lake conditions over the long term, which necessitated a sophisticated management system tailored to high frequency data streams. Several technologies have been used at different times for automation of management, quality control and archiving of these high volume data. Near real time lake conditions can be accessed on the NTL website and smart phone Apps. Easy access to long-term and sensor data in the NTL IMS has led NTL researchers to develop new analytical methods and the publication of several R statistical packages. Recent graduate students are now employed as data scientists helping define a new career path inspired by the availability of data. The NTL project has amassed one of the world's most comprehensive long-term datasets on lakes and their surrounding landscapes. The NTL IMS facilitates the use of these data by multiple groups for research, education, and communication of science to the public.}
}
@article{ROBSON201551,
title = {Suggestions for a web based universal exchange and inference language for medicine. Continuity of patient care with PCAST disaggregation},
journal = {Computers in Biology and Medicine},
volume = {56},
pages = {51-66},
year = {2015},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2014.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S0010482514002960},
author = {Barry Robson and Thomas P. Caruso and Ulysses G.J. Balis},
keywords = {Universal exchange language, PCAST report, Continuity of care, Interoperability, Electronic health record, Disaggregation},
abstract = {We describe here the applications of our recently proposed Q-UEL language to continuity of patient care between physicians, specialists and institutions as mediated via the Internet, giving examples derived from HL7 CDA and VistA of particular interest to workflow. Particular attention is given to the Universal Exchange Language for healthcare as requested by the US President׳s Council of Advisors on Science and Technology (PCAST) released in December 2010, especially in regard to disaggregation of the patient record on the Internet. To illustrate many features and options, one of our most elaborate configurations combining them, for disaggregation and reaggregation, is described. The Q-UEL tags used do not physically join, but query each other from a random mix via the application. Despite the computationally demanding complexity of the configuration with two joining tags for each data tag and four independently evolving keys, plus a valuable but rate limiting isomorphism test, packets of essential clinical data for patient could be recovered and displayed every 2s for a “club” of 30,000–50,000 patients in the mix. All computation here is on a standard laptop, but for practical use of the Internet to display downloaded data, the above is adequate, so focus is primarily on increasing club size. In practice, it is not necessary that a club comprise an entire nation. Assuming that one does not use purely random assignments of patients to arbitrary clubs, there could for example be a club comprising all schoolchildren in Scotland, or a club comprising all military veterans in Illinois. In such cases, one is typically dealing with clubs each of the order of a mere million patients. Using such club sizes efficiently, and in principle even a club the size of a whole country, appears to be possible.}
}
@article{BENTHAUS2016127,
title = {Social media management strategies for organizational impression management and their effect on public perception},
journal = {The Journal of Strategic Information Systems},
volume = {25},
number = {2},
pages = {127-139},
year = {2016},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2015.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0963868715000694},
author = {Janek Benthaus and Marten Risius and Roman Beck},
keywords = {Strategic social media management, Mixed methods approach, Impression management, Receiver operating characteristic analysis, Twitter, Public perception},
abstract = {With the growing importance of social media, companies increasingly rely on social media management tools to analyze social media activities and to professionalize their social media engagement. In this study, we evaluate how social media management tools, as part of an overarching social media strategy, help companies to positively influence the public perception among social media users. A mixed methods approach is applied, where we quantitatively analyze 15million user-generated Twitter messages containing information about 45 large global companies highly active on Twitter, as well as almost 160 thousand corresponding messages sent from these companies via their corporate Twitter accounts. Additionally, we conducted interviews with six social media experts to gain complementary insights. By these means, we are able to identify significant differences between different social media management strategies and measure the corresponding effects on the public perception.}
}
@article{PONS201689,
title = {A comprehensive open package format for preservation and distribution of geospatial data and metadata},
journal = {Computers & Geosciences},
volume = {97},
pages = {89-97},
year = {2016},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416303533},
author = {X. Pons and J. Masó},
keywords = {Data standard, Internet GIS, Metadata, Data model, Preservation, Package, MMZX},
abstract = {The complexities of the intricate geospatial resources and formats make preservation and distribution of GIS data difficult even among experts. The proliferation of, for instance, KML, Internet map services, etc, reflects the need for sharing geodata but a comprehensive solution when having to deal with data and metadata of a certain complexity is not currently provided. Original geospatial data is usually divided into several parts to record its different aspects (spatial and thematic features, etc), plus additional files containing, metadata, symbolization specifications and tables, etc; these parts are encoded in different formats, both standard and proprietary. To simplify data access, software providers encourage the use of an additional element that we call generically “map project”, and this contains links to other parts (local or remote). Consequently, in order to distribute the data and metadata refereed by the map in a complete way, or to apply the Open Archival Information System (OAIS) standard to preserve it for the future, we need to face the multipart problem. This paper proposes a package allowing the distribution of real (comprehensive although diverse and complex) GIS data over the Internet and for data preservation. This proposal, complemented with the right tools, hides but keeps the multipart structure, so providing a simpler but professional user experience. Several packaging strategies are reviewed in the paper, and a solution based on ISO 29500-2 standard is chosen. The solution also considers the adoption of the recent Open Geospatial Consortium Web Services common standard (OGC OWS) context document as map part, and as a way for also combining data files with geospatial services. Finally, and by using adequate strategies, different GIS implementations can use several parts of the package and ignore the rest: a philosophy that has proven useful (e.g. in TIFF).}
}
@article{DONG201599,
title = {Tracking the dynamics of paddy rice planting area in 1986–2010 through time series Landsat images and phenology-based algorithms},
journal = {Remote Sensing of Environment},
volume = {160},
pages = {99-113},
year = {2015},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2015.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0034425715000139},
author = {Jinwei Dong and Xiangming Xiao and Weili Kou and Yuanwei Qin and Geli Zhang and Li Li and Cui Jin and Yuting Zhou and Jie Wang and Chandrashekhar Biradar and Jiyuan Liu and Berrien Moore},
keywords = {Paddy rice, Landsat-RICE, Phenology, Land use change, Northeast China},
abstract = {Agricultural land use change substantially affects climate, water, ecosystems, biodiversity, and human welfare. In recent decades, due to increasing population and food demand and the backdrop of global warming, croplands have been expanding into higher latitude regions. One such hotspot is paddy rice expansion in northeast China. However, there are no maps available for documenting the spatial and temporal patterns of continuous paddy rice expansion. In this study, we developed an automated, Landsat-based paddy rice mapping (Landsat-RICE) system that uses time series Landsat images and a phenology-based algorithm based on the unique spectral characteristics of paddy rice during the flooding/transplanting phase. As a pilot study, we analyzed all the available Landsat images from 1986 to 2010 (498 scenes) in one tile (path/row 113/27) of northeast China, which tracked paddy rice expansion in epochs with five-year increments (1986–1990, 1991–1995, 1996–2000, 2001–2005, and 2006–2010). Several maps of land cover types (barren land and built-up land; evergreen, deciduous and sparse vegetation types; and water-related land cover types such as permanent water body, mixed pixels of water and vegetation, spring flooded wetlands and summer flooded land) were generated as masks. Air temperature was used to define phenology timing and crop calendar, which were then used to select Landsat images in the phenology-based algorithms for paddy rice and masks. The resultant maps of paddy rice in the five epochs were evaluated using validation samples from multiple sources, and the overall accuracies and Kappa coefficients ranged from 84 to 95% and 0.6–0.9, respectively. The paddy rice area in the study area substantially increased from 1986 to 2010, particularly after the 1990s. This study demonstrates the potential of the Landsat-RICE system and time series Landsat images for tracking agricultural land use changes at 30-m resolution in the temperate zone with single crop cultivation.}
}
@article{PFEFFER20141,
title = {Multigenerational approaches to social mobility. A multifaceted research agenda},
journal = {Research in Social Stratification and Mobility},
volume = {35},
pages = {1-12},
year = {2014},
note = {Inequality Across Multiple Generations},
issn = {0276-5624},
doi = {https://doi.org/10.1016/j.rssm.2014.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S027656241400002X},
author = {Fabian T. Pfeffer}
}
@article{WASAN20132476,
title = {Application of statistics and machine learning for risk stratification of heritable cardiac arrhythmias},
journal = {Expert Systems with Applications},
volume = {40},
number = {7},
pages = {2476-2486},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.10.054},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412011785},
author = {P.S. Wasan and M. Uttamchandani and S. Moochhala and V.B. Yap and P.H. Yap},
keywords = {Risk stratification, Brugada, Long QT, Statistics, Machine learning},
abstract = {In the clinical management of heritable cardiac arrhythmias (HCAs), risk stratification is of prime importance. The ability to predict the likelihood of individuals within a sub-population contracting a pathology potentially resulting in sudden death gives subjects the opportunity to put preventive measures in place, and make the necessary lifestyle adjustments to increase their chances of survival. In this paper, we review classical methods that have commonly been used in clinical studies for risk stratification in HCA, such as odds ratios, hazard ratios, Chi-squared tests, and logistic regression, discussing their benefits and shortcomings. We then explore less common and more recent statistical and machine learning methods adopted by other biological studies and assess their applicability in the study of HCA. These methods typically support the multivariate analysis of risk factors, such as decision trees, neural networks, support vector machines and Bayesian classifiers. They have been adopted for feature selection of predictor variables in risk stratification studies, and in some cases, prove better than classical methods.}
}
@article{KOSUKHIN20141667,
title = {Problem Solving Environment for Development and Maintenance of St. Petersburg‘s Flood Warning System},
journal = {Procedia Computer Science},
volume = {29},
pages = {1667-1676},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.152},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914003299},
author = {Sergey S. Kosukhin and Anna V. Kalyuzhnaya and Denis Nasonov},
keywords = {problem solving environment, composite application, flood warning system, flood simulation, ensemble forecasting},
abstract = {Saint-Petersburg Flood Warning System (FWS) is a life-critical system that requires permanent maintenance and development. Tasks that arise during thes e processes could be much more resource- intensive than an operational loop of the system and may involve complex problems for research. Thereby it is essential to have a special software tool to handle a collection of different models, data sources and auxiliary software that they could be combined in different ways according to a particular research problem to be solved. This paper aims to share the idea of Saint -Petersburg FWS evolution with help of problem-solving environment based on the cloud platform CLAVIRE.}
}
@article{THEKDI20151,
title = {Integrated risk management of safety and development on transportation corridors},
journal = {Reliability Engineering & System Safety},
volume = {138},
pages = {1-12},
year = {2015},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2014.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S095183201400297X},
author = {Shital A. Thekdi and James H. Lambert},
keywords = {Infrastructure management, Transportation and land use, Sustainable development, Multimodal transportation planning, Risk assessment and management},
abstract = {Prioritization of investments to protect safety and performance of multi-regional transportation networks from adjacent land development is a key concern for infrastructure agencies, land developers, and other stakeholders. Despite ample literature describing relationships between transportation and land use, no evidence-based methods exist for monitoring corridor needs on a large scale. Risk analysis is essential to the preservation of system safety and capacity, including avoidance of costly retrofits, regret, and belated action. This paper introduces the Corridor Trace Analysis (CTA) for prioritizing corridor segments that are vulnerable to adjacent land development. The method integrates several components: (i) estimation of likelihood of adjacent land development, using influence diagram and rule-based modeling, (ii) characterization of access point density using geospatial methods, and (iii) plural-model evaluation of corridors, monitoring indices of land development likelihood, access point densities, and traffic volumes. The results inform deployment of options that include closing access points, restricting development, and negotiation of agencies and developers. The CTA method is demonstrated on a region encompassing 6000 centerline miles (about 10,000km) of transportation corridors. The method will be of interest to managers investing in safety and performance of infrastructure systems, balancing safety, financial, and other criteria of concern for diverse stakeholders.}
}
@article{PUNZO201586,
title = {The role of 3-D interactive visualization in blind surveys of Hi in galaxies},
journal = {Astronomy and Computing},
volume = {12},
pages = {86-99},
year = {2015},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2015.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S221313371500058X},
author = {D. Punzo and J.M. {van der Hulst} and J.B.T.M. Roerdink and T.A. Oosterloo and M. Ramatsoku and M.A.W. Verheijen},
keywords = {Radio lines: galaxies, Galaxies: kinematics and dynamics, Surveys, Scientific visualization, Visual analytics},
abstract = {Upcoming Hi surveys will deliver large datasets, and automated processing using the full 3-D information (two positional dimensions and one spectral dimension) to find and characterize Hi objects is imperative. In this context, visualization is an essential tool for enabling qualitative and quantitative human control on an automated source finding and analysis pipeline. We discuss how Visual Analytics, the combination of automated data processing and human reasoning, creativity and intuition, supported by interactive visualization, enables flexible and fast interaction with the 3-D data, helping the astronomer to deal with the analysis of complex sources. 3-D visualization, coupled to modeling, provides additional capabilities helping the discovery and analysis of subtle structures in the 3-D domain. The requirements for a fully interactive visualization tool are: coupled 1-D/2-D/3-D visualization, quantitative and comparative capabilities, combined with supervised semi-automated analysis. Moreover, the source code must have the following characteristics for enabling collaborative work: open, modular, well documented, and well maintained. We review four state of-the-art, 3-D visualization packages assessing their capabilities and feasibility for use in the case of 3-D astronomical data.}
}
@incollection{LONIJ2016255,
title = {Chapter 8 - Cognitive Systems for the Food–Water–Energy Nexus},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {255-282},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S016971611630044X},
author = {V.P.A. Lonij and J.-B. Fiot},
keywords = {Cognitive computing, Machine learning, Resource management, Forecasting, Neural networks, Energy, Water, Food},
abstract = {Providing for the food, water, and energy needs of a growing world population is a grand challenge. The way we choose to address this challenge as a society will have far-reaching impacts, for instance, on public health, national security, and the global climate. The interactions between the food, water, and energy systems offer us an opportunity to improve efficiency, but can also make the system more complex. Cognitive systems can help mitigate this complexity and thus improve efficiency. What food, water, and energy have in common is that they are often not produced where they are consumed, they are costly to transport, and they are hard to store efficiently in large quantities. And this is where cognitive computing comes in: if you cannot store a resource you must have good forecasts of supply and demand. This requires handling large scale datasets from multiple sources, using machine learning methods to build forecasting models, and leveraging optimization techniques to help incorporate forecasting results into a decision-making process. We will use energy as an example throughout the bulk of this chapter with the understanding that the same methods, challenges, and solutions can be applied more broadly to food, water, and other constrained resources. Sense: We will first discuss methods to make the most of sensor data. For example, it is expensive to deploy large networks of ground sensors (e.g., weather stations), and it is therefore beneficial to make use of sensors that can cover large areas, like radar or satellite images. However, it is challenging to estimate ground conditions based on satellite measurements alone. We will discuss machine learning algorithms that can learn the mapping from wide area sensor data to local conditions based on only a few ground-truth measurements. Applications include, for example, to estimate rainfall based on radar measurements, or to estimate solar power generation based on satellite images of clouds. Predict: Next, we will discuss forecasting methods ranging from a few minutes ahead to days or even years ahead. We will discuss forecasting methods for energy demand, solar energy generation, and wind generation. Different methods are effective for these different technologies as well as different forecasting horizons. Very short-range forecasts might use autoregressive models, while mid- to long-range forecasts may require physical models. We also discuss hybrid methods that combine expert knowledge with machine learning. React: Finally, we will discuss how to use the outputs of these analytics tools to serve decision-making processes. How optimization can help improve infrastructure planning and economic dispatch of power generation. And how cognitive systems can help system operators to make sense of messy data from multiple sources, provide recommended actions, and enable better decision making. We will highlight several different mathematical methods, including autoregressive models, generalized additive models, fully connected neural networks, deep learning, convolutional neural networks, and nonlinear optimization in the context of energy. We will conclude the chapter with an outlook on how current trends in the cognitive computing will impact the broader challenge of managing constrained resources.}
}
@article{PENA2016242,
title = {Rule-based system to detect energy efficiency anomalies in smart buildings, a data mining approach},
journal = {Expert Systems with Applications},
volume = {56},
pages = {242-255},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416300938},
author = {Manuel Peña and Félix Biscarri and Juan Ignacio Guerrero and Iñigo Monedero and Carlos León},
keywords = {Energy efficiency, Smart building, Energy efficiency indicators, Analytics, Expert System, Decision support system},
abstract = {The rapidly growing world energy use already has concerns over the exhaustion of energy resources and heavy environmental impacts. As a result of these concerns, a trend of green and smart cities has been increasing. To respond to this increasing trend of smart cities with buildings every time more complex, in this paper we have proposed a new method to solve energy inefficiencies detection problem in smart buildings. This solution is based on a rule-based system developed through data mining techniques and applying the knowledge of energy efficiency experts. A set of useful energy efficiency indicators is also proposed to detect anomalies. The data mining system is developed through the knowledge extracted by a full set of building sensors. So, the results of this process provide a set of rules that are used as a part of a decision support system for the optimisation of energy consumption and the detection of anomalies in smart buildings.}
}
@article{COEBERGH2015997,
title = {EUROCOURSE lessons learned from and for population-based cancer registries in Europe and their programme owners: Improving performance by research programming for public health and clinical evaluation},
journal = {European Journal of Cancer},
volume = {51},
number = {9},
pages = {997-1017},
year = {2015},
note = {Cancer Registries in Europe for Cancer Surveillance and Research: EUROCOURSE Perspectives for Unity in Diversity},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2015.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0959804915002841},
author = {Jan Willem Coebergh and Corina {van den Hurk} and Stefano Rosso and Harry Comber and Hans Storm and Roberto Zanetti and Lidia Sacchetto and Maryska Janssen-Heijnen and Melissa Thong and Sabine Siesling and Janny {van den Eijnden-van Raaij}},
keywords = {Cancer registry, ERA-net, Programme owner, Linking databases, Epidemiology, Biobank, Mass screening evaluation, Data protection, Quality of oncological care, Survivorship},
abstract = {Population-based cancer registries (CRs) in Europe have played a supportive, sometimes guiding, role in describing geographic variation of cancer epidemics and comparisons of oncological practice and preventive interventions since the 1950s for all types of cancer, separate and simultaneously. This paper deals with historical and longitudinal developments of the roughly 160 CRs and their programme owners (POs) that emerged since 1927 and accelerating since the late 70s especially in southern and continental Europe. About 40 million newly diagnosed patients were recorded since the 1950s out of a total of 100 million of whom almost 20 million are still alive and about 10% annually dying from cancer. The perception of unity in diversity and suboptimal comparability in performance and governance of CRs was confirmed in the EUROCOURSE (EUROpe against cancer: Optimisation of the Use of Registries for Scientific Excellence in research) European Research Area (ERA)-net coordination FP7 project of the European Commission (EU) which explored best practices, bottlenecks and future challenges of CRs. Regional oncologic and public health changes but also academic embedding of CRs varied considerably, although Anno 2012 optimal cancer surveillance indeed demanded intensive collaboration with professional and institutional stakeholders in two major areas (public health and clinical research) and five minor overlapping cancer research domains: aetiologic research, mass screening evaluation, quality of care, translational prognostics and survivorship. Each of these domains address specific study questions, mixes of disciplines, methodologies, additional data-sources and funding mechanisms. POs tended to become more and more public health institutes, Health ministries, but also comprehensive cancer centres and cancer societies through more and more funding at project or programme basis. POs were not easy to pin down because of their multiple, sometimes competitive (funding) obligations and increasing complexity of cancer surveillance. But they also rather seemed to need guiding principles for Governance of ‘their’ CR(s) as well as to appreciate value of collaborative research in Europe and shield CRs against unreasonable data protection in case of linkages. Despite access to specialised care related shortcomings, especially of survival cohort studies, European databases for studies of incidence and survival (such as ACCIS and EUREG on the one hand and EUROCARE and RARECARE on the other hand) have proved to be powerful means for comparative national or regional cancer surveillance. Pooling of comparable data will exhibit much instructive variation in time and place. If POs of CRs would consider multinational European studies of risk and prognosis of cancer more to serve their own regional or national interest, then progress in this field will accelerate and lead to more consistent funding from the EU. The current 20 million cancer survivors and their care providers are likely to appreciate more feedback.
Conclusion
Most CRs remain uniquely able to report on progress against cancer by studies of variation in incidence (in time and place), detection and survival, referral and treatment patterns and their (side) effects in unselected patients, the latter especially in the (very) elderly. Programming and profiling its multiple and diverse clinical and prevention research is likely to promote involvement of public health and clinical stakeholders with a population-based research interest, increasingly patient groups and licensed ‘buyers’ of oncologic services.}
}
@incollection{2016683,
title = {Subject Index},
editor = {Jan L. Harrington},
booktitle = {Relational Database Design and Implementation (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
address = {Boston},
pages = {683-689},
year = {2016},
isbn = {978-0-12-804399-8},
doi = {https://doi.org/10.1016/B978-0-12-804399-8.00045-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128043998000454}
}
@article{DEHERT2012130,
title = {The proposed data protection Regulation replacing Directive 95/46/EC: A sound system for the protection of individuals},
journal = {Computer Law & Security Review},
volume = {28},
number = {2},
pages = {130-142},
year = {2012},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2012.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0267364912000295},
author = {Paul {De Hert} and Vagelis Papakonstantinou},
keywords = {EU Data Protection Directive, EU General Data Protection Regulation, Individual consent, DPIAs, The right to be forgotten, Data portability, Personal data breach notifications},
abstract = {The recent release by the European Commission of the first drafts for the amendment of the EU data protection regulatory framework is the culmination of a consulting and preparation process that lasted more than two years. At the same time, it opens up a law-making process that is intended to take at least as much time. The Commission has undertaken the herculean task to amend the whole EU data protection edifice, through the introduction of a General Data Protection Regulation, intended to replace the EU Data Protection Directive 95/46/EC, and a Police and Criminal Justice Data Protection Directive, intended to replace the Framework Decision 2008/977/JHA. This paper shall focus at the replacement of the EU Data Protection Directive by the draft General Data Protection Regulation. Due to the fact that the draft Regulation is a long (and ambitious) text, a selection has been made, with the aim of highlighting its treatment of basic data protection principles and elements, in order to identify merits and shortcomings for the general data protection purposes.}
}
@incollection{LUISI20141,
title = {Part I - Introduction},
editor = {James V. Luisi},
booktitle = {Pragmatic Enterprise Architecture},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-39},
year = {2014},
isbn = {978-0-12-800205-6},
doi = {https://doi.org/10.1016/B978-0-12-800205-6.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002056000019},
author = {James V. Luisi},
keywords = {Type of architecture, architecture, forms of standardization, reusable structures, accumulation of knowledge, relationships among components, taxonomy, symbology, senses, personal computers, midrange computers, laptops, smartphones, smart tablets, business-minded, business strategy, business vision, pet projects, pain points, ROI, business benefit, control, architectural challenges, acquisitions, mergers, shadow IT, lack of IT standards, nonstandard processes, competitive business, process improvement, component reuse, punch cards, inconsistent components, automation landscape, Big Data, open source, metrics, customer experience metrics, operational metrics, automated metrics collection, understanding ones' data, business glossary, business dictionary, percent of IT budget, informed decision making, business capabilities, layers of business capabilities, disaster recovery, complexity, understanding IT, number systems, written language, mechanical age, mental activities, fundamental principles, architectural disciplines, mindset of engineers, varieties of automation systems, control systems, information systems, mechanical equipment, virtual world control system, virtual control system, artificial intelligence, games, intangible, differences in error handling, error handling, process-oriented focus, differences in data and testing, sensory I/O devices, test bed, financial differences, mindset, customers, degrees of separation, enterprise architecture, centralized, decentralized, shared resources, stable team, SCRUM, shared vision, innovation, matrix organization, center of excellence, pooled architecture resources, enterprise architects, high price solutions, hype, role of enterprise architecture, frameworks, programmer analysts, solution architects, data modelers, information architects, The Hedgehog and the Fox, Archilochus, Isaiah Berlin, Hedgehog Concept, philosophical differentiators, business focus, development capabilities, agile staffing, synergistic multipurpose personnel, operations architecture, advanced business users, self-service is an accelerator, stakeholders frequently palaver, real transparency not virtual, business first, participators not spectators, new philosophy of enterprise architecture, taxonomy and organization of enterprise architecture, enterprise architecture frameworks, TOGAF, Zachman, business architecture and governance, information systems architecture and governance, information architecture and governance, control systems architecture and governance, operations architecture and governance, cross-discipline capabilities},
abstract = {This part sets the stage for the reader with some interesting observations during the evolution of automation that help put the rest of the book in context, beginning with one of the last large companies to adopt automation. We then get a glimpse into the challenges that automation created as it solved competitive challenges of handling a greater volume of business with less expense from labor. As with any form of modernization, as some problems are solved with advancements in technology, new problems emerge, and manifest themselves, occasionally overwhelming the organizations that failed to foresee or recognize these problems as they grew unchecked at their source. In fact, even today, many organizations leave the source of their problems unchecked by making attempting to remediate the symptoms with additional manual labor.}
}
@article{DANCIU201428,
title = {Secondary use of clinical data: The Vanderbilt approach},
journal = {Journal of Biomedical Informatics},
volume = {52},
pages = {28-35},
year = {2014},
note = {Special Section: Methods in Clinical Research Informatics},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414000392},
author = {Ioana Danciu and James D. Cowan and Melissa Basford and Xiaoming Wang and Alexander Saip and Susan Osgood and Jana Shirey-Rice and Jacqueline Kirby and Paul A. Harris},
keywords = {Biomedical informatics, Secondary use of clinical data, Research data warehouse, Research enterprise},
abstract = {The last decade has seen an exponential growth in the quantity of clinical data collected nationwide, triggering an increase in opportunities to reuse the data for biomedical research. The Vanderbilt research data warehouse framework consists of identified and de-identified clinical data repositories, fee-for-service custom services, and tools built atop the data layer to assist researchers across the enterprise. Providing resources dedicated to research initiatives benefits not only the research community, but also clinicians, patients and institutional leadership. This work provides a summary of our approach in the secondary use of clinical data for research domain, including a description of key components and a list of lessons learned, designed to assist others assembling similar services and infrastructure.}
}
@article{WRIGHT2014325,
title = {Ethical dilemma scenarios and emerging technologies},
journal = {Technological Forecasting and Social Change},
volume = {87},
pages = {325-336},
year = {2014},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2013.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0040162513003156},
author = {David Wright and Rachel Finn and Raphael Gellert and Serge Gutwirth and Philip Schütz and Michael Friedewald and Silvia Venier and Emilio Mordini},
keywords = {Ethical dilemma scenarios, Privacy impact assessment, Ethical impact assessment, Near field communications, Biometric technologies, Human enhancement, Drones},
abstract = {This paper posits that ethical dilemma scenarios are a useful instrument to provoke policy‐makers and other stakeholders, to including industry, in considering the privacy, ethical, social and other implications of new and emerging technologies. It describes a methodology for constructing and deconstructing such scenarios and provides four such scenarios in an orthogonal relationship with each other. The paper describes some different, but closely related scenario construction–deconstruction methodologies, which formed the basis for the methodology adopted in the European Commission-funded PRESCIENT project. The paper makes the point that in ethical dilemma scenarios, it is not immediately apparent what choices policy‐makers should select. Hence, there is a need for undertaking a privacy and ethical impact assessment and engaging stakeholders in the process to identify and discuss the issues raised in the scenarios.}
}
@incollection{KUMAR20143,
title = {Chapter 1 - Strategies and Structures of Financial Institutions},
editor = {Rajesh Kumar},
booktitle = {Strategies of Banks and Other Financial Institutions},
publisher = {Academic Press},
address = {San Diego},
pages = {3-30},
year = {2014},
isbn = {978-0-12-416997-5},
doi = {https://doi.org/10.1016/B978-0-12-416997-5.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124169975000014},
author = {Rajesh Kumar},
keywords = {Financial institutions, Strategic trends, Technology trends, Reform policy trends, Universal banking, Digital banking, Mobile banking, Remote deposit capture, Consolidation, Emerging markets, Corporate governance, Shadow banking},
abstract = {A financial sector comprises a set of institutions, instruments, and markets established in the context of a legal and regulatory framework. Financial institutions face challenges for gaining competitive advantage in the context of rapid changes in technological, economic, social, demographic, and regulatory environments. The deep transformation the financial sector is witnessing can be attributed to a number of factors such as technology innovation, deregulation, worldwide consolidation and restructuring, deregulation, and changing demographic profiles. Information technology is the primary force that keeps the financial industry dynamic. The post-economic crisis ­witnessed a series of policy reforms initiated by regulatory authorities. The global technology trends in the financial industry indicate the relevance of next-generation remote banking solutions, business intelligence, and analytics in transaction monitoring. Financial institutions face much complexity in the types of risks they have to manage. The trends indicate the growing significance of the emerging Asia market, consisting of China, India, and ASEAN countries for growth opportunities in the financial services industry.}
}
@article{SHERER2016570,
title = {Applying institutional theory to the adoption of electronic health records in the U.S.},
journal = {Information & Management},
volume = {53},
number = {5},
pages = {570-580},
year = {2016},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2016.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378720616000033},
author = {Susan A. Sherer and Chad D. Meyerhoefer and Lizhong Peng},
keywords = {Institutional theory, Electronic health records, IT adoption, Mimetic, Normative, Coercive},
abstract = {This study uses institutional theory to explain adoption of electronic health records (EHRs) in ambulatory medical practices in the U.S. Health care is a highly institutionalized industry, subject to multiple regulatory forces, high levels of professionalism, and growing network externalities that can influence adoption decisions. We found that mimetic forces were more critical predictors when there was greater uncertainty, coercive forces were significant predictors after the U.S. government established incentives, and normative forces have continually influenced adoption. This study demonstrates the impact of the institutional effect of government policies and industry norms on adoption of critical technologies.}
}
@article{DHAEN201669,
title = {Integrating expert knowledge and multilingual web crawling data in a lead qualification system},
journal = {Decision Support Systems},
volume = {82},
pages = {69-78},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S016792361500216X},
author = {J. D’Haen and D. {Van den Poel} and D. Thorleuchter and D.F. Benoit},
keywords = {Lead qualification, Multilingual text mining, Web crawling, Expert domain knowledge, Parameter optimization},
abstract = {Qualifying prospects as leads to contact is a complex exercise. Sales representatives often do not have the time or resources to rationally select the best leads to call. As a result, they rely on gut feeling and arbitrary rules to qualify leads. Model-based decision support systems make this process less subjective. Standard input for such an automated lead qualification system is commercial data. Commercial data, however, tends to be expensive and of ambiguous quality due to missing information. This study proposes web crawling data in combination with expert knowledge as an alternative. Web crawling data is freely available and of higher quality as it is generated by companies themselves. Potential customers use websites as a main information source, so companies benefit from correct and complete websites. Expert knowledge, on the other hand, augments web crawling data by inserting specific information. Web data consists of text that is converted to numbers using text mining techniques that make an abstraction of the text. A field experiment was conducted to test how a decision support system based on web crawling data and expert knowledge compares to a basic decision support system within an international energy retailer. Results verify the added value of the proposed approach.}
}
@incollection{CROSSWELL2014175,
title = {6.12 - EBI and ELIXIR},
editor = {Anders Brahme},
booktitle = {Comprehensive Biomedical Physics},
publisher = {Elsevier},
address = {Oxford},
pages = {175-190},
year = {2014},
isbn = {978-0-444-53633-4},
doi = {https://doi.org/10.1016/B978-0-444-53632-7.01123-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444536327011230},
author = {L.C. Crosswell and J.M. Thornton},
keywords = {Annotation, Big data, Bioinformatics, Curation, Database, Infrastructure, Ontology, Sequencing, Tools and standards},
abstract = {For 20 years, the European Molecular Biology Laboratory (EMBL)-the European Bioinformatics Institute (EMBL-EBI) has been providing biological data resources for life-science researchers and now hosts and maintains the world's largest collection of molecular databases. During the last two decades, biological data have increased exponentially in volume and complexity, spawning specialist roles for biologists and computer scientists under the new discipline of bioinformatics. The efficient management of huge volumes of data for Europe and beyond is now an important challenge that will be tackled collaboratively by the member states through ELIXIR, the distributed research infrastructure for life sciences data. This chapter charts the fast-moving pace of change in the creation and use of databases, their central role as a resource to enable scientists to find solutions to the collective grand challenges faced by Europe and the rest of the world, the pivotal role of the web in sharing these resources, and EMBL-EBI's role in the task of coordinating data.}
}
@article{SANTMANBERENDS2016103,
title = {Surveillance of cattle health in the Netherlands: Monitoring trends and developments using routinely collected cattle census data},
journal = {Preventive Veterinary Medicine},
volume = {134},
pages = {103-112},
year = {2016},
issn = {0167-5877},
doi = {https://doi.org/10.1016/j.prevetmed.2016.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167587716304275},
author = {I.M.G.A. Santman-Berends and H. Brouwer-Middelesch and L. {Van Wuijckhuise} and A.J.G. {de Bont-Smolenaars} and G. {Van Schaik}},
keywords = {Cattle health, Monitoring and surveillance systems, Trend analysis, Census data},
abstract = {Since 2002, a national cattle health surveillance system (CHSS) is in place that consists of several surveillance components. The CHSS combines enhanced passive reporting, diagnostic and post-mortem examinations, random surveys for prevalence estimation of endemic diseases and quarterly data analysis. The aim of the data-analysis component, which is called the Trend Analysis Surveillance Component (TASC), is to monitor trends and developments in cattle health using routine census data. The challenges that were faced during the development of TASC and the merits of this surveillance component are discussed, which might be of help to those who want to develop a monitoring and surveillance system that includes data analysis. When TASC was developed, there were process-oriented challenges and analytical related issues that had to be solved. Process-oriented challenges involved data availability, confidentiality, quality, uniformity and economic value of the data. Analytical issues involved data validation, aggregation and modeling. Eventually, the results had to provide information on cattle health that was intuitive to the stakeholders and that could support decision making. Within TASC, both quarterly analysis on census data and, on demand, additional in-depth analysis are performed. The key monitoring indicators that are analyzed as part of TASC all relate to cattle health and involve parameters such as mortality, fertility, udder health and antimicrobial usage. Population-Averaged Generalized Estimating Equations, with the appropriate distribution (i.e. Gaussian, Poisson, Negative Binomial or Binomial) and link function (independent, log or logit), are used for analysis. Both trends in time and associations between cattle health indicators and potential confounders are monitored, discussed and reported to the stakeholders on a quarterly level. The flexibility of the in-depth analyses provides the possibility to conduct additional analyses when anomalies in trends of cattle health occur or when developments in the cattle industry need further investigation. In addition, part of the budget for the in-depth analysis can also be used to improve the models or add new data sources. The TASC provides insight in cattle health parameters, it visualizes trends in time, can be used to support or nuance signals that are detected in one of the other surveillance components and can provide warnings or initiate changes in policy when unfavorable trends occur.}
}
@incollection{LINSTEDT2016195,
title = {Chapter 8 - Physical Data Warehouse Design},
editor = {Daniel Linstedt and Michael Olschimke},
booktitle = {Building a Scalable Data Warehouse with Data Vault 2.0},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {195-228},
year = {2016},
isbn = {978-0-12-802510-9},
doi = {https://doi.org/10.1016/B978-0-12-802510-9.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128025109000088},
author = {Daniel Linstedt and Michael Olschimke},
keywords = {hardware optimization, operating system, warehouse infrastructure, hardware, database},
abstract = {Data warehouse projects have special requirements for the physical architecture of the database system. These requirements distinguish data warehouse projects from operational data stores and are often underestimated. This chapter covers topics such as hardware optimization, optimization of the operating system, a “sales-pitch” for a dedicated data warehouse infrastructure (as opposed to adding the data warehouse to the existing, operational infrastructure), and some background information on hardware and database options. It also includes how to set up each individual layer of the data warehouse and the options available for each layer.}
}
@article{VALLEJOVAZ20161,
title = {Pooling and expanding registries of familial hypercholesterolaemia to assess gaps in care and improve disease management and outcomes: Rationale and design of the global EAS Familial Hypercholesterolaemia Studies Collaboration},
journal = {Atherosclerosis Supplements},
volume = {22},
pages = {1-32},
year = {2016},
issn = {1567-5688},
doi = {https://doi.org/10.1016/j.atherosclerosissup.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1567568816300496},
author = {Antonio J. Vallejo-Vaz and Asif Akram and Sreenivasa Rao {Kondapally Seshasai} and Della Cole and Gerald F. Watts and G. Kees Hovingh and John J.P. Kastelein and Pedro Mata and Frederick J. Raal and Raul D. Santos and Handrean Soran and Tomas Freiberger and Marianne Abifadel and Carlos A. Aguilar-Salinas and Fahad Alnouri and Rodrigo Alonso and Khalid Al-Rasadi and Maciej Banach and Martin P. Bogsrud and Mafalda Bourbon and Eric Bruckert and Josip Car and Richard Ceska and Pablo Corral and Olivier Descamps and Hans Dieplinger and Can T. Do and Ronen Durst and Marat V. Ezhov and Zlatko Fras and Dan Gaita and Isabel M. Gaspar and Jaques Genest and Mariko Harada-Shiba and Lixin Jiang and Meral Kayikcioglu and Carolyn S.P. Lam and Gustavs Latkovskis and Ulrich Laufs and Evangelos Liberopoulos and Jie Lin and Nan Lin and Vincent Maher and Nelson Majano and A. David Marais and Winfried März and Erkin Mirrakhimov and André R. Miserez and Olena Mitchenko and Hapizah Nawawi and Lennart Nilsson and Børge G. Nordestgaard and György Paragh and Zaneta Petrulioniene and Belma Pojskic and Željko Reiner and Amirhossein Sahebkar and Lourdes E. Santos and Heribert Schunkert and Abdullah Shehab and M. Naceur Slimane and Mario Stoll and Ta-Chen Su and Andrey Susekov and Myra Tilney and Brian Tomlinson and Alexandros D. Tselepis and Branislav Vohnout and Elisabeth Widén and Shizuya Yamashita and Alberico L. Catapano and Kausik K. Ray},
keywords = {Familial hypercholesterolaemia, LDL-Cholesterol, Cardiovascular disease, Registry, Study design, Familial Hypercholesterolaemia Studies Collaboration},
abstract = {Background
The potential for global collaborations to better inform public health policy regarding major non-communicable diseases has been successfully demonstrated by several large-scale international consortia. However, the true public health impact of familial hypercholesterolaemia (FH), a common genetic disorder associated with premature cardiovascular disease, is yet to be reliably ascertained using similar approaches. The European Atherosclerosis Society FH Studies Collaboration (EAS FHSC) is a new initiative of international stakeholders which will help establish a global FH registry to generate large-scale, robust data on the burden of FH worldwide.
Methods
The EAS FHSC will maximise the potential exploitation of currently available and future FH data (retrospective and prospective) by bringing together regional/national/international data sources with access to individuals with a clinical and/or genetic diagnosis of heterozygous or homozygous FH. A novel bespoke electronic platform and FH Data Warehouse will be developed to allow secure data sharing, validation, cleaning, pooling, harmonisation and analysis irrespective of the source or format. Standard statistical procedures will allow us to investigate cross-sectional associations, patterns of real-world practice, trends over time, and analyse risk and outcomes (e.g. cardiovascular outcomes, all-cause death), accounting for potential confounders and subgroup effects.
Conclusions
The EAS FHSC represents an excellent opportunity to integrate individual efforts across the world to tackle the global burden of FH. The information garnered from the registry will help reduce gaps in knowledge, inform best practices, assist in clinical trials design, support clinical guidelines and policies development, and ultimately improve the care of FH patients.}
}
@incollection{BASAK2015251,
title = {Chapter 11 - Current Landscape of Hierarchical QSAR Modeling and its Applications: Some Comments on the Importance of Mathematical Descriptors as well as Rigorous Statistical Methods of Model Building and Validation},
editor = {Subhash C. Basak and Guillermo Restrepo and José L. Villaveces},
booktitle = {Advances in Mathematical Chemistry and Applications},
publisher = {Bentham Science Publishers},
pages = {251-281},
year = {2015},
isbn = {978-1-68108-198-4},
doi = {https://doi.org/10.1016/B978-1-68108-198-4.50011-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781681081984500114},
author = {Subhash C. Basak and Subhabrata Majumdar},
keywords = {Property-activity relationship (PAR), graph theory, molecular graphs, weighted pseudograph, graph theoretic matrices, adjacency matrix, distance matrix, topological indices, topostructural indices, topochemical indices, information theoretic indices, connectivity indices, valence connectivity indices, E-state indices, quantum chemical descriptors, hierarchical quantitative structure-activity relationship (HiQSAR), partial least square (PLS), principal components regression (PCR), principal components analysis (PCA), ridge regression (RR), naïve , true , proper cross validation, leave one out (LOO) method, Envelope models, interrelated two-way clustering, linear discriminant analysis, mutagenicity, congenericity principle, diversity begets diversity principle, big data},
abstract = {Mathematical chemistry or more accurately discrete mathematical chemistry had a tremendous growth spurt in the second half of the twentieth century and the same trend is continuing in the twenty first century. This continual growth was fueled primarily by two major factors: 1) Novel applications of discrete mathematical concepts to chemical and biological systems, and 2) Availability of high speed computers and relevant software whereby hypothesis driven as well as discovery oriented research on large data sets could be carried out. This led to the development of not only a plethora of new concepts, but also to various useful applications. This chapter will discuss the major milestones in the development of hierarchical QSARs for the prediction of physical as well as biological properties of various classes of chemicals by the Basak group of researchers using mathematical descriptors and different statistical methods.}
}
@incollection{2015351,
title = {Index},
editor = {Peter Schüler and Brendan Buckley},
booktitle = {Re-Engineering Clinical Trials},
publisher = {Academic Press},
address = {Boston},
pages = {351-360},
year = {2015},
isbn = {978-0-12-420246-7},
doi = {https://doi.org/10.1016/B978-0-12-420246-7.18001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124202467180016}
}
@incollection{SHEIKH2013167,
title = {Chapter 10 - Analytics Organization and Architecture},
editor = {Nauman Sheikh},
booktitle = {Implementing Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {167-183},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-401696-5},
doi = {https://doi.org/10.1016/B978-0-12-401696-5.00010-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124016965000104},
author = {Nauman Sheikh},
keywords = {business intelligence competency center (BICC), analytics analyst, analytics modeler, decision strategy, model, scoring, ETL, roles and responsibilities, analytics skills, analytics technical architecture},
abstract = {This chapter covers two topics—organization structure and technical architecture—the final pieces in building a strong team and foundation for analytics projects in an organization. The purpose and theme of these two topics remains consistent with the rest of the book and are focused toward simplification and democratization. Both the organization structure and technical architecture are built on top of the existing data warehouse organization and architecture.}
}
@incollection{CHIAUZZI2016307,
title = {Chapter 16 - Crowdsourcing Advancements in Health Care Research: Applications for Cancer Treatment Discoveries},
editor = {Bradford W. Hesse and David K. Ahern and Ellen Beckjord},
booktitle = {Oncology Informatics},
publisher = {Academic Press},
address = {Boston},
pages = {307-329},
year = {2016},
isbn = {978-0-12-802115-6},
doi = {https://doi.org/10.1016/B978-0-12-802115-6.00016-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021156000161},
author = {Emil Chiauzzi and Gabriel Eichler and Paul Wicks},
keywords = {Crowdsourcing, genomics, participant-led research, cancer diagnostics, drug discovery, quantified self, informed consent},
abstract = {The field of oncology is challenged by the rise in cancer cases and the future needs of survivors. Unfortunately, traditional research and development methodologies are limited in their ability to address patient needs quickly and effectively, and advance patient-centered research and care. Crowdsourcing offers a methodological approach that is patient-centered and inclusive while providing more rapid discovery, development, and delivery of cancer treatments. This chapter reviews the history of crowdsourcing, highlights examples of such applications that can be applied in cancer research, discusses methodological limitations and ethical considerations, and offers an optimistic vision of the future of crowdsourcing in oncology.}
}
@article{RANKIN2016100,
title = {Towards a retrospective political geography of border studies: Partition and division in Ireland},
journal = {Political Geography},
volume = {51},
pages = {100-103},
year = {2016},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2015.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0962629815000591},
author = {K.J. Rankin}
}
@article{VANVLASSELAER201538,
title = {APATE: A novel approach for automated credit card transaction fraud detection using network-based extensions},
journal = {Decision Support Systems},
volume = {75},
pages = {38-48},
year = {2015},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2015.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167923615000846},
author = {Véronique {Van Vlasselaer} and Cristián Bravo and Olivier Caelen and Tina Eliassi-Rad and Leman Akoglu and Monique Snoeck and Bart Baesens},
keywords = {Credit card transaction fraud, Network analysis, Bipartite graphs, Supervised learning},
abstract = {In the last decade, the ease of online payment has opened up many new opportunities for e-commerce, lowering the geographical boundaries for retail. While e-commerce is still gaining popularity, it is also the playground of fraudsters who try to misuse the transparency of online purchases and the transfer of credit card records. This paper proposes APATE, a novel approach to detect fraudulent credit card transactions conducted in online stores. Our approach combines (1) intrinsic features derived from the characteristics of incoming transactions and the customer spending history using the fundamentals of RFM (Recency–Frequency–Monetary); and (2) network-based features by exploiting the network of credit card holders and merchants and deriving a time-dependent suspiciousness score for each network object. Our results show that both intrinsic and network-based features are two strongly intertwined sides of the same picture. The combination of these two types of features leads to the best performing models which reach AUC-scores higher than 0.98.}
}
@article{HADDARA2015721,
title = {The Readiness of ERP Systems for the Factory of the Future},
journal = {Procedia Computer Science},
volume = {64},
pages = {721-728},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.598},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915027337},
author = {Moutaz Haddara and Ahmed Elragal},
keywords = {ERP, Factory of the future, Industry 4.0, Multiple case study},
abstract = {In 2011, at the Hanover Fair, the term Industry 4.0 was first coined. In October 2012, the Working Group on Industry 4.0, presented a set of implementation recommendations to the German government. The term Industry 4.0 initiates from a project in the high-tech strategy of the German government. Such project advocates the computerization of the manufacturing industry. It is also known as the 4th industrial revolution. Precisely speaking, industry 4.0 is based on the technological concepts of cyber-physical systems, Internet of Things (IoT), which enables the Factory of the Future (FoF). Within the modular structured smart factories of Industry 4.0, cyber-physical systems monitor physical processes, create a virtual copy of the physical world and make decentralized decisions. Over the IoT, Cyber-physical systems communicate and cooperate with each other and with humans in real time. Enterprise resource planning (ERP) systems are considered the backbone for the Industry 4.0. Thus, this paper attempts to answer the research question: “Are today's ERP systems ready for the FoF?”. We have conducted interviews with manufacturers, ERP vendors, and partners in order to seek their feedback on the readiness of ERP systems for the FoF. Our results show that ERP systems are ready for the FoF.}
}
@article{BOLOGVA2016915,
title = {Human-Computer Interaction in Electronic Medical Records: From the Perspectives of Physicians and Data Scientists},
journal = {Procedia Computer Science},
volume = {100},
pages = {915-920},
year = {2016},
note = {International Conference on ENTERprise Information Systems/International Conference on Project MANagement/International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.248},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324176},
author = {Ekaterina V. Bologva and Diana I. Prokusheva and Alexey V. Krikunov and Nadezhda E. Zvartau and Sergey V. Kovalchuk},
keywords = {electronic health records, human-coumputer interaction, healthcare quality, medical data analysis, clinical disicion support systems},
abstract = {This study investigated the most common challenges of human computer interaction (HCI) while using electronic medical records (EMR) based on the experience of a large Russian medical research center. Inadequate HCI may have a dramatic effect on the quality of data stored in the electronic medical system. We identified the most common classes of mistakes that emerge because of poor HCI design in EMR. Possible consequences of such mistakes are discussed from clinical and data science perspectives. Integration of specially designed clinical decision support system (СDSS) is considered as a possible way to improve HCI with subsequent increase of the EMR quality. This study is a part of a larger project to develop complex CDSS on cardiovascular disorders for medical research centers.}
}
@incollection{HOLMBERG20169,
title = {1 - The Past},
editor = {Kim Holmberg},
booktitle = {Altmetrics for Information Professionals},
publisher = {Chandos Publishing},
pages = {9-54},
year = {2016},
isbn = {978-0-08-100273-5},
doi = {https://doi.org/10.1016/B978-0-08-100273-5.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002735000016},
author = {Kim Holmberg},
keywords = {scholarly communication, bibliometrics, scientometrics, informetrics, methods, bibliometric laws, citation analysis, co-citation analysis, co-word analysis, social network},
abstract = {The first part of the book will discuss the past of altmetrics, its origins, its scientific roots, and its connection with bibliometrics and webometrics. In many aspects the past of altmetrics is also the past of bibliometrics and webometrics, but it needs to be emphasized that the beginning of altmetrics does not mean the end of bibliometrics or webometrics. The three research areas are developing side by side, learning from each other, complementing each other. This part of the book will give an overview of scholarly communication and the research methods involved in “counting, measuring, and weighing” it, namely bibliometrics and more recently, after the advent of the web, webometrics to analyze scholarly communication on the web. The shortcomings and pitfalls of bibliometrics in research evaluation will be discussed and the current standards and practices for most reliable bibliometric analyses will be presented. With that, the technical developments and societal changes that paved the way for altmetrics will be presented. This part will end by focusing on developments in social media, which, as an increasingly important place for scholarly communication, has made altmetrics possible.}
}
@article{OHEMENG2015419,
title = {One way traffic: The open data initiative project and the need for an effective demand side initiative in Ghana},
journal = {Government Information Quarterly},
volume = {32},
number = {4},
pages = {419-428},
year = {2015},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X15000908},
author = {Frank L.K. Ohemeng and Kwaku Ofosu-Adarkwa},
keywords = {Developing countries, Ghana, Open data, Supply, Demand},
abstract = {In recent years the necessity for governments to develop new public values of openness and transparency, and thereby increase their citizenries' sense of inclusiveness, and their trust in and confidence about their governments, has risen to the point of urgency. The decline of trust in governments, especially in developing countries, has been unprecedented and continuous. A new paradigm that signifies a shift to citizen-driven initiatives over and above state- and market-centric ones calls for innovative thinking that requires openness in government. The need for this new synergy notwithstanding, Open Government cannot be considered truly open unless it also enhances citizen participation and engagement. The Ghana Open Data Initiative (GODI) project strives to create an open data community that will enable government (supply side) and civil society in general (demand side) to exchange data and information. We argue that the GODI is too narrowly focused on the supply side of the project, and suggest that it should generate an even platform to improve interaction between government and citizens to ensure a balance in knowledge sharing with and among all constituencies.}
}
@article{BECK2016825,
title = {Cryo-Electron Tomography: Can it Reveal the Molecular Sociology of Cells in Atomic Detail?},
journal = {Trends in Cell Biology},
volume = {26},
number = {11},
pages = {825-837},
year = {2016},
note = {Special Issue: Future of Cell Biology},
issn = {0962-8924},
doi = {https://doi.org/10.1016/j.tcb.2016.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0962892416301234},
author = {Martin Beck and Wolfgang Baumeister},
abstract = {Traditionally, macromolecular structure determination is performed ex situ, that is, with purified materials. But, there are strong incentives to develop approaches to study them in situ in their native functional context. In recent years, cryo-electron tomography (cryo-ET) has emerged as a powerful method for visualizing the molecular organization of unperturbed cellular landscapes with the potential to attain near-atomic resolution. Here, we review recent work on several macromolecular assemblies, demonstrating the power of in situ studies. We also highlight technical challenges and discuss ways to meet them.}
}
@article{ROSA201541,
title = {Using e-technologies in clinical trials},
journal = {Contemporary Clinical Trials},
volume = {45},
pages = {41-54},
year = {2015},
note = {10th Anniversary Special Issue},
issn = {1551-7144},
doi = {https://doi.org/10.1016/j.cct.2015.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1551714415300446},
author = {Carmen Rosa and Aimee N.C. Campbell and Gloria M. Miele and Meg Brunner and Erin L. Winstanley},
keywords = {Clinical trials, E-technology, Social media, Apps, Smartphones, Internet},
abstract = {Clinical trials have been slow to incorporate e-technology (digital and electronic technology that utilizes mobile devices or the Internet) into the design and execution of studies. In the meantime, individuals and corporations are relying more on electronic platforms and most have incorporated such technology into their daily lives. This paper provides a general overview of the use of e-technologies in clinical trials research, specifically within the last decade, marked by rapid growth of mobile and Internet-based tools. Benefits of and challenges to the use of e-technologies in data collection, recruitment and retention, delivery of interventions, and dissemination are provided, as well as a description of the current status of regulatory oversight of e-technologies in clinical trials research. As an example of ways in which e-technologies can be used for intervention delivery, a summary of e-technologies for treatment of substance use disorders is presented. Using e-technologies to design and implement clinical trials has the potential to reach a wide audience, making trials more efficient while also reducing costs; however, researchers should be cautious when adopting these tools given the many challenges in using new technologies, as well as threats to participant privacy/confidentiality. Challenges of using e-technologies can be overcome with careful planning, useful partnerships, and forethought. The role of web- and smartphone-based applications is expanding, and the increasing use of those platforms by scientists and the public alike make them tools that cannot be ignored.}
}
@article{JOLY20161150,
title = {Are Data Sharing and Privacy Protection Mutually Exclusive?},
journal = {Cell},
volume = {167},
number = {5},
pages = {1150-1154},
year = {2016},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2016.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0092867416315252},
author = {Yann Joly and Stephanie O.M. Dyke and Bartha M. Knoppers and Tomi Pastinen},
abstract = {We review emerging strategies to protect the privacy of research participants in international epigenome research: open consent, genome donation, registered access, automated procedures, and privacy-enhancing technologies.}
}
@article{JUNG2015353,
title = {A semantic (TRIZ) network analysis of South Korea's “Open Public Data” policy},
journal = {Government Information Quarterly},
volume = {32},
number = {3},
pages = {353-358},
year = {2015},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2015.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X1500057X},
author = {Kyujin Jung and Han Woo Park},
keywords = {Open Public Data, Semantic network, TRIZ, South Korea},
abstract = {This study provides an overview of the background and content of the Open Public Data Directive (OPDD), a long-term plan to implement South Korea's “open public data” policy from 2013 to 2017. The OPDD and the principles of social use are the main policy documents serving as a framework for comparing and evaluating policies on open public data of 34 government organizations in South Korea. That is, these 34 organizations have been directed to refer to the OPDD as official or authoritative guidelines for planning their own programs. To examine South Korea's open public data policy, a semantic (TRIZ) network analysis was conducted, and a preliminary descriptive analysis of the OPPD was implemented. For the OPPD, a total of 4162 keywords were identified based on the space between them. Among these, 118 were used at least 10 times (2.84%), and 49 occurred at least 20 times (1.12%). Therefore, the analysis focused on the most frequently occurring 100 keywords that further functioned as nodes in the semantic network analysis. The results for key problems and solutions expressed in the OPDD suggest that the open public data policy should explicitly incorporate the importance of a creative economy ecosystem to facilitate creative industries based on innovation and its diffusion through new types of products.}
}
@article{TOURASSI2016110,
title = {A novel web informatics approach for automated surveillance of cancer mortality trends},
journal = {Journal of Biomedical Informatics},
volume = {61},
pages = {110-118},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416300181},
author = {Georgia Tourassi and Hong-Jun Yoon and Songhua Xu},
keywords = {Web informatics, Web mining, Digital epidemiology, Cancer mortality, Breast cancer, Lung cancer},
abstract = {Cancer surveillance data are collected every year in the United States via the National Program of Cancer Registries (NPCR) and the Surveillance, Epidemiology and End Results (SEER) Program of the National Cancer Institute (NCI). General trends are closely monitored to measure the nation’s progress against cancer. The objective of this study was to apply a novel web informatics approach for enabling fully automated monitoring of cancer mortality trends. The approach involves automated collection and text mining of online obituaries to derive the age distribution, geospatial, and temporal trends of cancer deaths in the US. Using breast and lung cancer as examples, we mined 23,850 cancer-related and 413,024 general online obituaries spanning the timeframe 2008–2012. There was high correlation between the web-derived mortality trends and the official surveillance statistics reported by NCI with respect to the age distribution (ρ=0.981 for breast; ρ=0.994 for lung), the geospatial distribution (ρ=0.939 for breast; ρ=0.881 for lung), and the annual rates of cancer deaths (ρ=0.661 for breast; ρ=0.839 for lung). Additional experiments investigated the effect of sample size on the consistency of the web-based findings. Overall, our study findings support web informatics as a promising, cost-effective way to dynamically monitor spatiotemporal cancer mortality trends.}
}
@article{SEN2016325,
title = {GIST 2.0: A scalable multi-trait metric for quantifying population representativeness of individual clinical studies},
journal = {Journal of Biomedical Informatics},
volume = {63},
pages = {325-336},
year = {2016},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416301150},
author = {Anando Sen and Shreya Chakrabarti and Andrew Goldstein and Shuang Wang and Patrick B. Ryan and Chunhua Weng},
keywords = {Clinical trials, Generalizability, Population representativeness, Trait dependencies, Eligibility criteria},
abstract = {The design of randomized controlled clinical studies can greatly benefit from iterative assessments of population representativeness of eligibility criteria. We propose a multi-trait metric - GIST 2.0 that can compute the a priori generalizability based on the population representativeness of a clinical study by explicitly modeling the dependencies among all eligibility criteria. We evaluate this metric on twenty clinical studies of two diseases and analyze how a study’s eligibility criteria affect its generalizability (collectively and individually). We statistically analyze the effects of trial setting, trait selection and trait summarizing technique on GIST 2.0. Finally we provide theoretical as well as empirical validations for the expected properties of GIST 2.0.}
}
@article{TORRENCE20151,
title = {Forty years and still growing: Journal of Archaeological Science looks to the future},
journal = {Journal of Archaeological Science},
volume = {56},
pages = {1-8},
year = {2015},
note = {Scoping the Future of Archaeological Science: Papers in Honour of Richard Klein},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2015.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0305440315000850},
author = {Robin Torrence and Marcos Martinón-Torres and Th. Rehren},
keywords = {Archaeological science, Archaeology, Theory, Reproducibility, Disciplinary hygiene, Global access},
abstract = {This special issue honours Richard Klein's outstanding contributions to archaeology through his seminal role as a senior editor for the Journal of Archaeological Science (JAS). The papers presented here assess achievements in archaeological science during the 40 years of research since JAS began, and scope the future within evolutionary and social theory in archaeology and across the fields of dating, aDNA, environmental reconstruction, diet, subsistence, artefact technology and function, and provenancing. Science is shown to be integral to archaeology as a whole, but challenges are identified particularly in the continuing search for new methods to answer key questions and the maintenance of rigour, significance, sustainability and social responsibility.}
}
@incollection{BISWAS20149,
title = {Chapter 2 - The Pharmaceutical Value Chain—An Introduction},
editor = {Kamal Biswas},
booktitle = {Pharma's Prescription},
publisher = {Academic Press},
address = {San Diego},
pages = {9-65},
year = {2014},
isbn = {978-0-12-407662-4},
doi = {https://doi.org/10.1016/B978-0-12-407662-4.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124076624000027},
author = {Kamal Biswas},
keywords = {clinical research, clinical trial, drug approval, labeling, manufacturing, marketing, human resources, investigational new drug, process R&D, quality control, regulatory compliance, standardization, supply chain management, pharmaceutical value chain, pharmaceutical R&D, pharmaceutical manufacturing, pharmaceutical sales & marketing, contract manufacturing organization, FDA, sildenafil citrate, health authority, Active Pharmaceutical Ingredient, ADME, ANDA, IND, NDA, pharmaceutical big data, blockbuster drugs, California e-Pedigree law, Sunshine Act, CAPA, Certificate of Analysis, target identification, target validation, lead identification, PK/PD, IRB, CRO, protocol design, clinical trial management, clinical supply management, chemical synthesis, fermentation, health, safety and environment, digital marketing, multi-channel marketing, finance, accounting, product cost, Patient Protection and Affordable Care Act, sample management, ICH, TGA, 21 CFR Part 11, computer system validation, HIPAA, pharmaceutical industry trends, pharmaceutical audit},
abstract = {We can segment the pharmaceutical value chain into five broad areas: Research & Development, Manufacturing & Supply Chain, Sales & Marketing, Regulatory, and Corporate Functions. This chapter focuses on each of these areas in turn. We describe each function in detail in order to explain what it delivers, what processes it follows, and what types of input and stakeholder interactions it needs. This chapter goes beyond the process areas as well, describing the operating challenges and how technologies improve operations. A few examples are the technology-driven resourcing of chemical entities, the automation of CAPA, and the manufacturing of shop-floor analytics for faster decision-making. Several industry stories explain improvement opportunities and how to achieve them. This chapter also explains various regulatory compliance requirements and, by explaining the genesis of these regulations, shows why it is important to comply with them.}
}
@article{DESASSI201593,
title = {Towards integrated monitoring of REDD+},
journal = {Current Opinion in Environmental Sustainability},
volume = {14},
pages = {93-100},
year = {2015},
note = {Open Issue},
issn = {1877-3435},
doi = {https://doi.org/10.1016/j.cosust.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S187734351500041X},
author = {Claudio {de Sassi} and Shijo Joseph and Astrid B Bos and Amy E Duchelle and Ashwin Ravikumar and Martin Herold},
abstract = {Monitoring socioecological impacts of policy interventions aimed at changing land-use practices is a major challenge in sustainable development and conservation. Reducing emissions from deforestation and forest degradation (REDD+) intends to compensate local stakeholders for demonstrated carbon emission reduction and increased removals accounted for internationally, while promoting social and environmental benefits locally. Thus, monitoring REDD+ inherently requires the use of interdisciplinary data at different scales. Forest carbon monitoring, central to REDD+, is considerably advanced, yet the progress on social and environmental monitoring systems is uneven. We argue that scalar and interdisciplinary integration of REDD+ monitoring is crucial to uncover and understand trade-offs and synergies on which effectiveness, efficiency and equity of REDD+ may depend. We review previous efforts in integrating environmental and social monitoring, as well as efforts specific to REDD+, and discuss how old and new knowledge can contribute towards integrated monitoring. We observe that there are many challenges, but strong advantages, in an integrated monitoring approach. The current emergence of diverging standards and methodologies with narrow focus can inform future integrative efforts but could, in the long run, hinder coherence in national processes. We conclude that recent technological advances open new opportunities to integrate information across scale and disciplines, leveraging and combining existing data with targeted additional measures. The application of mixed methods in data collection can foster integration, in particular from the local level upwards. However, this requires greater coordination at higher levels to efficiently upscale multiple data streams. The unequal standpoint of carbon, social and environmental monitoring efforts provide a timely opportunity to promote integration, learn from advances in carbon monitoring, and build on existing and emerging platforms and tools that are locally to globally relevant.}
}
@article{RODRIGUEZBILBAO2015605,
title = {Evaluation of Precise Point Positioning accuracy under large total electron content variations in equatorial latitudes},
journal = {Advances in Space Research},
volume = {55},
number = {2},
pages = {605-616},
year = {2015},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2014.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0273117714006942},
author = {I. Rodríguez-Bilbao and B. {Moreno Monge} and G. Rodríguez-Caderot and M. Herraiz and S.M. Radicella},
keywords = {Ionosphere, GNSS, PPP, Equatorial plasma irregularities},
abstract = {The ionosphere is one of the largest contributors to errors in GNSS positioning. Although in Precise Point Positioning (PPP) the ionospheric delay is corrected to a first order through the ‘iono-free combination’, significant errors may still be observed when large electron density gradients are present. To confirm this phenomenon, the temporal behavior of intense fluctuations of total electron content (TEC) and PPP altitude accuracy at equatorial latitudes are analyzed during four years of different solar activity. For this purpose, equatorial plasma irregularities are identified with periods of high rate of change of TEC (ROT). The largest ROT values are observed from 19:00 to 01:00LT, especially around magnetic equinoxes, although some differences exist between the stations depending on their location. Highest ROT values are observed in the American and African regions. In general, large ROT events are accompanied by frequent satellite signal losses and an increase in the PPP altitude error during years 2001, 2004 and 2011. A significant increase in the PPP altitude error RMS is observed in epochs of high ROT with respect to epochs of low ROT in years 2001, 2004 and 2011, reaching up to 0.26m in the 19:00–01:00LT period.}
}
@article{WOJTOWICZ20161,
title = {Digital images authentication scheme based on bimodal biometric watermarking in an independent domain},
journal = {Journal of Visual Communication and Image Representation},
volume = {38},
pages = {1-10},
year = {2016},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2016.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1047320316000328},
author = {Wioletta Wójtowicz and Marek R. Ogiela},
keywords = {Image watermarking, Image authentication, Biometric watermarking, Independent Component Analysis (ICA), Biometric verification, Multibiometrics, Fingerprint recognition, Iris recognition},
abstract = {With the growing accessibility and usability of internet there is a growing concern over content protection of digital images. Recently, to eliminate the traditional use of passwords and to ensure that the access to the image is restricted only to legitimate users, security solutions are increasingly combined with biometrics. Consequently, biometric-based watermarking algorithms, that involve embedding the identity of the owner, are proposed to solve ownership disputes. This paper presents a new scheme for protecting and authenticating invisibly watermarked digital images. It applies Independent Component Analysis to the cover image and enables the insertion of two independent watermarks based on fingerprint and iris biometrics. In this approach biometric techniques are used for watermarks generation and for owners authentication. The main advantage of proposed algorithm is construction of ICA based watermarking domain to enable insertion of two independent watermarks, that improve authentication accuracy and makes scheme more robust.}
}
@article{BHATT20151926,
title = {ACC/AHA/STS Statement on the Future of Registries and the Performance Measurement Enterprise: A Report of the American College of Cardiology/American Heart Association Task Force on Performance Measures and The Society of Thoracic Surgeons},
journal = {The Annals of Thoracic Surgery},
volume = {100},
number = {5},
pages = {1926-1941},
year = {2015},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2015.07.078},
url = {https://www.sciencedirect.com/science/article/pii/S0003497515012916},
author = {Deepak L. Bhatt and Joseph P. Drozda and David M. Shahian and Paul S. Chan and Gregg C. Fonarow and Paul A. Heidenreich and Jeffrey P. Jacobs and Frederick A. Masoudi and Eric D. Peterson and Karl F. Welke}
}
@incollection{WISE201245,
title = {Chapter 5 - The increasing popularity of OS},
editor = {Lyndsay Wise},
booktitle = {Using Open Source Platforms for Business Intelligence},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {45-54},
year = {2012},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-415811-5},
doi = {https://doi.org/10.1016/B978-0-12-415811-5.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124158115000056},
author = {Lyndsay Wise}
}
@incollection{HUGHES2016503,
title = {Chapter 19 - The Agile EDW Subrelease Cycle},
editor = {Ralph Hughes},
booktitle = {Agile Data Warehousing for the Enterprise},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {503-520},
year = {2016},
isbn = {978-0-12-396464-9},
doi = {https://doi.org/10.1016/B978-0-12-396464-9.00019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964649000199},
author = {Ralph Hughes},
keywords = {Agile enterprise data warehousing, business intelligence and data analytics, value cycle, data discovery, prototyping, machine-driven development, collaborative analytics, data governance, enterprise information management, quality assurance, agile manifesto},
abstract = {The subrelease value cycle for agile enterprise data warehousing provides eight repeatable steps that EDW teams follow to create meaningful, incremental deliverables during large projects. Steps include machine-assisted bursts of data discovery, application prototyping, defining data dictionaries, ETL code generation, and collaborative analytics. EDW teams can embed data governance and quality assurance functions in the heart of this subrelease cycle so that it extensively supports the enterprise information management work patterns that many companies follow. With agile EDW techniques allowing fast and well-governed development, team leaders can offer their stakeholders an EDW customer’s bill of rights so that project sponsors will know what they can reasonably demand from their DW/BI development teams. Leaders can ensure their team delivers on that bill of rights by properly focusing their developers’ activities using a version of the agile manifesto adapted for data warehousing and business intelligence projects.}
}
@article{BANARESCU20151827,
title = {Detecting and Preventing Fraud with Data Analytics},
journal = {Procedia Economics and Finance},
volume = {32},
pages = {1827-1836},
year = {2015},
note = {Emerging Markets Queries in Finance and Business 2014, EMQFB 2014, 24-25 October 2014, Bucharest, Romania},
issn = {2212-5671},
doi = {https://doi.org/10.1016/S2212-5671(15)01485-9},
url = {https://www.sciencedirect.com/science/article/pii/S2212567115014859},
author = {Adrian Bănărescu},
keywords = {data analytics, preventing fraud, IT systems.},
abstract = {Although fraud is not a new issue, the current financial crisis has enlightened that fraud occurs mainly during a recession, as compared with normal periods of economic growth. In counterbalance to the slow economic recovery, managers need to start a series of antifraud measures, as a leverage of cost control, while reducing available resources. Fraud involves inclusively significant financial risks which may threaten profitability, and the image of an economic entity. In these circumstances, in which development of the IT systems plays a central role in the creation of competitive companies, the amount of processed data has grown exponentially. Internal control team members should need to look at every transaction that takes place, but, unfortunately this issue can no longer be manually performed, requiring the use of data analysis tools and programs. Since the companies usually operate with large volumes of data, it is absolutely necessary to implement such processes of continuous monitoring, in order to identify anomalies in the data stream or behavioral patterns, potentially fraudulent. Such new and significant information will be later used in directing investigations, as well as to make recommendations to improve the control activities. We strive to provide an overview of the way in which technology can be implemented to improve fraud prevention and detection, inside of a public or private economic entity.}
}
@incollection{NETTLETON2014159,
title = {Chapter 10 - Deployment Systems: From Query Reporting to EIS and Expert Systems},
editor = {David Nettleton},
booktitle = {Commercial Data Mining},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {159-170},
year = {2014},
isbn = {978-0-12-416602-8},
doi = {https://doi.org/10.1016/B978-0-12-416602-8.00010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166028000108},
author = {David Nettleton},
keywords = {querying, report generation, executive information systems},
abstract = {This chapter reviews the various ways that the results of data mining, such as predictive models, rules, and analysis results, can be presented in a way that allows a business to feed those results directly into its decision-making and operative processes. First, simple graphic and reporting formats are addressed, followed by more sophisticated executive information system interfaces and a discussion of how the results can be integrated into the business operations, such as CRM systems and call centers. Finally, the chapter looks at more complex applications, such as expert systems and case-based systems.}
}
@article{CANAUD201528,
title = {A Realistic Case Study for Comparison of Data Fusion and Assimilation on an Urban Network – The Archipel Platform},
journal = {Transportation Research Procedia},
volume = {6},
pages = {28-49},
year = {2015},
note = {4th International Symposium of Transport Simulation (ISTS'14) Selected Proceedings, Ajaccio, France, 1-4 June 2014},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2015.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S2352146515000320},
author = {M. Canaud and A. Nabavi and C. Bécarie and D. Villegas and N-E El Faouzi},
keywords = {ISpace&Time, Archipel, Symuvia, Data fusion, traffic state estimation, urban network},
abstract = {Achievements from conducted research as part of the team of the Laboratory of Traffic Engineering Transportation (LICIT) of IFSTTAR are integrated in platforms in order to develop and test new modules directly on real data. The ARCHIPEL platform focuses on data assimilation and fusion from multiple sources. The objective of ARCHIPEL is therefore to provide tools to process real-time data from different sources. This platform allows one to test and compare different approaches of assimilation and data fusion: loops, GPS, Bluetooth. Besides it offers to view details of traffic conditions in the form of space-time diagram on accurate and successive areas or larger areas by traficolor representation. In the context of ISpace&Time project, the city of Paris has been finely modelled in a traffic simulator developed by the LICIT, called SymuVia. This permits to have access to a case study in which we know all vehicle positions. In order to explore which fusion method is the most suitable depending on the context e.g. penetration rate and data noises, this realistic data has been put in the platform database and is utilized to give a complete overview of the more relevant methodology to apply. In this work, we propose some fusion scenarios and present the comparison results with respect to different conditions. The final aim is to give a roadmap of the more accurate solutions, since the ground truth is exactly known.}
}
@article{HSIEH2015101,
title = {Check the phone book: Testing information and communication technology (ICT) recall aids for personal network surveys},
journal = {Social Networks},
volume = {41},
pages = {101-112},
year = {2015},
issn = {0378-8733},
doi = {https://doi.org/10.1016/j.socnet.2014.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378873314000744},
author = {Yuli Patrick Hsieh},
keywords = {Name generator, Egocentric networks, Memory aids, Survey design, ICT use},
abstract = {This study tested two recall aids for the name generator procedure via a randomized web experiment with 447 college students, eliciting their personal networks. Compared to participants solely presented with the name generator, participants being prompted and probed to consult records saved in their communication devices provided more comprehensive network data and more weak ties. Furthermore, these data were garnered without either a substantial increase in item nonresponse or a decrease in completion time for subsequent name interpreters. Thus, ICT recall aids are deemed cost-effective and context-neutral techniques to improve the recall accuracy of data collected by the name generator.}
}
@incollection{HUGHES2013143,
title = {Chapter 5 - Deriving Initial Project Backlogs},
editor = {Ralph Hughes},
booktitle = {Agile Data Warehousing Project Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {143-174},
year = {2013},
isbn = {978-0-12-396463-2},
doi = {https://doi.org/10.1016/B978-0-12-396463-2.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964632000053},
author = {Ralph Hughes}
}
@article{OZKAN2016130,
title = {Challenges and priorities for modelling livestock health and pathogens in the context of climate change},
journal = {Environmental Research},
volume = {151},
pages = {130-144},
year = {2016},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2016.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S001393511630319X},
author = {Şeyda Özkan and Andrea Vitali and Nicola Lacetera and Barbara Amon and André Bannink and Dave J. Bartley and Isabel Blanco-Penedo and Yvette {de Haas} and Isabelle Dufrasne and John Elliott and Vera Eory and Naomi J. Fox and Phil C. Garnsworthy and Nicolas Gengler and Hedi Hammami and Ilias Kyriazakis and David Leclère and Françoise Lessire and Michael Macleod and Timothy P. Robinson and Alejandro Ruete and Daniel L. Sandars and Shailesh Shrestha and Alistair W. Stott and Stanislaw Twardy and Marie-Laure Vanrobays and Bouda Vosough Ahmadi and Isabelle Weindl and Nick Wheelhouse and Adrian G. Williams and Hefin W. Williams and Anthony J. Wilson and Søren Østergaard and Richard P. Kipling},
keywords = {Animal health, Climate change, Greenhouse gas emissions, Pathogens, Modelling},
abstract = {Climate change has the potential to impair livestock health, with consequences for animal welfare, productivity, greenhouse gas emissions, and human livelihoods and health. Modelling has an important role in assessing the impacts of climate change on livestock systems and the efficacy of potential adaptation strategies, to support decision making for more efficient, resilient and sustainable production. However, a coherent set of challenges and research priorities for modelling livestock health and pathogens under climate change has not previously been available. To identify such challenges and priorities, researchers from across Europe were engaged in a horizon-scanning study, involving workshop and questionnaire based exercises and focussed literature reviews. Eighteen key challenges were identified and grouped into six categories based on subject-specific and capacity building requirements. Across a number of challenges, the need for inventories relating model types to different applications (e.g. the pathogen species, region, scale of focus and purpose to which they can be applied) was identified, in order to identify gaps in capability in relation to the impacts of climate change on animal health. The need for collaboration and learning across disciplines was highlighted in several challenges, e.g. to better understand and model complex ecological interactions between pathogens, vectors, wildlife hosts and livestock in the context of climate change. Collaboration between socio-economic and biophysical disciplines was seen as important for better engagement with stakeholders and for improved modelling of the costs and benefits of poor livestock health. The need for more comprehensive validation of empirical relationships, for harmonising terminology and measurements, and for building capacity for under-researched nations, systems and health problems indicated the importance of joined up approaches across nations. The challenges and priorities identified can help focus the development of modelling capacity and future research structures in this vital field. Well-funded networks capable of managing the long-term development of shared resources are required in order to create a cohesive modelling community equipped to tackle the complex challenges of climate change.}
}