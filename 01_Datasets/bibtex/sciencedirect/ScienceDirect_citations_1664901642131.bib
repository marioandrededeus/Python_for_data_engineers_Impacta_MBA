@article{ZANG2020116502,
title = {Application of functional deep belief network for estimating daily global solar radiation: A case study in China},
journal = {Energy},
volume = {191},
pages = {116502},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.116502},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219321978},
author = {Haixiang Zang and Lilin Cheng and Tao Ding and Kwok W. Cheung and Miaomiao Wang and Zhinong Wei and Guoqiang Sun},
keywords = {Daily global solar radiation, Deep belief network, Empirical knowledge, Solar radiation model},
abstract = {Solar energy plays an essential role in environment governance and resource protection, as it is totally pollution-free and extensively accessed. An accurate knowledge of solar radiation is beneficial to the deployments of solar energy constructions, photovoltaic and thermal solar systems. In this study, a deep learning method is proposed for estimating daily global solar radiation, which is constituted by embedding clustering (EC) and functional deep belief network (DBN). Based on the curve shapes of daily solar radiation, EC divides the overall dataset into different subsets, which can be modeled separately. Knowledge from empirical radiation models is also merged as the input of functional DBN. The model can be directly applied to solar estimation in various stations due to its strong nonlinear representation. The case study in China is adopted that involves radiation data from a total of 30 stations to validate the practicability and accuracy of the proposed method. From the results, the method obtains better estimation precision with empirical knowledge, achieving 1.706 MJ/m2 of mean absolute error (MAE), 2.352 MJ/m2 of root mean square error (RMSE) and 13.71% of mean absolute percentage error (MAPE) according to the average values at the 30 stations.}
}
@article{FERNANDO2020785,
title = {Open Visualization Environment (OVE): A web framework for scalable rendering of data visualizations},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {785-799},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1932922X},
author = {Senaka Fernando and James Scott-Brown and Ovidiu Şerban and David Birch and David Akroyd and Miguel Molina-Solana and Thomas Heinis and Yike Guo},
keywords = {Large-scale visualization, Data visualization, Scalable resolution display environments, SAGE2},
abstract = {Scalable resolution display environments, including immersive data observatories, are emerging as equitable and socially engaging platforms for collaborative data exploration and decision making. These environments require specialized middleware to drive them, but, due to various limitations, there is still a gap in frameworks capable of scalable rendering of data visualizations. To overcome these limitations, we introduce a new modular open-source middleware, the Open Visualization Environment (OVE). This framework uses web technologies to provide an ecosystem for visualizing data using web browsers that span hundreds of displays. In this paper, we discuss the key design features and architecture of our framework as well as its limitations. This is followed by an extensive study on performance and scalability, which validates its design and compares it to the popular SAGE2 middleware. We show how our framework solves three key limitations in SAGE2. Thereafter, we present two of our projects that used OVE and show how it can extend SAGE2 to overcome limitations and simplify the user experience for common data visualization use-cases.}
}
@article{PARE2020103291,
title = {IT vendors’ legitimation strategies and market share: The case of EMR systems},
journal = {Information & Management},
volume = {57},
number = {5},
pages = {103291},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103291},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618307742},
author = {Guy Paré and Josianne Marsan and Mirou Jaana and Haitham Tamim and Roman Lukyanenko},
keywords = {Legitimacy, Organizing vision, Electronic medical record system, Market share, Content analysis},
abstract = {This study investigates the legitimation strategies adopted by information technology (IT) vendors and their respective influence on market share. We conducted an analysis of the public discourse on websites of top Electronic Medical Record (EMR) vendors in Ontario, Canada. A total of 815 segments extracted from these websites were analyzed. Our findings indicate that strategies under the cognitive and pragmatic forms of legitimacy were strongly represented in the EMR vendors’ discourses compared with regulative and normative strategies. Furthermore, the link between legitimation strategies and market share has not yet been clearly established. Implications for practice and research are discussed.}
}
@article{TANG2020272,
title = {OsNSUN2-Mediated 5-Methylcytosine mRNA Modification Enhances Rice Adaptation to High Temperature},
journal = {Developmental Cell},
volume = {53},
number = {3},
pages = {272-286.e7},
year = {2020},
issn = {1534-5807},
doi = {https://doi.org/10.1016/j.devcel.2020.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1534580720301933},
author = {Yongyan Tang and Chun-Chun Gao and Ying Gao and Ying Yang and Boyang Shi and Jia-Li Yu and Cong Lyu and Bao-Fa Sun and Hai-Lin Wang and Yunyuan Xu and Yun-Gui Yang and Kang Chong},
keywords = {OsNSUN2, mC, RNA modification, methyltransferase, rice, heat stress, photosynthesis, reactive oxygen species},
abstract = {Summary
Extreme weather events can cause heat stress that decreases crop production. Recent studies have demonstrated that protein degradation and rRNA homeostasis as well as transcription factors are involved in the thermoresponse in plants. However, how RNA modifications contribute to temperature stress response in plant remains largely unknown. Herein, we identified OsNSUN2 as an RNA 5-methylcytosine (m5C) methyltransferase in rice. osnsun2 mutant displayed severe temperature- and light-dependent lesion-mimic phenotypes and heat-stress hypersensitivity. Heat stress enhanced the OsNSUN2-dependent m5C modification of mRNAs involved in photosynthesis and detoxification systems, such as β-OsLCY, OsHO2, OsPAL1, and OsGLYI4, which increased protein synthesis. Furthermore, the photosystem of osnsun2 mutant was vulnerable to high ambient temperature and failed to undergo repair under tolerable heat stress. Thus, OsNSUN2 mutation reduced photosynthesis efficiency and accumulated excessive reactive oxygen species upon heat treatment. Our findings demonstrate an important mechanism of mRNA m5C-dependent heat acclimation in rice.}
}
@article{MESSAOUD2020100314,
title = {A survey on machine learning in Internet of Things: Algorithms, strategies, and applications},
journal = {Internet of Things},
volume = {12},
pages = {100314},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100314},
url = {https://www.sciencedirect.com/science/article/pii/S2542660520301451},
author = {Seifeddine Messaoud and Abbas Bradai and Syed Hashim Raza Bukhari and Pham Tran Anh Quang and Olfa Ben Ahmed and Mohamed Atri},
keywords = {Wireless sensor network, Internet of Things, Machine learning categories, Machine learning algorithms},
abstract = {In the IoT and WSN era, large number of connected objects and sensing devices are dedicated to collect, transfer, and generate a huge amount of data for a wide variety of fields and applications. To effectively run these complex networks of connected objects, there are several challenges like topology changes, link failures, memory constraints, interoperability, network congestion, coverage, scalability, network management, security, and privacy to name a few. Thus, to overcome these challenges and exploiting them to support this technological outbreak would be one of the most crucial tasks of modern world. In the recent years, the development of Artificial Intelligence (AI) led to the emergence of Machine Learning (ML) which has become the key enabler to figure out solutions and learning models in an attempt to enhance the QoS parameters of IoT and WSNs. By learning from past experiences, ML techniques aim to resolve issues in the WSN and IoT's fields by building algorithmic models. In this paper, we are going to highlight the most fundamental concepts of ML categories and Algorithms. We start by providing a thorough overview of the WSN and IoT's technologies. We also discuss the vital role of ML techniques in driving up the evolution of these technologies. Then, as the key contribution of this paper, a new taxonomy of ML algorithms is provided. We also summarize the major applications and research challenges that leveraged ML techniques in the WSN and IoT. Eventually, we analyze the critical issues and list some future research directions.}
}
@article{PEREIRA2020106357,
title = {Soil water balance models for determining crop water and irrigation requirements and irrigation scheduling focusing on the FAO56 method and the dual Kc approach},
journal = {Agricultural Water Management},
volume = {241},
pages = {106357},
year = {2020},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2020.106357},
url = {https://www.sciencedirect.com/science/article/pii/S0378377420303930},
author = {L.S. Pereira and P. Paredes and N. Jovanovic},
keywords = {Crop coefficients, Crop evapotranspiration, Dual K approach, Real time irrigation management, Water use assessment, SIMDualKc model},
abstract = {This study reviews soil water balance (SWB) model approaches to determine crop irrigation requirements and scheduling irrigation adopting the FAO56 method. The Kc-ETo approach is discussed with consideration of baseline concepts namely standard vs. actual Kc concepts, as well as single and dual Kc approaches. Requirements for accurate SWB and appropriate parameterization and calibration are introduced. The one-step vs. the two-step computational approaches is discussed before the review of the FAO56 method to compute and partition crop evapotranspiration and related soil water balance. A brief review on transient state models is also included. Baseline information is concluded with a discussion on yields prediction and performance indicators related to water productivity. The study is continued with an overview on models development and use after publication of FAO24, essentially single Kc models, followed by a review on models following FAO56, particularly adopting the dual Kc approach. Features of dual Kc modeling approaches are analyzed through a few applications of the SWB model SIMDualKc, mainly for derivation of basal and single Kc, extending the basal Kc approach to relay intercrop cultivation, assessing alternative planting dates, determining beneficial and non-beneficial uses of water by an irrigated crop, and assessing the groundwater contribution to crop ET in the presence of a shallow water table. The review finally discusses the challenges placed to SWB modeling for real time irrigation scheduling, particularly the new modeling approaches for large scale multi-users application, use of cloud computing and adopting the internet of things (IoT), as well as an improved wireless association of modeling with soil and plant sensors. Further challenges refer to the use of remote sensing energy balance and vegetation indices to map Kc, ET and crop water and irrigation requirements. Trends are expected to change research issues relative to SWB modeling, with traditional models mainly used for research while new, fast-responding and multi-users models based on cloud and IoT technologies will develop into applications to the farm practice. Likely, the Kc-ETo will continue to be used, with ETo from gridded networks, re-analysis and other sources, and Kc data available in real time from large databases and remote sensing.}
}
@article{MCLAFFERTY2020102282,
title = {Placing volunteered geographic health information: Socio-spatial bias in 311 bed bug report data for New York City},
journal = {Health & Place},
volume = {62},
pages = {102282},
year = {2020},
issn = {1353-8292},
doi = {https://doi.org/10.1016/j.healthplace.2019.102282},
url = {https://www.sciencedirect.com/science/article/pii/S1353829219309050},
author = {Sara McLafferty and Daniel Schneider and Kathryn Abelt},
keywords = {Volunteered geographic information (VGI), Socio-spatial bias, Bed bugs},
abstract = {Health researchers and policy-makers increasingly use volunteered geographic information (VGI) to analyze spatial variation in health and wellbeing and to develop interventions. As socially constructed data, health VGI reflect the people who perceive issues and choose to report them, and the digital systems that structure the reporting process. We propose a conceptual framework that describes the interlocking effects of socioeconomic, behavioral, geographic, and technological processes on VGI accuracy and credibility. GIS and statistical methods are used to analyze social and geographical biases in health-related VGI through a case study of bed bug complaint data from New York City's 311 system. Reports of bed bug infestation from 311 are mapped and modeled to uncover associations with socioeconomic and built environment characteristics. Factors associated with bed bug report credibility are examined by comparing characteristics of confirmed reports with those for reports in which inspectors found no evidence of infestation (negative reports). A multilevel model of credibility incorporating report-, building-, and tract-level variables reveals strong geographical and socioeconomic biases, with negative reports generated more frequently from high-value residential buildings located in high-income neighborhoods with predominately white, non-Hispanic populations. Using 311 data for all bed bug reports, rather than confirmed reports, obscures the burden of these pests in high poverty neighborhoods and diminishes socioeconomic disparities. Mistaken reporting also has economic costs, as each report triggers an inspection by city inspectors that entails time, monetary, and opportunity costs.}
}
@article{MARANGHI2020106074,
title = {Integrating urban metabolism and life cycle assessment to analyse urban sustainability},
journal = {Ecological Indicators},
volume = {112},
pages = {106074},
year = {2020},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2020.106074},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X2030011X},
author = {Simone Maranghi and Maria Laura Parisi and Angelo Facchini and Alessandro Rubino and Olga Kordas and Riccardo Basosi},
keywords = {City, Urban development, Life cycle assessment, Urban metabolism, Urban thermodynamics, Complex system},
abstract = {In recent decades, the close correlation between urban development and the concept of sustainability has become increasingly evident and important. This is demonstrated by European Union policies concerning EU cities and the United Nations 2030 Agenda for Sustainable Development, including sustainable development goal (SDG) 11: Sustainable cities and communities. In the context of increasing urbanization, it is essential to find innovative methods to manage urban living systems and to establish a standard method for assessing the environmental performance of cities and their infrastructures. A unified and complete methodology for assessing policies for urban sustainability that takes into consideration urban complexity is currently lacking. In this paper, we integrate the Urban Metabolism and Lice Cycle Assessment approach to assess urban sustainability by developing a multi-dimensional measure framework applied to cities. Our aim is to provide a holistic view of the city and unveiling the interconnections among a set of urban dimensions identified by means of an approach based on complex systems science and complex networks. We also propose a specific survey to investigate the city in a multi-dimensional perspective and suggest key indicators based on network centrality measures for investigating and comparing the interconnections among a set of urban dimensions specifically identified (e.g. energy, material, transport). Finally, a case study based on Beijing is considered to show potential applications.}
}
@incollection{DUTT2020271,
title = {Chapter 11 - The impact of artificial intelligence on healthcare insurances},
editor = {Adam Bohr and Kaveh Memarzadeh},
booktitle = {Artificial Intelligence in Healthcare},
publisher = {Academic Press},
pages = {271-293},
year = {2020},
isbn = {978-0-12-818438-7},
doi = {https://doi.org/10.1016/B978-0-12-818438-7.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128184387000113},
author = {Rajeev Dutt},
keywords = {Health insurance, preventative healthcare, risk assessment, premiums, fraud detection, claims assessments, analytics, data, privacy, regulation, ethics},
abstract = {The health insurance industry is facing many challenges such as privacy, increasing competition, rising costs, and an aging population. Artificial intelligence (AI) is increasingly being used to do risk assessments, determine premiums, prevent fraud, accelerate claims, prevent illnesses, and improve customer experience. Although AI does confer significant benefits to the health insurance industry, there are significant risks and challenges that need to be overcome that can slow down the rate of adoption or, in some cases, prevent the adoption of AI-based solutions.}
}
@article{XIAN2020105800,
title = {NetSRE: Link predictability measuring and regulating},
journal = {Knowledge-Based Systems},
volume = {196},
pages = {105800},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.105800},
url = {https://www.sciencedirect.com/science/article/pii/S095070512030188X},
author = {Xingping Xian and Tao Wu and Shaojie Qiao and Xi-Zhao Wang and Wei Wang and Yanbing Liu},
keywords = {Network data, Link prediction, Link predictability, Structural patterns, Low-rank coding, Structure perturbation},
abstract = {Link prediction is an elemental issue for network-structured data mining, which has already found a wide range of applications. The organization of real-world networks usually embodies both regularities and irregularities, and the precision of link prediction algorithms coincides with the portion of a network being categorized as regular. Quantifying and controlling how well an unobserved link can be predicted is a fundamental problem in link prediction. This paper proposes a structural regularity-exploring architecture, called NetSRE, for measuring and regulating link predictability of networks. The proposed NetSRE assumes that there are consistent interaction patterns across the local subgraphs of networks and one of them can be represented by a linear summation of the others, and thus, link predictability can be characterized by the self-representation degree of network structures. Specifically, NetSRE includes (1) a low Frobenius norm pursuit-based self-representation network model for predicting the “true” underlying networks, (2) a “structural regularity” index for measuring the link predictability of networks, i.e., the inherent difficulty of link prediction independent of specific algorithms, and (3) an importance measuring method for structural role exploration of network links and a link-based structure perturbation algorithm for link predictability regulation. Experimental results on real-world networks validate the performance of our method. It is found that real-world networks have various structural regularities and link predictability can be estimated based on structure mining directly. We show that network heterogeneity provides a way to intrinsically segregate network links into qualitatively distinct groups, which have different influences on the link predictability of networks.}
}
@article{GOLLER2020101855,
title = {Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany's programmes for long term unemployed},
journal = {Labour Economics},
volume = {65},
pages = {101855},
year = {2020},
issn = {0927-5371},
doi = {https://doi.org/10.1016/j.labeco.2020.101855},
url = {https://www.sciencedirect.com/science/article/pii/S0927537120300592},
author = {Daniel Goller and Michael Lechner and Andreas Moczall and Joachim Wolff},
keywords = {Programme evaluation, active labour market policy, causal machine learning, treatment effects, radius matching, propensity score},
abstract = {Matching-type estimators using the propensity score are the major workhorse in active labour market policy evaluation. This work investigates if machine learning algorithms for estimating the propensity score lead to more credible estimation of average treatment effects on the treated using a radius matching framework. Considering two popular methods, the results are ambiguous: We find that using LASSO based logit models to estimate the propensity score delivers more credible results than conventional methods in small and medium sized high dimensional datasets. However, the usage of Random Forests to estimate the propensity score may lead to a deterioration of the performance in situations with a low treatment share. The application reveals a positive effect of the training programme on days in employment for long-term unemployed. While the choice of the “first stage” is highly relevant for settings with low number of observations and few treated, machine learning and conventional estimation becomes more similar in larger samples and higher treatment shares.}
}
@article{QIAN2020102053,
title = {Impact of transportation network companies on urban congestion: Evidence from large-scale trajectory data},
journal = {Sustainable Cities and Society},
volume = {55},
pages = {102053},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102053},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720300408},
author = {Xinwu Qian and Tian Lei and Jiawei Xue and Zengxiang Lei and Satish V. Ukkusuri},
keywords = {Transportation network companies, Trajectory data, Urban traffic congestion, Emission},
abstract = {We collect vehicle trajectory data from major transportation network companies (TNCs) in New York City (NYC) in 2017 and 2019, and we use the trajectory data to understand how the growth of TNCs has impacted traffic congestion and emission in urban areas. By mining the large-scale trajectory data and conduct the case study in NYC, we confirm that the rise of TNC is the major contributing factor that makes urban traffic congestion worse. From 2017 to 2019, the number of for-hire vehicles (FHV) has increased by over 48% and served 90% more daily trips. These resulted in an average citywide speed reduction of 22.5% on weekdays, and the average speed in Manhattan decreased from 11.76 km/h in April 2017 to 9.56 km/h in March 2019. The heavier traffic congestion may have led to 136% more NOx, 152% more CO and 157% more HC emission per kilometer traveled by the FHV sector. Our results show that the traffic condition is consistently worse across different times of the day and at different locations in NYC. And we build the connection between the number of available FHVs and the reduction in travel speed between the two years of data and explain how the rise of TNC may impact traffic congestion in terms of moving speed and congestion time. The findings in our study provide valuable insights for different stakeholders and decision-makers in framing regulation and operation policies towards more effective and sustainable urban mobility.}
}
@article{IWENDI2020160,
title = {N-Sanitization: A semantic privacy-preserving framework for unstructured medical datasets},
journal = {Computer Communications},
volume = {161},
pages = {160-171},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.07.032},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420318442},
author = {Celestine Iwendi and Syed Atif Moqurrab and Adeel Anjum and Sangeen Khan and Senthilkumar Mohan and Gautam Srivastava},
keywords = {Anonymization, Document sanitization, Textual-privacy, Negated assertion, Medical data, IoMT},
abstract = {The introduction and rapid growth of the Internet of Medical Things (IoMT), a subset of the Internet of Things (IoT) in the medical and healthcare systems, has brought numerous changes and challenges to current medical and healthcare systems. Healthcare organizations share data about patients with research organizations for various medical discoveries. Releasing such information is a tedious task since it puts the privacy of patients at risk with the understanding that textual health documents about an individual contains specific sensitive terms that need to be sanitized before such document can be released. Recent approaches improved the utility of protected output by substituting sensitive terms with appropriate “generalizations” that are retrieved from several medical and general-purpose knowledge bases (KBs). However, these approaches perform unnecessary sanitization by anonymizing the negated assertions, e.g., AIDS-negative. This paper proposes a semantic privacy framework that effectively sanitizes the sensitive and semantically related terms in healthcare documents. The proposed model effectively identifies the negated assertions (e.g., AIDS-negative) before the sanitization process in IoMT which further improves the utility of sanitized documents. Moreover, besides considering the sensitive medical findings, we also incorporated state-of-the-art metrics, i.e., Protected Health Information (PHI), as defined in the privacy rules such as Health Insurance Portability and Accountability Act (HIPAA), Informatics for Integrating Biology & the Bedside (i2b2), and Materialize Interactive Medical Image Control System (MIMICS). The proposed approach is evaluated on real clinical data provided by i2b2. On average the detection (for both PHI’s and medical findings) accuracy is improved with Precision, Recall and F-measure score at 21%, 51%, and 54% respectively. The overall improved data utility of our proposed model is 8% as compared to C-sanitized and 25% when comparing it with a simple reduction approach. Experimental results show that our approach effectively manages the privacy and utility trade-off as compared to its counterparts.}
}
@article{ZHOU2020103766,
title = {3D dense connectivity network with atrous convolutional feature pyramid for brain tumor segmentation in magnetic resonance imaging of human heads},
journal = {Computers in Biology and Medicine},
volume = {121},
pages = {103766},
year = {2020},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2020.103766},
url = {https://www.sciencedirect.com/science/article/pii/S0010482520301396},
author = {Zexun Zhou and Zhongshi He and Meifeng Shi and Jinglong Du and Dingding Chen},
keywords = {Brain tumor segmentation, MRI, 3D dense connectivity network, Feature pyramid, Deep supervision},
abstract = {The existing deep convolutional neural networks (DCNNs) based methods have achieved significant progress regarding automatic glioma segmentation in magnetic resonance imaging (MRI) data. However, there are two main problems affecting the performance of traditional DCNNs constructed by simply stacking convolutional layers, namely, exploding/vanishing gradients and limitations to the feature computations. To address these challenges, we propose a novel framework to automatically segment brain tumors. First, a three-dimensional (3D) dense connectivity architecture is used to build the backbone for feature reuse. Second, we design a new feature pyramid module using 3D atrous convolutional layers and add this module to the end of the backbone to fuse multiscale contexts. Finally, a 3D deep supervision mechanism is equipped with the network to promote training. On the multimodal brain tumor image segmentation benchmark (BRATS) datasets, our method achieves Dice similarity coefficient values of 0.87, 0.72, and 0.70 on the BRATS 2013 Challenge, 0.84, 0.70, and 0.61 on the BRATS 2013 LeaderBoard, 0.83, 0.70, and 0.62 on the BRATS 2015 Testing, 0.8642, 0.7738, and 0.7525 on the BRATS 2018 Validation in terms of whole tumors, tumor cores, and enhancing cores, respectively. Compared to the published state-of-the-art methods, the proposed method achieves promising accuracy and fast processing, demonstrating good potential for clinical medicine.}
}
@article{SANCHEZ2020879,
title = {Semantic-based privacy settings negotiation and management},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {879-898},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18317035},
author = {Odnan Ref Sanchez and Ilaria Torre and Bart P. Knijnenburg},
keywords = {Privacy Preference Manager, Privacy ontology, Data sharing and permission management, IoT},
abstract = {By 2020, an individual is expected to own an average of 6.58 devices that share and integrate a wealth of personal user data. The management of privacy preferences across these devices is a complex task for which users are ill-equipped, which increases privacy risks. In this paper we propose an approach that exploits Semantic Web (SW) technology to manage the user’s IoT privacy preferences and negotiate the permissions for data sharing with third parties. SW technology comprises a web of data that can be processed by machines through a formal, universally shared representation. In our approach, SW enables a lightweight and interoperable communication between a Personal Data Manager (PDM) and the Third Parties (TPs) that request access to the user’s personal data. The PDM can handle multiple heterogeneous personal IoT devices and manages the negotiation process between the user and the TPs in a way that can relieve users from the burden of specifying their privacy requirement for each TP. The core of the approach is the definition of the Privacy Preference for IoT (PPIoT) Ontology which is based on the Privacy Preference Ontology, the W3C Semantic Sensor Network Ontology, the Fair Information Practices (FIP) principles, and state-of-the-art recommendation techniques for privacy protection in the IoT. This ontology aims to capture the complexity of privacy management in the IoT paradigm in light of the recent General Data Protection Regulation (GDPR) of the European Union. Along with presenting the ontology, in this paper we will provide an example on how to use the PPIoT ontology for the management of privacy preferences in the fitness IoT domain and we will show how the PDM handles the process of negotiation between the user and the TPs. The approach is based on an interactive PPIoT-based Privacy Preference Model (PPM) that meets the requirements of the GDPR to have transparent and simple TP privacy policies. Finally, we will report the results of an evaluation on a mockup fitness app that implements this PPM. The main contributions of this paper are: (i) to propose an ontology for privacy preference in the IoT context, which covers a knowledge gap in existing literature and can be used for IoT privacy management, (ii) to propose an interactive PPIoT-based Privacy Preference Model, which is in accordance with the GDPR objectives.}
}
@article{MAGALHAES2020101248,
title = {Open government data and the private sector: An empirical view on business models and value creation},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101248},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2017.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17302629},
author = {Gustavo Magalhaes and Catarina Roseira},
keywords = {Open government data, Private sector, Business models},
abstract = {The release of government data in an open format is broadly expected to generate innovation and economic value. However, despite the emerging public notoriety of this issue, literature is still scarce regarding the commercial application of open government data. The main goal of this study is to understand how firms use open government data to create value. More specifically, we aim to identify what types of use are currently in place and which industries are more prominent in exploiting open government data. Building on the analysis of a dataset of 178 firms that use open government data across various industries in the U.S. we find twelve different atomic models. Additionally, our findings suggest that the way in which open government is used to create value is contingent to the firms' activities. Supported by robust empirical data, we anticipate that our research produces practical insights to entrepreneurs as well as firm managers in deriving value from public datasets, and equip government officials with relevant evidence for advocacy and policy-making.}
}
@article{SAKELLARIOU2020119873,
title = {From user insights to user foresights: Applying video-based ethnographic narratives and user innovation in NPD},
journal = {Technological Forecasting and Social Change},
volume = {153},
pages = {119873},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2019.119873},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518310874},
author = {Evy Sakellariou and Kalipso Karantinou and Keith Goffin}
}
@article{CIMPOIASU2020114232,
title = {Potential of geoelectrical methods to monitor root zone processes and structure: A review},
journal = {Geoderma},
volume = {365},
pages = {114232},
year = {2020},
issn = {0016-7061},
doi = {https://doi.org/10.1016/j.geoderma.2020.114232},
url = {https://www.sciencedirect.com/science/article/pii/S0016706119322608},
author = {Mihai Octavian Cimpoiaşu and Oliver Kuras and Tony Pridmore and Sacha J. Mooney},
keywords = {Electrical-properties, Root zone soil moisture, Root system monitoring, Root zone structure, Root detection, modelling, Geophysics, Geoelectrical methods},
abstract = {Understanding the processes that control mass and energy exchanges between soil, plants and the atmosphere plays a critical role for understanding the root zone system, but it is also beneficial for practical applications such as sustainable agriculture and geotechnics. Improved process understanding demands fast, minimally invasive and cost-effective methods of monitoring the shallow subsurface. Geoelectrical monitoring methods fulfil these criteria and have therefore become of increasing interest to soil scientists. Such methods are particularly sensitive to variations in soil moisture and the presence of root material, both of which are essential drivers for processes and mechanisms in soil and root zone systems. This review analyses the recent use of geoelectrical methods in the soil sciences, and highlights their main achievements in focal areas such as estimating hydraulic properties and delineating root architecture. We discuss the specific advantages and limitations of geoelectrical monitoring in this context. Standing out amongst the latter are the non-uniqueness of inverse model solution and the appropriate choice of pedotransfer functions between electrical parameters and soil properties. The relationship between geoelectrical monitoring and alternative characterization methodologies is also examined. Finally, we advocate for future interdisciplinary research combining models of root hydrology and geoelectrical measurements. This includes the development of more appropriate analogue root electrical models, careful separation between different root zone contributors to the electrical response and integrating spatial and temporal geophysical measurements into plant hydrological models to improve the prediction of root zone development and hydraulic parameters.}
}
@article{FAIZ2020744,
title = {Predicting likelihood of legitimate data loss in email DLP},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {744-757},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19314943},
author = {Mohamed Falah Faiz and Junaid Arshad and Mamoun Alazab and Andrii Shalaginov},
keywords = {Data loss prevention, Email DLP, Insider threats, Threat prediction, Machine learning},
abstract = {The volume and variety of data collected for modern organisations has increased significantly over the last decade necessitating the detection and prevention of disclosure of sensitive data. Data loss prevention is an embedded process used to protect against disclosure of sensitive data to external uncontrolled environments. A typical Data Loss Prevention (DLP) system uses custom policies to identify and prevent accidental and malicious data leakage producing large number of security alerts including significant volume of false positives. Consequently, identifying legitimate data loss can be very challenging as each incident comprises of different characteristics often requiring extensive intervention by a domain expert to review alerts individually. This limits the ability to detect data loss alerts in real-time making organisations vulnerable to financial and reputational damages. The aim of this research is to strengthen data loss detection capabilities of a DLP system by implementing a machine learning model to predict the likelihood of legitimate data loss. We conducted extensive experimentation using Decision Tree and Random Forest algorithms with historical email incident data collected by a globally established telecommunication enterprise. The final model produced with Random Forest algorithm was identified as the most effective as it was successfully able to predict approximately 95% data loss incidents accurately with an average true positive value of 90%. Furthermore, the proposed solution successfully enables identification of legitimate data loss in email DLP whilst facilitating prioritisation of real data loss through human-understandable explanation of the decision thereby improving the efficiency of the process.}
}
@article{UNG20201,
title = {Clinical metagenomics for infectious corneal ulcers: Rags to riches?},
journal = {The Ocular Surface},
volume = {18},
number = {1},
pages = {1-12},
year = {2020},
issn = {1542-0124},
doi = {https://doi.org/10.1016/j.jtos.2019.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1542012419303969},
author = {Lawson Ung and Paulo J.M. Bispo and Thuy Doan and Russell N. {Van Gelder} and Michael S. Gilmore and Thomas Lietman and Todd P. Margolis and Michael E. Zegans and Cecilia S. Lee and James Chodosh},
keywords = {Clinical metagenomics, Next generation sequencing, Infectious corneal ulcers, Microbial keratitis},
abstract = {The emergence of clinical metagenomics as an unbiased, hypothesis-free approach to diagnostic testing is set to fundamentally alter the way infectious diseases are detected. Long envisioned as the solution to the limitations of culture-based conventional microbiology, next generation sequencing methods will soon mature, and our attention will inevitably turn to how they can be applied to areas of medicine which need it most urgently. In ophthalmology, the demand for this technology is particularly pressing for the care of infectious corneal ulcers, where current diagnostic tests may fail to identify a causative organism in over half of cases. However, the optimism found in the budding discourse surrounding clinical metagenomics belies the reality that clinicians and scientists will soon be inundated by oppressive volumes of sequencing data, much of which will be foreign and unfamiliar. Therefore, our success in translating clinical metagenomics is likely to hinge on how we make sense of these data, and understanding its implications for the interpretation and implementation of sequencing into routine clinical care. In this consortium-led review, we provide an outline of these data-related issues and how they may be used to inform technical workflows, with the hope that we may edge closer to realizing the potential of clinical metagenomics for this important unmet need.}
}
@article{LI2020106854,
title = {A review of applications in federated learning},
journal = {Computers & Industrial Engineering},
volume = {149},
pages = {106854},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106854},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220305532},
author = {Li Li and Yuxi Fan and Mike Tse and Kuo-Yi Lin},
keywords = {Federated learning, Literature review, Citation analysis, Research front},
abstract = {Federated Learning (FL) is a collaboratively decentralized privacy-preserving technology to overcome challenges of data silos and data sensibility. Exactly what research is carrying the research momentum forward is a question of interest to research communities as well as industrial engineering. This study reviews FL and explores the main evolution path for issues exist in FL development process to advance the understanding of FL. This study aims to review prevailing application in industrial engineering to guide for the future landing application. This study also identifies six research fronts to address FL literature and help advance our understanding of FL for future optimization. This study contributes to conclude application in industrial engineering and computer science and summarize a review of applications in FL.}
}
@article{MORLEY2020113172,
title = {The ethics of AI in health care: A mapping review},
journal = {Social Science & Medicine},
volume = {260},
pages = {113172},
year = {2020},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2020.113172},
url = {https://www.sciencedirect.com/science/article/pii/S0277953620303919},
author = {Jessica Morley and Caio C.V. Machado and Christopher Burr and Josh Cowls and Indra Joshi and Mariarosaria Taddeo and Luciano Floridi},
keywords = {Artificial intelligence, Ethics, Healthcare, Health policies, Machine learning},
abstract = {This article presents a mapping review of the literature concerning the ethics of artificial intelligence (AI) in health care. The goal of this review is to summarise current debates and identify open questions for future research. Five literature databases were searched to support the following research question: how can the primary ethical risks presented by AI-health be categorised, and what issues must policymakers, regulators and developers consider in order to be ‘ethically mindful? A series of screening stages were carried out—for example, removing articles that focused on digital health in general (e.g. data sharing, data access, data privacy, surveillance/nudging, consent, ownership of health data, evidence of efficacy)—yielding a total of 156 papers that were included in the review. We find that ethical issues can be (a) epistemic, related to misguided, inconclusive or inscrutable evidence; (b) normative, related to unfair outcomes and transformative effectives; or (c) related to traceability. We further find that these ethical issues arise at six levels of abstraction: individual, interpersonal, group, institutional, and societal or sectoral. Finally, we outline a number of considerations for policymakers and regulators, mapping these to existing literature, and categorising each as epistemic, normative or traceability-related and at the relevant level of abstraction. Our goal is to inform policymakers, regulators and developers of what they must consider if they are to enable health and care systems to capitalise on the dual advantage of ethical AI; maximising the opportunities to cut costs, improve care, and improve the efficiency of health and care systems, whilst proactively avoiding the potential harms. We argue that if action is not swiftly taken in this regard, a new ‘AI winter’ could occur due to chilling effects related to a loss of public trust in the benefits of AI for health care.}
}
@article{BENITEZPENA2020102068,
title = {Feature Selection in Data Envelopment Analysis: A Mathematical Optimization approach},
journal = {Omega},
volume = {96},
pages = {102068},
year = {2020},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0305048318312131},
author = {Sandra Benítez-Peña and Peter Bogetoft and Dolores {Romero Morales}},
keywords = {Benchmarking, Data Envelopment Analysis, Feature Selection, Mixed Integer Linear Programming},
abstract = {This paper proposes an integrative approach to feature (input and output) selection in Data Envelopment Analysis (DEA). The DEA model is enriched with zero-one decision variables modelling the selection of features, yielding a Mixed Integer Linear Programming formulation. This single-model approach can handle different objective functions as well as constraints to incorporate desirable properties from the real-world application. Our approach is illustrated on the benchmarking of electricity Distribution System Operators (DSOs). The numerical results highlight the advantages of our single-model approach provide to the user, in terms of making the choice of the number of features, as well as modeling their costs and their nature.}
}
@article{DOWDS2020124,
title = {Comparing alternative methods of collecting self-assessed overnight long-distance travel frequencies},
journal = {Travel Behaviour and Society},
volume = {19},
pages = {124-136},
year = {2020},
issn = {2214-367X},
doi = {https://doi.org/10.1016/j.tbs.2019.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S2214367X18300516},
author = {Jonathan Dowds and Lisa Aultman-Hall and Jeffrey J. LaMondia},
keywords = {Travel survey, Long-distance travel, Trip recall, Overnight travel},
abstract = {Due to the comparatively low frequency of long-distance and overnight travel, it can be challenging to measure using traditional travel surveys. In response to this dearth of data, several recent surveys have included self-reported frequency questions. However, the value and accuracy of these questions is unclear. This study leverages data from 628 panel members who completed a year-long, online survey in 2013–2014 by comparing their one-time self-assessment of typical overnight trip frequency to those reported in 12 subsequent monthly surveys. The self-assessed frequency of overnight tours, airplane tours and tours to non-North American destinations are consistent for only 68% and 70% of respondents for work and personal tours respectively. For most tour types, consistency is highest for low frequency (never and <1 per year) suggesting, unsurprisingly, that individuals are good at knowing they do not travel. Inconsistent estimators both over- and under-estimated trip-making suggest that not only is recall an issue but that prestige bias and the complexity and variability of long-distance travel are factors in recall as well. Few demographic factors were statistically significant in estimating whether participants were consistent reporters. Only 9% of participants consistently either over- or under-estimated both work and personal trips suggesting that there is simply a general inaccuracy in these survey questions for measuring long-distance travel. The aggregate trip rate across people and trip types was accurate suggesting this crude frequency measure may be acceptable for total frequency but not for understanding relative patterns or details in overnight long-distance travel.}
}
@incollection{LAGUE2020231,
title = {Chapter 8 - Terrestrial laser scanner applied to fluvial geomorphology},
editor = {Paolo Tarolli and Simon M. Mudd},
series = {Developments in Earth Surface Processes},
publisher = {Elsevier},
volume = {23},
pages = {231-254},
year = {2020},
booktitle = {Remote Sensing of Geomorphology},
issn = {0928-2025},
doi = {https://doi.org/10.1016/B978-0-444-64177-9.00008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444641779000084},
author = {Dimitri Lague},
keywords = {3D point clouds, Classification, Geomorphic change detection, Grain size, Vegetation},
abstract = {Terrestrial laser scanner (TLS) offers an unprecedented combination of sub-cm resolution, mm precision and survey extent that uniquely captures the geometry of individual pebbles and allows the spatial variability of channel evolution to be quantified precisely. Data processing can, however, be challenging, and the full scientific potential of fluvial 3D datasets remains arguably untapped. This chapter is an introduction to using TLS to solve fluvial geomorphology problems, synthesizing data acquisition, processing methods, and application examples. It covers practical aspects of field acquisition and addresses the respective benefits of TLS and structure from motion (SfM). Three-dimensional (3D) point cloud processing methods involved in data registration, vegetation classification, and spatial analysis are presented. Processing of repeat surveys and change detection methods are synthesized and the choice of raster-based or 3D point cloud differencing methods discussed in the context of fluvial processes and dynamics.}
}
@article{WEI2020135387,
title = {Updated information on soil salinity in a typical oasis agroecosystem and desert-oasis ecotone: Case study conducted along the Tarim River, China},
journal = {Science of The Total Environment},
volume = {716},
pages = {135387},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.135387},
url = {https://www.sciencedirect.com/science/article/pii/S004896971935380X},
author = {Yang Wei and Zhou Shi and Asim Biswas and Shengtian Yang and Jianli Ding and Fei Wang},
keywords = {Soil salinity, Machine learning, Digital soil mapping, Tarim Basin, Harmonized world Soil Database},
abstract = {Precise and spatially explicit regional estimates of soil salinity are necessary to efficiently management and utilise limited land and water resources. Despite advances achieved in remote sensing over the past century, knowledge about the distribution and severity of soil salinization in economically important areas, such as oasis agroecosystems and desert-oasis ecotones (OADoE), is currently limited. An example of an area is southern Xinjiang, where the OADoE has a high anthropogenic influence. This study was conducted with the aim of mapping soil salinity in typical OADoE using remote sensing and machine learning techniques (Cubist and Random Forest, RF). A range of covariates was obtained from the multi-temporal Landsat-8 operational land imager (OLI) satellite for the period from 2013 to 2018. The values of coefficients of determination (R2), Lin’s concordance correlation coefficient, root mean square error, and relative root mean squared error values, were 0.78, 0.87, 9.59, and 0.76, respectively, for the Cubist and 0.78, 0.86, 9.79, and 0.78, respectively, for RF models. The slope of the linear fitting equation was higher for the Cubist model (0.75) than for RF (0.69). The explanatory power of Cubist and RF for soil salinity variation were 33.22% and 31.41% in the agroecosystem, and 72.25% and 71.66% in desert-oasis ecotone, respectively. For the agroecosystem, the range of the predicted values for 89.13% (Cubist) and 84.78% (RF) of sample was controlled within the same observational range at an interval of 0–5 dS m−1. Compared to single-year data (from 2013 to 2018), the ability to account for model spatial variability in soil salinity based on multi-year Landsat images was increased by 16%–35%. According to the variable importance evaluation, soil-related indices are the most important predictor variables, followed by vegetation, topography, landform, and land use, with relative importance values of 60%, 21%, 16%, and 3%, respectively. The predicted map was also broadly consistent with those obtained for Xinjiang in the Harmonized World Soil Database (HWSD) from the second national soil survey of China conducted from 1984 to 1997. The results also showed that the average value of the study area is 8.10 dS m−1 based on the Cubist-based map whereas that of the HWSD is 10.60 dS m−1, this implied that the overall salinity level has reduced by 23.58%. The methodological framework presented covers all prediction process steps and has considerable potential to be used in future soil salinity mapping at large scales for other similar region as OADoEs. The map derived from the Cubist/RF model revealed more detailed variation information about spatial distribution of the soil salinity compared to HWSD, and can further assist with decision-making when planning and utilising on existing soil and water resources in OADoEs.}
}
@article{KORUCUOGLU2020116759,
title = {Test-retest reliability of fMRI-measured brain activity during decision making under risk},
journal = {NeuroImage},
volume = {214},
pages = {116759},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116759},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920302469},
author = {Ozlem Korucuoglu and Michael P. Harms and Serguei V. Astafiev and James T. Kennedy and Semyon Golosheykin and Deanna M. Barch and Andrey P. Anokhin},
keywords = {BART, Cohen’s d, Familiality, fMRI, ICC, Motion},
abstract = {Neural correlates of decision making under risk are being increasingly utilized as biomarkers of risk for substance abuse and other psychiatric disorders, treatment outcomes, and brain development. This research relies on the basic assumption that fMRI measures of decision making represent stable, trait-like individual differences. However, reliability needs to be established for each individual construct. Here we assessed long-term test-retest reliability (TRR) of regional brain activations related to decision making under risk using the Balloon Analogue Risk Taking task (BART) and identified regions with good TRRs and familial influences, an important prerequisite for the use of fMRI measures in genetic studies. A secondary goal was to examine the factors potentially affecting fMRI TRRs in one particular risk task, including the magnitude of neural activation, data analytical approaches, different methods of defining boundaries of a region, and participant motion. For the average BOLD response, reliabilities ranged across brain regions from poor to good (ICCs of 0 to 0.8, with a mean ICC of 0.17) and highest reliabilities were observed for parietal, occipital, and temporal regions. Among the regions that were of a priori theoretical importance due to their reported associations with decision making, the activation of left anterior insula and right caudate during the decision period showed the highest reliabilities (ICCs of 0.54 and 0.63, respectively). Among the regions with highest reliabilities, the right fusiform, right rostral anterior cingulate and left superior parietal regions also showed high familiality as indicated by intrapair monozygotic twin correlations (ranging from 0.66 to 0.69). Overall, regions identified by modeling the average BOLD response to a specific event type (rather than its modulation by a parametric regressor), regions including significantly activated vertices (compared to a whole parcel), and regions with greater magnitude of task-related activations showed greater reliabilities. Participant motion had a moderate negative effect on TRR. Regions activated during decision period rather than outcome period of risky decisions showed the greatest TRR and familiality. Regions with reliable activations can be utilized as neural markers of individual differences or endophenotypes in future clinical neuroscience and genetic studies of risk-taking.}
}
@article{NING2020117128,
title = {Cross-scanner and cross-protocol multi-shell diffusion MRI data harmonization: Algorithms and results},
journal = {NeuroImage},
volume = {221},
pages = {117128},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117128},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920306145},
author = {Lipeng Ning and Elisenda Bonet-Carne and Francesco Grussu and Farshid Sepehrband and Enrico Kaden and Jelle Veraart and Stefano B. Blumberg and Can Son Khoo and Marco Palombo and Iasonas Kokkinos and Daniel C. Alexander and Jaume Coll-Font and Benoit Scherrer and Simon K. Warfield and Suheyla Cetin Karayumak and Yogesh Rathi and Simon Koppers and Leon Weninger and Julia Ebert and Dorit Merhof and Daniel Moyer and Maximilian Pietsch and Daan Christiaens and Rui Azeredo {Gomes Teixeira} and Jacques-Donald Tournier and Kurt G. Schilling and Yuankai Huo and Vishwesh Nath and Colin Hansen and Justin Blaber and Bennett A. Landman and Andrey Zhylka and Josien P.W. Pluim and Greg Parker and Umesh Rudrapatna and John Evans and Cyril Charron and Derek K. Jones and Chantal M.W. Tax},
keywords = {Multi-shell diffusion MRI, Harmonization, Spherical harmonics, Deep learning, Regression},
abstract = {Cross-scanner and cross-protocol variability of diffusion magnetic resonance imaging (dMRI) data are known to be major obstacles in multi-site clinical studies since they limit the ability to aggregate dMRI data and derived measures. Computational algorithms that harmonize the data and minimize such variability are critical to reliably combine datasets acquired from different scanners and/or protocols, thus improving the statistical power and sensitivity of multi-site studies. Different computational approaches have been proposed to harmonize diffusion MRI data or remove scanner-specific differences. To date, these methods have mostly been developed for or evaluated on single b-value diffusion MRI data. In this work, we present the evaluation results of 19 algorithms that are developed to harmonize the cross-scanner and cross-protocol variability of multi-shell diffusion MRI using a benchmark database. The proposed algorithms rely on various signal representation approaches and computational tools, such as rotational invariant spherical harmonics, deep neural networks and hybrid biophysical and statistical approaches. The benchmark database consists of data acquired from the same subjects on two scanners with different maximum gradient strength (80 and 300 ​mT/m) and with two protocols. We evaluated the performance of these algorithms for mapping multi-shell diffusion MRI data across scanners and across protocols using several state-of-the-art imaging measures. The results show that data harmonization algorithms can reduce the cross-scanner and cross-protocol variabilities to a similar level as scan-rescan variability using the same scanner and protocol. In particular, the LinearRISH algorithm based on adaptive linear mapping of rotational invariant spherical harmonics features yields the lowest variability for our data in predicting the fractional anisotropy (FA), mean diffusivity (MD), mean kurtosis (MK) and the rotationally invariant spherical harmonic (RISH) features. But other algorithms, such as DIAMOND, SHResNet, DIQT, CMResNet show further improvement in harmonizing the return-to-origin probability (RTOP). The performance of different approaches provides useful guidelines on data harmonization in future multi-site studies.}
}
@article{LAN2020112926,
title = {Multivariable data imputation for the analysis of incomplete credit data},
journal = {Expert Systems with Applications},
volume = {141},
pages = {112926},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112926},
url = {https://www.sciencedirect.com/science/article/pii/S095741741930644X},
author = {Qiujun Lan and Xuqing Xu and Haojie Ma and Gang Li},
keywords = {Bayesian network, Credit scoring, Data missing, Data mining},
abstract = {Missing data significantly reduce the accuracy and usability of credit scoring models, especially in multivariate missing cases. Most credit scoring models address this problem by deleting the missing instances from the dataset or imputing missing values with the mean, mode, or regression values. However, these methods often result in a significant loss of information or a bias. We proposed a novel method called BNII to impute missing values, which can be helpful for intelligent credit scoring systems. The proposed BNII algorithm consisted of two stages: the preparatory stage and the imputation stage. In the first stage, a Bayesian network with all of the attributes in the original dataset was constructed from the complete dataset so that both the network structure that implied the dependencies between variables and the parameters at each variable's conditional distributions could be learned. In the second stage, multivariables with missing values were iteratively imputed using Bayesian network models from the first stage. The algorithm was found to be monotonically convergent. The most significant advantages of the method include, it exploits the inherent probability-dependent relationship between variables, but without a specific probability distribution hypothesis, and it is suitable for multivariate missing cases. Three datasets were used for experiments: one was the real dataset from a famous P2P financial company in China, and the other two were benchmark datasets provided by UCI. The experimental results showed that BNII performed significantly better than the other well-known imputation techniques. This suggested that the proposed method can be used to improve the performance of a credit scoring system and to be extended to other expert and intelligent systems.}
}
@article{COULTER202046,
title = {Code analysis for intelligent cyber systems: A data-driven approach},
journal = {Information Sciences},
volume = {524},
pages = {46-58},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520302164},
author = {Rory Coulter and Qing-Long Han and Lei Pan and Jun Zhang and Yang Xiang},
keywords = {Cyber security, Code analysis, Machine learning, Malware detection, Vulnerability discovery},
abstract = {Cyber code analysis is fundamental to malware detection and vulnerability discovery for defending cyber attacks. Traditional approaches resorting to manually defined rules are gradually replaced by automated approaches empowered by machine learning. This revolution is accelerated by big code from open source projects which support machine learning models with outstanding performance. In the context of a data-driven paradigm, this paper reviews recent analytic research on cyber code of malicious and common software by using a set of common concepts of similarity, correlation and collective indication. Sharing security goals in recognizing anomalous code that may be malicious or vulnerable. The ability to do so is not determined in isolation, rather drawn for code correlation and context awareness. This paper demonstrates a new research methodology of data driven cyber security (DDCS) and its application in cyber code analysis. The framework of the DDCS methodology consists of three components, i.e., cyber security data processing, cyber security feature engineering, and cyber security modeling. Some challenging issues are suggested to direct the future research.}
}
@article{NGUYEN2020106958,
title = {Smart dampers-based vibration control – Part 1: Measurement data processing},
journal = {Mechanical Systems and Signal Processing},
volume = {145},
pages = {106958},
year = {2020},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2020.106958},
url = {https://www.sciencedirect.com/science/article/pii/S0888327020303447},
author = {Sy Dzung Nguyen and Seung-Bok Choi and Joo-Hyung Kim},
keywords = {Data screening threshold, Impulse noise filtering, ANFIS-based filtering, Optima data screening threshold},
abstract = {Exploiting smart dampers (SmDs) based on data-driven models have been seen as an appropriate approach for many applications such as vehicle suspension system. Reality has shown that the error of SmDs’ identification due to noise in the measured data (MD) sets as well as uncertainty related to the mathematical tools selected to describe control systems reduces control efficiency. To overcome this issue we are interested in finding effective solutions for online filtering noise in MD, selecting and building data-driven models of SmDs, and seeking an appropriate approach to reduce the model errors. To undertake these, we divide the research into two parts; part 1 and part 2. In this current part, we focus on the filtering of the noise by proposing two new filters. Deriving from a discovered optimal data screening threshold (ODST), the first one is an ODST-based filter (ODSTbF) for dealing with random and impulse noise (IN). The second one named combined filter (CoFilter) is a combination of the ODSTbF and the median smoother to extend the filtering capability. To determine the ODST of a data source, a new algorithm for estimating the ODST named AfODST is proposed via an offline process. Many surveys using MD coming from a magnetorheological damper (MRD) are performed to evaluate positive effects of the proposed method.}
}
@incollection{CUMMINS2020231,
title = {Chapter 10 - Artificial intelligence to aid the detection of mood disorders},
editor = {Debmalya Barh},
booktitle = {Artificial Intelligence in Precision Health},
publisher = {Academic Press},
pages = {231-255},
year = {2020},
isbn = {978-0-12-817133-2},
doi = {https://doi.org/10.1016/B978-0-12-817133-2.00010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128171332000100},
author = {Nicholas Cummins and Faith Matcham and Julia Klapper and Björn Schuller},
keywords = {Artificial intelligence, Machine learning, Depression, Bipolar disorder, Objective markers},
abstract = {Changes in mental or mood state are not only marked by changes in cognitive functioning—physiological and behavioral changes also accompany them. There is an ever-growing body of literature indicating that a combination of using mobile and wearable technology to collect physiological and behavioral markers followed by the use of artificial intelligence (AI) to analyze these data can provide objective markers for conditions such as depression and bipolar disorder (BD). In this regard, this chapter is intended to highlight the advantages of using AI-based technologies in clinical psychology settings. Readers are given a brief introduction into the fundamental process that enables machine learning (ML) technologies, a subbranch of AI concerned with pattern recognition, to make generalizable predictions. Also given, is an overview of data sources, in particular, information streams collectable using mobile technologies, typically used in AI-based mental health analysis. Finally, two in-depth case studies are presented which outline a range of different state-of-the-art AI and ML approaches for the detection of either depression or BD.}
}
@article{SUN202042,
title = {A Transcriptional Network Promotes Anthocyanin Biosynthesis in Tomato Flesh},
journal = {Molecular Plant},
volume = {13},
number = {1},
pages = {42-58},
year = {2020},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2019.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1674205219303363},
author = {Chuanlong Sun and Lei Deng and Minmin Du and Jiuhai Zhao and Qian Chen and Tingting Huang and Hongling Jiang and Chang-Bao Li and Chuanyou Li},
keywords = {anthocyanin biosynthesis, purple tomato, MBW complex, master regulator, transcriptional regulation},
abstract = {Dietary anthocyanins are important health-promoting antioxidants that make a major contribution to the quality of fruits. It is intriguing that most tomato cultivars do not produce anthocyanins in fruit. However, the purple tomato variety Indigo Rose, which has the dominant Aft locus combined with the recessive atv locus from wild tomato species, exhibits light-dependent anthocyanin accumulation in the fruit skin. Here, we report that Aft encodes a functional anthocyanin activator named SlAN2-like, while atv encodes a nonfunctional version of the anthocyanin repressor SlMYBATV. The expression of SlAN2-like is responsive to light, and the functional SlAN2-like can activate the expression of both anthocyanin biosynthetic genes and their regulatory genes, suggesting that SlAN2-like acts as a master regulator in the activation of anthocyanin biosynthesis. We further showed that cultivated tomatoes contain nonfunctional alleles of SlAN2-like and therefore fail to produce anthocyanins. Consistently, expression of a functional SlAN2-like gene driven by the fruit-specific promoter in a tomato cultivar led to the activation of the entire anthocyanin biosynthesis pathway and high-level accumulation of anthocyanins in both the peel and flesh. Taken together, our study exemplifies that efficient engineering of complex metabolic pathways could be achieved through tissue-specific expression of master transcriptional regulators.}
}
@article{PAN2020114965,
title = {Data-driven estimation of building energy consumption with multi-source heterogeneous data},
journal = {Applied Energy},
volume = {268},
pages = {114965},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.114965},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920304773},
author = {Yue Pan and Limao Zhang},
keywords = {Building energy estimation, Data mining, Categorical boosting (CatBoost) model, Feature importance},
abstract = {For better energy evaluation and management, a categorical boosting (CatBoost)-based predictive method is presented to accurately estimate building energy consumption by learning large volumes of multi-source heterogeneous data collected from buildings. To be specific, the newly-developed CatBoost model belonging to the ensemble learning has superiority in handling categorical variables and producing reliable results. As a case study, our proposed method is validated in a multi-dimensional dataset about Seattle's building energy performance provided by the city’s government, aiming to estimate the weather normalized site energy use intensity of buildings and characterize its non-linear relationship with other 12 possible influential features. Results from the 5-fold cross-validation demonstrate that the model exhibits a strong ability in predicting the exact value of energy intensity precisely, which can even outperform popular machine learning algorithms including random forest and gradient boosting decision tree under R2 of 0.897. Based on a defined threshold, these predicted values can be classified as the normal or abnormal energy consumption reaching an accuracy of 99.32% for outlier detection, which is helpful in alarming potential risks at an early stage and developing strategies to enhance the energy efficiency. Moreover, results from the established model can be interpreted objectively, suggesting that features concerning the physical and energy characteristics contribute more to energy estimation than environmental features. Since such results understand the building energy consumption and efficiency in a data-driven manner, they can eventually serve as guidance for building owners and designers in designing and renovating buildings to achieve better energy-conserving performance.}
}
@incollection{RATHORE2020109,
title = {Chapter 6 - Deep learning-based security schemes for implantable medical devices},
editor = {Amr Mohamed},
booktitle = {Energy Efficiency of Medical Devices and Healthcare Applications},
publisher = {Academic Press},
pages = {109-130},
year = {2020},
isbn = {978-0-12-819045-6},
doi = {https://doi.org/10.1016/B978-0-12-819045-6.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128190456000066},
author = {Heena Rathore and Amr Mohamed and Mohsen Guizani},
keywords = {Brain, Deep learning, Medical devices, Security, Support vector machine},
abstract = {Deep learning is a subset of machine learning, which learns from the inherent patterns in the data for solving a diverse set of problems such as recognition, classification, and segmentation. It is a neural network-based, biologically inspired model, which has benefitted health, transport, energy, and public safety sectors in diverse ways. It has enabled new potential innovations in these domains, including data analytics, security, treatment, and diagnostics. Intelligent healthcare enables medical specialists to remotely monitor patients, thereby leading to an increase in the popularity of this field in recent years. Doctors are able to provide a better quality of treatment to their patients through a variety of implanted medical devices. The addition of communication ability enables such devices to talk with one another and to the Internet, which leads to the concept of the Internet of Things applied for medical devices. Such devices now have 802.11x or LTE chips on, with the goal that they can converse with one another, in addition to the conventional jobs of sensing and actuating. However, on the other end, the addition of wireless connectivity now makes these devices too prone to be hacked, leading sometimes to lethal events for patients if they are not mitigated. This chapter focuses on how deep learning can be utilized to make these devices more secure while addressing the tradeoffs related to constrained computations, and energy available on such devices.}
}
@incollection{2020583,
title = {Index},
editor = {Joel Faintuch and Salomao Faintuch},
booktitle = {Precision Medicine for Investigators, Practitioners and Providers},
publisher = {Academic Press},
pages = {583-614},
year = {2020},
isbn = {978-0-12-819178-1},
doi = {https://doi.org/10.1016/B978-0-12-819178-1.20001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191781200014}
}
@incollection{BARBA2020171,
title = {Chapter 9 - Translational Tools and Databases in Genomic Medicine},
editor = {George P. Patrinos},
booktitle = {Applied Genomics and Public Health},
publisher = {Academic Press},
pages = {171-187},
year = {2020},
series = {Translational and Applied Genomics},
isbn = {978-0-12-813695-9},
doi = {https://doi.org/10.1016/B978-0-12-813695-9.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128136959000091},
author = {Evaggelia Barba and Maria Koromina and Evangelia-Eirini Tsermpini and George P. Patrinos},
keywords = {Pharmacogenomics, translational tools, databases, personalized medicine, PGx workflows},
abstract = {In the area of -omics and translational biomedical sciences, it is becoming necessary to integrate, normalize, analyze, store, and encrypt an (ever) increasing volume of genomics data. Combining large (genomics) datasets and scientific knowledge into useful clinical information will be beneficial for both human health care and clinical practice. The huge volume of data from biomarker discovery and next-generation sequencing analysis will lead into designing comprehensive analyses for stratification of medicine and guide therapy, which will ultimately benefit patients. This chapter provides an update on useful translational databases and tools as well as on illustrating the basic steps of designing a translational tool for interpreting pharmacogenomics data.}
}
@article{SINGH2020431,
title = {Development and Utilization of a Framework for Data-Driven Life Cycle Management of Battery Cells},
journal = {Procedia Manufacturing},
volume = {43},
pages = {431-438},
year = {2020},
note = {Sustainable Manufacturing - Hand in Hand to Sustainability on Globe: Proceedings of the 17th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.191},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920307770},
author = {Soumya Singh and Max Weeber and Kai Peter Birke and Alexander Sauer},
keywords = {Battery cells, Product Lifecycle, Data-Driven, Framework, End of life, Data acquisition, Data Analytics},
abstract = {Advanced battery cells and modules are increasingly used in a variety of applications. Tracking the state of a cell along the product life cycle and in the consecutive life cycles poses a big challenge to the current battery manufacturing industry and OEMs. This is due to diverse types of influencing factors coming from raw materials, manufacturing, usage, product integration and end of life. The goal of this paper is to develop a framework that provides the capacity to survey and assess relevant data at different stages of a battery life cycle. Here we show a data driven framework for data acquisition of relevant product life cycle information. The acquired data is then handled within a data architecture for application of effective data analytics concepts. In our results, we demonstrate how the framework can be implemented for end of life of battery management. The framework focuses on making the information from each life cycle stage available to decision makers for application of possible data analytics concepts. Such organized processing of battery life cycle data can assist in the development of new business models and improvement of existing battery technologies and services.}
}
@article{ROBLESCARRILLO2020101937,
title = {Artificial intelligence: From ethics to law},
journal = {Telecommunications Policy},
volume = {44},
number = {6},
pages = {101937},
year = {2020},
note = {Artificial intelligence, economy and society},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2020.101937},
url = {https://www.sciencedirect.com/science/article/pii/S030859612030029X},
author = {Margarita {Robles Carrillo}},
keywords = {Artificial intelligence, Ethics, Law},
abstract = {AI is the subject of a wide-ranging debate in which there is a growing concern about its ethical and legal aspects. Frequently, the two are mixed and confused despite being different issues and areas of knowledge. The ethical debate raises two main problems: the first, conceptual, relates to the idea and content of ethics; the second, functional, concerns its relationship with law. Both establish models of social behaviour, but they are different in scope and nature. The juridical analysis is based on a non-formalistic scientific methodology. This means that it is necessary to consider the nature and characteristics of the AI as a preliminary step to the definition of its legal paradigm. In this regard, there are two main issues: the relationship between artificial and human intelligence and the question of the unitary or diverse nature of the AI. From that theoretical and practical basis, the study of the legal system is carried out by examining its foundations, the governance model and the regulatory bases. According to this analysis, throughout the work and in the conclusions, International Law is identified as the principal legal framework for the regulation of AI.}
}
@article{EKBRAND2020143,
title = {Fall-related injuries for three ages groups – Analysis of Swedish registry data 1999–2013},
journal = {Journal of Safety Research},
volume = {73},
pages = {143-152},
year = {2020},
issn = {0022-4375},
doi = {https://doi.org/10.1016/j.jsr.2020.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0022437520300232},
author = {Hans Ekbrand and Robert Ekman and Charlotta Thodelius and Michael Möller},
keywords = {Geographical differences, Fall injuries, Residence, Risk groups, Socio-economic factors},
abstract = {Introduction: The objective of this study was to analyze which factors (including factors pertaining to the individual, the household, and the local area) increase the risk of fall injuries for the three age groups with the highest risk for fall injuries in Sweden. Method: The study combined longitudinal data covering the period 1999–2013 from several different official registries from Statistics Sweden as well as from the Swedish health care system and fitted the models to data using mixed model regressions. Results: Three age groups had a markedly heightened risk for fall injuries: 1–3-year olds, 12–14 year olds, and the elderly (65+). The home was the most common location for fall injuries, as about 40% of all fall injuries occur in the home. Only for the elderly strong predictors for fall injuries were found, and these were: age, single household, and special housing. Conclusions: There is preventive potential in the special residences for the elderly and disabled. People living in these special residences make up a strongly selected group that needs extra safe environments. Our findings indicate that their needs are currently not meet. Practical applications: Design of special residences for the elderly and disabled should aim at reducing the consequences of falling.}
}
@article{JORGENSEN202036,
title = {The palaeodemographic and environmental dynamics of prehistoric Arctic Norway: An overview of human-climate covariation},
journal = {Quaternary International},
volume = {549},
pages = {36-51},
year = {2020},
note = {Long-term perspectives on circumpolar social-ecological systems},
issn = {1040-6182},
doi = {https://doi.org/10.1016/j.quaint.2018.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S1040618217315124},
author = {Erlend Kirkeng Jørgensen},
keywords = {Summed probability distribution (SPD), Palaeodemographic modelling, Human/climate covariation, Northern Norway, Archaeology, Human ecology},
abstract = {This paper presents the first palaeodemographic results of a newly assembled region-wide radiocarbon record of the Arctic regions of northern Norway. The dataset contains a comprehensive collection of radiocarbon dates in the area (N = 1205) and spans the 10,000-year period of hunter-gatherer settlement history from 11500 to 1500 cal BP. Utilizing local, high-resolution palaeoclimate data, the paper performs multi-proxy correlation testing of climate and demographic dynamics, looking for hunter-gatherer responses to climate variability. The paper compares both long-term climate trends and short-term disruptive climate events with the demographic development in the region. The results demonstrate marked demographic fluctuations throughout the period, characterized by a general increase, punctuated by three significant boom and bust-cycles centred on 6000, 3800 and 2200 cal BP, interpreted as instances of climate forcing of human demographic responses. The results strongly suggest the North Cape Current as a primary driver in the local environment and supports the patterns of covariance between coastal climate proxies and the palaeodemographic model. A mechanism of climate forcing mediation through marine trophic webs is proposed as a tentative explanation of the observed demographic fluxes, and a comparison with inter-regional results demonstrate remarkable similarity in demographic trends across mid-Holocene north and west Europe. The results of the north Norwegian radiocarbon record are thus consistent with independent, international efforts, corroborating the existing pan-European results and help further substantiate super-regional climate variability as the primary driver of population dynamics regardless of economic adaptation.}
}
@article{DU2020109811,
title = {In-situ monitoring of occupant behavior in residential buildings ‒ a timely review},
journal = {Energy and Buildings},
volume = {212},
pages = {109811},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.109811},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819330038},
author = {Jia Du and Wei Pan and Cong Yu},
keywords = {Occupant behavior, In-situ monitoring, Residential buildings},
abstract = {Occupant behavior has a significant impact on building energy consumption and sustainable development of the community. In-situ monitoring of occupant behavior is one of the most effective and widely used research methods. It collects data of occupant behavior using smart sensors in the natural environment and, if appropriately designed and applied, can effectively avoid bias in the results. However, previous studies have rarely discussed how to design and apply in-situ monitoring activities in residential buildings. This paper, through a comprehensive and critical literature review, aims to close the knowledge gap on in-situ monitoring of occupant behavior in residential buildings. Multiple review techniques were used. First, a conceptual framework of monitoring activities was proposed based on a narrative appraisal of related publications. Second, the body of literature was established through an exhaustive search of papers by Web of Science and Scopus, two popular search engines. In total, 68 monitoring activities from 74 journal papers were selected according to the inclusion criteria. Third, meta-analysis and meta-synthesis were applied to this body of literature under the conceptual framework to reflect the achievements of previous studies and to explore the challenges facing future research. Results show that previous studies had limited consideration of sampling methods, setting of time interval and monitoring duration, installation of sensors, and the impact of microclimate. Ignoring these issues would reduce the productivity of data collected from in-situ monitoring activities and thus bring bias into the results. To address such limitations, recommendations are given for the design procedure of in-situ monitoring activities. In addition, an empirical rule is proposed with regard to setting the time interval and monitoring duration. Possible areas of future research are also discussed, e.g. occupant behavior in high-rise residential buildings in hot humid zone. The findings of this paper should facilitate the application of in-situ monitoring in building energy research and familiarize future studies with regard to occupant behavior in residential buildings.}
}
@article{JAKIMOW2020104631,
title = {Visualizing and labeling dense multi-sensor earth observation time series: The EO Time Series Viewer},
journal = {Environmental Modelling & Software},
volume = {125},
pages = {104631},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104631},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219308771},
author = {Benjamin Jakimow and Sebastian {van der Linden} and Fabian Thiel and David Frantz and Patrick Hostert},
keywords = {Change detection, Training, Validation, Open-source, QGIS plugin, EO time series viewer},
abstract = {Multi-spectral spaceborne sensors with different spatial resolutions produce Earth observation (EO) time series (TS) with global coverage. The interactive visualization and interpretation of TS is essential to better understand changes in land-use and land-cover and to extract reference information for model calibration and validation. However, available software tools are often limited to specific sensors or optimized for application-specific visualizations. To overcome these limitations, we developed the EO Time Series Viewer, a free and open source QGIS plugin for user-friendly visualization, interpretation and labeling of multi-sensor TS data. The EO Time Series Viewer (i) combines advantages of spatial, spectral and temporal data visualization concepts that are so far not available in a single tool, (ii) provides maximum flexibility in terms of supported data formats, (iii) minimizes the user-interactions required to load and visualize multi-sensor TS data and (iv) speeds-up labeling of TS data based on enhanced GIS vector tools and formats.}
}
@article{CHINCHANACHOKCHAI2020474,
title = {A consumer socialization approach to understanding advertising avoidance on social media},
journal = {Journal of Business Research},
volume = {110},
pages = {474-483},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.01.062},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320300734},
author = {Sydney Chinchanachokchai and Federico {de Gregorio}},
keywords = {Advertising avoidance, Ad avoidance, Social media, Consumer socialization, Survey},
abstract = {The past five years have seen a rapid growth of advertising on social media platforms (SMPs). The current study adopts the consumer socialization framework to investigate predictors of advertising avoidance on SMPs (Facebook, Twitter, and Instagram) via an online survey of 693 U.S. adults. Results show that the effects of SMP usage, susceptibility to social media influence, and susceptibility to peer influence on SMP ad avoidance are all mediated by attitude toward social media advertising in general. Greater SMP usage and higher susceptibility to social media influence are positively related with SMP advertising attitudes, while greater peer influence susceptibility is negatively related. The data also show no differences by demographics for avoidance or attitudes.}
}
@article{SUN2020100028,
title = {Human reliability for safe and efficient civil infrastructure operation and maintenance – A review},
journal = {Developments in the Built Environment},
volume = {4},
pages = {100028},
year = {2020},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2020.100028},
url = {https://www.sciencedirect.com/science/article/pii/S2666165920300247},
author = {Zhe Sun and Jinding Xing and Pingbo Tang and Nancy J. Cooke and Ronald L. Boring},
keywords = {Human reliability, Civil infrastructure, Operation management},
abstract = {Civil infrastructure systems (CIS) require effective systems-level operation and maintenance (O&M) processes to ensure safety and efficiency. Such processes demand significant human efforts in human/team cognition, decision-making, and execution of activities. Poor human behaviors could affect CIS O&M safety and efficiency. This review synthesized human reliability issues on three aspects – 1) Human-Physical, 2) Human-Human, and 3) Human-Cyber reliabilities, and thereby revealed research gaps to guide the development of methods that could achieve guaranteed CIS O&M safety and efficiency. One challenge is the lack of quantitative representations to formalize spatiotemporal, engineering process, and team behavioral models that quantify the impacts of various human factors on CIS O&M safety and efficiency. Besides, limited human/team behavioral data are available yet for comprehending human factors in highly uncertain CIS O&M scenarios. The paper concludes with future directions to facilitate multidisciplinary discussions to tackle the identified challenges.}
}
@article{MOU2020103955,
title = {Exploring spatio-temporal changes of city inbound tourism flow: The case of Shanghai, China},
journal = {Tourism Management},
volume = {76},
pages = {103955},
year = {2020},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2019.103955},
url = {https://www.sciencedirect.com/science/article/pii/S0261517719301530},
author = {Naixia Mou and Rongzheng Yuan and Tengfei Yang and Hengcai Zhang and Jinwen(Jimmy) Tang and Teemu Makkonen},
keywords = {Inbound tourism flow, Spatio-temporal changes, Shanghai, Geotagged photo, Complex network},
abstract = {Knowledge on spatio-temporal changes of inbound tourism flow is important for destination economy, cultural communication and city image. This paper proposes a novel research framework for the spatio-temporal distribution and changes of inbound tourism flow by, first, using R-HDBSCAN clustering algorithm to extract tourism area of interest (AOI), second, by utilizing several key indicators adopted from the complex network theory literature to study the structure of inbound tourism flow with a case study example from Shanghai, China. The results show, first, that tourism in Shanghai is highly concentrated on the most popular AOI clustered in the city center relatively close to each other and, second, that, the inbound tourism flow network of Shanghai has small-world characteristics, while the distribution of its AOI (nodes) and tourist routes (edges) has general power law features, which has been influenced by the World Expo.}
}
@article{MUSYAFFA2020113135,
title = {IOTA: Interlinking of heterogeneous multilingual open fiscal DaTA},
journal = {Expert Systems with Applications},
volume = {147},
pages = {113135},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.113135},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419308528},
author = {Fathoni A. Musyaffa and Maria-Esther Vidal and Fabrizio Orlandi and Jens Lehmann and Hajira Jabeen},
keywords = {Data interlinking, Budget and spending data, String similarity measure, Open data, Translated string matching framework, Cluster computing},
abstract = {Open budget data are among the most frequently published datasets of the open data ecosystem, intended to improve public administrations and government transparency. Unfortunately, the prospects of analysis across different open budget data remain limited due to schematic and linguistic differences. Budget and spending datasets are published together with descriptive classifications. Various public administrations typically publish the classifications and concepts in their regional languages. These classifications can be exploited to perform a more in-depth analysis, such as comparing similar items across different, cross-lingual datasets. However, in order to enable such analysis, a mapping across the multilingual classifications of datasets is required. In this paper, we present the framework for Interlinking of Heterogeneous Multilingual Open Fiscal DaTA (IOTA). IOTA makes use of machine translation followed by string similarities to map concepts across different datasets. To the best of our knowledge, IOTA is the first framework to offer scalable implementation of string similarity using distributed computing. The results demonstrate the applicability of the proposed multilingual matching, the scalability of the proposed framework, and an in-depth comparison of string similarity measures.}
}
@article{FAROOQ20202877,
title = {HMST-Seq-Analyzer: A new python tool for differential methylation and hydroxymethylation analysis in various DNA methylation sequencing data},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {2877-2889},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.09.038},
url = {https://www.sciencedirect.com/science/article/pii/S2001037020304232},
author = {Amna Farooq and Sindre Grønmyr and Omer Ali and Torbjørn Rognes and Katja Scheffler and Magnar Bjørås and Junbai Wang},
keywords = {Methylation analysis, Hydroxy methylation, Differential methylation, Hydroxymethylation-and methylation-sensitive tag sequencing, Whole genome bisulfite sequencing},
abstract = {DNA methylation (5mC) and hydroxymethylation (5hmC) are chemical modifications of cytosine bases which play a crucial role in epigenetic gene regulation. However, cost, data complexity and unavailability of comprehensive analytical tools is one of the major challenges in exploring these epigenetic marks. Hydroxymethylation-and Methylation-Sensitive Tag sequencing (HMST-seq) is one of the most cost-effective techniques that enables simultaneous detection of 5mC and 5hmC at single base pair resolution. We present HMST-Seq-Analyzer as a comprehensive and robust method for performing simultaneous differential methylation analysis on 5mC and 5hmC data sets. HMST-Seq-Analyzer can detect Differentially Methylated Regions (DMRs), annotate them, give a visual overview of methylation status and also perform preliminary quality check on the data. In addition to HMST-Seq, our tool can be used on whole-genome bisulfite sequencing (WGBS) and reduced representation bisulfite sequencing (RRBS) data sets as well. The tool is written in Python with capacity to process data in parallel and is available at (https://hmst-seq.github.io/hmst/).}
}
@article{SUBRAMANIYAN2020106851,
title = {A data-driven approach to diagnosing throughput bottlenecks from a maintenance perspective},
journal = {Computers & Industrial Engineering},
volume = {150},
pages = {106851},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106851},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220305519},
author = {Mukund Subramaniyan and Anders Skoogh and Azam Sheikh Muhammad and Jon Bokrantz and Björn Johansson and Christoph Roser},
keywords = {Throughput bottlenecks, Production system, Manufacturing system, Maintenance, Machine learning, Data science},
abstract = {Prioritising maintenance activities in throughput bottlenecks increases the throughput from the production system. To facilitate the planning and execution of maintenance activities, throughput bottlenecks in the production system must be identified and diagnosed. Various research efforts have developed data-driven approaches using real-time machine data to identify throughput bottlenecks in the system. However, these efforts have mainly focused on identifying bottlenecks and only offer limited maintenance-related diagnostics for them. Moreover, these research efforts have been proposed from an academic perspective using rigorous scientific methods. A number of challenges must be addressed, if existing data-driven approaches are to be adapted to real-world practice. These include identifying relevant data types, data pre-processing and data modelling. Such challenges can be better addressed by including maintenance-practitioner input when developing data-driven approaches. The aim of this paper is therefore to demonstrate a data-driven approach to diagnosing throughput bottlenecks, using the combined knowledge of the maintenance and data-science domains. Diagnostic insights into throughput bottlenecks are obtained using unsupervised machine-learning techniques. The demonstration uses real-world machine datasets extracted from the production line. The novelty of the research presented in this paper is that it shows how inputs from maintenance practitioners can be used to develop data-driven approaches for diagnosing throughput bottlenecks having more practical relevance. By gaining these diagnostic insights, maintenance practitioners can better understand shop-floor throughput bottleneck behaviours from a maintenance perspective and thus prioritise various maintenance actions.}
}
@article{KIM2020127,
title = {Can sustainable restaurant practices enhance customer loyalty? The roles of value theory and environmental concerns},
journal = {Journal of Hospitality and Tourism Management},
volume = {43},
pages = {127-138},
year = {2020},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2020.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1447677020301248},
author = {Myung Ja Kim and C. Michael Hall},
keywords = {Sustainable restaurant practices, Environmental concern, Value theory, Waste reduction, Diner behavior, Eco-friendly eating},
abstract = {Attracting diner participation in sustainable restaurant practices, such as waste reduction, are highly significant for the environment and in reducing the carbon footprint of food consumption. However, there are concerns as to whether the adoption of sustainable practices in restaurant settings is favored by consumers. To examine such issues we aim to identify, by applying value theory, whether sustainable restaurant practices increase diner loyalty. To do this, this research develops and tests an integrated theoretical model on relationships between sustainable restaurant practices, consumer values (hedonic and utilitarian), environmental concern, and diner behavior. Results reveal that sustainable restaurant practices as a second-order construct of food sustainability and waste reduction influence hedonic/utilitarian values. Sustainable restaurant practices also positively influence diner behavior as a second-order construct of participation in waste reduction practices and loyalty to sustainable restaurants. Diner behavior is affected by hedonic/utilitarian values on waste reduction, which are moderated by environmental concern.}
}
@article{HENSCHEL2020117012,
title = {FastSurfer - A fast and accurate deep learning based neuroimaging pipeline},
journal = {NeuroImage},
volume = {219},
pages = {117012},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117012},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920304985},
author = {Leonie Henschel and Sailesh Conjeti and Santiago Estrada and Kersten Diers and Bruce Fischl and Martin Reuter},
keywords = {Freesurfer, Computational neuroimaging, Deep learning, Structural MRI, Artificial intelligence},
abstract = {Traditional neuroimage analysis pipelines involve computationally intensive, time-consuming optimization steps, and thus, do not scale well to large cohort studies with thousands or tens of thousands of individuals. In this work we propose a fast and accurate deep learning based neuroimaging pipeline for the automated processing of structural human brain MRI scans, replicating FreeSurfer’s anatomical segmentation including surface reconstruction and cortical parcellation. To this end, we introduce an advanced deep learning architecture capable of whole-brain segmentation into 95 classes. The network architecture incorporates local and global competition via competitive dense blocks and competitive skip pathways, as well as multi-slice information aggregation that specifically tailor network performance towards accurate segmentation of both cortical and subcortical structures. Further, we perform fast cortical surface reconstruction and thickness analysis by introducing a spectral spherical embedding and by directly mapping the cortical labels from the image to the surface. This approach provides a full FreeSurfer alternative for volumetric analysis (in under 1 ​min) and surface-based thickness analysis (within only around 1 ​h runtime). For sustainability of this approach we perform extensive validation: we assert high segmentation accuracy on several unseen datasets, measure generalizability and demonstrate increased test-retest reliability, and high sensitivity to group differences in dementia.}
}
@article{ADEDEJI2020122104,
title = {Neuro-fuzzy resource forecast in site suitability assessment for wind and solar energy: A mini review},
journal = {Journal of Cleaner Production},
volume = {269},
pages = {122104},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122104},
url = {https://www.sciencedirect.com/science/article/pii/S095965262032151X},
author = {Paul A. Adedeji and Stephen A. Akinlabi and Nkosinathi Madushele and Obafemi O. Olatunji},
keywords = {ANFIS-Based modeling, GIS, MCDM, Site suitability, Solar energy, Wind energy},
abstract = {Site suitability problems in renewable energy studies have taken a new turn since the advent of geographical information system (GIS). GIS has been used for site suitability analysis for renewable energy due to its prowess in processing and analyzing attributes with geospatial components. Multi-criteria decision-making (MCDM) tools are further used for criteria ranking in the order of influence on the study. Upon location of most appropriate sites, the need for intelligent resource forecast to aid in strategic and operational planning becomes necessary if viability of the investment will be enhanced and resource variability will be better understood. One of such intelligent models is the adaptive neuro-fuzzy inference system (ANFIS) and its variants. This study presents a mini-review of GIS-based MCDM facility location problems in wind and solar resource site suitability analysis and resource forecast using ANFIS-based models. Also,a framework for the integration of the two concepts in wind and solar energy studies was presented. Various MCDM techniques for decision making with their strengths and weaknesses were presented. Country specific studies that apply GIS-based method in site suitability were presented with the criteria considered. Similarly, country-specific studies in ANFIS-based resource forecasts for wind and solar energy were also presented. From our findings, there has been no technically valid range of values for spatial criteria in site suitability process for wind and solar resource exploration and the analytical hierarchical process (AHP) has been commonly used for criteria ranking, thus, leaving other MCDM techniques less explored. Also, hybrid ANFIS models are more effective compared to standalone ANFIS models in resource forecast, and ANFIS optimized with population-based models has been mostly used. Finally, we present a roadmap for integrating GIS-MCDM site suitability studies with ANFIS-based modeling for improved strategic and operational planning.}
}
@article{LI2020106123,
title = {Practical considerations of utilizing propensity score methods in clinical development using real-world and historical data},
journal = {Contemporary Clinical Trials},
volume = {97},
pages = {106123},
year = {2020},
issn = {1551-7144},
doi = {https://doi.org/10.1016/j.cct.2020.106123},
url = {https://www.sciencedirect.com/science/article/pii/S1551714420302019},
author = {Qing Li and Jianchang Lin and Andy Chi and Simon Davies},
keywords = {Propensity score, Real-world-data (RWD), Real-world-evidence (RWE), Historical data, External control, Natural history study, Drug development},
abstract = {In recent years, with the rapid increase in the volume and accessibility of Real-World-Data (RWD) and Real-World-Evidence (RWE), we have seen the unprecedented opportunities for their use in drug clinical development and life-cycle management. RWD and RWE have demonstrated the significant potential to improve the design, planning, and execution of clinical development. Furthermore, they can feature in the designs as either a substitute or compliment to traditional clinical trials. However, to utilize RWD and RWE appropriately and wisely, it is critical to apply rigorous statistical methodologies that enable the robustness of results to be characterized and ascertained. Several statistical methodologies including exact matching, propensity score methods, matching-adjusted indirect comparisons and meta-analysis have been proposed for analyzing RWD. Among them, propensity score method is one of the most commonly used methods for non-randomized trials with indirect comparison. Although massive methodologies and examples have been published and discussed since propensity score methods were introduced, systematic review and discussion of how to rigorously use propensity score methods in the practical clinical development is still deficient. This paper introduces commonly used and emerging propensity score methods with detailed discussions of their pros and cons. Three different case studies are presented to illustrate the practical considerations of utilizing propensity score methods in the study design and evaluation using real-world and historical data. Additional considerations including selection of patient populations, endpoints, baseline covariates, propensity score methods, sensitivity analysis and practical implementation flow in clinical development will be discussed.}
}
@incollection{MACHNICKA2020217,
title = {Chapter 9 - Machine learning and deep learning for the advancement of epigenomics},
editor = {Dieter Kabelitz and Jaydeep Bhat},
booktitle = {Epigenetics of the Immune System},
publisher = {Academic Press},
pages = {217-237},
year = {2020},
volume = {16},
series = {Translational Epigenetics},
issn = {25425358},
doi = {https://doi.org/10.1016/B978-0-12-817964-2.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128179642000095},
author = {Magdalena A. Machnicka and Bartek Wilczynski},
keywords = {Machine learning, Epigenomics, Enhancers, Supervised learning, Classification, Neural networks},
abstract = {In the recent decade, mostly because of the development of new techniques for large-scale measurement of different types of epigenetic marks, epigenomics has become a very active field of research. The availability of hundreds of genome-wide datasets has made the task of reconstructing the epigenetic code or its parts a very attractive target for study. As, at the same time, the machine learning methodology was undergoing a phase of rapid growth with the advent of new techniques, most notably the deep learning methods, it was natural for researchers from both fields to apply different machine learning methods to the growing body of the epigenomic data. This was both useful for the advancement of our understanding of the epigenetic mechanisms and a great test bed for the newly developed machine learning tools. This chapter first gives an introduction to the typical problem setting for machine learning in epigenomics and then discusses some of the representative works in the area.}
}
@article{VIGENTINI2020100728,
title = {Evaluating the scaling of a LA tool through the lens of the SHEILA framework: A comparison of two cases from tinkerers to institutional adoption},
journal = {The Internet and Higher Education},
volume = {45},
pages = {100728},
year = {2020},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2020.100728},
url = {https://www.sciencedirect.com/science/article/pii/S109675162030004X},
author = {Lorenzo Vigentini and Danny Y.T. Liu and Natasha Arthars and Mollie Dollinger},
keywords = {Learning analytics, Adoption framework, Scalability and sustainability of LA, Institutional adoption, Analytic autoethnography},
abstract = {The SHEILA framework provides a policy and strategy framework informing the strategic implementation and use of learning analytics. However, as evidenced in several ‘ground-up’ implementations of tools, the institutional preparedness and the governance around use often comes secondary to the policy. In this paper we depart from familiar approaches and evaluate one such example of a tool's development (SRES – Student Relationship Engagement System). SRES' adoption and scaling across two institutions are evaluated using an auto-ethnographic approach scaffolded through the dimensions of the SHEILA framework, focusing on the individual perspectives of the institutional champions who have been central to this journey. This practical approach and the emerging insights may enable other institutions to identify areas of potential improvement and inform senior academic managers about the strategic requirements to scale the approach, accounting for aspects not considered in the initial ‘organic growth’ of the implementation.}
}
@article{PATRIARCA2020104827,
title = {Framing the FRAM: A literature review on the functional resonance analysis method},
journal = {Safety Science},
volume = {129},
pages = {104827},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104827},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520302241},
author = {R. Patriarca and G. {Di Gravio} and R. Woltjer and F. Costantino and G. Praetorius and P. Ferreira and E. Hollnagel},
keywords = {Systematic review, Complex systems, Socio-technical systems, Resilience engineering, Safety},
abstract = {The development of the Functional Resonance Analysis Method (FRAM) has been motivated by the perceived limitations of fundamentally deterministic and probabilistic approaches to understand complex systems’ behaviour. Congruent with the principles of Resilience Engineering, over recent years the FRAM has been progressively developed in scientific terms, and increasingly adopted in industrial environments with reportedly successful results. Nevertheless, a wide literature review focused on the method is currently lacking. On these premises, this paper aims to summarise all available published research in English about FRAM. More than 1700 documents from multiple scientific repositories were reviewed through a protocol based on the PRISMA review technique. The paper aims to uncover a number of characteristics of the FRAM research, both in terms of the method’s application and of the authors contributing to its development. The systematic analysis explores the method in terms of its methodological aspects, application domains, and enhancements in qualitative and quantitative terms, as well as proposing potential future research directions.}
}
@article{CLEMENT202011,
title = {Technologies and Computational Analysis Strategies for CRISPR Applications},
journal = {Molecular Cell},
volume = {79},
number = {1},
pages = {11-29},
year = {2020},
issn = {1097-2765},
doi = {https://doi.org/10.1016/j.molcel.2020.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S109727652030397X},
author = {Kendell Clement and Jonathan Y. Hsu and Matthew C. Canver and J. Keith Joung and Luca Pinello},
abstract = {Summary
The CRISPR-Cas system offers a programmable platform for eukaryotic genome and epigenome editing. The ability to perform targeted genetic and epigenetic perturbations enables researchers to perform a variety of tasks, ranging from investigating questions in basic biology to potentially developing novel therapeutics for the treatment of disease. While CRISPR systems have been engineered to target DNA and RNA with increased precision, efficiency, and flexibility, assays to identify off-target editing are becoming more comprehensive and sensitive. Furthermore, techniques to perform high-throughput genome and epigenome editing can be paired with a variety of readouts and are uncovering important cellular functions and mechanisms. These technological advances drive and are driven by accompanying computational approaches. Here, we briefly present available CRISPR technologies and review key computational advances and considerations for various CRISPR applications. In particular, we focus on the analysis of on- and off-target editing and CRISPR pooled screen data.}
}
@incollection{LADLEY202081,
title = {Chapter 7 - Engagement},
editor = {John Ladley},
booktitle = {Data Governance (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {81-114},
year = {2020},
isbn = {978-0-12-815831-9},
doi = {https://doi.org/10.1016/B978-0-12-815831-9.00007-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158319000072},
author = {John Ladley},
keywords = {Scope, Engage, Assess, Project plan, Program plan, Sponsor, Team},
abstract = {This chapter covers the activities required to start the program development and attain ENGAGEMENT of all stakeholders. I will also introduce the second case study, which will reflect a data governance effort requiring a broader scope and approach than Rocky Health Care.}
}
@article{SUHAIL2020103334,
title = {Orchestrating product provenance story: When IOTA ecosystem meets electronics supply chain space},
journal = {Computers in Industry},
volume = {123},
pages = {103334},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103334},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305686},
author = {Sabah Suhail and Rasheed Hussain and Abid Khan and Choong Seon Hong},
keywords = {Blockchain, Distributed Ledger Technology, Internet of Things, Industrial Internet of Things, Industry 4.0, IOTA, Masked Authenticated Messaging, Provenance, Supply chain, Trustworthy data},
abstract = {Trustworthy data is the fuel for ensuring transparent traceability, precise decision-making, and cogent coordination in the Supply Chain (SC) space. However, the disparate data silos act as a trade barrier in orchestrating the provenance of the product lifecycle; starting from the raw materials to end products available for customers. Besides product traceability, the legacy SCs face several other problems including data validation, data accessibility, security, and privacy issues. In this regard, Blockchain – an advanced Distributed Ledger Technology (DLT) works well to address these challenges by linking fragmented and siloed SC events in an immutable audit trail. However, the underlying challenges with blockchain such as scalability, inability to access off-line data, vulnerability to quantum attacks, and high transaction fees necessitate a new solution to overcome the inefficiencies of the current blockchain design. In this regard, IOTA (the third generation of DLT) leverages a Directed Acyclic Graph (DAG)-based data structure in contrast to linear data structure of blockchain to address such challenges and facilitate a scalable, quantum-resistant, and miner-free solution for the Internet of Things (IoT). After realizing the crucial requirement of traceability and considering the limitations of blockchain in SC, in this work, we propose a provenance-enabled framework for the Electronics Supply Chain (ESC) through a permissioned IOTA ledger. To that end, we construct a transparent product ledger based on trade event details along with time-stamped SC processes to identify operational disruptions or counterfeiting issues. We further exploit the Masked Authenticated Messaging (MAM) protocol provided by IOTA that allows the SC players to procure distributed information while keeping confidential trade flows, ensuring restrictions on data retrieval, and facilitating the integration of fine-grained or coarse-grained data accessibility. Our experimental results show that the time required to construct secure provenance data aggregated from multiple SC entities takes 3s (on average) for a local node and 4s for a remote node, which is justifiable. Furthermore, we perform experiments on Raspberry Pi 3B to verify that the estimated energy consumption at resource-constrained devices is tolerable while implementing the proposed scheme.}
}
@article{CHENG2020101881,
title = {DT-II:Digital twin enhanced Industrial Internet reference framework towards smart manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {62},
pages = {101881},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101881},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300602},
author = {Jiangfeng Cheng and He Zhang and Fei Tao and Chia-Feng Juang},
keywords = {Digital twin, Industrial Internet, Smart manufacturing, Product lifecycle, Industrial Internet platform},
abstract = {In this paper, the interplay and relationship between digital twin and Industrial Internet are discussed at first. The sensing/transmission network capability, which is one of the main characteristics of Industrial Internet, can be a carrier for providing digital twin with a means of data acquisition and transmission. Conversely, with the capability of high-fidelity virtual modeling and simulation computing/analysis, digital twin evolving from lifecycle management for a single product to application in production/manufacturing in the shop-floor/enterprise, can further greatly enhance the simulation computing and analysis of Industrial Internet. This paper proposes a digital twin enhanced Industrial Internet (DT-II) reference framework towards smart manufacturing. To further illustrate the reference framework, the implementation and operation mechanism of DT-II is discussed from three perspectives, including product lifecycle level, intra-enterprise level and inter-enterprise level. Finally, steam turbine is taken as an example to illustrate the application scenes from above three perspectives under the circumstance of DT-II. The differences between with and without DT-II for design and development of steam turbine are also presented.}
}
@article{NAJJARPOUR2020107160,
title = {The effect of formation thickness on the performance of deterministic and machine learning models for rate of penetration management in inclined and horizontal wells},
journal = {Journal of Petroleum Science and Engineering},
volume = {191},
pages = {107160},
year = {2020},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.107160},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520302473},
author = {Mohammad Najjarpour and Hossein Jalalifar and Saeid Norouzi-Apourvari},
keywords = {Rate of penetration, Machine learning, Deterministic models, Data-driven methods, Trust-region method, Particle swarm optimization algorithm},
abstract = {Rate of penetration (ROP) is one of the most important parameters in reducing drilling expenditure. In this paper, a ROP management study has been conducted for a well in Southwest of Iran. As a part of this study, the best approach for ROP prediction was determined by comparing the performance of several methods and particle swarm optimization (PSO) algorithm was implemented for optimization of ROP. This paper highlights the effects of formation thickness and the magnitude of working data-set on the performance of different algorithms which are being used for ROP management studies, especially in horizontal and inclined wells. This topic has special importance, when the results show a massive difference in thin and thick formations for some ROP prediction methods. Comparing the results of different ROP prediction methods showed that hybrid Bingham model has the best performance in ROP prediction; only if a powerful mathematical tool like trust-region method is being used for the determination of its unknown coefficients. This superiority was not generalized in all individual formations and there were different results in the cases of thin and thick formations; so, application of the bag-of-algorithms strategy by using the most accurate method in each formation is suggested for the prediction of ROP. Performing this ROP management project resulted in the prediction of ROP with a total relative error of 13.59% and also 48.30% improvement in ROP.}
}
@incollection{CAO2020735,
title = {Chapter Twenty Three - Health and medical behavior informatics},
editor = {David Dagan Feng},
booktitle = {Biomedical Information Technology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {735-761},
year = {2020},
series = {Biomedical Engineering},
isbn = {978-0-12-816034-3},
doi = {https://doi.org/10.1016/B978-0-12-816034-3.00023-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128160343000237},
author = {Longbing Cao},
keywords = {Behavior analytics, Behavior computing, Behavior informatics, Behavior modeling, Behavioral medicine, Health behavior, Health behavior informatics, Health behavior research, Medical behavior, Medical behavior informatics},
abstract = {Behavior matters, and behavior informatics is the approach to discover and apply behavior intelligence and value for behavior-related problem-solving and interventions. Health behavior research and behavioral medicine take a behavior-centric approach and have been explored for decades in the health and medical communities. However, understanding, analyzing, and managing health and medical behavior from an informatics perspective has not been systematically explored, with very limited progress made. In this paper, we discuss the research areas, concepts, research topics, and applications of health behavior informatics and medical behavior informatics. A conceptual framework of health and medical behavior informatics is introduced, together with a brief overview of behavior informatics. We focus on the discussion opportunities of health and medical behavior informatics, and their connections to health behavior research, behavioral medicine, and data science and artificial intelligence.}
}
@article{ZHU2020102382,
title = {Crowdsourcing-data-based dynamic measures of accessibility to business establishments and individual destination choices},
journal = {Transportation Research Part D: Transport and Environment},
volume = {87},
pages = {102382},
year = {2020},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2020.102382},
url = {https://www.sciencedirect.com/science/article/pii/S1361920920305691},
author = {Yi Zhu and Mi Diao},
keywords = {Dynamic accessibility measure, Destination choice, Human activities, Urban built environment},
abstract = {Characterizing accessibility is essential in research efforts to investigate the reciprocal impacts between human activities and urban space. Although previous studies have proposed various accessibility indicators, they are mostly static in nature, capturing only the spatial dimension of accessibility. The advancement of the research agenda in activity-based studies calls for improved accessibility measures that can capture urban dynamics and human activity-travel patterns at more fine-grained resolutions, both spatially and temporally. In this paper, we draw on the rich spatial and temporal information from online crowdsourcing data on points-of-interest and propose a set of new measures of accessibility to opportunities and services that incorporate the dynamics of businesses environment in Singapore. The new measures consider both the spatial distribution of business establishments and their open time over the day. We then feed the new and traditional accessibility measures into destination choice models to assess whether capturing the temporal variation in accessibility and business environment help to improve the model performance. We find that the model with dynamic accessibility measures outperforms models with traditional static accessibility measures. The results suggest that travelers are likely to consider both the spatial and temporal dimensions of accessibility when choosing destinations for their activities. The proposed dynamic accessibility measures contribute to the research and planning for accessibility by capturing the temporal dimension of accessibility that are not sufficiently addressed in traditional urban built environment measures.}
}
@article{ABRAHAMSSON2020236,
title = {Six years of progress in the oral biopharmaceutics area – A summary from the IMI OrBiTo project},
journal = {European Journal of Pharmaceutics and Biopharmaceutics},
volume = {152},
pages = {236-247},
year = {2020},
issn = {0939-6411},
doi = {https://doi.org/10.1016/j.ejpb.2020.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0939641120301351},
author = {B. Abrahamsson and M. McAllister and P. Augustijns and P. Zane and J. Butler and R. Holm and P. Langguth and A. Lindahl and A. Müllertz and X. Pepin and A. Rostami-Hodjegan and E. Sjögren and M. Berntsson and H. Lennernäs},
keywords = {Biopharmaceutics, PBPK, IVIVC, Dissolution, Drug absorption, Permeability},
abstract = {OrBiTo was a precompetitive collaboration focused on the development of the next generation of Oral Biopharmaceutics Tools. The consortium included world leading scientists from nine universities, one regulatory agency, one non-profit research organisation, three small/medium sized specialist technology companies together with thirteen pharmaceutical companies. The goal of the OrBiTo project was to deliver a framework for rational application of predictive biopharmaceutics tools for oral drug delivery. This goal was achieved through novel prospective investigations to define new methodologies or refinement of existing tools. Extensive validation has been performed of novel and existing biopharmaceutics tools using historical datasets supplied by industry partners as well as laboratory ring studies. A combination of high quality in vitro and in vivo characterizations of active drugs and formulations have been integrated into physiologically based in silico biopharmaceutics models capturing the full complexity of gastrointestinal drug absorption and some of the best practices has been highlighted. This approach has given an unparalleled opportunity to deliver transformational change in European industrial research and development towards model based pharmaceutical product development in accordance with the vision of model-informed drug development.}
}
@incollection{CASTINEIRA2020107,
title = {Chapter 4 - Smart reservoir management in the oil and gas industry},
editor = {Masoud Soroush and Michael Baldea and Thomas F. Edgar},
booktitle = {Smart Manufacturing},
publisher = {Elsevier},
pages = {107-141},
year = {2020},
isbn = {978-0-12-820028-5},
doi = {https://doi.org/10.1016/B978-0-12-820028-5.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200285000047},
author = {David Castiñeira and Hamed Darabi and Xiang Zhai and Wassim Benhallam},
keywords = {Oil and gas, Data analytics, Digital transformation, Reservoir management},
abstract = {The oil and gas (O&G) industry is facing intense pressure to improve operational efficiencies as oil and gas prices continue to fluctuate. Many O&G companies are now rushing into a digital transformation race where digital technologies would be used to create new—of transform traditional—processes. This chapter focuses on the exploration and production part of the O&G industry (the upstream sector), which is primarily concerned with finding and producing crude oil and natural gas. It argues that the current analytical workflow and decision-making process are suboptimal and inadequate for full digital transformation. Higher levels of machine intelligence and automation are needed to bring extreme efficiency to O&G operations. Furthermore, augmentation (e.g., ability to infuse engineering experience into advanced analytics and data-driven solutions) is an absolute necessary component of any smart reservoir management framework, and, therefore, it is a critical element of the digital transformation itself. This chapter proposes specific methods for a more intelligent automation of the upstream business. It presents several (multidisciplinary) case studies that demonstrate the value of automated data processing, systematic engineering and geological workflows, and predictive analytics.}
}
@article{KUMAR2020101184,
title = {Enhancing demographic coverage of hurricane evacuation behavior modeling using social media},
journal = {Journal of Computational Science},
volume = {45},
pages = {101184},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101184},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320304853},
author = {Dheeraj Kumar and Satish V. Ukkusuri},
keywords = {Hurricane evacuation, Enhancing demographic coverage, Spatial coverage, Survey non-response, Evacuation decision making, Geotagged Tweets},
abstract = {Hurricane evacuation is a complex dynamic process and a better understanding of the factors which influence the evacuation behavior of the coastal residents could be helpful in planning a better evacuation policy. Traditionally, the various aspects of the household evacuation decisions have been determined by post-evacuation questionnaire surveys, however, these surveys have seen a deterioration in the quality of the data due to a gradual decrease in response rates in recent years, which may lead to non-response bias. Increased activity of users on social media, especially during emergencies, along with the geo-tagging of the posts, provides an opportunity to gain insights into user's decision-making process, as well as to gauge public opinion and activities using the social media data as a supplement to the traditional survey data. This paper leverages the geo-tagged Tweets posted in the New York City (NYC) and Jacksonville, FL in wake of Hurricane Sandy and Matthew respectively to understand the evacuation behavior of the Twitter users and compare them with that of the survey respondents. We design the Twitter user classification problem as a novel HMM modeling framework to classify them into one of the three categories: outside evacuation zone, evacuees, and non-evacuees. We compare the demographic composition (age, gender, and race/ethnicity) and spatial coverage of Twitter users with that of the survey respondents to highlight the complementary nature of the two data sources, which when combined give a representative sample of the population. We analyze the GPS coordinates of the tweets by evacuees to understand evacuation and return time and evacuation location patterns and compared them with survey respondents. The techniques presented in this paper provide an alternative (fast and voluntary) source of information for modeling evacuation behavior during emergencies, which is complementary in terms of demographics and spatial distribution as compared to the traditional surveys and could be useful for authorities to plan a better evacuation campaign to minimize the risk to the life of the residents of the emergency hit areas.}
}
@article{ILLGEN20201031,
title = {Digital assistance system for target date planning in the initiation phase of large-scale projects},
journal = {Procedia CIRP},
volume = {93},
pages = {1031-1036},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120306004},
author = {Benjamin Illgen and Jan Sender and Wilko Flügge},
keywords = {Digital Assistance, Material flow simulation, Large-scale projects, Target date planning, Production program planning},
abstract = {The project business is characterized by customer individual one-off production. Hence, companies have to adapt to recurring novel planning tasks. However, during the initiation phase only few information is available leading to decision-making using the rule of thumb instead of valid planning data. This paper introduces a digital assistance system for production program planning at early project stages based on material flow simulation. Therefore, product data is generated utilizing past orders in connection with a similarity assessment, the current machine occupancy is fed into the system via a real-time capable data interface and consequently target dates are simulatively evaluated.}
}
@article{LUO2020109776,
title = {Comparing machine learning algorithms in predicting thermal sensation using ASHRAE Comfort Database II},
journal = {Energy and Buildings},
volume = {210},
pages = {109776},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.109776},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819332372},
author = {Maohui Luo and Jiaqing Xie and Yichen Yan and Zhihao Ke and Peiran Yu and Zi Wang and Jingsi Zhang},
keywords = {Building environment, Thermal comfort, Comfort model, PMV},
abstract = {Predicting building occupants’ thermal comfort via machine learning (ML) is a hot research topic. Many algorithms and data processing methods have been applied to predict thermal comfort indices in different contexts. But few studies have systematically investigated how different algorithms and data processing methods can influence the prediction accuracy. In this study, we first summarized the recent literature from perspectives of predicted comfort indices, algorithms applied, input features, data sources, sample size, training proportion, predicting accuracy, etc. Then, we applied nine ML algorithms and three data sampling methods to predict the 3-point and 7-point thermal sensation vote (TSV) in ASHRAE Comfort Database II. The results show that with an accuracy of 66.3% and 61.1% for 3-point and 7-point TSV respectively, Random Forest (RF) has the best performance among the tested algorithms. Compared to the Predicted Mean Vote (PMV) model, ML TSV models generally have higher accuracy in TSV prediction. Based on feature importance analysis, the air temperature, humidity, clothing, air velocity, age, and metabolic rate are the top six important features for TSV prediction. The RF algorithm can achieve 63.6% overall accuracy in TSV prediction with the top three features, which is only 2.6% lower than involving 12 input features. Further, this paper addressed other common considerations in ML comfort model establishment such as tuning hyperparameters, splitting of training and testing data, and encoding methods. We also provided Python and R programming codes and packages as appendixes, which can be a good reference for future studies.}
}
@article{LAUDERDALE2020399,
title = {Model-based pre-election polling for national and sub-national outcomes in the US and UK},
journal = {International Journal of Forecasting},
volume = {36},
number = {2},
pages = {399-413},
year = {2020},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S016920701930189X},
author = {Benjamin E. Lauderdale and Delia Bailey and Jack Blumenau and Douglas Rivers},
keywords = {Election forecasting, Polling, UK politics, US politics, Multilevel regression and stratification},
abstract = {We describe a strategy for applying multilevel regression and post-stratification (MRP) methods to pre-election polling. Using a combination of contemporaneous polling, census data, past election polling, past election results, and other sources of information, we are able to construct probabilistic, internally consistent estimates of national votes and the sub-national electoral districts that determine seats or electoral votes in many electoral systems. We report on the performance of the general framework in three applications that were conducted and released publicly in advance of the 2016 UK Referendum on EU Membership, the 2016 US Presidential Election, and the 2017 UK General Election.}
}
@article{AGGARWAL2020270,
title = {Path planning techniques for unmanned aerial vehicles: A review, solutions, and challenges},
journal = {Computer Communications},
volume = {149},
pages = {270-299},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419308539},
author = {Shubhani Aggarwal and Neeraj Kumar},
keywords = {Unmanned aerial vehicles (UAVs) communications, Path planning techniques, Space-air-ground integrated networks, Coverage and connectivity, Security},
abstract = {Path planning is one of the most important problems to be explored in unmanned aerial vehicles (UAVs) for finding an optimal path between source and destination. Although, in literature, a lot of research proposals exist on the path planning problems of UAVs but still issues of target location and identification persist keeping in view of the high mobility of UAVs. To solve these issues in UAVs path planning, optimal decisions need to be taken for various mission-critical operations performed by UAVs. These decisions require a map or graph of the mission environment so that UAVs are aware of their locations with respect to the map or graph. Keeping focus on the aforementioned points, this paper analyzes various UAVs path planning techniques used over the past many years. The aim of path planning techniques is not only to find an optimal and shortest path but also to provide the collision-free environment to the UAVs. It is important to have path planning techniques to compute a safe path in the shortest possible time to the final destination. In this paper, various path planning techniques for UAVs are classified into three broad categories, i.e., representative techniques, cooperative techniques, and non-cooperative techniques. With these techniques, coverage and connectivity of the UAVs network communication are discussed and analyzed. Based on each category of UAVs path planning, a critical analysis of the existing proposals has also been done. For better understanding, various comparison tables using parameters such as-path length, optimality, completeness, cost-efficiency, time efficiency, energy-efficiency, robustness and collision avoidance are also included in the text. In addition, a number of open research problems based on UAVs path planning and UAVs network communication are explored to provide deep insights to the readers.}
}
@article{BOSSIER2020116601,
title = {The empirical replicability of task-based fMRI as a function of sample size},
journal = {NeuroImage},
volume = {212},
pages = {116601},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116601},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920300884},
author = {Han Bossier and Sanne P. Roels and Ruth Seurinck and Tobias Banaschewski and Gareth J. Barker and Arun L.W. Bokde and Erin Burke Quinlan and Sylvane Desrivières and Herta Flor and Antoine Grigis and Hugh Garavan and Penny Gowland and Andreas Heinz and Bernd Ittermann and Jean-Luc Martinot and Eric Artiges and Frauke Nees and Dimitri Papadopoulos Orfanos and Luise Poustka and Juliane H. {Fröhner Dipl-Psych} and Michael N. Smolka and Henrik Walter and Robert Whelan and Gunter Schumann and Beatrijs Moerkerke},
keywords = {Task-based fMRI, Replicability, Reproducibility, Reliability, Stability, Coherence},
abstract = {Replicating results (i.e. obtaining consistent results using a new independent dataset) is an essential part of good science. As replicability has consequences for theories derived from empirical studies, it is of utmost importance to better understand the underlying mechanisms influencing it. A popular tool for non-invasive neuroimaging studies is functional magnetic resonance imaging (fMRI). While the effect of underpowered studies is well documented, the empirical assessment of the interplay between sample size and replicability of results for task-based fMRI studies remains limited. In this work, we extend existing work on this assessment in two ways. Firstly, we use a large database of 1400 subjects performing four types of tasks from the IMAGEN project to subsample a series of independent samples of increasing size. Secondly, replicability is evaluated using a multi-dimensional framework consisting of 3 different measures: (un)conditional test-retest reliability, coherence and stability. We demonstrate not only a positive effect of sample size, but also a trade-off between spatial resolution and replicability. When replicability is assessed voxelwise or when observing small areas of activation, a larger sample size than typically used in fMRI is required to replicate results. On the other hand, when focussing on clusters of voxels, we observe a higher replicability. In addition, we observe variability in the size of clusters of activation between experimental paradigms or contrasts of parameter estimates within these.}
}
@article{KONG2020118590,
title = {Hierarchical optimal scheduling method of heat-electricity integrated energy system based on Power Internet of Things},
journal = {Energy},
volume = {210},
pages = {118590},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.118590},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220316984},
author = {Xiangyu Kong and Fangyuan Sun and Xianxu Huo and Xue Li and Yu Shen},
keywords = {Integrated energy system, Power internet of things, Bi-level optimization, Demand response},
abstract = {To guarantee the heat demand during winter, most combined heat and power (CHP) units in the integrated energy system (IES) usually work under following heat load (FTL) mode, and the renewable energy accommodation is limited. With the development of Power Internet of Things (PIoT), the information exchange in IES become more frequent. Through flexible interaction between different networks in IES, the accommodation capacity of renewable energy can increase significantly. Therefore, this paper focus on the optimization of IES under the background of PIoT. Firstly, based on the influence of PIoT on IES, a novel integrated demand response (DR) way and the model of the critical components in IES are established. Secondly, a Bi-level economic dispatching method for regional IES is developed, considering the cyber-physical infrastructure of PIoT and IES. The upper level of the dispatching method is used to optimize the overall IES operation; the lower level is to optimize the output of demand-side facilities and integrated DR. Thirdly, with adaptive particle swarm optimization (APSO) algorithm, the solution method for the Bi-level dispatch is established. Finally, the feasibility and effectiveness of the proposed method are verified in a standard IES and a real system in northern China.}
}
@incollection{SILVA2020545,
title = {Chapter 53 - Precision medicine at the academic-industry interface},
editor = {Joel Faintuch and Salomao Faintuch},
booktitle = {Precision Medicine for Investigators, Practitioners and Providers},
publisher = {Academic Press},
pages = {545-560},
year = {2020},
isbn = {978-0-12-819178-1},
doi = {https://doi.org/10.1016/B978-0-12-819178-1.00053-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191781000538},
author = {Patrick J. Silva and Kenneth S. Ramos},
keywords = {Biobanks, Biological specimens, cDNA patent, Clinical trials, Drug development, Gene patent, Genome sequencing, Molecular biomarkers, Probe patent, Reverse translation study},
abstract = {Precision medicine, and specifically the test-then-treat paradigm, have been common practice in medicine for over a century. Recent advances in high-throughput omics, notably next-generation sequencing, have enabled resolution of clinically actionable differences at the molecular level and facilitated greater understanding of human pathogenesis at the individual population scales. These new analytic technologies, coupled with a growing number of therapeutic agents designed to target specific molecules, has catalyzed the transition of treatment modalities from anatomically/pathologically defined interventions, to algorithm-based treatments, rooted on molecularly defined endpoints. Underlying the bifurcated population health-molecular disease perspective is the capability to curate highly granular clinical data, at the case level and throughout the lifecycle of disease. Academic medical centers are at the forefront of developing infrastructure to interface with these challenges, and are thus, a focal point of industry engagement, in the development and validation of precision medicine technology, products, and decision rules.}
}
@article{LI2020101851,
title = {Building Auto-Encoder Intrusion Detection System based on random forest feature selection},
journal = {Computers & Security},
volume = {95},
pages = {101851},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101851},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820301231},
author = {XuKui Li and Wei Chen and Qianru Zhang and Lifa Wu},
keywords = {Network security, Network Intrusion Detection System, Deep learning, Auto-Encoder, Unsupervised clustering},
abstract = {Machine learning techniques have been widely used in intrusion detection for many years. However, these techniques are still suffer from lack of labeled dataset, heavy overhead and low accuracy. To improve classification accuracy and reduce training time, this paper proposes an effective deep learning method, namely AE-IDS (Auto-Encoder Intrusion Detection System) based on random forest algorithm. This method constructs the training set with feature selection and feature grouping. After training, the model can predict the results with auto-encoder, which greatly reduces the detection time and effectively improves the prediction accuracy. The experimental results show that the proposed method is superior to traditional machine learning based intrusion detection methods in terms of easy training, strong adaptability, and high detection accuracy.}
}
@article{KABESHOVA2020103531,
title = {ZiMM: A deep learning model for long term and blurry relapses with non-clinical claims data},
journal = {Journal of Biomedical Informatics},
volume = {110},
pages = {103531},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103531},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301593},
author = {Anastasiia Kabeshova and Yiyang Yu and Bertrand Lukacs and Emmanuel Bacry and Stéphane Gaïffas},
keywords = {Deep learning, Mixture models, Zero-inflation, Electronic health records, Claims datasets, EHR data},
abstract = {This paper considers the problems of modeling and predicting a long-term and “blurry” relapse that occurs after a medical act, such as a surgery. We do not consider a short-term complication related to the act itself, but a long-term relapse that clinicians cannot explain easily, since it depends on unknown sets or sequences of past events that occurred before the act. The relapse is observed only indirectly, in a “blurry” fashion, through longitudinal prescriptions of drugs over a long period of time after the medical act. We introduce a new model, called ZiMM (Zero-inflated Mixture of Multinomial distributions) in order to capture long-term and blurry relapses. On top of it, we build an end-to-end deep-learning architecture called ZiMM Encoder-Decoder (ZiMM ED) that can learn from the complex, irregular, highly heterogeneous and sparse patterns of health events that are observed through a claims-only database. ZiMM ED is applied on a “non-clinical” claims database, that contains only timestamped reimbursement codes for drug purchases, medical procedures and hospital diagnoses, the only available clinical feature being the age of the patient. This setting is more challenging than a setting where bedside clinical signals are available. Our motivation for using such a non-clinical claims database is its exhaustivity population-wise, compared to clinical electronic health records coming from a single or a small set of hospitals. Indeed, we consider a dataset containing the claims of almost all French citizens who had surgery for prostatic problems, with a history between 1.5 and 5 years. We consider a long-term (18 months) relapse (urination problems still occur despite surgery), which is blurry since it is observed only through the reimbursement of a specific set of drugs for urination problems. Our experiments show that ZiMM ED improves several baselines, including non-deep learning and deep-learning approaches, and that it allows working on such a dataset with minimal preprocessing work.}
}
@article{LINDEMANN2020127,
title = {Methodical data-driven integration of customer needs from Social Media into the product development process},
journal = {Procedia CIRP},
volume = {88},
pages = {127-132},
year = {2020},
note = {13th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 17-19 July 2019, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120303383},
author = {Marie Lindemann and Kristof Briele and Robert H. Schmitt},
keywords = {Customer driven innovation, Customer needs, Product development process, Data analytics},
abstract = {Due to the strong technical and functional similarities of today’s consumer products, the emotional and sensory product characteristics become increasingly important in order to stand out from the competition. The objective of this research is to develop a data-driven methodology to systematically integrate customer needs into the product development process. For the comprehensive acquisition of valuable data, various sources of information need to be taken into account. The different sources and data have to be standardized and structured to enable data analytics. This research focuses on Social Media as an important data source to identify customer needs.}
}
@article{ALTENDEITERING2020400,
title = {Scalable Detection of Concept Drift: A Learning Technique Based on Support Vector Machines},
journal = {Procedia Manufacturing},
volume = {51},
pages = {400-407},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920319120},
author = {Marcel Altendeitering and Stephan Dübler},
keywords = {concept drift, SVM, FLORA3, machine learning, energy data},
abstract = {The issue of concept drift describes how static machine-learning models build on historical data can become unreliable over time and pose a significant challenge to many applications. Although, there is a growing body of literature investigating concept drift existing solutions are often limited to a small number of samples or features and do not work well in Industry 4.0 scenarios. We are proposing a novel algorithm that extends the existing concept drift algorithm FLORA3 by utilizing support vector machines for the classification process. Through this combination of dynamic and static approaches the algorithm is capable of effectively analyzing data streams of high volume. For evaluation, we tested our algorithm on the publicly available data set ‘elec2’, which is based on the energy market in Australia. Our results show that the proposed algorithm needs less computational resources compared to other algorithms while maintaining a high level of accuracy.}
}
@article{ISLAM2020119496,
title = {Reshaping WEEE management in Australia: An investigation on the untapped WEEE products},
journal = {Journal of Cleaner Production},
volume = {250},
pages = {119496},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119496},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619343665},
author = {Md Tasbirul Islam and Nazmul Huda},
keywords = {Circular economy, Delphi-AHP approach, AHP rating model, Product-centric material characterization, Priority products, e-waste},
abstract = {In Australia, the National Television and Computer Recycling Scheme (NTCRS) considers only waste televisions, computers, printers, and IT peripherals as e-waste. This study utilizes a combined Delphi-AHP approach to identify potential candidate products that are outside of the scheme. Besides, implementing the traditional AHP method, this study employed an AHP-rating model to minimize computation time in the approach. The results of the study reveal that seven evaluation criteria are crucial for product selection. From 47 initially selected products, 22 products are identified as priority products. Sensitivity analysis was performed based on the different weighting of the evaluation criteria. Potential future activities that need to be performed by the policymakers on the issue are also discussed in the light of the circular economy and environmental protection. This study will also help academics performing the approach, and subsequently, the AHP rating model for selecting/prioritizing alternatives that focused on multi-criteria decision-making problems.}
}
@article{AR2020113543,
title = {Evaluating the feasibility of blockchain in logistics operations: A decision framework},
journal = {Expert Systems with Applications},
volume = {158},
pages = {113543},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113543},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420303675},
author = {Ilker Murat Ar and Ismail Erol and Iskender Peker and Ali Ihsan Ozdemir and Tunc Durmus Medeni and Ihsan Tolga Medeni},
keywords = {Blockchain, Logistics, Multi-Criteria Decision Making, Intuitionistic Fuzzy AHP, Fuzzy VIKOR},
abstract = {The main purpose of this study is to investigate the feasibility of blockchain technology in logistics industry using a quantitative approach. To this end, a decision framework is proposed based on a multi-criteria decision structure that incorporates AHP into VIKOR under Intuitionistic Fuzzy Theory. This integration presents different solutions and rankings based on different decision-making strategies and also captures uncertainty in the evaluation process. While Intuitionistic Fuzzy AHP calculates the importance weights of the proposed criteria indicated as scalability, privacy, interoperability, audit, latency, visibility, trust, and security, Fuzzy VIKOR ranks the logistics operations demonstrated as materials handling, warehousing, order processing, transportation, packaging, fleet management, labeling, vehicle routing and product returns management. The proposed decision framework was applied in a large-scale logistics company located in Turkey. The findings of this study suggest that while the most important criteria are security, visibility and audit, the most feasible logistics operations proved to be transportation, materials handling, warehousing, order processing and fleet management in a possible blockchain implementation. The decision framework in this study may enable decision makers to evaluate the feasibility of blockchain in logistics operations, which is one of the main research gaps in the current blockchain research. Furthermore, this is the first study that integrates AHP and VIKOR methods under Intuitionistic Fuzzy Theory in the context of blockchain.}
}
@incollection{SAPUNDJIEV2020371,
title = {Chapter 20 - International Database of Neutron Monitor Measurements: Development and Applications},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {371-383},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00032-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000321},
author = {D. Sapundjiev and T. Verhulst and S. Stankov},
keywords = {Space weather, Neutron monitor, Neutron monitor network},
abstract = {This chapter presents the development and use of the most reliable and widely used instrument for cosmic ray measurements from the ground, the neutron monitor, and the global database collecting data from the neutron monitors worldwide, the international Neutron Monitor Database (NMDB). NMDB collects data from about 50 stations worldwide, more than half of which in real-time. The construction and principle of operation of the neutron monitor are presented in the beginning, together with the nature of the particles observed at ground level relevant to space weather applications. This naturally introduces the need for a global network of neutron monitors at different geomagnetic locations. The NMDB infrastructure is reviewed and a couple of applications related to space weather are presented.}
}
@article{THAKOR2020100833,
title = {Fintech and banking: What do we know?},
journal = {Journal of Financial Intermediation},
volume = {41},
pages = {100833},
year = {2020},
issn = {1042-9573},
doi = {https://doi.org/10.1016/j.jfi.2019.100833},
url = {https://www.sciencedirect.com/science/article/pii/S104295731930049X},
author = {Anjan V. Thakor},
keywords = {Fintech, Cryptocurrencies, P2P lending, Banking, Systemic risk},
abstract = {This paper is a review of the literature on fintech and its interaction with banking. Included in fintech are innovations in payment systems (including cryptocurrencies), credit markets (including P2P lending), and insurance, with Blockchain-assisted smart contracts playing a role. The paper provides a definition of fintech, examines some statistics and stylized facts, and then reviews the theoretical and empirical literature. The review is organized around four main research questions. The paper summarizes our knowledge on these questions and concludes with questions for future research.}
}
@incollection{GUPTA2020395,
title = {Chapter 20 - Next generation sequencing and its applications},
editor = {Ashish S. Verma and Anchal Singh},
booktitle = {Animal Biotechnology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Boston},
pages = {395-421},
year = {2020},
isbn = {978-0-12-811710-1},
doi = {https://doi.org/10.1016/B978-0-12-811710-1.00018-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128117101000185},
author = {Anuj Kumar Gupta and UD Gupta},
keywords = {Next Generation Sequencing, NGS, animal biotechnology, human health, NGS chemistries, NGS technologies},
abstract = {Next generation sequencing (NGS) has revolutionized nearly every area of biotechnology and has become a core technology in the area of Genomics. With its unprecedented throughput, scalability, and speed of data generation, it enables researchers and clinicians to study biological systems at a level and resolution never before possible. It has been applied to various aspects of biological science including animal, human, and plant biotechnology. The enormous information produced by NGS assists in understanding genomic variations, disease mechanisms, and resistance helping development of better diagnostics, therapies, and better breeds.}
}
@article{NEVES2020102860,
title = {The impacts of open data initiatives on smart cities: A framework for evaluation and monitoring},
journal = {Cities},
volume = {106},
pages = {102860},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102860},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120312087},
author = {Fátima Trindade Neves and Miguel {de Castro Neto} and Manuela Aparicio},
keywords = {Open data, Smart cities, Sustainable urban development, Impact evaluation, Randomized controlled trials, Theoretical framework},
abstract = {As the world's population is becoming progressively urban-dwelling, sustainable development challenges are increasingly concentrated in cities, placing tremendous pressure on society to build more sustainable, innovative, and equitable urban environments. Consequently, today's cities require integrated policies and new innovative ways to manage and improve the complexity of urban living conditions. The growing volume and variety of data produced in the urban ecosystem are crucial for obtaining the city's insights and building knowledge-based solutions for a smarter and more sustainable urban development. In this paper, we look at the open data impacts on these complex ecosystems and its crucial enabler role for the generation and analysis of contextual and actionable data aimed at understanding, managing, and planning the city. Despite the importance of open data, the literature is scarce in systematic and structured research that evaluates its impacts on the smart city context. This paper explores this gap by proposing a theoretical framework, composed of a model and an experiment grounded on the use of Randomized Controlled Trials (RCT), designed to give a more detailed view concerning the context and characteristics of the impacts of open data initiatives on smart cities' sustainable development. This work will contribute to open data management and smart city development, providing boundaries and theoretical insights for further research and experimentation on how open data can be leveraged to develop better smart cities.}
}
@article{DEBAUCHE2020534,
title = {Edge Computing and Artificial Intelligence for Real-time Poultry Monitoring},
journal = {Procedia Computer Science},
volume = {175},
pages = {534-541},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.076},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920317762},
author = {Olivier Debauche and Saïd Mahmoudi and Sidi Ahmed Mahmoudi and Pierre Manneback and Jérôme Bindelle and Frédéric Lebeau},
keywords = {Edge AIoT, Edge Computing, Edge Artificial Intelligence, Internet of Things, Artificial Intelligence, Poultry, Smart Poultry, Gated Recurrent Unit, GRU},
abstract = {Smart Poultry acquires data from aviaries by means of sensor network at reduced intervals of time (every minute) that generate hundred thousands of data. The conjunction of Internet of Things and Artificial Intelligence open the field of the real-time monitoring of poultry and, advance analytics and automation if data is from high quality. In this paper, we propose a scalable monitoring of a poultry achieved with open hardware wireless sensors network and software. We use a Gated Recurrent Unit, an artificial intelligence algorithm to validate and predicate environmental parameters.}
}
@article{BOKRANTZ2020107534,
title = {Smart Maintenance: an empirically grounded conceptualization},
journal = {International Journal of Production Economics},
volume = {223},
pages = {107534},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2019.107534},
url = {https://www.sciencedirect.com/science/article/pii/S0925527319303615},
author = {Jon Bokrantz and Anders Skoogh and Cecilia Berlin and Thorsten Wuest and Johan Stahre},
keywords = {Maintenance, Manufacturing, Industrial organization, Digitalization, Industry 4.0, Smart manufacturing},
abstract = {How do modernized maintenance operations, often referred to as “Smart Maintenance”, impact the performance of manufacturing plants? The inability to answer this question backed by data is a problem for industrial maintenance management, especially in light of the ongoing rapid transition towards an industrial environment with pervasive digital technologies. To this end, this paper, which is the first part of a two-paper series, aims to investigate and answer the question, “What is Smart Maintenance?“. The authors deployed an empirical, inductive research approach to conceptualize Smart Maintenance using focus groups and interviews with more than 110 experts from over 20 different firms. By viewing our original data through the lens of multiple general theories, our findings chart new directions for contemporary and future maintenance research. This paper describes empirical observations and theoretical interpretations cumulating in the first empirically grounded definition of Smart Maintenance and its four underlying dimensions; data-driven decision-making, human capital resource, internal integration, and external integration. In addition, the relationships between the underlying dimensions are specified and the concept structure formally modeled. This study thus achieves concept clarity with respect to Smart Maintenance, thereby making several theoretical and managerial contributions that guide both scholars and practitioners within the field of industrial maintenance management.}
}
@article{JENA2020101723,
title = {Integrated ANN-cross-validation and AHP-TOPSIS model to improve earthquake risk assessment},
journal = {International Journal of Disaster Risk Reduction},
volume = {50},
pages = {101723},
year = {2020},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2020.101723},
url = {https://www.sciencedirect.com/science/article/pii/S2212420919317273},
author = {Ratiranjan Jena and Biswajeet Pradhan},
keywords = {Earthquake, ANN CV, AHP-TOPSIS, GIS, Vulnerability, Risk},
abstract = {The current study presents a novel combination of artificial neural network cross-validation (fourfold ANN-CV) with a hybrid analytic hierarchy process-Technique for Order of Preference by Similarity to Ideal Solution (AHP-TOPSIS) method to improve the earthquake risk assessment (ERA) and applied it to Aceh, Indonesia, to test the model. Recent studies have suggested that neural networks improve probability mapping in a city scale. The network architecture design with probability index remains unexplored in earthquake-based probability studies. This study explored and specified the major indicators needed to improve the predictive accuracy in probability mapping. First, probability mapping was conducted and used for hazard assessment in the next step. Second, a vulnerability map was created based on social and structural factors. Finally, hazard and vulnerability indices were multiplied to produce the ERA, and the population and areas under risk were calculated. Results show that the proposed model achieves 85.4% accuracy, and its consistency ratio is 0.06. Risk varies from very high to high in the city center, approximately covering an area of 23% (14.82 km2) and a total population of 54,695. The model's performance changes on the basis of the input parameters, indicating the selection and importance of input layers on network architecture selection. The proposed model is found to generalize better results than traditional and some existing probabilistic models. The proposed model is simple and transferable to other regions by localizing the input parameters that contribute to earthquake risk mitigation and prevention planning.}
}
@article{LIDDELOW2020820,
title = {Microglia and Astrocytes in Disease: Dynamic Duo or Partners in Crime?},
journal = {Trends in Immunology},
volume = {41},
number = {9},
pages = {820-835},
year = {2020},
note = {Special Issue: Microglia and Astrocytes},
issn = {1471-4906},
doi = {https://doi.org/10.1016/j.it.2020.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1471490620301551},
author = {Shane A. Liddelow and Samuel E. Marsh and Beth Stevens},
keywords = {microglia, astrocytes, neuroimmune, single cell sequencing, neuroinflammation, neurodegeneration},
abstract = {Microglia–astrocyte interactions represent a delicate balance affecting neural cell functions in health and disease. Tightly controlled to maintain homeostasis during physiological conditions, rapid and prolonged departures during disease, infection, and following trauma drive multiple outcomes: both beneficial and detrimental. Recent sequencing studies at the bulk and single-cell level in humans and rodents provide new insight into microglia–astrocyte communication in homeostasis and disease. However, the complex changing ways these two cell types functionally interact has been a barrier to understanding disease initiation, progression, and disease mechanisms. Single cell sequencing is providing new insights; however, many questions remain. Here, we discuss how to bridge transcriptional states to specific functions so we can develop therapies to mediate negative effects of altered microglia–astrocyte interactions.}
}
@article{RODRIGUEZBARROSO2020270,
title = {Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy},
journal = {Information Fusion},
volume = {64},
pages = {270-292},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253520303213},
author = {Nuria Rodríguez-Barroso and Goran Stipcich and Daniel Jiménez-López and José Antonio Ruiz-Millán and Eugenio Martínez-Cámara and Gerardo González-Seco and M. Victoria Luzón and Miguel Angel Veganzones and Francisco Herrera},
keywords = {Federated learning, Differential privacy, Software framework,  Federated Learning framework},
abstract = {The high demand of artificial intelligence services at the edges that also preserve data privacy has pushed the research on novel machine learning paradigms that fit these requirements. Federated learning has the ambition to protect data privacy through distributed learning methods that keep the data in its storage silos. Likewise, differential privacy attains to improve the protection of data privacy by measuring the privacy loss in the communication among the elements of federated learning. The prospective matching of federated learning and differential privacy to the challenges of data privacy protection has caused the release of several software tools that support their functionalities, but they lack a unified vision of these techniques, and a methodological workflow that supports their usage. Hence, we present the Sherpa.ai Federated Learning framework that is built upon a holistic view of federated learning and differential privacy. It results from both the study of how to adapt the machine learning paradigm to federated learning, and the definition of methodological guidelines for developing artificial intelligence services based on federated learning and differential privacy. We show how to follow the methodological guidelines with the Sherpa.ai Federated Learning framework by means of a classification and a regression use cases.}
}
@incollection{LI2020103,
title = {Chapter 4 - Virtual screening of small-molecule libraries},
editor = {Andrea Trabocchi and Elena Lenci},
booktitle = {Small Molecule Drug Discovery},
publisher = {Elsevier},
pages = {103-125},
year = {2020},
isbn = {978-0-12-818349-6},
doi = {https://doi.org/10.1016/B978-0-12-818349-6.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128183496000042},
author = {Qingliang Li},
keywords = {Docking, Molecular fingerprints, Pharmacophore, Shape, Small-molecule library, Virtual screening},
abstract = {Virtual screening is a computational strategy used in drug discovery research to identify active small molecules against a certain biological target from a large chemical library. It typically consists of a cascade of steps to narrow down a set of hits with potential biological activities from a large small-molecule library. Virtual screening methods can be divided into two broad categories: (1) structure-based virtual screening (SBVS) and (2) ligand-based virtual screening (LBVS). SBVS methods utilize target structure information to identify molecules that fit into the binding site of the target structure. It is also named docking-based virtual screening because it uses molecular docking as the core technology. LBVS methods utilize a set of active ligands to identify similar compounds based on certain measurements. In contrast, the LBVS methods directly search a small-molecule library with a set of molecules that known activities. According to the molecular representation, the LBVS methods can be grouped as two–dimensional (2D) fingerprint similarity searching pharmacophore matching and three-dimensional (3D) shape screening. In this work, we introduced the methodology and implementations of both SBVS and LBVS and small-molecular libraries that are widely used in the field, as well as the issues and challenges facing the virtual screening campaigns.}
}
@article{YIN2020107306,
title = {Drilling performance improvement in offshore batch wells based on rig state classification using machine learning},
journal = {Journal of Petroleum Science and Engineering},
volume = {192},
pages = {107306},
year = {2020},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.107306},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520303831},
author = {Qishuai Yin and Jin Yang and Xinxin Hou and Mayank Tyagi and Xu Zhou and Bohan Cao and Ting Sun and Lilin Li and Dongsheng Xu},
keywords = {Offshore batch drilling, Rig state classification, Artificial Neural Network, Rig crew evaluation, Invisible Lost Time, Drilling performance improvement},
abstract = {In this study, a novel Artificial Neural Network (ANN) model is developed for rig state classification and its utility as an efficient method is demonstrated for rig crew performance evaluation. The input characteristic vector of ANN is composed of comprehensive logging data. The structure and parameters of ANN are determined according to the output characteristics of rig state that are used to predict in real-time the rig state of the offshore batch wells. Next, the operational time is analyzed, the histograms of operational time components are visualized and the rig crew performance evaluation is conducted. Additionally, the Invisible Lost Time (ILT) is detected and reduced by comparing the operational time and Key Performance Indicators (KPIs) (as designed and set by the operator) to improve the overall drilling performance. The accuracy of developed ANN model is approximately 93%. Finally, the ILT decreases by 45.23% and the overall drilling performance improves by 31.19% through the application of the ANN model for the rig crew performance evaluation.}
}
@article{DONG2020111602,
title = {A shadow constrained conditional generative adversarial net for SRTM data restoration},
journal = {Remote Sensing of Environment},
volume = {237},
pages = {111602},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2019.111602},
url = {https://www.sciencedirect.com/science/article/pii/S0034425719306224},
author = {Guoshuai Dong and Weimin Huang and William A.P. Smith and Peng Ren},
keywords = {SRTM data restoration, Multi-source data, Shadow geometric constraints, Shadow constrained conditional generative network},
abstract = {The original data produced by the Shuttle Radar Topography Mission (SRTM) tend to have an abundance of voids in mountainous areas where the elevation measurements are missing. In this paper, deep learning models are investigated for restoring SRTM data. To this end, we explore generative adversarial nets, which represent one state-of-the-art family of deep learning models. A conditional generative adversarial network (CGAN) is introduced as the baseline method for filling voids in incomplete SRTM data. The problem regarding shadow violation that possibly arises from the CGAN restored data is investigated. To address this deficiency, shadow geometric constraints based on shadow maps of satellite images are devised. In addition, a shadow constrained conditional generative adversarial network (SCGAN), which incorporates the shadow geometric constraints into the CGAN, is developed. Training the SCGAN model requires both the remote sensing observations (i.e., the original incomplete SRTM data and satellite images) and the ground truth data (i.e., the complete SRTM data, which are manually refined from the incomplete SRTM data with the reference of in-situ measurements). The integration of the multi-source training data enables the SCGAN model to be characterized by comprehensive information including both mountain shape variation and mountain shadow geometry. Experimental results validate the superiority of the SCGAN over the comparison methods, i.e., the interpolation, the convolutional neural network (CNN) and the baseline CGAN, in SRTM data restoration.}
}
@article{YANG2020109665,
title = {Framework for developing a building material property database using web crawling to improve the applicability of energy simulation tools},
journal = {Renewable and Sustainable Energy Reviews},
volume = {121},
pages = {109665},
year = {2020},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2019.109665},
url = {https://www.sciencedirect.com/science/article/pii/S1364032119308706},
author = {Sungwoong Yang and Seunghwan Wi and Ji Hun Park and Hyun Mi Cho and Sumin Kim},
keywords = {Energy simulation, Building material, Crawler, Physical properties, Algorithm, Framework},
abstract = {As the need to evaluate the energy performance of buildings has increased, the use of energy analysis tools has become more widespread and their results are now a key factor in building energy assessments. The current emphasis on the interpretation of building energy performance means that the enhancement of energy analysis tools and their ease of use is worthy of study. Based on analyses of building energy and the physical properties of materials, material properties essential to the analysis of building energy performance were selected. These properties were automatically extracted and stored using an algorithm to collect information from the internet. Based on the designed algorithms, we conducted a questionnaire-based survey and qualitative analysis to measure their convenience. From the analysis, the satisfaction level was found to exceed an average of 80%, resulting in a high level of satisfaction for practitioners using the energy analysis tool. It was also shown that the perceived convenience could be improved by reducing the duration of the search by at least 60% and by applying physical property information to the energy analysis application.}
}
@article{LI2020117434,
title = {Estimating high-resolution PM1 concentration from Himawari-8 combining extreme gradient boosting-geographically and temporally weighted regression (XGBoost-GTWR)},
journal = {Atmospheric Environment},
volume = {229},
pages = {117434},
year = {2020},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2020.117434},
url = {https://www.sciencedirect.com/science/article/pii/S1352231020301722},
author = {Rui Li and Lulu Cui and Hongbo Fu and Ya Meng and Junlin Li and Jianping Guo},
keywords = {AOD, PM, XGBoost, GTWR, Zhejiang},
abstract = {As a much finer particle, particulate matter less than 1 μm (PM1) plays an important role on the haze formation and human health. However, the capability of mapping PM1 concentration is severely impaired by coarse temporal resolution and low estimation accuracy, largely due to the neglect of spatial or temporal autocorrelation of PM1. In order to improve the estimation of high-resolution PM1, here we developed a novel spatiotemporal model named extreme gradient boosting (XGBoost)-geographically and temporally weighted regression (GTWR) using Himawari-8 aerosol optical depth (AOD), meteorological factors, and geographical covariates. The estimation of PM1 over Zhejiang province showed that XGBoost-GTWR method was characterized by greater predictive ability (10-fold cross-validation R2 = 0.83, root mean squared error (RMSE) = 10.72 μg/m3) compared with other 11 models. Additionally, the extrapolation test was performed to validate the robustness of the hybrid model and the result demonstrated that XGBoost-GTWR can accurately predict the out-of-band PM1 concentration (R2 = 0.75 (0.60), RMSE = 12.71 (12.58) μg/m3). The PM1 concentration displayed pronounced spatial heterogeneity, with the highest value in Quzhou (34.72 ± 1.77 μg/m3) and the lowest in Zhoushan (26.39 ± 1.56 μg/m3), respectively. In terms of the seasonality, the highest PM1 concentration was observed in winter (39.06 ± 3.08 μg/m3), followed by those in spring (32.54 ± 3.09 μg/m3) and autumn (30.97 ± 4.50 μg/m3), and the lowest one in summer (25.57 ± 5.22 μg/m3). The high aerosol emission and adverse meteorological conditions (e.g., low boundary layer height and lack of precipitation) were key factors accounting for the peak PM1 concentration observed in winter. Also, the PM1 concentration exhibited significant diurnal variation, peaking at 1500 local solar time (LST) but reaching the lowest value at 1000 LST. This method enhances our capability of estimating hourly PM1 from space, and lays a solid data foundation for improving the assessment of the fine particle-related health effect.}
}
@article{GUO2020100905,
title = {Impact of high-speed rail on urban economic development: An observation from the Beijing-Guangzhou line based on night-time light images},
journal = {Socio-Economic Planning Sciences},
volume = {72},
pages = {100905},
year = {2020},
issn = {0038-0121},
doi = {https://doi.org/10.1016/j.seps.2020.100905},
url = {https://www.sciencedirect.com/science/article/pii/S0038012119303052},
author = {Yunxiang Guo and Wenhao Yu and Zhanlong Chen and Renwei Zou},
keywords = {High-speed rail, Urban economic development, Nighttime light image, Urban agglomeration, China},
abstract = {The high-speed rail (HSR) of China has developed and expanded rapidly and made great achievements in the past twenty years. The ongoing HSR plan is expected to have a significant impact on the urban economy and spatial structure in China. However, relevant data-driven research is still lacking. Traditional data collection approaches such as field surveys are costly to assure the accuracy of materials. In this study, a new remote sensing perspective of night-time light (NTL) was adopted to observe the long-term impact of the HSR on cities along the rail. More specifically, we investigated the impact of the Beijing–Guangzhou High-Speed Railway (BGHSR) on urban economic development by using night-time light data from 2002 to 2018. Such a line connects the capital (located in the north of China) and southern China and lies on the most important geographic axis of the country. Our results find that the construction of BGHSR line has a considerable positive impact on economies of first-tier cities (e.g., Beijing and Guangzhou) and new-first-tier cities (e.g., Zhengzhou, Wuhan, and Changsha), but also hurt some second-tier and third-tier cities such as Baoding and Handan. Generally, the spatial economic pattern of cities along the BGHSR line has been rapidly reshaped with the change of the transportation system. Each city needs to reconsider its role and value in the coming regionalization process to adapt to the national strategy.}
}
@article{SAUVANT2020s207,
title = {Review: Use and misuse of meta-analysis in Animal Science},
journal = {Animal},
volume = {14},
pages = {s207-s222},
year = {2020},
issn = {1751-7311},
doi = {https://doi.org/10.1017/S1751731120001688},
url = {https://www.sciencedirect.com/science/article/pii/S1751731120001688},
author = {D. Sauvant and M.P. Letourneau-Montminy and P. Schmidely and M. Boval and C. Loncke and J.B. Daniel},
keywords = {modeling, database, meta-analysis, random and fixed effects, nutrition},
abstract = {In animal sciences, the number of published meta-analyses is increasing at a rate of 15% per year. This current review focuses on the good practices and the potential pitfalls in the conduct of meta-analyses in animal sciences, nutrition in particular. Once the study objectives have been defined, several key phases must be considered when doing a meta-analysis. First, as a principle of traceability, criteria used to select or discard publications should be clearly stated in a way that one could reproduce the final selection of data. Then, the coding phase, aiming to isolate specific experimental factors for an accurate graphical and statistical interpretation of the database, is discussed. Following this step, the study of the levels of independence of factors and of the degree of data balance of the meta-design represents an essential phase to ensure the validity of statistical processing. The consideration of the study effect as fixed or random must next be considered. It appears based on several examples that this choice does not generally have any influence on the conclusions of a meta-analysis when the number of experiments is sufficient.}
}
@article{LEBO2020163,
title = {Bioinformatics in Clinical Genomic Sequencing},
journal = {Clinics in Laboratory Medicine},
volume = {40},
number = {2},
pages = {163-187},
year = {2020},
note = {Precision Medicine in Practice: Molecular Diagnosis Enabling Precision Therapies},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2020.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0272271220300159},
author = {Matthew S. Lebo and Limin Hao and Chiao-Feng Lin and Arti Singh},
keywords = {Genome sequencing, Exome sequencing, Alignment, Variant calling, Annotation, Filtration, Validation, Bioinformatic infrastructure}
}
@article{FEENSTRA2020104832,
title = {The AirSensor open-source R-package and DataViewer web application for interpreting community data collected by low-cost sensor networks},
journal = {Environmental Modelling & Software},
volume = {134},
pages = {104832},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104832},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220308896},
author = {Brandon Feenstra and Ashley Collier-Oxandale and Vasileios Papapostolou and David Cocker and Andrea Polidori},
keywords = {Community air monitoring, Citizen scientist, Low-cost air quality sensor, Open-source R package, Particulate matter PM2.5, Data interpretation},
abstract = {While large-scale low-cost sensor networks are now recording air pollutant concentrations at finer spatial and temporal scales than previously measured, the large environmental data sets generated by these sensor networks can become overwhelming when considering the scientific skills required to analyze the data and generate interpretable results. This paper summarizes the development of an open-source R package (AirSensor) and interactive web application (DataViewer) designed to address the environmental data science challenges of visualizing and understanding local air quality conditions with community networks of low-cost air quality sensors. AirSensor allows users to access historical data, add spatial metadata, and create maps and plots for viewing community monitoring data. The DataViewer application was developed to incorporate the functionality and plotting functions of the R package into a user-friendly web experience that would serve as the primary source for data communication for community-based organizations and citizen scientists.}
}
@article{KOUHESTANI2020103069,
title = {IFC-based process mining for design authoring},
journal = {Automation in Construction},
volume = {112},
pages = {103069},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.103069},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519306594},
author = {Sobhan Kouhestani and Mazdak Nik-Bakht},
keywords = {Building Information Modeling, Business processes management, Data mining, Process mining, BIM management, BIM Execution Planning},
abstract = {Building Information Modelling (BIM) is defined as the process of creation and management of digital replica for building products in a collaborative design set-up. On this basis, BIM as a digital collaboration platform in AECO (Architecture, Engineering, Construction, and Operation) industry, can be upgraded to assist monitoring, control and improvement of the business processes related to planning, design, construction and operation of building facilities. The main problem in this regard, is the wastage of data related to activities completed by different actors during the project; and subsequently, the lack of analytics to discover latent patterns in collaboration and execution of such processes. The present study aims to enable BIM to capture digital footprints of project actors and create event logs for design authoring phase of building projects. This is done using files in IFC (Industry Foundation Classes) format, archived during the design process. We have developed algorithms to create event logs from such archives, and analyzed the event logs using process mining (i.e. process discovery, conformance checking and bottleneck analysis), to identify measures derived from as-happened processes. BIM managers can implement such measures in monitoring, controlling and re-engineering work processes related to design authoring. Two case studies were completed to validate and verify the products and findings of the research. Our results show that process models discovered/fine-tuned at various resolutions and from different perspectives (including ‘actor-centric’ and ‘phase-centric’ views) can provide a realistic view of the BIM project execution. This includes understanding the structure of collaboration and hand-over of work; evaluation of compliance with the BIM execution plan; and detection of bottlenecks and re-works. While the scope of the study has been limited to design authoring processes, this mindset can be extended to other BIM uses, and other phases (such as construction and operation) of building projects. Given the growing efforts on upgrading BIM to capture and formalize the lifecycle data on the products, processes and actors, this study can strongly support BIM managers with documentation and evaluation of the business processes and workflows in their project teams.}
}
@incollection{WEBER202059,
title = {Chapter 3 - Smart manufacturing in the semiconductor industry: An evolving nexus of business drivers, technologies, and standards},
editor = {Masoud Soroush and Michael Baldea and Thomas F. Edgar},
booktitle = {Smart Manufacturing},
publisher = {Elsevier},
pages = {59-105},
year = {2020},
isbn = {978-0-12-820028-5},
doi = {https://doi.org/10.1016/B978-0-12-820028-5.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200285000035},
author = {Alan Weber},
keywords = {Semiconductor manufacturing, SEMI standards, Manufacturing automation, Manufacturing applications, Traceability, Factory KPIs, Connectivity, Process control, Equipment data acquisition, Equipment model},
abstract = {The semiconductor industry embarked on its own “smart manufacturing” journey well over 30 years ago, long before the term was coined. The continuous productivity improvements that we now take for granted are essential for creating and building the devices that fuel our electronic-based global economy and maintaining commercial viability in a hypercompetitive industry. However, what we have learned in the process is that like many scientific endeavors, it is a journey without a destination. As new market opportunities are met with new device and system technologies in an ever-changing business environment, the list of manufacturing challenges is never complete. This is where the global smart manufacturing initiative enters the picture. Although its key tenets are not specific to the semiconductor industry, the attention it drew to this topic triggered the formation of the SEMI Smart Manufacturing Community, which now provides a forum for thought leaders across the semiconductor manufacturing value chain to focus on these important challenges. To put this initiative in its proper perspective, this chapter explores the past, present, and future of smart manufacturing in the semiconductor industry.}
}
@article{PANGBOURNE202035,
title = {Questioning mobility as a service: Unanticipated implications for society and governance},
journal = {Transportation Research Part A: Policy and Practice},
volume = {131},
pages = {35-49},
year = {2020},
note = {Developments in Mobility as a Service (MaaS) and Intelligent Mobility},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2019.09.033},
url = {https://www.sciencedirect.com/science/article/pii/S0965856418309601},
author = {Kate Pangbourne and Miloš N. Mladenović and Dominic Stead and Dimitris Milakis},
keywords = {Smart mobility, Mobility as a service, Governance, Equity, Technological transition},
abstract = {In this paper we focus on the development of a new service model for accessing transport, namely Mobility as a Service (MaaS) and present one of the first critical analyses of the rhetoric surrounding the concept. One central assumption of one prevalent MaaS conceptualization is that transport services are bundled into service packages for monthly payment, as in the telecommunication or media service sectors. Various other forms of MaaS are being developed but all tend to offer door-to-door multi-modal mobility services, brokered via digital platforms connecting users and service operators. By drawing on literature concerned with socio-technical transitions, we address two multi-layered questions. First, to what extent can the MaaS promises (to citizens and cities) be delivered, and what are the unanticipated societal implications that could arise from a wholesale adoption of MaaS in relation to key issues such as wellbeing, emissions and social inclusion? Second, what are de facto challenges for urban governance if the packaged services model of MaaS is widely adopted, and what are the recommended responses? To address these questions, we begin by considering the evolution of intelligent transport systems that underpin the current vision of MaaS and highlight how the new business model could provide a mechanism to make MaaS truly disruptive. We then identify a set of plausible unanticipated societal effects that have implications for urban planning and transport governance. This is followed by a critical assessment of the persuasive rhetoric around MaaS that makes grand promises about efficiency, choice and freedom. Our conclusion is that the range of possible unanticipated consequences carries risks that require public intervention (i.e. steering) for reasons of both efficiency and equity.}
}