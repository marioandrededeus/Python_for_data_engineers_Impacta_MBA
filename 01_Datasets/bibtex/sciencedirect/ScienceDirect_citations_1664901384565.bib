@article{FARAHZADI2018176,
title = {Middleware technologies for cloud of things: a survey},
journal = {Digital Communications and Networks},
volume = {4},
number = {3},
pages = {176-188},
year = {2018},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352864817301268},
author = {Amirhossein Farahzadi and Pooyan Shams and Javad Rezazadeh and Reza Farahbakhsh},
keywords = {CoT, IoT, Middleware, Fog computing, Cloud},
abstract = {The next wave of communication and applications will rely on new services provided by the Internet of Things which is becoming an important aspect in human and machines future. IoT services are a key solution for providing smart environments in homes, buildings, and cities. In the era of massive number of connected things and objects with high growth rate, several challenges have been raised, such as management, aggregation, and storage for big produced data. To address some of these issues, cloud computing emerged to the IoT as Cloud of Things (CoT), which provides virtually unlimited cloud services to enhance the large-scale IoT platforms. There are several factors to be considered in the design and implementation of a CoT platform. One of the most important and challenging problems is the heterogeneity of different objects. This problem can be addressed by deploying a suitable “middleware” which sits between things and applications as a reliable platform for communication among things with different interfaces, operating systems, and architectures. The main aim of this paper is to study the middleware technologies for CoT. Toward this end, we first present the main features and characteristics of middlewares. Next, we study different architecture styles and service domains. Then, we present several middlewares that are suitable for CoT-based platforms and finally, a list of current challenges and issues in the design of CoT-based middlewares is discussed.}
}
@article{ZANETTI2018151,
title = {To accelerate cancer prevention in Europe: Challenges for cancer registries},
journal = {European Journal of Cancer},
volume = {104},
pages = {151-159},
year = {2018},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0959804918313716},
author = {R. Zanetti and L. Sacchetto and J.W. Coebergh and S. Rosso},
keywords = {Cancer registration, Europe, Preventive interventions, Cancer burden, Data innovation},
abstract = {The availability of population-based cancer registry (CR) data is paramount in the development of modern oncology. Major contributions consisted in accurately measuring cancer burden (incidence, survival and prevalence, beside mortality), identifying and quantifying risk factors (case control and cohort studies that, in the last two decades, included gene variant assessment) and evaluating outcomes of treatments and preventive interventions, including mass screening. Cancer registration coverage of European populations progressed slowly since 1940 and is now almost 50%. Areas lacking high-quality national population-based cancer registration still exist within large countries such as France, Italy, Romania and Spain, Germany and Poland having national plans and legislation to reach complete coverage. Depending on programme ownership, history and institutional organisation, European CRs showed huge variations in the scope (research domain), size, available resources and finally exploitation of collected data. This reflects their heterogeneous origins stemming from different professional backgrounds and healthcare systems. This review discusses not only the potential for contributing to acceleration of prevention but also the coverage expansion by and innovation of CR organizations. The latter can be attained not only by more standardisation in institutional organisation and operative methodologies but also by intensification of scientific production and risk communication. The CR's agenda should focus on cancers caused by identifiable risk factor(s) that are amenable to preventive actions, including early detection; short-term priorities usually are with tobacco, and medium-term priorities are with alcohol, occupational exposures, infection-related cancers and ultraviolet-related skin cancers, while obesity-related cancers are likely to increase gradually further in the long term.}
}
@article{STEWART2017736,
title = {Crowdsourcing Samples in Cognitive Science},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {10},
pages = {736-748},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301316},
author = {Neil Stewart and Jesse Chandler and Gabriele Paolacci},
abstract = {Crowdsourcing data collection from research participants recruited from online labor markets is now common in cognitive science. We review who is in the crowd and who can be reached by the average laboratory. We discuss reproducibility and review some recent methodological innovations for online experiments. We consider the design of research studies and arising ethical issues. We review how to code experiments for the web, what is known about video and audio presentation, and the measurement of reaction times. We close with comments about the high levels of experience of many participants and an emerging tragedy of the commons.}
}
@article{SERVA2023109391,
title = {Testing two NIRs instruments to predict chicken breast meat quality and exploiting machine learning approaches to discriminate among genotypes and presence of myopathies},
journal = {Food Control},
volume = {144},
pages = {109391},
year = {2023},
issn = {0956-7135},
doi = {https://doi.org/10.1016/j.foodcont.2022.109391},
url = {https://www.sciencedirect.com/science/article/pii/S0956713522005849},
author = {Lorenzo Serva and Giorgio Marchesini and Marco Cullere and Rebecca Ricci and Antonella {Dalle Zotte}},
keywords = {Chicken, Genotype, Myopathies, Meat quality, Near infra-red spectroscopy, Machine learning},
abstract = {To discriminate among three poultry meat types (hybrid broiler, hybrid broiler affected by breast myopathies, and slow-growing native breed), and to predict the proximate and the amino acid (AA) composition of breast meat, two NIRs (Near-Infrared) instruments operating between 850 and 2500 nm coupled with chemometric algorithms and Machine Learning (ML) approaches, were tested. The Partial Least Square Discriminant Analysis was performed for genotype identification, resulting in a Mathew Correlation Coefficient (MCC) ranging from 0.61 to 1.00, according to the spectra pretreatments and instrument adopted. The Partial Least Square Regression allowed reaching a high cross-validation determination coefficient (R2cv) for crude protein (0.98) and ether extract (0.99), while only three AA (aspartic acid, alanine and methionine) reached R2cv > 0.55. The latter predictions were successfully used to discriminate between genotypes using Factorial Discriminant Analysis, with an MCC ranging from 0.67 to 0.95. Overall, both tested NIRs instruments allowed to determine the chemical composition of fresh and freeze-dried chicken meat. In this sense, a significant improvement of NIRs data interpretability was achieved thanks to the use of ML algorithms, as it was possible to discriminate the chemical composition of meat depending on the genetic group and the presence of breast myopathies.}
}
@article{PI201819,
title = {Understanding Transit System Performance Using AVL-APC Data: An Analytics Platform with Case Studies for the Pittsburgh Region},
journal = {Journal of Public Transportation},
volume = {21},
number = {2},
pages = {19-40},
year = {2018},
issn = {1077-291X},
doi = {https://doi.org/10.5038/2375-0901.21.2.2},
url = {https://www.sciencedirect.com/science/article/pii/S1077291X22000601},
author = {Xidong Pi and Mark Egge and Jackson Whitmore and Zhen (Sean) Qian and Amy Silbermann},
keywords = {Transit system, Automatic Vehicle Location, Automatic Passenger Counting, data analytics platform, performance metrics, bus bunching, service quality},
abstract = {This paper introduces a novel transit data analytics platform for public transit planning, assessing service quality and revealing service problems in high spatiotemporal resolution for public transit systems based on Automatic Passenger Counting (APC) and Automatic Vehicle Location (AVL) technologies. The platform offers a systematic way for users and decision makers to understand system performance from many aspects of service quality, including passenger waiting time, stop-skipping frequency, bus bunching level, bus travel time, on-time performance, and bus fullness. The AVL-APC data from September 2012 to March 2016 were archived in a database to support the development of a user-friendly web application that allows both users and managers to interactively query bus performance metrics for any bus routes, stops, or trips for any time period. This paper demonstrates a case study using the platform to examine bus bunching in a transit system operated by the Port Authority of Allegheny County (PAAC) in Pittsburgh. It is found that the incidence of bus bunching is heavily impacted by the location on the route as well as the time of day, and the bunching problem is more severe for bus routes operating in mixed traffic than for bus rapid transit, which operates along a dedicated busway. Furthermore, a second case study is presented with a comprehensive analysis on a representative route in Pittsburgh under schedule changes. Suggestions for operation of this route to improve service quality are proposed based on the data analytics results.}
}
@article{CHAO201858,
title = {A gray-box performance model for Apache Spark},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {58-67},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17323233},
author = {Zemin Chao and Shengfei Shi and Hong Gao and Jizhou Luo and Hongzhi Wang},
keywords = {Gray-box method, Apache Spark, Performance prediction model, Machine learning},
abstract = {Apache Spark is a powerful open source data processing platform. It is getting more and more popular with the growing need of processing massive amounts of data. A performance prediction model not only helps administrators to have a better understanding of system behavior, but also is useful in performance tuning. However, considering the complex application processing mechanism of Spark, it is not an easy job to model the relationship between system performance and configuration settings. In this paper, we present a gray-box performance model for Spark applications based on machine learning algorithms. Given a specific Spark application, the size of its input data and some key system parameters, this performance model is able to forecast its execution time according to history information. To achieve better accuracy, our model takes basic hardware information and the resource allocation strategy of Spark into consideration. In our experiments, result shows our gray-box model is better than typical black-box approaches in most of the cases. We consider this model is helpful for further researches on Apache Spark.}
}
@article{KIM201795,
title = {Report of the Second Asian Prostate Cancer (A-CaP) Study Meeting},
journal = {Prostate International},
volume = {5},
number = {3},
pages = {95-103},
year = {2017},
issn = {2287-8882},
doi = {https://doi.org/10.1016/j.prnil.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S2287888217300491},
author = {Choung-Soo Kim and Ji Youl Lee and Byung Ha Chung and Wun-Jae Kim and Ng Chi Fai and Lukman Hakim and Rainy Umbas and Teng Aik Ong and Jasmine Lim and Jason L. Letran and Edmund Chiong and Tong-lin Wu and Bannakij Lojanapiwat and Levent Türkeri and Declan G. Murphy and Robert A. Gardiner and Kim Moretti and Matthew Cooperberg and Peter Carroll and Seong Ki Mun and Shiro Hinotsu and Yoshihiko Hirao and Seiichiro Ozono and Shigeo Horie and Mizuki Onozawa and Yasuhide Kitagawa and Tadaichi Kitamura and Mikio Namiki and Hideyuki Akaza},
keywords = {Asia, Database, Prospective study, Prostate cancer},
abstract = {The Asian Prostate Cancer (A-CaP) Study is an Asia-wide initiative that has been developed over the course of 2 years. The study was launched in December 2015 in Tokyo, Japan, and the participating countries and regions engaged in preparations for the study during the course of 2016, including patient registration and creation of databases for the purpose of the study. The Second A-CaP Meeting was held on September 8, 2016 in Seoul, Korea, with the participation of members and collaborators from 12 countries and regions. Under the study, each participating country or region will begin registration of newly diagnosed prostate cancer patients and conduct prognostic investigations. From the data gathered, common research themes will be identified, such as comparisons among Asian countries of background factors in newly diagnosed prostate cancer patients. This is the first Asia-wide study of prostate cancer and has developed from single country research efforts in this field, including in Japan and Korea. At the Second Meeting, participating countries and regions discussed the status of preparations and discussed various issues that are being faced. These issues include technical challenges in creating databases, promoting participation in each country or region, clarifying issues relating to data input, addressing institutional issues such as institutional review board requirements, and the need for dedicated data managers. The meeting was positioned as an opportunity to share information and address outstanding issues prior to the initiation of the study. In addition to A-CaP-specific discussions, a series of special lectures was also delivered as a means of providing international perspectives on the latest developments in prostate cancer and the use of databases and registration studies around the world.}
}
@article{BUER20181035,
title = {The Data-Driven Process Improvement Cycle: Using Digitalization for Continuous Improvement},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1035-1040},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.471},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318315994},
author = {Sven-Vegard Buer and Giuseppe Ismael Fragapane and Jan Ola Strandhagen},
keywords = {digitalization, digitization, Industry 4.0, improvement cycle, lean manufacturing},
abstract = {Industry 4.0 is the first industrial revolution to be announced a priori, and there is thus a significant ambiguity surrounding the term and what it actually entails. This paper aims to clearly define digitalization, a key enabler of Industry 4.0, and illustrate how it can be used for improvement through proposing an improvement cycle and an associated digitalization typology. These tools can be used by organizations to guide improvement processes, focusing on the new possibilities introduced by the enormous amounts of data currently available. The usage of the tools is illustrated by presenting four scenarios from Kanban control, where each scenario is mapped according to their digitalization level.}
}
@incollection{CUMMINS2017183,
title = {Chapter 6 - Enterprise Data Management},
editor = {Fred A. Cummins},
booktitle = {Building the Agile Enterprise (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {183-208},
year = {2017},
series = {The MK/OMG Press},
isbn = {978-0-12-805160-3},
doi = {https://doi.org/10.1016/B978-0-12-805160-3.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128051603000065},
author = {Fred A. Cummins},
keywords = {Data management architecture, Master data, Data modeling, Business metadata, Data services, Data ownership, Legal records, Data residency, Analytics, Knowledge management, Enterprise logical data model},
abstract = {Data are the means to coordinate activity, record actions and responsibilities, solve problems, develop plans, and measure performance. Every business activity has supporting data that, in context, are information about the business. This chapter describes a data management architecture that is appropriate for the agile enterprise, to support the integration of the building blocks, recognize relevant events and trends, capture and share knowledge, and evaluate performance and value delivery. The architecture includes management of master data, business metadata, and other key enterprise data services.}
}
@article{KANG2023118814,
title = {Hierarchical level fault detection and diagnosis of ship engine systems},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118814},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118814},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422018322},
author = {Young-Jin Kang and Yoojeong Noh and Min-Sung Jang and Sunyoung Park and Ju-Tae Kim},
keywords = {Optimal hierarchical clustering and dimension reduction, Dynamic thresholds, Domain knowledge, Sensor error, Abnormality labeling, Duel fuel marine engine},
abstract = {As smart ships are developed, research into engine management through remote monitoring and support from data collected at onshore control centers is being conducted. However, ship engine data collected from various sensors have high dimensions, large measurement errors, few labeled data, and insufficient amounts and quality of data, making it difficult to determine the condition of engines with complex failure modes under various operating environments.This study proposes a hierarchical level fault detection and diagnosis (HL-FDD) method that combines domain knowledge of ship engines and advanced data analysis techniques. The developed method extracts key features of reduced dimensions from the original variables (sensors) through an optimal hierarchical clustering and dimension-reduction model, allowing a hierarchy divided into the top (the entire system combining all features), middle (subsystems and feature sets), and bottom (components and sensors) levels. The reduced key features are used to generate robust regression models and dynamic thresholds (prediction intervals) according to the engine load, and dynamic thresholds determine whether the engine’s condition is normal or abnormal. The dynamic thresholds are able to automatically label abnormal conditions of the engine. Once anomalies are detected at the top level, the proposed method can sequentially search for data on features belonging to the middle and bottom hierarchies for detailed fault diagnosis to determine which engine subsystem or component(s) caused the engine fault(s). Actual data collected by ship operators verify the proposed method’s efficiency, reliability, and accuracy.}
}
@article{ZHANG2018576,
title = {DRI-RCNN: An approach to deceptive review identification using recurrent convolutional neural network},
journal = {Information Processing & Management},
volume = {54},
number = {4},
pages = {576-592},
year = {2018},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317304272},
author = {Wen Zhang and Yuhang Du and Taketoshi Yoshida and Qing Wang},
keywords = {Deceptive review identification, Recurrent convolutional vector, Contextual knowledge, Word embedding, DRI-RCNN},
abstract = {With the widespread of deceptive opinions in the Internet, how to identify online deceptive reviews automatically has become an attractive topic in research field. Traditional methods concentrate on extracting different features from online reviews and training machine learning classifiers to produce models to decide whether an incoming review is deceptive or not. This paper proposes an approach called DRI-RCNN (Deceptive Review Identification by Recurrent Convolutional Neural Network) to identify deceptive reviews by using word contexts and deep learning. The basic idea is that since deceptive reviews and truthful reviews are written by writers without and with real experience respectively, the writers of the reviews should have different contextual knowledge on their target objectives under description. In order to differentiate the deceptive and truthful contextual knowledge embodied in the online reviews, we represent each word in a review with six components as a recurrent convolutional vector. The first and second components are two numerical word vectors derived from training deceptive and truthful reviews, respectively. The third and fourth components are left neighboring deceptive and truthful context vectors derived by training a recurrent convolutional neural network on context vectors and word vectors of left words. The fifth and six components are right neighboring deceptive and truthful context vectors of right words. Further, we employ max-pooling and ReLU (Rectified Linear Unit) filter to transfer recurrent convolutional vectors of words in a review to a review vector by extracting positive maximum feature elements in recurrent convolutional vectors of words in the review. Experiment results on the spam dataset and the deception dataset demonstrate that the proposed DRI-RCNN approach outperforms the state-of-the-art techniques in deceptive review identification.}
}
@article{SABERI2018356,
title = {Interactive feature selection for efficient customer recognition in contact centers: Dealing with common names},
journal = {Expert Systems with Applications},
volume = {113},
pages = {356-376},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418304299},
author = {Morteza Saberi and Martin Theobald and Omar K Hussain and Elizabeth Chang and Farookh Khadeer Hussain},
keywords = {Decision support systems, Interactive customer recognition, Entity resolution, Common personal names},
abstract = {We propose an interactive decision-making framework to assist a Customer Service Representative (CSR) in the efficient and effective recognition of customer records in a database with many ambiguous entries. Our proposed framework consists of three integrated modules. The first module focuses on the detection and resolution of duplicate records to improve effectiveness and efficiency in customer recognition. The second module determines the level of ambiguity in recognizing an individual customer when there are multiple records with the same name. The third module recommends the series of feature-related questions that the CSR should ask the customer to enable rapid recognition, based on that level of ambiguity. In the first module, the F-Swoosh approach for duplicate detection is used, and in the second module a dynamic programming-based technique is used to determine the level of ambiguity within the customer database for a given name. In the third module, Levenshtein edit distance is used for feature selection in combination with weights based on the Inverse Document Frequency (IDF) of terms. The algorithm that requires the minimum number of questions to be put to the customer to achieve recognition is the algorithm that is chosen. We evaluate the proposed framework on a synthetic dataset and demonstrate how it assists the CSR to rapidly recognize the correct customer.}
}
@article{UHLEMANN2017335,
title = {The Digital Twin: Realizing the Cyber-Physical Production System for Industry 4.0},
journal = {Procedia CIRP},
volume = {61},
pages = {335-340},
year = {2017},
note = {The 24th CIRP Conference on Life Cycle Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.11.152},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116313129},
author = {Thomas H.-J. Uhlemann and Christian Lehmann and Rolf Steinhilper},
keywords = {cyber-physical production systems, multimodal data acquisition, digital twin, production system optimization, Industry 4.0},
abstract = {Concerning current approaches to planning of manufacturing processes, the acquisition of a sufficient data basis of the relevant process information and subsequent development of feasible layout options requires 74% of the overall time-consumption. However, the application of fully automated techniques within planning processes is not yet common practice. Deficits are to be observed in the course of the use of a fully automated data acquisition of the underlying process data, a key element of Industry 4.0, as well as the evaluation and quantification and analysis of the gathered data. As the majority of the planning operations are conducted manually, the lack of any theoretical evaluation renders a benchmarking of the results difficult. Current planning processes analyze the manually achieved results with the aid of simulation. Evaluation and quantification of the planning procedure are limited by complexity that defies manual controllability. Research is therefore required with regard to automated data acquisition and selection, as the near real-time evaluation and analysis of a highly complex production systems relies on a real-time generated database. The paper presents practically feasible approaches to a multi-modal data acquisition approach, its requirements and limitations. The further concept of the Digital Twin for a production process enables a coupling of the production system with its digital equivalent as a base for an optimization with a minimized delay between the time of data acquisition and the creation of the Digital Twin. Therefore a digital data acquisition approach is necessary. As a consequence a cyber-physical production system can be generated, that opens up powerful applications. To ensure a maximum concordance of the cyber-physical process with its real-life model a multimodal data acquisition and evaluation has to be conducted. The paper therefore presents a concept for the composition of a database and proposes guidelines for the implementation of the Digital Twin in production systems in small and medium-sized enterprises.}
}
@incollection{CARVAJAL20181,
title = {Chapter One - Introduction to Digital Oil and Gas Field Systems},
editor = {Gustavo Carvajal and Marko Maucec and Stan Cullick},
booktitle = {Intelligent Digital Oil and Gas Fields},
publisher = {Gulf Professional Publishing},
address = {Boston},
pages = {1-41},
year = {2018},
isbn = {978-0-12-804642-5},
doi = {https://doi.org/10.1016/B978-0-12-804642-5.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046425000013},
author = {Gustavo Carvajal and Marko Maucec and Stan Cullick},
keywords = {Digital oil field, Data streaming, Collaboration, Workflows, Visualization},
abstract = {World energy demand will grow from about 550 QBTU in 2012 to 850 QBTU in 2040 according to 2016 projections from the International Energy Agency (IEA). Although renewable energy sources will grow by a large percentage, petroleum-based liquids (oil) and natural gas will continue to be the largest contributors to energy utilization by the world's population, representing about 55% of the total in 2040. As any current oil and gas production naturally declines, the continued growth of petroleum fuels will be made possible only by forward leaps in technology in finding, drilling, and producing those resources more efficiently and economically. One of the great stories in oil and gas production is the industry's implementation of new digital technologies that increase production for less unit cost. This “revolution” of the “digital oil field” is the subject of this book.}
}
@article{KADLEC2017258,
title = {Using crowdsourced and weather station data to fill cloud gaps in MODIS snow cover datasets},
journal = {Environmental Modelling & Software},
volume = {95},
pages = {258-270},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217306746},
author = {Jiří Kadlec and Daniel P. Ames},
keywords = {Snow cover, Crowdsourcing, Interpolation, Winter sports},
abstract = {We present the design, development, and testing of a new software package for generating snow cover maps. Using a custom inverse distance weighting method, we combine volunteer snow reports, cross-country ski track reports and station measurements to fill cloud gaps in the Moderate Resolution Imaging Spectroradiometer (MODIS) snow cover product. The method is demonstrated by producing a continuous daily time step snow probability map dataset for the Czech Republic region. For validation, we checked the ability of our method to reconstruct MODIS snow cover under cloud by simulating cloud cover datasets and comparing estimated snow cover to actual MODIS snow cover. The percent correctly classified indicator showed accuracy between 80 and 90% using this method. The software is available as an R package. The output data sets are published on the HydroShare website for download and through a web map service for re-use in third-party applications.}
}
@article{LOBO2018241,
title = {KnowBR: An application to map the geographical variation of survey effort and identify well-surveyed areas from biodiversity databases},
journal = {Ecological Indicators},
volume = {91},
pages = {241-248},
year = {2018},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.03.077},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X18302322},
author = {Jorge M. Lobo and Joaquín Hortal and José Luís Yela and Andrés Millán and David Sánchez-Fernández and Emilio García-Roselló and Jacinto González-Dacosta and Juergen Heine and Luís González-Vilas and Castor Guisande},
keywords = {Spatial bias, Data limitations, Database records, Geographic distribution, Survey completeness, Wallacean shortfall},
abstract = {Biodiversity databases are typically incomplete and biased. We identify their three main limitations for characterizing the geographic distributions of species: unknown levels of survey effort, unknown absences of a species from a region, and unknown level of repeated occurrence of a species in different samples collected at the same location. These limitations hinder our ability to distinguish between the actual absence of a species at a given location and its (erroneous) apparent absence as consequence of inadequate surveys. Good practice in biodiversity research requires knowledge of the number, location and degree of completeness of relatively well-surveyed inventories within territorial units. We herein present KnowBR, an application designed to simultaneously estimate the completeness of species inventories across an unlimited number of spatial units and different geographical extents, resolutions and unit expanses from any biodiversity database. We use the number of database records gathered in a territorial unit as a surrogate of survey effort, assuming that such number correlates positively with the probability of recording a species within such area. Consequently, KnowBR uses a “record-by-species” matrix to estimate the relationship between the accumulated number of species and the number of database records to characterize the degree of completeness of the surveys. The final slope of the species accumulation curves and completeness percentages are used to discriminate and map well-surveyed territorial units according to user criteria. The capacity and possibilities of KnowBR are demonstrated through two examples derived from data of varying geographic extent and numbers of records. Further, we identify the main advances that would improve the current functionality of KnowBR.}
}
@incollection{SLIWA2018121,
title = {Chapter 7 - Security, Privacy, and Ethical Issues in Smart Sensor Health and Well-Being Applications},
editor = {Miguel Wister and Pablo Pancardo and Francisco Acosta and José Adán Hernández},
booktitle = {Intelligent Data Sensing and Processing for Health and Well-Being Applications},
publisher = {Academic Press},
pages = {121-140},
year = {2018},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-812130-6},
doi = {https://doi.org/10.1016/B978-0-12-812130-6.00007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812130600007X},
author = {Jan Sliwa},
keywords = {Privacy, Safety, Research, Internet of Things, Internet of Everything, Randomized control trials},
abstract = {The rapidly evolving domain of sensor-based smart medical devices offers new opportunities and creates new risks. An overly enthusiastic approach, concentrating principally on novel functions and reducing the time to market, may lead to forgetting about important safety issues. The risk may be caused by one's own poor design or by malicious actions of others. We try here to take a broad perspective and discuss security, privacy, and ethical issues regarding such devices.}
}
@incollection{KENT2018305,
title = {Chapter 7 - Data, information and the smart factory},
editor = {Robin Kent},
booktitle = {Cost Management in Plastics Processing (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
pages = {305-326},
year = {2018},
isbn = {978-0-08-102269-6},
doi = {https://doi.org/10.1016/B978-0-08-102269-6.50007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022696500070},
author = {Robin Kent}
}
@article{PURCELL2023100094,
title = {Digital Twins in Agriculture: A State-of-the-art review},
journal = {Smart Agricultural Technology},
volume = {3},
pages = {100094},
year = {2023},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2022.100094},
url = {https://www.sciencedirect.com/science/article/pii/S2772375522000594},
author = {Warren Purcell and Thomas Neubauer},
keywords = {Digital Twin, Agriculture},
abstract = {The Digital Twin enables the distinctions between state sensing, entity understanding and physical automation to be eliminated, through high-fidelity modelling and bi-directional data streams. The concept of real-time virtual representation places the Digital Twin in a unique position to enable digitization in agriculture. The union of data, modelling and what-if simulation can provide an approach to overcome current limitations in decision-making support and automation, across a diverse range of agricultural enterprises. This paper conducts a Systematic Literature Review of Digital Twins in agriculture, identifying current trends and open questions with the goal of increasing awareness and understanding of the Digital Twin and its possibilities.}
}
@article{GU2017201,
title = {Web-ADARE: A web-aided data repairing system},
journal = {Neurocomputing},
volume = {253},
pages = {201-214},
year = {2017},
note = {Learning Multimodal Data},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.09.132},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217304642},
author = {Binbin Gu and Zhixu Li and Qiang Yang and Qing Xie and An Liu and Guanfeng Liu and Kai Zheng and Xiangliang Zhang},
keywords = {Data repairing, Rule-based method, Web-aided method, Multiple sources},
abstract = {Data repairing aims at discovering and correcting erroneous data in databases. In this paper, we develop Web-ADARE, an end-to-end web-aided data repairing system, to provide a feasible way to involve the vast data sources on the Web in data repairing. Our main attention in developing Web-ADARE is paid on the interaction problem between web-aided repairing and rule-based repairing, in order to minimize the Web consultation cost while reaching predefined quality requirements. The same interaction problem also exists in crowd-based methods but this is not yet formally defined and addressed. We first prove in theory that the optimal interaction scheme is not feasible to be achieved, and then propose an algorithm to identify a scheme for efficient interaction by investigating the inconsistencies and the dependencies between values in the repairing process. Extensive experiments on three data collections demonstrate the high repairing precision and recall of Web-ADARE, and the efficiency of the generated interaction scheme over several baseline ones.}
}
@article{LISMONT201813,
title = {Predicting tax avoidance by means of social network analytics},
journal = {Decision Support Systems},
volume = {108},
pages = {13-24},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300228},
author = {Jasmien Lismont and Eddy Cardinaels and Liesbeth Bruynseels and Sander {De Groote} and Bart Baesens and Wilfried Lemahieu and Jan Vanthienen},
keywords = {Board interlocks, Predictive analytics, Social network analytics, Social ties, Tax avoidance, Tax planning},
abstract = {This study predicts tax avoidance by means of social network analytics. We extend previous literature by being the first to build a predictive model including a larger variation of network features. We construct a network of firms connected through shared board membership. Then, we apply three analytical techniques, logistic regression, decision trees, and random forests; to create five models using either firm characteristics, network characteristics or different combinations of both. A random forest including firm characteristics, network characteristics of firms and network characteristics of board members provides the best performance with a minimal increase of 7 pp in AUC. Hence, including network effects significantly improves the predictive ability of tax avoidance models, implying that board members exhibit specific knowledge which can carry over across firms. We find that having board members with no connections to low-tax companies lowers the likelihood of being a low-tax firm. Similarly, the higher the average tax rate of the companies a board member is connected to, the lower the chance of being low-tax. On the other hand, being connected to more low-tax firms increases the probability of being low-tax. Consistent with prior literature on firm-specific variables, PP&E has a positive influence on the probability of being low-tax, while EBITDA has a negative effect. Our results are informative for companies as to the director expertise they want to attract in their boards. Additionally, financial analysts and regulatory agencies can use our insights to predict which firms are likely to be low-tax and potentially at risk.}
}
@article{KACHA20182789,
title = {Clinical Study Designs and Sources of Error in Medical Research},
journal = {Journal of Cardiothoracic and Vascular Anesthesia},
volume = {32},
number = {6},
pages = {2789-2801},
year = {2018},
issn = {1053-0770},
doi = {https://doi.org/10.1053/j.jvca.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1053077018301095},
author = {Aalok K. Kacha and Sarah L. Nizamuddin and Junaid Nizamuddin and Harish Ramakrishna and Sajid S. Shahul},
keywords = {error, bias, clinical research, trial design, study design, methodology, systematic review, meta-analysis, evidence-based medicine, Bayesian statistics, group sequential design, trial sequential analysis, data reporting}
}
@article{FARBER2017200,
title = {Can data repositories help find effective treatments for complex diseases?},
journal = {Progress in Neurobiology},
volume = {152},
pages = {200-212},
year = {2017},
note = {Developing drugs for neurological and psychiatric disorders: challenges and opportunities},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2016.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0301008215300253},
author = {Gregory K. Farber},
keywords = {Data sharing, Data repositories, Data infrastructure, Imaging data, Common data elements, Consents},
abstract = {There are many challenges to developing treatments for complex diseases. This review explores the question of whether it is possible to imagine a data repository that would increase the pace of understanding complex diseases sufficiently well to facilitate the development of effective treatments. First, consideration is given to the amount of data that might be needed for such a data repository and whether the existing data storage infrastructure is enough. Several successful data repositories are then examined to see if they have common characteristics. An area of science where unsuccessful attempts to develop a data infrastructure is then described to see what lessons could be learned for a data repository devoted to complex disease. Then, a variety of issues related to sharing data are discussed. In some of these areas, it is reasonably clear how to move forward. In other areas, there are significant open questions that need to be addressed by all data repositories. Using that baseline information, the question of whether data archives can be effective in understanding a complex disease is explored. The major goal of such a data archive is likely to be identifying biomarkers that define sub-populations of the disease.}
}
@article{LI201751,
title = {Designing utilization-based spatial healthcare accessibility decision support systems: A case of a regional health plan},
journal = {Decision Support Systems},
volume = {99},
pages = {51-63},
year = {2017},
note = {Location Analytics and Decision Support},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167923617300891},
author = {Yan Li and Au Vo and Manjit Randhawa and Genia Fick},
keywords = {Two-step floating catchment area, Healthcare access, Spatial analytics, Spatial decision support, The Behavioral Model of Health Services Use},
abstract = {In the U.S., myriad healthcare reforms have begun to show some positive effects on enabling “potential access”. One facet of healthcare access, “having access”, which is the availability and accessibility of health services for the surrounding populations, has not been adequately addressed. Research regarding “having access” is presently championed by a family of methods called Floating Catchment Area (FCA). However, existing scholarship is limited in integrating non-spatial factors within the FCA methods. In this research, we propose a novel utilization-based framework as the first attempt to adopt the Behavioral Model of Health Services Use as a theoretical lens to integrate non-spatial factors in spatial healthcare accessibility research. The framework employs a unique approach to derive categorical and factor weights for different population subgroup's healthcare needs using predictive analytics. The proposed framework is evaluated using a case study of a regional health plan. A Spatial Decision Support System (SDSS) instantiates the framework and enables decision makers to explore physician shortage areas. The SDSS validates the practicality of the proposed utilization-based framework and subsequently allows other FCA methods to be implemented in real-world applications.}
}
@article{RAMONYCAJAL2017484,
title = {Cancer as an ecomolecular disease and a neoplastic consortium},
journal = {Biochimica et Biophysica Acta (BBA) - Reviews on Cancer},
volume = {1868},
number = {2},
pages = {484-499},
year = {2017},
issn = {0304-419X},
doi = {https://doi.org/10.1016/j.bbcan.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0304419X17300525},
author = {Santiago {Ramón y Cajal} and Claudia Capdevila and Javier Hernandez-Losa and Leticia {De Mattos-Arruda} and Abhishek Ghosh and Julie Lorent and Ola Larsson and Trond Aasen and Lynne-Marie Postovit and Ivan Topisirovic},
keywords = {Cancer, Ecomolecular, Consortium, Paradigms, Systems biology, Heterogeneity},
abstract = {Current anticancer paradigms largely target driver mutations considered integral for cancer cell survival and tumor progression. Although initially successful, many of these strategies are unable to overcome the tremendous heterogeneity that characterizes advanced tumors, resulting in the emergence of resistant disease. Cancer is a rapidly evolving, multifactorial disease that accumulates numerous genetic and epigenetic alterations. This results in wide phenotypic and molecular heterogeneity within the tumor, the complexity of which is further amplified through specific interactions between cancer cells and the tumor microenvironment. In this context, cancer may be perceived as an “ecomolecular” disease that involves cooperation between several neoplastic clones and their interactions with immune cells, stromal fibroblasts, and other cell types present in the microenvironment. This collaboration is mediated by a variety of secreted factors. Cancer is therefore analogous to complex ecosystems such as microbial consortia. In the present article, we comment on the current paradigms and perspectives guiding the development of cancer diagnostics and therapeutics and the potential application of systems biology to untangle the complexity of neoplasia. In our opinion, conceptualization of neoplasia as an ecomolecular disease is warranted. Advances in knowledge pertinent to the complexity and dynamics of interactions within the cancer ecosystem are likely to improve understanding of tumor etiology, pathogenesis, and progression. This knowledge is anticipated to facilitate the design of new and more effective therapeutic approaches that target the tumor ecosystem in its entirety.}
}
@article{RASMY201811,
title = {A study of generalizability of recurrent neural network-based predictive models for heart failure onset risk using a large and heterogeneous EHR data set},
journal = {Journal of Biomedical Informatics},
volume = {84},
pages = {11-16},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301175},
author = {Laila Rasmy and Yonghui Wu and Ningtao Wang and Xin Geng and W. Jim Zheng and Fei Wang and Hulin Wu and Hua Xu and Degui Zhi},
keywords = {EHR, Deep learning, Predictive modeling, RNN},
abstract = {Recently, recurrent neural networks (RNNs) have been applied in predicting disease onset risks with Electronic Health Record (EHR) data. While these models demonstrated promising results on relatively small data sets, the generalizability and transferability of those models and its applicability to different patient populations across hospitals have not been evaluated. In this study, we evaluated an RNN model, RETAIN, over Cerner Health Facts® EMR data, for heart failure onset risk prediction. Our data set included over 150,000 heart failure patients and over 1,000,000 controls from nearly 400 hospitals. Convincingly, RETAIN achieved an AUC of 82% in comparison to an AUC of 79% for logistic regression, demonstrating the power of more expressive deep learning models for EHR predictive modeling. The prediction performance fluctuated across different patient groups and varied from hospital to hospital. Also, we trained RETAIN models on individual hospitals and found that the model can be applied to other hospitals with only about 3.6% of reduction of AUC. Our results demonstrated the capability of RNN for predictive modeling with large and heterogeneous EHR data, and pave the road for future improvements.}
}
@article{HUAHU2023118762,
title = {An exploration of the key determinants for the application of AI-enabled higher education based on a hybrid Soft-computing technique and a DEMATEL approach},
journal = {Expert Systems with Applications},
volume = {212},
pages = {118762},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118762},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422017808},
author = {Kuang {Hua Hu}},
keywords = {Artificial intelligence, Higher education, Fuzzy rough set theory (FRST), Ant colony optimization (ACO), Decision making trial and evaluation laboratory (DEMATEL)},
abstract = {The application of AI in higher education has greatly increased globally in the dynamic digital age. The adoption of developmentally appropriate practices using AI-enabled techniques for facilitating the performance of teaching and learning in the higher education domain is thus a necessary task, especially in the COVID 19 pandemic era. The development and implementation of such techniques involve many factors and are related to the classical multiple criteria decision-making (MCDM) issue; however, these factors surrounding supervisors will confuse them and may result in misjudgment. To clarify the relevant issues and illustrate the cause-and-effect relationships among factors, a hybrid soft-computing technique (i.e., the fuzzy rough set theory (FRST) with ant colony optimization (ACO)) and a DEMATEL approach was proposed in this study, which can help decision makers capture the best model necessary for achieving aspiration-level in a higher education management strategy. In the results submitted, the improvement priority for dimensions is based on the measurement of the influences, running in order of tutors for learners (A), skills and competences (B), interaction data to support learning (C), and universal access to global classrooms (D), and which can serve as a reference for the plan of AI-enabled teaching/learning for higher education.}
}
@article{CHAN201887,
title = {Dynamic soft sensors with active forward-update learning for selection of useful data from historical big database},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {175},
pages = {87-103},
year = {2018},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2018.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0169743917303088},
author = {Lester Lik Teck Chan and Qing-Yang Wu and Junghui Chen},
keywords = {Active learning, Forward-update, Large database, Latent variable model, Model uncertainty, Soft sensor},
abstract = {Conventional static soft sensor is incapable of handling the dynamic of processes. With abundance of data, the problem of variable correlations and a large number of samples are encountered; moreover, the quality of the data for the construction of the soft sensors can be crucial for performance. An active learning strategy based on a latent variable model (LVM) to select representative data for efficient development of the dynamic soft sensor model is proposed. The uncertainty information for data selection is provided by the Gaussian process (GP) model. The developed LVM with the auxiliary GP model can handle the process dynamic. An active forward-update scheme which can update the soft sensor model in advance is proposed to reflect the current status of the process and improve the prediction performance without waiting for the quality measurements. Two case studies are done to demonstrate the features and the applicability of the proposed method.}
}
@article{POLIMENI20181,
title = {Neuroimaging with ultra-high field MRI: Present and future},
journal = {NeuroImage},
volume = {168},
pages = {1-6},
year = {2018},
note = {Neuroimaging with Ultra-high Field MRI: Present and Future},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.01.072},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918300727},
author = {Jonathan R. Polimeni and Kâmil Uludağ}
}
@article{LIU2017751,
title = {Trajectories of Emergent Central Sleep Apnea During CPAP Therapy},
journal = {Chest},
volume = {152},
number = {4},
pages = {751-760},
year = {2017},
issn = {0012-3692},
doi = {https://doi.org/10.1016/j.chest.2017.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0012369217310796},
author = {Dongquan Liu and Jeff Armitstead and Adam Benjafield and Shiyun Shao and Atul Malhotra and Peter A. Cistulli and Jean-Louis Pepin and Holger Woehrle},
keywords = {central sleep apnea, CPAP, telemonitoring},
abstract = {Background
The emergence of central sleep apnea (CSA) during positive airway pressure (PAP) therapy has been observed clinically in approximately 10% of obstructive sleep apnea titration studies. This study assessed a PAP database to investigate trajectories of treatment-emergent CSA during continuous PAP (CPAP) therapy.
Methods
U.S. telemonitoring device data were analyzed for the presence/absence of emergent CSA at baseline (week 1) and week 13. Defined groups were as follows: obstructive sleep apnea (average central apnea index [CAI] < 5/h in week 1, < 5/h in week 13); transient CSA (CAI ≥ 5/h in week 1, < 5/h in week 13); persistent CSA (CAI ≥ 5/h in week 1, ≥ 5/h in week 13); emergent CSA (CAI < 5/h in week 1, ≥ 5/h in week 13).
Results
Patients (133,006) used CPAP for ≥ 90 days and had ≥ 1 day with use of ≥ 1 h in week 1 and week 13. The proportion of patients with CSA in week 1 or week 13 was 3.5%; of these, CSA was transient, persistent, or emergent in 55.1%, 25.2%, and 19.7%, respectively. Patients with vs without treatment-emergent CSA were older, had higher residual apnea-hypopnea index and CAI at week 13, and more leaks (all P < .001). Patients with any treatment-emergent CSA were at higher risk of therapy termination vs those who did not develop CSA (all P < .001).
Conclusions
Our study identified a variety of CSA trajectories during CPAP therapy, identifying several different clinical phenotypes. Identification of treatment-emergent CSA by telemonitoring could facilitate early intervention to reduce the risk of therapy discontinuation and shift to more efficient ventilator modalities.}
}
@article{JIANG201713072,
title = {A Monitoring Framework of Collaborative Supply Chain for Agility},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {13072-13077},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2007},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317326459},
author = {Zheng Jiang and Jacques Lamothe and Frederick Benaben},
keywords = {supply chain monitoring, planning execution, supply chain modeling, data interpretation, agile detection, adaptation, assessment},
abstract = {The project C2NET "Cloud Collaborative Manufacturing Networks" is a H2020 project started in January 2015. It aims at developing a cloud platform, which allows the development of services for the management of collaborative planning processes between partners of a supply chain. This paper introduces a monitoring framework organized through 4 main services in the platform: modeling, detection, adaptation and assessment services. The framework provides main features: data collection in the supply chain field and automatic model generation of supply chain plans and situations; model comparison for detecting deviations and propagation of the deviations; agile adaptation for different deviations; a dashboard for visualize and assess the supply chain situation.}
}
@article{CURCIN20171,
title = {Templates as a method for implementing data provenance in decision support systems},
journal = {Journal of Biomedical Informatics},
volume = {65},
pages = {1-21},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416301599},
author = {Vasa Curcin and Elliot Fairweather and Roxana Danger and Derek Corrigan},
keywords = {D2.1 (Software Engineering) Requirements/specification J.3 (Life and Medical Sciences): Health data provenance, Model-driven architectures, Decision support systems},
abstract = {Decision support systems are used as a method of promoting consistent guideline-based diagnosis supporting clinical reasoning at point of care. However, despite the availability of numerous commercial products, the wider acceptance of these systems has been hampered by concerns about diagnostic performance and a perceived lack of transparency in the process of generating clinical recommendations. This resonates with the Learning Health System paradigm that promotes data-driven medicine relying on routine data capture and transformation, which also stresses the need for trust in an evidence-based system. Data provenance is a way of automatically capturing the trace of a research task and its resulting data, thereby facilitating trust and the principles of reproducible research. While computational domains have started to embrace this technology through provenance-enabled execution middlewares, traditionally non-computational disciplines, such as medical research, that do not rely on a single software platform, are still struggling with its adoption. In order to address these issues, we introduce provenance templates – abstract provenance fragments representing meaningful domain actions. Templates can be used to generate a model-driven service interface for domain software tools to routinely capture the provenance of their data and tasks. This paper specifies the requirements for a Decision Support tool based on the Learning Health System, introduces the theoretical model for provenance templates and demonstrates the resulting architecture. Our methods were tested and validated on the provenance infrastructure for a Diagnostic Decision Support System that was developed as part of the EU FP7 TRANSFoRm project.}
}
@article{GALPERN201837,
title = {Assessing urban connectivity using volunteered mobile phone GPS locations},
journal = {Applied Geography},
volume = {93},
pages = {37-46},
year = {2018},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0143622817310731},
author = {Paul Galpern and Andrew Ladle and Francisco {Alaniz Uribe} and Beverly Sandalack and Patricia Doyle-Baker},
abstract = {The mode, timing and frequency of travel by residents of urban neighbourhoods is affected by sociodemographic and environmental factors. Among these, the layout and connectivity of the street network is amenable to design by urban planners and developers. Here we focus on street connectivity as a variable influencing mobility in cities by examining GPS-enabled mobile phone location data volunteered by a group of 127 university students (234,709 h of behavioural observation over a six-year period). We used a Bayesian beta regression framework to model the proportion of time spent inactive, walking and travelling at vehicle speeds, relative to street connectivity and other environmental attributes measured within a radius of home. Results indicated that lower street connectivity, measured using a simple measure we call network warp, is associated with more time spent inactive and more time travelling at vehicle speeds by students. The proportion of time spent walking was higher in areas with more street connectivity, and for student homes that were closer to campus. Our study confirms the importance of street connectivity as a factor influencing the walkability of neighborhoods and the selection of passive forms of transport, and builds on earlier studies of this relationship by incorporating longitudinal data with high spatial and temporal resolution. We conclude that crowdsourcing data that is recorded automatically by GPS-enabled mobile phones can provide an accessible and flexible evidence base to support the design of urban areas.}
}
@article{SOMERVILLE2018456,
title = {The Lifespan Human Connectome Project in Development: A large-scale study of brain connectivity development in 5–21 year olds},
journal = {NeuroImage},
volume = {183},
pages = {456-468},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.08.050},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918307481},
author = {Leah H. Somerville and Susan Y. Bookheimer and Randy L. Buckner and Gregory C. Burgess and Sandra W. Curtiss and Mirella Dapretto and Jennifer Stine Elam and Michael S. Gaffrey and Michael P. Harms and Cynthia Hodge and Sridhar Kandala and Erik K. Kastman and Thomas E. Nichols and Bradley L. Schlaggar and Stephen M. Smith and Kathleen M. Thomas and Essa Yacoub and David C. {Van Essen} and Deanna M. Barch},
keywords = {Neurodevelopment, Brain, MRI, Connectivity, Connectome, Network, Child, Adolescent, Development},
abstract = {Recent technological and analytical progress in brain imaging has enabled the examination of brain organization and connectivity at unprecedented levels of detail. The Human Connectome Project in Development (HCP-D) is exploiting these tools to chart developmental changes in brain connectivity. When complete, the HCP-D will comprise approximately ∼1750 open access datasets from 1300 + healthy human participants, ages 5–21 years, acquired at four sites across the USA. The participants are from diverse geographical, ethnic, and socioeconomic backgrounds. While most participants are tested once, others take part in a three-wave longitudinal component focused on the pubertal period (ages 9–17 years). Brain imaging sessions are acquired on a 3 T Siemens Prisma platform and include structural, functional (resting state and task-based), diffusion, and perfusion imaging, physiological monitoring, and a battery of cognitive tasks and self-reports. For minors, parents additionally complete a battery of instruments to characterize cognitive and emotional development, and environmental variables relevant to development. Participants provide biological samples of blood, saliva, and hair, enabling assays of pubertal hormones, health markers, and banked DNA samples. This paper outlines the overarching aims of the project, the approach taken to acquire maximally informative data while minimizing participant burden, preliminary analyses, and discussion of the intended uses and limitations of the dataset.}
}
@article{ZHANG2018914,
title = {Social media security and trustworthiness: Overview and new direction},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {914-925},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16303879},
author = {Zhiyong Zhang and Brij B. Gupta},
keywords = {Social media networks, Security, Trustworthiness, Measurement, Crowd computing},
abstract = {The emerging social media with inherent capabilities seems to be gaining edge over comprehensiveness, diversity and wisdom, nevertheless its security and trustworthiness issues have also become increasingly serious, which need to be addressed urgently. The available studies mainly aim at both social media content and user security, including model, protocol, mechanism and algorithm. Unfortunately, there is a lack of investigating on effective and efficient evaluations and measurements for security and trustworthiness of various social media tools, platforms and applications, thus has effect on their further improvement and evolution. To address the challenge, this paper firstly made a survey on the state-of-the-art of social media networks security and trustworthiness particularly for the increasingly growing sophistication and variety of attacks as well as related intelligence applications. And then, we highlighted a new direction on evaluating and measuring those fundamental and underlying platforms, meanwhile proposing a hierarchical architecture for crowd evaluations based on signaling theory and crowd computing, which is essential for social media ecosystem. Finally, we conclude our work with several open issues and cutting-edge challenges.}
}
@article{FAN20181123,
title = {Analytical investigation of autoencoder-based methods for unsupervised anomaly detection in building energy data},
journal = {Applied Energy},
volume = {211},
pages = {1123-1135},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306261917317166},
author = {Cheng Fan and Fu Xiao and Yang Zhao and Jiayuan Wang},
keywords = {Autoencoder, Unsupervised data analytics, Anomaly detection, Building operational performance, Building energy management},
abstract = {Practical building operations usually deviate from the designed building operational performance due to the wide existence of operating faults and improper control strategies. Great energy saving potential can be realized if inefficient or faulty operations are detected and amended in time. The vast amounts of building operational data collected by the Building Automation System have made it feasible to develop data-driven approaches to anomaly detection. Compared with supervised analytics, unsupervised anomaly detection is more practical in analyzing real-world building operational data, as anomaly labels are typically not available. Autoencoder is a very powerful method for the unsupervised learning of high-level data representations. Recent development in deep learning has endowed autoencoders with even greater capability in analyzing complex, high-dimensional and large-scale data. This study investigates the potential of autoencoders in detecting anomalies in building energy data. An autoencoder-based ensemble method is proposed while providing a comprehensive comparison on different autoencoder types and training schemes. Considering the unique learning mechanism of autoencoders, specific methods have been designed to evaluate the autoencoder performance. The research results can be used as foundation for building professionals to develop advanced tools for anomaly detection and performance benchmarking.}
}
@incollection{ABRAHMAN2017131,
title = {Chapter 9 - Implications of Emerging Technologies to Incident Handling and Digital Forensic Strategies: A Routine Activity Theory},
editor = {Kim-Kwang Raymond Choo and Ali Dehghantanha},
booktitle = {Contemporary Digital Forensic Investigations of Cloud and Mobile Applications},
publisher = {Syngress},
pages = {131-146},
year = {2017},
isbn = {978-0-12-805303-4},
doi = {https://doi.org/10.1016/B978-0-12-805303-4.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053034000095},
author = {N.H. {Ab Rahman} and G.C. Kessler and K.-K.R. Choo},
keywords = {Incident handling, Digital forensics, Routine activity theory, Cyber threat analysis},
abstract = {A changing cyber threat landscape may impact incident handling and digital forensic practitioners in providing the best mitigation and response strategies. This study seeks to understand the challenges of emerging threats to incident handling and digital forensic by utilizing the routine activity theory, which comprises three main factors—motivation, opportunities, and guardianship. Data were collected using an open-ended questionnaire completed by respondents from 20 organizations. Our findings suggest that the emerging technologies pose significant motivations and opportunities to cybercriminals, and thus, increase the challenges in incident handling and digital forensics to provide effective guardianship.}
}
@article{ZHENG2017267,
title = {Understanding the tourist mobility using GPS: Where is the next place?},
journal = {Tourism Management},
volume = {59},
pages = {267-280},
year = {2017},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2016.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0261517716301509},
author = {Weimin Zheng and Xiaoting Huang and Yuan Li},
keywords = {Tourist mobility, Prediction, GPS technology, Data mining},
abstract = {Understanding the mobility of tourists plays a fundamental role in the administration and design of tourist destinations, planning of on-site movement and marketing of attractions. In this paper, we focus on how to accurately predict the tourist's next location within a given attraction. A heuristic method based on data mining is proposed, which considers the trajectory of a focal tourist and the movements of past visitors. To evaluate the performance of the proposed method, a case study was conducted at the Summer Palace in Beijing, China. We collected movement information from tourists using GPS tracking technology, and the results of an independent samples t-test indicate that the proposed method indeed performs significantly better than existing methods. We further explore the potential applications of the proposed method. Our results significantly contribute to enhancing the level of personalized location-based service, tourist attraction administration, and real-time crowd control.}
}
@article{MONSEN2017e75,
title = {Empirical evaluation of the changes in public health nursing interventions after the implementation of an evidence-based family home visiting guideline},
journal = {Kontakt},
volume = {19},
number = {2},
pages = {e75-e85},
year = {2017},
issn = {1212-4117},
doi = {https://doi.org/10.1016/j.kontakt.2017.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1212411717300168},
author = {Karen A. Monsen and Sadie M. Swenson and Lisa V. Klotzbach and Michelle A. Mathiason and Karen E. Johnson},
keywords = {Family home visiting, Omaha System, Intervention, Guideline, Evaluation},
abstract = {The objective of this quality evaluation was to evaluate the changes in public health nursing (PHN) interventions after the implementation of an evidence-based family home visiting (EB-FHV) guideline encoded using the Omaha System.
Design and sample
This quality improvement evaluation was conducted using a secondary dataset of 27,910 PHN family home visiting interventions from visits to 129 adult clients enrolled in EB-FHV programs in a Midwestern PHN agency. The interventions were documented 12 months before and 14 months after EB-FHV Guideline implementation. The EB-FHV consisted of 94 PHN interventions for 10 Omaha System problems, with electronic health record (EHR) data generated by PHNs during routine clinical documentation. Standard descriptive and inferential statistics were employed in the analysis.
Measures
The Omaha System was used to compare PHN practice before and after the guideline implementation.
Results
Documentation patterns revealed that PHNs tailored interventions while also shifting toward the use of the EB-FHV guideline interventions. Ten EB-FHV problems accounted for 96.3% of interventions documented before and 98.5% of interventions documented after implementation. The proportion of interventions before and after EB-FHV by problem differed significantly for all problems except Substance use. Fewer interventions were provided after EB-FHV for the primary problems of Pregnancy and Postpartum, with a shift to more interventions for Caretaking/parenting.
Conclusion
The PHN documentation after guideline implementation demonstrated adherence to the EB-FHV guideline, while tailoring the evidence-based interventions differentially by problem. Further research is needed to extend this quality improvement approach to other guidelines and populations.}
}
@article{ESQUIVEL201836,
title = {Predictability of seasonal precipitation across major crop growing areas in Colombia},
journal = {Climate Services},
volume = {12},
pages = {36-47},
year = {2018},
issn = {2405-8807},
doi = {https://doi.org/10.1016/j.cliser.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405880718300177},
author = {Alejandra Esquivel and Lizeth Llanos-Herrera and Diego Agudelo and Steven D. Prager and Katia Fernandes and Alexander Rojas and Jhon Jairo Valencia and Julian Ramirez-Villegas},
keywords = {Seasonal forecast, Predictability, Sea surface temperatures, Climate services, Colombian agriculture},
abstract = {Agriculture is one of the sectors that has greatly benefitted from the establishment of climate services. In Colombia, interannual climate variability can disrupt agricultural production, lower farmers' incomes and increase market prices. Increasing demand thus exists for agro-climatic services in the country. Fulfilling such demand requires robust and consistent approaches for seasonal climate forecasting. Here, we assess seasonal precipitation predictability and forecast skill at agriculturally-relevant timescales for five departments that represent key growing areas of major staple crops (rice, maize, and beans). Analyses use Canonical Correlation Analysis, with both observed SSTs and modeled (NCEP-CFSv2) SSTs, as well as with CFSv2 predicted precipitation fields (through a Model-Output-Statistics analysis). Some 74.4% of the forecast situations analyzed (5 departments ∗ 4 seasons ∗ 3 predictors ∗ 3 lead times) showed correlation-based goodness index (Kendall’s tau, τ-) values above 0.1, 38.8% above 0.2, and 18.8% above 0.3. Predictability was limited towards eastern Colombia, and during wet periods of the year in the Inter-Andean Valleys. Importantly, results were consistent between ERSST and CFSv2-driven forecasts, implying that both can offer valuable outlooks for Colombia. While our study is a first important step toward the establishment of a sustainable and successful climate service for agriculture in Colombia, further work is required to (1) improve seasonal forecast skill; (2) link seasonal forecasts to agricultural modelling applications; (3) design appropriate delivery means; and (4) establish stakeholder-driven processes that allow two-way communication between forecast issuing institutions (e.g. IDEAM–Colombian Meteorological Service) and famers’ organizations and farming communities.}
}
@article{ZHANG2018126,
title = {Residual compensation extreme learning machine for regression},
journal = {Neurocomputing},
volume = {311},
pages = {126-136},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.05.057},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218306428},
author = {Jie Zhang and Wendong Xiao and Yanjiao Li and Sen Zhang},
keywords = {Extreme learning machine, Regression problem, Residual compensation extreme learning machine, Device-free localization, Gas utilization ratio prediction},
abstract = {Extreme learning machine (ELM) was proposed for training single hidden layer feedforward neural networks (SLFNs), and can provide an efficient learning solution for regression problem. However, the prediction error of ELM is unavoidable due to its limited modeling capability, and the nonlinear and stochastic nature of the regression problem. In this paper, a novel ELM, residual compensation ELM (RC-ELM), is proposed for regression problem by employing a multilayer structure with the baseline layer for building the feature mapping between the input and the output, and the other layers for residual compensation layer by layer iteratively. Two real world applications, device-free localization (DFL) and gas utilization ratio (GUR) prediction in blast furnace, are used for experimental testing of the proposed RC-ELM. Experimental results show that RC-ELM has better generalization performance and robustness than other machine learning approaches, including the classic ELM, weighted K-nearest neighbor (WKNN), support vector machine (SVM), and back propagation neural network (BPNN).}
}
@incollection{FRYMAN2017177,
title = {Chapter 7 - Playbook Deployment},
editor = {Lowell Fryman and Gregory Lampshire and Dan Meers},
booktitle = {The Data and Analytics Playbook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {177-197},
year = {2017},
isbn = {978-0-12-802307-5},
doi = {https://doi.org/10.1016/B978-0-12-802307-5.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023075000071},
author = {Lowell Fryman and Gregory Lampshire and Dan Meers},
keywords = {Data capabilities, Data cycle, Data governance, Deployment, Execution, Organizational models},
abstract = {This chapter focuses on getting the work done—real execution. Deployment is about deploying the Playbook methods, practices, and approaches to accomplish multiple goals. The first goal is always the improvement of enterprise or shared data. The second, which is tightly linked to the first, is the development and expansion of capabilities related to improving and governing data. The third goal is the overall course of progress that can be made across the scope of enterprise data. As we avoid the methods and build capabilities, more and more of the critical enterprise data is governed and improved in measurable ways.}
}
@article{CHENG2018186,
title = {Short-term traffic forecasting: An adaptive ST-KNN model that considers spatial heterogeneity},
journal = {Computers, Environment and Urban Systems},
volume = {71},
pages = {186-198},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518300140},
author = {Shifen Cheng and Feng Lu and Peng Peng and Sheng Wu},
keywords = {Short-term traffic forecasting, Adaptive spatiotemporal k-nearest neighbor model, Spatial heterogeneity, Traffic patterns},
abstract = {Accurate and robust short-term traffic forecasting is a critical issue in intelligent transportation systems and real-time traffic-related applications. Existing short-term traffic forecasting approaches adopt fixed model structures and assume traffic correlations between adjacent road segments within assigned time periods. Due to the inherent spatial heterogeneity of city traffic, it is difficult for these approaches to obtain stable and satisfying results. To overcome the problems of fixed model structures and quantitatively unclear spatiotemporal dependency relationships, this paper proposes an adaptive spatiotemporal k-nearest neighbor model (adaptive-STKNN) for short-term traffic forecasting. It comprehensively considers the spatial heterogeneity of city traffic based on adaptive spatial neighbors, time windows, spatiotemporal weights and other parameters. First, for each road segment, we determine the sizes of spatial neighbors and the lengths of time windows for traffic influence using cross-correlation and autocorrelation functions, respectively. Second, adaptive spatiotemporal weights are introduced into the distance functions to optimize the candidate neighbor search mechanism. Next, we establish adaptive spatiotemporal parameters to reflect continuous changes in traffic conditions, including the number of candidate neighbors and the weight allocation parameter in the predictive function. Finally, we evaluate the adaptive-STKNN model using two vehicular speed datasets collected on expressways in California, U.S.A., and on city roads in Beijing, China. Four traditional prediction models are compared with the adaptive-STKNN model in terms of forecasting accuracy and generalization ability. The results demonstrate that the adaptive-STKNN model outperforms those models during all time periods and especially the peak period. In addition, the results also show the generalization ability of the adaptive-STKNN model.}
}
@article{LAMBIN2017131,
title = {Decision support systems for personalized and participative radiation oncology},
journal = {Advanced Drug Delivery Reviews},
volume = {109},
pages = {131-153},
year = {2017},
note = {Radiotherapy for cancer: present and future},
issn = {0169-409X},
doi = {https://doi.org/10.1016/j.addr.2016.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169409X16300084},
author = {Philippe Lambin and Jaap Zindler and Ben G.L. Vanneste and Lien Van {De Voorde} and Daniëlle Eekers and Inge Compter and Kranthi Marella Panth and Jurgen Peerlings and Ruben T.H.M. Larue and Timo M. Deist and Arthur Jochems and Tim Lustberg and Johan {van Soest} and Evelyn E.C. {de Jong} and Aniek J.G. Even and Bart Reymen and Nicolle Rekers and Marike {van Gisbergen} and Erik Roelofs and Sara Carvalho and Ralph T.H. Leijenaar and Catharina M.L. Zegers and Maria Jacobs and Janita {van Timmeren} and Patricia Brouwers and Jonathan A. Lal and Ludwig Dubois and Ala Yaromina and Evert Jan {Van Limbergen} and Maaike Berbee and Wouter {van Elmpt} and Cary Oberije and Bram Ramaekers and Andre Dekker and Liesbeth J. Boersma and Frank Hoebers and Kim M. Smits and Adriana J. Berlanga and Sean Walsh},
keywords = {Radiotherapy, Decision support systems, Prediction models, Shared decision making},
abstract = {A paradigm shift from current population based medicine to personalized and participative medicine is underway. This transition is being supported by the development of clinical decision support systems based on prediction models of treatment outcome. In radiation oncology, these models ‘learn’ using advanced and innovative information technologies (ideally in a distributed fashion — please watch the animation: http://youtu.be/ZDJFOxpwqEA) from all available/appropriate medical data (clinical, treatment, imaging, biological/genetic, etc.) to achieve the highest possible accuracy with respect to prediction of tumor response and normal tissue toxicity. In this position paper, we deliver an overview of the factors that are associated with outcome in radiation oncology and discuss the methodology behind the development of accurate prediction models, which is a multi-faceted process. Subsequent to initial development/validation and clinical introduction, decision support systems should be constantly re-evaluated (through quality assurance procedures) in different patient datasets in order to refine and re-optimize the models, ensuring the continuous utility of the models. In the reasonably near future, decision support systems will be fully integrated within the clinic, with data and knowledge being shared in a standardized, dynamic, and potentially global manner enabling truly personalized and participative medicine.}
}
@article{HAAG2018850,
title = {A Framework for Self-Evaluation and Increase of Resource-Efficient Production through Digitalization},
journal = {Procedia CIRP},
volume = {72},
pages = {850-855},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.304},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304852},
author = {Sebastian Haag and Christoph Bauerdick and Alessio Campitelli and Reiner Anderl and Eberhard Abele and Liselotte Schebek},
keywords = {Digital Transformation, Industrie 4.0, Resource Efficiency, Data Analysis},
abstract = {Modern sensor technology and decreasing hardware costs enable the collection of a wide range of data. Nonetheless, the collection of data itself does not generate value. The collected data must be processed and analysed. Many small and medium-sized enterprises already collect a number of data. However, there is no definite strategy, which data needs to be collected in order to acquire relevant insights into processes. The enormous potential of data analysis and the current lack of its implementation caused the development of this framework. It will assist enterprises to evaluate their own level of digitalization to assess resource use.}
}
@incollection{PATRINOS2017353,
title = {Chapter 20 - Genomic Databases: Emerging Tools for Molecular Diagnostics},
editor = {George P. Patrinos},
booktitle = {Molecular Diagnostics (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {353-369},
year = {2017},
isbn = {978-0-12-802971-8},
doi = {https://doi.org/10.1016/B978-0-12-802971-8.00020-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128029718000201},
author = {G.P. Patrinos and T. Katsila and E. Viennas and G. Tzimas},
keywords = {Database management systems, Genomic databases, Genomic variants, Locus-specific databases, Microattribution, National/ethnic mutation databases},
abstract = {Genome informatics deals with informatics tools used in molecular biology, and it is an important scientific discipline that emerged in the postgenomic era from developments in the field of human genomics. Advances in the understanding of the genetic etiology of human disorders, coupled with advances in technology, have led to the identification of numerous genomic variants. These dictate the organization of this knowledge and these alterations in structured repositories that could eventually be useful not only for molecular diagnosis but also for clinicians and researchers. Genetic or mutation databases are referred to as online repositories of genomic variants, mainly described for one or more genes or specifically for a population or ethnic group, aiming to facilitate diagnosis at the DNA level and to correlate genomic variants with specific phenotypic patterns and clinical features. In this chapter we will summarize the key features of the main types of genetic databases that are frequently used in molecular diagnostics, namely locus-specific and national/ethnic genetic databases. In particular, the main activities relating to these genetic database types will be highlighted to describe the existing and emerging database types in this domain and emphasize their potential applications in modern medical genetics. We will also critically discuss and touch upon the key elements that are still missing and holding back the field.}
}
@article{KOVALCHUK2018128,
title = {Simulation of patient flow in multiple healthcare units using process and data mining techniques for model identification},
journal = {Journal of Biomedical Informatics},
volume = {82},
pages = {128-142},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S153204641830087X},
author = {Sergey V. Kovalchuk and Anastasia A. Funkner and Oleg G. Metsker and Aleksey N. Yakovlev},
keywords = {Clinical pathways, Discrete-event simulation, Process mining, Data mining, Acute coronary syndrome, Electronic health records, Classification},
abstract = {Introduction
An approach to building a hybrid simulation of patient flow is introduced with a combination of data-driven methods for automation of model identification. The approach is described with a conceptual framework and basic methods for combination of different techniques. The implementation of the proposed approach for simulation of the acute coronary syndrome (ACS) was developed and used in an experimental study.
Methods
A combination of data, text, process mining techniques, and machine learning approaches for the analysis of electronic health records (EHRs) with discrete-event simulation (DES) and queueing theory for the simulation of patient flow was proposed. The performed analysis of EHRs for ACS patients enabled identification of several classes of clinical pathways (CPs) which were used to implement a more realistic simulation of the patient flow. The developed solution was implemented using Python libraries (SimPy, SciPy, and others).
Results
The proposed approach enables more a realistic and detailed simulation of the patient flow within a group of related departments. An experimental study shows an improved simulation of patient length of stay for ACS patient flow obtained from EHRs in Almazov National Medical Research Centre in Saint Petersburg, Russia.
Conclusion
The proposed approach, methods, and solutions provide a conceptual, methodological, and programming framework for the implementation of a simulation of complex and diverse scenarios within a flow of patients for different purposes: decision making, training, management optimization, and others.}
}
@article{GAO201841,
title = {Two novel lncRNAs discovered in human mitochondrial DNA using PacBio full-length transcriptome data},
journal = {Mitochondrion},
volume = {38},
pages = {41-47},
year = {2018},
issn = {1567-7249},
doi = {https://doi.org/10.1016/j.mito.2017.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1567724917301058},
author = {Shan Gao and Xiaoxuan Tian and Hong Chang and Yu Sun and Zhenfeng Wu and Zhi Cheng and Pengzhi Dong and Qiang Zhao and Jishou Ruan and Wenjun Bu},
keywords = {Mitochondrial transcriptome, Full-length transcriptome, lncRNA, psRNA, ltiRNA},
abstract = {In this study, we established a general framework to use PacBio full-length transcriptome sequencing for the investigation of mitochondrial RNAs. As a result, we produced the first full-length human mitochondrial transcriptome using public PacBio data and characterized the human mitochondrial genome with more comprehensive and accurate information. Other results included determination of the H-strand primary transcript, identification of the ND5/ND6AS/tRNAGluAS transcript, discovery of palindrome small RNAs (psRNAs) and construction of the “mitochondrial cleavage” model, etc. These results reported for the first time in this study fundamentally changed annotations of human mitochondrial genome and enriched knowledge in the field of animal mitochondrial studies. The most important finding was two novel long non-coding RNAs (lncRNAs) of MDL1 and MDL1AS exist ubiquitously in animal mitochondrial genomes.}
}
@article{PAN2018489,
title = {Diet and Cardiovascular Disease: Advances and Challenges in Population-Based Studies},
journal = {Cell Metabolism},
volume = {27},
number = {3},
pages = {489-496},
year = {2018},
issn = {1550-4131},
doi = {https://doi.org/10.1016/j.cmet.2018.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S1550413118301281},
author = {An Pan and Xu Lin and Elena Hemler and Frank B. Hu},
abstract = {Summary
In this Minireview, we provide an epidemiologist’s perspective on the debate and recent advances in determining the relationship between diet and cardiovascular health. We conclude that, in order to reduce the global burden of cardiovascular disease, there should be a greater emphasis on improving overall diet quality and food sources of macronutrients, such as dietary fats and carbohydrates. In addition, building a strong evidence base through high-quality intervention and observational studies is crucial for effective policy changes, which can greatly improve the food environment and population health.}
}
@article{MAREK201741,
title = {Shaking for innovation: The (re)building of a (smart) city in a post disaster environment},
journal = {Cities},
volume = {63},
pages = {41-50},
year = {2017},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2016.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0264275116309519},
author = {Lukas Marek and Malcolm Campbell and Lily Bui},
keywords = {Smart city, Citizen science, Sensors, Urban data},
abstract = {This paper begins by exploring a smart city approach in post-earthquake Christchurch, New Zealand, by telling the city's story so far. We take the position of critical scholars who are engaged in a live smart cities project that involves the measurement of air quality by using sensor tools. As the project is still ongoing, the final results of the work are yet to be seen, but, nonetheless worth documenting. This article is composed as an early analysis of the air quality sensing project as a framework for the larger smart city story of Christchurch. It provides an overview of the experiences and lessons learned about the implementation of new technologies in a post-disaster environment. We examine how the narrative of the smart city is constructed, with focus on the terminology used by citizens, academicians, government and corporations. We then argue that top-down technocratic solutions to urban problems alone do not suffice to improve life in the city; rather, they can result in misaligned expectations or outcomes for stakeholders at the government and citizen level. We conclude by suggesting that citizen-led initiatives may be a way to promote more nuanced and inclusive ways of addressing local urban problems in a smart cities context.}
}
@article{MORAN2017e205,
title = {Development of a model web-based system to support a statewide quality consortium in radiation oncology},
journal = {Practical Radiation Oncology},
volume = {7},
number = {3},
pages = {e205-e213},
year = {2017},
issn = {1879-8500},
doi = {https://doi.org/10.1016/j.prro.2016.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S187985001630220X},
author = {Jean M. Moran and Mary Feng and Lisa A. Benedetti and Robin Marsh and Kent A. Griffith and Martha M. Matuszak and Michael Hess and Matthew McMullen and Jennifer H. Fisher and Teamour Nurushev and Margaret Grubb and Stephen Gardner and Daniel Nielsen and Reshma Jagsi and James A. Hayman and Lori J. Pierce},
abstract = {Purpose
A database in which patient data are compiled allows analytic opportunities for continuous improvements in treatment quality and comparative effectiveness research. We describe the development of a novel, web-based system that supports the collection of complex radiation treatment planning information from centers that use diverse techniques, software, and hardware for radiation oncology care in a statewide quality collaborative, the Michigan Radiation Oncology Quality Consortium (MROQC).
Methods and materials
The MROQC database seeks to enable assessment of physician- and patient-reported outcomes and quality improvement as a function of treatment planning and delivery techniques for breast and lung cancer patients. We created tools to collect anonymized data based on all plans.
Results
The MROQC system representing 24 institutions has been successfully deployed in the state of Michigan. Since 2012, dose-volume histogram and Digital Imaging and Communications in Medicine-radiation therapy plan data and information on simulation, planning, and delivery techniques have been collected. Audits indicated >90% accurate data submission and spurred refinements to data collection methodology.
Conclusions
This model web-based system captures detailed, high-quality radiation therapy dosimetry data along with patient- and physician-reported outcomes and clinical data for a radiation therapy collaborative quality initiative. The collaborative nature of the project has been integral to its success. Our methodology can be applied to setting up analogous consortiums and databases.}
}
@article{MEINECKE201713,
title = {Series: Pragmatic trials and real world evidence: Paper 8. Data collection and management},
journal = {Journal of Clinical Epidemiology},
volume = {91},
pages = {13-22},
year = {2017},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S089543561730776X},
author = {Anna-Katharina Meinecke and Paco Welsing and George Kafatos and Des Burke and Sven Trelle and Maria Kubin and Gaelle Nachbaur and Matthias Egger and Mira Zuidgeest},
keywords = {Pragmatic trial, Routinely collected data, Electronic health records, Registries, Claims databases, eCRF},
abstract = {Pragmatic trials can improve our understanding of how treatments will perform in routine practice. In a series of eight papers, the GetReal Consortium has evaluated the challenges in designing and conducting pragmatic trials and their specific methodological, operational, regulatory, and ethical implications. The present final paper of the series discusses the operational and methodological challenges of data collection in pragmatic trials. A more pragmatic data collection needs to balance the delivery of highly accurate and complete data with minimizing the level of interference that data entry and verification induce with clinical practice. Furthermore, it should allow for the involvement of a representative sample of practices, physicians, and patients who prescribe/receive treatment in routine care. This paper discusses challenges that are related to the different methods of data collection and presents potential solutions where possible. No one-size-fits-all recommendation can be given for the collection of data in pragmatic trials, although in general the application of existing routinely used data-collection systems and processes seems to best suit the pragmatic approach. However, data access and privacy, the time points of data collection, the level of detail in the data, and the lack of a clear understanding of the data-collection process were identified as main challenges for the usage of routinely collected data in pragmatic trials. A first step should be to determine to what extent existing health care databases provide the necessary study data and can accommodate data collection and management. When more elaborate or detailed data collection or more structured follow-up is required, data collection in a pragmatic trial will have to be tailor-made, often using a hybrid approach using a dedicated electronic case report form (eCRF). In this case, the eCRF should be kept as simple as possible to reduce the burden for practitioners and minimize influence on routine clinical practice.}
}
@article{DHAKAL2018145,
title = {Using CyclePhilly data to assess wrong-way riding of cyclists in Philadelphia},
journal = {Journal of Safety Research},
volume = {67},
pages = {145-153},
year = {2018},
issn = {0022-4375},
doi = {https://doi.org/10.1016/j.jsr.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022437517307338},
author = {Nirbesh Dhakal and Christopher R. Cherry and Ziwen Ling and Mojdeh Azad},
keywords = {Cycling behavior, Naturalistic data, Smartphones, Wrong-way riding, Bicycle safety},
abstract = {Problem
The increasing use of smartphones and low cost GPS have provided new sources for collecting data and using them to explain travel behavior. This study aims to use data collected from a smartphone application (CyclePhilly) to explain wrong-way riding behavior of cyclists on one-way segments to help better identify the demographic and network factors influencing the wrong-way riding decision making.
Methods
The data used in this study consist of two different sources: (a) Route trips data downloaded from the CyclePhilly Website contained trips detailed up to segment level, collected from May 2014 to April 2016 (12,202 trips by 300 unique users); and (b) Open Street Maps (OSM). Using ArcGIS, we calculate detour routes for each wrong way segment. We then built a mixed logistic regression model to identify the trip and riders' characteristics affecting wrong-way riding behavior. Next, we explore the characteristics of road facilities associated with wrong-way riding behavior.
Results and discussion
Only 2.7% of travel distance is wrong-way, yet 42% of trips include a wrong-way segment. Commute trips have a higher chance of wrong-way riding. The longer the trips also include more wrong-way riding. Segments with higher detour ratios (ratio of distance with a detour to the wrong-way distance) are found to be associated with more wrong-way behavior. Compared to roads with no bike lane, roads with sharrow markings and buffered bike lane discourage wrong way riding.
Practical applications
This study proposes new methods that can be adapted to use naturalistic and probe data and analyze city-wide aberrant riders' behavior. These help planners and engineers choose between various types of bike infrastructure. Wrong-way riding is one application that can be investigated, but probe bicycle datasets provide unprecedented resolution and volume of data that will allow for more sophisticated safety and planning analyses.}
}
@incollection{LOPES201835,
title = {Chapter 3 - Data Processing in Multivariate Analysis of Pharmaceutical Processes},
editor = {Ana Patricia Ferreira and José C. Menezes and Mike Tobyn},
booktitle = {Multivariate Analysis in the Pharmaceutical Industry},
publisher = {Academic Press},
pages = {35-51},
year = {2018},
isbn = {978-0-12-811065-2},
doi = {https://doi.org/10.1016/B978-0-12-811065-2.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128110652000023},
author = {João A. Lopes and Mafalda C. Sarraguça},
keywords = {Pharmaceutical process data, multivariate data analysis, pharmaceutical processes, quality-by-design, signal processing, statistical process control, process analytical technology, continuous manufacturing},
abstract = {With the recent guidelines promoted by the major health authorities, combined in the International Conference on Harmonization Guidelines Q8, Q9, and Q10, regulating the pharmaceutical development, risk management, and quality management systems, respectively, pharmaceutical process data and their appropriate handling become more relevant than never. The development of a drug product production process is steadily advancing toward the adoption of science-based approaches for better understanding of processes underlying mechanisms, aiming at minimizing product quality variability. The adoption of new technology for assessing the quality of pharmaceutical products in real time (process analytical technology tools) and new paradigms of production, such as continuous manufacturing, brings additional challenges for pharmaceutical data scientists. Finally, product conception and manufacturing processes development can no longer be considered apart from the concept of life-cycle management. This chapter provides an overview on the recent evolution of drug product manufacturing approaches, and implications in terms of data handling and processing.}
}
@article{ALRUITHE2017223,
title = {Analysis and Classification of Barriers and Critical Success Factors for Implementing a Cloud Data Governance Strategy},
journal = {Procedia Computer Science},
volume = {113},
pages = {223-232},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.352},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917317623},
author = {Majid Al-Ruithe and Elhadj Benkhelifa},
keywords = {Cloud computing, data governance, cloud data governance, critical success factors (CSFs), systematic literature review (SLR), barriers},
abstract = {The general consensus in literature seems to suggest that data governance refers to the entirety of decision rights and responsibilities concerning the management of data assets in organisations. These definitions do not however provide equal prominence for the data governance within the cloud computing technology context. As such, this deficit calls for in-depth understanding of data governance and cloud Computing. This trend contributes to changes in data governance strategy in the organisation, such as the organisation’s structure and regulations, people, technology, process, roles and responsibilities. This is one of the great challenges facing organizations today when they move their data to Cloud Computing environments, particularly how Cloud technology affects data governance. The authors’ general observation reveals that the area of data governance in general is under researched and not widely practiced by organisations, let alone when it is concerned with cloud computing, where research is really in its infancy and far from reaching maturity. This paper attempts to identify the possible common barriers and critical success factors for implementing cloud data governance in the hope that it helps the reader to be aware of these barriers and consider them in future developments in the field.}
}
@article{MCKINLEY201715,
title = {Citizen science can improve conservation science, natural resource management, and environmental protection},
journal = {Biological Conservation},
volume = {208},
pages = {15-28},
year = {2017},
note = {The role of citizen science in biological conservation},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2016.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0006320716301963},
author = {Duncan C. McKinley and Abe J. Miller-Rushing and Heidi L. Ballard and Rick Bonney and Hutch Brown and Susan C. Cook-Patton and Daniel M. Evans and Rebecca A. French and Julia K. Parrish and Tina B. Phillips and Sean F. Ryan and Lea A. Shanley and Jennifer L. Shirk and Kristine F. Stepenuck and Jake F. Weltzin and Andrea Wiggins and Owen D. Boyle and Russell D. Briggs and Stuart F. Chapin and David A. Hewitt and Peter W. Preuss and Michael A. Soukup},
keywords = {Citizen science, Public participation in scientific research, Conservation, Policymaking, Natural resource management, Public input, Public engagement},
abstract = {Citizen science has advanced science for hundreds of years, contributed to many peer-reviewed articles, and informed land management decisions and policies across the United States. Over the last 10years, citizen science has grown immensely in the United States and many other countries. Here, we show how citizen science is a powerful tool for tackling many of the challenges faced in the field of conservation biology. We describe the two interwoven paths by which citizen science can improve conservation efforts, natural resource management, and environmental protection. The first path includes building scientific knowledge, while the other path involves informing policy and encouraging public action. We explore how citizen science is currently used and describe the investments needed to create a citizen science program. We find that:1.Citizen science already contributes substantially to many domains of science, including conservation, natural resource, and environmental science. Citizen science informs natural resource management, environmental protection, and policymaking and fosters public input and engagement.2.Many types of projects can benefit from citizen science, but one must be careful to match the needs for science and public involvement with the right type of citizen science project and the right method of public participation.3.Citizen science is a rigorous process of scientific discovery, indistinguishable from conventional science apart from the participation of volunteers. When properly designed, carried out, and evaluated, citizen science can provide sound science, efficiently generate high-quality data, and help solve problems.}
}
@article{WERNER201729,
title = {Cloud identity management: A survey on privacy strategies},
journal = {Computer Networks},
volume = {122},
pages = {29-42},
year = {2017},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2017.04.030},
url = {https://www.sciencedirect.com/science/article/pii/S1389128617301664},
author = {Jorge Werner and Carla Merkle Westphall and Carlos Becker Westphall},
keywords = {Privacy, Identity management, Cloud computing},
abstract = {With the rise of cloud computing, thousands of users and multiple applications have sought to communicate with each other, exchanging sensitive data. Thus, for effectively managing applications and resources, the use of models and tools is essential for the secure management of identities and to avoid compromising data privacy. There are models and tools that address federated identity management, and it is important that they use privacy mechanisms to assist in compliance with current legislation. Therefore, this article aims to present a survey of privacy in cloud identity management, presenting and comparing main features and challenges described in the literature. At the end of this work there is a discussion of the use of privacy and future research directions.}
}
@article{ARAI2018e515s,
title = {Academic health centers: integration of clinical research with healthcare and education. Comments on a workshop},
journal = {Clinics},
volume = {73},
pages = {e515s},
year = {2018},
issn = {1807-5932},
doi = {https://doi.org/10.6061/clinics/2017/e515s},
url = {https://www.sciencedirect.com/science/article/pii/S1807593222011334},
author = {Roberto Jun Arai and Irene {de Lourdes Noronha} and José Carlos Nicolau and Charles Schmidt and Gustavo Moreira {de Albuquerque} and Kenneth W Mahaffey and Eduardo Moacyr Krieger and José Otávio Costa Auler Júnior}
}
@article{FAN2018305,
title = {Relational data imputation with quality guarantee},
journal = {Information Sciences},
volume = {465},
pages = {305-322},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518305309},
author = {Fengfeng Fan and Zhanhuai Li and Qun Chen and Lei Chen},
keywords = {Missing data imputation, Data cleaning, Quality guarantee, Generalized feature dependency},
abstract = {Missing attribute values are prevalent in real relational data, especially the data extracted from the Web. Their accurate imputation is important for ensuring high quality of data analytics. Even though many techniques have been proposed for this task, none of them provides a flexible mechanism for quality control. The lack of quality guarantee may result in many missing data being filled with wrong values, which can easily result in biased data analysis. In this paper, we first propose a novel probabilistic framework based on the concept of Generalized Feature Dependency (GFD). By exploiting the monotonicity between imputation precision and match probability, it enables a flexible mechanism for quality control. We then present the imputation model with precision guarantee and the techniques to maximize recall while meeting a user-specified precision requirement. Finally, we evaluate the performance of the proposed approach on real data. Our extensive experiments show that it has performance advantage over the state-of-the-art alternatives and most importantly, its quality control mechanism is effective.}
}
@article{ZHOU2018139,
title = {Harnessing social media for health information management},
journal = {Electronic Commerce Research and Applications},
volume = {27},
pages = {139-151},
year = {2018},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1567422317300960},
author = {Lina Zhou and Dongsong Zhang and Christopher C. Yang and Yu Wang},
keywords = {Conceptual framework, Health information management, Data analytics, Social media},
abstract = {The remarkable upsurge of social media has dramatic impacts on health care research and practice. Social media are reshaping health information management in a variety of ways, ranging from providing cost-effective ways to improve clinician-patient communication and exchange health-related information and experience, to enabling the discovery of new medical knowledge and information. Despite some demonstrated initial success, social media use and analytics for improving health as a research field is still at its infancy. Information systems researchers can potentially play a key role in advancing the field. This study proposes a conceptual framework for social media-based health information management by drawing on multi-disciplinary research. With the guidance of the framework, this paper presents related research challenges, identifies important yet under-explored research issues, and discusses promising directions for future research.}
}
@article{LIU2017571,
title = {Adaptive just-in-time and relevant vector machine based soft-sensors with adaptive differential evolution algorithms for parameter optimization},
journal = {Chemical Engineering Science},
volume = {172},
pages = {571-584},
year = {2017},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2017.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0009250917304463},
author = {Yiqi Liu},
keywords = {Soft-sensors, Just-in-time, Relevant vector machine, Differential evolution, Parameters},
abstract = {Just-in-time (JIT) and Relevant vector machine (RVM) are two of commonly used models for soft-sensors modeling, the efficiency of which is governed by few critical parameters and hyper-parameters significantly. These parameters are routinely selected by trial and error or experience, thus leading to over- or under-fitting for the prediction. Adaptive differential evolution with optional external archive (JADE) has been used to optimize the parameters of JIT and RVM in this paper. The resulted JADE-JIT and JADE-RVM based soft-sensors are further enhanced into an adaptive format by the moving window (WM) technique. The proposed methodologies are applied to prediction of hard-to-measured variables in the wastewater treatment plants (WWTPs) and successful results are obtained.}
}
@article{KEENAN201813,
title = {The Library of Integrated Network-Based Cellular Signatures NIH Program: System-Level Cataloging of Human Cells Response to Perturbations},
journal = {Cell Systems},
volume = {6},
number = {1},
pages = {13-24},
year = {2018},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405471217304908},
author = {Alexandra B. Keenan and Sherry L. Jenkins and Kathleen M. Jagodnik and Simon Koplev and Edward He and Denis Torre and Zichen Wang and Anders B. Dohlman and Moshe C. Silverstein and Alexander Lachmann and Maxim V. Kuleshov and Avi Ma'ayan and Vasileios Stathias and Raymond Terryn and Daniel Cooper and Michele Forlin and Amar Koleti and Dusica Vidovic and Caty Chung and Stephan C. Schürer and Jouzas Vasiliauskas and Marcin Pilarczyk and Behrouz Shamsaei and Mehdi Fazel and Yan Ren and Wen Niu and Nicholas A. Clark and Shana White and Naim Mahi and Lixia Zhang and Michal Kouril and John F. Reichard and Siva Sivaganesan and Mario Medvedovic and Jaroslaw Meller and Rick J. Koch and Marc R. Birtwistle and Ravi Iyengar and Eric A. Sobie and Evren U. Azeloglu and Julia Kaye and Jeannette Osterloh and Kelly Haston and Jaslin Kalra and Steve Finkbiener and Jonathan Li and Pamela Milani and Miriam Adam and Renan Escalante-Chong and Karen Sachs and Alex Lenail and Divya Ramamoorthy and Ernest Fraenkel and Gavin Daigle and Uzma Hussain and Alyssa Coye and Jeffrey Rothstein and Dhruv Sareen and Loren Ornelas and Maria Banuelos and Berhan Mandefro and Ritchie Ho and Clive N. Svendsen and Ryan G. Lim and Jennifer Stocksdale and Malcolm S. Casale and Terri G. Thompson and Jie Wu and Leslie M. Thompson and Victoria Dardov and Vidya Venkatraman and Andrea Matlock and Jennifer E. {Van Eyk} and Jacob D. Jaffe and Malvina Papanastasiou and Aravind Subramanian and Todd R. Golub and Sean D. Erickson and Mohammad Fallahi-Sichani and Marc Hafner and Nathanael S. Gray and Jia-Ren Lin and Caitlin E. Mills and Jeremy L. Muhlich and Mario Niepel and Caroline E. Shamu and Elizabeth H. Williams and David Wrobel and Peter K. Sorger and Laura M. Heiser and Joe W. Gray and James E. Korkola and Gordon B. Mills and Mark LaBarge and Heidi S. Feiler and Mark A. Dane and Elmar Bucher and Michel Nederlof and Damir Sudar and Sean Gross and David F. Kilburn and Rebecca Smith and Kaylyn Devlin and Ron Margolis and Leslie Derr and Albert Lee and Ajay Pillai},
keywords = {L1000, MEMA, P100, systems pharmacology, systems biology, lincsproject, lincsprogram, data integration, BD2K, MCF10A},
abstract = {The Library of Integrated Network-Based Cellular Signatures (LINCS) is an NIH Common Fund program that catalogs how human cells globally respond to chemical, genetic, and disease perturbations. Resources generated by LINCS include experimental and computational methods, visualization tools, molecular and imaging data, and signatures. By assembling an integrated picture of the range of responses of human cells exposed to many perturbations, the LINCS program aims to better understand human disease and to advance the development of new therapies. Perturbations under study include drugs, genetic perturbations, tissue micro-environments, antibodies, and disease-causing mutations. Responses to perturbations are measured by transcript profiling, mass spectrometry, cell imaging, and biochemical methods, among other assays. The LINCS program focuses on cellular physiology shared among tissues and cell types relevant to an array of diseases, including cancer, heart disease, and neurodegenerative disorders. This Perspective describes LINCS technologies, datasets, tools, and approaches to data accessibility and reusability.}
}
@article{DELIMANETO2018225,
title = {A semiotic-inspired machine for personalized multi-criteria intelligent decision support},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {225-238},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300757},
author = {Fernando Buarque {de Lima Neto} and Denis Mayr {Lima Martins} and Gottfried Vossen},
keywords = {Multi-criteria decision support, Computational intelligence, Computational semiotics, Intelligent semiotic machine},
abstract = {The need for appropriate decisions to tackle complex problems increases every day. Selecting destinations for vacation, comparing and optimizing resources to create valuable products, or purchasing a suitable car are just a few examples of puzzling situations in which there is no standard form to find an appropriate solution. Such scenarios become arduous when the number of possibilities, restrictions, and factors affecting the decision rise, thereby turning decision makers into almost mere spectators. In such circumstances, decision support systems (DSS) can play an important role in guiding people and organizations towards more accurate decision making. However, conventional DSS lack the necessary adaptability to account for dynamic changes and are frequently inadequate to tackle the subjectivity inherent in decision-maker's preferences and intention. We argue that these shortcomings can be addressed by a suitable combination of Semiotic Theory and Computational Intelligence algorithms, which together can make up a new generation of DSS. In this article, a formal description of an Intelligent Semiotic Machine is provided and tried out in practical decision contexts. The results obtained show that our approach can provide well-suited decisions based on user preferences, achieving appropriateness while fanning out subjective options without losing decision context, objectivity, or accuracy.}
}
@article{CAO2017494,
title = {Developmental Connectomics from Infancy through Early Childhood},
journal = {Trends in Neurosciences},
volume = {40},
number = {8},
pages = {494-506},
year = {2017},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2017.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0166223617301157},
author = {Miao Cao and Hao Huang and Yong He},
keywords = {connectome, graph theory, segregation and integration, functional connectivity, structural connectivity, developmental disorder},
abstract = {The human brain undergoes rapid growth in both structure and function from infancy through early childhood, and this significantly influences cognitive and behavioral development in later life. A newly emerging research framework, developmental connectomics, provides unprecedented opportunities for exploring the developing brain through non-invasive mapping of structural and functional connectivity patterns. Within this framework, we review recent neuroimaging and neurophysiological studies investigating connectome development from 20 postmenstrual weeks to 5 years of age. Specifically, we highlight five fundamental principles of brain network development during the critical first years of life, emphasizing strengthened segregation/integration balance, a remarkable hierarchical order from primary to higher-order regions, unparalleled structural and functional maturations, substantial individual variability, and high vulnerability to risk factors and developmental disorders.}
}
@article{YANG2018861,
title = {Identifying household electricity consumption patterns: A case study of Kunshan, China},
journal = {Renewable and Sustainable Energy Reviews},
volume = {91},
pages = {861-868},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2018.04.037},
url = {https://www.sciencedirect.com/science/article/pii/S136403211830265X},
author = {Ting Yang and Minglun Ren and Kaile Zhou},
keywords = {Electricity consumption patterns, Load profiling, Smart energy management, Case study, Smart grid},
abstract = {A case study of residential electricity consumption patterns mining and abnormal user identification using hierarchical clustering is presented in this paper. First, based on a brief introduction of hierarchical clustering, a process model and the specific steps of electricity consumption patterns mining in smart grid environment are proposed. Then, a case study using the daily electricity consumption data of 300 residential users in an eastern city of China, Kunshan, from November 16, 2014 to December 16, 2014, is presented. Through the implementation of hierarchical clustering, 9 abnormal users and 4 types of monthly electricity consumption patterns are successfully identified. The results show that most residential users in Kunshan city, nearly 81%, have a similar monthly electricity consumption pattern. Their average daily electricity consumption is about 7.73 kWh in the early winter with small fluctuations. Also, their daily electricity consumption is significantly associated with the temperature changes. However, it is worth noting that the special electricity consumption patterns of a small proportion of electricity users cannot be ignored, which is of great significance for the planning, operation, policy formulation and decision-making of smart grid.}
}
@article{WALKER201714,
title = {Redefining the incidents to learn from: Safety science insights acquired on the journey from black boxes to Flight Data Monitoring},
journal = {Safety Science},
volume = {99},
pages = {14-22},
year = {2017},
note = {Learning from Incidents},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517309086},
author = {Guy Walker},
keywords = {Leading and lagging indicators, Data recording, Flight Data Monitoring, Safety management systems, Risk triangle},
abstract = {The reason Flight Data and Cockpit Voice Recorders (FDRs and CVRs) exist is to learn from incidents. Probably no other single invention has yielded such significant improvements in aviation safety. Indeed, they have been so effective that we now need to redefine what is meant by the term ‘incident’ and the uses to which data recording technologies are now put. The paradox is that at no previous point in history have we collected so much data, yet safety performance is such that it is rarely used for its original purpose: as a lagging indicator of problems following an accident. In this paper the history of black boxes is briefly surveyed and connected to the underlying safety science knowledge base. Flight Data Monitoring (FDM) is then presented as an exemplar of the paradigm shift from lagging to leading indicators needed in order to continue learning from incidents. In many industries the pre-requisites for comparable Data Monitoring processes are already in place. The benefits to be accrued by following the example set by the aviation industry are considerable.}
}
@article{CITRIN2018197,
title = {Developing and deploying a community healthcare worker-driven, digitally- enabled integrated care system for municipalities in rural Nepal},
journal = {Healthcare},
volume = {6},
number = {3},
pages = {197-204},
year = {2018},
issn = {2213-0764},
doi = {https://doi.org/10.1016/j.hjdsi.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213076417302609},
author = {David Citrin and Poshan Thapa and Isha Nirola and Sachit Pandey and Lal Bahadur Kunwar and Jasmine Tenpa and Bibhav Acharya and Hari Rayamazi and Aradhana Thapa and Sheela Maru and Anant Raut and Sanjaya Poudel and Diwash Timilsina and Santosh Kumar Dhungana and Mukesh Adhikari and Mukti Nath Khanal and Naresh {Pratap KC} and Bhim Acharya and Khem Bahadur Karki and Dipendra Raman Singh and Alex Harsha Bangura and Jeremy Wacksman and Daniel Storisteanu and Scott Halliday and Ryan Schwarz and Dan Schwarz and Nandini Choudhury and Anirudh Kumar and Wan-Ju Wu and S.P. Kalaunee and Pushpa Chaudhari and Duncan Maru},
keywords = {Community health workers, Delivery of healthcare, integrated, Electronic health records, Biometric identification, Health information systems, Nepal},
abstract = {Integrating care at the home and facility level is a critical yet neglected function of healthcare delivery systems. There are few examples in practice or in the academic literature of affordable, digitally-enabled integrated care approaches embedded within healthcare delivery systems in low- and middle-income countries. Simultaneous advances in affordable digital technologies and community healthcare workers offer an opportunity to address this challenge. We describe the development of an integrated care system involving community healthcare worker networks that utilize a home-to-facility electronic health record platform for rural municipalities in Nepal. Key aspects of our approach of relevance to a global audience include: community healthcare workers continuously engaging with populations through household visits every three months; community healthcare workers using digital tools during the routine course of clinical care; individual and population-level data generated routinely being utilized for program improvement; and being responsive to privacy, security, and human rights concerns. We discuss implementation, lessons learned, challenges, and opportunities for future directions in integrated care delivery systems.}
}
@article{ZHOU2017900,
title = {Household monthly electricity consumption pattern mining: A fuzzy clustering-based model and a case study},
journal = {Journal of Cleaner Production},
volume = {141},
pages = {900-908},
year = {2017},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.09.165},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616315062},
author = {Kaile Zhou and Shanlin Yang and Zhen Shao},
keywords = {Electricity consumption pattern, Households, Fuzzy clustering, Smart grid},
abstract = {Household monthly electricity consumption pattern mining is to discover different energy use patterns of households in a month from their daily electricity consumption data. In this study, we develop an improved fuzzy clustering model for the monthly electricity consumption pattern mining of households. First, the background of clustering and fuzzy c-means clustering is introduced. Then a process model of household electricity consumption pattern mining and an improved fuzzy c-means clustering model are provided. Three key aspects of the improved fuzzy c-means clustering model, namely fuzzifier selection, cluster validation and searching capability optimization, are discussed. Finally, the daily electricity consumption data of 1200 households in Jiangsu Province, China, during a month from December 1, 2014 to December 31, 2014 are used in the experiment. With the proposed model, 938 valid households are successfully divided into four and six groups respectively, and the characteristics of each group are extracted. The results revealed the different electricity consumption patterns of different households and demonstrated the effectiveness of the clustering-based model. The customer segmentation based on consumption pattern mining in electric power industry is of great significance to support the development of personalized and targeted marketing strategies and the improvement of energy efficiency.}
}
@article{BISASO2017366,
title = {A survey of machine learning applications in HIV clinical research and care},
journal = {Computers in Biology and Medicine},
volume = {91},
pages = {366-371},
year = {2017},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S001048251730361X},
author = {Kuteesa R. Bisaso and Godwin T. Anguzu and Susan A. Karungi and Agnes Kiragga and Barbara Castelnuovo},
keywords = {HIV, Machine learning, Clinical research, Application paradigms},
abstract = {A wealth of genetic, demographic, clinical and biomarker data is collected from routine clinical care of HIV patients and exists in the form of medical records available among the medical care and research communities. Machine learning (ML) methods have the ability to identify and discover patterns in complex datasets and predict future outcomes of HIV treatment. We survey published studies that make use of ML techniques in HIV clinical research and care. An advanced search relevant to the use of ML in HIV research was conducted in the PubMed biomedical database. The survey outcomes of interest include data sources, ML techniques, ML tasks and ML application paradigms. A growing trend in application of ML in HIV research was observed. The application paradigm has diversified to include practical clinical application, but statistical analysis remains the most dominant application. There is an increase in the use of genomic sources of data and high performance non-parametric ML methods with a focus on combating resistance to antiretroviral therapy (ART). There is need for improvement in collection of health records data and increased training in ML so as to translate ML research into clinical application in HIV management.}
}
@article{FAZEKAS201835,
title = {The extent and cost of corruption in transport infrastructure. New evidence from Europe},
journal = {Transportation Research Part A: Policy and Practice},
volume = {113},
pages = {35-54},
year = {2018},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2018.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417311199},
author = {Mihály Fazekas and Bence Tóth},
keywords = {Corruption, Transport infrastructure, Public procurement, Europe, Spending structure, Price effect},
abstract = {Transport infrastructure provision from roads to waterways involves large amounts of public funds in very complex projects. It is hardly a surprise that all across Europe, but especially in high corruption risk countries, it is a primary target of corrupt elites. This article provides a state-of-the-art review of the literature on the cost of corruption and estimates the level of corruption risks and associated costs in European infrastructure development and maintenance in 2009–2014 using novel data on over 40,000 government contracts. Two forms of corruption costs are investigated in the empirical section: (1) distorting spending structure and project design, and (2) inflating prices. Findings indicate that corruption steers infrastructure spending towards high value as opposed to small value investment projects. It also inflates prices by 30–35% on average with largest excesses in high corruption risk regions. Contrary to perceptions, corruption risks in infrastructure are decoupled to a considerable extent from the national corruption environment. Source data and risk scores are made downloadable at digiwhist.eu/resources/data.}
}
@article{GROGGERT2018243,
title = {Scenario-based Manufacturing Data Analytics with the Example of Order Tracing through BLE-Beacons},
journal = {Procedia Manufacturing},
volume = {24},
pages = {243-249},
year = {2018},
note = {4th International Conference on System-Integrated Intelligence: Intelligent, Flexible and Connected Systems in Products and Production},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918305493},
author = {Sebastian Groggert and Hannes Elser and Quoc Hao Ngo and Robert H. Schmitt},
keywords = {Manufacturing, Data Analytics, Beacons},
abstract = {The progressing digitalization in manufacturing companies results in a continuously increasing available database. These data are a key resource for maintaining competitiveness on a global market. Nevertheless, most manufacturing companies struggle in creating benefits from these data. In an empirical study, conducted in 2017 by WZL of RWTH Aachen University and ITEM of University St. Gallen, a low level of efficiency and maturity in applying Manufacturing Data Analytics has been identified even though the potentials especially for quality management are well known. The study shows a lack of specific use cases for the application of Data Analytics in manufacturing companies as one of the most important obstacles. Therefore, this paper presents the approach of a Scenario-based Manufacturing Data Analytics. The developed approach is applied on the order processing in single and small batch production companies. Nine different Data Analytics application scenarios were derived based on the specific challenges and the data availability. With the example of order tracing, one specific scenario is described in detail by using the established CRISP-DM framework. Additionally, this paper describes a Data Analytics application scenario to improve Order Tracing by implementing a new concept for indoor-localization based on BLE-beacons.}
}
@incollection{2017293,
title = {Subject Index},
editor = {Heimar {de Fátima Marin} and Eduardo Massad and Marco Antonio Gutierrez and Roberto J. Rodrigues and Daniel Sigulem},
booktitle = {Global Health Informatics},
publisher = {Academic Press},
pages = {293-301},
year = {2017},
isbn = {978-0-12-804591-6},
doi = {https://doi.org/10.1016/B978-0-12-804591-6.00027-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128045916000276}
}
@article{CAPALBO2017191,
title = {Next generation data systems and knowledge products to support agricultural producers and science-based policy decision making},
journal = {Agricultural Systems},
volume = {155},
pages = {191-199},
year = {2017},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2016.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16306898},
author = {Susan M. Capalbo and John M. Antle and Clark Seavert},
keywords = {Data systems, Knowledge products, AgBizLogic, TOA-MD, Next generation},
abstract = {Research on next generation agricultural systems models shows that the most important current limitation is data, both for on-farm decision support and for research investment and policy decision making. One of the greatest data challenges is to obtain reliable data on farm management decision making, both for current conditions and under scenarios of changed bio-physical and socio-economic conditions. This paper presents a framework for the use of farm-level and landscape-scale models and data to provide analysis that could be used in NextGen knowledge products, such as mobile applications or personal computer data analysis and visualization software. We describe two analytical tools - AgBiz Logic and TOA-MD - that demonstrate the current capability of farmlevel and landscape-scale models. The use of these tools is explored with a case study of an oilseed crop, Camelina sativa, which could be used to produce jet aviation fuel. We conclude with a discussion of innovations needed to facilitate the use of farm and policy-level models to generate data and analysis for improved knowledge products.}
}
@article{AHANI2017560,
title = {Forecasting social CRM adoption in SMEs: A combined SEM-neural network method},
journal = {Computers in Human Behavior},
volume = {75},
pages = {560-578},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217303539},
author = {Ali Ahani and Nor Zairah Ab. Rahim and Mehrbakhsh Nilashi},
keywords = {CRM adoption, Social CRM, Technology-organization-environment-process, SMEs, Social media, SEM-Neural network},
abstract = {The growth of social media usage questions the old-style idea of customer relationship management (CRM). Social CRM strategy is a novel version of CRM empowered by social media technology that offers a new way of managing relationships with customers effectively. This study aims to forecast the predictors of social CRM strategy adoption by small and medium enterprises (SMEs). The proposed model used in this study derived its theoretical support from IT/IS, marketing, and CRM literature. In the proposed Technology-Organization-Environment-Process (TOEP) adoption model, several hypotheses are developed which examine the role of Technological factors, such as Cost of Adoption, Relative Advantages, Complexity, and Compatibility; Organizational factors, such as IT/IS knowledge of employee, and Top management support; Environmental factors such as Competitive Pressure, and Customer Pressure; and Process factors such as Information Capture, Information Use, and Information Sharing; all having a positive relationship with social CRM adoption. This research applied a following two staged SEM-neural network method combining both structural equation modelling (SEM) and neural network analyses. The proposed hypothetical model is examined by using SEM on the collected data of SMEs in Kuala Lumpur, the central city of Malaysia. The SEM approach with a neural network method can be used to investigate the complicated relations involved in the adoption of social CRM. The study finds that compatibility, information capture, IT/IS knowledge of employee, top management support, information sharing, competitive pressure, cost, relative advantage, and customer pressure are the most important factors influencing social CRM adoption. Remarkably, the results of neural network analysis show that compatibility and information capture of social CRM are the most significant factors which affect SMEs' adoption of this form of customer relationship management. The outcomes of this research benefit executives' decision-making by identifying and ranking factors that enable them to discover how they can advance the usage of social CRM in their firms. Furthermore, the findings of this study can help the managers/owners of SMEs assign their resources, according to the ranking of social CRM adoption factors, when they are making plans to adopt social CRM. This study differs from previous studies as it proposes an innovative new approach to determine what influences the adoption of social CRM. By proposing the TOEP adoption model, additional information process factors advance the traditional TOE adoption model.}
}
@article{EMMANOUILIDIS2018435,
title = {Internet of Things - Enabled Visual Analytics for Linked Maintenance and Product Lifecycle Management},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {435-440},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.339},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314630},
author = {C. Emmanouilidis and L. Bertoncelj and M. Bevilacqua and S. Tedeschi and C. Ruiz-Carcel},
keywords = {Internet of Things, Visual Analytics, Maintenance, Product Lifecycle Management},
abstract = {When closed loop product lifecycle management was first introduced, much effort focused on establishing ways to communicate data between different lifecycle phase activities. The concept of a smart product, able to communicate its own identity and status, had a key role to play to this end. Such a concept has further matured, benefiting from internet things-enabled product lifecycle management advancements. Product data exchanges can now be brought closer to the point of end use consumption, enabling users to become more proactive actors within the product lifecycle management process. This paper presents a conceptual approach and a pilot implementation of how this can be achieved by superimposing middle of life relevant product information to beginning of life product views, such as a 3D product CAD model. In this way, linked maintenance data and knowledge become visual features of a product design representation, facilitating a user’s understanding of middle-of life concepts, such as occurrence of failure modes. The proposed approach can be particularly useful when dealing with product data streams as a natural visual analytics add-in to closed loop product lifecycle management.}
}
@article{MURRAY2018130,
title = {Data challenges and opportunities for environmental management of North Sea oil and gas decommissioning in an era of blue growth},
journal = {Marine Policy},
volume = {97},
pages = {130-138},
year = {2018},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2018.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X18302355},
author = {Fiona Murray and Katherine Needham and Kate Gormley and Sally Rouse and Joop W.P. Coolen and David Billett and Jennifer Dannheim and Silvana N.R. Birchenough and Kieran Hyder and Richard Heard and Joseph S. Ferris and Jan M. Holstein and Lea-Anne Henry and Oonagh McMeel and Jan-Bart Calewaert and J. Murray Roberts},
keywords = {Decommissioning, Offshore energy, Environmental assessment, Blue economy, Open access, ROV survey},
abstract = {Maritime industries routinely collect critical environmental data needed for sustainable management of marine ecosystems, supporting both the blue economy and future growth. Collating this information would provide a valuable resource for all stakeholders. For the North Sea, the oil and gas industry has been a dominant presence for over 50 years that has contributed to a wealth of knowledge about the environment. As the industry begins to decommission its offshore structures, this information will be critical for avoiding duplication of effort in data collection and ensuring best environmental management of offshore activities. This paper summarises the outcomes of a Blue Growth Data Challenge Workshop held in 2017 with participants from: the oil and gas industry; the key UK regulatory and management bodies for oil and gas decommissioning; open access data facilitators; and academic and research institutes. Here, environmental data collection and archiving by oil and gas operators in the North Sea are described, alongside how this compares to other offshore industries; what the barriers and opportunities surrounding environmental data sharing are; and how wider data sharing from offshore industries could be achieved. Five primary barriers to data sharing were identified: 1) Incentives, 2) Risk Perception, 3) Working Cultures, 4) Financial Models, and 5) Data Ownership. Active and transparent communication and collaboration between stakeholders including industry, regulatory bodies, data portals and academic institutions will be key to unlocking the data that will be critical to informing responsible decommissioning decisions for offshore oil and gas structures in the North Sea.}
}
@article{DU201824,
title = {A machine learning based approach to identify protected health information in Chinese clinical text},
journal = {International Journal of Medical Informatics},
volume = {116},
pages = {24-32},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618303447},
author = {Liting Du and Chenxi Xia and Zhaohua Deng and Gary Lu and Shuxu Xia and Jingdong Ma},
keywords = {Protected health information, De-identification, Electronic health records, Conditional random fields},
abstract = {Background
With the increasing application of electronic health records (EHRs) in the world, protecting private information in clinical text has drawn extensive attention from healthcare providers to researchers. De-identification, the process of identifying and removing protected health information (PHI) from clinical text, has been central to the discourse on medical privacy since 2006. While de-identification is becoming the global norm for handling medical records, there is a paucity of studies on its application on Chinese clinical text. Without efficient and effective privacy protection algorithms in place, the use of indispensable clinical information would be confined.
Objectives
We aimed to (i) describe the current process for PHI in China, (ii) propose a machine learning based approach to identify PHI in Chinese clinical text, and (iii) validate the effectiveness of the machine learning algorithm for de-identification in Chinese clinical text.
Methods
Based on 14,719 discharge summaries from regional health centers in Ya'an City, Sichuan province, China, we built a conditional random fields (CRF) model to identify PHI in clinical text, and then used the regular expressions to optimize the recognition results of the PHI categories with fewer samples.
Results
We constructed a Chinese clinical text corpus with PHI tags through substantial manual annotation, wherein the descriptive statistics of PHI manifested its wide range and diverse categories. The evaluation showed with a high F-measure of 0.9878 that our CRF-based model had a good performance for identifying PHI in Chinese clinical text.
Conclusion
The rapid adoption of EHR in the health sector has created an urgent need for tools that can parse patient specific information from Chinese clinical text. Our application of CRF algorithms for de-identification has shown the potential to meet this need by offering a highly accurate and flexible solution to analyzing Chinese clinical text.}
}
@incollection{BERTHIER201885,
title = {5 - False Data and Fictitious Algorithmic Projections},
editor = {Thierry Berthier and Bruno Teboul},
booktitle = {From Digital Traces to Algorithmic Projections},
publisher = {Elsevier},
pages = {85-112},
year = {2018},
isbn = {978-1-78548-270-0},
doi = {https://doi.org/10.1016/B978-1-78548-270-0.50005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978178548270050005X},
author = {Thierry Berthier and Bruno Teboul},
keywords = {Anti-fragility, Attractive fictitious profiles, Digital security, Digital space, Fictitious data, Fictitious projections, Honeypot, Integrity, Natural language processing, Robin Sage},
abstract = {Abstract:
The creation of false data (data conveying false information) can be considered as a collateral effect of the algorithmization of the environment. The use of false or fictitious data always responds to an initial aim to fool or dissimulate that can be expressed in very varied contexts, such as protecting anonymity, economic or military inquiries, cyber-espionage, cyber-crime, financial fraud or the manipulation of stock for listed companies. Creators of false data seek to deceive a number of users or a calculation system with the aim of benefitting from doing so.}
}
@article{PAGOROPOULOS2017369,
title = {Assessing transformational change from institutionalising digital capabilities on implementation and development of Product-Service Systems: Learnings from the maritime industry},
journal = {Journal of Cleaner Production},
volume = {166},
pages = {369-380},
year = {2017},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617317341},
author = {Aris Pagoropoulos and Anja Maier and Tim C. McAloone},
keywords = {Product-Service Systems, Digitization, Customer, Maritime industry},
abstract = {Digitization is rapidly reshaping industries and economic sectors. It enables novel Product-Service Systems (PSS) that transform customer/supplier relationships and introduces new value propositions. However, while opportunities for novel types of PSS arise, it is not clear how digitization and the institutionalisation of digital capabilities, particularly within the customer organisations, may affect implementation of PSS, potentially leading to transformational changes in the customer organisation. This paper examines one such potential transformational change from three complementary viewpoints – the resource based, the dynamic, and the relational viewpoint. It does so through action research study in the context of the maritime industry, which is particularly attractive for PSS offerings. The research methodology comprised a two-step action research process, focusing on both digitization and PSS development and implementation. The main findings are that rather than facilitating procurement to co-development of PSS, institutionalisation of digital capabilities facilitated development of PSS by stakeholders internal to the company, and strategic co-development with external stakeholders. The new digital capabilities circumvented cost barriers associated with the procurement of services from external stakeholders, supported process standardisation - to the expense of process innovation-, and transformed the network that delivered PSS by closing opportunity gaps for externally procured services. Furthermore, the uptake of digital capabilities highlighted the importance of cost estimation in making the customer more responsive to threats and opportunities.}
}
@article{LEE201734,
title = {Online resources for studies of genome biology and epigenetics},
journal = {Current Opinion in Toxicology},
volume = {6},
pages = {34-41},
year = {2017},
note = {Genomic Toxicology: Epigenetics},
issn = {2468-2020},
doi = {https://doi.org/10.1016/j.cotox.2017.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S2468202017300621},
author = {Paul J. Lee and Mayank NK Choudhary and Ting Wang},
keywords = {Next generation sequencing, Online database, Epigenome, Methylome, Methylation, Chromatin accessibility, Histone modifications, 3D genome, TADs, ENCODE, ROADMAP},
abstract = {Environmental exposure to chemical toxins alters epigenetic modifications that culminate in altered cellular gene expression without changing the underlying DNA sequence. The complex interplay between the layers of epigenetic regulators ultimately results in observed cellular phenotype. This review highlights epigenetics annotations assayed in the Encyclopedia of DNA Elements (ENCODE) community resource project—a publically accessible database for understanding genomic function, development and disease etiologies. We outline the multiple levels of epigenetic control (DNA methylation, chromatin accessibility, histone modifications, genome topology) with their associated interrogation methodology. We explore the limitations and strengths of each methodology at every epigenetic checkpoint. This review points readers to epigenetic resources that have gathered focused scientific data and directs them toward data visualization tools that can help answer questions related to epigenetic controls. The purpose of this review is to highlight online resources available to toxicological epigenetic researchers that can help fast track novel insights using already curated reference epigenome datasets.}
}
@article{AUCHTER20188,
title = {A description of the ABCD organizational structure and communication framework},
journal = {Developmental Cognitive Neuroscience},
volume = {32},
pages = {8-15},
year = {2018},
note = {The Adolescent Brain Cognitive Development (ABCD) Consortium: Rationale, Aims, and Assessment Strategy},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2018.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1878929317302268},
author = {Allison M. Auchter and Margie {Hernandez Mejia} and Charles J. Heyser and Paul D. Shilling and Terry L. Jernigan and Sandra A. Brown and Susan F. Tapert and Gayathri J. Dowling},
keywords = {Adolescence, Development, Neuroimaging, Longitudinal, Organizational framework, Governance},
abstract = {The Adolescent Brain Cognitive Development (ABCD) study is designed to be the largest study of brain development and child health in the United States, performing comprehensive assessments of 11,500 children repeatedly for 10 years. An endeavor of this magnitude requires an organized framework of governance and communication that promotes collaborative decision-making and dissemination of information. The ABCD consortium structure, built upon the Matrix Management approach of organizational theory, facilitates the integration of input from all institutions, numerous internal workgroups and committees, federal partners, and external advisory groups to make use of a broad range of expertise to ensure the study’s success.}
}
@article{SHI20181390,
title = {Applying high-frequency surrogate measurements and a wavelet-ANN model to provide early warnings of rapid surface water quality anomalies},
journal = {Science of The Total Environment},
volume = {610-611},
pages = {1390-1399},
year = {2018},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2017.08.232},
url = {https://www.sciencedirect.com/science/article/pii/S0048969717322386},
author = {Bin Shi and Peng Wang and Jiping Jiang and Rentao Liu},
keywords = {Water quality, Surrogate parameters, Anomaly detection, Wavelet denoising, Back-propagation neural networks},
abstract = {It is critical for surface water management systems to provide early warnings of abrupt, large variations in water quality, which likely indicate the occurrence of spill incidents. In this study, a combined approach integrating a wavelet artificial neural network (wavelet-ANN) model and high-frequency surrogate measurements is proposed as a method of water quality anomaly detection and warning provision. High-frequency time series of major water quality indexes (TN, TP, COD, etc.) were produced via a regression-based surrogate model. After wavelet decomposition and denoising, a low-frequency signal was imported into a back-propagation neural network for one-step prediction to identify the major features of water quality variations. The precisely trained site-specific wavelet-ANN outputs the time series of residual errors. A warning is triggered when the actual residual error exceeds a given threshold, i.e., baseline pattern, estimated based on long-term water quality variations. A case study based on the monitoring program applied to the Potomac River Basin in Virginia, USA, was conducted. The integrated approach successfully identified two anomaly events of TP variations at a 15-minute scale from high-frequency online sensors. A storm event and point source inputs likely accounted for these events. The results show that the wavelet-ANN model is slightly more accurate than the ANN for high-frequency surface water quality prediction, and it meets the requirements of anomaly detection. Analyses of the performance at different stations and over different periods illustrated the stability of the proposed method. By combining monitoring instruments and surrogate measures, the presented approach can support timely anomaly identification and be applied to urban aquatic environments for watershed management.}
}
@article{LUCAS2018260,
title = {Workshop Synthesis: Dealing with immobility and survey non-response},
journal = {Transportation Research Procedia},
volume = {32},
pages = {260-267},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.048},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518302023},
author = {Karen Lucas and Jean-Loup Madre},
keywords = {travel survey methods, non-response, hard-to reach-populations, immobility, longitudinal data},
abstract = {This paper discusses the related issues associated with dealing with immobility and survey non-response within survey design, sampling protocols and for different survey instruments and methodological approaches. How to develop new tools and methods to capture the travel under-surveyed, ‘hard to reach’ and ‘survey shy’ populations was discussed. Also, how to adapt standard survey designs and sampling approaches to include overlooked population sectors, such as young people, non-travellers and the residents of informal settlements in developing countries. The paper recommends that best known practice in this area is a long way from common practices, and that the academic, commercial consultancy and policy worlds are very different places. This raises the need to develop some minimum standards/checklists of survey inclusion protocols, together with basic training on sample composition, screening and proxy data recording.}
}
@article{DZURANIN201824,
title = {Infusing data analytics into the accounting curriculum: A framework and insights from faculty},
journal = {Journal of Accounting Education},
volume = {43},
pages = {24-39},
year = {2018},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301257},
author = {Ann C. Dzuranin and Janet R. Jones and Renee M. Olvera},
keywords = {Data analytics, Accounting curricula, Professional competencies, Survey research},
abstract = {Understanding how to use data to formulate and solve business problems provides an opportunity for the accounting professional to become a forward thinking strategic partner in the organization. The challenge for accountants is to develop the skills needed to extract value from data through advanced analytics. The challenge for accounting academic departments is determining the data analytic skills and tools that are relevant to the accounting profession and how and when to incorporate those topics into an already full curriculum. This is especially true for accounting programs that have separate AACSB accreditation, given that Accreditation Standard A7 requires universities with separate accounting accreditation to include content and learning objectives associated with data analytics and information technology skills. To address the challenges, we propose three data analytic implementation methods: a focused approach, integrated approach, and a hybrid approach. We present the results of a broad exploratory survey of accounting faculty regarding which data analytic skills and tools should be taught and how, when and where these topics should be provided to accounting students. We find support for a hybrid approach; whereby accounting programs include both a stand-alone course emphasizing data analytic competencies and accounting courses with data analytic competencies ingrained. We conclude with a discussion of the support for and limitations of each of our proposed implementation methods.}
}
@article{MAGANHA2018120,
title = {Understanding reconfigurability of manufacturing systems: An empirical analysis},
journal = {Journal of Manufacturing Systems},
volume = {48},
pages = {120-130},
year = {2018},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518302036},
author = {Isabela Maganha and Cristovao Silva and Luis Miguel D.F. Ferreira},
keywords = {Reconfigurable manufacturing system, Reconfigurability, Exploratory analysis, Questionnaire survey},
abstract = {The need for more responsive manufacturing systems to deal with high product variety and large fluctuations in market demand requires new approaches that enable the system to react to changes quickly and efficiently. Reconfigurability is an ability that allows the addition, removal or rearrangement of manufacturing system components and functions to better cope with high product variety and significant fluctuations in market demand in a cost effective way. This paper empirically investigates the understanding of reconfigurability in industrial manufacturing companies and tests and validates its core characteristics using a questionnaire survey, which was carried out with Portuguese companies. Findings show the existence of five core characteristics of reconfigurability. The implications of these characteristics, concerning the implementation of Reconfigurable Manufacturing Systems, are also analysed and discussed.}
}
@article{GELLER2018106,
title = {Quality assurance of biomedical terminologies and ontologies},
journal = {Journal of Biomedical Informatics},
volume = {86},
pages = {106-108},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301801},
author = {James Geller and Yehoshua Perl and Licong Cui and G.Q. Zhang}
}
@incollection{DONG2018409,
title = {Chapter 18 - Energy Disaggregation and the Utility-Privacy Tradeoff},
editor = {Reza Arghandeh and Yuxun Zhou},
booktitle = {Big Data Application in Power Systems},
publisher = {Elsevier},
pages = {409-444},
year = {2018},
isbn = {978-0-12-811968-6},
doi = {https://doi.org/10.1016/B978-0-12-811968-6.00018-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119686000188},
author = {Roy Dong and Lillian J. Ratliff},
keywords = {Energy disaggregation, Privacy metrics, Utility-privacy tradeoff, Estimation, Fundamental limits, Direct load control},
abstract = {CHAPTER OVERVIEW
The problem of energy disaggregation is the estimation of individual device usage patterns from available aggregate energy consumption measurements. In this work, we consider the fundamental limits of the energy disaggregation problem, and use these limits to quantify the tradeoff between the utilization of data for smart grid operations and the privacy provided to energy consumers. First, our fundamental limits build on a statistical testing framework to provide a theoretical bound to the accuracy of energy disaggregation that can be achieved by any algorithm. Then, we present a framework for understanding how variations in system design can affect the operational benefits of collecting data, as well as the privacy of users. We instantiate this framework in a direct load control example where we use thermostatically controlled loads and vary the frequency with which a centralized controller receives sensor measurements. Our work formalizes the process of incorporating privacy considerations into the design of modern energy systems.}
}
@article{DADZIE201851,
title = {Structuring visual exploratory analysis of skill demand},
journal = {Journal of Web Semantics},
volume = {49},
pages = {51-70},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300690},
author = {A.-S. Dadzie and E.M. Sibarani and I. Novalija and S. Scerri},
keywords = {Domain modeling, Knowledge discovery, Visual exploration, Ontology-guided visual analytics, Trend identification, Demand analysis},
abstract = {The analysis of increasingly large and diverse data for meaningful interpretation and question answering is handicapped by human cognitive limitations. Consequently, semi-automatic abstraction of complex data within structured information spaces becomes increasingly important, if its knowledge content is to support intuitive, exploratory discovery. Exploration of skill demand is an area where regularly updated, multi-dimensional data may be exploited to assess capability within the workforce to manage the demands of the modern, technology- and data-driven economy. The knowledge derived may be employed by skilled practitioners in defining career pathways, to identify where, when and how to update their skillsets in line with advancing technology and changing work demands. This same knowledge may also be used to identify the combination of skills essential in recruiting for new roles. To address the challenges inherent in exploring the complex, heterogeneous, dynamic data that feeds into such applications, we investigate the use of an ontology to guide structuring of the information space, to allow individuals and institutions to interactively explore and interpret the dynamic skill demand landscape for their specific needs. As a test case we consider the relatively new and highly dynamic field of Data Science, where insightful, exploratory data analysis and knowledge discovery are critical. We employ context-driven and task-centred scenarios to explore our research questions and guide iterative design, development and formative evaluation of our ontology-driven, visual exploratory discovery and analysis approach, to measure where it adds value to users’ analytical activity. Our findings reinforce the potential in our approach, and point us to future paths to build on.}
}
@incollection{2018351,
title = {Index},
editor = {Gustavo Carvajal and Marko Maucec and Stan Cullick},
booktitle = {Intelligent Digital Oil and Gas Fields},
publisher = {Gulf Professional Publishing},
address = {Boston},
pages = {351-357},
year = {2018},
isbn = {978-0-12-804642-5},
doi = {https://doi.org/10.1016/B978-0-12-804642-5.09992-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046425099928}
}
@article{FAN2018116,
title = {Discovering gradual patterns in building operations for improving building energy efficiency},
journal = {Applied Energy},
volume = {224},
pages = {116-123},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.04.118},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918306858},
author = {Cheng Fan and Yongjun Sun and Kui Shan and Fu Xiao and Jiayuan Wang},
keywords = {Gradual pattern mining, Motif discovery, Data mining, Building operational performance, Building energy efficiency},
abstract = {The development of information technologies has enabled real-time monitoring and controls over building operations. Massive amounts of building operational data are being collected and available for knowledge discovery. Advanced data analytics are urgently needed to fully realize the potentials of big building operational data in enhancing building energy efficiency. The rapid development of data mining has provided powerful tools for extracting insights in various knowledge representations. Gradual pattern mining is a promising technique for discovering useful patterns from building operational data. The knowledge discovered is represented as gradual relationships, i.e., “the more/less A, the more/less B”. It can bring special interests to building energy management by highlighting co-variations among numerical building variables. This study investigated the usefulness of gradual pattern mining for building energy management. A generic methodology was proposed to ensure the quality and applicability of the knowledge discovered. The methodology was validated through a case study. The results showed that the methodology could successfully extract valuable insights on building operation characteristics and provide opportunities for building energy efficiency enhancement.}
}
@article{ZENG20174,
title = {Energy finance data warehouse: Tracking revenues through the power sector},
journal = {The Electricity Journal},
volume = {30},
number = {3},
pages = {4-9},
year = {2017},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2017.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1040619017300131},
author = {Claire Zeng and Stephen Hendrickson and Sangkeun Matt Lee and Supriya Chinthavali and Jessica Lin and Eric Hsieh and Mallikarjun Shankar},
keywords = {Energy, Financial data, Power sector revenue, Sankey, Visualization},
abstract = {Reliable data is needed to understand financial relationships in the power sector. However, relevant data acquisition and visualization can be a challenge due to the fragmented nature of the power sector. The US DOE and ORNL leveraged a Sankey prototype to elucidate the ‘big picture’ of financial flows to understand the complex relationships between specific actors within the power sector. The continued incorporation of high quality data can improve the fidelity of such an approach and lead to an increasingly detailed understanding of financial relationships in the power sector and their implications for policymakers.}
}
@article{WASSERSTEIN2018483,
title = {Administrative Databases in Sports Medicine Research},
journal = {Clinics in Sports Medicine},
volume = {37},
number = {3},
pages = {483-494},
year = {2018},
note = {Sports Medicine Statistics},
issn = {0278-5919},
doi = {https://doi.org/10.1016/j.csm.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278591918300279},
author = {David Wasserstein and Ujash Sheth},
keywords = {Administrative database, Cohort study, Epidemiology, Incidence rate, Sports medicine}
}
@article{MA201814,
title = {Bike sharing and users’ subjective well-being: An empirical study in China},
journal = {Transportation Research Part A: Policy and Practice},
volume = {118},
pages = {14-24},
year = {2018},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2018.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417307966},
author = {Liang Ma and Xin Zhang and Xiaoyan Ding and Gaoshan Wang},
keywords = {Perceived value, Social influence, Trust attitude, Personal accomplishment, Subjective well-being},
abstract = {The rise of bike sharing has been phenomenal in China. However, few studies have focused on it relation to subjective well-being. Here we develop an integrated model to investigate factors that affect the subjective well-being of shared bike users in China. An online survey of 908 users was conducted. The highlights are: (1) perceived value has a positive effect on users’ subjective well-being through users’ trust attitude. Hedonic value has the greatest impact on users’ subjective well-being, followed by social value and utilitarian value; (2) social influence has a positive effect on users’ trust attitude and hence to subjective well-being; (3) perceived ease of use and perceived usefulness of the system have positive effects on users’ trust attitude; (4) personal accomplishment and users’ trust attitude have a positive effect on users’ subjective well-being. Theoretical and practical implications are also discussed.}
}
@incollection{2017271,
title = {Index},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {271-282},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000156}
}
@article{KAFFEE201866,
title = {The Human Face of the Web of Data: A Cross-sectional Study of Labels},
journal = {Procedia Computer Science},
volume = {137},
pages = {66-77},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316119},
author = {Lucie-Aimée Kaffee and Elena Simperl},
keywords = {Linked Data, Web of Data, Labels, Human Accessibility, Multilingual 2010 MSC: 00-01, 99-00},
abstract = {Labels in the web of data are the key element for humans to access the data. We introduce a framework to measure the coverage of information with labels. The framework is based on a set of metrics including completeness, unambiguity, multilinguality, labeled object usage, and monolingual islands. We apply this framework on seven diverse datasets, from the web of data, a collaborative knowledge base, open governmental and GLAM data. We gain an insight into the current state of labels and multilinguality on the web of data. Comparing a set of differently sourced datasets can help data publishers to understand what they can improve and what other ways of collecting and data can be adopted.}
}
@article{FASSBENDER2017181,
title = {Perspectives on Chemical Oceanography in the 21st century: Participants of the COME ABOARD Meeting examine aspects of the field in the context of 40years of DISCO},
journal = {Marine Chemistry},
volume = {196},
pages = {181-190},
year = {2017},
issn = {0304-4203},
doi = {https://doi.org/10.1016/j.marchem.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0304420317300567},
author = {Andrea J. Fassbender and Hilary I. Palevsky and Todd R. Martz and Anitra E. Ingalls and Martha Gledhill and Sarah E. Fawcett and Jay A. Brandes and Lihini I. Aluwihare},
abstract = {The questions that chemical oceanographers prioritize over the coming decades, and the methods we use to address these questions, will define our field's contribution to 21st century science. In recognition of this, the U.S. National Science Foundation and National Oceanic and Atmospheric Administration galvanized a community effort (the Chemical Oceanography MEeting: A BOttom-up Approach to Research Directions, or COME ABOARD) to synthesize bottom-up perspectives on selected areas of research in Chemical Oceanography. Representing only a small subset of the community, COME ABOARD participants did not attempt to identify targeted research directions for the field. Instead, we focused on how best to foster diverse research in Chemical Oceanography, placing emphasis on the following themes: strengthening our core chemical skillset; expanding our tools through collaboration with chemists, engineers, and computer scientists; considering new roles for large programs; enhancing interface research through interdisciplinary collaboration; and expanding ocean literacy by engaging with the public. For each theme, COME ABOARD participants reflected on the present state of Chemical Oceanography, where the community hopes to go and why, and actionable pathways to get there. A unifying concept among the discussions was that dissimilar funding structures and metrics of success may be required to accommodate the various levels of readiness and stages of knowledge development found throughout our community. In addition to the science, participants of the concurrent Dissertations Symposium in Chemical Oceanography (DISCO) XXV, a meeting of recent and forthcoming Ph.D. graduates in Chemical Oceanography, provided perspectives on how our field could show leadership in addressing long-standing diversity and early-career challenges that are pervasive throughout science. Here we summarize the COME ABOARD Meeting discussions, providing a synthesis of reflections and perspectives on the field.}
}
@article{SAFADI20171684,
title = {Mapping for the Future: Business Intelligence Tool to Map Regional Housing Stock},
journal = {Procedia Engineering},
volume = {180},
pages = {1684-1694},
year = {2017},
note = {International High-Performance Built Environment Conference – A Sustainable Built Environment Conference 2016 Series (SBE16), iHBE 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.04.331},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817318386},
author = {Murad Safadi and Jun Ma and Rohan Wickramasuriya and Daniel Daly and Pascal Perez and Georgios Kokogiannakis},
keywords = {energy epidemiology, housing stock mapping, energy, visualisation, built environment},
abstract = {The amount of data available and the lack of data integration represent an increasing challenge to effective planning for government agencies. Integration of data from multiple sources has the potential to enable a user to draw valuable insights, which can be used to enhance service targeting and delivery, and to improve program evaluation. In recognition of the need to improve data integration the University of Wollongong and the NSW Office of Environment and Heritage (OEH) partnered to create an integrated housing stock database for the Illawarra region. The database serves as the backbone for an online and interactive Housing Stock Mapping Dashboard (HSMD). It assembled multilevel granular information (including at the Statistical Area Level 1 (SA1) and Local Government Area (LGA) level) collected from multiple historical programs by multiple agencies. This centralised, integrated data repository can help agencies understand the existing housing stock, and improve access to information to support evidence-based policy. This paper presents a model of how data can be integrated from multiple agencies to provide an online collaboration platform. The platform, HSMD, was designed to demonstrate to government, industry, and the research community the opportunity of data integration and advanced analytics. Potential applications of the HSMD include characterisation of the existing housing stock according to a range of building attributes, for instance the presence of ceiling insulation or rainwater tanks. Comparison of these attributes with energy consumption data can indicate the influence of the attribute, or the impact of a specific intervention. This can help policy makers understand uptake and penetration of previous rebate schemes.}
}
@article{REUTER2017487,
title = {Benefit Oriented Production Data Acquisition for the Production Planning and Control},
journal = {Procedia CIRP},
volume = {61},
pages = {487-492},
year = {2017},
note = {The 24th CIRP Conference on Life Cycle Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.11.142},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116313026},
author = {Christina Reuter and Felix Brambring and Thomas Hempel and Phil Kopp},
keywords = {data acquisition, production control, traceability},
abstract = {In order to stay competitive, many manufacturing companies, especially small and medium sized enterprises (SME), face the challenge to transform their production and the corresponding production planning and control (PPC) processes for the upcoming Internet of Things (IoT) Era. Since their time and cost budget is a constraint, SME need to focus particularly on relevant data types, data acquisition points and technologies that will be beneficial for their manufacturing processes. State of the art approaches are lacking in supporting SME sustainably, systematically and company-specific on their way to IoT from a PPC-perspective. Therefore, in this paper a systematic approach is described, which is providing a sustainable, benefit-oriented, and gradual guideline for companies which aim to build an IoT-supported production data acquisition for PPC processes, in order to enable sustainable manufacturing. The developed method is taking into account company-specific production structures through quantitative key performance indicators and qualitative morphological checklists. With the help of the approach proposed in this paper, SMEs are supported systematically on their transformation path of the production data acquisition for the PPC into the IoT Era in order to enable a sustainable production.}
}
@article{201883,
title = {Literature listing},
journal = {World Patent Information},
volume = {55},
pages = {83-98},
year = {2018},
note = {Advanced Analytics of Intellectual Property Information for TechMining},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0172219018301108}
}
@article{ZHANG20171412,
title = {Knowledge management of eco-industrial park for efficient energy utilization through ontology-based approach},
journal = {Applied Energy},
volume = {204},
pages = {1412-1421},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.03.130},
url = {https://www.sciencedirect.com/science/article/pii/S0306261917303756},
author = {Chuan Zhang and Alessandro Romagnoli and Li Zhou and Markus Kraft},
keywords = {Eco-industrial park, Knowledge management, Ontology, Energy efficiency, Waste heat recovery systems, Industrial symbiosis},
abstract = {An ontology-based approach for Eco-Industrial Park (EIP) knowledge management is proposed in this paper. The designed ontology in this study is formalized conceptualization of EIP. Based on such an ontological representation, a Knowledge-Based System (KBS) for EIP energy management named J-Park Simulator (JPS) is developed. By applying JPS to the solution of EIP waste heat utilization problem, the results of this study show that ontology is a powerful tool for knowledge management of complex systems such as EIP. The ontology-based approach can increase knowledge interoperability between different companies in EIP. The ontology-based approach can also allow intelligent decision making by using disparate data from remote databases, which implies the possibility of self-optimization without human intervention scenario of Internet of Things (IoT). It is shown through this study that KBS can bridge the communication gaps between different companies in EIP, sequentially more potential Industrial Symbiosis (IS) links can be established to improve the overall energy efficiency of the whole EIP.}
}